{"index":{"_id":"1"}}
{"datatype":"inproceedings","key":"Chen:2016:TME:2883851.2883951","author":"Chen, Ye and Yu, Bei and Zhang, Xuewei and Yu, Yihan","title":"Topic Modeling for Evaluating Students' Reflective Writing: A Case Study of Pre-service Teachers' Journals","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"1--5","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883951","doi":"10.1145/2883851.2883951","acmid":"2883951","publisher":"ACM","address":"New York, NY, USA","keywords":"LDA, automated grading, education, journal writing, learning analytics, reflection, text mining, topic modeling","abstract":"Journal writing is an important and common reflective practice in education. Students' reflection journals also offer a rich source of data for formative assessment. However, the analysis of the textual reflections in class of large size presents challenges. Automatic analysis of students' reflective writing holds great promise for providing adaptive real time support for students. This paper proposes a method based on topic modeling techniques for the task of themes exploration and reflection grade prediction. We evaluated this method on a sample of journal writings from pre-service teachers. The topic modeling method was able to discover the important themes and patterns emerged in students' reflection journals. Weekly topic relevance and word count were identified as important indicators of their journal grades. Based on the patterns discovered by topic modeling, prediction models were developed to automate the assessing and grading of reflection journals. The findings indicate the potential of topic modeling in serving as an analytic tool for teachers to explore and assess students' reflective thoughts in written journals.","pdf":"Topic Modeling for Evaluating Students' Reflective  Writing: a Case Study of Pre-service Teachers' Journals   Ye Chen, Bei Yu, Xuewei Zhang, Yihan Yu  Syracuse University   New York, USA  {ychen129, byu, xzhang77, yyu41}@syr.edu     ABSTRACT  Journal writing is an important and common reflective practice in  education. Students reflection journals also offer a rich source of  data for formative assessment. However, the analysis of the  textual reflections in class of large size presents challenges.  Automatic analysis of students reflective writing holds great  promise for providing adaptive real time support for students.  This paper proposes a method based on topic modeling techniques  for the task of themes exploration and reflection grade prediction.  We evaluated this method on a sample of journal writings from  pre-service teachers. The topic modeling method was able to  discover the important themes and patterns emerged in students  reflection journals. Weekly topic relevance and word count were  identified as important indicators of their journal grades. Based on  the patterns discovered by topic modeling, prediction models were  developed to automate the assessing and grading of reflection  journals. The findings indicate the potential of topic modeling in  serving as an analytic tool for teachers to explore and assess  students reflective thoughts in written journals.     Categories and Subject Descriptors  K.3.1 [Computer Use in Education]: Computer-assisted  instruction (CAI); I.2.7 [Natural Language Processing] Text  analysis; I.5.4. [Applications] Text processing   General Terms  Algorithms, Measurement, Performance, Experimentation.   Keywords  Text mining, Topic modeling, LDA, Reflection, Journal writing,  Automated grading, Learning analytics, Education.   1. INTRODUCTION  According to Boud et al. [1, p.3], reflection practices are those  intellectual and affective activities in which individuals engage  to explore their experiences in order to lead to new  understandings and appreciation. Journal writing is one of the  commonly used reflective activities [2, 3]. In writing their  journals after class, students step back and reflect on how they  went through the learning activities, and what they have learned.  They can reconstruct the class process, and see the separate  aspects of the class together and derive important and meaningful  knowledge from it [4]. Besides, in reflection, they make their own  interpretation of the class activities and content to make sense of   their learning experience. Mezirow believes that learning happens  when they use this reflection-based interpretation to guide  subsequent actions and understandings [5].   To enhance students learning through reflection, teachers need to  know whether students are engaged in reflective practice, and to  probe what students are exactly caring and thinking about [3].  What course content was reflected in their journals Did they talk  about things not covered in class How deep are they engaged in  reflective thoughts By answering these questions, teachers can  assess student learning outcome, provide informative feedback,  and adjust future teaching. Usually, teachers manually read and  grade journals using a rubric or coding scheme [6, 7], which is  labor-intensive and time-consuming, especially for large classes.  In this study we propose a new method that uses topic modeling  to automatically explore and assess students reflective writing.  By extracting an optimal number of topics from the pre-service  teachers' weekly journals for an education course, we were able to  examine the relevance between common topics in student journals  and the weekly content, additional thoughts that students  mentioned in their journals, and the extent to which the topic  relevance correlates with the received grade. Finally, we used the  topic relevance factors, combined with other textual features like  word count, to develop a writing quality prediction model.    2. LITERATURE REVIEW  Topic modeling is a family of computational methods that  facilitate exploratory analysis of large text collections, extracting  the common themes discussed in the corpora. LSA, an early topic  modeling method, extracts salient topics by examining word co- occurrence. It has been widely used in analyzing and grading  students textual work. Sorour et al. [8] predicted students final  grade based on the topics models generated from using LSA on  students comments in course evaluation. Graesser et al. [9]  applied LSA in an intelligent tutoring system that is capable of  comprehending and grading students written answers in tutorial  dialogue. Wiemer-Hastings and Graesser [10] generated idea  outlines of students essay by clustering analysis on LSA semantic  space. In addition, LSA was also used to track online learners  conceptual development [11], assess students comprehension in  reading tasks [12, 13], generate personalized feedback for  students summary writing [14], and evaluate students  contribution to group discussion [15]. However, Crain et al. [16]  pointed out that LSA is problematic in overfitting training data as  its parameters increase linearly with the number of documents.  Later, Blei et al. [17] developed Latent Dirichlet Allocation  (LDA), a probability-based technique with more simplicity.  Similar to LSA, LDA recognizes the coherent topics by finding  the pattern of co-occurrence of words. But LDA treats each  document as a random mixture of topics. Each topic can be  understood as a probability distribution over a collection of  words; while at the same time, a single document is represented as   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883951     a probability distribution over these topics. The Mallet topic  modeling package [18] contains an extremely fast implementation  of LDA, and has been used in many applications.  LDA has seen increasing use in educational context. The  applications have been of three broad types. First, LDA has been  used as a learning analytic tool in MOOCs to analyze students  forum discussions. Reich et al. [19] adopted LDA to identify  themes and patterns in students MOOC forum discussion.  Ramesh et al.[20] built topic models using LDA to explore forum  discussion and predict students survival. Ezen-Can et al.[21]  applied LDA, along with clustering analysis of online discussion,  to extract the topical themes discussed by students. Based on  LDA, Hsiao and Awasthi [22] proposed a Topic Facet Model to  uncover the content latent structure of students discussion posts.   Second, researches have employed LDA to analyze students  essays. Gibson and Kitto [23] identified students thoughts by  discovering significant topics in their reflective texts. Southavilay  et al. [24] used LDA and its extension DiffLDA to extract topics  in students collaborative writing with cloud writing tools (e.g.  google doc). They tracked topic evolution during their writing  process, and generated topic-based collaboration networks by  linking topics with author contribution. Kakkonen et al. [25]  applied LDA to automatic essay grading.  Third, LDA has been used for the purpose of document analysis  or recommendation. Sekiya et al. [26] proposed a LDA-supported  method to analyze course syllabus and compare curriculums.  Kovanovi, et al. [27] discovered the key themes in MOOC- related mainstream news reports. Chen et al. [28] investigated the  common topics in learning analytics community by analyzing  Twitter archives. Kandula et al. [29] developed a system with  LDA to recommend relevant educational materials to diabetic  patients.   Overall, topic modeling has not been fully exploited that looked  into mining students reflective writing in chronicle setting and  automating the grading of their journals. In this study, we adopted  the topic modeling technique LDA, as implemented in the Mallet  package, to address these issues.   3. METHODS  3.1 Research context and data preparation  The context of this study was a 6-week undergraduate course in  the School of Education at a northeastern university in the United  States. The purpose of this course was to educate pre-service  teachers on integrating technologies into teaching and learning.  Students in this class were encouraged to learn through hands-on  practices and writing reflection journals. In the reflection journal,  students were encouraged to include a narrative description of  their class experience and pick the things personally important to  them.   Based on the journal content, the instructors graded students  journals into three levels. Level 3 is Excellent, if students  demonstrate deep or integrated thinking and search for meaning  inherent in class activities. For example, they may explain the  pedagogical intention and principles behind activities, use analogy  or examples to make a concept real and tangible, refer to external  resources to justify their stands, or personalize the class content  by connecting to their own experience. Level 2 is Great, if  students comprehensively summarize the learning content or give  detailed description of the class process. At this level, they  configure their class experience by memorizing and recalling.  Level 1 is Good, if students make simple description of class   experience and reflect on the surface aspects of learning activities  or knowledge content.   A total of 367 journals were collected from 80 students in three  sections of the class. Table 1 showed the descriptive statistics of  the data. The distribution of the grade levels was illustrated in  Table 2. The main learning content and activities for each week  are summarized in Table 3.   Table 1: Description of the data set   2014   Spring  2014  Fall   2015  Spring   Sum   Week 1 31 16 25 72  Week 2 31 15 26 72  Week 3 31 15 26 72  Week 4 32 14 27 73  Week 5 33 15 30 78  Sum 158 75 134 367   Table 2: Grade level distribution  Grades Excellent (3) Great (2) Good (1)  Journal entries # 121 130 116   Table 3: Weekly Topic  Week Topic  Week 1 Course Introduction and Website Evaluation  Week 2 Word/Powerpoint as Teaching Tools  Week 3 Using Excel as a Tool  Week 4 Electronic Communication Tool  Week 5 Introduction to Assistive Technology    We retrieved the online journals from the Blackboard System, and  then converted them from the Word format to text files using the  tool textutil. The data were then ready for analysis.   3.2 Topic modeling  We first used the topic modeling algorithm provided in the  MALLET toolkit to discover common topics discussed in  students' reflective writings.   A major challenge for tuning topic models is the choice of K, the  number of topics. Selecting the appropriate topic number is  important for obtaining meaningful and useful topics [30]. If the  number is too large, the topics will be redundant; if the number is  too small, the different categories cant be separated from each  other and the topics will be too broad.   K is usually tuned manually by running topic modeling repeatedly  for a set of potential K values, and then choosing the best K value  with the most sense-making results based on prior knowledge in  the task domain. In our task, we explored the K value in the range  between 5 and 30 based on the following rationale. Because this is  a six-week course with five weeks of instruction and one week of  review, we assume the students would at least reflect on the  weekly content from Week 1 to 5, and therefore expect K>=5. The  students may discuss other topics that they consider relevant, but  the additional topics are not expected to be large, at most several  new and common topics each week Thus we assume K<=30. After  fitting topic models with K from 5 to 30, the first author, who was  also the instructor of this class, determined that the topic model  with K=10 makes the most sense based on her prior knowledge.  Each topic in the topic model is represented by a list of significant  keywords, for example, 20 top keywords. We then manually  labeled these topics based on the keywords in each topic.  The manually tuned results may be subject to human bias. In  recent years, more research has been conducted to objectively  evaluate the topic models [31] and estimate K, such as the  term- centric stability  measure developed by Greene et al. [32]. The  basic idea behind this technique is that, a topic model with the     optimal number of topics will be more robust and consistent in  producing similar solutions on data from the same source [33].   Given a range of K values, e.g. (5, 30) in our task, the term-centric  stability measure first fits 26 topic models, one for each K value.  For each candidate K value, 10 sub-samples would be generated  from the data set. Then 10 topic models would be generated, one  for each sub-sample, resulting in 260 topic models in total. Since  every topic is represented by a list of most significant keywords,  an agreement score between two topics is defined as the level of  agreement between the two word lists, inversely weighted by the  word ranks. An agreement score between two topic models is  further defined as the average agreement between every pair of  term lists in these two topic models. The final stability score for  each K is then calculated by averaging the agreement score among  all the 10 topic models. The higher the stability score, the more  robust the model is.   In our task, we found the highest stability score when K=10. See  Figure 1 for the stability scores for K from 5 to 30. Thus, the  manual tuning result and the automated tuning result consistently  support that there are 10 common topics in our data. We then  chose this model as our final model, and then obtained the topic  distribution for each journal.      Figure 1: Stability analysis of different topic solutions   3.3 Correlation analysis and grade prediction  Several factors, such as journals relevance to common topics, the  sentiment that students showed in their writing, and the length of  the writing may be related to the quality of the writing. Thus we  did correlation analysis to test the significance of these factors,  which would then be used to build prediction model. Students  sentiment level was analyzed using the program SentiStrength  [34]. Polarity scores of both positive and negative sentiment were  reported as the output, which ranged from 1 to 5 and -5 to -1  respectively.    Three types of classification algorithms were used to build  prediction models: Nave Bayes, Decision Tree J48, and Support  vector machines (SVM). We selected the three algorithms because  they have been commonly used in data mining of students- generated data in education context (e.g. [35, 36]). Three  algorithms were implemented in Weka, and the accuracy of  prediction models was evaluated through 10-fold cross validation.   4. RESULTS & DISCUSSIONS  4.1 Common topics in reflection journals  10 common topics were discovered from the students reflection  journals. Table 4 summarizes the samples of top keywords and the  manual label that we assigned to each topic. We also compared  the weekly teaching content against the discovered topic clusters,  and found that our topic model has well recognized the weekly  teaching content.   Besides the weekly content, students also mentioned some  additional topics. Four such topics were discovered: age &  computer (cluster #3), instructional strategies (cluster #9),   positive sentiment toward class activities (cluster #7), and the  topic about class activities in a general sense (cluster #5).   Table 4: Topic and the key terms   Clust er #   Topic label Keywords   0 Word-PPT-  Story   activity students word story create powerpoint  make created read creating power point writing  pumpkin order stories show book creative  microsoft    1 Class  introduction  (activity)   lesson work teaching activities learn  understand group groups ideas teach concept  working topic video lessons instructional task  strategy subjects walking    2 Assistive  technology   picture children pictures words assistive  disabilities ipad centers visual student  technologies instructions software web  sentences young read called reading station    3 Age &  computer   teacher good today information questions  computer easy ide made kids making time find  elementary answer interactive younger age  website specific    4 Excel use excel grade create class make data grades  reflection microsoft assignment survey classes  book directions teachers graphs instructions  question taught chart    5 General students great program student learn tool  learning classroom tools feel worked helps  involved extremely step makes easily walk  peers opportunity    6 Class  introduction  (content)   technology learning ways ide reflection  websites classrooms skills types computers  integration education experience discussed  content helped stations multiple focused teach    7 Positive  sentiment- assignment   class helpful project mini ide interesting fun lot  thought reflection things found week enjoyed  people gave idea beneficial thing todays    8 Communicat ion  technology    teachers website communication teacher  synchronous asynchronous edmodo school  time assignments site social webinar discussed  parents educational online diagram inspiration  talked    9 Instructional  strategies   classroom class learned future today important  strategies programs incorporate make  knowledge integrate management easier asked  share observed setting give apply    4.2 Topic evolution over time  The topic model not only lists the most significant keywords for  each topic cluster, but also estimates the topic distribution for  each document. We hypothesized that if the topic model makes  sense, then each weekly topic should yield highest relevance  among journals in the corresponding week, and the four  additional topics should occur more evenly in each week.  The results in Figure 2 confirmed our hypothesis. In Figure 2, the  x axis represents the journal entries from Week 1 to Week 5, and  the y axis represents the relevance proportions of a journal entry  to the topic in a particular week. Figures 2 (c)-(f) shows that  journal entries in Weeks 2-5 were significantly more relevant to  the topics in the corresponding weeks than other weeks. The  average relevance to Week 1 topics, which were of overview  nature, was slightly higher than that to other weeks' topics.   Figure 2 serves as a validity check of our finding that the topic  model is able to accurately recognize the weekly teaching topic  from students reflection writing, which has laid a foundation for  automatically assessing to what extent students reflect on their  learning experience and how much knowledge they have taken  away from class. If students reflect at a surface level, they might     not clearly articulate what they have learned. In this case, there  would be more statements simply expressing opinion as I like  todays class, This was fun, or I learned a lot from class  activities without pointing to a particular topic. Or, students  might even talk about things that are irrelevant to the class  content. In contrast, if students go beyond the surface level, they  are more likely to connect to specific teaching topics or learning  activities and to retrieve more details about their learning  experience. The more such connections they have built, the more  possible that they construct meaning and understanding from their  experience. The elements they retrieved from the reality can serve  as the mental objects that they can manipulate on in order to  develop their thinking.                           (a) Topic #1-Week1               (b) Topic #6-Week1                         (c) Topic #0-Week2                 (d) Topic #4-Week3                          (e) Topic #8-Week4               (f) Topic #2-Week5   Figure 2: Distribution of weekly topics  In reflections, students were encouraged to identify the things of  their personal importance. Thus, we can discover what students  care about by examining the most common themes in addition to  the weekly teaching topics. In each weeks class, students  explored different computer programs and study how to integrate  them into teaching. It appeared that, in reflections, students were  interested in picking practical matters. For example, they  mentioned/discussed the appropriate age of using the programs,  e.g. A question that I thought of while working on this was what  age group is excel applicable towards, it may not be  appropriate for students younger than fifth grade, or proposed  the instructional strategies that could help integrate the technology  into their future teaching, e.g. Some classroom strategies I  learned about in this class are to put the students in groups in  different ways. Their reflections also involved sentiment  information expressed through terms like helpful,  interesting, fun, enjoyed, and beneficial.   Figure 3 illustrated the relevance between the weekly reflection  and these four additional topics. These four topics were almost  evenly distributed across five weeks, indicating that students  maintain their interests on these topics throughout the course,  regardless of the weekly teaching content. One of the benefits of  mining the frequently mentioned themes in students journals is to  help teachers get to know about what knowledge is valuable from  students perspective and what knowledge have been retained in  students mind after class. The findings based on topic modeling  could guide teachers to develop future teaching content and   activities that are tailored toward students needs, and thus boost  their motivation in learning. For example, in the studied context,  when teaching a certain type of technology, the teachers might  consider to incorporate more discussions on the appropriate age  for using this technology, and more inquires that investigate the  effective strategies for integrating the technology to teaching  practices.                 (a) Topic #3 (Computer & Age)  (b) Topic #9 (Instructional  Strategies)                 (c) Topic #7 (Positive Sentiment)     (d) Topic #5 (General)  Figure 3: Distribution of additional topics that are personally   important to students   4.3 Correlation analysis & Grade prediction   Weekly topic relevance (Pearson r=.15, p<.01) and word count  (Pearson r=.607, p<.001) were found to be significantly correlated  with journal grade. No significant correlation was found between  journal grades and any sentiment levels (positive, negative, and  overall sentiment) or any additional topics.   Using the word count and the weekly content relevance as  attributes, we built three classifiers based on decision tree, nave  Bayes, and SVM algorithms respectively. All classifiers  outperformed the random guess baseline (.333). Among them, the  Nave Bayes model obtained the highest accuracy .651. The J48  model performed at similar level of accuracy .649, while SVM's  accuracy .594 is slightly lower. Decision tree model illustrated the  rules for prediction: all writings with length <=183 words were  classified as level 1; those with length in the (183, 262] range  were classified as level 2; the ones longer than 262 words were  further examined by their weekly content relevance  level 2 if  relevance <= .15, level 3 otherwise.    5. CONCLUSIONS  In this study, by using the topic modeling approach, we conducted  an exploratory analysis of pre-service teachers reflection  journals. The results suggest the potential of topic modeling in  analyzing reflection journals, and indicate that topic modeling can  contribute to the construction of analytic tool for formative  assessment of students learning. Future work may include  experimenting on more factors that might relate with journal  grades, and building a comparison-oriented prediction model by  extracting the characteristics of Excellent level journals.   6. REFERENCES  [1] Boud, D., Keogh, M., and Walker, D. 1985. Reflection: Turning   experience into learning. London: Kogan Page.  [2] Boud, D. 2001. Using journal writing to enhance reflective   practice. New Directions for Adult and Continuing Education. 2001,  90 (2001), 9-18. DOI=10.1002/ace.16   [3] Kember, D. 1999. Determining the level of reflective thinking from  students' written journals using a coding scheme based on the work     of Mezirow. International Journal of Lifelong Education. 18,  1(1999), 18-30. DOI=10.1080/026013799293928   [4] Collins, A., and Brown, J. S. 1988. The computer as a tool for  learning through reflection. Springer US   [5] Mezirow, J. 1990. How critical reflection triggers transformative  learning. Fostering critical reflection in adulthood, Jossey-Bass, San  Francisco, CA, 1-20.   [6] Dyment, J. E., and O'Connell, T. S. 2011. Assessing the quality of  reflection in student journals: a review of the research. Teaching in  Higher Education. 16, 1(2011), 81-97.   [7] Lee, H. J. 2005. Understanding and assessing preservice teachers  reflective thinking. Teaching and Teacher Education. 21, 6(2005),  699-715.   [8] Sorour, S., Goda, K., and Mine, T. 2015. Correlation of topic model  and student grades using comment data mining. In Proceedings of  the 46th ACM Technical Symposium on Computer Science  Education (Kansas City, MO, USA March 4-7, 2015). ACM, New  York, NY, 441-446.   [9] Graesser, A. C., Wiemer-Hastings, P., Wiemer-Hastings, K., Harter,  D., Tutoring Research Group, T. R. G., and Person, N. 2000. Using  latent semantic analysis to evaluate the contributions of students in  AutoTutor. Interactive Learning Environments. 8, 2(2000), 129- 147.   [10] Wiemer-Hastings, P., and Graesser, A. C. 2000. Select-a-Kibitzer: A  computer tool that gives meaningful feedback on student  compositions. Interactive Learning Environments. 8, 2(2000), 149- 169.   [11] Wild, F., Haley, D., and Blow, K. 2011. Using latent-semantic  analysis and network analysis for monitoring conceptual  development. Journal for Language Technology and Computational  Linguistics. 26, 1(2011), 9-21.   [12] McNamara, D. S., Boonthum, C., Levinstein, I. B., and Millis, K.  2007. Evaluating self-explanations in iSTART: Comparing word- based and LSA algorithms. In Handbook of latent semantic  analysis, T.Landauer, D. S.McNamara, S.Dennis, and  W.Kintsch Eds. Mahwah, NJ: Erlbaum, 227-241.   [13] Millis, K., Magliano, J., Wiemer-Hastings, K., Todaro, S., and  McNamara, D. S.2007. Assessing and improving comprehension  with latent semantic analysis. In Handbook of latent semantic  analysis, T.Landauer, D. S.McNamara, S.Dennis, and  W.Kintsch Eds. Mahwah, NJ: Erlbaum, 207-225.   [14] Wade-Stein, D., and Kintsch, E. 2004. Summary Street: Interactive  computer support for writing. Cognition and Instruction. 22,  3(2004), 333-362.   [15] Streeter, L. A., Lochbaum, K. E., LaVoie, N., and Psotka, J. E. 2007.  Automated tools for collaborative learning environments. In  Handbook of latent semantic analysis, T.Landauer, D.  S.McNamara, S.Dennis, and W.Kintsch Eds. Mahwah, NJ: Erlbaum,  279-290.   [16] Crain, S. P., Zhou, K., Yang, S. H., and Zha, H. 2012.  Dimensionality reduction and topic modeling: From latent semantic  indexing to latent dirichlet allocation and beyond. In Mining Text  Data, C. Aggarwal, C., C. Zhai Eds. NY: Springer, 129-161.   [17] Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent dirichlet  allocation. Journal of Machine Learning Research, 3 (2003), 993- 1022.    [18] McCallum, A. 2002. MALLET: A Machine Learning for Language  Toolkit. Retrieve from:  http://mallet.cs.umass.edu.   [19] Reich, J., Tingley, D. H., Leder-Luis, J., Roberts, M. E., and  Stewart, B. 2014. Computer-assisted reading and discovery for  student generated text in massive open online courses. Journal of  learning analytics. 2, 1(2014), 156-184.   [20] Ramesh, A., Goldwasser, D., Huang, B., Daume III, H., and Getoor,  L. 2014. Understanding MOOC discussion forums using seeded  LDA. In Proceeding of 9th Workshop on Innovative Use of NLP for  Building Educational Applications (Baltimore, MD, USA, June 26),  28-33.   [21] Ezen-Can, A., and Boyer, K. E. 2013. Unsupervised classification of  student dialogue acts with query-likelihood clustering.   In Proceedings of Educational Data Mining (Memphis, Tennessee,  USA, July 6-9, 2013). 20-27   [22] Hsiao, I. H., and Awasthi, P. 2015. Topic facet modeling: semantic  visual analytics for online discussion forums. In Proceedings of the  Fifth International Conference on Learning Analytics And  Knowledge (Poughkeepsie, New York, US, March 16-20, 2015).  ACM, New York, NY, 231-235.   [23] Gibson, A., and Kitto, K. 2015. Analysing reflective text for learning  analytics: an approach using anomaly recontextualisation.  In Proceedings of the Fifth International Conference on Learning  Analytics And Knowledge (Poughkeepsie, New York, US, March  16-20, 2015). ACM, New York, NY, 275-279.   [24] Southavilay, V., Yacef, K., Reimann, P., adn Calvo, R. A. 2013.  Analysis of collaborative writing processes using revision maps and  probabilistic topic models. In Proceedings of the Third  International Conference on Learning Analytics and Knowledge  (Leuven, Belgium, April 8-12, 2013). ACM, New York, NY, 38-47.   [25] Kakkonen, T., Myller, N., and Sutinen, E. 2006. Applying latent  dirichlet allocation to automatic essay grading. In Proceedings of  the 5th International Conference on Natural Language Processing  (Turku, Finland, August 23-25, 2006), Berlin/Heidelberg: Springer,  110120.   [26] Sekiya, T., Matsuda, Y., and Yamaguchi, K. 2015. Curriculum  analysis of CS departments based on CS2013 by simplified,  supervised LDA. In Proceedings of the Fifth International  Conference on Learning Analytics And Knowledge (Poughkeepsie,  New York, US, March 16-20, 2015). ACM, New York, NY, 330- 339.   [27] Kovanovi, V., Joksimovi, S., Gaevi, D., Siemens, G., and  Hatala, M. 2015. What public media reveals about MOOCs: A  systematic analysis of news reports. British Journal of Educational  Technology. 46, 3(2015), 510-527.   [28] Chen, B., Chen, X., & Xing, W. 2015. Twitter archeology of  learning analytics and knowledge conferences. In Proceedings of the  Fifth International Conference on Learning Analytics And  Knowledge (Poughkeepsie, New York, US, March 16-20, 2015).  ACM, New York, NY, 340-349.   [29] Kandula, S., Curtis, D., Hill, B., & Zeng-Treitler, Q. 2011. Use of  topic modeling for recommending relevant education material to  diabetic patients. In AMIA Annual Symposium Proceedings  (Washington, DC, US, October 22-26, 2011). American Medical  Informatics Association, 674682.    [30] Law, M. H., & Jain, A. K. 2003. Cluster validity by bootstrapping  partitions. Technical Report MSU-CSE-03-5, Department of  Computer Science and Engineering, Michigan State University,  Michigan, USA 2003    [31] Chang, J., Boyd-Graber, J., Gerrish, S., Wang, C., and Blei. D.  2009. Reading tea leaves: How humans interpret topic models. In  Advances in Neural Information Processing Systems 22, Y. D.  Bengio, J. L. Schuurmans, C. K. I. Williams, and A. Culotta, Eds.  MIT Press, 288296.   [32] Greene, D., OCallaghan, D., & Cunningham, P. 2014. How many  topics Stability analysis for topic models. In Proceeding of  Machine Learning and Knowledge Discovery in Databases -  European Conference, ECML PKDD 14 (Nancy, France, September  15-19, 2014). Springer Berlin Heidelberg, 498-513.    [33] Lange, T., Roth, V., Braun, M. L., & Buhmann, J. M. 2004.  Stability-based validation of clustering solutions. Neural  Computation. 16, 6(2004), 1299-1323.   [34] Thelwall, M., & Buckley, K. 2013. Topic-based sentiment analysis  for the social web: The role of mood and issue-related words.  Journal of the American Society for Information Science and  Technology. 64, 8(2013), 16081617.   [35] Ortigosa, A., Martn, J. M., & Carro, R. M. 2014. Sentiment  analysis in Facebook and its application to e-learning. Computers in  Human Behavior. 31 (2014), 527-541.   [36] Sundar, P. P. 2013. A Comparative Study for Predicting Students  Academic Performance Using Bayesian Network Classifiers. IOSR  Journal of Engineering (IOSRJEN) e-ISSN, (2013), 2250-3021            "}
{"index":{"_id":"2"}}
{"datatype":"inproceedings","key":"Crossley:2016:CCD:2883851.2883931","author":"Crossley, Scott and Paquette, Luc and Dascalu, Mihai and McNamara, Danielle S. and Baker, Ryan S.","title":"Combining Click-stream Data with NLP Tools to Better Understand MOOC Completion","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"6--14","numpages":"9","url":"http://doi.acm.org/10.1145/2883851.2883931","doi":"10.1145/2883851.2883931","acmid":"2883931","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOC, click-stream data, educational data mining, educational success, natural language processing, predictive analytics, sentiment analysis","abstract":"Completion rates for massive open online classes (MOOCs) are notoriously low. Identifying student patterns related to course completion may help to develop interventions that can improve retention and learning outcomes in MOOCs. Previous research predicting MOOC completion has focused on click-stream data, student demographics, and natural language processing (NLP) analyses. However, most of these analyses have not taken full advantage of the multiple types of data available. This study combines click-stream data and NLP approaches to examine if students' on-line activity and the language they produce in the online discussion forum is predictive of successful class completion. We study this analysis in the context of a subsample of 320 students who completed at least one graded assignment and produced at least 50 words in discussion forums, in a MOOC on educational data mining. The findings indicate that a mix of click-stream data and NLP indices can predict with substantial accuracy (78%) whether students complete the MOOC. This predictive power suggests that student interaction data and language data within a MOOC can help us both to understand student retention in MOOCs and to develop automated signals of student success.","pdf":"Combining Click-Stream Data with NLP Tools to Better  Understand MOOC Completion    Scott Crossley  Georgia State University  25 Park Place, Ste 1500   Atlanta, GA 30303  scrossley@gsu.edu        Danielle S. McNamara  Arizona State University   PO Box 872111  Tempe, AZ 85287   dsmcnamara1@gmail.com   Luc Paquette  University of Illinois at Urbana-  Champaign  1310 S. 6th St.   Champaign, IL, 61820  lpaq@illinois.edu     Ryan S. Baker   Teachers College, Columbia  University   525 West 120th Street  New York, NY, 10027   baker2@exchange.tc.columbia.edu     Mihai Dascalu  University Politehnica of Bucharest   313 SplaiulIndepententei  Bucharest, Romania   mihai.dascalu@cs.pub.ro       ABSTRACT  Completion rates for massive open online classes (MOOCs) are  notoriously low. Identifying student patterns related to course  completion may help to develop interventions that can improve  retention and learning outcomes in MOOCs. Previous research  predicting MOOC completion has focused on click-stream data,  student demographics, and natural language processing (NLP)  analyses. However, most of these analyses have not taken full  advantage of the multiple types of data available. This study  combines click-stream data and NLP approaches to examine if  students' on-line activity and the language they produce in the on- line discussion forum is predictive of successful class completion.  We study this analysis in the context of a subsample of 320  students who completed at least one graded assignment and  produced at least 50 words in discussion forums, in a MOOC on  educational data mining. The findings indicate that a mix of click- stream data and NLP indices can predict with substantial accuracy  (78%) whether students complete the MOOC. This predictive  power suggests that student interaction data and language data  within a MOOC can help us both to understand student retention  in MOOCs and to develop automated signals of student success.   Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Computer-assisted  Instruction (CAI); J.5 [Computer Applications: Arts and  Humanities]: Linguistics   General Terms  Algorithms, Measurement, Performance   Keywords  MOOC, click-stream data, educational data mining, natural  language processing, sentiment analysis, educational success,  predictive analytics   1. INTRODUCTION  The considerable size of student populations in massive open  online classes (MOOCs) requires educators and administrators to  rethink traditional approaches to instructor intervention and the  manner in which student engagement, motivation, and success is  assessed [34]. As a result of differences between traditional  classes and MOOCs, a new research agenda focusing on  predicting or explaining attrition and overall student success in  MOOCs has emerged. The most common data available for such  analyses is click-stream data (i.e., student interactions within the  MOOC software). Such data provides researchers with evidence  of engagement within the course and activities associated with  individual course goals [22] and, because of this, most research  assessing student success in MOOCs has involved the  examination of click-stream data. Additional approaches used in  recent studies to assessing student success include the use of  sentiment analysis tools to gauge students affective states [42,  43], linguistic features that measure the sophistication and  organization of student writing within a MOOC [9], and  individual difference measures such as student background and  other demographic variables [16].   In this paper, we combine NLP and click-stream data approaches  to examine success in an educational data mining MOOC as  called for by Crossley and colleagues [9]. Thus, unlike prior  research, we investigate the potential for NLP indices related to  text length, social collaboration, sentiment analysis, text cohesion,  syntactic complexity, lexical sophistication, and writing quality in  conjunction with click-stream data such as lecture viewing and  page views to predict MOOC completion. We hypothesize that  developing complete and predictive understandings of student  outcomes requires multiple sources of data and a variety of  approaches because learning is not simple, but rather a complex  process with multiple layers and time-scales. Relying on a single  data sources, be it language information or click-stream data,  limits the potential for understanding student differences,  especially when automated streams of data from different learner  dimension are readily available.    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions from Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883931        As such, this papers aim is to create an automated model of  MOOC success based on both click-stream and NLP indices. We  focus on student completion rate because it is an important  component of student success [20], not only in itself but because it  predicts participation in a scientific community of practice after  taking the MOOC [45]. The NLP tools we use in conjunction with  traditional click-stream data open a wide array of opportunities for  better understanding student success. Our objective is to examine  the efficacy of combined approach in predicting the probability of  MOOC course performance and completion. The long-term goal  of this research is to inform interventions that provide  personalized feedback in terms of language use and system  interaction to MOOC students or their teachers in order to  increase completion rate, as well as to increase scientific  understanding of the factors associated with MOOC completion.    1.1 Click Stream Data in MOOCS  Researchers and instructors have embraced MOOCs for their  potential to increase accessibility to distance and lifelong learners  [24]. From a research perspective, MOOCs provide a tremendous  amount of data via click-stream logs which provide detailed  records of the students' interactions with course content, including  video lectures, discussion forums, assignments, and additional  course content, within the MOOC platform. These data can be  mined to investigate student completion [1, 2, 18, 19, 23, 39, 46]  and student engagement [22, 38].    Typical measures computed from click-stream data and used in  MOOC analyses includes variables related to the timing of  actions, counts of the different possible types of actions, forum  interactions and assignments attempts [37]. Timestamps in the  click-stream have been used to compute average delays in  watching lectures as they become available [38], time difference  between assignment submissions and assignment deadlines [2. 38]  or to identify students who are lagging behind [18]. Student  interactions with course forums can be used to determine the  number of times each student read different forum thread or  contributed by posting on the forums [1]. Click-stream data has  also been used to examine how students interact with course  assignments by calculating the number of distinct problems a  student attempts and the average number of submissions by  problem [2]. Data about interaction with video lectures can be  used to compute the percentage of available lectures that the  student has watched [1, 19].   Click-stream has been used to build models predictive of whether  a student will drop out of the course in the following weeks or fail  to obtain a certificate of completion of the course [46].  Researchers have applied machine learning techniques, including  logistic regression [2, 46] and hidden Markov models [1], in order  to create models able to detect at-risk students early in the course.  In addition, researchers have been able to develop models of  student success based on click-stream data that can generalize to  new courses [2, 46]. For instance, Balakrishnan [1] used click- stream data to examine the probability that a student would stay in  the course for the following week. They reported high predictive  power for students that remained in the class (F1 scores of .888),  but low ability to identify the students that dropped (F1 scores of  .115). A similar approach has also been applied to study student  retention in e-Learning courses using learning management  systems (LMS). Accuracy rates in these studies reach around 90- 95% for data that includes course assignments performance (i.e.,   quiz grades and assignment completion) along with demographic  information such as gender and work experience and click-stream  data [26, 27].    In addition to the creation of predictive models of dropout, click- stream data has been used to study student engagement in  MOOCs. Sharma and colleagues [38] studied the behavior of  students in relation to pre-defined engagement profiles. In their  study, they used data about graded assignment submissions to  classify students as either an active student or a viewer. They  further subdivided active students into categories indicating  whether a student earned a certificate with distinction, completed  the course, or failed and subdivided viewers into active viewers,  passive viewers, wiki-users, dropout and completer. Kizilcec and  colleagues [22] applied clustering algorithms to click-stream data  in order to identify four engagement profiles, completing,  auditing, disengaging and sampling, based on students weekly  behavior in the course. As such, click-stream data has been shown  to be a useful tool for mining and modeling student behavior in  MOOCs in that it can predict completion, continued participation,  and engagement.   1.2 NLP and MOOC Analysis  Less frequently mined than click-stream data (at least in the  context of MOOCs) are data related to language use using NLP  tools [7, 42, 43]. NLP refers to the computational examination of  texts linguistic properties. NLP centers on how computers can be  used to understand and manipulate natural language texts (e.g.,  student posts in a MOOC discussion forum) to do useful things  (e.g., predict success in a MOOC) [8]. Traditional NLP tools  focus on a texts syntactic and lexical properties, usually by  counting the length of sentences or words or using databases to  compare the contents of a single text to that of a larger, more  representative corpus of texts. More advanced tools provide  measurements of text cohesion, the use of rhetorical structures,  syntactic similarity, social collaboration, sentiment analysis, topic  development, and sophisticated indices of word usage.  A number of analyses have used NLP techniques to investigate  elements of MOOCs unrelated to student success. For instance,  Elousazizi [17] used linguistic markers of point of view to  determine that MOOC participants showed low levels of cognitive  presence and engagement. Moon, Potdar, and Martin [32] used  emotion terms and semantic similarity among participants to  identify student leaders. Lastly, Chaturvedi, Goldwasser, and  Daume [3] used words related to assessment, technical problems,  politeness, and requests to predict instructor intervention.    The most common NLP approach to analyzing student language  production in MOOCS has been through the use of sentiment  analysis tools. Such tools examine language for positive or  negative emotion words or words related to motivation,  agreement, cognitive mechanisms, or engagement. Of relevance to  the current study are sentiment analyses that predict student  success such as Wen et al. [43], who examined the sentiment of  forum posts in a MOOC to examine trends in students opinions  toward the course and overall success. Wen et al. reported that  students who used words related to motivation had a lower risk of  dropping out of the course. In addition, the more students used  personal pronouns in forum posts, the less likely they were to drop  out of the course. In a similar study, Wen et al [42] reported a  significant correlation between sentiment variables and the  number of students who dropped from a MOOC on a daily basis.  However, Wen et al. did not report a consistent relation between     students sentiment across individual courses and dropout rates  (e.g., in some courses, negative words such as challenging or  frustrating were a sign of engagement), indicating a need for  caution in the interpretation of sentiment analysis tools.   More recently, Crossley et al. [9] used a number of NLP tools to  examine the sophistication of language in forum posts and  whether or not the language used was predictive of MOOC  completion. Crossley et al. drew from a number of linguistic  variables related to text cohesion, syntactic complexity, and  lexical sophistication. Their analysis indicated that language  related to forum post length, lexical sophistication, situational  cohesion, cardinal numbers, trigram production, and writing  quality were significantly predictive of whether a MOOC student  completed the course or not (with an accuracy of 67%). Their  findings supported the notion that students who demonstrate more  advanced linguistic skills, produce more coherent text, and  produce more content specific posts are more likely to complete a  MOOC.   METHODS  The goal of this study is to examine the potential for NLP tools in  conjunction with click-stream data to predict success in a MOOC.  Specifically, we examine student interaction within the system  and the language used by MOOC students in discussion forums to  predict student completion rates. We do not investigate  performance variables such as quiz grades and homework scores  because completion and performance data strongly overlap.    2.1 The MOOC: Big Data in Education  The MOOC of interest for this study is the Big Data in Education  MOOC hosted on the Coursera platform in 2013 as the inaugural  MOOC offered by Teachers College Columbia University. It is  the same MOOC investigated by Crossley and colleagues [9]. The  MOOC was created in response to the increasing interest in the  learning sciences and educational technology communities in  learning to use EDM methods with fine-grained log data. The  overall goal of this course was to enable students to apply a range  of EDM methods to answer education research questions and to  drive intervention and improvement in educational software and  systems. The course covered roughly the same material as a  graduate-level course, Core Methods in Educational Data Mining,  at Teachers College Columbia University. The MOOC ran from  October 24, 2013 to December 26, 2013. The weekly course  comprised lecture videos and 8 weekly assignments. Most of the  videos contained in-video quizzes (that did not count toward the  final grade).    All the weekly assignments were automatically graded, and  composed of numeric input or multiple-choice questions. In each  assignment, students were asked to conduct an analysis on a data  set provided to them and answer questions about it. In order to  receive a grade, students had to complete this assignment within  two weeks of its release with up to three attempts for each  assignment. The best score out of the three attempts was counted  towards the final grade. The course had a total enrollment of over  48,000 in the time before the official course end, but a much  smaller number actively participated; 13,314 students watched at  least one video; 1,242 students watched all the videos; 1,380  students completed at least one assignment; and 710 made a post  in the discussion forums. Of those with posts, 426 completed at  least one class assignment; 638 students completed the online  course and received a certificate (meaning that some students   earned a certificate without participating in the discussion forums  at all).    1.3 Student Completion Rates  We selected completion rate as our variable of success because it  is one of the most common metrics used in MOOC research [19].  We compute completion rates based on a smaller sample of forum  posters as described below. Completion was pre-defined as  earning an overall grade average of 70% or above. The overall  grade was calculated by averaging the 6 highest grades extracted  out of the total of 8 assignments.   1.4 Discussion Posts  Discussion posts are of interest within research on student  participation in MOOCs because they are one of the few instances  in x-MOOCs that provide students with the opportunity to engage  in social learning [34, 43]. Discussion forums provide students  with a platform to exchange ideas, discuss the lectures, ask  questions about the course, and seek technical help, all of which  lead to the production of language in a natural setting. Such  natural language can provide researchers with a window into  individual student motivation, linguistics skills, writing strategies,  and affective states. This information can in turn be used to  develop models to improve student learning experiences [34]. In  the EDM MOOC, students and teaching staff participated in  weekly forum discussions. Each week, new discussion threads  were created for each week's content including both videos and  assignments under sub-forums. Forum participation did not count  toward students final grades. For this study, we focused on the  forum participation in the weekly course discussions.  For the 426 students who both made a forum post and completed  an assignment, we aggregated each of their posts such that each  post became a paragraph in a text file. We selected only those  students who produced at least 50 words in their aggregated posts  (n = 320). We selected a cut off of 50 words in order to have  sufficient linguistic information to reliably assess students  language using NLP tools. Of these 320 students, 132 did not  successfully complete the course while the remaining 188 students  completed the course.   1.5 Click-Stream Data  1.5.1 Lectures  Click-stream data allowed us to create variables related to the  students' interactions with video lectures during the course. First,  we computed a global variable that indicated the average number  of times, per active week (i.e., only the weeks in which the student  was actively involved in the MOOC), where the student interacted  with the lectures by accessing (viewing or downloading) a lecture  (M = 12.51; SD = 6.59). Second, we computed the percentage of  available lectures that a student had accessed at the end of each  week (e.g., the percentage of 1st week lectures the student had  accessed by the end of week 1, the percentage of 1st and 2nd  week lectures that were accessed by the end of week 2). We refer  to this variable as the student's weekly lecture coverage. This  coverage was averaged across each week of student activity.  Three versions of this variable were created depending on whether  the student viewed the videos online (M = 0.66; SD = 0.28),  downloaded the videos (M = 0.58; SD = 0.35), or accessed the  videos using either method (M = 0.81; SD = 0.20).   1.5.2 Forum Interaction  We also used the click-stream data to create variables related to  the students' interactions with discussion forums. From this data,     we computed averages, per week, of how much the student was  active by measuring how often a student accessed a forum  (M = 21.91; SD = 18.13), created a post (M = 2.11; SD = 1.46), or  commented on an existing post (M = 1.91; SD = 1.15). In  addition, we looked at students' interactions with the forum  reputation system. When using the forum, students had the option  of providing upvotes and downvotes on posts and comments to  indicate whether it was useful or not. We computed the average  number of times, per week of activity, that a student upvoted  (M = 3.01; SD = 3.36) or downvoted (M = 1.54; SD = 1.12) a post  or a comment on the forums.   1.5.4 Page Views  We computed two variables related to page views in the course.  First, we computed a variable indicating the average number of  times, per active week, that a student accessed any of the course's  web pages (M = 81.53; SD = 45.54). Second, we computed a  similar variable indicating the average number of times, per active  week, that a student accessed the course's syllabus (M = 2.45;  SD = 1.65).   1.5.5 Assignments  Finally, three variables were computed relative to the students'  interactions with the course's assignments. First, we computed the  average number of times, per active week, that a student  submitted an assignment (M = 4.38; SD = 2.22). We also looked  at how quickly students completed the assignments. We computed  variables indicating the average time (in seconds), per active  week, before an assignment's submission deadline that a student  submitted first attempts (M = 645,801.7; SD = 356,234.4) and last  attempts (M = 608,993.1; SD = 352,286.8).   1.6 Natural Language Processing Tools  We used several NLP tools to assess the linguistic features in the  aggregated posts that were of a sufficient length (50 words).  These included the Writing Assessment Tool (WAT [29]), the  Tool for the Automatic Analysis of Lexical Sophistication  (TAALES [25]), the Tool for the Automatic Analysis of  COhesion (TAACO [10]), ReaderBench (RB [11]), and The  SEntiment ANalysis and Cognition Engine (SEANCE). The latter  three tools were not used in Crossley et al. [9]. We provide a brief  description of the indices reported by these tools below.   1.6.1 WAT  WAT was developed specifically to assess writing quality. As  such, it includes a number of writing specific indices related to  text structure (text length, sentence length, paragraph length),  cohesion (e.g., local, global, and situational cohesion), lexical  sophistication (e.g., word frequency, age of acquisition, word  hypernymy, word meaningfulness), key word use, part of speech  tags (adjectives, adverbs, cardinal numbers), syntactic complexity,  and rhetorical features. It also reports on a number of writing  quality algorithms such as introduction, body, and conclusion  paragraph quality and the overall quality of an essay.   1.6.2 TAALES  TAALES is a freely available NLP tool that reports on a number  of indices related to lexical sophistication. TAALES incorporates  about 150 indices related to basic lexical information (e.g., the  number of tokens and types), lexical frequency, lexical range,  psycholinguistic word information (e.g., concreteness,  meaningfulness), and academic language for both single words  and multi-word units (e.g., bigrams and trigrams).   1.6.3 TAACO  TAACO is a freely available NLP tool that reports on a number of  indices related to text cohesion. TAACO incorporates over 150  classic and recently developed indices related to text cohesion.  The cohesion indices reported by TAACO evenly focus on local  cohesion, global cohesion, and overall text cohesion. Local  cohesion refers to cohesion at the sentence level (i.e., cohesion  between smaller chunks of text such as noun overlap between  sentences or linking sentences through connectives) while global  cohesion refers to cohesion between larger chunks of text such as  paragraphs (e.g., noun overlap between paragraphs in a text).  Overall text cohesion refers to the incidence of cohesion features  in an entire text, but not in comparison to other parts of the text  (e.g., lexical diversity which is calculated as the repetition of  words across a text).   1.6.4 ReaderBench   ReaderBench (RB) is an automated software framework that  integrates text mining techniques, advanced natural language  processing, and social network analysis tools in order to predict  and assess learner comprehension [11, 12]. ReaderBench is based  on a cohesion-based representation of the discourse that can be  applied to language from different educational sources (e.g.,  Computer Supported Collaborative Learning  CSCL   conversations conducted in MOOC, textual materials, or  metacognitive explanations) [14]. In terms of CSCL analyses, RB  introduces two computational models used to automatically assess  collaboration based on social knowledge building and voice inter- animation [15]. In addition, RB provides the bases for a multi- dimensional analysis of textual complexity applied on learner  contributions that covers a multitude of factors ranging from  classic readability formulas, surface metrics derived from  automatic essay grading techniques, morphology, syntax indices,  as well as a particular emphasis on semantics [13].   1.6.5 SEANCE  The SEntiment ANalysis and Cognition Engine (SEANCE) is a  freely available sentiment analysis tool that relies on a number of  pre-existing sentiment, social positioning, and cognition  dictionaries. Unlike other sentiment analysis tools commonly used  data mining studies (i.e., LIWC [33]), SEANCE is freely available  and contains part of speech (POS) tags and valence features.  SANCE, TAALES, and TAACO are available at  http://www.kristopherkyle.com/seance.html. SEANCE indices are  taken from freely available source databases such as SenticNet [5,  6] and EmoLex [30, 31]. For many of these dictionaries,  SEANCE also provides a negation feature (i.e., a contextual  valence shifter) that ignores positive terms that are negated. The  negation feature, which is based on [21], checks for negation  words in the three words preceding a target word. SEANCE also  includes the Stanford part of speech (POS) tagger [41] included in  Stanford CoreNLP (Manning et al., 2014). The POS tagger allows  for POS tagged specific indices for nouns, verbs, and adjectives.  POS tagging is an important component of sentiment analysis  because unique aspects of sentiment may reside more strongly in  adjectives or in verbs and adverbs [20, 40]. The SEANCE tool can  report on almost 3,000 indices, but such a large number of indices  can be unwieldy. Thus, SEANCE also reports on 20 component  scores derived from the SEANCE indices.   1.7 Analyses  The click-stream variables and the indices reported by WAT,  TAALES, TAACO, Reader Bench and SEANCE that yielded  non-normal distributions were removed because they violated     statistical assumptions. A multivariate analysis of variance  (MANOVA) was conducted to examine which variables reported  differences between the postings written by students who  successfully completed the course and those who did not. The  MANOVA was followed by stepwise discriminant function  analysis (DFA) using the selected NLP indices that demonstrated  significant differences between those students who completed the  course and those who did not, and did not exhibit multicollinearity  (r > .90) with other indices in the set. In the case of  multicollinearity, the index demonstrating the largest effect size  was retained in the analysis. The DFA was used to develop an  algorithm to predict group membership through a discriminant  function co-efficient. A DFA model was first developed for the  entire corpus of postings. This model was then used to predict  group membership of the postings using leave-one-out-cross- validation (LOOCV) in order to ensure that the model was stable  across the dataset.   Table 1: MANOVA Results Predicting Whether Students  Completed the MOOC  Index F 2  Average weekly lecture coverage  (online views) 64.049** 0.169   Time before deadline for first attempt 20.920** 0.062   Time before deadline for last attempt 18.879** 0.056   Average page views 16.252** 0.049   Average number of lectures accessed 15.173** 0.046   High school essay score 12.800** 0.039   Certainty component score 12.248** 0.037   Type token ratio 11.962** 0.036   Coverage of top 10 topics 11.601** 0.035   New threads started 11.098** 0.034   Number of words produced 11.137** 0.034   Number of contributions 10.835** 0.033   Average post length 10.102* 0.031   Cardinal numbers used 9.789* 0.030   Concreteness 9.892* 0.030   Overall social knowledge building 9.284* 0.029   Tri-gram frequency 9.309* 0.029   Degree of voice inter-animation 9.222* 0.028   Bi-gram frequency 8.869* 0.027  Celex content word frequency (written)  SD 7.774* 0.024   Incidence of periods 7.642* 0.024   Situational cohesion 7.732* 0.024   Word meaningfulness 7.549* 0.023   Word hypernymy SD 7.387* 0.023   Semantic similarity between paragraphs 7.397* 0.023  CELEX content word frequency  (spoken) 6.294* 0.020   Lexical diversity M 6.373* 0.020   Unique named entities per paragraph 6.002* 0.019   Number of upvotes 5.973* 0.019   Number of entities per sentence 5.043* 0.016   Average number of order per paragraph 4.985* 0.016   Verb polarity 4.566* 0.014   Average forum reads 4.066* 0.013   Lexical diversity MTLD 3.996* 0.012  ** p < .001, * p < .050   2. RESULTS  2.1 MANOVA  A MANOVA was conducted using the click-stream and the NLP  indices calculated by WAT, TAALES, TAACO, RB, and  SEANCE as the dependent variables and the postings by students  who completed the course and those who did not as the  independent variables. The strongest predictors were click-stream  variables followed by NLP variables. A number of click-stream  variables related to videos viewed, time before turning in  assignments, page views, and lectures views showed strong effect  sizes. The NLP indices that showed strong effect sizes came from  a range of different tools and measured a variety of language  constructs including writing proficiency, fluency, local and global  cohesion, sentiment, use of numbers, lexical sophistication, n- gram use, named entities, and social collaboration (see Table 1 for  the MANOVA results). These indices were used in the subsequent  DFA.   2.2 Discriminant Function Analysis  A stepwise DFA using the indices selected through the  MANOVA retained ten variables related to videos viewed,  number of entities in the posts, post length, lexical sophistication,  writing proficiency, time before deadline for completing  assignments, cohesion between paragraphs, and certainty (see  Table 2 for indices and standardized co-efficients). The remaining  variables were removed as non-significant predictors.   Table 2: Discriminant Function Co-efficients  Index Co-efficient  Average weekly lecture coverage (online  views) 0.688   Number of entities per sentence -0.358   Average post length (sentences) -0.329   Tri-gram frequency 0.324   High school essay score 0.275   Word hypernymy SD -0.257   Time before deadline for first attempt 0.229   Semantic similarity between paragraphs 0.222   Certainty component score 0.201   Word meaningfulness -0.194               Table 3: Confusion matrix for DFA classifying postings      Predicted       Actual - Cert +Cert F1 score   Whole set  - Certificate 99 32 0.773   +Certificate 39 148 0.766   LOOCV  - Certificate 95 36 0.754   +Certificate 40 147 0.750     The results demonstrate that the DFA using these 10 indices  correctly allocated 247 of the 320 participants in the total set, 2  (df = 1) = 93.893 p < .001, for an accuracy of 77.7%. For the  leave-one-out cross-validation (LOOCV), the discriminant  analysis allocated 242 of the 320 participants for an accuracy of  76.1% (see the confusion matrix reported in Table 3 for results  and F1 scores). The Cohens Kappa measure of agreement  between the predicted and actual completion rate was 0.543 when  LOOCV was conducted, demonstrating moderate agreement.   3. DISCUSSION  Previous MOOC studies have investigated completion rates  though click-stream data or NLP techniques. The current study  combines these two techniques to examine successful MOOC  completion, choosing these variables, instead of performance  variables and demographic information, in order to develop a  model that is actionable and non-tautological. The findings  indicate that variables based on click-stream data were the  strongest predictors of MOOC completion but that NLP variables  were also predictive. The click-stream variables that were most  predictive included the weekly lecture coverage (online views)  and how early students submitted their assignments. In terms of  language features, indices related to the number of entities in a  forum post, the post length (average number of sentences), the  overall quality of the written post, lexical sophistication, cohesion  between posts, and word certainty were also strong predictors of  MOOC completion. Such findings have important implications for  how students interactions within the MOOC (i.e., observed  behaviors) and individual differences (in this case, language  skills) can be used to predict success.    The results indicate that those who completed the course  interacted more within the system on average (not cumulative)  and were more active in forums. For instance, in terms of course  interaction, students who had a greater weekly lecture coverage  (online views), turned in their assignments earlier, viewed more  pages, and accessed lectures more often were more likely to  complete the course. In addition, in terms of forum participation,  students who created more forum threads and contributed more  posts, gave upvotes more often, and read more forum posts were  more likely to complete the MOOC. These findings hold even  though completion depended solely on success on technical  assignments.   The linguistic results demonstrate that students who are more  likely to complete the MOOC produced higher quality writing  samples (as indicated by essay scoring algorithms) and, in some  cases, more sophisticated language as evidenced by less  meaningful (i.e., words with fewer associations), less concrete  words, and longer sentences (i.e., fewer sentences per post).   However, students who were more likely to complete the MOOC  also used more frequent words, bigrams, and trigrams. For a  cohesion perspective, students who were more likely to complete  the MOOC produced writing samples that were more coherent  with lower type-token ratios (i.e., greater repetition of words),  more cohesion between paragraphs (i.e., semantic similarity  between paragraphs), and greater use of connectives (i.e., order  connectives). From a content perspective, students more likely to  complete the MOOC used a greater incidence of cardinal numbers  and fewer named entities indicating a focus on numbers and not  names. Successful students were also more likely to stay on topic  (i.e., reporting higher on-topic relatedness and content coverage  scores) and were more likely to exhibit social collaboration (as  seen in cohesion building graphs). Lastly, from a sentiment  perspective, successful students expressed more certainty and  used more negative verbs (e.g., neglect, dislike).   To illustrate these findings, we present excerpts from two students  in the MOOC (students 97211 and 1780650, see Table 4 for  excerpts). Student 97211 completed the certificate (an average  score of 1), while student 1780650 did not complete the course or  receive a certificate (average score of .50). The linguistic and  click-stream data for each participant trend in a manner similar to  the co-efficients reported in the DFA (see Table 2 for the DFA co- efficients). Mean scores for each of the linguistic indices and  click-stream variables are reported in Table 5).    Table 4: Text excerpts from students that completed and did  not complete the EDM MOOC  Student 97211 (completed the MOOC)     Yes, from the goal of finding temporal patterns (i.e. sequences),  sequences of length 1 don't make much sense. After all, those  were not included in the final slides of the GSP-algoritm part of  the lecture. On the other hand: Why starts the algorithm with  calculating support for the length-1-sequences The algorithm  ought to start with length 2 then. To understand the algorithm, I  just took the letters as symbols, while you interpreted 'a' as   GAMING and BORED 5:05:20 , 'd' as  GAMING and BORED  5.06:20  and so on. But what then is the meaning of triplet 'aad'  Two times gaming+bored at the same time and once again a  minute later    Student 1780650 (did not complete the MOOC)    Hi, In the video lecture W5V3, the rule: If the student took  advanced data mining, then the student took intro to statistics. The  Support/Coverage = 2/11 (why not 7/11) Because the implication  A=>B if B is true than the implication is always true (The student  took advanced data mining doesn't matter anymore) And from the  definition of Support/Coverage: Number of data points that fit the  rule (Fit means that the true is true right) Any ideas                   Table 5: Mean scores for selected students   Index/Variable Student 97211  Student  1780650   Average videos viewed 1 0.187   Number of entities per sentence 0.848 0.591   Average post length 1.344 2.667   Tri-gram frequency 1.822 1.806   High school essay score 6 4   Word hypernymy SD 2.862 2.866  Time before deadline for first  attempt 954201.25 -112404.5   Semantic similarity between  paragraphs 0.312 0.243   Certainty component score 0.191 0.19   Word meaningfulness 96.004 100.691    The excerpts provide some indication of the linguistic differences  between students who completed the MOOC and those that did  not. For instance, student 97211, who completed the MOOC, used  more certainty words (e.g., yes, ought to) and used longer  sentences (i.e., fewer sentences per post). Student 1780650, who  did not complete the MOOC, used short sentences, more  meaningful words (e.g., student, true, number, idea), and less  frequent trigrams (e.g., true is true). Such differences likely  informed the writing quality algorithm as well, which indicated  that student 97211s posts were scored a 6 while student  1780650s posts were scored a 4. However, it should be noted that  these excerpts do not represent the entire population or even the  entire sample size from each participant. Rather, the excerpts  provide an illustration of linguistic trends that are likely captured  on a much larger scale by the machine learning algorithms we  employed in our analyses.  In terms of click-stream variables, student 97211, who completed  the MOOC and received a certificate, viewed all the videos  available on the MOOC, while student 1780650, who did not  complete the MOOC, only viewed 19% of the videos. In addition,  student 97211 had a positive time before the deadline for first  attempt while student 1780650 had a negative time before the  deadline for first attempt. These findings demonstrate that more  successful students viewed more videos and turned in their  assignments early.  Including both linguistic indices and click-stream variables offers  an improvement over models based solely on linguistic features of  about 10%, although we did use a greater number of linguistic  features than previous research [9]. This provides some support  for our hypothesis that better understanding student outcomes  requires multiple sources of data and a variety of approaches  because learning is not simple, but rather a complex process with  multiple layers and time-scales. Relying on a single data source,  such as language features alone, limits the potential for  understanding student differences. However, combine language  features with click-stream data improves both our understanding  of student success and our predictive models   The findings from this study have practical implications. As noted  in previous studies [9], the models developed in this paper could  be used to monitor MOOC students and potentially identify those  students who are less likely to complete the course. Such students   could then be targeted for interventions (e.g., sending e-mails,  suggesting assignments or tutoring) to improve immediate  engagement in the MOOC and promote long-term completion.  Also as noted previously [9], models such as those developed here  require students to produce language samples in order to conduct  the NLP analysis. Language samples are not always readily  available and not always required in MOOCs indicating that NLP  is not a necessary requirement for automatically modeling student  success. However, knowing that models need to be dynamic and  knowing that the number and types of interactions available in a  MOOC are not infinite, NLP techniques would seem a strong  counterpart to click-stream data for developing rigorous models  that are domain independent. MOOCs that want to take fuller  advantage of models such as those reported here should offer  and/or require students opportunities to interact in forums or  produce language samples beyond forum spaces. Such samples  could include collaborative chats or written assignments that can  be automatically scored such as lecture summarizations. Once  MOOCs start to involve a greater amount of language production,  the models discussed here can be tested across class domains and  the generalizability of combined models can be assessed. We  presume that since tacit features of language (like many of those  examined in this study) will remain stable across learners  regardless of topic, models informed through both NLP and click- stream data can be expected to prove to be reliable across  domains, although this remains to be tested in domains beyond the  one studied in this paper (Educational Data Mining). Thus, future  iterations of this work will examine MOOCs in other domains,  MOOCs taught by other instructors, and MOOCs using other  platforms.   4. ACKNOWLEDGMENTS  This research was supported in part by the Institute for Education  Sciences and National Science Foundation (IES R305A080589,  IES R305G20018-02, and DRL- 1418378). Ideas expressed in this  material are those of the authors and do not necessarily reflect the  views of the IES or the NSF.   5. REFERENCES  [1] Balakrishnan, G., & Coetzee, D. 2013. Predicting Student   Retention in Massive Open Online Courses Using Hidden  Markov Models. Electrical Engineering and Computer  Sciences University of California at Berkeley.   [2] Boyer, S., & Veeramachaneni, K. 2015. Transfer Learning  for Predictive Models in Massive Open Online Courses. In  the Proceedings of the International Conference on Artificial  Intelligence in Education.   [3] Bradley, M. M., and Lang, P. J. 1999. Affective norms for  English words (ANEW): Stimuli, instruction manual and  affective ratings. Technical report. The Center for Research  in Psychophysiology, University of Florida.   [4] Cambria, E. and Hussain, A. 2015. Sentic Computing: A  Common-Sense-Based Framework for Concept-Level  Sentiment Analysis. Cham, Switzerland: Springer.   [5] Cambria, E., Havasi, C., & Hussain, A. 2012. SenticNet 2: A  semantic and affective resource for opinion mining and  sentiment analysis. In G. M. Youngblood & P. M. Mcarthy  (Eds.), Proceedings of the 25th Florida artificial intelligence  research society conference (pp. 202-207).   [6] Cambria, E., Speer, R., Havasi, C., & Hussain, A. 2010.  SenticNet: A publicly available semantic resource for  opinion mining. In C. Havasi, D. Lenat, & B. Van Durme     (Eds.), Commonsense Knowledge: Papers from the AAAI  Fall Symposium (pp. 14-18).   [7] Chaturvedi, S., Goldwasser, D., & Daume, H. 2014.  Predicting instructor's intervention in MOOC forums.  Proceedings of the 52nd Meeting of the Association for  Computational Linguistics.    [8] Crossley, S. A. 2013. Advancing research in second language  writing through computational tools and machine learning  techniques. Language Teaching, 46 (2), 256-271.   [9] Crossley, S. A., McNamara, D. S., Baker, R., Wang, Y.,  Paquette, L., Barnes, T., & Bergner, Y. 2015. Language to  completion: Success in an educational data mining massive  open online class. In Santos, O. C., Boticario, J. G., Romero,  C., Pechenizkiy, M., Merceron, A., Mitros, P., Luna, J. M.,  Mihaescu, C., Moreno, P., Hershkovitz, A., Ventura, S., &  Desmarais, M. (eds.) Proceedings of the 8th International  Conference on Educational Data Mining. (pp. 388-392).   [10] Crossley, S. A., Kyle, K., & McNamara, D. S. in press. The  Tool for the Automatic Analysis of Text Cohesion  (TAACO): Automatic Assessment of Local, Global, and  Text Cohesion. Behavior Research Methods.   [11] Dascalu, M., 2014. Analyzing discourse and text complexity  for learning and collaborating, Studies in Computational  Intelligence. Springer, Switzerland.   [12] Dascalu, M., Dessus, P., Bianco, M., Trausan-Matu, S., &  Nardy, A., 2014. Mining texts, learners productions and  strategies with ReaderBench. In Educational Data Mining:  Applications and Trends, A. Pea-Ayala Ed. Springer,  Switzerland, 335377.   [13] Dascalu, M., Stavarache, L.L., Trausan-Matu, S., Dessus, P.,  & Bianco, M., 2014. Reflecting Comprehension through  French Textual Complexity Factors. In 26th Int. Conf. on  Tools with Artificial Intelligence (ICTAI 2014) IEEE,  Limassol, Cyprus, 615619.   [14] Dascalu, M., Trausan-Matu, S., Dessus, P., & Mcnamara,  D.S., 2015. Discourse cohesion: A signature of collaboration.  In 5th Int. Learning Analytics & Knowledge Conf. (LAK'15)  ACM, Poughkeepsie, NY, 350354.   [15] Dascalu, M., Trausan-Matu, S., Mcnamara, D.S., & Dessus,  P., in press. ReaderBench  Automated Evaluation of  Collaboration based on Cohesion and Dialogism.  International Journal of Computer-Supported Collaborative  Learning.   [16] DeBoer, J., Ho, A. D., Stump, G. S., & Breslow, L. 2014.  Changing Course: Reconceptualizing Educational  Variables for Massive Open Online Courses. Educational  Researcher, March, 7484.    [17] Elouazizi, N. 2014. Point of view mining and cognitive  presence in MOOCs: A (computational) linguistic  perspective. Proceedings of the Empirical Methods in  Natural Language Processing Workshop, 32-37.   [18] Halawa, S., Greene, D., & Mitchell, J. 2014. Dropout  Prediction in MOOCs Using Learner Activity Features.  Experiences and Best Practices in and Around MOOCs, 7.   [19] He, J., Bailey, J., Rubinstein, B.I., Zhang, R. 2015.  Identifying At-Risk Students in Massive Open Online  Courses. In Twenty-Ninth AAAI Conference on Artificial  Intelligence.   [20] Hu, M., & Liu, B. 2004. Mining and summarizing customer  reviews. In W. Kim & R. Kohavi (Eds.), Proceedings of the  Tenth ACM SIGKDD International Conference on  Knowledge Discovery and Data Mining (pp. 168-177).   [21] Hutto, C. J., & Gilbert, E. 2014. Vader: A parsimonious rule- based model for sentiment analysis of social media text. In E.  Adar & P. Resnick (Eds.), Proceedings of the Eighth  International AAAI Conference on Weblogs and Social  Media (pp. 216-225).   [22] Kizilcec, R. F.,Piech, C., and Schneider, E. 2013.  Deconstructing disengagement: analyzing  learnersubpopulations in massive open online courses. In the  Proceedings of the Third International Conference on  Learning Analytics and Knowledge, 170-179.   [23] Kloft, M., Stiehler, F., Zheng, Z., & Pinkwart, N. 2014.  Predicting MOOC Dropout over Weeks Using Machine  Learning Methods. The 2014 Conference on Empirical  Methods on Natural Language Processing.   [24] Koller, D., Ng, A., Do, C., and Chen, Z. 2013. Retention and  Intention in Massive Open OnlineCourses. Educause.   [25] Kyle, K., and Crossley, S. A. in press. Automatically  Assessing Lexical Sophistication: Indices, Tools, Findings,  and Application. TESOL Quarterly.   [26] Lauria, E.J., Baron, J.D., Devireddy, M., Sundararaju, V., &  Jayaprakash, S.M. 2012. Mining Academic Data to Improve  College Student Retention: An Open Source Perspective. In  Proceedings of the 2nd Conference on Learning Analytics  and Knowledge, 139-142.   [27] Lykourentzou, I., Giannoukos, I., Nikolopoulos, V., Mpardis,  G., & Loumos, V. 2009. Dropout Prediction in e-Learning  Courses Through the Combination of Machine Learning  Techniques. Computers & Education, 53(3), 950-965.   [28] Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard,  S. J., & McClosky, D. 2014. The Stanford CoreNLP natural  language processing toolkit. In Proceedings of 52nd Annual  Meeting of the Association for Computational Linguistics:  System Demonstrations (pp. 55-60).   [29] McNamara, D. S., Crossley, S. A., & Roscoe, R. 2013.  Natural Language Processingin an Intelligent Writing  Strategy Tutoring System. Behavior Research Methods, 45  (2), 499-515.   [30] Mohammad, S. M., & Turney, P. D. 2010. Emotions evoked  by common words and phrases: Using Mechanical Turk to  create an emotion lexicon. In Proceedings of the NAACL  HLT 2010 Workshop on Computational Approaches to  Analysis and Generation of Emotion in Text (pp. 26-34).  Association for Computational Linguistics.   [31] Mohammad, S. M., & Turney, P. D. 2013. Crowdsourcing a  wordemotion association lexicon. Computational  Intelligence, 29(3), 436-465.   [32] Moon, S., Potdar, S., & Martin, L. 2014. Identifying student  leaders from MOOC discussion forums through language  influence. Proceedings of the Empirical Methods in Natural  Language Processing Workshop, 15-20.   [33] Pennebaker, J. W., Booth, R. J., and Francis, M. E. 2007.  LIWC2007: Linguistic inquiry and word count. Austin,  Texas.     [34] Ramesh, A., Goldwasser, D., Huang, B., Daume, H., and  Getoor, L. 2014. Understanding MOOC Discussion Forums  using Seeded LDA. ACL Workshop on Innovative Use of  NLP for Building Educational Applications, 22-27.   [35] Saif, M., and Turney, P. 2013. Crowdsourcing a Word- Emotion Association Lexicon, Computational Intelligence,  29 (3), 436-465.   [36] Scherer, K. R. 2005. What are emotions And how should  they be measured Social Science Information, 44 (4), 695- 729.   [37] Seaton, D. T., Bergner, Y., Chuang, I., Mitros, P., &  Pritchard, D. E. (2014). Who does what in a massive open  online course Communications of the ACM, 57(4), 5865.    [38] Sharma, K., Jermann, P., & Dillenbourg, P. 2015. Identifying  Styles and Paths Toward Success in MOOCs. In the  Proceedings of the 8th International Conference on  Educational Data Mining, 408-411.   [39] Taylor, C., Veeramachaneni, K., & O'Reilly, U.M. 2014.  Likely to Stop Predicting Stopout in Massive Open Online  Courses. arXiv preprint, arXiv:1408.3382.   [40] Taboada, M., Brooke, J., Tofiloski, M., Voll, K., & Stede, M.  2011. Lexicon-based methods for sentiment analysis.  Computational linguistics, 37(2), 267-307.   [41] Toutanova, K., Klein, D., Manning, C. D., & Singer, Y.  2003. Feature-rich part-of-speech tagging with a cyclic  dependency network. In Proceedings of the 2003 Conference  of the North American Chapter of the Association for  Computational Linguistics on Human Language Technology- Volume 1 (pp. 173-180). Association for Computational  Linguistics.   [42] Wen, M., Yang, D. and Rose, C. P. 2014. Sentiment Analysis  in MOOC Discussion Forums: What does it tell us In the  Proceedings of the 7th International Conference on  Educational Data Mining, 130-137.   [43] Wen, M., Yang, D. and Rose, C. P. 2014. Linguistic  Reflections of Student Engagement in Massive Open Online  Courses. In the Proceedings of the International Conference  on Weblogs and Social Media.   [44] Wang, Y. 2014. MOOC Leaner Motivation and Learning  Pattern Discovery. In the Proceedings of the 7th  International Conference on Educational Data Mining, 452- 454.   [45] Wang, Y.E., Paquette, L., Baker, R. 2015. A Longitudinal  Study on Learner Career Advancement in MOOCs. Journal  of Learning Analytics, 1 (3), 203-206.   [46] Whitehill, J., Williams, J.J., Lopez, G., Coleman, C.A., &  Reich, J. 2015. Beyond Predictions: First Steps Toward  Automatic Intervention in MOOC Student Dtopout.  Available at SSRN 2611750.      "}
{"index":{"_id":"3"}}
{"datatype":"inproceedings","key":"Kovanovic:2016:TAC:2883851.2883950","author":"Kovanovi'c, Vitomir and Joksimovi'c, Sre'cko and Waters, Zak and Gavsevi'c, Dragan and Kitto, Kirsty and Hatala, Marek and Siemens, George","title":"Towards Automated Content Analysis of Discussion Transcripts: A Cognitive Presence Case","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"15--24","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883950","doi":"10.1145/2883851.2883950","acmid":"2883950","publisher":"ACM","address":"New York, NY, USA","keywords":"community of inquiry (CoI) model, content analysis, content analytics, online discussions, text classification","abstract":"In this paper, we present the results of an exploratory study that examined the problem of automating content analysis of student online discussion transcripts. We looked at the problem of coding discussion transcripts for the levels of cognitive presence, one of the three main constructs in the Community of Inquiry (CoI) model of distance education. Using Coh-Metrix and LIWC features, together with a set of custom features developed to capture discussion context, we developed a random forest classification system that achieved 70.3% classification accuracy and 0.63 Cohen's kappa, which is significantly higher than values reported in the previous studies. Besides improvement in classification accuracy, the developed system is also less sensitive to overfitting as it uses only 205 classification features, which is around 100 times less features than in similar systems based on bag-of-words features. We also provide an overview of the classification features most indicative of the different phases of cognitive presence that gives an additional insights into the nature of cognitive presence learning cycle. Overall, our results show great potential of the proposed approach, with an added benefit of providing further characterization of the cognitive presence coding scheme.","pdf":"Towards Automated Content Analysis of Discussion Transcripts: A Cognitive Presence Case  Vitomir Kovanovic School of Informatics  The University of Edinburgh Edinburgh, UK  v.kovanovic@ed.ac.uk  Srecko Joksimovic Moray House School of  Education The University of Edinburgh  Edinburgh, UK s.joksimovic@ed.ac.uk  Zak Waters Queensland University of  Technology Brisbane, Australia  z.waters@qut.edu.au  Dragan Gaevic Moray House School of Education and School  of Informatics The University of Edinburgh  Edinburgh, UK dgasevic@acm.org  Kirsty Kitto Queensland University of  Technology Brisbane, Australia  kirsty.kitto@qut.edu.au  Marek Hatala School of Interactive Arts and  Technology Simon Fraser University  Burnaby, Canada mhatala@sfu.ca  George Siemens LINK Research Lab  University of Texas at Arlington Arlington, USA  gsiemens@uta.edu  ABSTRACT  In this paper, we present the results of an exploratory study that examined the problem of automating content analysis of student online discussion transcripts. We looked at the problem of cod- ing discussion transcripts for the levels of cognitive presence, one of the three main constructs in the Community of Inquiry (CoI) model of distance education. Using Coh-Metrix and LIWC fea- tures, together with a set of custom features developed to capture discussion context, we developed a random forest classification sys- tem that achieved 70.3% classification accuracy and 0.63 Cohens kappa, which is significantly higher than values reported in the pre- vious studies. Besides improvement in classification accuracy, the developed system is also less sensitive to overfitting as it uses only 205 classification features, which is around 100 times less features than in similar systems based on bag-of-words features. We also provide an overview of the classification features most indicative of the different phases of cognitive presence that gives an additional in- sights into the nature of cognitive presence learning cycle. Overall, our results show great potential of the proposed approach, with an added benefit of providing further characterization of the cognitive presence coding scheme.  CCS Concepts  Information systemsClustering and classification; Applied computing  E-learning; Distance learning;  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00  DOI: http://dx.doi.org/10.1145/2883851.2883950  Keywords  Community of Inquiry (CoI) model, content analysis, content ana- lytics, online discussions, text classification  1. INTRODUCTION Online discussions are commonly used in modern higher educa-  tion, both for blended and fully online learning [42]. In distance education, given the absence of face to face interactions, online discussions represent an important component of the whole edu- cational experience. This is especially important for the social- constructivist pedagogies which emphasize the value of social con- struction of knowledge through interactions and discussions among a group of learners [2]. In this regard, the Community of Inquiry (CoI) model [23, 24] represents perhaps one of the best researched and validated models of online and distance education, focused on explaining important dimensions  also known as presences  that shape students online learning experience.  The most commonly used approaches to the analysis of online discussion transcripts are based on the quantitative content analysis (QCA) [12, 54, 51, 16]. According to Krippendorff [37] content analysis is a research technique for making replicable and valid inferences from texts (or other meaningful matter) to the contexts of their use[p18]. In the case of the study presented in this paper, contexts is online learning environments. QCA is a well defined research technique commonly used in social science research, and it makes use of specifically designed coding schemes to analyze text artifacts with respect to the defined research goals and objectives. For instance, the CoI model defines a set of coding schemes which are used by the educational researchers to assess the levels of three CoI presences.  In the domain of educational research, QCA of student discus- sion data have been mainly used for the retrospection and research after the courses are over without an impact on the courses learning outcomes [53]. In the field of content analytics [36]  which focuses    on building analytical models based on the learning content includ- ing student-produced content such as online discussion messages  there have been some attempts to automate some of those coding schemes. Most notable are the efforts of McKlin [44] and Corich et al. [11] on automation of the CoI coding schemes, which served as a starting point for our research in the area [35, 62]. One of the main challenges for automation of content analysis is the fact that the most important constructs from the educational perspective (e.g., student group learning progress, motivation, engagement, so- cial climate) are latent constructs not explicitly present in the dis- cussion transcripts. This means the assessment of these constructs requires human interpretation and judgment.  This paper presents the results of a study that explored the use of content analytics for automating content analysis of student online discussions based on the CoI coding schemes. We focused on au- tomation of the content analysis of cognitive presence, one of the main constructs in the CoI model. By building upon the existing work in the fields of text mining and text classification and our pre- vious work in this area [35, 62], we developed a random forests clas- sifier which makes use of a novel set of classification features and provides a classification accuracy of 70.3% and Cohens  of 0.63 in our cross validation testing. In this paper, we describe the de- veloped classifier and the adopted classification features. We also report on the findings of the empirical evaluation of the classifier and critically discuss the findings.  2. BACKGROUND WORK  2.1 The Community of Inquiry (CoI) model The Community of Inquiry (CoI) model is a widely researched  model that explains different dimensions of social learning in on- line learning communities [23, 24]. Central to the model are the three constructs, also known as presences, which together provide a comprehensive understanding of learning processes [23, 24]: 1) Cognitive presence which is the central construct in the CoI  model and describes different phases of student knowledge con- struction within a learning community [24].  2) Social presence captures different social relationships within a learning community that have a significant impact on the success and quality of the learning process [50].  3) Teaching presence explains the role of instructors during the course delivery as well as their role in the course design and preparation [3].  The focus of this study is on the analysis of cognitive presence, which is defined by Garrison et al. [24] as an extent to which the participants in any particular configuration of a community of in- quiry are able to construct meaning through sustained communi- cation.[p11]. Cognitive presence is grounded in the constructivist views of Dewey [14] and is the element in this [CoI] model that is most basic to success in higher education [23, p89]. Cogni- tive presence is operationalized by the practical inquiry model [24], which defines the following four phases: 1) Triggering event: In this phase, an issue, dilemma or problem  is identified. In the case of a formal educational context, those are often explicitly defined by the instructors; however, they can also be initiated by the other discussion participants [24].  2) Exploration: This phase is characterized by the transition be- tween the private world of reflective learning and the shared world of social construction of knowledge [24]. Questioning, brainstorming and information exchange are the main activities which characterize this phase [24].  3) Integration: In this phase, students move between reflection and discourse. The phase is characterized by the synthesis of the ideas generated in the exploration phase. The synthesis ulti-  mately leads to the construction of meaning [24]. From a teach- ing perspective, this is the most difficult phase to detect from the discussion transcripts, as the integration of ideas is often not clearly identifiable.  4) Resolution: In this phase, students resolve the original prob- lem or dilemma that started the learning cycle. In the formal educational setting, this is typically achieved through a vicari- ous hypothesis testing or consensus building within a learning community [24].  The CoI model defines its own multi-dimensional content analy- sis schemes [23, 24] and 34-item likert-scale survey instrument [5] which are used for the assessment of the three presences. The model has gained a considerable attention in the research community re- sulting in a fairly large number of replication studies and empirical validations (for an overview see [25]) including the studies about the interaction dynamics between the three presences [26]. In general, the model has been shown to be robust, and its coding scheme ex- hibits sufficient levels of inter-rater reliability for it to be considered a valid construct [25].  While the CoI model has been proven to be a very useful model for assessment of the social distance learning, there are several prac- tical issues that still remain open. First, the use of the CoI coding schemes requires a substantial amount of manual work, which is very time consuming and requires trained coders. For example, to code the dataset used in this study, two experienced coders spent around 130 hours each to manually code 1,747 messages [22]. The coding process started with the calibration of the use of the coding scheme which was then followed by the independent coding, and finally reconciliation of the coding disagreements.  One major consequence of manual coding of messages in the CoI model is that it has been used mostly for research purposes and not for the real-time monitoring of students learning progress and guid- ing instructional interventions. This is not unique to the CoI model and is very common with most of content analysis schemes used in education. The lack of automated content analysis approaches has been identified by Donnelly and Gardner [16] as one of the main rea- sons why transcript analysis techniques have had almost zero impact on educational practice. The development of the CoI survey instru- ment [5] is one attempt to eliminate, or at least to lessen the need for the manual content analysis of discussion transcripts. Still, the instrument is based on self-reported survey data, which makes it not so suitable for the real-time monitoring and guidance of student learning.  In order to enable for a broader adoption of the CoI model, the coding process needs to be automated and this is precisely the goal of the current study. While this study focuses on automation of cod- ing online discussion transcripts for the levels of cognitive presence, a more general goal is to automate coding for all three presences, which would enable for a more comprehensive view of social learn- ing phenomena and the development of more sophisticated social learning environments [60]. This in turn could be used by the in- structors to inform their interventions leading to better achievement of learning objectives. From the standpoint of self-regulated learn- ing research [7]  a major theory in contemporary education  in order to regulate their own learning effectively, learners need real- time feedback, which is an inherent catalyst for all self-regulated activities [7]. By providing learners with timely feedback on their own learning and the learning of their peers, they would be in a position to better regulate their own learning activities.  2.2 Automating Cognitive Presence Analysis Several studies have investigated automating content analysis us-  ing the cognitive presence coding scheme. A study by McKlin [44] describes a system built using feed-forward, back-propagation ar-    tificial neural network that was trained on a single semester worth of discussion messages (N=1,997). The classification features were the counts of words in the one of the 182 different word categories as defined in the General Inquirer category model [52]. McKlin [44] also used a binary indicator whether a message is a reply to another message, as triggering events are more likely to be the discussion starters and thus not replies to other messages. Finally, McKlin [44] defined custom categories of words and phrases, which are thought to be indicative of the different phases of cognitive presence and included count of words in those categories as additional classifi- cation features. For example, indicative words category contains compared to, I agree, that reminds me of, and thanks as it is hypothesized that integration messages would contain larger num- ber of these phrases in order to connect the message with the previ- ously given information. Unfortunately, these additional coding cat- egories are very briefly described and thus is not possible to repli- cate them and evaluate their usability in future studies. McKlins findings show that classification system overgeneralized the explo- ration phase and under-generalized the integration phase. Further- more, given the very low frequency of messages in the resolution phase (i.e., < 1% and only 3 messages in total in their data set), the neural network developed by McKlin simply ignored the resolution category and never predicted the resolution phase for any message in the corpus. Overall, they reported Holstis Coefficient of Relia- bility [30] of 0.69 and Cohens of 0.31, which show some potential of the proposed approach with much room for improvement in order to reach reliability levels commonly found among two independent coders  usually Cohens  of at least 0.70 [28].  Following the work of McKlin [44], a study by Corich et al. [11] presented ACAT, a very general classification framework that can support any coding scheme besides cognitive presence which is also based on word count features. In order to use ACAT, users are re- quired to provide a set of labeled training examples, which are used for training of classification models. Furthermore, as ACAT does not specify a particular set of word categories that are used as classi- fication features, users are required to provide definitions (i.e., cate- gory name and list of words) that are used as classification features. Interestingly, the use of the ACAT system is also evaluated on the problem of coding cognitive presence of the CoI model. However, instead of classifying each message to one of the four phases of cognitive presence, Corich et al. [11] classified each sentence of each message to four cognitive presence levels. This poses some theoretical challenges as the CoI coding schemes are originally de- signed to be used for message-level content analysis. The dataset used by Corich et al. [11] consists of 484 sentences originating from 74 discussion messages and they report Holstis coefficient of reli- ability of 0.71 in their best test case. However, given that their re- port did not provide sufficient details about the classification scheme used in terms of the specific indicators for each category of cogni- tive presence, nor did it discuss the types of features that were used for classification, it is hard to evaluate the significance of their re- sults.  Besides the studies by McKlin [44] and Corich et al. [11], we should also mention our previous work in this domain. A study by Kovanovi et al. [35] investigated the use of Support Vector Ma- chines (SVMs) [59] classification for the automation of cognitive presence coding using a bag-of-words approach based on the N- gram and Part-of-Speech (POS) N-gram features. Using a 10-fold cross-validation, a classification accuracy of 0.41 Cohens  was achieved  which is higher than values reported in the previous stud- ies [44, 11].  Several challenges related to the classification of online discus- sion massages based on cognitive presence were observed in our ex- isting work [35]. First, the distribution of classes in the used dataset  (i.e., phases of cognitive presence) was uneven, which is in agree- ment with the findings commonly reported in the literature [25]. This poses some challenges to the classification accuracy. This was already seen in the McKlin [44] study whose classifier completely ignored the resolution phase (as only three messages were coded as being in resolution phase). Secondly, the use of bag-of-words features (i.e., n-grams, POS n-grams, and back-off n-grams) cre- ates a very large feature space (i.e., more than 20,000 features) rel- ative to the number of classification instances (i.e., 1,747) which poses challenge of over-fitting. Next, the use of bag-of-words fea- tures makes the classification system highly domain dependent, as the space of bag-of-words features is defined based on the training set. For instance, a classification system trained on a introductory programming course would likely have a bigram feature java pro- grammingwhich is highly specific to a particular domain and would impede the performance of the classifier in other domains. Finally, given that each message belongs to a discussion and represents a part of the overall conversation, the context of the previous mes- sages in the discussion thread is very important. For example, given the structure and cyclic nature of inquiry process, it is highly un- likely that a discussion would start with a resolution message, or that the first response to a triggering message will be an integration message [22]. These dependencies between discussion messages are not taken into the account when each message is classified in- dependently of other messages in the discussion.  In order to address the challenge of isolated classification of dis- cussion messages, Waters et al. [62] developed a structured classi- fication system using conditional random fields (CRFs) [38]. This classifier does a prediction for the whole sequence of messages within a discussion, taking into the account orderings of messages within a discussion thread. Using a 10-fold cross-validation, the devel- oped classifier achieved Cohens  of 0.48 which is significantly higher than 0.41 Cohens  reported by [35], showing a promise of the structured classification approach. However, there are still cou- ple of unresolved issues which warrant further investigation. First of all, although the classification accuracy is improved, it is still far below the Cohens  of 0.7 which is considered a norm for assessing the quality of the coding in the CoI research community [28]. Sec- ondly, CRFs are an example of black-box classification method [27] that are hard to interpret, which limits their potential use for under- standing how cognitive presence is captured in the discourse.  3. METHOD  3.1 Data set The dataset used in this study is the same dataset that was used  in studies by Kovanovi et al. [35] and Waters et al. [62]. The data comes from a masters level, and research-intensive course in soft- ware engineering offered through a fully online instructional con- dition at a Canadian open public university. The dataset consists of six offerings of the course between 2008 and 2011 with the to- tal of 81 students that produced 1,747 discussion messages (Ta- ble 1). On average, each offering of the course had  13-14 stu- dents (SD = 5.1) that produced on average  291 messages, al- beit with a large variation in the number of messages per course offer (SD = 192.4). The whole dataset was coded by the two ex- pert coders for the four levels of cognitive presence enabling for a supervised learning approach. The inter-rater agreement was excel- lent (percent agreement = 98.1%, Cohens  = 0.974) with a total of only 33 disagreements.  Table 2 shows the distribution of four phases of cognitive pres- ence. In addition to the four categories of cognitive presence, we in- cluded the category other, which is used for messages that did not exhibit signs of any phase of cognitive presence. The most frequent    Table 1: Course offerings statistics  Student count Message count  Winter 2008 15 212 Fall 2008 22 633 Summer 2009 10 243 Fall 2009 7 63 Winter 2010 14 359 Winter 2011 13 237  Average (SD) 13.5 (5.1) 291.2 (192.4) Total 81 1,747  Table 2: Distribution of cognitive presence phases  ID Phase Messages (%)  0 Other 140 8.0% 1 Triggering Event 308 17.6% 2 Exploration 684 39.2% 3 Integration 508 29.1% 4 Resolution 107 6.1%  Average (SD) 349.4 (245.7) 20.0% (10.0%) Total 1,747 100%  messages were exploration messages (39% of messages), while the least frequent were the resolution messages (6% of messages). This large difference between the frequencies of the four phases was ex- pected. It is consistent with the previous studies of cognitive pres- ence [26], which found that a majority of students were not pro- gressing to the later stages of integration and resolution. While there are various interpretations for this pattern, including the va- lidity of the model, the design and expectations of the courses  i.e., not requiring students to move to those phases  seems to be the most compelling reason, as shown by its growing acceptance in the literature [25]. Psychologically, if students are going through the four phases of the practical inquiry model that underlies the cog- nitive presence construct, it does seem reasonable that students will spend more time exploring and hypothesizing different solutions, before they could come up with a final resolution [1, 22]. More- over, as discussions were designed to occur between the third and the fifth week of the course, students did not typically move to the resolution phase this early in the course. Specifically, the discus- sions were organized to provide the students with opportunities to discuss ideas that would inform the individual research projects that they planned for the later stages of the course.  3.2 Feature Extraction While the majority of the previous work related to text classi-  fication is based on lexical N-gram features (e.g., unigrams, bi- grams, trigrams) and similar features (e.g., POS bigrams, depen- dency triplets), we eventually decided not to include N-gram and similar features described in the Kovanovi et al. [35] study for sev- eral reasons. First of all, the use of those features inflates the fea- ture space, generating thousands of features even for small datasets. This strongly increases the chances for over-fitting the training data. Secondly, the use of those features is also very dataset dependent, as data itself defines the classification space. Thus, it is hard to define a fixed set of classification features in advance, as the par- ticular choice of words in the training documents will define what features are used for classification (i.e., what N-gram variables are extracted). Finally and most importantly, given that N-grams and other simple text mining features are not based on any existing the- ory of human cognition related to the CoI model, it is hard to un- derstand what they might theoretically mean. Given that our goal is also to understand how cognitive presence is captured within  discourse, we focused our work on extracting features which are strongly theory-driven and based on empirical studies. In total, we extracted 205 classification features which are described in the re- minder of this subsection.  3.2.1 LIWC features In this study, we used the LIWC (Linguistic Inquiry and Word  Count) tool [57], to extract a large number of word counts which are indicative of different psychological processes (e.g., affective, cognitive, social, perceptual). Our previous research [33] showed that different linguistic features operationalized through the LIWC word categories offer distinct proxies of cognitive presence.  In contrast to extracting N-grams, which produce a very large number of independent features, LIWC provides us with exactly 93 different word counts which are all based on extensive empirical re- search [58, cf.]. LIWC features essentially merge related  and domain-independent  N-gram features together to produce more meaningful classification features. We used the 2015 version of the LIWC software package, which also provides four high-level aggre- gate measures of i) analytical thinking, ii) social status, confidence, and leadership, iii) authenticity, and iv) emotional tone.  3.2.2 Coh-Metrix features For extraction of features for classification we also used Coh-  Metrix [29, 45], a computational linguistics tool that provides 108 different metrics of text coherence (i.e., co-reference, referential, causal, spatial, temporal, and structural cohesion), linguistic com- plexity, text readability, and lexical category use. Coh-Metrix has been extensively used a large number of studies to measure subtle differences in different forms of text and discourse and is currently used by the Common Core initiative to analyze learning texts in K- 12 education [45].  Coh-Metrix has been previously used in the domain of social learning to measure the student performance [17] and development of social ties [32, 34] based on the language used in the discourse. For example, a study by Dowell et al. [17] showed that character- istics of the discourse  as measured by Coh-Metrix  were able to account for 21% of the variability in the performance of active MOOC students. Students performed significantly better when then engaged in exploratory-style discourse, with the high levels of deep cohesion and the use of simple syntactic structures and abstract lan- guage. With the goal of the existing CoI content schemes to pre- scribe different indicators of important socio-cognitive processes in the discourse, the use of Coh-Metrix provides a valuable set of metrics that can be easily extracted and used for automation of the CoI coding schemes.  3.2.3 Discussion context features Drawing on the study by Waters et al. [62], we also focused on  incorporating more context information in our feature space. Thus, we included all features (except unigrams) which were used in the Waters et al. study. Those included:   Number of replies: An integer variable indicating the number of replies a given message received.   Message Depth: An integer variable showing a position of message within a discussion.   Cosine similarity to previous/next message: The rationale be- hind these features is to capture how much a message builds on the previously presented information.   Start/end indicators: Simple 0/1 indicator variables showing whether a message is first/last in the discussion.  As the CoI model  from the perspective of educational psychology  is a process model [25], students cognitive presence is viewed as being developed over time through discourse and reflection. There- fore, in order to reach higher levels of cognitive presence students    need to either:i) construct knowledge in the shared-world through the exchange of a certain number of discussion messages, or ii) con- struct knowledge in the their own private world of reflective learn- ing. Given the social-constructivist view of learning in the CoI model, we can expect that the distribution of messages exhibiting the characteristics of the different phases of cognitive presence will tend to change over time, as the students progress through those phases. Thus, we can expect that triggering and exploration mes- sages will be more frequent in the early stages of the discussions, while integration and resolution messages will be more common in the later stages.  3.2.4 LSA similarity Messages belonging to different phases of cognitive presence are  characterized with various socio-cognitive processes [24]. The trig- gering phase introduces a certain topic in a tentative form, present- ing a concept(s) that might not be completely developed, while the exploration phase further elaborates on various approaches to the inquiry initiated in the triggering phase. More precisely, the explo- ration phase introduces new ideas, divergent from the community, or even several contrasting topics within the same message [49]. On the other hand, the integration phase assumes a continuous pro- cess of reflection and integration, which leads to the construction of meaning from the introduced ideas [24]. Finally, the resolu- tion phase presents explicit guidelines for applying knowledge con- structed through the inquiry process [24, 49]. Based on these in- sights, we assumed that information presented in the various stages of the learning process might have an important influence on mes- sage comprehension. Still, given the differences among the learners and their learning habits, we did not expect this to be manifested as a general rule, but more as a slight tendency which would be useful in combination with the other classification features.  Following the approach suggested by Foltz et al. [20], we used LSA with the sentence as a unit of analysis to define a single vari- able lsa.similarity, which represents the average sentence sim- ilarity (i.e., coherence) within a message. As LSA determines the coherence based on the semantic relatedness between terms (i.e., terms that tend to occur in a similar context) [13], we first had to define a semantic space in which the similarity estimates are given. Having in mind that different discussions might relate to the dif- ferent concepts, we decided to create a separate semantic space for each discussion. We identified the most important concepts from the first message in a discussion with a semantic annotation tool TAGME [19] and then each identified concept was linked to an appropriate Wikipedia page from which we extracted information about that concept [19]. Given that previous studies [55, 21] showed that Wikipedia can be used for estimation of semantic similarity be- tween different concepts, we used information from the extracted pages to construct the semantic space on which LSA similarity of the concepts is calculated.  3.2.5 Number of named entities Based on the work described in [47] and our previous study [35],  we hypothesized that messages belonging to the different phases of cognitive presence would contain different count of named entities (e.g., named objects such as people, organizations, and geographi- cal locations). The basis for this is taken from the definition of the cognitive presence construct [24]. Exploration messages are char- acterized by the brainstorming and exploration of new ideas, and thus, those messages are expected to contain more named entities than integration and resolution messages. Given the subject of the course in which the data for this study were collected, we extracted from each message a number of entities that are related to the com- puter science category of Wikipedia by using the DBPedia Spotlight  annotation tool [46].  3.3 Data preprocessing As the first step in our analysis, we addressed the problem of dif-  ferent number of messages in five classification categories (i.e., four phases of cognitive presence and other). The imbalance of dif- ferent classes can have very negative effects on the results of the classification analyses [56]. Generally speaking, there are two pos- sible ways of addressing this problem [8]: i) cost-sensitive classi- fication, in which different penalties are assigned for misclassifi- cation of instances from different categories (higher penalties for smaller classes), and thus forcing the algorithm to put more em- phasis on properly recognizing smaller classes; and ii) resampling methods, either by oversampling smaller classes, undersampling large classes, or through a combination of these two approaches. Given that cost-sensitive classification is used typically for two class problems (positive vs. negative), where correctly classifying one of the classes is the primary goal of the classifier (i.e., patients with a disease, fraudulent banking transaction), it makes sense to as- sign different misclassification costs as correctly identifying neg- ative class is not important. However, in our case, we are equally interested in all five classes (four cognitive presence categories and the other messages), as they represent different phases in student learning cycles and it is not immediately clear whether misclassi- fication of resolution messages is worse than misclassification of triggering event messages. Thus, in our study, we used resampling techniques and in particular a very popular SMOTE algorithm [9], which is a hybrid approach that combines oversampling the minor- ity class with undersampling of the majority class.  One interesting property of SMOTE is that instead of simply re- sampling minority class instances  which would generate simple copies of the existing data points  it generates new synthetic in- stances which are similar to the existing instances but not exactly the same. For example, in n-dimensional feature space, for every data point (X = {f1, f2, ...fn}) of the class Ci that is selected for resampling, SMOTE:  1) Find K (in our case five) nearest neighboring instances from the classCi. As the distances between originalCi data points are known in advance, the list of K nearest neighbors for all instances in Ci class are calculated and stored in N K ma- trix (where N is the number of data points in the Ci class).  2) Randomly picks one of the identified neighbors (Y ).  3) Generates a new data point Z as:  Z = X + rand(0, 1)  Y  where rand(0, 1) is a function returning a random number between 0 and 1.  Figure 1 shows the results of applying SMOTE to our dataset. As our original dataset consists of 1,747 messages, the class distri- bution would be uniform if each of the classes contained approxi- mately 350 messages (i.e., 1, 747/5  350). Thus, we first user SMOTE oversampling procedure explained previously to generate additional 210, 42, and 243 instances of Other, Triggering, and Resolution classes, respectively. This increased the total num- ber of messages in each of these three classes to 350 messages in total. We then undersampled messages in Exploration and Inte- gration categories to create a smaller groups of also 350 messages. Hence, we removed 334 Exploration messages and 158 Integra- tion messages, to produce smaller groups of also 350 messages in total. Overall, after applying SMOTE the new dataset consists of 1,750 messages, with each of the five categories of messages repre- sented with exactly 350 messages.    Other Trig. Exp. Integ. Resol.  M es  sa ge   C ou  nt 0  35 0  70 0  140  308  684  508  107  Figure 1: SMOTE preprocessing for class balancing. Dark blue  original instances which are preserved, light blue  synthetic instances, red  original instances which are removed.  Besides compensating for class imbalance problem, we also re- moved the two duplicate features that were provided by both LIWC and Coh-Metrix: i) the total number of words in a message, and ii) the average number of words in a sentence. We decided to re- move LIWC values and use only the ones provided by Coh-Metrix. The primary reason for using Coh-Metrix features is consistency, as there are some small differences in how those two systems pro- cess corner cases (e.g., hyphenated words, interpunction signs) and given that Coh-Metrix provides additional set of metrics (e.g., num- ber of sentences, number of paragraphs) we wanted to use consistent calculations for all of the included metrics.  3.4 Model Selection and Evaluation To build our classifier, we used random forests [6], a state-of-the  art tree-based classification technique. A large comparative analysis of 179 general-purpose (i.e., not domain-specific, offline, and un- structured) classification algorithms on 121 different datasets used in the previously published studies by Fernndez-Delgado et al. [18] found that random forests were the top performing classification al- gorithm, only matched by Gaussian kernel SVMs. Random forests are ensemble tree-based method that combines bagging (bootstrap aggregating) with the idea of random-subspace to create a robust classification system which has low variance without increasing the bias [18]. Random forests work by creating a large number of trees and then the final prediction is decided using the majority voting scheme. Each tree is constructed on a different bootstrap sample (sub-sample of the same size with repetition) and evaluated on data- points that did not enter the bootstrap sample (in general, around one third of the training dataset size). In addition, each tree does not use the complete feature set, but has a random selection of N attributes (i.e., a subspace) which are then used for growing an in- dividual tree without any pruning. Random forests are widely used technique that can handle large datasets with thousands of features.  It is important to note that random forests can also be used to measure importance of individual classification features. While im- portance of individual classification features might be calculated in many different ways [41], one popular measure is Mean Decrease Gini (MDG) which is based on the reduction in Gini impurity mea- sure. Generally speaking, Gini impurity index measures how much the data points of a given tree node belong to the same class (i..e, how much they are clean). For every internal (split) node we can measure the decrease in Gini impurity, which shows how useful a given tree node is for separating the data (i..e, how much it reduces the impurity of the resulting groups of data). For random forests, MDG measure for a feature Xj is calculated as a mean decrease in Gini impurity of all tree nodes where a given feature Xj is used.  As there are two parameters used for configuration of random forests (i.e., ntree  number of trees constructed, and mtry  the number of randomly selected features), we used a cross-validation to select the optimal random forest parameters. As the performance  Number of attributes in a tree  Ac cu  ra cy  0.68  0.69  0.70  0.71  0 50 100 150 200  Figure 2: Random forest parameter tuning results.  of random forests typically stabilizes after a certain number of trees are built, we decided to build a large ensemble of 1,000 trees to make sure that convergence is reached. Thus, we focused on se- lecting optimal number of features used in every three (i.e., mtry parameter). We used a 10-fold cross validation and repeated it 10 times in order to reduce variability and get more accurate estimates of cross validated performance. In each run of the cross validation, we examined 20 different values for the mtry parameter: {2, 12, 23, 34, 44, 55, 66, 76, 87, 98, 108, 119, 130, 140, 151, 162, 172, 183, 194, 205}. The exact set of these values is obtained by using the var_seq function from Rs caret package.  Before training and evaluating our classification models, we split data to 75% for model training and 25% for testing. We used strat- ified sampling, so that class distribution in both sub-samples is the same. We selected the best mtry value using the 10 repetitions of the 10-fold cross validation and then reported the classification ac- curacy of the best performing model on the testing data.  3.5 Implementation We implemented our classifier in the R and Java programming  languages using several software packages:  for feature extraction we used Coh-Metrix [45, 29] and LIWC  2015 software packages [58],  for developing random forest classifier, we used the randomForest  R package [40],  for running repeated cross validation and aggregating model per-  formance, we used the caret R package [31],  for running the SMOTE algorithm we used the Weka [63] Java  package, and  for calculation of LSA similarity measure, we used the Text Min-  ing Library for LSA (TML)1. The complete dataset for the study and source code of the implemen- tation is publicly available at github.com/kovanovic/lak16 _classification repository.  3.6 Limitations The major limitations of our approach are related to the size of  our data set. Although we have six course offerings, they are all from the same course at a single university, and together with the particular details of adopted pedagogical and instructional approach they might potentially have an effect on the generalizability of our classification model. Thus, in our future work, we plan to test the generalization power of our classifier on a different dataset, which would preferably also account for other important confounding vari- ables recognized in research of the CoI model such as subject do- main [4], level of education (i.e., undergraduate vs. graduate) [26], and mode of instruction (blended vs. fully online vs. MOOC) [61].  4. RESULTS  1tml-java.sourceforge.net    Table 3: Random forest parameter tuning results  mtry Accuracy Kappa  Min 194 0.68 (0.04) 0.59 (0.04) Max 12 0.72 (0.04) 0.65 (0.05)  Difference 0.04 0.06  0 200 400 600 800 1000  0. 1  0. 3  0. 5  0. 7  Number of trees  Er ro  r  OOB other triggering exploration integration resolution  Figure 3: Best random forest configuration performance.  4.1 Model training and evaluation Figure 2 shows the results of our model selection and evaluation  procedure. The best classification accuracy of 0.72 (SD = 0.04) and 0.65 Cohens  (SD = 0.05) was obtained with mtry value of 12, which means that each decision tree takes into the account only 12 out of 205 features. The difference between the best- and worst- performing configurations was 0.06 Cohens (Table 3), which sug- gest that parameter optimization plays an important role in the final classifier performance. Looking at the best performing configura- tion (Figure 3), we can see that the use of 1,000 trees in an ensem- ble resulted in reasonably stable error rates, with an average out-of- bag (OOB) error rate of 0.29, (i.e., an average misclassification rate for all data points in cases when they were non used in bootstrap samples). As expected, the highest error rates were associated with the undersampled classes (i.e., exploration and integration) and the smallest with the classes that were most heavily oversampled (i.e., resolution and other).  Following the model building, we evaluated its performance on the hold-out 25% of the data. Our random forest classifier obtained 70.3% classification accuracy (95% CI[0.66, 0.75]) and 0.63 Co- hens  which were significant improvements over 0.41 and 0.48 reported in Kovanovi et al. [35] and Waters et al. [62] studies, re- spectively. Table 4 shows the confusion matrix obtained on the test- ing dataset. We can see that the most significant misclassifications are between exploration and integration messages which are hard- est to distinguish. This is already witnessed in the [62] where most of the misclassifications were related to exploration and integration messages.  4.2 Variable importance analysis Figure 4 shows the variable importance measures for all the 205  classification features. The median MDG score was 4.43, with the most of the features having smaller MDG scores, and only few fea- tures having very high MDG scores. Table5 shows the values of top 20 variables based on their MDG scores and their average values in  Table 4: Confusion matrix for the best performing model  Predicted  Actual Other Triggering Explorat. Integrat. Resolut.  Other 79 2 2 2 2 Triggering 5 67 9 6 0 Exploration 9 15 35 27 1 Integration 2 2 23 44 16 Resolution 0 0 4 2 81  Mean Decrease Gini  Va ria  bl e  0  50  100  150  200  0 10 20 30                                                                                                                    Figure 4: Variable importance by Mean Decrease Gini measure. Blue line separates top twenty features.  each class (i.e., cognitive presence phase). We can see that the most important variable was the cm.DESCWC, i.e., the number of words in a message; that is, the longer the message was, the higher the chance of the message was to be in the later phases of the cognitive presence cycle. Also, the number of paragraphs, number of sentences, and average sentence length showed similar trends, with higher values being associated with the later phase of cognitive presence.  The most important Coh-Metrix features were related to lexical diversity of the student vocabulary with the highest lexical diver- sity being displayed by other messages. Standard deviation of the number of syllables  which is an indicator of the use of words of different lengths  had the strongest association with the trig- gering event phase. In contrast, the givenness (i.e., how much of the information in text is previously given) had the highest associ- ation with the resolution phase messages. Finally, the low Flesch- Kincaid Grade level readability score and the low overlap between verbs used had the strongest association with other messages (i.e., messages without traces of cognitive presence).  The most important LIWC features were i) the number of ques- tion marks used, which was strongly associated with the trigger- ing event phase, ii) the number of first person pronouns, which was highly associated with the other (i.e., non-cognitive presence) mes- sages, and iii) use of money-related words, which is mostly associ- ated with the integration and resolution phases.  Message context features also scored high, with message depth being higher for the later stages of cognitive presence, and highest for other messages. A similar trend was observed for similarity with the previous message, which was highest for the integration and resolution messages and lowest for the triggering event mes- sages. In contrast, similarity with the next message and number of replies were highest for triggering events and lowest for the other messages. It is interesting to note that both LSA similarity and the number of named entities obtained high MDG scores. The number of named entities was the second most important feature and was highly associated with the later stages of the cognitive presence cy- cle. A similar trend was also observed for LSA similarity however, its importance was much lower.  5. DISCUSSION Based on the testing results of the developed classifier, we can see  that the use of the LIWC and Coh-Metrix features, together with a small number of thread-based context features could be used to provide reasonably high classification performance. The obtained Cohens  value of 0.63 falls in the range of substantial inter- rater agreement [39], and is just slightly below the 0.70 Cohens  which is the CoI research community commonly used as a threshold value for that is required before coding results are considered valid. We can also see that the parameter tuning plays an important role in optimizing the classifier performance, as the different classifier configurations obtained results different up to 0.05 Cohens  and 0.04% classification accuracy (Table 3).    Table 5: Twenty most important variables and their mean scores for messages in different phases of cognitive presence  Cognitive presence phase  # Variable Description MDG Other Triggering Exploration Integration Resolution  1 cm.DESWC Number of words 32.91 55.41 (61.06) 80.91 (41.56) 117.71 (67.23) 183.30 (102.94) 280.68 (189.62) 2 ner.entity.cnt Number of named entities 26.41 13.44 (15.36) 21.67 (10.55) 28.84 (16.93) 44.75 (24.85) 64.18 (32.54) 3 cm.LDTTRa Lexical diversity, all words 21.98 0.85 (0.12) 0.77 (0.09) 0.71 (0.10) 0.65 (0.09) 0.58 (0.09) 4 message.depth Position within discussion 19.09 2.39 (1.13) 1.00 (0.90) 1.84 (0.97) 1.87 (0.94) 2.00 (0.68) 5 cm.LDTTRc Lexical diversity, content words 17.12 0.95 (0.06) 0.90 (0.06) 0.86 (0.08) 0.82 (0.07) 0.78 (0.07) 6 cm.LSAGN Avg. givenness of each sentence 16.63 0.10 (0.07) 0.14 (0.06) 0.18 (0.07) 0.21 (0.06) 0.24 (0.06) 7 liwc.QMark Number of question marks 16.59 0.27 (0.85) 1.84 (1.63) 0.92 (1.26) 0.58 (0.82) 0.38 (0.55) 8 message.sim.prev Similarity with previous message 16.41 0.20 (0.17) 0.06 (0.13) 0.22 (0.21) 0.30 (0.24) 0.39 (0.19) 9 cm.LDVOCD Lexical diversity, VOCD 15.43 12.92 (33.93) 28.99 (50.61) 53.57 (54.68) 83.47 (43.00) 97.16 (28.95)  10 liwc.money Number of money-related words 14.38 0.21 (0.69) 0.32 (0.74) 0.32 (0.75) 0.65 (1.12) 0.99 (1.04) 11 cm.DESPL Avg. number of paragraphs sent. 12.47 4.26 (2.98) 6.37 (2.76) 7.49 (4.11) 10.17 (5.64) 14.05 (8.88) 12 message.sim.next Similarity with next message 11.74 0.08 (0.14) 0.34 (0.40) 0.20 (0.22) 0.22 (0.24) 0.22 (0.23) 13 message.reply.cnt Number of replies 11.67 0.42 (0.67) 1.44 (1.89) 0.82 (1.70) 1.10 (2.66) 0.84 (1.24) 14 cm.DESSC Sentence count 11.67 4.28 (3.17) 6.36 (2.75) 7.49 (4.11) 10.17 (5.64) 14.29 (10.15) 15 lsa.similarity Avg. LSA sim. between sentences 9.69 0.29 (0.27) 0.47 (0.23) 0.54 (0.23) 0.62 (0.20) 0.67 (0.17) 16 cm.DESSL Avg. sentence length 9.60 11.88 (6.82) 13.62 (5.85) 16.69 (6.54) 19.36 (8.39) 21.73 (8.61) 17 cm.DESWLsyd SD of word syllables count 8.92 0.98 (0.69) 1.33 (0.70) 0.98 (0.18) 0.97 (0.14) 0.97 (0.11) 18 liwc.i Number of FPS pronouns 8.84 4.33 (3.53) 2.82 (2.06) 2.37 (1.94) 2.51 (1.65) 2.19 (1.23) 19 cm.RDFKGL Flesch-Kincaid Grade Level 8.29 7.68 (4.28) 10.30 (3.50) 10.19 (3.11) 11.13 (3.46) 11.99 (3.37) 20 cm.SMCAUSwn WordNet overlap between verbs 8.14 0.38 (0.25) 0.48 (0.20) 0.51 (0.13) 0.50 (0.10) 0.47 (0.06)  MDG - Mean decrease Gini impurity index, FPS - first person singular  Given that the same dataset is used as in the [35] and [62] stud- ies, it is possible to directly compare the results of the classification algorithms. The obtained Cohens  is 0.15 and 0.22 higher than the ones reported by Waters et al. [62] and Kovanovi et al. [35], re- spectively. Furthermore, the resulting feature space is much smaller, with only 205 classification features in total, which is 100x smaller than the number of bag-of-words features extracted by Kovanovi et al. [35] classifier. This limits the chances of over-fitting the train- ing data and also improves the performance of the classifier. This is particularly important for the prospective use of the classifier in different subject domains, and pedagogical contexts.  Another important finding of this study is the list of important classification features. We see that a small subset of features is highly predictive of the different phases of cognitive presence, while a majority of the features have a much lower predictive power (Fig- ure 4). It is interesting to notice that most of the discussion context features (except the discussion start/end indicators) obtained high importance scores, indicating the value in providing contextual in- formation to the classification algorithm. In our future work, we will focus on investigation of the additional features that would provide even more contextualized information to the classifier.  It is important to notice that the list of the most important vari- ables is aligned with the conceptions of cognitive presence in the existing CoI literature. If we look at the messages in the four phases of cognitive presence, we can see that the higher levels of cognitive presence are associated with messages that are i) generally longer, with more sentences and paragraphs, ii) adopt more complex lan- guage with generally longer sentences, iii) include more named en- tities (e.g., names of different constructs, theories, people, compa- nies, and geographical locations) iv) have lower lexical diversity, v) occur later in the discussion, vi) have higher givenness of the information, higher coherence, and higher verb overlap, vii) use fewer question marks and first-person singular pronouns, viii) ex- hibit higher similarity with the previous messages, and ix) more frequently use money-related terms. Interestingly, the feature of the highest importance is also the simple word count implying that the longer the message, the more likely it is in the higher levels of cogni- tive presence cycle. This is also consistent with the findings of a pre- vious study with the same dataset [33]. Joksimovi et al. [33] found that word count was the only LIWC 2007 variable that yielded sta-  tistically significant differences among all four cognitive presence categories. This is not totally surprising as the similar findings are reported by essay grading studies who found that the strongest pre- dictor of the final essay grade is the length of the essay [48].  Looking at the non-cognitive or other messages, we can see that they are characterized by the large lexical diversity. This is expected, as non-cognitive messages tend to be shorter (i.e., fewer words, paragraphs, and sentences) and more informal. Higher lev- els of lexical diversity are known to be associated with very short tests or texts of low cohesion [10]. As other messages often are not related to the course topic, they also tend to have a lower num- ber of named entities, and lower givenness and verb overlap. Such messages also tend to adopt a simpler language, as indicated by the lowest scores on the Flesch-Kincaid grade level. Other messages also tend to occur more frequently near the end of the discussion, as indicated by their high values for message.depth feature and also more often are related to expression of personal information, as indicated by the highest values for the use of first-person singu- lar pronouns. This is expected as many discussions would typically finish with students thanking each other for their contributions.  6. CONCLUSIONS This paper has twofold contributions. First, we developed a clas-  sifier for coding student discussion transcripts for the levels of cog- nitive presence with a much higher performance (0.63 Cohens ) than previously reported ones [35, 62] in the studies with the same dataset. The performance of the developed classifier is in the range which is generally considered to be a substantial level of agree- ment [39]. We can see that the proposed approach, which is based on the use of Coh-Metrix, LIWC, and discussion context features, shows a great promise for providing a fully automated system for coding cognitive presence. The feature space that is used is also much smaller, which limits the chances for over-fitting the data and makes the developed classifier more generalizable to other contexts.  Secondly, we can see a particular subset of classification features that are very highly predictive of the different phases of cognitive presence. The most predictive feature is simple word count, which implies that the longer the message is, the higher the chances are for the message to display higher levels of cognitive presence. We also identified several additional features which are also highly pre-    dictive of the cognitive presence phase, in particular the number of named entities that are used (higher values are associated with integration and resolution phase) and lexical diversity (lower val- ues are associated with other and triggering messages). We also see that features that provide information on the discussion context (i.e., similarity the with previous/next message, order in the discus- sion thread, and number of replies) are highly valuable and provide important information to the classification algorithm.  In our future work, we will focus on exploring additional fea- tures for improving the classification performance [43]. The study presented in this paper and our previous work [35] indicate that con- textual features have a significant effect on classification accuracy and we will examine additional features of this kind. As our results reveal that the number of named entities has a significant effect on classification accuracy, and we will further explore similar features, such as concept maps [64], which would provide additional infor- mation about relationships between important concepts discussed in text-based messages. Finally, we will look at the different data preprocessing steps, including the use of the different algorithms for resolving the class imbalance problem. As we also observed that some of the students used direct quotes of other student mes- sages which can cause problems for many of the text metrics that we used for classification, we will further examine the effects of the quotation on the final classification accuracy.  Finally, following the results presented in [15], we are explor- ing ideas for the development of a system that would  beside class labels  provide associated probabilities. Such a classifier could be used to develop a semi-automated classification system in which only one part of the data for which probabilities are sufficiently high would be automatically classified, and the rest would be manually classified. This would be advantageous as the combined desired accuracy of automatic-manual coding could be reached by setting a corresponding probability threshold. For achieving high levels of accuracy, a large majority of data would be classified automati- cally eliminating the large part of the manual work. Besides using it for coding discussion transcripts for research purposes, such sys- tem could be use, for example, to provide a real-time overview of the progress for a group of students and to point out the students for which an progress estimates are uncertain.  ACKNOWLEDGMENTS  This work was partially supported by the JISC 2 Effective Learn- ing Analytics grant No.36 titled Automated System for Cognitive Presence Coding.  REFERENCES [1] Z. Akyol, J. B. Arbaugh, M. Cleveland-Innes, D. R. Garrison, P. Ice,  J. C. Richardson, and K. Swan. A response to the review of the com- munity of inquiry framework. Journal of distance education, 23(2), 2009. url: http://www.ijede.ca/index.php/jde/article/view/630/884.  [2] T. Anderson and J. Dron. Three generations of distance education pedagogy. The international review of research in open and distance learning, 12(3):8097, 2010. url: http://www.irrodl.org/index.php/ irrodl/article/view/890/.  [3] T. Anderson, L. Rourke, D. R. Garrison, and W. Archer. Assessing teaching presence in a computer conferencing context. Journal of asynchronous learning networks, 5:117, 2001. url: http://auspace. athabascau.ca/handle/2149/725.  [4] J. B. Arbaugh, A. Bangert, and M. Cleveland-Innes. Subject mat- ter effects and the community of inquiry (coi) framework: an ex- ploratory study. The internet and higher education, 13(1):3744, 2010.  2https://www.jisc.ac.uk/  [5] J. Arbaugh, M. Cleveland-Innes, S. R. Diaz, D. R. Garrison, P. Ice, J. C. Richardson, and K. P. Swan. Developing a community of inquiry instrument: testing a measure of the community of inquiry frame- work using a multi-institutional sample. The internet and higher ed- ucation, 11(34):133136, 2008.  [6] L. Breiman. Random Forests. Machine learning, 45(1):532, 2001.  [7] D. L. Butler and P. H. Winne. Feedback and self-regulated learning: a theoretical synthesis. Review of educational research, 65(3):245 281, 1995.  [8] N. V. Chawla, N. Japkowicz, and A. Kotcz. Editorial: special issue on learning from imbalanced data sets. ACM SIGKDD explorations newsletter, 6(1):16, 2004.  [9] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. Smote: synthetic minority over-sampling technique. Journal of arti- ficial intelligence research:321357, 2002. url: https://www.jair. org/media/953/live-953-2037-jair.pdf.  [10] Coh-Metrix 3.0 indicies. http : / / cohmetrix . com / documentation _ indices.html.  [11] S. Corich, K. Hunt, and L. Hunt. Computerised content analysis for measuring critical thinking within discussion forums. Journal of e- learning and knowledge society, 2(1), 2012. url: http : / /www.je- lks.org/ojs/index.php/Je-LKS_EN/article/view/700.  [12] B. De Wever, T. Schellens, M. Valcke, and H. Van Keer. Content analysis schemes to analyze transcripts of online asynchronous dis- cussion groups: a review. Computers & education, 46(1):628, 2006.  [13] S. Deerwester, S. T. Dumais, G. W. Furnas, T. K. Landauer, and R. Harshman. Indexing by latent semantic analysis. Journal of the american society for information science, 41(6):391407, 1990.  [14] J. Dewey. My pedagogical creed. School journal, 54(3):7780, 1897.  [15] P. Dnmez, C. Ros, K. Stegmann, A. Weinberger, and F. Fischer. Supporting CSCL with automatic corpus analysis technology. In Pro- ceedings of th 2005 conference on computer support for collabo- rative learning: learning 2005: the next 10 years!, 2005, 125134. url: https://telearn.archives-ouvertes.fr/hal-00190638.  [16] R. Donnelly and J. Gardner. Content analysis of computer conferenc- ing transcripts. Interactive learning environments, 19(4):303315, 2011. url: http://eprints.teachingandlearning.ie/3930/.  [17] N. Dowell, O. Skrypnyk, S. Joksimovi, A. C. Graesser, S. Daw- son, D. Gaevi, P. d. Vries, T. Hennis, and V. Kovanovi. Modeling Learners Social Centrality and Performance through Language and Discourse. In Proceedings of the 8th International Conference on Educational Data Mining (EDM 2015), 2015. url: http : / / www. educationaldatamining .org / EDM2015/ proceedings / full250- 257 . pdf.  [18] M. Fernndez-Delgado, E. Cernadas, S. Barro, and D. Amorim. Do we need hundreds of classifiers to solve real world classification prob- lems The journal of machine learning research, 15(1):31333181, 2014. url: http://jmlr.org/papers/v15/delgado14a.html.  [19] P. Ferragina and U. Scaiella. Fast and accurate annotation of short texts with wikipedia pages. Software, ieee, 29(1):7075, 2012.  [20] P. W. Foltz, W. Kintsch, and T. K. Landauer. The measurement of tex- tual coherence with latent semantic analysis. Discourse processes, 25:285307, 1998. url: http://eric.ed.gov/id=EJ589329.  [21] E. Gabrilovich and S. Markovitch. Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artifical Intelligence. Morgan Kaufmann Publishers Inc., 2007, pp. 16061611. url: http: //dl.acm.org/citation.cfmid=1625275.1625535.  [22] D. Gaevi, O. Adesope, S. Joksimovi, and V. Kovanovi. Externally- facilitated regulation scaffolding and role assignment to develop cog- nitive presence in asynchronous online discussions. The internet and higher education, 24:5365, 2015.  [23] D. R. Garrison, T. Anderson, and W. Archer. Critical inquiry in a text-based environment: computer conferencing in higher education. The internet and higher education, 2(23):87105, 1999.  [24] D. R. Garrison, T. Anderson, and W. Archer. Critical thinking, cog- nitive presence, and computer conferencing in distance education. American journal of distance education, 15(1):723, 2001.  [25] D. R. Garrison, T. Anderson, and W. Archer. The first decade of the community of inquiry framework: a retrospective. The internet and higher education, 13(12):59, 2010.  [26] R. Garrison, M. Cleveland-Innes, and T. S. Fung. Exploring causal relationships among teaching, cognitive and social presence: student perceptions of the community of inquiry framework. The internet and higher education, 13(12):3136, 2010.  [27] L. Getoor. Introduction to Statistical Relational Learning. MIT Press, 2007. isbn: 978-0-262-07288-5.    [28] P. Gorsky, A. Caspi, I. Blau, Y. Vine, and A. Billet. Toward a coi pop- ulation parameter: the impact of unit (sentence vs. message) on the results of quantitative content analysis. The international review of research in open and distributed learning, 13(1):1737, 2011. url: http://www.irrodl.org/index.php/irrodl/article/view/1073.  [29] A. C. Graesser, D. S. McNamara, and J. M. Kulikowich. Coh-Metrix Providing Multilevel Analyses of Text Characteristics. Educational researcher, 40(5):223234, 2011.  [30] O. R. Holsti. Content analysis for the social sciences and humanities. Addison-Wesley Reading, MA, 1969.  [31] M. K. C. f. Jed Wing, S. Weston, A. Williams, C. Keefer, A. En- gelhardt, T. Cooper, Z. Mayer, B. Kenkel, t. R Core Team, M. Ben- esty, R. Lescarbeau, A. Ziem, L. Scrucca, Y. Tang, and C. Candan. Caret: classification and regression training. R package version 6.0- 58, 2015. url: http://CRAN.R-project.org/package=caret.  [32] S. Joksimovi, N. Dowell, O. Skrypnyk, V. Kovanovi, D. Gaevi, S. Dawson, and A. C. Graesser. Exploring the Accumulation of So- cial Capital in cMOOC Through Language and Discourse. Submit- ted, 2015.  [33] S. Joksimovi, D. Gaevi, V. Kovanovi, O. Adesope, and M. Hatala. Psychological characteristics in cognitive presence of communities of inquiry: A linguistic analysis of online discussions. The internet and higher education, 22:110, 2014.  [34] S. Joksimovi, V. Kovanovi, J. Jovanovi, A. Zouaq, D. Gaevi, and M. Hatala. What Do cMOOC Participants Talk About in Social Media: A Topic Analysis of Discourse in a cMOOC. In Proceed- ings of the Fifth International Conference on Learning Analytics And Knowledge, 2015, pp. 156165.  [35] V. Kovanovi, S. Joksimovi, D. Gaevi, and M. Hatala. Automated Content Analysis of Online Discussion Transcripts. In Proceedings of the Workshops at the LAK 2014 Conference co-located with 4th In- ternational Conference on Learning Analytics and Knowledge (LAK 2014), 2014. url: http://ceur-ws.org/Vol-1137/.  [36] V. Kovanovi, S. Joksimovi, D. Gaevi, M. Hatala, and G. Siemens. Content Analytics: the definition, scope, and an overview of pub- lished research. In, Handbook of Learning Analyitcs, 2015.  [37] K. H. Krippendorff. Content analysis: an introduction to its method- ology. Sage Publications, 2003.  [38] J. Lafferty, A. McCallum, and F. C. Pereira. Conditional random fields: probabilistic models for segmenting and labeling sequence data. In Proceedings of the eighteenth international conference on machine learning (ICML 01), 2001. url: http://dl.acm.org/citation. cfmid=655813.  [39] J. R. Landis and G. G. Koch. The measurement of observer agree- ment for categorical data. Biometrics, 33(1):159174, 1977.  [40] A. Liaw and M. Wiener. Classification and regression by random- forest. R news, 2(3):1822, 2002. url: http://CRAN.R-project.org/ doc/Rnews/.  [41] G. Louppe, L. Wehenkel, A. Sutera, and P. Geurts. Understanding variable importances in forests of randomized trees. In Advances in neural information processing systems 26, 2013, pp. 431439. url: http://media.nips.cc/nipsbooks/nipspapers/paper_files/nips26/281. pdf.  [42] R. Luppicini. Review of computer mediated communication research for education. Instructional science, 35(2):141185, 2007.  [43] E. Mayfield and C. Penstein-Ros. Using feature construction to avoid large feature spaces in text classification. In Proceedings of the 12th annual conference on genetic and evolutionary computation, 2010, 12991306.  [44] T. McKlin. Analyzing Cognitive Presence in Online Courses Using an Artificial Neural Network. PhD thesis. Georgia State University, College of Education, 2004.  [45] D. S. McNamara, A. C. Graesser, P. M. McCarthy, and Z. Cai. Au- tomated Evaluation of Text and Discourse with Coh-Metrix. Cam- bridge University Press, 2014.  [46] P. N. Mendes, M. Jakob, A. Garca-Silva, and C. Bizer. DBpedia spotlight: shedding light on the web of documents. In Proceedings of the 7th international conference on semantic systems, 2011, 18.  [47] J. Mu, K. Stegmann, E. Mayfield, C. Ros, and F. Fischer. The ACODEA framework: developing segmentation and classification schemes for fully automatic analysis of online discussions. International journal of computer-supported collaborative learning, 7(2):285305, 2012.  [48] E. B. Page and N. S. Petersen. The computer moves into essay grad- ing: Updating the ancient test. Phi delta kappan, 76(7):561, 1995. url: http://search.proquest.com/docview/218533317/abstract.  [49] C. L. Park. Replicating the Use of a Cognitive Presence Measure- ment Tool. Journal of interactive online learning, 8:140155, 2, 2009. url: http://www.ncolr.org/issues/jiol/v8/n2/replicating-the-use-of- a-cognitive-presence-measurement-tool#.VrVSebKUFhE.  [50] L. Rourke, T. Anderson, D. R. Garrison, and W. Archer. Assessing social presence in asynchronous text-based computer conferencing. The journal of distance education / revue de lducation  distance, 14(2):5071, 2007. url: http://eric.ed.gov/id=EJ616753.  [51] L. Rourke, T. Anderson, D. R. Garrison, and W. Archer. Method- ological issues in the content analysis of computer conference tran- scripts. International journal of artificial intelligence in education (IJAIED), 12:822, 2001.  [52] P. J. Stone, D. C. Dunphy, and M. S. Smith. The general inquirer: a computer approach to content analysis. MIT press, 1966.  [53] J.-W. Strijbos. Assessment of (computer-supported) collaborative learn- ing. IEEE transactions on learning technologies, 4(1):5973, 2011.  [54] J.-W. Strijbos, R. L. Martens, F. J. Prins, and W. M. G. Jochems. Con- tent analysis: what are they talking about Computers & education, 46(1):2948, 2006.  [55] M. Strube and S. P. Ponzetto. WikiRelate! Computing Semantic Re- latedness Using Wikipedia. In Proceedings of the 21st National Con- ference on Artificial Intelligence - Volume 2. AAAI Press, 2006, pp. 1419 1424. isbn: 978-1-57735-281-5. url: http : / / dl .acm.org/ citation . cfmid=1597348.1597414.  [56] P.-N. Tan, V. Kumar, and M. Steinbach. Introduction to Data Mining. Addison-Wesley Longman Publishing Co., Inc., 2005. isbn: 0-321- 32136-7.  [57] Y. R. Tausczik and J. W. Pennebaker. The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of language and social psychology, 29(1):2454, 2010.  [58] Y. R. Tausczik and J. W. Pennebaker. The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. Journal of language and social psychology, 29(1):2454, 2010.  [59] V. N. Vapnik. Statistical learning theory. Wiley-Interscience, 1998.  [60] J. Vassileva. Toward social learning environments. IEEE transac- tions on learning technologies, 1(4):199214, 2008.  [61] N. Vaughan and D. R. Garrison. Creating cognitive presence in a blended faculty development community. The internet and higher education, 8(1):112, 2005.  [62] Z. Waters, V. Kovanovi, K. Kitto, and D. Gaevi. Structure mat- ters: Adoption of structured classification approach in the context of cognitive presence classification. In Proceedings of the 11th Asia Information Retrieval Societies Conference, AIRS 2015, 2015.  [63] I. H. Witten, E. Frank, and M. A. Hall. Data mining: practical ma- chine learning tools and techniques. Morgan Kaufmann, 3rd ed., 2011.  [64] A. Zouaq and R. Nkambou. Building domain ontologies from text for educational purposes. IEEE transactions on learning technologies,  1(1):4962, 2008.    "}
{"index":{"_id":"4"}}
{"datatype":"inproceedings","key":"Hu:2016:TPE:2883851.2883959","author":"Hu, Xiao and Zhang, Yinfei and Chu, Samuel K. W. and Ke, Xiaobo","title":"Towards Personalizing an e-Quiz Bank for Primary School Students: An Exploration with Association Rule Mining and Clustering","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"25--29","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883959","doi":"10.1145/2883851.2883959","acmid":"2883959","publisher":"ACM","address":"New York, NY, USA","keywords":"association rule mining, clustering, e-quiz bank, reading","abstract":"Given the importance of reading proficiency and habits for young students, an online e-quiz bank, Reading Battle, was launched in 2014 to facilitate reading improvement for primary-school students. With more than ten thousand questions in both English and Chinese, the system has attracted nearly five thousand learners who have made about half a million question answering records. In an effort towards delivering personalized learning experience to the learners, this study aims to discover potentially useful knowledge from learners' reading and question answering records in the Reading Battle system, by applying association rule mining and clustering analysis. The results show that learners could be grouped into three clusters based on their self-reported reading habits. The rules mined from different learner clusters can be used to develop personalized recommendations to the learners. Implications of the results on evaluating and further improving the Reading Battle system are also discussed.","pdf":"Towards Personalizing An E-quiz Bank for Primary School  Students: An Exploration with Association Rule Mining   and Clustering   Xiao Hu   Faculty of Education,  University of Hong Kong  Pokfulam, Hong Kong  xiaoxhu@hku.hk   Yinfei Zhang  Faculty of Education,   University of Hong Kong  Pokfulam, Hong Kong   zhangyinfei99@gmail.com   Samuel K. W. Chu  Faculty of Education,   University of Hong Kong  Pokfulam, Hong Kong  samchu@hku.hk   Xiaobo Ke  School of Management   Guangdong University of  Technology   1833724585@qq.com      ABSTRACT  Given the importance of reading proficiency and habits for young  students, an online e-quiz bank, Reading Battle, was launched in  2014 to facilitate reading improvement for primary-school students.   With more than ten thousand questions in both English and Chinese,  the system has attracted nearly five thousand learners who have  made about half a million question answering records. In an effort  towards delivering personalized learning experience to the learners,   this study aims to discover potentially useful knowledge from  learners reading and question answering records in the Reading  Battle system, by applying association rule mining and clustering  analysis. The results show that learners could be grouped into three  clusters based on their self-reported reading habits. The rules mined   from different learner clusters can be used to develop personalized  recommendations to the learners. Implications of the results on  evaluating and further improving the Reading Battle system are also  discussed.   Categories and Subject Descriptors   Information systems  Association rules    Information  systems  Clustering    Applied computing  E-learning   Keywords  Association rule mining; clustering; e-quiz bank; reading.   1. INTRODUCTION  Reading proficiency is fundamental for students as it is closely  associated with their learning abilities and academic performance  [2]. It is widely recognized that the advancement of reading abilities  is essential to primary-school students as it facilitates their   understanding of academic materials and tasks [1], [2]. As a result,  students with higher reading proficiency tend to perform better in  academic activities [1], [2].    Reading Battle (http://battle.cite.hku.hk/) is an online e-quiz system  designed to develop primary-school students reading interests and  enhance their reading proficiency [2]. Launched in 2014, Reading   Battle has included over 500 English and Chinese books available in  local school libraries, as well as more than 10,500 well-tailored  questions related to these books. The books are categorized into a   23-category taxonomy (e.g., fiction or folklore), in English and  Chinese (see Table 4). Each question in the system is also encoded  with a question category (ranging from information search,  interpretation and synthesis, inference to evaluation) and a  difficulty level (ranging from level 1 to level 4). The materials are  cataloged and developed by reading literacy experts including  language teachers and school librarians, with the guidance of the  international PIRLS assessment framework on childrens reading  literacy [2].    After reading a book registered to the system, a student can log in to  the Reading Battle system and take a quiz corresponding to that  book. Around 30 questions are designed for each book and each  time the system randomly selects 10 questions to form a quiz for the   student. The questions in Reading Battle are in the same language  (English or Chinese) as that of the books they are associated with.  The bilingual setting is uniquely designed, following the policy of  two written languages and three spoken codes in Hong Kong. To  date, the Reading Battle system has attracted over 30 primary  schools in Hong Kong and Taiwan, a public library summer  program and a kindergarten in the U.S., with a total of over four  thousand student users.   A significant trend in the education field is to deliver personalized  learning experience to the learners. There has been strong empirical  evidence on the positive effects of personalized learning on  academic performance and learning effectiveness [3]. As there are   different categories of books and questions, and students interests  and abilities vary, it would be more effective if the system can  provide personalized learning experience by recommending books a  student likes and questions that fit his or her abilities.   This study thus aims to discover potentially useful knowledge from  reading and question answering records of primary-school students  enrolled in Reading Battle. The analysis results will be used to  improve the system design and develop personalized   recommendations of books and questions to learners. As our first  attempt along this direction, this paper aims to answer the following  research questions:    1) What kinds of books and questions have the students read and  answered   2) Are there interesting association rules among the books the  students read   3) Are there interesting association rules among the questions the  students answered      Answers to the first question help develop overall understanding of  the usage patterns of books and questions since the launch of     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than the author(s) must be honored. Abstracting  with credit is permitted. To copy otherwise, or republish, to post on servers or  to redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom   Copyright is held by the owner/author(s). Publication rights licensed to ACM.   ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883959     Reading Battle. The examination of association rules on the book  level will reveal potentially preferable reading orders. An  association rule on the book level would be like if a student reads  book X (or books in category A), he/she is likely to read book Y (or  books in category B). Such rules can be readily used to make book  recommendations to students based on their reading records.    The examination of association rules on the question level will   provide insights on the question design. If a student answers  questions in category A (or level X) correctly, association rule  mining will help find out whether this student is likely to answer  questions in category B (or level Y) correctly. Such rules will help   indicate whether the design of question categories and difficulty  levels is suitable for the student users. Suggestions for system  improvement can be made based on the indications. Furthermore,  the results can be used to generate personalized question sets to   accommodate individual students abilities.    By analyzing learners question answering records in Reading  Battle, this study helps develop a comprehensive understanding on  how primary-school students use an e-quiz bank for reading,   particularly a bilingual system such as Reading Battle. It also  provides evidence on how well the system is serving the student  users, based on what suggestions for further improvement of the  system can be proposed. Furthermore, the rules mined from   learners records can be used to deliver personalized book  recommendations and question selection, which will likely be a  significant enhancement to the learning experience. Overall, the  study is expected to provide empirical evidence on how learning   analytics can be applied to enhance the understanding of learners  and improve the learning environment.    2. METHODS  2.1 Association Rule Mining and Clustering  Association rule mining and clustering are common data mining  methods used to analyze user data from learning systems [6].   Association rule mining is used to explore the relationships between  items in a large dataset. It helps discover rules in the form of  premise  conclusion which stands for if the premise occurs in  the dataset, the conclusion is likely to occur as well. Such rules are  very useful for disclosing relationships between items in a dataset.   Frequent Pattern Growth (FP-Growth) algorithm is used for  association rule mining in this study, for its efficiency and clarity as  implemented in the RapidMiner toolkit [5]. Whether a resultant rule  is sufficiently strong or interesting is decided by a series of   measures. In this study, we follow the suggestions from Merceron  and Yacef [7] on rule mining in the education domain, and employ  the Cosine and Lift measures of interestingness to evaluate rules  output by the algorithm, with the minimum Cosine threshold set as   0.65 and the minimum Lift as 1.10 [7]. Considering both measures  helps ensure the retained rules are meaningful and interesting.  Clustering is a data mining technique for discovering proximity  patterns in a given dataset [4]. Data samples are separated into   different groups such that a high similarity is expected within groups  while different groups should be able to differentiate themselves  from each other. Agglomerative clustering algorithm is adopted in  this study for its deterministic nature and flexibility in deriving the   number of resultant clusters.   2.2 Data Collection and Analysis  The data analyzed in this study is a snapshot of the Reading System  taken in May 2015. There were a total of 3,175 students producing  26,189 book reading records and 457,235 question answering   records in Reading Battle as of May 12, 2015. Among these active  users, 523 of them responded to an online questionnaire about their   reading behaviors, preferences and attitudes towards reading. As this  group of learners and their parents have given consent to the project  team to analyze the data, in this study we extracted and analyzed the   119,377 system records of these learners. Two sets of user data were  extracted from the system: 1) students reading records, each  including book title, unique ID of the books, book category and  language; 2) students quiz records, each of which includes the  question, question answer, question category and difficulty level.  Clustering was applied to group the students into clusters based on  their questionnaire answers. Association rule mining was applied to  analyze the data on four levels: 1) book category; 2) individual   book; 3) question difficulty level; and 4) question category.    3. RESULTS AND DISCUSSION  3.1 Student Clusters  A total 523 responses were received for the online survey and the  demographic information is summarized in Table 1. All but five of  the students were primary students ranging from Grade 1 to Grade 6.   The majority of them were in Grade 5, accounting for 67.4% of the  students considered in this study.   Table 1: Student demographics     Characteristics N %   Gender   Male 270   51.6   Female 253   48.4   Total 523 100.0   Grade   P1     3     0.6   P2    15     2.9   P3    56   10.8   P4    81   15.6   P5 349   67.4   P6   14     2.7   Missing     5     1.0   Total 523 100.0          Figure 1: Centroid plot of three clusters   Cluster analysis was carried out to group the students into clusters  based on their responses to 12 questions related to reading habits,  including hours and frequencies of reading various kinds of books,  as well as hours of watching TV and playing games. The     agglomerative clustering results showed three major clusters and  thus three clusters were formed. The normalized values of the  cluster centroids are presented in Figure 1. Cluster 1 (n=91) consists   of students with long reading hours and a high reading frequencies,  and thus it is identified as the cluster with good reading habits.  Cluster 3 (n=124) with short reading hours and low reading  frequencies is identified as the cluster with bad reading habits.  Cluster 2 (n=308) lies in between the other two clusters, with middle  values of reading time and reading frequencies. It is thus identified  as the cluster with moderate reading habits.    Association rule mining was then applied to mine interesting rules   from the reading and question answering records among all students  and students in each cluster. Table 2 provides an overview of the  interesting and non-redundant rules for each student group on the  book and question levels.    Table 2: Numbers of interesting and non-redundant association  rules for each student cluster    Book  category   Individual  book   Question  category   Question  level   All students   1   0 19 29  Cluster 1 46 16 43 77  Cluster 2 11   4 12 24  Cluster 3   2   0 15 20   3.2 Book-level Rules  3.2.1 Book Category  Twenty three book categories were defined for English and Chinese  books in the Reading Battle. Table 3 presents the number of book in   each category and how many times each category has been read.  Picture book and fiction were among the most popular categories in  both languages. In addition, English fantasy, Chinese fairytale and  Chinese science book were also read quite frequently.    Table 3: Book categories   Book category  English Book Chinese book   Number  Times   being read  Number   Times being  read   Biography     2   19     7     63   Detective story     2   15     9   137  Fable     7 101     1    36   Fairytale     9   71   26   283  Fantasy   38 114     2     18   Fiction   30 124   60   285  Folklore     6   28   21   227   Nonfiction   16   35   23     80  Picture book   57 163   17   221   Science   14   73   34   299  Science &   picture book      1   25     0      0   Fantasy &   picture book      0     0     1      43   History     0     0     4    104   Total 182 768 205 1,796   Based on the criteria described in Section 2.1, only one interesting  rules was mined among all 523 students considered in our study. A  closer look at candidate rules discloses their low Support values,  which means most of the students read books in different categories.   However, the Support values of candidate rules increase when  student clusters are considered separately. This indicates students  with similar reading habits (in the same cluster) tended to read  books in the same categories more often than students with different   reading habits (in different clusters). Table 4 shows all the   interesting rules mined on book categories in records of students in  Cluster 2 (moderate reading habits), two of which are also found  among those mined from Cluster 3 (bad habits). In fact, Cluster 3  has very few interesting rules (n=2), probably due to the fact that  those students did not read much and thus had few records in the  system. Most of the rules in Table 4 are also included in rules mined  from Cluster 1 (good habits) which in fact has much more rules  (n=46) that cannot be enumerated in Table 4. In addition, Cluster 1  is the only cluster with rules involving English book categories. Not  surprisingly, this is because students in this cluster read a lot and  produced many records in the system. Table 5 presents a sample of   interesting rules mined from Cluster 1 and their measures.    Table 4: Interesting and non-redundant rules on book categories  for student clusters   No. Premises Conclusion Cluster 1  Cluster   2  Cluster   3   1 (fairytale)    (folklore)     -   2  (fiction)   (fairytale),    (science)    -   3  (folklore)   (fairytale),    (science)    -   4  (folklore)   (fairytale),    (fiction)    -   5   (fairytale),     (science)    (fiction),     (folklore)   -  -   6  (fairytale)   (science),    (fiction)    -   7  (science)    (picture book)     -   8     (picture book)    (science)   -   9  (fiction)    (folklore)     -   10  (science)   (fairytale),    (fiction)    -   11  (science)    (folklore)   -  -   12  (fairytale)  (fiction)     13  (fiction)  (fairytale)      Recommendations can be made based on the rules for each cluster.  For example, if a student with good or moderately good reading  habits (Cluster 1 or 2) has read a Chinese fairytale, he/she is likely   to read a Chinese folklore (Rule #1 in Table 4). Next time when  he/she logs in to the system, a recommendation list of Chinese  folklores can be presented on his/her homepage. In the case of  Cluster 3 (bad habits), after exhausting the two rules of its own (i.e.  recommending Chinese fiction to students who have read Chinese  fairytale and vice versa), rules from Cluster 2 (moderate habits) can  be borrowed and applied to the students in Cluster 3 (bad habits).  The premise of this rule borrowing strategy lies in that rules in a  student cluster with better reading habits can be beneficial to  students in another cluster with worse reading habits, and that rules  in one cluster can largely be acceptable by students in an adjacent  cluster. Similarly, when rules in Cluster 2 (moderate habits) have all  been applied to a student in Cluster 2, rules in Cluster 1 (good  habits) can be applied to this student. As there are more rules in  clusters with better reading habits, this strategy can help mitigate the  potential problem of rule deficiency. Of course, the effectiveness of  the strategy needs to be evaluated with students, which will be   conducted in our future work.      Table 5: Sample of interesting rules on book categories for students with good reading habits (Cluster 1)  Premises Conclusion Support Confidence Lift Cosine  Fable (fiction) ,(fairytale), picture book, fiction 0.21 0.76 2.93 0.78  Fable (science), picture book 0.21 0.76 2.67 0.74   Fable (fiction), (picture book), (fairytale) 0.21 0.76 2.10 0.66   Fiction (fiction), (fairytale), (folklore), picture book 0.22 0.59 2.38 0.72    (picture book) ,   (fairytale), fiction  (folklore), picture book 0.21 0.76 2.44 0.71           3.2.2 Individual Book  A total of 387 books were read by the students, among which 362  books were read more than one time. No interesting rules are  discovered in the entire student population or among the students   in Cluster 3. A few interesting and non-redundant rules are mined  for Cluster 2 (n=4) and Cluster 1 (n=16). The reason for the small  number of rules is that there are many individual books in the  system and reading records of individual books are sparse.   Limited recommendations of individual books can be made based  on the rules from these student clusters. Cluster 3 can then borrow  the rules from the other two clusters considering that the other two  clusters have better reading habits and more interesting rules.   More reading records are needed to establish interesting and  strong rules on the individual book level.    3.3 Question-level Rules  There were 4,436 and 5, 968 English and Chinese questions that  have been answered by the students respectively. Their  distribution across difficulty levels and categories is shown in   Table 6. As in books, there were more Chinese questions  answered than English ones in all difficulty levels and categories  except for Evaluation.    Table 6: Question difficulty level and category   English   Questions  Chinese   Questions  Difficulty level  Level 1 1,540 1,577   Level 2 1,439 1,756  Level 3 648 1,461  Level 4 809 1,174   Total 4,436 5,968   Category  Information search (infn) 2,184 2,139  Interpretation & synthesis (inte) 781 2,036   Inference (infer) 901 1,386  Evaluation (eva) 570    407   Total 4,436 5,968   Interesting association rules mined from all students are listed in  Table 7, Rule 1 to 16 on question difficulty levels and Rule 17 to  27 on question categories. It is not surprising that answering  questions of higher difficulty levels correctly would imply   answering those of lower difficulty levels correctly, for both  English and Chinese questions (Rule #4, 8, 9 10, 11, 12).  However, among Chinese questions, quite a few rules also  indicate the opposite: answering questions of lower difficulty   levels correctly could imply answering those of higher difficulty  levels correctly (Rule # 1, 2, 3, 5, 6, 7). The high Support and  Confidence values of Rule #1 and #2 indicate these cases occurred  quite often. In contrast, rules of English questions seem to follow   the difficulty levels well. No rules with correct answers in lower  difficulty levels alone imply correct answers in higher difficulty  levels. This seems to indicate that the difficulty levels of English   questions are better scaled and more suitable for the student users  in consideration. For Chinese questions, Rule #1 and #3 are  symmetric (i.e., answering Level 1 questions correctly implies   answering Level 2 questions correctly, and vice versa), so are  Rule #2 and #3. These rules show that, to this group of students,  the difficulty levels of Chinese questions in Level 1 and Level 2,  and those in Level 1 and Level 3 may not be clearly  distinguishable. One possible reason is that Chinese questions of   Level 1-3 are relatively easy for this group of student users. A  closer look at the rules from student clusters further verifies this  conjecture: all three clusters contain rules like Rule # 1 and 2, as   well as [level_2_Chi_r]  [level_3_Chi_r]. Therefore, it can be  suggested to the Reading Battle system to increase the difficulty  degree of Level 2 and Level 3 Chinese questions, so that they   would be more challenging to the students.    The case is a little different for Level 4 Chinese questions. To  have the conclusion of answering Level 4 Chinese questions   correctly, the premise clauses have to include correctly answering  questions in two other levels at the same time, i.e. Level 1 and  Level 2 questions (Rule #5), Level 1 and Level 3 questions (Rule  #6), or Level 2 and Level 3 questions (Rule #7). These rules   indicate that Level 4 Chinese questions are indeed more  demanding than those in other levels. Rule #14 to #16 reveal  associations among questions across languages. It seems that  correctly answering questions in a higher difficulty level in one   language could help boost the chance of correctly answering  questions in a lower difficulty level in the other language.  However, this needs to be verified when more data are collected  and more rules can be mined.   For question categories, Rule #17 and #18 indicate if students  answered Evaluation Chinese questions correctly, they would  likely to answer all other three kinds of Chinese questions   correctly. For these students, it is recommended that they proceed  to a higher difficulty level in Chinese questions in order to keep  them challenged and continuing improving their abilities. For  English questions, Rule #24 to #26 indicate if students correctly   answered any other kind of English questions than Information  search, they would correctly answer English questions in  Information search category. These results are in accordance  with the common knowledge that Evaluation questions are of a  higher complexity than others and Information search questions  are more straightforward than others. Rule #19 to #21 reveal two  symmetric pairs: (infer_Chi_r, infn_Chi_r) and (infer_Chi_r,  inte_Chi_r). Another symmetric pair between (infn_Chi_r,   inte_Chi_r) is revealed by Rule # 22 and 23. These pairs are also  found in each of the three student clusters. They suggest that the  complexity levels of Chinese questions in these three categories  (infn, inte, and infer) are mostly comparable, to the students   considered in this study. Again, similar to difficulty levels, the  results suggest that Reading Battle probably should adjust the  complexity of Chinese questions in these three categories, so that  they can be more useful in differentiating students abilities in  information search, interpretation & synthesis and inference.      Table 7: Interesting and non-redundant rules among question difficulty levels and categories for the entire student sample  No. Premises Conclusion Support Confidence Lift Cosine   Question difficulty level  1 level_1_Chi_r level_2_Chi_r 0.66 0.87 1.20 0.89  2 level_1_Chi_r level_3_Chi_r 0.64 0.85 1.16 0.86  3 level_2_Chi_r level_1_Chi_r, level_3_Chi_r 0.59 0.82 1.27 0.87  4 level_3_Chi_r level_1_Chi_r, level_2_Chi_r 0.59 0.81 1.22 0.85   5 level_1_Chi_r, level_2_Chi_r level_4_Chi_r 0.55 0.83 1.32 0.85  6 level_1_Chi_r, level_3_Chi_r level_4_Chi_r 0.54 0.84 1.33 0.85  7 level_3_Chi_r, level_2_Chi_r level_4_Chi_r 0.53 0.83 1.31 0.83  8 level_4_Chi_r level_1_Chi_r, level_3_Chi_r, level_2_Chi_r 0.51 0.81 1.37 0.84   9 level_2_Eng_r level_1_Eng_r 0.33 0.85 1.97 0.81  10 level_3_Eng_r level_1_Eng_r 0.30 0.86 2.00 0.77  11 level_4_Eng_r level_1_Eng_r, level_2_Eng_r 0.25 0.80 2.40 0.77  12 level_4_Eng_r level_3_Eng_r 0.25 0.80 2.32 0.75  13 level_1_Eng_r, level_3_Eng_r level_2_Eng_r 0.25 0.84 2.14 0.73   14 level_3_Chi_r, level_1_Eng_r level_2_Eng_r 0.26 0.80 2.05 0.73  15 level_4_Chi_r, level_1_Eng_r level_2_Eng_r 0.24 0.81 2.05 0.70  16 level_2_Chi_r, level_3_Eng_r level_4_Chi_r, level_1_Eng_r 0.22 0.81 2.72 0.77   Question category  17 eva_Chi_r infn_Chi_r, inte_Chi_r 0.50 0.85 1.24 0.79  18 eva_Chi_r infn_Chi_r, infer_Chi_r 0.48 0.81 1.27 0.78   19 infer_Chi_r infn_Chi_r, inte_Chi_r 0.60 0.86 1.26 0.87  20 infn_Chi_r infer_Chi_r 0.64 0.84 1.20 0.87  21 inte_Chi_r infer_Chi_r 0.63 0.84 1.20 0.87  22 inte_Chi_r infn_Chi_r 0.68 0.91 1.19 0.90  23 infn_Chi_r inte_Chi_r 0.68 0.90 1.19 0.90   24 infer_Eng_r infn_Eng_r 0.31 0.88 2.02 0.79  25 inte_Eng_r infn_Eng_r 0.29 0.87 2.01 0.76  26 eva_Eng_r infn_Eng_r 0.28 0.90 2.07 0.77  27 infn_Eng_r, inte_Eng_r infer_Eng_r 0.23 0.81 2.31 0.73   Note: infn=information search; inte=interpretation & synthesis; infer=inference; eva=evaluation; Chi=Chinese; Eng=English; r=right/correct.    4. CONCLUSION AND FUTURE WORK  The study explores the associations among books and questions   in an e-quiz system, Reading Battle, which supports primary- school students in improving their reading interests and  proficiency. Clustering and association rule mining techniques  were used to analyze user records extracted from the system.   The resultant association rules can be used to improve the  question design and develop personalized recommendations to  student users. The results also help further deepen our  understanding of students usage of the Reading Battle system,  which in turn provides rationales for further improvement. As   the system continues to run, more user data will be collected in  the future. The expansion of record size for each student will not  only enlarge the scale of association rules but also help improve  the strength and confidence of the rules. Besides, participation in   the online survey of reading habits should also be encouraged in  the future, so that more students can be grouped into clusters and  cluster-based recommendations could be made available to  them. In addition, the online survey provides an opportunity to   collect relevant data for building up user profiles based on which  more rules for recommendation can be explored.   This study strives to provide empirical evidence that learning  analytics can be applied to mine valuable knowledge from   student interaction with online learning system and facilitate the  development of personalized learning, particularly for an online  reading platform for primary-school students. It contributes to  the literature on the development of personalization in online   reading platforms. As the next step, we will implement the rules  identified in this study into the recommender system in Reading  Battle and evaluate their effectiveness with real users.    REFERENCES  [1] Baker, L. and Wigfield, A. 1999. Dimensions of children's   motivation for reading and their relations to reading activity  and reading achievement. Reading Research Quarterly, 34,  4, 452-477. DOI= http://dx.doi.org/10.1598/RRQ.34.4.4   [2] Chu, S., Chan, H., Wong, J., Wu, W., and Mok, S. 2014.  Strengthening students reading comprehension ability  (both Chinese and English) through developing childrens  literature equiz bank on the cloud. The 19th International  Education and Technology Conference, Hong Kong. DOI=  http://hdl.handle.net/10722/204504   [3] Duran, J. I., Laitakari, J., Pakkala, D., and Perala, J. 2010.  A user meta-model for context-aware recommender  systems. In Proceedings of the HetRec, Barcelona, Spain.  DOI= http://dx.doi.org/10.1145/1869446.1869456   [4] Han, J., Kamber, M., and Pei, J. 2011. Data mining:  concepts and techniques. Elsevier.   [5] Hofmann, M., and Klinkenberg, R. 2013. (Eds.).  Rapidminer: Data Mining Use Cases and Business  Analytics Applications. CRC Press.   [6] Hsu, C.-K., Hwang, G.-J., and Chang, C.-K. 2010.  Development of a reading material recommendation system  based on a knowledge engineering approach. Computers &  Education, 55, 1, 76-83. DOI=  http://dx.doi.org/10.1016/j.compedu.2009.12.004   [7] Merceron, A. and Yacef, K. 2008. Interestingness measures  for association rules in educational data. EDM, 57-66.     "}
{"index":{"_id":"5"}}
{"datatype":"inproceedings","key":"Bull:2016:ILV:2883851.2883853","author":"Bull, Susan and Ginon, Blandine and Boscolo, Clelia and Johnson, Matthew","title":"Introduction of Learning Visualisations and Metacognitive Support in a Persuadable Open Learner Model","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"30--39","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883853","doi":"10.1145/2883851.2883853","acmid":"2883853","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics for learners, open learner models, persuading the learner model, visual learning analytics","abstract":"This paper describes open learner models as visualisations of learning for learners, with a particular focus on how information about their learning can be used to help them reflect on their skills, identify gaps in their skills, and plan their future learning. We offer an approach that, in addition to providing visualisations of their learning, allows learners to propose changes to their learner model. This aims to help improve the accuracy of the learner model by taking into account student viewpoints on their learning, while also promoting learner reflection on their learning as part of a discussion of the content of their learner model. This aligns well with recent calls for learning analytics for learners. Building on previous research showing that learners will use open learner models, we here investigate their initial reactions to open learner model features to identify the likelihood of uptake in contexts where an open learner model is offered on an optional basis. We focus on university students' perceptions of a range of visualisations and their stated preferences for a facility to view evidence for the learner model data and to propose changes to the values.","pdf":"Introduction of Learning Visualisations and Metacognitive  Support in a Persuadable Open Learner Model   Susan Bull  Institute of Education   University College London,  UK   s.bull@ucl.ac.uk.  Blandine Ginon  University of Birmingham,   UK  b.ginon.1@bham.ac.uk   Clelia Boscolo   University of Birmingham,   UK  c.boscolo@bham.ac.  uk  Matthew Johnson  University of Birmingham,   UK  m.d.johnson.1@bham  .ac.uk     ABSTRACT  This paper describes open learner models as visualisations of  learning for learners, with a particular focus on how information  about their learning can be used to help them reflect on their  skills, identify gaps in their skills, and plan their future learning.  We offer an approach that, in addition to providing visualisations  of their learning, allows learners to propose changes to their  learner model. This aims to help improve the accuracy of the  learner model by taking into account student viewpoints on their  learning, while also promoting learner reflection on their learning  as part of a discussion of the content of their learner model. This  aligns well with recent calls for learning analytics for learners.  Building on previous research showing that learners will use open  learner models, we here investigate their initial reactions to open  learner model features to identify the likelihood of uptake in  contexts where an open learner model is offered on an optional  basis. We focus on university students perceptions of a range of   visualisations and their stated preferences for a facility to view  evidence for the learner model data and to propose changes to the  values.     CCS Concepts   Human-centered computing Human computer interaction  (HCI); user model, user studies Visualization; Visual  analytics; Visualization systems and tools  Applied Computing   Education; Interactive learning environments.   Keywords  Visual Learning Analytics; Open Learner Models; Learning  Analytics for Learners; Persuading the Learner Model.   1. INTRODUCTION  Visual analytics combine the strengths of people and computers in  processing data [25]. With the growth of interest in learning  analytics, visualisations and dashboards have been developed for  education settings (e.g. [2; 10; 19; 43]). There is also growing  recognition that, because learning analytics are concerned with  learning, they should offer pedagogically useful information [22].   This information also needs to be actionable, i.e. it has to be able  to support decision-making [2]. For example, learning analytics  can help teachers to compare activity and performance indicators  in large datasets, to allow them to take decisions about their use of  particular activities [20]; or social learning analytics dashboards  may help teachers better identify learner-learner interactions,  which may help them to intervene in cases such as where there are  disconnected students, or help them recognise those who have  influence over others as indicated by ratings or followers [21].    Learning analytics visualisations can also be helpful for the  learners themselves (e.g. [15;18;21;44]), and it is this area that is  of particular interest in this paper; specifically, open learner  models (OLM). The essence of a learner model is a representation  of an individuals current state of knowledge, skills,   competencies, etc., which is inferred according to their interaction  with an educational system [46]. Thus, it is not so much a count of  performance or other data as is more common in learning  analytics, but it is, indeed, a representation of understanding or  learning. It is the learner model that allows an adaptive system to  appropriately personalise the interaction to suit the individual  learners needs at the time.    Open learner models are learner models that externalise the  inferred learner model contents to the learner (or other user),  usually with some kind of visualisation. Often this has the aim of  promoting metacognitive activities such as reflection, self- monitoring and planning (see [3]), and therefore has a similar goal  to some of the arguments for learning analytics visualisations to  facilitate such activities, e.g. self-directed learning [15] and  metacognition [18].   Typically, in learning analytics, traditional visualisation methods  have been used in learning dashboards. These include bar charts  [20; 23; 36; 38]; pie charts [23; 41]; histograms [27; 41]; radar  plots [21]; scatterplots [36]; tables [38]; or timelines [20; 27; 38].  Other examples include networks [21; 41] and tag clouds [41]. As  can be seen from the above, often several visualisations are  available on a learning dashboard (e.g. [38; 41]).   OLMs often use different visualisation forms than those most  commonly found in learning analytics dashboards. For example:  skill meters [6; 7; 12; 17; 28; 33; 45], concept maps [1; 17; 30; 37]  and hierarchical tree structures [11; 17; 24; 30] are particularly  common. Other visualisations as mentioned above for learning  analytics, can also be found in open learner models (e.g. tag/word  clouds [7; 31] and network visualisations [7]). The overlap  between learning analytics visualisations and OLM visualisations  is often in the newer visualisation types. As with learning  analytics dashboards, some open learner models have multiple  visualisations (e.g. [7; 17]). Figures 1 and 2 show examples from  the LEAs Box open learner model.      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than ACM  must be honored. Abstracting with credit is permitted. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee. Request permissions  from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883853         Figure 1: The LEAs Box OLM: word cloud and treemap visualisations.                    Figure 2: The LEAs Box OLM: stars, gauges, network, table, skill meters, smileys, histogram and radar plot visualisations.     In addition to allowing users to see the information in their learner  model, and in a way that is meaningful and facilitates action,  learner models can in some cases also be negotiated (e.g. [4; 16;  26]), where either the learner or the system can initiate discussion  to try to resolve any differences between their respective beliefs  about the learners knowledge, and each partner provides evidence  or justifications of their viewpoints. Separate learner model  representations (for the systems and students beliefs about the  learners skills) are retained if there is no agreement.  Alternatively, some learner models allow attempts at persuasion: a  learner can try to persuade the system to change a value in their  learner model, providing evidence to convince the system (e.g. [9;  29; 40; 42]). Only if the system is convinced by this evidence, will  it update the learner model to reflect the learners requested   change. While there are similarities between negotiated and  persuadable learner models, the main difference is that in the  latter, control of the learner model data lies with the system, while  negotiated learner models allow parallel representations of student  and system beliefs if there is disagreement.   Approaches such as the above allow learners to suggest changes  to outdated or inaccurate representations in their learner model,  and often also aim to promote learner reflection on their learning,  as part of the discussion process [5]. This is possible in OLMs  because the visualised data is not simply a count of activity or  behavior  which will be an accurate count of instances (e.g.  clicks, materials accessed, exercises attempted, performance  scores)  but rather, comprises inferences about learning. As  stated above, this interaction about the content of the learner  model aims to provide a means to improve the accuracy of the  model: e.g. if a learner had obtained correct answers through  guessing, the learner may try to decrease a value; or if the learner  had done some additional reading, they may try to increase a  value in their learner model (or, indeed, they may try to decrease  it, if on reading they realise that their understanding is not as  strong as the representations in their model suggest). Importantly,  this discussion process also aims to prompt reflection on learning  as learners consider the evidence for the learner model values, and  must provide justifications if they attempt to change any value in  their model.    In the next section we introduce the LEAs Box (Learning  Analytics Toolbox) OLM, which has multiple visualisations and  offers a persuasion mechanism. We then describe a study  investigating which features university students would expect to  use, to help identify useful features for OLM developers, and  ways to introduce OLMs in optional settings to encourage uptake.  While we already know that students will use an OLM similar to  this one [8], we know less about their intentions before they adopt  it. This information is also important.    2. THE LEAs BOX OLM  Multiple visualisations have previously been suggested to be  beneficial in OLMs, to allow users to select the visualisation  according to their reason for viewing the learner model and their  individual preferences for viewing [7; 30; 39; 42]. The LEAs Box  OLM has ten visualisations. It also has a discussion facility to  allow students to obtain evidence for their learner model data and,  if considered appropriate, to try to argue for changes to the values  in their model. In this version of the LEAs Box OLM, we take  the approach of persuasion, where the student alone can initiate  discussion, and if there is no agreed outcome, the model remains  unchanged. Future work will develop this into a full negotiation  mechanism, hence the appearance of the term negotiation in the  interface.   In line with some other technology-enhanced learning contexts  where learning data from multiple sources is held in the learner  model (e.g. [8; 14; 32; 34]), the LEAs Box OLM can take data  from a range of activities or sources. In our current example, the  activities are completed in the course Learning Management  System. While the OLM can be used in any subject, we here use  the example of language learning, and specifically, vocabulary  topics.   2.1 OLM Visualisations  Figures 1 and 2 give examples of how the learner model is  visualised to learners (based on [7]). Figure 1 shows the word  cloud visualisation for a Test Student, where the larger (blue)  text on the left indicates strong areas, with less strong topics in  smaller text; and the larger (black) text on the right shows weaker  areas, with less weak (but still low) areas in smaller text. The  treemap shows the strength of topics by the size of the  corresponding squares. These screens also show the quiz names,  again with the quizzes that contribute the highest level shown on  the left of the word cloud (in blue), and the quizzes with lower  levels of mastery on the right (in black text); and the size of the  square for a quiz in the treemap indicating that quizs influence in   the learner model. This allows students to see their levels across  all vocabulary topics, as well as the specific activities that  contributed the data. Figure 2 gives examples of the other  visualisations available: stars, gauges, network, table, skill meters,  smileys, histogram and radar plot. Skill meters, as indicated  above, are common OLM visualisations. Unlike progress bars,  commonly used in computer systems, these reflect the current  inferred learning state. In LEAs Box we have two versions  the  stars which quantise the data into five levels, and the continuous  skill meters. The gauge and radar plot also show continuous  values. The table, smileys and histogram show discrete values.  The treemap and network visualisations use size (and brightness,  in the case of the network), to indicate the strength of topics. The  treemap allows the learner to click on a cell to view the next layer  in the hierarchical structure, while clicking on a node in the  network allows the nodes to be expanded or collapsed, to view  more or less information. Thus, the different visualisations may be  more or less useful depending on the number of areas displayed,  and may also be viewed according to individual preferences.    Table 1: LEAs Box OLM visualisation categories.    Quantised Continuous Structured Unstructured   Skillmeters      Table      Smileys      Stars      Gauges      Word cloud      Histogram      Radar plot      Treemap      Network           Table 1 gives an overview of some of the features of the  visualisations. As stated above, some visualisations are quantised  on a five point scale, while others use a continuous scale. Most  visualisations also indicate the structure of the domain. Thus,  using those visualisations, it is possible to see a topic parent or  child, if applicable.   2.2 OLM Persuasion   After viewing their OLM as in Figure 1 or 2, if a student is unsure  about the accuracy of a value, or if they disagree with it, they can  opt to try to change it through discussion (see bottom of the word  cloud screen in Figure 1). The persuasion workflow is shown in  Figure 3. When discussion of a given vocabulary topic is initiated  by the student, the students current level for this topic is   displayed as a statement. The student can then either request  evidence or give a self-assessment. This is shown in Figure 4.     Figure 3: Discussion workflow.   The move request evidence (right, middle) is available to the  learner during all persuasion steps. The evidence explains how the  current level is calculated for the area being discussed (Figure 5).  It takes into account all pieces of evidence that are directly  associated with this, as well as any sub-topics. A piece of  evidence can, for example, be a score in an exam or a quiz, a  teacher assessment, an activity in another system that may or may  not model the learners knowledge, or the result of a past resolved  discussion of the topic. Each piece of evidence has a weight  (based on [7]): the more recent a piece of evidence, the higher its  weight (unless the relative weights have been changed by the  teacher). In this example there are three pieces of evidence.   The students self-assessment is followed by a statement from the  system that reminds the learner of their current level and self- assessment. The system then requests justifications before it will  increase or decrease the students level for this topic in their   learner model, to match their self-assessment (Figure 5).  Depending on the students justification, current level and self- assessment, the system uses the discussion parameters defined by  the instructor (Figure 6) to accept or decline the students self- assessment, or to propose a compromise between the students   self-assessment and the current representation of their level in  their learner model. If a self-assessment or a compromise is   accepted, the discussion ends because the system has been  persuaded, and the model is updated with a level that both the  student and the system accept. This agreed level becomes a new  piece of evidence for this topic, with negotiation noted as the   source of evidence and the student as contributor. However, if a  self-assessment or a compromise is declined, the discussion ends,  but the model is not changed.     Figure 4: System statement and student self-assessment.   In the example in Figure 6, the teacher has defined all  justifications to have equal weight (5), with justifications for  upward changes having a positive value, and justifications for  downward changes having a negative value. In principle, the same  phrase could be used both positively and negatively. For example,  a learner could consider themselves to be stronger or weaker than  reflected in their learner model, for a particular area. They might  therefore give the reason I have done an exercise as justification  for a change in either direction, if this option has been defined by  the instructor for both cases. This is easily done, since  justifications with positive associated values only show (as in  Figure 5) for selection when a learner tries to increase a value in  their learner model, and justifications with negative associated  values are only available if a learner tries to decrease a value.   The maximum threshold defined by the instructor (Figure 6) is the  maximum increase allowed to a learner model value during  persuasion, without the system requiring further information; and  the minimum threshold is the maximum decrease allowed without  requiring justification. Thus, in this example, a self-assessment of  +10 or -10 will be accepted; a request to change a value by more  than this will not be automatically accepted. The minimum  number of pieces of evidence between negotiations can be defined  to ensure that students cannot simply change a learner model  value multiple times for a topic, without additional evidence being  collected from another source. In this example, two further pieces  of evidence for a topic are required before a new discussion of the  value can take place. The minimum time between negotiations has  a similar purpose. It can be set to no minimum; 15 minutes (as  here); 30 minutes; 1 hour; 1 day; or 1 week (future work will  allow instructors to define other values). This is another method to  ensure that a student cannot easily change values without  appropriate consideration.  When a student proposes a new value, the system may offer a  compromise if the proposed value is beyond the given threshold.  They will be required to provide one or more justifications, and  the values defined for these (here +5 or -5) will be used to  calculate the compromise value. This value must fall within the  instructors threshold (here 10, which can include two   justifications as all combinations of two justifications will total  10); but will be as close to the students proposed value as is  permitted. This implementation of persuasion differs from  previous work in that usually a student can demonstrate their  skills by answering questions selected to verify their claims (e.g.  [9; 29; 40; 42]). Because LEAs Box has multiple data sources  and is domain- independent, all justifications are currently in the  form of statements predefined by the teacher.       Figure 5: Justifying a change to the learner model.     In future work students may also be directed back to specific  activities in specific data sources, allowing evidence of their skills  to be demonstrated, as in other approaches to persuading and  negotiating the learner model. The two approaches will then be  used together.  Table 2 describes the possible discussion moves for student and  system, with examples. (Except for self-assess, future work on  negotiation will implement the currently missing cells.)       Figure 6: Defining the justification options.     Table 2: Negotiation moves and examples.    Student System   Initiate discussion Select a topic to discuss.    Accept/agree  Accept a compromise.  Agree with the  evidence provided.   Accept a compromise.  Agree with the students   justifications.   Decline Decline a compromise proposed by the system.   Your last negotiation for   this is too recent to allow a  new negotiation.   Compromise   Propose a compromise  between the current value  and the students self- assessment.   Request evidence  Request    justifications   Request evidence for  the current value.   Request justifications for a  self-assessment.   Provide evidence  Provide   justifications   I have done some  homework.  I have had a class.  I have done some   reading.   Your level in vocabulary   is 72 and this is a sub-topic  of Italian.   Self-assess I think my level should be 80.   Challenge  evidence     Statement   Your level for vocabulary  is 75 and you think it  should be 80.         3. STUDENT PERCEPTIONS   The LEAs Box OLM was introduced in a final year module on  Italian language, in the Department of Modern Languages at the  University of Birmingham, UK. The Italian course uses a suite of  over 340 formative assessment quizzes in the institutions chosen  Learning Management System. There are no summative marks  associated with these activities. The quizzes cover a variety of  skills: grammar, vocabulary, reading, listening, etc. In this study  we used the area of vocabulary. The vocabulary quizzes are  mapped onto the Common European Framework of Reference for  Languages (CEFR) [13]. For example, for vocabulary, 26 topics  are covered in 14 quizzes. The vocabulary topics and quizzes are  linked to three of the CEFR levels (B1, B2, C1) on the basis of  range and complexity of topic, as appropriate for the participants  (Figure 7).    Vocabulary range        Vocabulary for everyday life (B1)       Varied vocabulary for specialism and general topics (B2)       Broad lexical repertoire, idioms and colloquialisms (C1)  Vocabulary control        Control of elementary vocabulary (B1)       Ability to communicate well (B2)       Extensive vocabulary control (C1)  Example vocabulary topics and levels       B1: people, the family, past people and things       B2: traditions and celebrations, cities, out-of-town living       C1: politics and political parties, Italian economy, social issues   Figure 7: CEFR levels and examples for vocabulary.  The LEAs Box OLM was introduced to help overcome the   problem of use of the quizzes not peaking until the exam  preparation period, arguably not the best time to practise and learn  vocabulary. The LEAs Box OLM offers an overarching   context/environment for students learning of Italian vocabulary,  and a more meaningful way of working with the quizzes, since it  allows students to visualise their progress throughout their study  and to contextualise this progress in the various aspects of their  lexical competence. The opportunity to discuss their levels is seen  as an excellent way for students to engage with their learning  process, take responsibility for it and also be empowered by it.   In this study we investigate the students intentions towards using  the LEAs Box OLM, as initial perceptions are likely to influence   whether students who are completing activities on a formative  basis only, proceed to use an environment such as the LEAs Box  OLM.   3.1 Participants, Materials and Methods  Participants were 25 volunteers in their final year of an  undergraduate degree in Modern Languages, who were taking a  course in Italian language (see above). They received a demo of  the LEAs Box OLM using a Test Student account, and an  explanation of how the OLM could be used to further explore quiz  outcomes with reference to CEFR. They were advised that their  expected levels for the current stage of the course were already  entered into their OLM, but that their subsequent quiz results may  move their levels for the various topic areas and corresponding  competences.   Questionnaires were administered to identify participants  anticipated use of OLM features, with items requiring responses  on a five point scale: strongly agree (5), agree (4), neutral (3),  disagree (2), strongly disagree (1). Strongly agree and agree  are combined in the analysis, as these indicate positive responses.  However, the Figures show strongly agree and agree   separately, to provide further detail. There were 10 questionnaire  items addressing whether each of the visualisations were likely to  be used; 5 relating to how participants expected to use the  visualisations; and 6 items referring to their expectations about  their use of the persuasion mechanism (referred to in the  questionnaires as negotiation to be consistent with the terms in  the interface).    3.2 Results  Figure 8 shows participants stated intentions for using each of the  visualisations. The skill meters are anticipated to be the most  likely used, followed by the table and stars. The radar plot and  treemap are expected to be least used. Most students indicated that  they intend to use several visualisations: mean 3.84 visualisations;  median 4; range 0-10. Table 3 shows the breakdown for expected  use of structured/unstructured and quantised/continuous  visualisations, a split between students opting for structured only  and both, with none expecting to use only unstructured  visualisations; and 22 of the 25 participants expecting to use both  quantised and continuous visualisations, 1 anticipating using only  continuous, and 1, only quantised.   Figure 9 shows participants stated expected purposes for  accessing the OLM visualisations, and Table 4 shows the mean,  median and range values. All purposes (comparing topics,  planning, reflection, identifying relative strengths and gaps) are  expected to be highly relevant. Table 5 provides further  breakdown: 23 of the 25 students gave positive responses for all  four purposes of viewing their learner model; 1 gave positive  responses for 3 purposes (omitting the reflection option); and 1  indicated that they would use the OLM for only one of the given  purposes (planning).  Figure 10 shows participants expectations regarding their use of   the discussion component of the LEAs Box OLM. 23 of the 25  students claim that they would want to view the evidence for  values when they disagree with them; and 24, when they agree  with the values. 19 expect to discuss values when they disagree  with them; and 14, when they agree. 16 stated that they wish to  explain their viewpoint (justify their self-assessments) when they  disagree with values; and 13, when they agree. Table 6 gives the  mean, median and range values. While some values are lower, the  medians show that most participants expect to engage in  discussion with the system, regardless of whether they agree or  disagree with the values shown in the OLM.      Figure 8: Students stated choice of visualisations.  Table 3: Expected use of visualisation types.   Structured Unstructured Structured and Unstructured  13 0 11   Quantised Continuous Quantised and Continuous  1 1 22             Figure 9: Students stated purposes for viewing the OLM.      Table 6 shows a high expectation for viewing evidence, but while  still positive, less high for discussing values and explaining ones   point of view. Figure 11 considers the latter two, with reference to  the number of visualisations used. The 3 participants who expect  to use 7 or more visualisations also intend to engage more in  discussion. A proportionally higher number of students who  expect to use only two visualisations, also expect to engage less in  attempting to change values or explain their viewpoint.   Table 4: Students stated purposes for viewing the OLM.  Purposes Mean Median Range  Compare levels in topics 4.4 4 3-5  Plan 4.68 5 4-5  Think about competencies 4.52 5 2-5  Identify strengths / difficulties 4.6 5 3-5     Table 5: Number of purposes for viewing the OLM.   Four   purposes  Three   purposes  Two   purposes  One   purpose  Students 23 1 0 1     Figure 10: Students expectations for using discussion feature.        x  number of visualisations; y  number of students   Figure 11: Discussing, explaining (upper); not discussing, not  explaining (lower).      Table 6: Students expectations for using discussion feature.   Discussion Components Mean Median Range  Discuss value (agree) 3.44 4 1-5  Discuss value (disagree) 3.88 4 2-5  Explain viewpoint (agree) 3.6 4 2-5  Explain viewpoint (disagree) 3.84 4 2-5  View evidence (agree) 4.4 5 2-5  View evidence (disagree) 4.48 5 2-5     3.3 Discussion  As shown in Figure 8, participants anticipated that they would use  the simpler skill meters, table and stars, rather than what may be  considered more complex visualisations such as radar plot, word  cloud and treemap. Most expected to use multiple visualisations.  While we have already seen that most of these visualisations will  be used by university students in practice [8], identifying that  students would consider some visualisations beneficial before they  use an OLM on a voluntary basis leads us to be more confident  that they might take the first steps also in contexts in which there  is no support for their use of the technology, and in a subject that  is not related to technology (in contrast, for example, to [8]). Even  if their predictions about their use of the technology do not match  how they subsequently use it (see [35]), their expectations of the  utility of certain visualisations might motivate them to initially  engage with the OLM.   As previously stated, skill meters are amongst the most common  learner model visualisation, and it appears that this and similar  displays are also considered likely to be beneficial by a large  proportion of students before thy use an OLM. Therefore, while  there might be a strong case for using complex visualisations: for  example, where extensive relationships between different parts of  the domain need to be understood (as is perhaps easier with the  network view), or expertise across areas of a curriculum should be  easily identifiable (as facilitated by the radar plot), inclusion of a  simpler visualisation might help engage students initially. It could  be argued that our participants had limited experience of the  visualisations available, and that for this reason they preferred the  simpler visualisations. However, this is the same situation as they  faced before first using the OLM on an optional basis. Therefore,  our first recommendation to OLM developers is:  1. As well as any visualisations that are particularly relevant to  the specific context, include simpler visualisations such as skill  meters or similar displays to help students identify a visualisation  they can envisage using.  We also found that most students expected to use multiple  visualisations. This has also been observed in practice [6; 8; 30;  42], and because students anticipated this also before using the  OLM, we recommend:  2. Offer multiple learning visualisations in an OLM to allow  students to identify a range of options that they consider suitable.  In cases where specific visualisations are likely to be most  appropriate, their use can still be encouraged when students access  the OLM.   Table 3 showed that there were no participants who expected to  use only unstructured visualisations, with quite an equal split  between expected use of structured visualisations only, and both     structured and unstructured views. We do not know from this data  whether the fact that some visualisations were structured,  contributed to the preferences suggested for them. However, until  this has been investigated further, to facilitate awareness of  competency, topic or other structures, we suggest to:  3. Offer at least one structured visualisation in an OLM.   We also found that 23 of the 25 participants expected to use both  quantised and continuous visualisations. However, since this  distinction may be difficult to determine without actual use of the  OLM, and because half the visualisations are quantised, and half  continuous, we do not draw any conclusions at this stage.  However, we suggest this as a feature for further investigation.  Figure 9, Table 4 and Table 5 show participants stated intentions  regarding how they expect to use the visualisations. Nearly all  (24) want to use them to help identify their strengths and  difficulties; 23, to support reflection (here defined as think about  competencies); all 25 stated that they expected to use the  visualisations to plan what to work on next; and 24 stated that  they wish to compare their levels across the topics. This indicates  that the participants view the OLM as a potentially useful support  for metacognitive activities, which have long been amongst the  main purposes of OLMs (see [3]), and now increasingly noted as  important in learning analytics visualisations [15; 18]. We do not  know whether our participants recognised these benefits because  we asked about them specifically. We would hope that they would  identify this in any case, but to be sure we give the perhaps rather  obvious reminder:  4. Explain how an OLM can support metacognition and self- regulated learning to ensure that learners are aware of this  purpose.  In line with other research suggesting the utility of providing  evidence for learner model values (eg [24]), Figure 10 and Table 6  indicate that participants are keen to see the evidence for the data  in their OLM, regardless of whether they agree or disagree with  the values. This may be related to their keenness to reflect on their  learning, as discussed above  they may view the evidence as a  support for their self-directed learning. We therefore recommend  to:  5. Offer evidence for learner model values, as a means to  facilitate self-monitoring, reflection, planning, etc.  Most participants also stated that they wanted to explain their  point of view to the system: around two thirds of the participants  when they disagree with a value, and about half also if they agree  with it. This further supports our suggestion that students may  view the OLM as a useful tool to support metacognition. Based on  this, we suggest to:  6. Offer provision for learners to justify their own viewpoints on  their understanding, skills, etc., as a means to further prompt  metacognitive processes, even if an open learner model does not  have a persuade or negotiate facility.   This is further supported by participants interest in discussing the  learner model values: 19 stated that they would wish to discuss a  value if they disagreed with it, while 14 would like to do so also if  they agreed with it. It appears that some perceive they would  benefit from such discussion regardless of whether they aim to  change a value. We therefore propose:  7. Allow students to discuss the contents of their learner model  with the system if this is feasible in the context of use (i.e. if the  learner modelling is sufficient to be able to support this).   Given that we have presented results on both visualisations and  learner model discussion options, we also compared the number  of visualisations expected to be used, with whether participants  anticipated discussing and explaining their learner model. (We  omitted the options relating to viewing evidence, as these applied  to all participants.) The 3 students who anticipated using a very  high number of visualisations also intended to engage in the  metacognitive purposes for viewing the learner model. However,  this may simply reflect the fact that they wish to make full use of  the support available. Conversely, students who did not expect to  engage in any of the metacognitive activities associated with the  OLM, were proportionally more likely to use only two  visualisations. However, the numbers are quite low. We therefore  do not conclude anything specific in relation to this, but rather,  await additional results.   Our original purpose was to investigate the extent to which  students anticipate using different features of an OLM in a  formative assessment setting, where there are no scheduled  sessions for use of the OLM. The aim was to be able to  recommend what to consider when developing and first deploying  an OLM amongst a group such as this. One aspect of interest is  the persuade feature, that we aim to develop into full negotiation  (i.e. where student and system will have equal power, and the  same moves will be available to each). We found that most  students would want to discuss their OLM values, explain their  viewpoints and, especially, view evidence for the learner model  values. This often appears to be the case even if students agree  with the representations in their model. We therefore broaden our  initial goal, and suggest that features that aim primarily to support  persuasion or negotiation, may also facilitate metacognition more  generally. This is nicely in line with the aim of OLMs to facilitate  metacognitive activities [3], and with the call for learning  analytics visualisations to be pedagogically useful [22], actionable  [2], and to support metacognition [18] and self-directed learning  [15]. It also offers further support for long-standing  recommendations to provide evidence for learner model values  also in OLMs that are not persuadable or negotiable [24].  A limitation of this study is that participants had not yet used the  OLM. However, it was also precisely this stage that we wished to  investigate. Even though they had not yet had the opportunity to  try out the OLM themselves, they appeared to be keen to engage  with it. It is important that students perceive benefit if they are to  take up a new tool that is not part of their summative assessment,  and for which there is little or no time available to support its use.  We have therefore made a few recommendations or suggestions  for what to include in an OLM or OLM introduction, that might  help students recognise the likely benefits, and lead them to  engage and then experience these, in contexts where there are no  structured sessions and use of the OLM is optional. While we here  focused in particular on OLMs, some of the points may also be  applicable in other types of learning analytics visualisations  designed for students. Our findings should also be followed up  with a larger group of participants.   4. SUMMARY AND CONCLUSIONS  This paper has presented the LEAs Box OLM, which offers 10  visualisations, and the possibility for students to try to persuade  the system to change any learner model values that they disagree  with. Our aim was to step back from the use data as has been  reported previously, to consider learner perceptions at the time of  introduction of an OLM. We found that, in a university setting  with no scheduled sessions or support for using the OLM, and  where there was no summative assessment associated with the     activities that would provide the data, students were nevertheless  keen to use it to support their learning. Most stated that they  intended to use a combination of visualisations, and also that they  expected to engage in the metacognitive aspects of the OLM. In  particular, students reported that they wanted to be able to view  the evidence for OLM values, regardless of whether they would  want to try to change those values. We therefore extended our  scope to suggest not only that the processes of persuading a  system to change a value in an OLM might be beneficial to  learning, but also that inclusion of such features in OLMs that do  not allow students to try to change values, may also be able to  facilitate metacognitive activities.  Based on this initial study, we have made several  recommendations or suggestions regarding introduction of, and  features of an OLM, for those deploying OLMs and for OLM  developers. Future work will compare these initial perceptions to  actual usage based on questionnaires and log data, to determine  whether use remains in line with the initial expectations. If there is  a difference it will be important for future research to consider  both initial perceptions and subsequent use, to ensure that students  in contexts such as ours will anticipate sufficient benefit to  initially engage with the OLM. Subsequent use will then help  identify which features are most used and most useful as a  learning support. Taken together, this information will help us to  design and introduce OLMs that will be used and be useful in  practice.   ACKNOWLEDGEMENT  This project is supported by the European Commission under the  Information Society Technology priority FP7 for R&D, contract  619762 LEAs Box. This document does not represent the opinion   of the European Commission and the European Commission is not  responsible for any use that might be made of its contents.   REFERENCES  [1] Ahmad, N., & Bull, S. (2009). Learner trust in learner model   externalisations. In V. Dimitrova, R. Mizoguchi, B. du  Boulay, A. Graesser (Ed.), Artificial intelligence in  education, Amsterdam: IOS Press, 617-619.   [2] Brown, M. (2012).  Learning analytics: Moving from concept  to practice, EDUCAUSE Learning Initiative, 1-5.   [3] Bull, S. & Kay, J. (2013). Open Learner Models as Drivers  for Metacognitive Processes, in R. Azevedo & V. Aleven  (eds), International Handbook of Metacognition and  Learning Technologies, Springer, New York, 349-365. DOI:  10.1007/978-1-4419-5546-3_23   [4] Bull, S. & Pain, H. (1995). 'Did I Say What I Think I Said,  And Do You Agree With Me': Inspecting and Questioning  the Student Model, in J. Greer (ed), Proceedings of World  Conference on Artificial Intelligence and Education, AACE,  Charlottesvville VA, 501-508.   [5] Bull, S. (in press). Negotiated Learner Modelling to Maintain  Todays Learner Models, Research and Practice in   Technology Enhanced Learning.  [6] Bull, S., Jackson, T. J., & Lancaster, M. J. (2010). Students'   interest in their misconceptions in first-year electrical circuits  and mathematics courses. International Journal of Electrical  Engineering Education, 47(3), 307-318. DOI:  10.7227/ijeee.47.3.6   [7] Bull, S., Johnson, M. D., Masci, D., & & Biel, C. (in press).  Integrating and visualising diagnostic information for the   benefit of learning. In P. Reimann, S. Bull, M. Kickmeier- Rust, R.K. Vatrapu & B. Wasson (Ed.), Measuring and  visualizing learning in the information-rich classroom,  Routledge/Taylor and Francis.   [8] Bull, S., Johnson, M.D., Alotaibi, M., Byrne, W. & Cierniak,  G. (2013). Visualising Multiple Data Sources in an  Independent Open Learner Model, in H.C. Lane, K. Yacef, J.  Mostow & P. Pavlik (eds), Artificial Intelligence in  Education, Springer-Verlag, Berlin Heidelberg, 199-208.  DOI: 10.1007/978-3-642-39112-5_21   [9] Bull, S., Mabbott, A. & Abu-Issa, A. (2007). UMPTEEN:  Named and Anonymous Learner Model Access for  Instructors and Peers, Int. Journal of Artificial Intelligence in  Education 17(3), 227-253.   [10] Charleer, S., Klerkx, J. & Duval, E. (2014). Learning  dashboards. Journal of Learning Analytics, 1(3), 199-202.   [11] Conejo, R., Trella, M., Cruces, I., & Garcia, R. (2011).  INGRID: A web service tool for hierarchical open learner  model visualization. UMAP 2011 Adjunct Poster  Proceedings, 406-409. DOI:10.1007/978-3-642-28509-7_38   [12] Corbett, A. T., & Bhatnagar, A. (1997). Student modeling in  the ACT programming tutor: adjusting a procedural learning  model with declarative knowledge. User Modeling, 243-254.  DOI: 10.1007/978-3-7091-2670-7_25   [13] Council of Europe (nd). The Common European Framework  of Reference for Languages: Learning, Teaching,  Assessment, http://www.coe.int/t/dg4/linguistic/Source/  Framework_EN.pdf. Accessed 16 October 2015.   [14] Cruces, I., Trella, M., Conejo, R. & Galvez J. (2010). Student  Modeling Services for Hybrid Web Applications,  International Workshop on Architectures and Building  Blocks of Web-Based User-Adaptive Systems, http://ceur- ws.org/Vol-609/paper1.pdf.   [15] Dawson, S., Macfadyen, L., Risko, E. F., Foulsham, T., &  Kingstone, A. (2012). Using technology to encourage self- directed learning: The collaborative lecture annotation  system. Proceedings of Ascilite, Wellington, New Zealand.    [16] Dimitrova, V. (2003). StyLE-OLM: Interactive Open Learner  Modelling. International Journal of Artificial Intelligence in  Education 13(1), 35-78.   [17] Duan, D., Mitrovic, A., & Churcher, N. (2010). Evaluating  the effectiveness of multiple open student models in EER- tutor. In S. L. e. a. Wong (Ed.), International conference on  computers in education, Asia-Pacific Society for Computers  in Education, 86-88.   [18] Durall, E., & Gros, B. (2014). Learning analytics as a  metacognitive tool. Proceedings of 6th International  Conference on Computer Supported Education (CSEDU),  380-384.    [19] Duval, E. (2011). Attention please!: Learning analytics for  visualization and recommendation. Proceedings of the 1st  international conference on learning analytics and  knowledge, ACM, NY, USA, 9-17. DOI:  10.1145/2090116.2090118   [20] Dyckhoff, A. L., Zielke, D., Bltmann, M., Chatti, M. A., &  Schroeder, U. (2012). Design and implementation of a  learning analytics toolkit for teachers. Educational  Technology & Society, 15(3), 58-76.     [21] Ferguson, R., & Buckingham Shum, S. (2012). Social  learning analytics: Five approaches. Proceedings of the 2nd  International Conference on Learning Analytics and  Knowledge, 23-33. DOI: 10.1145/2330601.2330616   [22] Gaevi, D., Dawson, S., & Siemens, G. (2015). Lets not  forget: Learning analytics are about learning. Techtrends,  59(1), 64-71. DOI: 10.1007/s11528-014-0822-x   [23] Jacovina, M. E., Snow, E. L., Allen, L. K., Roscoe, R. D.,  Weston, J. L., Dai, J., et al. (2015). How to visualize success:  Presenting complex data in a writing strategy tutor. In C.  Romero, & M. Pechenizkiy (eds.), Proceedings of 8th  international conference on educational data mining.   [24] Kay, J. (1997). Learner know thyself: Student models to give  learner control and responsibility. In Z. Halin, T. Ottomann  & Z. Razak (Eds.), Proceedings of international conference  on computers in education AACE, 17-24.   [25] Keim, D. A., Kohlhammer, J., Mansmann, F., May, T., &  Wanner, F. (2010). Introduction. In D. A. Keim, J.  Kohlhammer, G. Ellis & F. Mansmann (Eds.), Mastering the  information age: Solving problems with visual analytics,  Goslar, Germany: Eurographics Association, 1-6.   [26] Kerly, A. & Bull, S. (2008). Children's Interactions with  Inspectable and Negotiated Learner Models, in B.P. Woolf,  E. Aimeur, R. Nkambou & S. Lajoie (eds), Intelligent  Tutoring Systems: 9th International Conference, Springer- Verlag, Berlin Heidelberg, 132-141. DOI: 10.1007/978-3- 540-69132-7_18   [27] Leony, D., Pardo, A., de la Fuente Valentn, Luis, de Castro,  D. S., & Kloos, C. D. (2012). GLASS: A learning analytics  visualization tool. Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge, pp. 162- 163. DOI: 10.1145/2330601.2330642   [28] Long, Y., & Aleven, V. (2013). Supporting students self- regulated learning with an open learner model in a linear  equation tutor. Artificial Intelligence in Education, pp. 219- 228. DOI: 10.1007/978-3-642-39112-5_23   [29] Mabbott, A. & Bull, S. (2006). Student Preferences for  Editing, Persuading and Negotiating the Open Learner  Model, in M. Ikeda, K. Ashley & TW. Chan (eds), Intelligent  Tutoring Systems, Springer-Verlag, Berlin Heidelberg, 481- 490. DOI: 10.1007/11774303_48    [30] Mabbott, A. and Bull, S. (2004). Alternative Views on  Knowledge: Presentation of Open Learner Models, in J.C.  Lester, R.M. Vicari & F. Paraguacu (eds), 7th International  Conference of Intelligent Tutoring Systems, Springer-Verlag,  Berlin Heidelberg, 689-698. DOI: 10.1007/978-3-540-30139- 4_65    [31] Mathews, M., Mitrovic, A., Lin, B., Holland, J., & Churcher,  N. (2012). Do your eyes give it away Using eye tracking  data to understand students attitudes towards open student   model representations. Intelligent Tutoring Systems, 422- 427. DOI: 10.1007/978-3-642-30950-2_54    [32] Mazzola, L., Mazza, R. (2010). GVIS: A Facility for  Adaptively Mashing Up and Presenting Open Learner  Models, in M. Wolpers, P.A. Kirschner, M. Scheffel, S.  Lindstaedt & V. Dimitrova (eds), EC-TEL 2010, Springer- Verlag, Berlin Heidelberg, 554-559. DOI: 10.1007/978-3- 642-16020-2_53    [33] Mitrovic, A. & Martin, B. (2007). Evaluating the Effect of  Open Student Models on Self-Assessment. Int. Journal of  Artificial Intelligence in Education 17(2), 121-144.   [34] Morales, R., Van Labeke, N., Brna, P. & Chan, M.E. (2009).  Open Learner Modelling as the Keystone of the Next  generation of Adaptive Learning Environments. In C.  Mourlas & P. Germanakos (eds), Intelligent User Interfaces,  Information Science Reference, ICI Global, London, 288- 312. DOI: 10.4018/978-1-60566-032-5.ch014    [35] Nielsen, J. (2001). First Rule of Usability Dont Listen to  Users, Nielsen Norman Group, nngroup.com/articles/first- rule-of-usability-dont-listen-to-users (accessed 4/2/2016).    [36] Park, Y., & Jo, I. (2015). Development of the learning  analytics dashboard to support students' learning  performance. Journal of Universal Computer Science, 21(1),  110-133.   [37] Prez-Marn, D., Alfonseca, E., Rodrguez, P., & Pascual- Nieto, I. (2007). A study on the possibility of automatically  estimating the confidence value of students knowledge in  generated conceptual models. Journal of Computers, 2(5),  17-26. DOI: 10.4304/jcp.2.5.17-26    [38] Santos, J. L., Govaerts, S., Verbert, K., & Duval, E. (2012).  Goal-oriented visualizations of activity tracking: A case  study with engineering students. Proceedings of the 2nd  International Conference on Learning Analytics and  Knowledge, 143-152. DOI: 10.1145/2330601.2330639    [39] Sek, Y. W., Deng, H., & McKay, E. (2014). Investigating  learner preferences in an open learner model (OLM)  program: A Malaysian case study. ACIS.   [40] Tchetagni, J., Nkambou, R. & Bourdeau, J. (2007). Explicit  Reflection in Prolog Tutor, Int. Journal of Artificial  Intelligence in Education 17(2), 169-215.   [41] Tervakari, A., Silius, K., Koro, J., Paukkeri, J., & Pirttila, O.  (2014). Usefulness of information visualizations based on  educational data. IEEE Global Engineering Education  Conference (EDUCON), 142-151. DOI:  10.1109/educon.2014.6826081    [42] Thomson, D. & Mitrovic, A. (2010). Preliminary Evaluation  of a Negotiable Student Model in a Constraint-Based ITS,  Research and Practice in Technology Enhanced Learning  5(1), 19-33. DOI: 10.1142/s1793206810000797    [43] Verbert, K., Govaerts, S., Duval, E., Santos, J.L., Van  Assche, F., Parra, G. & Klerkx, J. (2014). Learning  Dashboards: an Overview and Future Research  Opportunities, Personal and Ubiquitous Computing 18, 1499- 1514. DOI: 10.1007/s00779-013-0751-2    [44] Vozniuk, A., Govaerts, S., & Gillet, D. (2013). Towards  portable learning analytics dashboards. 13th International  Conference on Advanced Learning Technologies (ICALT),  IEEE, 412-416. DOI: 10.1109/icalt.2013.126    [45] Weber, G., & Brusilovsky, P. (2001). ELM-ART: An  adaptive versatile system for web-based instruction.  International Journal of Artificial Intelligence in Education  12, 351-384.   [46] Woolf, B.P. (2010). Chapter 13: Student Modeling, in R.  Nkambou, J. Bourdeau & R. Mizoguchi (eds), Advances in  Intelligent Tutoring Systems, Springer-Verlag, Berlin  Heidelberg, 267-279.     "}
{"index":{"_id":"6"}}
{"datatype":"inproceedings","key":"Pelanek:2016:IDC:2883851.2883868","author":"Pel'anek, Radek and Rih'ak, Jir'i and Papouvsek, Jan","title":"Impact of Data Collection on Interpretation and Evaluation of Student Models","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"40--47","numpages":"8","url":"http://doi.acm.org/10.1145/2883851.2883868","doi":"10.1145/2883851.2883868","acmid":"2883868","publisher":"ACM","address":"New York, NY, USA","keywords":"attition, bias, data sets, evaluation, parameter fitting, student modeling","abstract":"Student modeling techniques are evaluated mostly using historical data. Researchers typically do not pay attention to details of the origin of the used data sets. However, the way data are collected can have important impact on evaluation and interpretation of student models. We discuss in detail two ways how data collection in educational systems can influence results: mastery attrition bias and adaptive choice of items. We systematically discuss previous work related to these biases and illustrate the main points using both simulated and real data. We summarize specific consequences for practice -- not just for doing evaluation of student models, but also for data collection and publication of data sets.","pdf":"Impact of Data Collection on Interpretation and Evaluation  of Student Models  Radek Pelnek  Faculty of Informatics  Masaryk University  Brno, Czech Republic  pelanek@mail.muni.cz  Ji    r    Rihk  Faculty of Informatics  Masaryk University  Brno, Czech Republic  thran@mail.muni.cz  Jan Papouek  Faculty of Informatics  Masaryk University  Brno, Czech Republic  jan.papousek@mail.muni.cz  ABSTRACT Student modeling techniques are evaluated mostly using his- torical data. Researchers typically do not pay attention to details of the origin of the used data sets. However, the way data are collected can have important impact on evaluation and interpretation of student models. We discuss in detail two ways how data collection in educational systems can in- fluence results: mastery attrition bias and adaptive choice of items. We systematically discuss previous work related to these biases and illustrate the main points using both simu- lated and real data. We summarize specific consequences for practice  not just for doing evaluation of student models, but also for data collection and publication of data sets.  CCS Concepts Human-centered computing!User models; Applied computing ! Interactive learning environments;  Keywords attition, bias, data sets, evaluation, parameter fitting, stu- dent modeling  1. INTRODUCTION The way we collect data can have significant influence on  results that we obtain by analysis of the collected data. A typical example is selection bias  if data are not represen- tative of the studied phenomenon, results are not general- izable. In learning analytics research a typical example is self-selection in massive open online courses or voluntary questionnaires; techniques for reduction of such bias have been already studied [4]. The impact of data collection be- comes particularly challenging issue when the data collection is done by an adaptive system. Student modeling techniques are developed with the aim of being applied in adaptive sys- tems and are typically evaluated on data from such systems.  In student modeling research, however, the potential im- pact of data collection on results is typically not taken into  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883868  account. That is unfortunate because uncritical use of his- torical data sets is prone to biases and misleading results. For example, intelligent tutoring systems often used mastery learning approach, which leads to attrition bias in logged data. System behaviour (e.g., choice of items or mastery detection) is typically done by a student model. The same model may be used for data collection and during model evaluation, which may bias the evaluation  it can happen that the used model does not collect data that would show its deficiencies. The presence of this feedback loop is an im- portant dierence compared to other forecasting domains. For example in weather forecasting models do not directly influence the system and cannot distort collected data. In student modeling they can.  Potential biases caused by data collection have been dis- cussed in research in related domains. Research on eval- uation of recommender systems [13, 14, 37] discussed po- tential biases in data collection, e.g., by filtering users, and train/test set issues when using oine data. Proposal for layered evaluation of adaptive systems [32] includes evalua- tion of data collection, but does not address specifically its impact on subsequent steps. The presence of a feedback loop between data collection and model evaluation has been pre- viously discussed in the context of exploration vs exploita- tion problem (multiarmed bandits), with applications for news [20, 43] and advertisement [3, 19] selection.  In the context of student modeling, issues related to data collection have not been systematically studied before, al- though specific aspects have already been addressed by pre- vious work, particularly in the context of learning curves [24] and mastery learning. When a tutoring system uses mastery learning, students with high skill drop out earlier from the system (and thus from the collected data), thus a straight- forward interpretation of aggregated learning curves may be misleading [11, 16, 25, 27]. Previous work showed specific illustrations of confounded learning curves [27], discussed methods for disagreggation of learning curves [25], and pro- posed mastery-aligned models [16] to take this bias into ac- count. Confounding eect of item ordering on learning and item diculty has been mentioned in several works [12, 15, 17, 36], but only as a side note.  In this work we provide a systematic overview of potential biases caused by data collection. We provide discussion of previous works that mention specific biases over real data from tutoring systems and present some new illustrations on our data. We also present specific artificial scenarios, which are highly simplified (compared to real systems), but clearly demonstrate the core principles of discussed biases.    Our summary shows that the choice of data used for ex- periments can make important dierence on fitted param- eter values and results of evaluation. This has important consequences for research practice, since currently this issue is neglected and neither research papers nor descriptions of data set discuss in details the way in which the used data were collected. To contribute to the improvement of state of the art we conclude our overview with specific consequences for research practice.  2. BACKGROUND Our aim is to illustrate the impact of data collection in  many dierent contexts, and thus discussion of potential bi- ases refers to many dierent student models and experimen- tal settings. In this section we provide brief overview of used notions and pointers to more detailed explanations of used modeling techniques.  2.1 Bayesian Knowledge Tracing Bayesian Knowledge Tracing (BKT) [7, 40] is a model of  learning which assumes a sudden change in knowledge. It is a hidden Markov model where skill is a binary latent vari- able (either learned or unlearned). The basic version of the model has 4 parameters: Pinit is the probability that the skill is initially learned, Plearn is the probability of learning a skill in one step, Pslip is the probability of incorrect an- swer when the skill is learned, and Pguess is the probability of correct answer when the skill is unlearned. The estimated skill is updated using a Bayes rule based on the observed an- swers; the prediction of student response is then done based on the estimated skill. Estimation of model parameters (the tuple Pinit , Plearn , Pslip , Pguess) can be done using several dif- ferent techniques (the expectation-maximization algorithm, stochastic gradient descent, exhaustive search). For experi- ments in this work we use Yudelsons implementation [44]. The model has many extensions, but for our purposes (illus- tration of biases) the basic version is sucient.  2.2 The Rasch Model and the Elo Rating Sys- tem  The Rasch model is used typically in item response the- ory [9]. It assumes a constant student skill (no learning) and items with varying diculty. Probability of correct answer for a student with skill  and item with diculty d is given by (  d), where  is a logistic function (x) = 1  1+ex . The Elo rating system [10] has been originally proposed  for rating chess players, but recently it has been used also for student modeling [34]. It is closely related to the Rasch model since it also uses the same equation for predicting the probability of correct answers. The main dierence is in the approach to parameter estimation. The estimation of parameters of the Rasch model is typically done using some iterative maximal likelihood procedure [9], whereas the Elo rating system uses simple update equations suitable for on- line updates. Previous research [34] showed that the ob- tained estimates are very similar.  2.3 Models of Learning based on Logistic Func- tion  Models based on logistic function can be also extended to incorporate learning. For generating simulated data we con- sider a simple linear growth of the skill. More specifically,  for the initial skill 0 we assume normally distributed skill 0  N (,2) and we model the change in skill by linear learning: k = 0+k , where  is either a global parame- ter or an individualized learning parameter (in that case we assume a normal distribution of its values). This model is a simplified version of the Additive Factors Model [5, 6, 16]; the original additive factor model uses multiple skills. A dif- ferent variant of this model [38] usesrandom walk learning: k+1 = k + ,   N (,2).  For estimating student skills from data a commonly used technique based on logistic function is Performance Factor Analysis (PFA) [33]. The skill estimate is given by a lin- ear combination of the initial skill1 and past successes and failures of a student:  P (correct |k) = ( +   sk +   fk)  where  is the initial skill, sk and fk are counts of pre- vious successes and failures of a student during the first k attempts,  and  are parameters that determine the change of the skill associated with a correct and incorrect answer. Parameters , ,  can be easily estimated using standard lo- gistic regression. Note that originally the technique was for- mulated in terms of vectors, as it uses multiple knowledge components [33]. In our setting only the one-dimensional version is relevant.  3. TRACE LENGTH AND MASTERY ATTRI- TION BIAS  One commonly used approach to adaptation in educa- tional systems is to let students solve varying number of items (problems, questions) of similar type and diculty. Typical approach is mastery learning  students solve items until they master the topic. The termination decision (mastery detection) can be done by some simple rule (e.g., 3 correct in a row) or by a student model (e.g., probability of knowing the skill is at least 95%).  A consequence for the collected data is that students have dierent trace length (solve dierent number of items) and this dierence is not random. This creates a mastery at- trition bias2, which can influence several aspects of model evaluation and interpretation.  3.1 Learning Curves One popular approach to evaluation of educational sys-  tems are learning curves [24]. Learning curve plots the error rate (or another student performance measure like response time) as a function of number of attempts. A decreasing learning curve is evidence of learning, curves can be used to compare models or find ill-specified knowledge components.  Learning curves are based on aggregating behaviour across students and this aggregation may complicate their interpre- tation [24]. Particularly, learning curves based on data from adaptive systems are prone to the mastery attrition bias. This issue has been discussed in recent research [16, 25, 27], but is not taken into account suciently in the current prac- tice. For example a recent work on mixture modeling using learning curves [39] does assume constant length of trace  1This is usually denoted as items diculty, but in our set- ting the item diculty and initial skill are interchangeable. 2Attrition bias is a type of selection bias, which is often present for example in medical experiments.    Attempt  Er ro  r r at  e  Figure 1: Learning curves  impact of mastery at- trition.  and would require significant modification to work correctly in the presence of mastery attrition.  Figure 1 shows a specific example of the mastery attrition bias using simulated data. We use simulated students with probability of correct answer given by (+ k  0.15), where   N (2, 2) is the initial skill of a student, k is the number of attempts, and  is the logistic function. When all students solve all problems, we get a nice learning curve. When we use mastery attrition (in this case realized by a simple 3 correct in a rowrule), we get much flatter and noisier curve, which underestimates student learning.  In educational systems, particularly those that are used by students voluntary, we have have self-selection bias, which can work in opposite direction to mastery attrition. In many systems the length of trace is decided by students (rather than mastery learning or other system rule). In such cases the length of trace depends on student motivation, which can be influenced for example by success. As a simple model scenario consider the following case of heterogeneous stu- dent population consisting of two subpopulations. Students in the first subpopulation have constant success rate 50 %, students in the second subpopulation have constant success rate 80 %, i.e., students do not learn, they just dier in their prior knowledge. Now if the length of trace is cor- related with success rate (as for example in [22, 29]) and we plot the aggregated learning curve, the curve will show improvement in student performance  but this is just an illusory eect caused by student attrition. In some systems it may even happen that both the mastery attrition bias and self-selection occur [31], which can make the interpretation of data highly challenging.  3.2 Fitted Parameters The length of trace and mastery attrition bias also influ-  ence values of parameters of student models. For illustration we use two commonly used student modeling techniques de- scribed in Section 2: Bayesian Knowledge Tracing (BKT) and Performance Factor Analysis (PFA).  The trace length can have large impact on fitted param- eters particularly when there is a mismatch between model assumptions and characteristics of data. If we generate sim-  Attempt  Figure 2: Calibration of dierent variants of the PFA model for data generated by BKT. PFA X denotes a model with parameters fitted on traces of length X.  ulated data by the BKT model and then fit the data by the PFA model, there is an interesting impact of the trace length. Fitted PFA parameters (and thus also predictions of the model) may dier significantly; Figure 2 shows calibra- tion graphs for dierent trace lengths (data were generated by the BKT model with Pinit = 0.15, Plearn = 0.35, Pslip = 0.18, Pguess = 0.25). PFA is able to fit either the beginning of the trace or the end of the trace, and parameter values depends on the trace length used for parameter fitting. We obtain similar result in the opposite direction. We generate data using logistic function (+k0.4), where   N (1, 1). When we fit the data using BKT, we get significantly dier- ent values for some parameters for trace of length 5 (Pinit = 0.25, Plearn = 0.25, Pslip = 0.29, Pguess = 0.19,) and length 25 (Pinit = 0.08, Plearn = 0.24, Pslip = 0.03, Pguess = 0.21).  When we consider mastery attrition, data collection im- pacts fitted parameters even in the case when the fitted model exactly corresponds to the way data were created. Consider data generated by BKT with parameters Pinit = 0.25, Plearn = 0.08, Pslip = 0.12, Pguess = 0.3. When data are collected without attrition bias, the fitted parameters corre- spond well to the ground truth (e.g., for trace of fixed length 20 we get Pinit = 0.27, Plearn = 0.08, Pslip = 0.1, Pguess = 0.27). But when data are collected using the 3 correct in a row condition, the fitted parameters are significantly dier- ent: Pinit = 0.72, Plearn = 0.23, Pslip = 0.52, Pguess = 0.15.  These illustrations show that when researchers attempt to interpret or further use model parameters, e.g., when doing discovery with models [1], they should carefully investigate whether fitted parameters are dependent on details of data collection (trace length, attrition bias). At least researchers should report properties of the used data set (which is not the current practice).  3.3 Evaluation of Models A standard approach for evaluation and comparison of  models is to use historical data, split them into training and testing set, train model parameters on the training set,    and evaluate the performance of models on testing set using metrics [35] like RMSE, AUC, or log-likelihood.  Results of such comparison are typically interpreted as ability of models to fit student behaviour. However, these results can be influenced not just by student behaviour, but also by the way data were collected, specifically by the length of trace and the stopping condition (mastery learn- ing). Models may dier in they ability to model initial phase of learning and plateau of performance. For exam- ple if we generate data using the logistic function (( + k  0.1), where   N (0.4, 2)) and compare the fit of PFA and BKT models (using RMSE), we get that PFA has better performance if we use constant trace length, whereas BKT has better performance if data are collected using mastery learning (k correct in a row).  Another aspect of evaluation, which should be treated with caution, is the division of data between train and test set. In the context of student modeling there are multi- ple possibilities how to approach this division (e.g., student stratified, item stratified). One approach researchers have used (e.g., [17]) is to put the last 20% of attempts of some students in test set. This approach to evaluation is dis- putable because it evaluates ability of models to fit only part of student behaviour. It can be problematic particu- larly if the used data set was collected using mastery learning  in that case the last attempts in each sequence would be biased towards correct answer (whatever mastery learning criterion is used, it is based on seeing correct answers). This can bias results of model comparison. As a model situation, consider students who answer completely randomly and two student models: model A predicting probability of success 0.5, model B predicting probability of success 0.7. Since students answer randomly, the unbiased model A is better. However, if we perform evaluation on the last attempts of data collected by simple mastery learning condition 3 cor- rect in a row, model B will achieve better performance as correct answer will dominate in this test set, even though their occurrence is just due to the data collection condition, not due to some inherent aspect of student behaviour.  4. ITEM ORDERING AND SELECTION Another approach to personalization is to adaptively choose  items to suit needs of a particular student. This selection is often done with respect to diculty, i.e., stronger stu- dents get more dicult items quickly, weaker students keep practicing easier items.  Similarly to mastery learning, adaptive choice of items complicates evaluation and can bias results if untreated. As a specific example consider models build using tabulating success rate for the next problem as used in several AS- SISTment papers (e.g., [41, 42]). These models would not work in the case of an adaptive choice of items, where the educational system actively tries to achieve a given target success rate. The application of such models is thus limited to (implicitly assumed) properties of a particular data set.  4.1 Item Ordering In educational systems, it is quite natural that the order  of items is related to their diculty  students solve easier items first and then proceed to more dicult ones. If all stu- dents solve items in similar order, it may be impossible to disentangle increase in problem diculty and student learn- ing. This confounding eects has been in dierent forms  noted in several recent works [12, 15, 17, 36]. To make this eect clear, let us consider the following  model scenario. Assume that students skill linearly in- creases (k = 0+k , where k is the order of an attempt), items are ordered in a fixed order with linearly increasing dif- ficulty (dk = d0 + k 0), and probability of correct answer is given by a logistic function with respect to the dierence of skill and diculty: P (correct |k) = 1/(1 + e(kdk)). In this situation parameters are non-identifiable, we get iden- tical probabilities of correct answers for completely dier- ent situations, e.g., no learning and fixed diculty of items ( = 0 = 0) and significant learning and large increase in diculty of items ( = 0 = 0.5).  The ordering of items may influence collected data also in other ways. There may be a local transfer between con- secutive items, e.g., when math problems with very similar structure are asked in sequence or due to short term mem- ory in factual knowledge learning. Such eects can have large impact on correctness of answers, but provide little ev- idence about long term learning. Items in the beginning of a sequence may have lower success rate (or higher response times) just due to user interface issues (students have to get used to peculiarities of a particular system). Last items may have lower success rate due to fatigue.  It may be hard to overcome item ordering eects. We may try to incorporate some variability into item ordering for in- dividual students, but typically the basic easier to dicult progression is desirable. We should nevertheless take this issue into account and at least analyze the collected data to understand its properties and potential limitations. Use- ful descriptive statistics for this purpose include analysis of mean presentation order of each item (used in [17]), anal- ysis of correlation between orderings of dierent students (used in [36]), or analysis of transitions between items (used in [23]).  4.2 Adaptive Choice of Items The goal of many adaptive educational systems is to se-  lect items of suitable diculty (neither too dicult, nor too easy) to keep students in the flow state [8]. This adaptive choice of items can have important impact on evaluation of models. We illustrate this impact on data from our widely used application for learning geography [30].  We utilize data from an experiment which compares four question construction algorithms with dierent degree of adaptivity [31]. Questions used by the system are of the form What is the name of the highlighted place and Where is X. The question construction process has two phases: at first, selection of the question stem (e.g., Rwanda), at second, decision how many options to use for the multiple- choice question and what distractors to use (e.g., Burundi and Tanzania). For each step two choices were considered: random and adaptive. The decisions of the adaptive algo- rithm are done using predictions of a student model based on the Elo rating system [30, 28], aiming at a specific tar- get success rate [29] and using the most confusing distrac- tors. Users of the system were allocated randomly into one of four versions of the question construction algorithm: adaptive-adaptive, adaptive-random, random-adaptive, and random-random. During this experiment we collected be- tween 200 000 and 250 000 answers for each of the four groups. Every data set was split into a train set (20%) and a test set (80%) in a student-stratified manner. Since    The used data set  Figure 3: RMSE comparison over data from slepemapy.cz collected using dierent item selection algorithm.  all models work online and update their parameters during evaluations, the choice of the size of the train set does not have big influence on reported results.  Figure 3 shows comparison of several models over these data sets. As our point is not to study models, but to show impact of data collection, we have chosen simple mod- els (predictors of student performance): constant predic- tor (given by global average), student average, item aver- age, and the Elo-based model used in the actual applica- tion [30]. The figure show several interesting results. With higher adaptivity in question construction, the RMSE val- ues are higher (prediction is more dicult) and closer to- gether (naive predictors have similar performance as better models since questions are constructed to be o a specified diculty). We also see a swap in ordering of models  the relative performance of two models (item average, student average) depends on what data are used to compare them. These results clearly illustrate the impact of the method that is used to collect data on model comparisons done using the data set.  4.3 Feedback Loop between Student Model and Data Collection  The evaluation described in the previous section is a spe- cific illustration of a feedback loop between student model and data collection  a model influences which data are col- lected, the collected data are used to evaluate the model. To explore this feedback loop in more detail we performed experiments with a simulation of a simplified version of an adaptive question answering systems described in the previ- ous section. Details of the simulation are described in [26], the basic idea of the experiment is that we choose one stu- dent model and use it as an input for the adaptive choice of items. At the same time we let other models do predictions as well and log answers together with all predictions. Since we are using simulated data, we know the ground truth and we can compare models with optimal predictions.  Figure 4 shows the resulting RMSE for each model in indi- vidual runs (data collected using specific model). The figure shows the same basic results that we have seen for real data. When the data are collected using the optimal model, the RMSE values are largest (at least for more sophisticated models) and closest together; even the ordering of models is dierent from other cases. In this case even the constant model provides comparable performance to other models   but it would be very wrong to conclude that predictive ac- curacy of models is so similar that the choice of model does not matter, since in the simulated system dierent models lead to dierent choice of items and consequently to dier- ent student experience. The reason for small dierences in RMSE is not similarity between models, but characteristics of data (good choice of suitable items), which make pre- dictions dicult and even a naive predictor comparatively good.  In real systems, the content is often organized in knowl- edge components (KCs, also called concepts or skills), and this domain model is also used by student models. The relation between items and knowledge components can be generally described by a Q-matrix [2] or  in case of the strict division of items to components  by item-KC map- ping. The quality of a Q-matrix (or a item-KC mapping) can have strong impact on adaptive behaviour of the sys- tem [18], but this quality is dicult to measure. The dif- ference in performance of models with dierent Q-matrices can be  as demonstrated by following hypothetical scenario  relatively small. Consider two Q-matrices, one of them is correct and has 10 knowledge components, the other one is incorrect and merges two of the skills together. The dier- ence between performance metrics of these two models will be necessarily small, since in most cases their predictions will be identical. The dierence is, however, of practical significance, because if a system uses the incorrect model, students may miss practice of one of the concepts. A realis- tic scenario of this type is reported in [21].  Our simulated experiment suggests that this naturally small but important dierence in performance metrics is also influenced by the used data set (resp. method used to collect the data). To highlight the point we use a simple setup: two knowledge components, every item is assigned to exactly one of these KCs and every student has two independent skills corresponding to these KCs. The basic Elomodel does not consider item division and assumes only one KC, the Elo concepts model considers correct item-KC mapping, and the Elo wrong concepts model contains random mistakes in its item-KC mapping. Figure 4 shows that models with concepts achieve nearly the same performance (i.e., models seem to be of the same quality), when data are collected by the most sophisticated models (models with concepts and optimal one). But over other data sets, the dierence be- tween the two models becomes more apparent and the re- sults clearly show that Elo conceptsmodel is better. Note that the data are able to better distinguish between models if they are actually collected by worse models.  Similarly, it can be dicult to detect wrongly specified prerequisites if we are using them to collect our data. To illustrate this point we consider the following simple sce- nario. We wrongly assume that a concept A is prerequisite for a concept B. It would be possible to detect this error by observing students who are able to solve problems from the concept B but have diculty solving problems from the con- cept A. However, if our adaptive system uses the wrongly specified prerequisite, it oers problems from the concept B only to students who have mastered the concept A and thus is not able to collect data which would provide evidence that the specified prerequisite is wrong.  4.4 Parameter Estimation In Section 3.2 we have shown how the length of trace    Figure 4: Comparison of student model performance (measured by RMSE) over data collected using dierent models.  and attrition bias can influence estimated parameter values. Adaptive choice of items can also have impact on parameter estimates. In previous work [34] we have shown this eect using simulated data, here we illustrate the eect using real data.  We use the same data as in Section 4.2  experiment with adaptive and random question selection in geography prac- tice system. We compare two techniques for estimating dif- ficulty of individual items  a naive percent correct tech- nique and a variant of the Elo rating system that is used in the actual implementation [30]. Figure 5 shows distributions of the estimated diculties (expressed as probability of cor- rect answer for an average student) for European countries (which have most answers in the used data set). The figure shows dierences between the used techniques, particularly the ability of the Elo rating system is to better dierenti- ate the diculty of individual items in the case of adaptive data collection. But the impact of the data set is much larger than the impact of the used model.  This experiment clearly illustrates that the way data are collected may have much larger influence on the fitted pa- rameters than a choice of the model. This is important particularly for discovery with models [1], in which model parameters are further utilized and interpreted. Such analy- sis needs to take the data collection mechanism into account.  5. CONSEQUENCES FOR PRACTICE We have described dierent ways how data collection mech-  anism influences interpretation and evaluation of student models. These issues have direct consequences not just for the realization of evaluation of models, but also for publica- tion of data sets and the way we collect data in our systems in the first place.  5.1 Publication of Data Sets Currently, most published data set document only the  data itself, but not the way in which the data were collected. As the data collection can have important impact on the in- terpretation of the data, it is necessary to document data collection mechanism as well.  The behaviour of adaptive systems is quite complicated (e.g., many parameters often influence the exact choice of items) and it may not be feasible to document the data col- lection mechanism up to all details. But authors of data  Figure 5: Distributions of estimates of diculty for European countries across dierent data sets and models.  sets should explicitly discuss all major issues and potential limitations due to the data collection mechanism, particu- larly attrition bias and algorithms used for item ordering and selection.  5.2 Evaluation and Interpretation of Models Researchers who use published data that they did not col-  lect themselves should inquire into details of the used data collection mechanism. It is useful to perform exploration of data set properties to get understanding of the data and its potential biases.  When doing evaluation and interpretation of student mod- els (and their parameters), special attention should be paid to the influence of properties of the used data set. We should make sure that our results are not superficially created by    the data collection mechanism. Careful attention should be paid particularly to division of data between train and test set as data collection mechanism can easily cause bias, which may lead to poor generalization of results.  In order to avoid biases caused by data collection, it is useful to probe stability of achieved results (comparison of model performance, fitted parameter values). What hap- pens when we use artificially shorter trace lengths Do re- sults stay the same (similar) In some cases it may be im- possible to perform such probes  for example if we have access only to oine data and we care about ordering of items for dierent student, we cannot perform experiments with dierent orderings. In such cases it is important to explicitly discuss limitations and future work should try to replicate results with newly collected data overcoming stated limitations.  5.3 Data Collection Our results also have consequences for the data collection  itself. We have repeatedly illustrated how the use of adap- tive techniques leads to data set, which make it dicult or even impossible to compare student models and find mis- takes in their specification (e.g., in knowledge components or prerequisites). The adaptive behaviour is the purpose of student modeling and is beneficial for students, it is, how- ever, detrimental for evaluation purposes.  It should be possible to find a reasonable compromise be- tween our dierent goals. We can modify behaviour of our educational systems in a way that would enable easier evalu- ation without hampering their main goal (i.e., student learn- ing). Specifically, we may employ controlled use of random- ization. If some items are chosen randomly (from a reason- ably defined set of items), the impact on user experience may be negligible and the collected data can be used for evalua- tion in much more straightforward manner than adaptively chosen items.  6. ACKNOWLEDGMENTS This publication was written with the support of the Spe-  cific University Research provided by the Ministry of Edu- cation, Youth and Sports of the Czech Republic.  7. REFERENCES [1] R. S. Baker and K. Yacef. The state of educational  data mining in 2009: A review and future visions. Journal of Educational Data Mining, 1(1):317, 2009.  [2] T. Barnes. The q-matrix method: Mining student response data for knowledge. In Educational Data Mining, 2005.  [3] L. Bottou, J. Peters, J. Quinonero-Candela, D. X. Charles, D. M. Chickering, E. Portugaly, D. Ray, P. Simard, and E. Snelson. Counterfactual reasoning and learning systems: The example of computational advertising. The Journal of Machine Learning Research, 14(1):32073260, 2013.  [4] C. Brooks, O. Chavez, J. Tritz, and S. Teasley. Reducing selection bias in quasi-experimental educational studies. In Learning Analytics And Knowledge, pages 295299. ACM, 2015.  [5] H. Cen, K. Koedinger, and B. Junker. Comparing two irt models for conjunctive skills. In Intelligent Tutoring Systems, pages 796798. Springer, 2008.  [6] H. Cen, K. R. Koedinger, and B. Junker. Is over practice necessary-improving learning eciency with the cognitive tutor through educational data mining. Frontiers in Artificial Intelligence and Applications, 158:511, 2007.  [7] A. T. Corbett and J. R. Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and user-adapted interaction, 4(4):253278, 1994.  [8] M. Csikszentmihalyi. Flow: The psychology of optimal experience. Harper Perennial, 1991.  [9] R. De Ayala. The theory and practice of item response theory. The Guilford Press, 2008.  [10] A. E. Elo. The rating of chessplayers, past and present, volume 3. Batsford London, 1978.  [11] S. E. Fancsali, T. Nixon, A. Vuong, and S. Ritter. Simulated students, mastery learning, and improved learning curves for real-world cognitive tutors. In AIED Workshops, 2013.  [12] J. Gonzalez-Brenes, Y. Huang, and P. Brusilovsky. General features in knowledge tracing: applications to multiple subskills, temporal item response theory, and expert knowledge. 2014.  [13] A. Gunawardana and G. Shani. A survey of accuracy evaluation metrics of recommendation tasks. The Journal of Machine Learning Research, 10:29352962, 2009.  [14] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems, 22(1):553, 2004.  [15] P. Jarusek, M. Klusacek, and R. Pelanek. Modeling students learning and variability of performance in problem solving. In Educational Data Mining, pages 256259, 2013.  [16] T. Kaser, K. R. Koedinger, and M. Gross. Dierent parameters-same prediction: An analysis of learning curves. In Educational Data Mining, 2014.  [17] M. M. Khajah, Y. Huang, J. P. Gonzalez-Brenes, M. C. Mozer, and P. Brusilovsky. Integrating knowledge tracing and item response theory: A tale of two frameworks. Personalization Approaches in Learning Environments, page 7, 2014.  [18] K. R. Koedinger, J. C. Stamper, E. A. McLaughlin, and T. Nixon. Using data-driven discovery of better student models to improve student learning. In Artificial Intelligence in Education, pages 421430. Springer, 2013.  [19] J. Langford, A. Strehl, and J. Wortman. Exploration scavenging. In International Conference on Machine learning, pages 528535. ACM, 2008.  [20] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased oine evaluation of contextual-bandit-based news article recommendation algorithms. In Web search and data mining, pages 297306. ACM, 2011.  [21] R. Liu, K. R. Koedinger, and E. A. McLaughlin. Interpreting model discovery and testing generalization to a new dataset. In Educational Data Mining, pages 107113, 2014.  [22] D. Lomas, K. Patel, J. L. Forlizzi, and K. R. Koedinger. Optimizing challenge in an educational    game using large-scale design experiments. In SIGCHI Conference on Human Factors in Computing Systems, pages 8998. ACM, 2013.  [23] M. Lopes, B. Clement, D. Roy, and P.-Y. Oudeyer. Multi-armed bandits for intelligent tutoring systems. Journal of Educational Data Mining, 7(2):2048, 2015.  [24] B. Martin, A. Mitrovic, K. R. Koedinger, and S. Mathan. Evaluating and improving adaptive educational systems with learning curves. User Modeling and User-Adapted Interaction, 21(3):249283, 2011.  [25] R. C. Murray, S. Ritter, T. Nixon, R. Schwiebert, R. G. Hausmann, B. Towle, S. E. Fancsali, and A. Vuong. Revealing the learning in learning curves. In Artificial Intelligence in Education, pages 473482. Springer, 2013.  [26] J. Niznan, R. Pelanek, and J. Papousek. Exploring the role of small dierences in predictive accuracy using simulated data. In AIED Workshop on Simulated Learners, 2015.  [27] T. Nixon, S. Fancsali, and S. Ritter. The complex dynamics of aggregate learning curves. In Educational Data Mining, 2013.  [28] J. Niznan, R. Pelanek, and J. Rihak. Student models for prior knowledge estimation. In Educational Data Mining, pages 109116, 2015.  [29] J. Papousek and R. Pelanek. Impact of adaptive educational system behaviour on student motivation. In Artificial Intelligence in Education, volume 9112, pages 348357, 2015.  [30] J. Papousek, R. Pelanek, and V. Stanislav. Adaptive practice of facts in domains with varied prior knowledge. In Educational Data Mining, pages 613, 2014.  [31] J. Papousek, V. Stanislav, and R. Pelanek. Evaluation of an adaptive practice system for learning geography facts. In Learning Analytics & Knowledge, 2016.  [32] A. Paramythis, S. Weibelzahl, and J. Mastho. Layered evaluation of interactive adaptive systems: framework and formative methods. User Modeling and User-Adapted Interaction, 20(5):383453, 2010.  [33] P. I. Pavlik, H. Cen, and K. R. Koedinger. Performance factors analysis-a new alternative to knowledge tracing. In Artificial Intelligence in Education, volume 200 of Frontiers in Artificial Intelligence and Applications, pages 531538. IOS Press, 2009.  [34] R. Pelanek. Application of time decay functions and Elo system in student modeling. In Educational Data Mining, pages 2127, 2014.  [35] R. Pelanek. Metrics for evaluation of student models. Journal of Educational Data Mining, 7(2), 2015.  [36] R. Pelanek and P. Jarusek. Student modeling based on problem solving times. International Journal of Artificial Intelligence in Education, pages 127, 2015.  [37] G. Shani and A. Gunawardana. Evaluating recommendation systems. In Recommender systems handbook, pages 257297. Springer, 2011.  [38] A. C. Smith, L. M. Frank, S. Wirth, M. Yanike, D. Hu, Y. Kubota, A. M. Graybiel, W. A. Suzuki, and E. N. Brown. Dynamic analysis of learning in  behavioral experiments. The journal of neuroscience, 24(2):447461, 2004.  [39] M. Streeter. Mixture modeling of individual learning curves. In Educational Data Mining, 2015.  [40] B. van de Sande. Properties of the bayesian knowledge tracing model. Journal of Educational Data Mining, 5(2):1, 2013.  [41] E. Van Inwegen, S. Adjei, Y. Wang, and N. Heernan. An analysis of the impact of action order on future performance: the fine-grain action model. In Learning Analytics And Knowledge, pages 320324. ACM, 2015.  [42] E. G. Van Inwegen, S. A. Adjei, Y. Wang, and N. T. Heernan. Using partial credit and response history to model user knowledge. In Educational Data Mining, 2015.  [43] S. Wager, N. Chamandy, O. Muralidharan, and A. Najmi. Feedback detection for live predictors. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages 34283436. Curran Associates, Inc., 2014.  [44] M. Yudelson. Tool for fitting bayesian knowledge tracing models, 2014. https://github.com/IEDMS/standard-bkt.    "}
{"index":{"_id":"7"}}
{"datatype":"inproceedings","key":"Hsiao:2016:SVA:2883851.2883915","author":"Hsiao, I-Han and Pandhalkudi Govindarajan, Sesha Kumar and Lin, Yi-Ling","title":"Semantic Visual Analytics for Today's Programming Courses","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"48--53","numpages":"6","url":"http://doi.acm.org/10.1145/2883851.2883915","doi":"10.1145/2883851.2883915","acmid":"2883915","publisher":"ACM","address":"New York, NY, USA","keywords":"auto grading, dashboard, intelligent authoring, orchestration technology, programming, semantic analytics, visual analytics","abstract":"We designed and studied an innovative semantic visual learning analytics for orchestrating today's programming classes. The visual analytics integrates sources of learning activities by their content semantics. It automatically processs paper-based exams by associating sets of concepts to the exam questions. Results indicated the automatic concept extraction from exams were promising and could be a potential technological solution to address a real world issue. We also discovered that indexing effectiveness was especially prevalent for complex content by covering more comprehensive semantics. Subjective evaluation revealed that the dynamic concept indexing provided teachers with immediate feedback on producing more balanced exams.","pdf":"Semantic Visual Analytics for Todays Programming  Courses  I-Han Hsiao  School of Computing, Informatics &   Decision Systems Engineering,  Arizona State University, Tempe, AZ,   USA   Sharon.Hsiao@asu.edu   Sesha Kumar Pandhalkudi  Govindarajan   School of Computing, Informatics &  Decision Systems Engineering,   Arizona State University, Tempe, AZ,  USA    spgovind@asu.edu  Yi-Ling Lin   Department of Information   Management  The National Sun Yat-Sen University   No. 70, Lienhai Rd.,   Kaohsiung 80424, Taiwan   yllin@mis.nsysu.edu.tw     ABSTRACT  We designed and studied an innovative semantic visual learning  analytics for orchestrating todays programming classes. The  visual analytics integrates sources of learning activities by their  content semantics. It automatically processs paper-based exams  by associating sets of concepts to the exam questions. Results  indicated the automatic concept extraction from exams were  promising and could be a potential technological solution to  address a real world issue. We also discovered that indexing  effectiveness was especially prevalent for complex content by  covering more comprehensive semantics. Subjective evaluation  revealed that the dynamic concept indexing provided teachers  with immediate feedback on producing more balanced exams.    Categories and Subject Descriptors   Computing education ! Computing education Programs !  Computer science education!CS1   Keywords  Visual analytics, programming, auto grading, semantic analytics,  intelligent authoring, dashboard, orchestration technology.    1. INTRODUCTION  Technology has become much more affordable and made  abundant learning materials available online, which provides  unprecedented opportunities to integrate various learning analytics  in tracking diverse learning activities. However, paper-based  exams are still one of the main assessment methods in todays  majority of classrooms. On one hand, cheating remains a big issue  in the exams taken online. On the other hand, making meaningful  exams is very time-consuming. Paper based exams simply do not  convert to online tests overnight. There exists a technology gap  between real classrooms and ideal technology-enabled ones. Such  gap becomes more apparent especially in blended classrooms,  where lectures and exams are delivered in traditional settings, but  lecture slides, study guides, assignments and other educational  resources are provided electronically through online portal or  course management systems.    In the field of Computer Supported Collaborative Learning  (CSCL), researchers describe it as a field in transition for  classroom orchestration, which defines how a teacher manages  multilayered activities in real time and in a multi-constraints  context [1]. Orchestration emphasizes attention to the challenges  of classroom use and adoption of research-based technologies [2].  Managing physical and cyber courses together can be demanding  enough, yet adding more complex tools on top may make the  complexity and time demands of technology even worse.  Essentially orchestration technology may introduce new and  unnecessary complications [3]. Our goal in this work is to study a  less intrusive learning analytics approach that taps into blended  classrooms with minimum technology introduction. We aim to  assist teachers in continuing managing blended classrooms with  their own traditional instructional and assessment methods, but  connect with advanced learning analytics without modifying  existing course delivery process. According to the broadening  participation in Computer Science Education, we currently focus  on Introduction to Programming classes, which are the  cornerstone courses offered across programs & majors in almost  all universities.    In doing so, we have identified the good, the bad and the ugly of  traditional assessments in programming classrooms.  Take paper- based programming exams for instance, they are easier for  instructors to proctor and to manage the exam questions and to  prevent from cheating, but they are suffering from (a) challenge  for teachers to give personalized feedback on each individual test;  (b) grading large scale amount of paper-based exams can be very  time consuming and can create inconsistencies among graders; (c)  paper-based exams are harder to keep persistent traces on detailed  performances (i.e. no traces on how a student received partial  credits; semantic level assessments and several other learning  analytics are unavailable). When a student receives marks on a  paper-based exam, it is difficult to get feedback in understanding  if it is a single concept mistake, careless mistake or a long term  misconception. Without the persistent traces of learning analytics,  it is difficult for student to manage learning. With the existing  pitfalls of instrumenting paper-based exams, it may result in  students focusing solely on the score they earned on the returned  exams, and missing several learning opportunities, such as  identification of strength and weakness, characterizing the nature  of their errors or any recurring patterns if any, appropriateness of  their study strategies and preparation [4]. Thus, the grand  objectives are not only to be able to support advanced learning  analytics by providing detailed and in-depth semantic feedback  through traditional assessment methods, but also to leverage using  visualizations in visual analytics to promote reflection, self- monitoring, and support planning. In this work, we focus on  presenting the design and evaluating the automatic indexing   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK16, April 2529, 2016, Edinburgh, United Kingdom  Copyright 2016 ACM ISBN 978-1-4503-4190-5/16/04$15.00.  DOI: http://dx.doi.org/10.1145/2883851.2883915         methods to associate semantics to paper-based exam questions  and establish online persistent semantic visual learning analytics.   The rest of the paper is structured with literature review, research  platform (semantic visual analytics system, EduAnalysis) and  study methodology with underlying assumptions. Finally we  present the evaluation results and discussed implications.    2. LITERATURE REVIEW  2.1 Orchestration & Learning Analytics   In the field of Computer Supported Collaborative Learning  (CSCL), researchers describe it as a field in transition for  classroom orchestration, which defines how a teacher manages  multilayered activities in real time and in a multi-constraints  context. It discusses how and what research-based technologies  have been adopted and should be done in classrooms [1]. We have  begun to see more tabletops, smart classrooms or interactive tools  such as Classroom Response Systems (AKA: Clickers) etc.  provide dynamic feedback and integrative students knowledge  updates [5-8]. One of the biggest criticisms of introducing  orchestration technology in class is that it might potentially add  more complexity and time demands of technology and introduce  new and unnecessary complications [3].     In the inaugural LAK proceeding, researchers describe a  framework, TMTA [9], in discussing the importance involving  three stakeholders in learning analytics: teaching expert, visual  analytics expert and design-based research expert. The focus of  learning analytics has been on the integration of computational  and methodological support for teachers to properly design,  deploy and assess learning activities. In addition, the focus is also  to immerse students in rich, personalized and varied learning  activities in information ecologies and data-rich classrooms [9].  One of the pioneer systems that align with TMTA framework is  eLab (exploratory Learning Analytics Toolkit). It was designed to  enable teachers to explore and correlate content usage, to help  teachers reflect on their teaching according to their own interests  [10]. ASSISTments [11] an integrative tutoring system includes  assistance and assessment components for students and teachers.  The system is built on a mantra - put the teacher in charge, not  the computer, which creates flexibility to allow teachers to use the  tool in organizing the classroom routines.    2.2 Visual Learning Analytics & Student  Modeling  Visual learning analytics, essentially, extends the scope of  information visualization by using computer-supported techniques  to visualize learning information in amplifying human cognition.  It goes beyond the footprints representation of summarizing and  visualizing interactions or behaviors between students and  learning content. Examples like network visualizations in  semantic discourse analysis [12], dashboard visualizations to  provide historical data in supporting awareness, teaching  practices, explore and/or identify monitor status [13, 14]. The  working group, VISual Approaches to Learning Analytics  (VISLA) workshop, in the Fifth International Conference of  Learning Analytics and Knowledge [15], gathered a range of  visual learning analytics cases. For instance, applying sentence  compression technique in analyzing short answer questions in  network visualizations; utilizing predictive modeling to visualize  uncertainty of academic risks; innovative visualizations for  visualizing semantics in discussion forums [16] etc. Studies  showed that the majority of visual learning analytics discusses  visual representations or the systems usefulness while the core   should be focused on real impact to improve learning or teaching  [13]. However, from student modeling literature, we found several  successful examples presented interactive visualizations in  supporting students learning. Such approach is called Open  Student Modeling (OSM). It is a group of approaches that makes  traditionally hidden student models available to the learner for  exploration and possible editing. Representations of the student  models vary from displaying high-level summaries (such as skill  meters) to complex concept maps or Bayesian networks. A  spectrum of OSM benefits have been reported, such as increasing  the learners awareness of their own developing knowledge and  difficulties in the learning process; as well as student engagement,  motivation, and knowledge reflection [17-20]. Several other  examples of OSM interfaces reported promising results too. For  instance, interacting with open learner modeling engages learners  in negotiating with the system during the modeling process [21].  Progressor system integrates open learning models with social  visualization that can dramatically increase student motivation to  work with non-mandatory educational content [22] and encourage  students to start working sooner. Chen et al. [23] investigated  active open learner models in order to motivate learners to  improve their academic performance. Both individual and group  open learner models were studied and demonstrated the increase  of reflection and helpful interactions among teammates.   CourseVis provides graphical visualization to teachers and  learners for multiple groups of users and helps instructors to  identify problems early on, and to prevent some of the common  problems in distance learning [24].  3. SEMANTIC VISUAL ANALYTICS  SUITE: EduAnalysis   EduAnalysis is a semantic visual analytics suite specifically  designed to extract semantics from physical learning environment  and map onto a virtual setup to integrate blended learning  activities. We developed a web application, including three main  components, frontend analytics dashboard and web services to  process physical data input (such as paper exam processing  service, manual concept indexing service etc.), backend consists  of an ontology parser, a concept mapper that maps sources of  collected data to their corresponding concepts, and an analytics  framework that exposes insights from data using APIs, and output  via dashboard. EduAnalysis is designed to provide semantics  representation of diverse learning activities between inside and  outside classrooms, in order to provide more holistic and realistic  learning analytics by harnessing the learning content semantics.    Teachers can upload an exam paper with a simple one click  (interface omitted) and EduAnalysis will trigger exam parsing  service to perform automatic concept indexing and immediately  lead teachers to an overview (Figure 1). It guides teachers to  navigate the entire exam concept distribution. Teachers can opt  for further editing on exam questions or provide concept emphasis  configuration. Figure 3 shows a view of the authoring phase. Left  panel displays each question texts, which enables dynamic editing  and indexing to provide teachers instant feedback of the indexing  performances. The correct answer and corresponding marks can  also be collected and adjusted here, for future auto-grading  services and partial credit assignment based on semantics  services. Middle panel shows an interactive ontology authoring  circle packing visualization. Teachers can select the bubble to  zoom in and out to examine the concept coverage. Teachers can  select/deselect concepts for the corresponding question by  clicking on the bubble. They can also adjust the slider bar to  configure the concept emphasis.       4. METHODOLOGY  This project aims to study an innovative semantic visual analytics  that supports sources of learning activities and how teachers  would perceive of using it in programming language courses. We  hypothesized that intelligent automatic semantic indexer is an  effective method to collect semantic information from course  content. We call the instance of automatic exam concept indexing  service as ExamParser, it inherits from generic Topic Facet  Model [25], which consists of natural language parser and domain  specific language Parser. It recognizes exam question patterns in a  document and extracts content by indexing each question to  corresponding concepts (a high level concept topic and sets of  facets). A typical exam question pattern includes  question_text{phrased as natural language, may or may not  contain domain specifics} , codes{composed as fully or partially  of an entire executable program}, answer_type{ranges from  multiple choices, fill-in-the-blanks, short answers, code writing  etc.} For instance, a sample exam question is presented in Figure  2. It contains mainly natural language phrased question  descriptions, a piece of partial executable java codes, and multiple  choices answer type for this question. ExamParser will translate  this question as a set of concepts {Q1: ForStatement,  VariableInitialization, ConditionalStatement, LessOperator,  IncrementOperator, MethodInvocation, AssignmentOperator,  Arithmetics}. However, do these concepts all weigh equally in this  exam question If we purely count the concept appearances,  Question 1 consists of three AssignmentOperators and one  ForStatement. Does it mean that ForStatement is less important  than AssignmentOperator in this question The answer is it  depends! Therefore, we design a dynamic concept indexing  authoring interface in the parser (Figure 3). It labels each parsed  concept with the equivalent, default quantity weights, but the  weights are adjustable according to teachers emphasis. In the  case of using Question 1 in a CS1 midterm exam, the focus should  be on ForStatement, ConditionalStatement concepts; using the  same question in a CS1 final exam, every concept should weigh  equally proportionally. In addition, providing dynamic concept  weight authoring interfaces not only allows teachers to include or  exclude additional or redundant concepts to exam questions, but  also enables dynamic exam content editing and corresponding  concept indexing (Figure 3). Embedding such dynamic authoring  mechanism along with intelligent parsing can help raise teachers  flexibility to configure and coordinate entire exam topical   emphasis, at the same time, complement to algorithmic flaws, in  case of any missing concepts.     Figure 1: Exam overview on topics and concepts distribution     Figure 2: Question1: a sample exam question.   The concept indexing method enables a scalable framework in  two essential educational technology aspects: (1) systematically  assign partial credits, which they are traditionally provided by  teachers experiences or generic grading rubrics (such as credits to  right path toward key concepts but erroneous implementation). By   1. What is the final value of sum displayed to the  console:      for(int i = 0; i < 5; i++)     {       int sum = 0;       sum = sum + i;       System.out.println(sum);       }   a. 5  b. 10  c. 15  d. Compiler error  e. None of the above         Figure 3: EduAnalysis authoring interface provides (a) dynamic question text editing and indexing; (b) correct answer  indication & question marks assignment (c) concept emphasis configuration.        associating each programming problem to weighted concept sets  facilitates an organized fashion to quantitatively distribute partial  credits in semantic level; and (2) harness different levels of  learning analytics on both individual and group levels, including  strong and weak concept clusters, misconceptions co-occurrences,  conceptual progress over time etc. Currently we focus on  aggregating various levels of semantics analytics as teachers  overviews, however, these results can serve as detailed feedback  to students as well. For instance, on a returned exam to student,  student will no longer just receive the grade marks, but also a  detailed feedback on what kinds of errors they made on the  exams. The approach provides (1) individualized detail conceptual  feedback, which normally cant be done especially on large class  size; (2) analytics to keep persistent traces on students conceptual  growth; (3) opportunities for students to engage in reflection and  self-monitor their own learning (foster metacognition  development).    5. EVALUATION RESULTS  To evaluate the proposed semantic visual analytics for  programming courses, we performed (1) content evaluation to  examine the semantic approach impacts on exam question  content; and (2) subjective evaluation by collecting interview  feedback from teachers to understand user experiences and system  usefulness. We collected 4 programming introductory courses  exams, with a total 76 exam questions in the subject of Object- Oriented Programming. Each exam was populated in  EduAnalysis; each question was automatically associated with a  set of concepts through Topic Facet Model algorithm [25]. In  order to verify the embedded indexing algorithm effectiveness, we  had also collected the ground truth of corpus concept indexing by  two expert judges, who both have more than 5 years teaching  experiences in the subject domain. They manually examined the  entire corpus by selecting concepts from a list of JAVA ontology  keywords, which the inter-rater reliability was measured using  Cohens Kappa= 0.386.    5.1 Concept Indexing Effects on Content  We consider the following aspects to assess analytics impacts on  domain content: Content Complexity & Content Knowledge  Structure.   5.1.1 Content Complexity:   According to CS1 course curriculum, depending on the exam foci  topics, we split each exam by three levels of complexities, easy,  moderate and complex. For instance, first exam usually covers  topics from variables, primitive data types, arithmetic operations,  Strings, conditions etc. These topics are usually considered  relatively easy in the entire CS1 curriculum. However, in order to  assess students knowledge, an early CS1 test usually is devised  with a mixture of difficulty levels questions. Thus, a question  comprising of multiple topics was considered as a complex  question in that exam. We have tabulated two interesting findings.  Firstly, we found that human experts (experienced teachers) had  no differences in indicating the mount of concepts among three  complexity levels of questions. The results supported the point  that teacher experts tended to point out key concepts, instead of all  concepts. Secondly, ExamParser indexed significantly more  concepts in complex questions than the other two categories  (Table 1). This result was very encouraging. More complicated  questions were usually the ones that students made mistakes,  which suggested more attention was demanded. However,  teachers may not necessarily have the class time to go through  details on every single question. Even if they did, such as   mentioned key concepts of the tougher questions, the amount of  feedback may not be sufficient. This is where the ExamParser can  make a difference by supplying more detail feedback.    Table 1: Average # indexed concept by content complexity &  Concept coverage by knowledge type   Avg #concept Baseline ExamParser   Easy 1.9430.121 4.4000.541   Moderate 2.3180.253 4.4550.443   Complex 2.3160.410 8.1580.763   Concept/question Baseline ExamParser   Declarative  2.4891.527 5.5313.000   Procedural  2.4280.986 6.3361.776   5.1.2 Knowledge Structure: Procedural vs.  Declarative Knowledge  In order to address the cognitive aspects of our approachs impact  on learning content, we analyzed the indexed exam questions  based on their knowledge types, procedural knowledge and  declarative knowledge. A coarse-grained definition on procedural  knowledge explains one knows how to do something; declarative  knowledge approximately defines the knowledge about  something. Thus, we identified the majority of the code writing  questions were to test students procedural knowledge, and most  of the multiple choices questions were designed to assess  declarative knowledge. However, there were a few exception  cases did not follow such classification. For instance, in one of the  code-writing questions, students were asked to write Java code to  Instantiate an ArrayList that contains decimal numbers and  assign it to an appropriate variable. The question only involved  syntactical tasks of the programming language, but excluded the  application of syntax to perform further problem solving tasks.  Thus, even though it was a code-writing question, it was classified  as declarative question. Overall, we found 55% procedural  questions and 45% declarative questions in the corpus. Based on  the indexed concepts both by human experts and ExamParser, we  found that, both types of questions had significant higher concepts  indexed by ExamParser than the experts. This was consistent with  5.1.1, where ExamParser achieved higher coverage. What was  interesting to note was that there were no significant differences  between declarative and procedural knowledge types of questions,  no matter who and how the questions were indexed. It showed the  consistency among experts and the algorithm, which indicated  ExamParsers stability. Although, we anticipated procedural type  questions would have been indexed more concepts due to knowing  how to do may inherently involve some declarative knowledge  components in addition to apply them to solve problems.  However, we did not find such pattern observed. Possible  explanations could be declarative types of questions (i.e. multiple  choices) tend to include a range of meaningful distractor choices  to prevent from simple memorization tasks. It also explained why  there were larger variances in declarative type of questions  compared to procedural ones.    5.2 Subjective Evaluation   We conducted a structured interview with two programming  course instructors. Both are currently using Blackboard as course  management platform and both give lectures and paper-based  exams. One teaches medium size of Java courses (20~50 students  averagely) and one teaches large size of courses (> 100 students     averagely). We were mostly interested in finding how do  instructors analyze students learning activities outside classrooms  if any. Both instructors provide extra online learning materials  (i.e. problem-solving or the sort) for students to perform self- assessments as non-mandatory resources for their courses. They  encourage students to do more work through the selected online  resources and provide partial credits for formal assessment as  incentive.    We then allowed both instructors to explore EduAnalysis system  and solicited feedback on the usefulness and potential threats of  current implementation. They were instructed to test on the  concept indexing procedure for different types of questions. They  tried multiple choices and open-ended questions, and both agreed  that the dynamic concept indexing provided them immediate  feedback on producing more balanced exams. Both instructors  reported that they found it convenient to perform one-click to  upload and index exam concepts. They compared the experience  with Blackboard evaluation feature, which requires them to  configure each question one by one. Although the indexing  authoring interface is available for every question, instructors  considered it as flexible to assign designated emphasis to  accommodate CS1/CS2 exams, or first/final exams. There were  two major criticisms from both instructors: (1) they worried the  auto-indexing precision may not be stable and result in them  doing more configurations; and (2) the usability was not  conclusive at the moment, at least not until they adopt the tool for  their courses. However, both instructors expressed the current  semantic visual analytics was reasonably useful, and both  indicated extreme interests in using for their own classes in the  future.    6. DISCUSSIONS AND CONCLUSIONS  In this work, we designed and studied a semantic visual learning  analytics, EduAnalysis. It embeded intelligent concept indexing  support to assist teachers in analyzing exam semanitc composition  in detail. We evaluated the effectiveness of the indexing services,  the indexing effects on content and investigated instructors  experiences and perceive usefulness on the system.    We found that the proposed approach shed some lights in  extracting semantic information from paper-based exams. Such  findings unlock the opportunities to (1) make  persistant traces of  learning analytics in semantic level; (2) provide more  personalized feedback for students that is normally difficult to  achieve or afford in a traditional (large) classroom. In addition, we  found that EduAnalysis empowered teachers to configure exam  topical emphasis and the results of indexed concepts appeared to  maintain coherence within exam. It suggested that the proposed  ExamParser approach could potentially make it possible to assign  partical credits by concepts. We also discovered that the  ExamParser indexing effect was especially prevalent for complex  content. The results complemented the cases when teachers could  not afford a lot of class time, but were forced to discuss key  concepts on the tougher problems on a returned exam. Moreover,  we also found the automatic indexing method was consistent with  teacher experts in indexing both procedural and declarative types  of questions. Sujective evaluation revealed that dynamic concept  indexing provided teachers immediate feedback on producing  more balanced exams; teachers expressed strong interests in using  EduAnalysis for their own classes.   There were a few limitations under current study setup. For  instance, current exams selection was a sample of CS1 four exams  from our home university, which can be a biased sample. We  should consider a wider range of exams and questions, such as   textbook sample exams etc. There were a few evaluation  limitations; such as teacher experts Cohen Kappa only indicated  moderate agreement in our baseline group. As a result, the  automatic ExamParser could potentially easily outperform  experts. However, we argued that one of the reasons the inter- raters agreement was low could be due to the nature of indexing  challenges and the setup for experts to pick out concepts from a  long list of ontology. In addition, teachers were used to  identifying key concepts even though they were instructed to be as  comprehensive as they could when indexing. Given that the  ground truth was not perfectly satisfying, we did not measure  indexing error rate at this moment.    In the near future, we need to address the teachers concerns and  to improve current design and evaluation. We plan to conduct  field studies to collect larger scale of actual classroom usages and  evaluate the semantic learning analytics impacts on students  learning. In the mean time, we need to establish a stronger ground  truth for future evaluation validation. In the long run, we would  like to implement and examine the mechanism on assigning  partial credits based semantics, experiment related technology to  facilitate auto-grading, investigate different visualization impacts  and finally, nevertheless, integrate other learning activities for  more comprehensive analysis. More exhaustive evaluation is  required.    7. REFERENCES  [1] Dillenbourg, P., Design for classroom orchestration.   Computers & Education, 2013. 69: p. 485-492.   [2] Roschelle, J., Y. Dimitriadis, and U. Hoppe, Classroom  orchestration: synthesis. Computers & Education, 2013. 69:  p. 523-526.   [3] Sharples, M., Shared orchestration within and beyond the  classroom. Computers & Education, 2013. 69: p. 504-506.   [4] Ambrose, S. A., Bridges, M. W., DiPietro, M., Lovett, M. C.,  & Norman, M. K. (2010). How learning works: Seven  research-based principles for smart teaching. John Wiley &  Sons.   [5] Maldonado, R.M., Analysing, visualising and supporting  collaborative learning using interactive tabletops. 2014,  University of Sydney.   [6] Martinez-Maldonado, R., Dimitriadis, Y., Martinez-Mons,  A., Kay, J., & Yacef, K. (2013). Capturing and analyzing  verbal and physical collaborative learning interactions at an  enriched interactive tabletop. International Journal of  Computer-Supported Collaborative Learning, 8(4), 455-485.    [7] Slotta, J.D., M. Tissenbaum, and M. Lui. Orchestrating of  complex inquiry: three roles for learning analytics in a smart  classroom infrastructure. in Proceedings of the Third  International Conference on Learning Analytics and  Knowledge. 2013. ACM.   [8] Roschelle, J., Penuel, W. R., & Abrahamson, L., Classroom  response and communication systems: Research review and  theory., in In Annual Meeting of the American Educational  Research Association (AERA). 2004: San Diego, CA p. 1-8.   [9] Vatrapu, R., Teplovs, C., Fujita, N., & Bull, S. (2011,  February). Towards visual analytics for teachers' dynamic  diagnostic pedagogical decision-making. In Proceedings of  the 1st International Conference on Learning Analytics and  Knowledge (pp. 93-98). ACM.      [10] Dyckhoff, A.L., et al., Design and implementation of a  learning analytics toolkit for teachers. Journal of Educational  Technology & Society, 2012. 15(3): p. 58-76.   [11] Heffernan, N. and C. Heffernan, The ASSISTments  Ecosystem: Building a Platform that Brings Scientists and  Teachers Together for Minimally Invasive Research on  Human Learning and Teaching. International Journal of  Artificial Intelligence in Education, 2014. 24(4): p. 470-497.   [12] De Liddo, A., Shum, S. B., Quinto, I., Bachler, M., &  Cannavacciuolo, L. (2011, February). Discourse-centric  learning analytics. In Proceedings of the 1st International  Conference on Learning Analytics and Knowledge (pp. 23- 33). ACM.   [13] Verbert, K., Duval, E., Klerkx, J., Govaerts, S., & Santos, J.  L. (2013). Learning analytics dashboard applications.  American Behavioral Scientist, 0002764213479363.   [14] Demmans Epp, C. and S. Bull, Uncertainty Representation in  Visualizations of Learning Analytics for Learners: Current  Approaches and Opportunities. Learning Technologies, IEEE  Transactions on, 2015. PP(99): p. 1-1.   [15] Duval, E., Verbert, K., Klerkx, J., Wolpers, M., Pardo, A.,  Govaerts, S., Gillet, D., Ochoa, X. & Parra, D. (2015,  March). VISLA: visual aspects of learning analytics. In  Proceedings of the Fifth International Conference on  Learning Analytics And Knowledge (pp. 394-395). ACM.   [16] Awasthi, P. and I.-H. Hsiao. INSIGHT: a Semanitc Visual  Analytics for Porgramming Discussion Forums. in 1st  International workshop on VISual Approaches to Learning  Analytics, in conjunction with 5th international learning  analytics and knowledge conference (LAK15). 2015. Marist  College, Poughkeepsie, NY, USA.   [17] Bull, S. Supporting learning with open learner models. in 4th  Hellenic Conference on Information and Communication  Technologies in Education. 2004. Athens, Greece.   [18] Mitrovic, A. and B. Martin, Evaluating the Effect of Open  Student Models on Self-Assessment. International Journal of  Artificial Intelligence in Education, 2007. 17(2): p. 121-144.   [19] Zapata-Rivera, J.-D. and J.E. Greer. Inspecting and  Visualizing Distributed Bayesian Student Models. in  Intelligent Tutoring Systems. 2000.   [20] Bull, S. and M. Britland. Group Interaction Prompted by a  Simple Assessed Open Learner Model that can be Optionally  Released to Peers. in Workshop on Personalization in E- learning Environments at Individual and Group Level at the  11th International Conference on User Modeling, UM 2007.  2007. Corfu, Greece.   [21] Dimitrova, V., J. Self, and P. Brna. Applying interactive  open learner models to learning technical terminology. in 8th  International Conference on User Modeling, UM 2001. 2001.  Berlin: Springer-Verlag.   [22] Hsiao, I. H., Bakalov, F., Brusilovsky, P., & Knig-Ries, B.  (2013). Progressor: social navigation support through open  social student modeling. New Review of Hypermedia and  Multimedia, 19(2), 112-131.   [23] Chen, Z. H., Chou, C. Y., Deng, Y. C., & Chan, T. W.  (2007). Active open learner models as animal companions:  Motivating children to learn through interacting with My-Pet  and Our-Pet. International Journal of Artificial Intelligence in  Education, 17(2), 145-167.   [24] Mazza, R. and V. Dimitrova, CourseVis: A graphical student  monitoring tool for supporting instructors in web-based  distance courses. International Journal of Human-Computer  Studies, 2007. 65(2): p. 125-139.   [25] Hsiao, I.-H. and P. Awasthi. Topic Facet Modeling: Visual  Analytics for Online Discussion Forums. in The 5th  international Learning Analytics & Knowledge Conference.  2015. Marist College, Poughkeepsie, NY, USA.        "}
{"index":{"_id":"8"}}
{"datatype":"inproceedings","key":"Beheshitha:2016:RAG:2883851.2883904","author":"Beheshitha, Sanam Shirazi and Hatala, Marek and Gavsevi'c, Dragan and Joksimovi'c, Sre'cko","title":"The Role of Achievement Goal Orientations when Studying Effect of Learning Analytics Visualizations","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"54--63","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883904","doi":"10.1145/2883851.2883904","acmid":"2883904","publisher":"ACM","address":"New York, NY, USA","keywords":"achievement goal orientation, dashboards, learning analytics, online discussions, visualizations","abstract":"When designing learning analytics tools for use by learners we have an opportunity to provide tools that consider a particular learner's situation and the learner herself. To afford actual impact on learning, such tools have to be informed by theories of education. Particularly, educational research shows that individual differences play a significant role in explaining students' learning process. However, limited empirical research in learning analytics has investigated the role of theoretical constructs, such as motivational factors, that are underlying the observed differences between individuals. In this work, we conducted a field experiment to examine the effect of three designed learning analytics visualizations on students' participation in online discussions in authentic course settings. Using hierarchical linear mixed models, our results revealed that effects of visualizations on the quantity and quality of messages posted by students with differences in achievement goal orientations could either be positive or negative. Our findings highlight the methodological importance of considering individual differences and pose important implications for future design and research of learning analytics visualizations.","pdf":"The Role of Achievement Goal Orientations When   Studying Effect of Learning Analytics Visualizations    Sanam Shirazi Beheshitha, Marek Hatala  School of Interactive Arts and Technology   Simon Fraser University  Surrey, Canada   sshirazi,mhatala@sfu.ca    Dragan Gaevi, Sreko Joksimovi  Schools of Education and Informatics   University of Edinburgh   Edinburgh, UK   dragan.gasevic,s.joksimovic@ed.ac.uk     ABSTRACT  When designing learning analytics tools for use by learners we  have an opportunity to provide tools that consider a particular  learners situation and the learner herself. To afford actual impact  on learning, such tools have to be informed by theories of educa- tion. Particularly, educational research shows that individual dif- ferences play a significant role in explaining students learning  process. However, limited empirical research in learning analytics  has investigated the role of theoretical constructs, such as motiva- tional factors, that are underlying the observed differences be- tween individuals. In this work, we conducted a field experiment  to examine the effect of three designed learning analytics visuali- zations on students participation in online discussions in authen- tic course settings. Using hierarchical linear mixed models, our  results revealed that effects of visualizations on the quantity and  quality of messages posted by students with differences in  achievement goal orientations could either be positive or negative.  Our findings highlight the methodological importance of consid- ering individual differences and pose important implications for  future design and research of learning analytics visualizations.    Categories and Subject Descriptors  K.3.1[Computers and Education] Distance Learning   General Terms  Human Factors, Measurement.   Keywords  Learning Analytics, Visualizations, Dashboards, Achievement  Goal Orientation, Online Discussions   1. INTRODUCTION  Recent advancement in technology-enhanced learning offers a  powerful and yet challenging opportunity to observe learning  analytics from students perspective. Learning analytics tools,  when put in the hands of students, can support their learning, par- ticularly at higher education [23]. One way of presenting learning  analytics to students is through visualizations and dashboards  [35]. With the intent to offer opportunities for awareness, reflec-  tion, sense-making and impact on students learning [35] existing  learning analytics visualizations and dashboards  present trace  data to students about their interaction with the learning environ- ment such as use of resources [17, 24, 30], time spent on activities  [2, 22], generated artifacts [2], or social interactions with others  [7, 21, 27]. While some of the existing dashboards are targeted at  providing general information that can facilitate awareness and  monitoring of learning activities , others go further and directly  guide students to take actions to control their learning [5][8].   In terms of evaluation, a line of existing research have focused on  usability and students perceived usefulness [17, 30]. Some other  research studies have also been conducted that indicate positive  influence of learning analytics dashboards and visualizations on  improving engagement [28], academic performance [2], test re- sults and assessments [4, 22], and retention rates [2] of the overall  population of students. A large number of studies that focus on  assessing leaning impact have been carried out in limited lab set- tings [4, 22, 28]. There are also few studies that have been inves- tigated in course settings at large sale, such as [2].   As research on learning analytics dashboards and visualizations is  expanding, further empirical research is needed to understand the  varying impact of information selected to be presented through  visualizations on different aspects of an individual students learn- ing process and outcome. Research on educational psychology  shows that individuals differ in their readiness to profit from a  particular treatment in a particular context [34]. This indicates the  possible varying effect of a treatment for individual students. In  our study we focus on theoretical constructs of so called aptitudes  that can shed light up on the observed differences between indi- viduals in learning context (e.g., motivational constructs, epistem- ic beliefs, approaches to learning, and attitudes) [36]. Our aim is  to investigate the effect of learning analytics visualizations learn- ing behavior by taking into account individual differences.   In so doing, we conducted a field experiment to examine the ef- fects of different information presented through learning analytics  visualizations on students learning behavior while controlling for  their individual differences. In this work we focused on a motiva- tional construct called achievement goal orientation [12].    Achievement goal orientation (AGO) is a well-established moti- vational construct describing  the purpose of engagement in an  achievement behavior  [12]. In the early definitions, two main  goal orientations were identified: i) mastery goal, which was con- ceptualized in terms of development of task competence; and ii)  performance goal, which was conceived as the illustration of per- formance competence [26]. In terms of valence, these achieve- ment goals were further distinguished by approaching success and  avoiding failure, e.g., being able to accomplish a task or avoiding  failing the test, respectively [11].  In recent AGO models, compe-  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for com- ponents of this work owned by others than ACM must be honored. Ab- stracting with credit is permitted. To copy otherwise, or republish, to post  on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883904     tence has been re-identified as the standard used in evaluation of  how well one is doing [13]. Task-based goals use absolute stand- ards and define competence based on doing well or poorly relative  to the requirements of the task. Self-based and other-based goals  adopt intrapersonal and interpersonal standards, respectively, and  define competence in terms of doing well or poorly with respect  to how one has done before or can potentially do in the future, or  in comparison to others [13].   To discover possible association between individual differences  and information presented we designed three learning analytics  visualizations where each showed particular information about an  aspect of students participation in online discussions in a univer- sity-level blended course. The visualizations were selected in a  way to potentially address students with particular goal orienta- tions. We chose to focus on asynchronous online discussions, as  these are commonly exploited to support collaborative learning  [25] and can be seen as an environment in which students can  interact to build both collective and individual understanding  through conversation with their peers [20]. Critically, the level  and quality of students participation is largely influenced by stu- dents agency [37]. Learning analytics in the form of reports and  visualizations have been suggested to be supportive of participa- tion and productive engagement in online discussions for the pop- ulation of students as a whole [41]. However, more attention to- wards the impact of what is presented to students with differences  in achievement goals is warranted [40]. Our results not only sub- stantiate this assumption, they also have significant implication  for broader learning analytics research.   2. METHOD  2.1 Study Design and Research Questions  To study the effects of different information presented through  visualization on the posting behavior of students with individual  differences, we conducted a field experiment in an authentic  blended course setting. Students participated in an online group  discussion activity on a topic related to the course content. Each  student was randomly assigned to an experimental condition in  which they had access to one of the three visualizations informing  them on how they are performing in the group discussion activity.  Students goal orientations were measured through a self-reported  instrument.   We defined our research questions as follows:   RQ1: Is there an association between visualization type and the  quantity of students posts when controlled for their self-reported  achievement goal orientations   RQ2: Is there an association between visualization type and the  quality of students posts when controlled for their self-reported  achievement goal orientations    2.2 Learning Analytics Visualizations  The choice of learning analytics visualizations was guided by the  main goal of this study, i.e., to establish the association between  type of information visualized and its effect on students behavior.  Secondly, we expected that the effect the visualizations will vary  with goals students are pursuing. The three visualizations selected  aimed to potentially align with different types of motivations un- derlying students goals. Each visualization also considered which  norm students will be evaluating themselves against, which varies  for different goal orientations. When cumulative performance is  shown, we used class average values (up to 200 students), rather  than the group average of 4-11 students. The reason is that our  LMS system students always see all contributions in a single   view, and hence, they could judge other group members perfor- mance directly.   The Class Average visualization allows students to compare their  posting performance with the average number of messages posted  by the rest of the class (Figure 1). Comparison of the students  with the class average has been the most widely used approach  when offering learning analytics dashboards and visualiza- tions [6]. Students who have a stronger inclination towards per- formance orientation may find this visualization beneficial, with a  caveat that its effect on students participation and learning was  not always positive [6, 41]. We included this visualization mainly  because of its prevalence in deployed systems.    The Top Contributors visualization shows the count of posted  messages by the student in comparison to the top contributors in  the class. Top contributors are the top 5+ individuals in the class  who have had the highest number of messages posted (Figure 2).  Not only students are able to see performance of the top contribu- tors, but the visualization increases their individual recognition by  showing their names and profile pictures. The norm shown by this  visualization is that of the best performing students and we expect  that it will positively motivate students with other-approach ten- dency and somewhat those with self-approach tendency, while it     Figure 1: The design of the Class Average visualization     Figure 2: The design of Top Contributors visualization     Figure 3: The design of the Quality visualization     will be disturbing to students with avoidance valence for the same  goals.    While both prior visualizations focused on count of messages, the  Quality visualization focuses the content of posted messages. It  represents how many of the key concepts student has covered  within his/her posted messages and how well he/she has integrat- ed those with logically related ideas. Students can compare quality  of their message with that of the rest of the class, hence we expect  that it will have positive effect on students pursuing mastery, both  those with task-approach or self-approach tendency. The key con- cepts for each discussion topic were previously identified by the  course instructor. The visualization (Figure 3) showed the quality  for each key concept as a color-coded square. The color was de- termined by computing the Latent Semantic Analysis (LSA), a  natural language processing technique for measuring the coher- ence of the text1, at the sentence level [14].   More complex dashboards with several metrics may address dif- ferent achievement goal orientations at the same time. For the  purpose of this study our selected visualizations included only one  metric of student performance and the same metric was shown at  the class level. We were explicitly not concerned with providing a  comprehensive overview of students performance in a single  cumulative view.   2.3 Online Group Discussion Activity  Design and facilitation of discussions in all participating courses  followed guidelines suggested in collaborative learning literature  [29, 43]. The students were split into several groups of 4-11  members and were asked to participate over a period of 7-14 days.  All of the groups in a particular discussion were given the same  open-ended question related to the course content and were ex- pected to engage in the discussion by exploring different aspects  of the question itself, proposing different ideas to address them,  selecting some ideas and finally deciding on one answer as a  group and justifying it with a clear rationale. Engagement in the  discussion was mandatory and was considered a graded compo- nent of the course (5% of final grade per discussion task).  Mark- ing rubric was also provided which thoroughly explained the  marking criteria in terms of quantity and content of individual  posts, as well as, tone and mechanics, collaboration between  group members, and quality of arguments in the final response.   Each group had access to their private discussion space inside the  Canvas Learning Management System used in the course. This  space was composed of the discussion activity description, link to  visualization, and of the discussion thread itself (can be viewed at  http://at.sfu.ca/gCXQNW (permalink)). Once students clicked on  the visualization link, a new tab would open up and display the  assigned visualization to the student.    2.4 Courses, Discussions and Participants  The study was run in the Spring and Summer 2015 terms across  four different blended course offerings at the second and third  levels in a multidisciplinary Design, Media Arts and Technology  program in a Canadian post-secondary institution. Table 1 shows  the number of students (i.e., study participants) assigned to each  visualization per discussion (D1- D6) across courses (C1SP,  C1SM, C2SP and C3SP).                                                                        1 Coherence has been described as the unifying element of good writing   and hence it can be used in a way to measure quality of text.  (http://www.elc.polyu.edu.hk/elsc/material/Writing/coherenc.htm)   2.5 Data Collection a Measurement  The time stamped log data of students interaction with the visual- ization was recorded. Also, the messages posted by each student  and the group structures were captured within the Learning Man- agement System. The count of posted messages and count of vis- ualization views was computed for each student per discussion  across different courses.    The 32 AGQ instrument was used to investigate students  Achievement Goal Orientations [13]. The instrument consists of  18 items, grouped into 6 scales corresponding to achievement  goals (task-approach, task-avoidance, self-approach, self- avoidance, other-approach, and other-avoidance, whereby self and  task represent mastery goals and other represents performance  goals). The responses were recorded on a Likert-type scale, from  1 (not at all true of me) to 7 (very true of me). The total scores on  every 3 items corresponding to a scale were used as the overall  measure on that AGO scale.    2.6 Data Analysis  2.6.1 Coh-Metrix Analyses  Discourse analysis can be used to help identify effectiveness of  discussions and quality of argumentation in collaborative learning  environments [33]. We used Coh-Metrix, a computational linguis- tics facility that provides various measures of text characteristics  (e.g., text coherence, linguistic complexity, characteristics of  words and readability scores), to analyze content of the messages  posted by students [18]. We adopted the five latent components  that in a recent study on a corpus of around 37,520 texts explained  over 50% of the variability among texts [18]:    Narrativity: the degree to which the text is a narrative and con- veys a story. On the opposite end of the spectrum are exposito- ry texts.    Deep Cohesion: the degree to which the ideas in the text are  cohesively connected at a mental and conceptual level.    Referential Cohesion: reflects the degree to which explicit  words and ideas in the text overlap with each other.    Syntactic Simplicity: reflects the degree to which sentences  have lower number of words and use more simple and familiar  structures rather than dense sentences and high frequency of  embedded phrases.    Word Concreteness: the degree to which the text includes  words that are concrete and induce mental images in contrast to  abstract words.   In this study, the above discourse features were analyzed at the  level of each message using Co-Metrix. Then for each single stu- dent we computed the average measures of all the messages per  discussion. Only the messages that included at least one of the key  concepts related to discussion topic (as identified by the instruc- tor) were included in this analysis. These are the messages gauged  to have traces of higher level of knowledge construction [19].   Table 1. Number of students assigned to each visualization   Condition C1SP C1SM C2SP C3SP    (Visualization) D1 D2 D3 D4 D5 D6 All   Class Average 25 11 8 N/A 7 7 58   Top Contributors 23 12 7 5 4 4 55   Quality 13 17 5 11 5 5 56        2.6.2 Statistical Analysis  Due to the nested structure of the data and the crossed variables in  our analysis we identified hierarchical linear mixed models to be a  suitable method [31]. The primary analyses for RQ1 focused on  association between the visualization type for those students who  used them to monitor their participation in discussion and the  quantity of posted messages, after controlling for self-reported  AGOs. Hence, we identified students counts of posts as the de- pendent variable.  The subsequent analysis was centered around RQ2 to find the  association between the visualization type and the quality of post- ed messages, measured through the discourse features, after con- trolling for the self-reported AGOs. Therefore, we identified five  dependent variables: Narrativity, Deep Cohesion, Referential  Cohesion, Syntactic Simplicity and Word Concreteness. The in- dependent variables in all models for both RQ1 and RQ2 were the  visualization type assigned to the student (i.e., Class Average, Top  Contributors, or Quality) and the covariates were the scores on six  AGO scales (i.e., task-approach, task-avoidance, self-approach,  self-avoidance, other-approach, and other-avoidance).  Six different linear mixed models were constructed, one for the  dependent variable in RQ1 (count of posts) and one for each of  the five dependent variables in RQ2 (Narrativity, Deep Cohesion,  Referential Cohesion, Syntactic Simplicity and Word Concrete- ness). The choice of the best fitting model for each dependent  variable was finalized after two steps of the model construction:  1) null model with student within a course as the only random  effect2 2) fixed model with the random effects introduced in the  null model and the interaction between visualization type and six  AGO scale scores as the fixed effect.    A comparison between the null random-effects only model and  fixed-effects model allows us to determine whether the model that  considers visualization types estimates quantity and quality of  posts when controlled for the self-reported AGO score better the  random effects model. Akaike Information Criterion (AIC) and  the likelihood ratio test were used to decide the best fitting model  [15]. Primarily, the model with lower AIC was suggested to have  a better fit. We used the likelihood ratio test to confirm AIC re- sult. We also calculated an estimate of effect size (R2) for each  model, which reveals the variance explained by the model [42].   3. RESULTS  Since the students use of learning analytics visualizations was  voluntary, not all chose to engage with the visualizations. The  subset of students who engaged with the visualization more than  once are considered the actual users of the visualization and the  focus of our analysis in RQ1 and RQ2 (Table 2).    Table 2. Count of visualization views for students who used  visualizations   Visualization N Median (25%,75%)  Class Average 38 7.00 (4.00, 9.00)  Top Contributors 22 6.50 (3.25, 15.50)  Quality 38 5.00 (3.00, 10.00)                                                                          2 In model construction, discussion groups were considered an additional   levels in the nested structure of the random effect. Also, the total  activity count of students was accounted as another random effect. In all  models our findings showed that considering either or both of these  variables did not yield a better model.      3.1 RQ1  According to the AIC and the likelihood ratio test the fixed model  that included the interaction between learning analytics visualiza- tion and AGO scales yielded a significantly better fit than the null  model (Table 3). The linear mixed-effect analysis uncovered a  significant interaction effect between the learning analytics visual- ization and other-approach scale scores, (F(2,79.11)=4.12,  p<0.05) (Further details in Table 4).    Further investigation on interaction effect between learning ana- lytics visualization and other-approach shows a marginal signifi- cant difference in the count of posts between the users of the Class  Average visualization and the users of the Top Contributor visual- ization (z=2.14, p<0.1) and significant difference between the  users of the Class Average visualization and the users of the Qual- ity visualization (z=2.79, p<0.05). The other-approach scale is  positively associated with counts of posts for the users of the Top  Contributors and Quality visualization, while the other-approach  scale is negatively associated with counts of posted messages for  the users of the Class Average visualizations.    3.2 RQ2  For all of the five Coh-Metrix principal components, fixed effect  models that included interaction between learning analytics visu- alization and the six AGO scales resulted with better overall   Table 3. Inferential Statistic for Model fit assessment - RQ1    2 Df R2 AIC  Null Model   0.70 251.21  Fixed Model 40.60** 20 0.91 250.61  2 values show the differences between the model in the current  row and the model in the previous row.  Significance codes: *** p<0.001 , ** p<0.01 , *p<0.05   Table 4. Analysis of the fixed effects for the model - RQ1   Variable  SE 95% CI  Lower Upper   Intercept (Class Average)** 0.478 0.174 0.130 0.826  Viz (Top Contributors) 0.156 0.274 -0.705 0.392  Viz (Quality)* -0.511 0.228 -0.967 -0.054  TaskAp 0.002 0.185 -0.369 0.373  TaskAv -0.067 0.346 -0.759 0.625  SelfAp 0.023 0.251 -0.480 0.525  SelfAv 0.402 0.510 -0.619 1.423  OtherAp*** -0.986 0.357 -1.700 -0.274  OtherAv* 0.707 0.480 -0.254 1.668        Viz (Top Contributors)*TaskAp -0.641 0.394 -1.428 0.146  Viz (Top Contributors)*TaskAv -0.151 0.565 -1.281 0.980  Viz (Top Contributors)* SelfAp. 1.076 0.628 -0.181 2.333  Viz (Top Contributors)* SelfAv -0.866 0.769 -2.404 0.671  Viz(Top Contributors)* OtherAp* 1.047 0.490 0.067 2.026  Viz (Top Contributors)* OtherAv     -0.724 0.694 -2.112 0.665   Viz (Quality)*TaskAp -0.180 0.222 -0.623 0.263  Viz (Quality)*TaskAv -0.016 0.391 -0.799 0.767  Viz (Quality)* SelfAp 0.024 0.370 -0.716 0.765  Viz (Quality)* SelfAv -0.206 0.589 -1.384 0.972  Viz (Quality)* OtherAp** 1.199 0.430 0.340 2.059  Viz (Quality)* OtherAv. -1.076 0.547 -2.169 0.018   Significance code: *** p<0.001 , ** p<0.01 , *p<0.05, . p<0.1  (marginal)   All variables are scaled     goodness of fit measures (AIC, likelihood ratio test and R2) than  null models (Table 5). As an example of analysis performed, Ta- ble 6 shows analysis of the fixed models for Deep Cohesion, simi- lar tables for remaining components can be viewed at  http://at.sfu.ca/shKRxa (permalink). In the context of online dis- cussions we believe Deep Cohesion should be given higher  weight given the importance of text cohesion for knowledge con- struction as emphasized by cognitive scientists [10].   3.3 Narrativity  The linear mixed model for narrativity further revealed significant  interaction effect between learning analytics visualization and  task-approach (F(2,81.52)=9.27, p<0.001), learning analytics  visualization and task-avoidance (F(2,81.02)=5.26, p<0.01),  learning analytics visualization and self-approach  (F(2,80.66)=3.64, p<0.05), and learning analytics visualization  and self-avoidance (F(2,81.36)=4.08, p<0.05). Also, the interac- tion between learning analytics visualization and other-avoidance  is marginally significant, F(2,80.62)=2.99, p<0.1.   Further investigation on interaction effect between learning ana- lytics visualization and task-approach shows a significant differ- ence between the scores of narrativity of the users of the Top Con- tributors visualization compared to the scores of narrativity of the  users of the Quality visualization (z=-3.22, p<0.01) and between  the scores of narrativity of the Class Average visualization and  those of the users of Top Contributors (z=4.31, p<0.001). The  positive association between the task-approach scale and narrativ- ity scores was largest for Top Contributors, followed by the posi- tive association for the users of the Quality visualization, while a  negative association was found for the users of the Class Average  visualization.   Probing the interaction effect between learning analytics visuali- zation and task-avoidance shows a significant difference in narra- tivity scores between the users of Top Contributors compared to  the users of Quality (z=-3.00, p<0.01). The effect of task- avoidance was negative on narrativity for the users of the Quality  visualization, while this effect was positive on the narrativity  scores of the users of the other two visualizations.   Further exploration on the interaction effect between learning  analytics visualization and self-approach exhibited a significant  difference in the scores of narrativity between the users of Class  Average visualization and those of the Quality visualization (z=- 2.32, p<0.05). Self-approach scale scores were positively associ- ated with narrativity scores for the users of the Class Average  visualization, whereas this self-avoidance scale scores were nega- tively associated with the narrativity scores for the messages post- ed by the users of Top Contributors and Quality visualization.   Finally, exploring the interaction effect between learning analytics  visualization and self-avoidance goal-orientation exhibits signifi- cant difference in the scores of narrativity between the users of the  Top Contributors and Quality visualizations (z=2.61, p<0.05).  Self-avoidance scale scores were positively associated with narra- tivity scores for the users of the Quality visualization, whereas  these self-avoidance scores were negatively associated with both  narrativity scores of the users of both Top Contributors and Class  Average visualizations.   3.4 Deep Cohesion  The deep cohesion model revealed significant interaction effects  between learning analytics visualization and task-approach  (F(2,82.38)=10.02, p<0.001), learning analytics visualization and  self-avoidance scales (F(2,82.28)=4.36, p<0.05), and learning   analytics visualization and other-avoidance (F(2,81.14)=3.65,  p<0.05). Also, the interaction between learning analytics visuali- zation and task-avoidance was marginally significant,  F(2,81.62)=2.94, p<0.1 (Further details in Table 6).    Further investigation on the interaction effect between learning  analytics visualization and task-approach shows a significant  difference in the deep cohesion scores between the users of the  Class Average visualization compared to the deep cohesions  scores of the users of the Top Contributors visualization (z=4.33,  p<0.001), and between deep cohesions scores of the users of the  Top Contributors and Quality visualizations (z=-3.99, p<0.001).  The positive association between task-approach scales and deep  cohesion was largest for the Top Contributors, while much small- er positive association was found for the Quality visualization  followed by the negative association for the users of the Class  Average visualization.    Further exploration on the interaction effect between learning  analytics visualization and self-avoidance exhibited a significant  difference in the scores of deep cohesion between the Class Aver- age visualization users and Quality visualization users (z=2.47,  p<0.05), and marginally significant difference between Top Con- tributor visualization users and Quality visualization users  (z=2.21, p<0.1). Self-avoidance scale scores were positively asso- ciated with deep cohesion scores for the users of the Quality visu- alization, whereas this self-avoidance scale scores were negatively  associated with the deep cohesion scores for the messages posted  by the Top Contributors and Class Average visualization users.   Further investigation on interaction effect between learning ana- lytics visualization and other-avoidance showed a significant  difference in the scores of deep cohesion between the users of  Class Average visualization with those of Quality visualization  (z=-2.58, p<0.05). The association between other-avoidance scale  scores and deep cohesion scores was negative for the users of the  Quality visualization, while the association was positive for the  users of both Top Contributors and Class Average visualizations.   3.5 Syntactic Simplicity  Analysis for syntactic simplicity principal component revealed  significant interaction effect between learning analytics visualiza- tion and self-avoidance (F(2,80.99)=3.46, p<0.05).  Further exploration on the interaction effect between learning  analytics visualization and self-avoidance exhibited a significant  difference in the scores of deep cohesion between the Top Con- tributors visualization users and the Quality visualization users  (z=2.56, p<0.05). Self-avoidance scale scores were positively  associated with syntactic simplicity scores for the users of the  Quality visualization, whereas this self-avoidance scale scores  were negatively associated with the syntactic simplicity for the  messages posted by the Top Contributors and Class Average visu- alization users.   3.6 Referential Cohesion  Analysis of mixed models for referential cohesion revealed a sig- nificant interaction effect between learning analytics visualization  and task-approach scales (F(2,78.05)=7.44, p<0.01), learning  analytics visualization and self-avoidance (F(2,75.33)=3.93,  p<0.05), and learning analytics visualization and other-approach  (F(2,73.33)=3.61,p<0.05).   Further investigation of the interaction effect between learning  analytics visualization and task-approach showed a significant  difference in the scores of referential cohesion between the users  of the Top Contributor visualization and the users of the Quality     visualization (z=-3.066, p<0.01), and between the Class Average  visualizations users and Top Contributor users (z=3.86 ,p<0.001).  The positive association between task-approach scales and refer- ential cohesion was largest for the Top Contributors, while much  smaller positive association was found for the Quality visualiza- tion followed by the negative association for the users of the Class  Average visualization.    Probing the interaction effect between learning analytics visuali- zation and self-avoidance shows a significant difference in refer- ential cohesion scores between the users of Top Contributors  compared to the users of Quality (z=2.77, p<0.05) and marginally  significant difference between the users of Class Average visuali- zation compared to the users of Top Contributors visualization  (z=-2.22, 0<0.1). Self-avoidance scale scores were positively as- sociated with referential scores for the users of the Quality visual- ization and Class Average, whereas this self-avoidance scale  scores were negatively associated with the referential cohesions  for the messages posted by the users of Top Contributors visuali- zation.   Further exploration on the interaction effect between learning  analytics visualization and task-approach exhibited a significant  difference in the scores of referential cohesion between the users  of Top Contributors visualization and those of Quality visualiza- tion (z=2.68, p<0.5). The other-approach scale scores were posi- tively associated with referential cohesion scores for the users of  the Quality visualization, whereas this self-avoidance scale scores  were negatively associated with the deep cohesion scores for the  messages posted by the Top Contributors and Class Average visu- alization users.   3.7 Word Concreteness  Further analysis of the models for word concreteness uncovered a  significant interaction between learning analytics visualization  and task-approach (F(2,80.24)=4.41, p<0.05), learning analytics  visualization and task-avoidance (F(2,80.17)=4.00, p<0.05),  learning analytics visualization and other-approach  (F(2,80.57)=3.68, p<0.05), and learning analytics visualization  and other-avoidance scales (F(2,80.06)=4.35, p<0.05).  Further investigation of the interaction effect between learning  analytics visualization and task-approach showed a significant  difference in the word concreteness scores between users of the  Top Contributor visualization and the Quality visualization (z=- 2.59, p<0.05), as well as, users of the Top Contributors and Class  Average visualization (z=2.90, p<001). The positive association  between the task-approach scale and word concreteness scores  was largest for Top Contributors, followed by the users of the  Quality visualization and the Class Average visualization.    Probing the interaction effect between learning analytics visuali- zation and task-avoidance showed a significant difference in the  word concreteness scores between the users of the Top Contribu- tors visualization and the Quality visualization (z=2.63, p<0.05).  Further analysis showed a positive effect on word concreteness  scores for the Quality visualization users, while this effect was  negative on the word concreteness scores for the users of the other  two visualizations.   Further investigation of the interaction effect between learning  analytics visualization and other-approach showed a significant  difference in the word concreteness scores between users of the  Class Average visualization and the users of the Top Contributors  visualization (z=-2.69, p<0.05). Further analysis showed a posi- tive effect on word concreteness scores for the users of the Class  Average and Quality visualization, while this effect was negative   on the word concreteness scores for the users of the Top Contribu- tors visualization.   Finally, the interaction effect between learning analytics visualiza- tion and other-avoidance showed a significant difference in the  word concreteness scores between the users of the Class Average   Table 5. Inferential Statistic for Model fit assessment - RQ2   Narrativity   2 df R2 AIC  Null Model   0.51 251.70  Fixed Model 74.42*** 20 0.68 217.28   Deep Cohesion   2 df R2 AIC  Null Model   0.36 246.40  Fixed Model 56.64*** 20 0.44 229.76   Syntactic Simplicity   2 df R2 AIC  Null Model   0.14 248.32  Fixed Model 44.10** 20 0.28 244.22   Referential Cohesion   2 df R2 AIC  Null Model   0.69 245.88  Fixed Model 57.47*** 20 0.77 228.42   Word Concreteness   2 df R2 AIC  Null Model   0.44 226.46  Fixed Model 52.99*** 20 0.68 213.47  2 values show the differences between the model in the current  row and the model in the previous row.  Significance codes: *** p<0.001 , ** p<0.01 , *p<0.05   Table 6. Analysis of the fixed effects for the model - RQ2   Deep Cohesion  Variable  SE 95% CI     Lower Upper  Intercept (Class Average) 0.252 0.179 -0.105 0.609  Viz (Top Contributors) 0.195 0.235 -0.276 0.665  Viz (Quality) -0.165 0.198 -0.561 0.231  TaskAp -0.019 0.160 0.339 0.301  TaskAv* 0.681 0.308 0.065 1.296  SelfAp 0.009 0.212 -0.416 0.433  SelfAv. -0.867 0.468 -1.803 0.070  OtherAp* -0.771 0.322 -1.414 -0.128  OtherAv* 1.107 0.446 0.214 1.999        Viz(TopContributors)*TaskAp*** 1.523 0.351 0.820 2.225  Viz (Top Contributors)*TaskAv -0.767 0.499 -1.764 0.231  Viz (Top Contributors)* SelfAp -0.740 0.690 -2.119 0.640  Viz (Top Contributors)* SelfAv -0.074 0.741 -1.554 1.406  Viz (Top Contributors)* OtherAp 0.162 0.433 -0.705 1.028  Viz (Top Contributors)* OtherAv   -0.604 0.608 -1.820 0.612   Viz (Quality)*TaskAp 0.029 0.258 -0.488 0.545  Viz (Quality)*TaskAv* -0.886 0.370 -1.626 -0.146  Viz (Quality)* SelfAp -0.167 0.349 -0.864 0.530  Viz (Quality)* SelfAv* 1.375 0.557 0.262 2.489  Viz (Quality)* OtherAp 0.508 0.401 -0.293 1.310  Viz (Quality)* OtherAv** -1.333 0.516 -2.365 -0.300  Significance code: *** p<0.001 , ** p<0.01 , *p<0.05, . p<0.1  (marginal)   All variables are scaled     visualization and the users of both Top Contributors (z=2.67,  p<0.05) and Quality visualizations (z=2.64, p<0.05). The associa- tion between other-avoidance scale scores and word concreteness  scores was negative for the users of the Class Average visualiza- tion, while the association was positive for the Top Contributors  and Quality visualizations.     4. DISCUSSION AND CONCLUSIONS  The overall goal of this study was to investigate the effect of dif- ferent information presented through learning analytics visualiza- tions on the posting behavior of students with different self- reported achievement goal orientations in online group discussion  activities.     4.1 Interpretation of the results  4.1.1 Different Visualizations and Students Quantity  of Posts Considering their AGOs  Our analysis showed that after controlling for achievement goals,  some learning analytics visualizations had positive and some had  negative effects on students quantity of posts.    For students who used Top Contributors and Quality visualiza- tion, higher scores on other-approach scale were significantly  associated with higher numbers of posts, whereas for those who  used Class Average, the association with count of posts was nega- tive.   The positive effect of Top Contributors visualization on the quan- tity of posts is in alignment with prior research showing that stu- dents with other-approach goals assess their competence level in  terms of normative standards and aim at outperforming their peers  [13]. In this case, the students who used the Top Contributors  visualization may have interpreted the norm based on the contri- bution level of those who had the highest number of postings in  the class. Another interpretation is that they may have strived to  gain visibility by the rest of the class, which means being listed as  top contributors themselves. Hence, this positive association be- tween the other-approach scale scores and numbers of posts for  users of this visualization is not surprising.  The Quality visualiza- tion may have motivated students with orientation towards other- approach goal to outperform the rest of the class in terms of the  depth and breadth of the key concepts covered in their messages.  In order to reach that goal, this visualization may have indirectly  encouraged them to contribute more.   For the Class Average visualization the students judgment of  how their peers were doing may have been influenced by the dis- played average performance of the entire class. Research shows  that students who adopt normative standards, through other- approach, usually rely on the instructors criteria, as they believe  this can best lead to outperforming their peers if no other visible  norm exists [32]. In light of this, real-time updates presented in  the visualization may push the instructors clearly expressed crite- ria behind the analytics metrics. If the class average is below  teachers expectation at any given time, students with other- approach tendency may follow that as their normative standards  for their goal.    Previous research shows that normative goal-standards can range  from modest to extreme [32]. It might be that learning analytics  visualization can be an influencing factor in determining the end  points of this range. The Top Contributors and Quality visualiza- tions encourages setting a higher standard to outperform in com- parison to the class average, which means it is more challenging  to achieve and requires more effort. This is in accordance with the  idea that if desirable participation behaviors are explicitly exposed   to students oriented towards performance goals (i.e. other-based  and self-based goals in 32 AGO model), it can encourage them  to engage more productively in the discussion activity [40].     4.1.2 Different Visualizations and Students Quality  of Posts Considering their AGOs  Our results showed that after controlling for achievement goals,  some learning analytics visualizations had positive and some had  negative effects on students quality of posts observed through  discourse features (i.e., Narrativity, Deep Cohesion, Referential  Cohesion, and Word Concreteness). For each achievement goal,  summary of significant associations are reported in Table 7.    In Table 7, positive associations show that higher scores on a  specific AGO scale are associated with higher scores on a specific  discourse component when using the visualization, whereas nega- tive associations indicate that higher scores on an AGO scale are  associated with lower scores on discourse features for a particular  visualization.  Table 7 uncovers non-homogenous findings across  different goal orientations and different visualizations.   Out of the five visible features presented in Table 7, the most  highlighted and frequent discourse component is deep cohesion.  For long, the importance of cohesion in text and oral communica- tion has been emphasized by cognitive scientists who aimed at  understanding how human mind constructs meaning from dis- course [10]. In fact, measuring cohesion was the main driver for  the development of Coh-Metrix which later expanded to other  discourse features. There are almost consistent findings in the  collaborative learning literature showing positive outcomes of  deep cohesion. Higher levels of deep cohesion show deeper inte- gration of the ideas with background knowledge and fever con- ceptual gaps, as well as, better individual and group performance  [9].    Our non-homogenous results across different visualizations show  that using a particular visualization followed a positive association  between a certain goal and a discourse feature, while another vis- ualization may have followed a negative association for the same  achievement goal and the same discourse component. For in- stance, those with higher tendency towards self-avoidance goals  constructed messages with higher deep cohesion when using the  Quality visualization but lower deep cohesion when using the  Class Average or Top Contributors visualizations. As discussed  previously, high deep cohesion is associated with positive out- comes and thus, it is highly desirable [9]. Students with avoidance  goals often suffer from the lack of task focus and hence, are more  likely to experience low deep cohesion. It seems that the Quality  visualization may have played a positive role in directing the stu- dents with high self-avoidance goals towards overcoming task  disrupting thoughts and integrating more cohesive messages,  while the other two visualizations may have played a negative  role. A possible explanation is that presentation of information in  the Quality visualization was more focused on improvements of  self over time (key concepts covered), which can increase feeling  of self-efficacy and self-confidence, and hence, improve the task  focus [32].    Similarly, our non-homogenous findings across different  achievement goals indicate that using a particular visualization  followed a positive association between one achievement goal and  a discourse feature, whereas the same visualization may have  followed negative association for the same discourse feature and  another achievement goal. For instance, despite positive outcomes  of Quality visualization for students oriented towards self- avoidance goals, the role this visualization played on construction     of deep cohesive messages appeared to be negative on the indi- viduals with higher tendency towards task-avoidance. It seems  that for students with task-avoidance strivings, seeing the con- cepts they have not covered increased their stress of doing poorly  in the discussions.   A highlighted aspect of the summary table is the presence of  negative valence goals. In the literature, avoidance goals  regard- less of the competence definition  have mostly been associated  with negative outcomes because of their tendency to avoid failure.  Low cognitive engagement, low self-efficacy, high anxiety and  feeling of shame, confusion, disorganized study habits, task- disrupting thoughts, help-avoidance, poor performance and inter- est are among destructive outcomes of mastery-avoidance (task- avoidance) and performance-avoidance goals (other-avoidance  and self-avoidance) [32]. Therefore, providing feedback to help  reduce some of the negative aspects of these avoidance goals is  desirable in addition to the prevision of the information shown in  the learning analytics visualizations.    The most visible achievement goal with positive valance in Ta- ble 7 is task-approach. Prior research shows that students with a  high task-approach tendency in a particular context compared to  others, find the topic interesting, have positive feelings about the  task and perceive it as valuable, use deep learning strategies and  appreciate both cooperativeness and help seeking [32]. Therefore,  it is not surprising that their deep approach to learning can help  them mentally connect ideas and construct messages that show  stronger signs of deep cohesion [1]. Our findings indicate all the  Quality and Top Contributors visualization had a positive effect  on deep cohesion when controlled for task-approach scores. This  finding is not surprising for the Quality visualization, as it directly  promotes coherent discussion of some key concepts and logical  integration with related ideas. As for Top Contributors, quality  may indirectly be promoted through externalization of high stand- ards on the contribution level. Therefore, it may encourage deeper  investigation into the topic of discussion.    4.2 Implications for Theory and Practice  The findings present some methodological, theoretical, and prac- tical implications. On a methodological side, the study shows the  importance of assessing learning analytics visualizations in au- thentic course settings to evaluate the actual effect of the present- ed information on students behaviors and outcomes. Combining  traditionally collected data through self-reported surveys, such as  individual achievement goal orientation, with fine grained data  such as interaction logs and generated artifacts. In this study, the  effect of different learning analytics visualizations on students  behavior was uncovered only when looking at the fine-grained  data and after controlling for students achievement goals, as mo- tivational constructs. In addition, analysis of discourse patterns  provided in-depth insight into the quality of students contribu- tions that complemented traditional metrics that rely on quantity  of contributions.   The study poses some important theoretical and practical implica- tions for the further research and use of learning dashboards and  tools by encouraging adoption of effective instructional practices  to support their use. From the instructional design point of view,  our findings show the potentials of learning analytics visualiza- tions as a feedback mechanism for students in online learning  environments. In our study, the instructional design of the discus- sion activity followed guidelines based on theories and practices  for effective and productive discussions. We are continuing to  investigate both the effect of pedagogical framing of learning  analytics visualization and the effect of connection of information   Table 7. Summary of Mixed Model Analysis for Interaction  between Learning Analytics Visualization and AGO Scale on   Quality of Posts   AGO Scale Visualization  Dependent Variable  Assoc.   Direction  Task- Approach   Class Average Narrativity -  Deep Cohesion -  Referential Cohesion -  Word Concreteness +   Top  Contributors   Narrativity +  Deep Cohesion +  Referential Cohesion +  Word Concreteness +   Quality Narrativity +  Deep Cohesion +  Referential Cohesion +  Word Concreteness +   Task  Avoidance   Class Average Narrativity +  Deep Cohesion +  Word Concreteness -   Top  Contributors   Narrativity +  Deep Cohesion +  Word Concreteness -   Quality Narrativity -  Deep Cohesion -  Word Concreteness +   Self- Approach   Class Average Narrativity +  Top  Contributors   Narrativity -   Quality Narrativity -  Self- Avoidance   Class Average Narrativity -  Deep Cohesion -  Syntactic Simplicity -  Referential Cohesion +   Top  Contributors   Narrativity -  Deep Cohesion -  Syntactic Simplicity -  Referential Cohesion -   Quality Narrativity +  Deep Cohesion +  Syntactic Simplicity +  Referential Cohesion +   Other- Approach   Class Average Referential Cohesion -  Word Concreteness +   Top  Contributors   Referential Cohesion -  Word Concreteness -   Quality Referential Cohesion +    Word Concreteness +  Other- Avoidance   Class Average Deep Cohesion +  Word Concreteness -   Top  Contributors   Deep Cohesion +  Word Concreteness +   Quality Deep Cohesion -  Word Concreteness +        presented to the learning activities on students learning out- comes. Also, our results confirm the findings of the limited re- search in this area that reveals learning analytics in the form of  dashboards or reports can lead to the change of activities in online  discussions that are sometimes intentional and goal-oriented and  sometimes unconscious [41].  Our research has implications for direction of empirical studies  around learning analytics visualizations and subsequently their  designs. The findings of our field study reveal that the effect of a  particular learning analytics visualization on students behavior  differs when students are inclined to different achievement goals.  This can motivate further empirical studies to investigate the con- nection between other theoretical constructs that underlie individ- ual differences and effectiveness of learning analytics dashboards.  Such studies can help move towards developing a body of  knowledge that could guide design and application of learning  analytics tools that are theoretically informed.   For instance, our results showed that the use of a particular learn- ing analytics visualization can be associated with positive changes  on students learning behavior with tendency towards a certain  goal, even for avoidance goals. We know that avoidance goals  have been mostly associated with negatives outcomes. Hence, our  findings encourages further examination of the role that personal- ized interventions can play in encouraging positive changes that  may lead to improved learning processes and outcomes. If the  feedback provided through these visualizations alleviates negative  outcomes associated with pursuit of avoidance goals, such as anx- iety and low self-efficacy, it may even have the potential to direct  students towards pursuit of approach goals which according to  research have been associated with more positive learning out- comes [32].    On the opposite side, our results show that each of the three visu- alizations can be negatively associated with learning behavior of  students with certain individual difference. For example, showing  the Top Contributors visualization to students with tendency to- wards self-avoidance was negatively associated with four dis- course features in their postings. This was discovered only after  carefully analyzing the interaction effect of visualizations with  goal orientations. Other examples can be extracted from Table 7.  Such insight can encourage adoption of more stringent require- ments for empirical evaluation of learning analytics visualizations  before deploying them to wide-scale use.   The choice of visualizations in this research was guided by their  ability to engage different goals individual students may pursue.  We did not tap into knowledge in information visualization field.  As our results show, both what is being presented and how, very  likely have different effect on individual students, with some vis- ualizations being more effective than others. A systematic study is  needed to understand the effect of different learning analytics  visualization designs by controlling for certain individual differ- ences, eventually leading to clear guidelines how to provide per- sonalized learning analytics [4].   4.3 Limitations and Future Work   The current work has several limitations that require further re- search to complement our results. First, our learning analytics  visualizations were integrated into the learning management sys- tem by providing a link that required additional effort and motiva- tion on students part to click and be directed to the visualization.  This may have affected how many students and how often viewed  the visualizations. Future work should explore other integration  options and their influence on the adoption and engagement with   the tool, while considering the platform and affordances it pro- vides.   Secondly, in this study, we considered achievement goal orienta- tion, a theoretical construct that could reveal individual differ- ences with respect to motivational factors in educational context.  However, other aptitude constructs that illuminate on students'  preferred approaches to learning [3] can also help understand how  particular students interact with learning analytics visualizations  and how those visualizations affect their learning behaviors. Addi- tionally, since we are dealing with visual information and writing,  further linking motivational disposition to other individuals traits,  such as attention and perception, processing and evaluation, and  in case of the discussion argumentative writing, is needed to build  fuller understanding how visualizations influence individuals. In  our current set of studies we are also probing for an individuals  numeracy, graph literacy and other related cognitive characteris- tics.   Our findings open several other directions for future research.  First, in learning analytics of discussion activities, the listening  behaviors, i.e. reading other students post, is critical for effective  discussion [39, 40]. As listening behavior constitutes over 75% of  discussion activities, understanding the effect of learning analytics  visualizations on listening behavior for would complement our  research. Next, instructional scaffolds can produce desirable ef- fects on development of critical thinking [16]. From this arises the  question how scaffolding, or the lack thereof, fosters the positive  association between engaging with the visualizations and posting  behaviors. Following a suggested framework [38], in a follow up  study, we will investigate the effect of reflection and goal setting  by embedding a space around the visualization for students to set  their goals and write a reflection journal and have it appear every  time they view the visualizations.   This work focused on learning analytics for discussions. Investi- gating the association between individual characteristics and dif- ferent ways of visualizing other learning activities is needed to  generalize our findings.    Acknowledgement. This research was supported by the Social  Sciences and Humanities Research Council of Canada.   5. REFERENCES  [1] Akyol, Z. and Garrison, D.R. 2011. Understanding cognitive   presence in an online and blended community of inquiry: As- sessing outcomes and processes for deep approaches to learn- ing. British J. of Educational Technology. 42, 2, 233  250.   [2] Arnold, K. and Pistilli, M. 2012. Course signals at Purdue:  using learning analytics to increase student success. The 2nd  Int. Conf. on Learning Analytics and Knowledge, 267  270.   [3] Biggs, J.B. 1987. Student Approaches to Learning and Study- ing. Research Monograph. ERIC.   [4] Brusilovsky, P., Hsiao, I.-H. and Folajimi, Y. 2011. Quiz- Map: open social student modeling and adaptive navigation  support with TreeMaps. Towards Ubiq. Learning. 7182.   [5] Bull, S. and Kay, J. 2008. Metacognition and open learner  models. The 3rd Workshop on Meta-Cognition and Self- Regulated Learning in Educational Technologies, at  ITS2008, 720.   [6] Corrin, L. and de Barba, P. 2014. Exploring students inter- pretation of feedback delivered through learning analytics  dashboards. Proc. of the ascilite 2014 conference, 201-205.   [7] Dawson, S., Bakharia, A. and Heathcote, E. 2010. SNAPP:  Realising the affordances of real-time SNA within networked     learning environments. Proc. of the 7th Int. Conf. on Net- worked Learning, 125133.   [8] Dimitrova, V. 2003. STyLE-OLM: Interactive open learner  modelling. Int. Journal of Artificial Intelligence in Education  (IJAIED). 13, 3578.   [9] Dowell, N.M., Cade, W.L., Tausczik, Y., Pennebaker, J. and  Graesser, A.C. 2014. What works: Creating adaptive and in- telligent systems for collaborative learning support. Intelli- gent Tutoring Systems, 124133.   [10] Dowell, N.M., Graesser, A.C. and Cai, Z. Language and  Discourse Analysis with Coh-Metrix: Applications from Ed- ucational Material to Learning Environments at Scale, Jour- nal of Learning Analytics, in press.   [11] Elliot, A.J. 1999. Approach and avoidance motivation and  achievement goals. Educ. Psychologist. 34, 3, 169-189.   [12] Elliot, A.J., Elliot, A.J. and Dweck, C.S. 2005. A conceptual  history of the achievement goal construct. Handbook of  Competence and Motivation. 16, 5272.   [13] Elliot, A.J., Murayama, K. and Pekrun, R. 2011. A 32  achievement goal model. Journal of Educational Psychology.  103, 3, 632.   [14] Foltz, P.W., Kintsch, W. and Landauer, T.K. 1998. The  measurement of textual coherence with latent semantic anal- ysis. Discourse Processes. 25, 2-3, 285307.   [15] Friedman, J., Hastie, T. and Tibshirani, R. 2001. The ele- ments of statistical learning. Springer series in statistics  Springer, Berlin.   [16] Gaevi, D., Adesope, O., Joksimovi, S. and Kovanovi, V.  2015. Externally-facilitated regulation scaffolding and role  assignment to develop cognitive presence in asynchronous  online discussions. The Internet and Higher Education. 24,  5365.   [17] Govaerts, S., Verbert, K., Duval, E. and Pardo, A. 2012. The  student activity meter for awareness and self-reflection.  CHI12 Extended Abstracts on Human Factors in Computing  Systems, 869884.   [18] Graesser, A.C., McNamara, D.S. and Kulikowich, J.M. 2011.  Coh-Metrix providing multilevel analyses of text characteris- tics. Educational Researcher. 40, 5, 223234.   [19] Joksimovic, S., Gasevic, D., Kovanovic, V., Adesope, O. and  Hatala, M. 2014. Psychological characteristics in cognitive  presence of communities of inquiry: A linguistic analysis of  online discussions. The Internet and Higher Education. 22,  110.   [20] Kanuka, H. and Anderson, T. 2007. Online social inter- change, discord, and knowledge construction. International  Journal of E-Learning & Distance Education. 13, 1, 5774.   [21] Kay, J., Maisonneuve, N., Yacef, K. and Reimann, P. 2006.  The big five and visualisations of team work activity. Proc.  of the 8th Int. Conf. on Intel. Tutoring Systems, 197206.   [22] Kerly, A., Ellis, R. and Bull, S. 2008. CALMsystem: a con- versational agent for learner modelling. Knowledge-Based  Systems. 21, 3, 238246.   [23] Kruse, A. and Pongsajapan, R. 2012. Student-centered learn- ing analytics. CNDLS Thought Papers, 19.   [24] Leony, D., Pardo, A., de la Fuente Valentn, L., de Castro,  D.S. and Kloos, C.D. 2012. GLASS: a learning analytics vis- ualization tool. Proc. of the 2nd Int. Conf. on Learning Ana- lytics and Knowledge, 162163.   [25] Luppicini, R. 2007. Review of computer mediated communi- cation research for education. Instructional Science. 35, 2,  141185.   [26] Maehr, M.L. 1989. Thoughts about motivation. Research on  motivation in education: Goals and cognitions. 3, 1, 299 315.   [27] Mazza, R. and Milani, C. 2004. Gismo: a graphical interac- tive student monitoring tool for course management systems.  Int. Conf. on Technology-Enhanced Learning, 18.   [28] Nakahara, J., Hisamatsu, S., Yaegashi, K. and Yamauchi, Y.  2005. iTree: Does the mobile phone encourage learners to be  more involved in collaborative learning Proc. of the Conf.  on Computer Support for Collaborative Learning: Learning  2005: the next 10 years! , 470478.   [29] Rovai, A.P. 2007. Facilitating online discussions effectively.  The Internet and Higher Education. 10, 1, 7788.   [30] Santos, J.L., Verbert, K., Govaerts, S. and Duval, E. 2013.  Addressing learner issues with StepUp!: an Evaluation. The  3rd Int. Conf. on Learning Analytics and Knowledge, 1422.   [31] Schielzeth, H. and Nakagawa, S. 2013. Nested by design:  model fitting and interpretation in a mixed model era. Meth- ods in Ecology and Evolution. 4, 1, 1424.   [32] Senko, C., Hulleman, C.S. and Harackiewicz, J.M. 2011.  Achievement goal theory at the crossroads: Old controver- sies, current challenges, and new directions. Educational  Psychologist. 46, 1, 2647.   [33] Shum, S.B. and Ferguson, R. 2012. Social learning analytics.  Journal of Educational Technology & Society. 15, 3, 326.   [34] Snow, R.E. 1991. Aptitude-treatment interaction as a frame- work for research on individual differences in psychotherapy.  J. of Consulting and Clinical Psychology. 59, 2, 205-216.   [35] Verbert, K., Duval, E., Klerkx, J., Govaerts, S. and Santos,  J.L. 2013. Learning analytics dashboard applications. Ameri- can Behavioral Scientist. 57, 10, 1500-1509.   [36] Winne, P.H. 2010. Improving measurements of self- regulated learning. Educ. Psychologist. 45, 4, 267276.   [37] Winne, P.H. and Hadwin, A.F. 1998. Studying as self- regulated learning. Metacognition in Educational Theory and  Practice. 93, 2730.   [38] Wise, A.F. 2014. Designing pedagogical interventions to  support student use of learning analytics. Proc. of the 4th Int.  Conf. on Learning Analytics and Knowledge, 203211.   [39] Wise, A.F., Hausknecht, S.N. and Zhao, Y. 2014. Attending  to others posts in asynchronous discussions: Learners  online listening and its relationship to speaking. Int. Jour- nal of Computer-Supported Collaborative Learning. 9, 2,  185209.   [40] Wise, A.F., Speer, J., Marbouti, F. and Hsiao, Y.-T. 2013.  Broadening the notion of participation in online discussions:  examining patterns in learners online listening behaviors.  Instructional Science. 41, 2, 323343.   [41] Wise, A., Zhao, Y. and Hausknecht, S. 2014. Learning ana- lytics for online discussions: Embedded and extracted ap- proaches. Journal of Learning Analytics. 1, 2, 4871.   [42] Xu, R. 2003. Measuring explained variation in linear mixed  effects models. Statistics in Medicine. 22, 22, 35273541.   [43] Yuan, J. and Kim, C. 2014. Guidelines for facilitating the  development of learning communities in online courses.  Journal of Computer-Assisted Learning. 30, 3, 220232.        "}
{"index":{"_id":"9"}}
{"datatype":"inproceedings","key":"Pijeira-Diaz:2016:ICL:2883851.2883897","author":"Pijeira-D'iaz, H'ector J. and Drachsler, Hendrik and Jarvela, Sanna and Kirschner, Paul A.","title":"Investigating Collaborative Learning Success with Physiological Coupling Indices Based on Electrodermal Activity","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"64--73","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883897","doi":"10.1145/2883851.2883897","acmid":"2883897","publisher":"ACM","address":"New York, NY, USA","keywords":"biosensors, collaborative learning, electrodermal activity, learning analytics, physiological coupling indices","abstract":"Collaborative learning is considered a critical 21st century skill. Much is known about its contribution to learning, but still investigating a process of collaboration remains a challenge. This paper approaches the investigation on collaborative learning from a psychophysiological perspective. An experiment was set up to explore whether biosensors can play a role in analysing collaborative learning. On the one hand, we identified five physiological coupling indices (PCIs) found in the literature: 1) Signal Matching (SM), 2) Instantaneous Derivative Matching (IDM), 3) Directional Agreement (DA), 4) Pearson's correlation coefficient (PCC) and the 5) Fisher's z-transform (FZT) of the PCC. On the other hand, three collaborative learning measurements were used: 1) collaborative will (CW), 2) collaborative learning product (CLP) and 3) dual learning gain (DLG). Regression analyses showed that out of the five PCIs, IDM related the most to CW and was the best predictor of the CLP. Meanwhile, DA predicted DLG the best. These results play a role in determining informative collaboration measures for designing a learning analytics, biofeedback dashboard.","pdf":"Investigating collaborative learning success  with physiological coupling indices  based on electrodermal activity  Hctor J. Pijeira-Daz University of Oulu  Oulu, Finland Hector.PijeiraDiaz@oulu.fi  Hendrik Drachsler Welten Institute  Heerlen, The Netherlands Hendrik.Drachsler@ou.nl  Sanna Jrvel University of Oulu  Oulu, Finland Sanna.Jarvela@oulu.fi  Paul A. Kirschner Welten Institute  Heerlen, The Netherlands Paul.Kirschner@ou.nl  ABSTRACT Collaborative learning is considered a critical 21st century skill. Much is known about its contribution to learning, but still investigating a process of collaboration remains a challenge. This paper approaches the investigation on col- laborative learning from a psychophysiological perspective. An experiment was set up to explore whether biosensors can play a role in analysing collaborative learning. On the one hand, we identified five physiological coupling indices (PCIs) found in the literature: 1) Signal Matching (SM), 2) Instan- taneous Derivative Matching (IDM), 3) Directional Agree- ment (DA), 4) Pearsons correlation coecient (PCC) and the 5) Fishers z-transform (FZT) of the PCC. On the other hand, three collaborative learning measurements were used: 1) collaborative will (CW), 2) collaborative learning product (CLP) and 3) dual learning gain (DLG). Regression analy- ses showed that out of the five PCIs, IDM related the most to CW and was the best predictor of the CLP. Meanwhile, DA predicted DLG the best. These results play a role in de- termining informative collaboration measures for designing a learning analytics, biofeedback dashboard.  CCS Concepts Information systems ! Data analytics; Data min- ing; Applied computing ! Collaborative learning; General and reference ! Empirical studies; Metrics;  Keywords learning analytics, biosensors, electrodermal activity, collab- orative learning, physiological coupling indices  1. INTRODUCTION Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16 April 25 - 29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883897  Collaboration is regarded as a critical 21st century skill [53]. Labor market demands team players as todays chal- lenges require professionals to work with each other. The earlier students start developing collaboration skills the sooner they will be able to incorporate them to everyday life and work. The interest in collaboration therefore concerns the students of today and workforce of tomorrow, teachers and researchers as the most direct stakeholders. Studies on col- laborative learning have shown that collaboration promotes important cognitive activities, such as question asking, elab- orating own understanding, argumentation of various points of views and conclusion making [34]. Research has also shown that collaborative learning is not self-evident and that true collaboration is infrequent [3]. Reaching high-level cog- nitive processing is a demanding task [29], and cognitive challenges tend to cause tension and increase the poten- tial for both socio-emotional conflicts and poor collabora- tive learning experiences [36, 2]. More is needed to know about these socio-emotional and cognitive interaction pro- cesses, and new methodological solutions may oer what the earlier have not achieved. The measurement of collaboration in general is a challenge as such, as there are not enough in- dicators available so far. Operationalizing collaboration is not a straightforward process and there is not a single way to do it [16].  This paper approaches the collaboration quantification through Learning Analytics (LA) [54] and physiological data from biosensors. LA is the measurement, collection, analysis and reporting of learning data. It leverages information visu- alization, learning sciences, software engineering, statistics and data mining methods for providing feedback and aware- ness of the learning process. The interest in LA concerns a broad variety of roles: researchers in education, leaders and policy-makers, educational practitioners, organisational ad- ministrators, instructional designers, product vendors, and [...] the learners themselves [7].  The LA field started to develop from the bulk of digi- tal traces that came with the Massive Open Online Courses (MOOCs). Together with the open access to high quality ed- ucational resources, the MOOCs user-transparent tracking capabilities started to produce learning datasets of unprece- dented dimensions. To tackle the necessity of turning the  http://dx.doi.org/10.1145/2883851.2883897   raw data into useful information and present it in friendly, relevant ways, the multidisciplinary field of LA emerged.  LA techniques are a powerful tool for sense-making of ed- ucational data [49]. This study on collaborative learning features applies LA to an increasingly available [51] data modality: physiological data. Activity from the cardiovascu- lar and electrodermal systems  just to name the two most popular physiological data sources in psychological studies [30]  has been linked to diverse cognitive, emotional and behavioural processes at both within- and between-subject research. We believe that tracking physiological data in learning situations can enrich the LA field, which has been mainly focused so far on digital traces.  Concurrently, collaborative learning research has relied on analysing transcribed verbal interactions and using self- reporting questionnaires [16]. There is a potential for com- plementing subjective reports with objective physiological data [11]. Wearable biosensors are gaining popularity in everyday life mostly for sports and fitness activity track- ing. Despite not being their primary purpose to date, these biosensors can also be used to study the cognitive and aec- tive domains of learning [46]. Physiological measures can be informative both at individual and group levels, each having a set of usages [11]. Therefore, we envision an application to monitor the learning process with wearables to provide timely feedback to the teacher and/or the learner. Before that promising future, extensive research is needed in three dimensions: 1) the physiological responses relevant for the learning process, 2) the significant features extractable from them, and 3) the learning-related psychological states they can describe. The field has been called Wearable Enhanced Learning1 (WELL) and the architecture has been referred to as sensor-based platforms [46].  There is a number of physiological responses. The multi- sensor wristband used in the experiment allowed for the recording of Blood Volume Pulse (BVP), electrodermal ac- tivity (EDA) and skin temperature. This study considers EDA. It is an easy measurable, sensitive response [12] that previous studies have linked to arousal [11, 38], attention [55], task engagement [40], and cognitive load [48], among others.  The notion of a relationship between the physiological re- sponses of two individuals interacting with each other has been long scientifically explored [31]. Little or no agree- ment on how to call this physiological relationship has been reached. The diverse nomenclature found includes (chrono- logically sorted): physiological linkage [31], physiological compliance [50], physiological synchronization [24], physio- logical correlation [15], joint changes in the physiological sig- nals [25], physiological coupling [11] and physiological mark- ers of togetherness [39]. From now on, this paper uses phys- iological coupling (PC) for the idea of influence it suggests. PC has been proposed for the assessment of social interac- tions and as a direct feedback [11].  To measure PC several indices have been used [35], six to our knowledge. [15] presented Signal Matching (SM), Instantaneous Derivative Matching (IDM) and Directional Agreement (DA). Pearsons correlation coecient (PCC) has also been employed [23] as well as a variation: its Fishers z-transform (FZT) [9] (i.e. the inverse hyperbolic tangent of the PCC). Weighted coherence (WC) was developed by [44]  1http://ea-tel.eu/special-interest-groups/well (Last ac- cessed: 20/10/2015)  for frequency domain analysis. This study obtained five of those PCIs  SM, IDM, DA,  PCC and FZT  from pairwise EDA time series to explore for the first time their ability to predict three collabora- tive learning features: 1) collaborative will, 2) collaborative learning product and 3) dual learning gain. The best pre- dictor for each was determined among the five PCIs.  Our near future goal is the design and development of a biofeedback dashboard for monitoring collaborative learn- ing with physiological data. Thus, apart from its value in the quantification of three collaboration features, this explo- ration is useful as a first step in determining those PCIs more informative for the dashboard and what they may signal.  The remainder of this paper is as follows. Section 2 moti- vates the physiological signals application to learning. Sec- tion 3 details the experimental design, data collection and processing. The results coming out of the analysis are dis- cussed in section 4. Finally, conclusions and agenda points for further research are stated in section 5.  2. RATIONALE Biosensors are in a process of democratization. General-  purpose, low-cost, biosensors are increasingly available and becoming part of everyday life [51]. Technology has made possible the integration of a variety of sensors in a sin- gle device thanks to the progress in nanotechnology, low- power electronics and biosensor design [21]. Final prod- ucts in this category include, but are not limited to, smart- watches, wristband sensors, wearable sensor patches, and smartclothes. The multi-sensor approach is a trend in the in- dustry and the combination of cardiovascular activity, tem- perature, motion and EDA sensors has been regarded as the next-generation multi-sensor standard [51].  The physiological responses that biosensors record are orig- inated in the nervous system, of which a simplified view is presented in figure 1. There are two big groups of physio- logical responses depending on whether they are controlled by the Central Nervous System (CNS) or the Autonomic Nervous System (ANS). The CNS controls the brain and the spinal cord. The ANS innervates the electrodermal, cardiovascular and respiratory systems. ANS responses of- fer a great potential for research and physiological comput- ing since their measurement mechanisms are cheaper, faster and more unobtrusive than those of the CNS responses [38]. However, their easy measurement contrasts with the chal- lenge that their intricate interpretation represents. Inter- pretation is inextricably linked to context as several stimuli can produce the same physiological responses [8].  Figure 1: Nervous system simplification.    This paper considers the electrodermal system responses during a collaborative learning task. EDA phenomenon and measurement were first proposed in 1888 by Fere, who dis- covered a change in skin electric conductance elicited by external stimuli [18]. The increase in skin conductivity is caused by a sweat increment. Sweat production is induced by thermoregulatory and psychological mechanisms as well [12]. Therefore, a constant room temperature of 23 C is rec- ommended [5] to measure EDA produced by psychological sweat, the one of interest in psychophysiology.  Psychophysiology is the discipline that studies the infer- ence of psychological states from physiological responses. The psychological states comprise cognitive, emotional, and behavioural phenomena [8]. The first systematic work in psychophysiology is considered to be the 1915 publication Bodily changes in pain, hunger, fear and rageby the Amer- ican physiologist Walter Bradford Cannon, a pioneer in the study of the physiology of emotions [6]. Science is since looking for an improved understanding of the physiological footprints that psychological mechanisms leave.  The psychophysiological process is depicted in figure 2. The human body produces physiological responses as a re- sult of its own functioning or upon external stimuli. The physiological responses can be recorded into physiological signals by means of biosensors. Dierent sampling rates (i.e. number of measurements per time unit) are used depend- ing on the response and the application (see table 4 for the ones used in the experiment). Examples of physiological sig- nals are EDA, electrocardiogram (ECG), electrooculogram (EOG), electromiogram (EMG) and electroencephalogram (EEG), among others. The same physiological response can be recorded as dierent physiological signals depending on the biosensor used. Thus for example, cardiovascular activ- ity can be recorded as ECG or as BVP signals depending on whether an electrocardiograph or photoplethysmograph sensor are used respectively.  Every signal in turn has a number of physiological fea- tures. The features can be descriptive statistics such as the mean and the standard deviation, or signal-specific such as the heart rate extracted from either the ECG or BVP sig- nals, or the Skin Conductance Level (SCL) and Skin Con- ductance Response (SCR) extracted from the EDA signal. Statistical features are clearly common to all the signals, while the number of signal-specific features varies from sig- nal to signal. Typically, studies use from a couple to less than a dozen features, but at least 128 features from 4 sig- nals have been used [19]. This provides a rough idea on how many features are extractable from physiological sig- nals. Physiological features can be extracted both at indi- vidual and group levels. The physiological features are the final numbers to be interpreted so as to determine the psy- chological state. Either the number or the interpretation could be provided as biofeedback in applications.  A scientific review of research on the physiology of emotion [30] reported that the most common physiological responses used in the field are heart rate and EDA in that order.  There is widespread documentation on EDA proportion- ality to arousal (psychological activation) [38]. Significant dierences in EDA have been observed between pleasant and unpleasant stimuli, that is, it can indicate valence [11]. EDA is highly sensitive to the novelty, intensity and perceived sig- nificance of stimulus [55]. An increase in cognitive load has also been reported to have a match in EDA rise. These are  Figure 2: Biofeedback process (adapted from [38]).  just examples of EDA expressiveness at the individual level. The computer science field has been attracted to psy-  chophysiology. Aective computing [41] is devoted to the variety of applications of machines able to recognize and properly respond to human emotions. It is based on the principle that no intelligent system is possible without emo- tions. Physiological computing [17] is a generalization of the aective computing field to include also cognitive com- puting, aimed at increasing user performance in contrast to the improved user satisfaction goal of aective computing. Although the very high complexity of psychophysiological inference and validation remains a hurdle, it has been sug- gested that even low accuracy psychophysiological inferences may be informative [11]. Potential applications would be ground-breaking and paradigm-changing [38], and learning might not be an exception.  Applications to the learning area include, but are not lim- ited to, interaction support [11], feedback [46], intelligent tutors [1], the current educational challenge of formative as- sessment [45], intervention [14], teacher training [22], emo- tion recognition in e-learning [47] and self-regulated learning [37]. A recent review on sensor-based platforms [46] did not identify prototypes purposely focused on learning applica- tions. Nevertheless, most prototypes were found to have capabilities relevant to at least one of the three learning do- mains [4]: cognitive, aective and psychomotor. Therefore, there is a need for and a lack of research on how can biosen- sors leverage the learning process [46, 47] at both individual and group scales.  PCIs and collaboration At group level the physiological features are the PCIs, dif- ferent indices to measure the relationship between the phys- iological signals of individuals within the group. Six PCIs have been used in research to our knowledge: SM, IDM, DA, PCC, FZT and WC. Signal Matching (SM) accounts for the pairwise dierence between the signals once they have been normalized. The need for normalization comes from the fact that physiological signal levels are strongly dependent on individual characteristics [12]. Instantaneous Derivative Matching (IDM) compares the rate of change of physiological signals from a dyad by means of the derivative. Directional Agreement (DA) represents the per cent of data points going in the same direction across individuals (i.e. going up, down or staying constant at the same time). It is the most basic PCI. The Pearsons correlation coecient    (PCC) determines the strength of the linear relationship be- tween the physiological signals from two individuals. The Fishers z-transform (FZT) is a transformation of PCC  by applying the inverse hyperbolic tangent  so as to obtain a normally distributed index. FZT is basically the same as PCC for values between -0.5 and 0.5. Weighted Coherence (WC) is a frequency domain PCI and therefore more useful for periodic physiological responses such as the heart beat and the respiration rate. Since EDA is the signal this study is focused on and it is not a periodical signal, WC has not been considered.  According to each PCI definition, the higher the values the higher or lower the PC. This direct or inverse relationship is summarised in table 1, together with the property that each PCI measures. By definition, PCIs are mostly computed pairwise, being DA the exception. DA can be calculated for as many individuals as desired. PCIs can be obtained on either an instantaneous or aggregated basis. Aggregated can be either for time windows, say for example 65s as in [15], or for the whole measurement session. They can all be aggregated but not all can be computed on an instantaneous basis. PCC and FZT are meaningless calculated for a single instant. Although in this study aggregated values for the whole collaborative learning task are used for the prediction of collaboration features, instantaneous values might have the power to detect particular points of interest in the study of collaboration. It has been claimed that understanding the underlying mechanisms of collaborative learning requires research to zoom in the collaborative interactions [13].  Table 1: Summary of PCIs meaning PCI Interpretation Value PC  SM dierence lower higher IDM rate of change lower higher DA direction higher higher PCC linear relationship higher higher FZT weighted mean PCC higher higher  PCIs can play a role in studying the social interactions [11] by providing an objective measure, although research in this direction remain scarce. A relatively small number of studies has focused on the PC phenomenon [25], as most of the research carried out in psychophysiology and applica- tions has targeted the individual rather than the group level [11]. Although insucient, psychophysiological research for collaboration has proved promising.  Evidence has been found for a connection between PC and team performance [35], with PCC and DA of cardiovascu- lar activity being the most sensitive indices to dierences between low and high performers [15]. PCC of heart rate in dyads has been reported to predict task completion time [23]. Also in dyads, PC has been associated to interaction and self-reported social presence [25, 10]. Conflicting in- teractions have been reflected in a significantly higher PC increment than that of collaborative interactions by means of the PCC index [10]. The directionality of the PC might point to who is the group leader [11]. Some of the collabora- tion features studied from the PC phenomenon perspective are listed in table 2 together with the PCIs employed.  Physiological signals are valuable social cues for the study of collaborative learning. Dierent collaboration features have been previously studied. This paper enriches the field  Table 2: Some collaboration features studied through PCIs Collaboration feature Study PCIs  Team performance [15, 35] SM, IDM, DA, PCC Interaction [25] PCC, WC Task completion time [23] PCC, WC Conflicting interactions [10] PCC Team work [22] PCC, WC  with three collaborative learning specific features that have not been approached through PCIs before.  3. METHOD Based on the exposed rationale, we post the research ques-  tions:  1. Which PCI reflects better the collaborative will  2. Which PCI predicts the collaborative learning product and the dual learning gain the best  At this point it is convenient to remember that the PCIs are calculated on a certain physiological response  EDA in this study. Therefore, it has to be kept in mind that here the answer to the research questions is linked to and valid only for the EDA signal. Further studies are needed for the comparison of PCIs across dierent physiological responses.  3.1 Setting An experiment was conducted in the University of Oulus  Learning and Interaction Observation Forum (LeaForum2). This state-of-the-art research infrastructure was designed as a roomy, cosy space for up to 30 people. It is a convertible facility with flexible fittings to allow provision for dierent group situations. LeaForum is equipped with:   a proprietary observation and recording system [27];  trapezoidal tables convenient for group work;  14 tablets connected to the Internet via Wi-Fi;  6 Empatica S3 biosensors [20];  2 eye tracking glasses; and  a backstage room prepared to follow the experiments  in real time through the live video signals.  The video recording system is able to collect 360 video and high-quality audio from multiple microphones. The trapezoidal tables allow for a suitable and comfortable group setting, as they form an heptagon when seven of them are sided consecutively. In front and close to each teams ta- bles, there was a table supplied with a variety of food items (cardboard boxes, plastic bags, fruits...) relevant for the task.  The tablets enabled the students to take pictures, search information on the web, respond to online questionnaires and access weSPOT [33], the virtual learning environment (VLE) used. WeSPOT is a working environment with social and personal open tools for Inquiry Based Learning (IBL).  The Empatica S3 wristband is a multi-sensor device es- pecially designed for research purposes. Four sensors come embedded: 1) a photoplethysmograph (for cardiovascular  2http://leaforum.fi (Last accessed: 20/10/2015)    activity), 2) a thermometer, 3) an accelerometer, and 4) an EDA sensor. Two modes of operation are available: 1) real time Bluetooth streaming and 2) internal memory recording. The latter was used. Ambient temperature in LeaForum is usually controlled by means of a thermostat to be 23C, the recommended temperature to measure EDA [5].  3.2 Participants Participants in the experiment were 48 high-school stu-  dents from the University of Oulus Teacher Training School. Students came from six dierent classes and were aged 16 to 19. Gender distribution was 27 females and 21 males. No previous knowledge was required. The task was coordinated with their science teachers so that it was included within their course work, meaning it did not supposed extra eort for the students. Participation in the study was voluntary.  3.3 Task description The task design in close collaboration with the students  science teachers ensured that it was aligned with their cog- nitive level. It also made it meaningful for their current studies. The experiment was run four times. The task was the same but two levels of scripting were used: guided and unguided  two runs each.  The task consisted in the design of a healthy, appropriate breakfast for an athlete training for a marathon. The spec- ification of parameters included age, height, weight, daily caloric intake, number of weekly trainings and session length. Students were provided with two Google-DocsR files: a document and a spreadsheet. For both guided and un- guided runs, the document contained a general description of marathon runners nutritional needs. The spreadsheet had a template with rows for food items and columns for the weight (in grams) of the dierent nutrients contained by the food item. Several examples were provided.  The unguided task required students to search on the In- ternet which and how much of each nutrient a marathon runner needs for breakfast. They were asked to write their most relevant findings on the nutrients and their function.  In the guided runs of the experiment the provided doc- ument already contained the recommendation for the com- position of the athletes breakfast in per cent of each nu- trient, energy consumption of marathon runners as well as the description and organic role of nutrient categories such as carbohydrates, proteins, minerals and fat. Students were prompted the steps to follow through the VLE. The steps according to the IBL paradigm were 1) plan the design method, 2) set the criteria, 3) collect the data, 4) discuss the findings, and 5) communicate the results.  3.4 Procedure A pre-test and a Motivated Strategies for Learning Ques-  tionnaire (MSLQ) [43] were applied to the participants two weeks in advance to the laboratory experiment. The aims were on the one side to have a preliminary evaluation of students previous knowledge on the task subject, motiva- tion and attitudes towards learning; and on the other side to have a group formation criterion. The MSLQ question- naire is considered a social-cognitive view of motivation and self-regulation of learning. Students scores in each of the two tests were categorized in low, middle and high. These categories were then used as the basis to form the groups as heterogeneously as possible.  Students were organized in 16 triads for the collaborative task. Due to LeaForum capacity and the number of avail- able biosensors, the experiment was run four times in two consecutive days, a morning and afternoon run each day. The unguided task was used in the first days sessions. On the contrary, second days sessions were carried out with the guided task.  Every session lasted for two hours (on task) and fifteen minutes (preparation). At the beginning of each session, the students were introduced to LeaForum and its equip- ment, especially the sensors they were to wear throughout the task. Each student was provided with a tablet and a microphone for the videotaping. Two students in separate groups were wearing eye-tracking glasses each run. The six biosensors available at LeaForum allowed for the physiolog- ical tracking of two groups each session. After the sensor familiarization, the collaborative learning task together with the VLE were explained. Having completed the task under- standing phase, students started to work with their groups. During the whole experiment, two researchers from the team were around to handle possible issues with the task or the equipment. The rest of the research team was following the experiment from LeaForums backstage room, prepared for observation without causing disturbance or distraction to students.  Upon task completion, students delivered a report with their solution via the VLE. Finally, they were asked to take the post-test.  3.5 Collected data The number of sensors available formed a limitation for  tracking every student. In addition, as the task was collab- orative, not all the students interacted with the VLE (task distribution). From the capability to track the physiolog- ical responses of 24 students (6 biosensors  4 runs), the real number was reduced to 20 due to four cases of device misplacement. Those 20 were the students we finally con- sidered for this study from the experiment data focused on EDA. The total number of students in each data modality collected is specified in table 3.  Table 3: Students tracked in each data modality Data modality Students  Pre-, post- and MSLQ questionnaires 48/48 Videotaping 48/48 VLE activity logs 33/48 Wristband biosensors 20/48 Eye-tracking 8/48  At the group level, data from the biosensors is distributed as follows: 5 groups with all three members tracked, 2 groups with two out of the three members tracked and 1 group with a single member tracked. The latter cannot be used therefore to measure collaboration, but it can be compared to students working in other groups during the same session for contrasting the PCIs of people working together with those of people working simultaneously but in other teams.  The dimensions of the data appear in table 4. The sample frequency specifies the number of measurements the biosen- sors collect every second. Thus for example, EDA data for each participant adds up to around 28,800 samples (tak- ing four samples a second during approximately two hours).    Considering the 20 participants tracked with this data modal- ity, accounts for over half a million records.  Table 4: Physiological data dimensions Data Units Sample frequency (Hz)  Temperature C 4 EDA S 4 BVP nW 64 Accelerometer 1/64g 32 IBI s  Pupil diameter mm 64  BVP is the direct measure coming from the photoplethys- mograph sensor. BVP peaks indicate a heart beat. Using the BVP signal an Empatica proprietary algorithm extracts the interbeat interval (IBI) (i.e. the time in between two consecutive heart beats), from which in turn heart rate is calculated.  From the data collected, figure 3 is very illustrative of the power of EDA data. There, the EDA signals have been syn- chronized first among themselves and then with the video. The figure shows shadowed an interval where there is an almost simultaneous peak in the three EDA responses. A video inspection reveals that the peak occurs at the time the team feels that something has gone wrong with the VLE, that is, a technical challenge. It is also seen in the image that the student in the middle has taken his hands to his head, signaling thereby that he is worried. The figure shows also the individual dierences in the level of the EDA sig- nal. As it has been stated before, EDA is strongly dependent on individual characteristics, and so are other physiological responses.  Figure 3: Challenge-driven team EDA peaks.  3.6 Data processing Data processing steps are summarized in figure 4. After the collection the data had a high degree of sparsity.  Dierent sources usually imply dierent formats. Even raw  Physiological data Pre-processing  Individual syncingArtifact detection  Normalization Group syncing  PCI calculationCorrelation  Figure 4: Physiological data processing.  data coming from the same source may be separated. This is the case of the biosensors. The five magnitudes they provide (first five rows in table 4) are each saved to a dierent file. File format is typically comma separated values (CSV) with a first row indicating the start recording time, a second row displaying the sample frequency and the remaining rows for the measured values. First step is therefore to have the data in such a way that every value is time stamped for each magnitude and participant (pre-processing).  The synchronization of multimodal data and even within the same data modality is a challenge [11, 28]. It is also a core process for the exploration of PCIs as they rely on responses happening at the very same instant. Otherwise the results would not be reliable. Mostly manual synchro- nization was performed in the data processing. Automated mechanisms are under study for the goal of a timely, learning- oriented, biofeedback dashboard.  We chose to begin the physiological signals synchroniza- tion within individuals with a magnitude sampled at 4Hz, which is the maximum common divisor of the sampling fre- quencies in table 4. Thus, starting with temperature, the physiological signals were synchronized using Microsoft Ex- cel VLOOKUP function one participant and signal at a time (individual synchronization).  Biosensor recordings may include artifacts as a result of a variation in the contact between the skin and the sen- sor due to pressure, excessive movement, or adjustment of the device [52]. There is a need to detect those artifacts to prevent wrong data from distorting the analysis. Three free applications were found for the processing of EDA data: Ledalab [26], EDA Explorer [52] and PsPM3. Ledalab allows for the manual detection of artifacts. PsPM does not in- clude this feature either in manual or automatic way. EDA Explorer automatically detects artifacts based on 5-second epoch classification. EDA Explorer was therefore used to categorize EDA data points into signal or noise (artifact de- tection). As a result, 90% of the EDA data (490,880 from 547,848 records) was classified as correct and the remaining 10% was taken out of the analysis.  The strong dependence of EDA values on individual char- acteristics makes a normalization process necessary for com- parability  at least for the level-dependent PCIs. EDA sig- nals are then brought to a baseline. Normalization was done by computing the t-scores (i.e. subtracting the sample mean from every value and then dividing by the sample standard deviation). Both sample mean and standard deviation were computed on an individual basis.  3http://pspm.sourceforge.net (Last accessed: 20/10/2015)    To explore the collaboration through physiological responses, a second synchronization has to be done to align the data across individuals. The group synchronization is carried out on a session basis, meaning for all the individuals tracked with biosensors within a session.  The indices SM, IDM and DA were computed according to [15]; PCC with lag 0 as in [23] and FZT following [9]. PCIs were obtained pairwise for all team member combina- tions (AB, AC and BC) to ensure a greater index reliability [22]. The instantaneous values were then aggregated as the arithmetic mean throughout the collaborative learning ses- sion. The mean over a certain time period is a commonly accepted physiological feature [38]. As a result of the whole process, five 21x21 matrices were obtained, one for each PCI.  3.7 Collaborative learning measures Three collaborative learning measures (CLMs) were used  in the study: 1) collaborative will (CW), 2) collaborative learning product (CLP) and 3) dual learning gain (DLG).  The predisposition to collaborate in a learning task, that is, the CW, was measured using the peer learning scale of the MSLQ questionnaire. MSLQ is a widely used self-report instrument with 81 items distributed in two sections  mo- tivation and learning strategies  with 6 and 9 scales re- spectively. Students rate ranges from 1 (not at all true of me) to 7 (very true of me). The peer learning scale com- prises three items  numbers 34, 45 and 50  in which the students rate their willingness to explain learning material to their team members, work together with others to com- plete a learning assignment and discuss task material with the team. Individual MSLQ scales are analysed by averag- ing the ratings of the dierent items the scale is comprised of. Therefore, the CW measured in this way ranges from 1 to 7, just as the individual items.  The CLP was measured by the score of the report deliv- ered by the students after the task. The report included two files: 1) a document with their task notes and conclusions, and 2) a spreadsheet with their task solution. The assess- ment criteria involved the diversity of the solution (number of nutrients considered), depth of the answer, accuracy and focus on the problem. The scoring scale ranges from 4 to 15 points.  The individual learning gain was computed by subtracting pre-test from post-tests scores. DLG was then calculated as the sum of the individual learning gains.  4. ANALYSIS AND RESULTS Figure 5 shows in a scatter chart with smooth lines the  pairwise PCI values and the CLMs sorted from left to right by descendent CLP. The figure illustrates nearly the same behaviour between FZT and PCC. Therefore, they may be considered redundant PCIs and it is simpler to use PCC instead of FZT, as FZT is a derivative measure of PCC and thus requires further calculation, which in turn translates into no added benefit.  With the five PCIs computed on the one side, and the three CLMs on the other side, fifteen regression analyses were performed pairwise (see figure 6). The aim was to explore the power of every PCI to predict each CLM and thereby being able to answer the research questions. The results are summarized in table 5.  In addition, a regression analysis was performed between pre- and post-test scores. High correlation (0.86) was found  between the two tests, with the pre-test score being able to predict 74% of the post-test scores variance. The regression line obtained is displayed in figure 7. The line has a positive slope (0.70), indicating that in general there was a positive learning gain for participants in the experiment.  Table 5: Correlations in the regression analyses CLM SM IDM DA PCC FZT  CW 0.01 0.50 0.12 0.09 0.09 CLP 0.13 0.59 0.12 0.32 0.35 DLG 0.27 0.20 0.70 0.17 0.17  In general, weak correlation (below 0.3) was found among the PCIs and the CLM. IDM showed a moderate to strong correlation with the CW (0.5) and the CLP (0.59). IDM was therefore the best predictor for those CLMs. DA revealed a strong correlation to the DLG (0.7), meaning it explains almost half of DLG variation (determined by the square of the correlation coecient). This might indicate that while the linearity (i.e. proportional changes) in the EDA of stu- dents learning collaboratively appears to have no predictive power, the direction in which the signals are changing could tell about the resulting learning gain.  Once again the simplest solution proved to be the better. DA is the simplest of the PCIs, yet it yielded the highest correlation in the regression analysis. This is in agreement with former studies finding DA as the most sensitive PCI to dierences in team performance [15]. It is also the most powerful at group level by enabling the computation of one index for all the members and not only pairwise, which is a common limitation of the other PCIs. In addition, DA does not require normalization thanks to its level independence, being just driven by the direction of change as the name itself implies. All of this means that DA can get to a dashboard quicker than its counterparts, making it the most suitable PCI for real time applications.  It is also worth stating that this study is not without its limitations. The sample size was bounded by the number of biosensors available. Also, the experiment was conducted in a laboratory setting, which diers from the students nat- ural learning environments. The EDA signal was recorded from a single wrist  as in the overwhelming majority of studies using EDA, but research has shown that EDA is an asymmetrical response (i.e. it manifests dierent in left and right hemispheres) [32, 42]. Therefore, recording EDA in the two wrists might oer additional information.  5. CONCLUSIONS AND FUTURE WORK A physiological approach to collaborative learning research  has been presented. This study used the EDA records from the rich multimodal dataset produced in the experiment. EDA, a measure of psychological sweat if temperature is kept constant, has been linked in former studies to cognitive and emotional processes. Here, five PCIs were calculated from the EDA of students working collaboratively to inves- tigate for the first time their possible connection to three collaborative learning features. The best predictor for each of the latter was obtained from the PCIs: IDM for the CW and the CLP; and DA for the DLG.  Further studies on the applications of EDA to the learn- ing sciences should consider the laterality eect. Lateral measures need to be compared and correlated to learning    Figure 5: PCI and learning measures.  Figure 6: Pairwise regression analyses.  Figure 7: Post- vs. pre-test regression line.  outcome indicators. It may result that measurements from one side are more significant or sensitive in a learning con- text. But it may also turn out that none of them could be safely ruled out.  EDA is valuable as a highly sensitive index of psychologi- cal activation, but other physiological responses also deserve to be explored. The PCIs predictive power is likely to vary across physiological responses, and it is of interest to find out which physiological response maximizes the PCIs corre- lation to dierent learning features.  Research is needed with focus on whether there is a PC among people in a collaborative setting. An experiment de- sign might be to track the physiological signals of a num- ber of individuals working alone. Then, the individuals are asked to work in groups. The physiological responses in the two situations can be compared to find changes. The con-  trast of PCI values alone and in group can tell if the PC phenomenon occurred.  As biosensors are becoming more and more common in everyday life, we believe that they can increase the acces- sibility to a data source with potential to enrich the LA field. They can be used as input for a learning-oriented biofeedback dashboard in a computer supported collabora- tive learning (CSCL) context. Such a dashboard is the next milestone in our research agenda.  6. ACKNOWLEDGEMENTS This study is funded by the Academy of Finland, SLAM  project number: 275440. The authors would like to acknowl- edge the research assistance of Eetu Haataja. In addition, the current experiment could not have been carried out with- out the collaboration of teachers and high school students from the University of Oulus Teacher Training School.  7. REFERENCES [1] I. Arroyo, D. G. Cooper, W. Burleson, B. P. Woolf,  K. Muldner, and R. Christopherson. Emotion sensors go to school. In Proceedings of the 2009 Conference on Artificial Intelligence in Education: Building Learning Systems that Care: from Knowledge Representation to Aective Modelling, pages 1724, Amsterdam, The Netherlands, The Netherlands, 2009. IOS Press. URL: http://dl.acm.org/citation.cfmid=1659450.1659458.  [2] M. Baker, S. Jarvela, and J. Andriessen. Aective Learning Together: social and emotional dimensions of collaborative learning. Routledge, 2013.  [3] B. Barron. When smart groups fail. Journal of the Learning Sciences, 12(3):307359, 2003. doi:10.1207/S15327809JLS1203_1.  [4] B. S. Bloom. Taxonomy of Educational Objectives: The Classification of Educational Goals. Handbook 1-2. Longmans: McKay, 1974.  [5] W. Boucsein. Electrodermal Activity. Springer, 2 edition, 2012. doi:10.1007/978-1-4614-1126-0.  http://dl.acm.org/citation.cfmid=1659450.1659458 http://dx.doi.org/10.1207/S15327809JLS1203_1 http://dx.doi.org/10.1007/978-1-4614-1126-0   [6] T. M. Brown and E. Fee. Walter Bradford Cannon: Pioneer physiologist of human emotions. American Journal of Public Health, 92(10):15941595, 2002. doi:10.2105/AJPH.92.10.1594.  [7] S. Buckingham-Shum. Learning Analytics. UNESCO Institute for Information Technologies in Education (ITTE), Policy Brief, 2012. URL: http: //iite.unesco.org/pics/publications/en/files/3214711.  [8] J. T. Cacioppo, L. G. Tassinary, and G. G. Berntson. Handbook of Psychophysiology, chapter Psychophysiological Science: Interdisciplinary Approaches to Classic Questions About the Mind, pages 116. Cambridge University Press, 3 edition, 2007.  [9] G. Chanel, M. Betrancourt, T. Pun, D. Cereghetti, and G. Molinari. Assessment of computer-supported collaborative processes using interpersonal physiological and eye-movement coupling. Proceedings - 2013 Humaine Association Conference on Aective Computing and Intelligent Interaction, ACII 2013, pages 116122, 2013. doi:10.1109/ACII.2013.26.  [10] G. Chanel, J. M. Kivikangas, and N. Ravaja. Physiological compliance for social gaming analysis: Cooperative versus competitive play. Interacting with Computers, 24(4):306316, July 2012. doi:10.1016/j.intcom.2012.04.012.  [11] G. Chanel and C. Muhl. Connecting Brains and Bodies: Applying Physiological Computing to Support Social Interaction. Interacting with Computers, 2015. doi:10.1093/iwc/iwv013.  [12] M. E. Dawson, A. M. Schell, and D. L. Filion. Handbook of Psychophysiology, chapter The Electrodermal System, pages 159181. Cambridge University Press, 3 edition, 2007.  [13] P. Dillenbourg. What do you mean by collaborative learning Collaborative-learning: Cognitive and Computational Approaches, pages 119, 1999.  [14] ECAR-ANALYTICS Working Group. The predictive learning analytics revolution: Leveraging learning data for student success. ECAR working group paper, 2015.  [15] A. N. Elkins, E. R. Muth, A. W. Hoover, A. D. Walker, T. L. Carpenter, and F. S. Switzer. Physiological compliance and team performance. Applied ergonomics, 40(6):9971003, Nov. 2009. doi:10.1016/j.apergo.2009.02.002.  [16] N. Enyedy and R. Stevens. The Cambridge Handbook of the Learning Sciences, chapter Analyzing collaboration, pages 191212. Cambridge University Press, 2014.  [17] S. H. Fairclough. Fundamentals of physiological computing. Interacting with Computers, 21(1-2):133145, 2009. doi:10.1016/j.intcom.2008.10.011.  [18] C. Fere. Note sur les modifications de la resistance electrique sous linfluence des excitations sensorielles et des emotions. Comptes Rendus des Seances de la Societe de Biologie, 5:217219, 1888.  [19] E. Ferreira, D. Ferreira, S. Kim, P. Siirtola, J. Roning, J. F. Forlizzi, and A. K. Dey. Assessing real-time cognitive load based on psycho- physiological measures for younger and older adults. In Computational Intelligence, Cognitive Algorithms, Mind, and Brain  (CCMB), 2014 IEEE Symposium on, pages 3948, Orlando, FL, 2014. IEEE. doi:10.1109/CCMB.2014.7020692.  [20] M. Garbarino, M. Lai, S. Tognetti, R. Picard, and D. Bender. Empatica E3 - A wearable wireless multi-sensor device for real-time computerized biofeedback and data acquisition. Proceedings of the 4th International Conference on Wireless Mobile Communication and Healthcare - Transforming healthcare through innovations in mobile and wireless technologies, pages 3942, 2014. doi:10.4108/icst.mobihealth.2014.257418.  [21] M. K. Garg, D.-J. Kim, D. S. Turaga, and B. Prabhakaran. Multimodal analysis of body sensor network data streams for real-time healthcare. In Proceedings of the International Conference on Multimedia Information Retrieval - MIR 10, page 469, New York, New York, USA, Mar. 2010. ACM Press. doi:10.1145/1743384.1743467.  [22] R. A. Henning, A. G. Armstead, and J. K. Ferris. Social psychophysiological compliance in a four-person research team. Applied Ergonomics, 40(6):10041010, 2009. doi:10.1016/j.apergo.2009.04.009.  [23] R. A. Henning, W. Boucsein, and M. Claudia Gil. Social-physiological compliance as a determinant of team performance. International Journal of Psychophysiology, 40(3):221232, Apr. 2001. doi:10.1016/S0167-8760(00)00190-2.  [24] R. A. Henning and S. L. Sauter. Work-physiological synchronization as a determinant of performance in repetitive computer work. Biological Psychology, 42(3):269286, Feb. 1996. doi:10.1016/0301-0511(95)05162-7.  [25] S. Jarvela, J. M. Kivikangas, J. Katsyri, and N. Ravaja. Physiological linkage of dyadic gaming experience. Simulation & Gaming, 45(1):2440, 2014. doi:10.1177/1046878113513080.  [26] C. Karenbach. Ledalab  a software package for the analysis of phasic electrodermal activity. Technical report, Allgemeine Psychologie, Institut fur Psychologie, 2005.  [27] A. Keskinarkaus, S. Huttunen, A. Siipo, J. Holappa, M. Laszlo, I. Juuso, E. Vayrynen, J. Heikkila, M. Lehtihalmes, T. Seppanen, and S. Laukka. MORE  a multimodal observation and analysis system for social interaction research. Multimedia Tools and Applications, 2015. doi:10.1007/s11042-015-2574-9.  [28] J. Kim, M. Snodgrass, M. Pietrowicz, K. Karahalios, and J. Halle. BEDA: Visual analytics for behavioral and physiological data. Proceedings of the Workshop on Visual Analytics in Healthcare, 2013.  [29] P. A. Kirschner, J. Sweller, and R. E. Clark. Why minimal guidance during instruction does not work: An analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-based teaching. Educational Psychologist, 41(2):7586, 2006. doi:10.1207/s15326985ep4102_1.  [30] S. D. Kreibig. Autonomic Nervous System activity in emotion: a review. Biological psychology, 84(3):394421, July 2010. doi:10.1016/j.biopsycho.2010.03.010.  [31] R. W. Levenson and J. M. Gottman. Marital  http://dx.doi.org/10.2105/AJPH.92.10.1594 http://dx.doi.org/10.1109/ACII.2013.26 http://dx.doi.org/10.1016/j.intcom.2012.04.012 http://dx.doi.org/10.1093/iwc/iwv013 http://dx.doi.org/10.1016/j.apergo.2009.02.002 http://dx.doi.org/10.1016/j.intcom.2008.10.011 http://dx.doi.org/10.1109/CCMB.2014.7020692 http://dx.doi.org/10.4108/icst.mobihealth.2014.257418 http://dx.doi.org/10.1145/1743384.1743467 http://dx.doi.org/10.1016/j.apergo.2009.04.009 http://dx.doi.org/10.1016/S0167-8760(00)00190-2 http://dx.doi.org/10.1016/0301-0511(95)05162-7 http://dx.doi.org/10.1177/1046878113513080 http://dx.doi.org/10.1007/s11042-015-2574-9 http://dx.doi.org/10.1207/s15326985ep4102_1 http://dx.doi.org/10.1016/j.biopsycho.2010.03.010   interaction: Physiological linkage and aective exchange. Journal of Personality and Social Psychology, 45(3):587597, 1983. doi:10.1037/0022-3514.45.3.587.  [32] C. A. Mangina and J. Beuzeron-Mangina. Direct electrical stimulation of specific human brain structures and bilateral electrodermal activity. International Journal of Psychophysiology, 22(1-2):18, Apr. 1996. doi:10.1016/0167-8760(96)00022-0.  [33] A. Mikroyannidis, A. Okada, P. Scott, E. Rusman, M. Specht, K. Stefanov, P. Boytchev, A. Protopsaltis, P. Held, S. Hetzner, K. Kikis-Papadakis, and F. Chaimala. weSPOT: A personal and social approach to inquiry-based learning. Journal of Universal Computer Science, 19(14):20932111, 2013. doi:10.3217/jucs-019-14-2093.  [34] N. Miyake and P. A. Kirschner. The social and interactive dimensions of collaborative learning. In The Cambridge handbook of the learning sciences, pages 418438, 2014.  [35] E. Montague, J. Xu, and E. Chiou. Shared experiences of technology and trust: An experimental study of physiological compliance between active and passive users in technology-mediated collaborative encounters. IEEE Transactions on Human-Machine Systems, 44(5):614624, 2014. doi:10.1109/THMS.2014.2325859.  [36] P. Naykki, H. Jarvenoja, S. Jarvela, and P. Kirschner. Monitoring makes a dierence: quality and temporal variation in teacher education students collaborative learning. Scandinavian Journal of Educational Research, pages 116, 2015.  [37] D. J. Nicol and D. Macfarlane-Dick. Formative assessment and self-regulated learning: a model and seven principles of good feedback practice. Studies in higher education, 31(2):199218, 2006. doi:10.1080/03075070600572090.  [38] D. Novak, M. Mihelj, and M. Munih. A survey of methods for data fusion and system adaptation using autonomic nervous system responses in physiological computing. Interacting with Computers, 24(3):154172, May 2012. doi:10.1016/j.intcom.2012.04.003.  [39] L. Noy, N. Levit Binnun, U. Alon, and Y. Golland. Being in the zone: physiological markers of togetherness in joint improvisation. Frontiers in Human Neuroscience, 9(187), 2015. doi:10.3389/fnhum.2015.00187.  [40] A. Pecchinenda. The Aective Significance of Skin Conductance Activity During a Dicult Problem-solving Task. Cognition & Emotion, 10(5):481504, 1996. doi:10.1080/026999396380123.  [41] R. Picard. Aective Computing. MIT press, 1997. [42] R. W. Picard, S. Fedor, and Y. Ayzenberg. Multiple  arousal theory and daily-life electrodermal activity asymmetry. Emotion Review, 2015. doi:10.1177/1754073914565517.  [43] P. R. Pintrich et al. A manual for the use of the Motivated Strategies for Learning Questionnaire (MSLQ). ERIC, 1991.  [44] S. W. Porges, R. E. Bohrer, M. N. Cheung,  F. Drasgow, P. M. McCabe, and G. Keren. New time-series statistic for detecting rhythmic co-occurrence in the frequency domain: the weighted coherence and its application to psychophysiological research. Psychological bulletin, 88(3):580, 1980.  [45] M. Russel. Improving Student Retention in Higher Education: The Role of Teaching and Learning, chapter Leveraging student engagement with assessment: collecting intelligence to support teaching, student progress and retention, page 108. Taylor & Francis, 2008.  [46] J. Schneider, D. Borner, P. van Rosmalen, and M. Specht. Augmenting the Senses: A Review on Sensor-Based Learning Support. Sensors, 15(2):40974133, 2015. doi:10.3390/s150204097.  [47] L. Shen, M. Wang, and R. Shen. Aective e-Learning: Using Emotional Data to Improve Learning in Pervasive Learning Environment Related Work and the Pervasive e-Learning Platform. Educational Technology & Society, 12(2):176189, 2009.  [48] Y. Shi, N. Ruiz, R. Taib, E. Choi, and F. Chen. Galvanic skin response (gsr) as an index of cognitive load. In CHI 07 Extended Abstracts on Human Factors in Computing Systems, CHI EA 07, pages 26512656, New York, NY, USA, 2007. ACM. doi:10.1145/1240866.1241057.  [49] G. Siemens and R. S. J. d. Baker. Learning analytics and educational data mining. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK 12, page 252, New York, New York, USA, Apr. 2012. ACM Press. doi:10.1145/2330601.2330661.  [50] T. Smith and K. Smith. Handbook of Human Factors, chapter Feedback-control mechanisms of human behavior, pages 251293. Wiley, 1987.  [51] M. Swan. Sensor Mania! The Internet of Things, Wearable Computing, Objective Metrics, and the Quantified Self 2.0. Journal of Sensor and Actuator Networks, 1(3):217253, Nov. 2012. doi:10.3390/jsan1030217.  [52] S. Taylor, N. Jaques, W. Chen, S. Fedor, A. Sano, and R. Picard. Automatic identification of artifacts in electrodermal activity data. EMBC (to appear). IEEE, 2015.  [53] B. Trilling and C. Fadel. 21st Century Skills: Learning for Life in Our Times. Wiley, 2009.  [54] G. Wolfgang and H. Drachsler. Translating learning into numbers: a generic framework for Learning Analytics. Proceedings of the 2nd International Conference on Learning Analytics and Knowledge, 15(3):4257, 2012. doi:10.1145/2330601.2330634.  [55] R. S. Woodworth and H. Schlosberg. Experimental psychology. Oxford and IBH Publishing, 1954.  http://dx.doi.org/10.1037/0022-3514.45.3.587 http://dx.doi.org/10.1016/0167-8760(96)00022-0 http://dx.doi.org/10.3217/jucs-019-14-2093 http://dx.doi.org/10.1109/THMS.2014.2325859 http://dx.doi.org/10.1080/03075070600572090 http://dx.doi.org/10.1016/j.intcom.2012.04.003 http://dx.doi.org/10.3389/fnhum.2015.00187 http://dx.doi.org/10.1080/026999396380123 http://dx.doi.org/10.1177/1754073914565517 http://dx.doi.org/10.3390/s150204097 http://dx.doi.org/10.1145/1240866.1241057 http://dx.doi.org/10.1145/2330601.2330661 http://dx.doi.org/10.3390/jsan1030217 http://dx.doi.org/10.1145/2330601.2330634   Introduction  Rationale  Method  Setting  Participants  Task description  Procedure  Collected data  Data processing  Collaborative learning measures   Analysis and results  Conclusions and future work  Acknowledgements  References   "}
{"index":{"_id":"10"}}
{"datatype":"inproceedings","key":"Koh:2016:PFL:2883851.2883914","author":"Koh, Elizabeth and Shibani, Antonette and Tan, Jennifer Pei-Ling and Hong, Helen","title":"A Pedagogical Framework for Learning Analytics in Collaborative Inquiry Tasks: An Example from a Teamwork Competency Awareness Program","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"74--83","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883914","doi":"10.1145/2883851.2883914","acmid":"2883914","publisher":"ACM","address":"New York, NY, USA","keywords":"assessment, collaboration, dispositional analytics, evaluation, learning design, pedagogical model, teamwork, teamwork competency, twenty-first century skills","abstract":"Many pedagogical models in the field of learning analytics are implicit and do not overtly direct learner behavior. While this allows flexibility of use, this could also result in misaligned practice, and there are calls for more explicit pedagogical models in learning analytics. This paper presents an explicit pedagogical model, the Team and Self Diagnostic Learning (TSDL) framework, in the context of collaborative inquiry tasks. Key informing theories include experiential learning, collaborative learning, and the learning analytics process model. The framework was trialed through a teamwork competency awareness program for 14 year old students. A total of 272 students participated in the program. This paper foregrounds students' and teachers' evaluative accounts of the program. Findings reveal positive perceptions of the stages of the TSDL framework, despite identified challenges, which points to its potential usefulness for teaching and learning. The TSDL framework aims to provide theoretical clarity of the learning process, and foster alignment between learning analytics and the learning design. The current work provides trial outcomes of a teamwork competency awareness program that used dispositional analytics, and further efforts are underway to develop the discourse layer of the analytic engine. Future work will also be dedicated to application and refinement of the framework for other contexts and participants, both learners and teachers alike.","pdf":"A Pedagogical Framework for Learning Analytics in  Collaborative Inquiry Tasks: An Example from a   Teamwork Competency Awareness Program  Elizabeth Koh, Antonette Shibani, Jennifer Pei-Ling Tan, and Helen Hong   National Institute of Education,  Nanyang Technological University, Singapore   elizabeth.koh@nie.edu.sg, antonette.x@nie.edu.sg, jen.tan@nie.edu.sg, helen.hong@nie.edu.sg    ABSTRACT  Many pedagogical models in the field of learning analytics are  implicit and do not overtly direct learner behavior. While this  allows flexibility of use, this could also result in misaligned  practice, and there are calls for more explicit pedagogical models  in learning analytics. This paper presents an explicit pedagogical  model, the Team and Self Diagnostic Learning (TSDL)  framework, in the context of collaborative inquiry tasks. Key  informing theories include experiential learning, collaborative  learning, and the learning analytics process model. The  framework was trialed through a teamwork competency  awareness program for 14 year old students. A total of 272  students participated in the program. This paper foregrounds  students and teachers evaluative accounts of the program.  Findings reveal positive perceptions of the stages of the TSDL  framework, despite identified challenges, which points to its  potential usefulness for teaching and learning. The TSDL  framework aims to provide theoretical clarity of the learning  process, and foster alignment between learning analytics and the  learning design. The current work provides trial outcomes of a  teamwork competency awareness program that used dispositional  analytics, and further efforts are underway to develop the  discourse layer of the analytic engine. Future work will also be  dedicated to application and refinement of the framework for  other contexts and participants, both learners and teachers alike.   CCS Concepts   Applied computing~Collaborative learning    Human- centered computing~Visual analytics    Human-centered  computing~Synchronous editors    General and reference~Design   Keywords  Teamwork, teamwork competency, collaboration, pedagogical  model, learning design, dispositional analytics, assessment,  twenty-first century skills, evaluation.   1. INTRODUCTION  Several learning analytics papers have proposed pedagogical  models for learning. Greller and Drachsler [14] conceptualize that  learning analytics (LA) enable many different types of pedagogies  and this can either be implicit or made explicit in the design.   Implicit pedagogies refer to pedagogical strategies that are  implicitly contained in the input datasets that encapsulate the  pedagogic behavior of users [14, p.53]. In this paper, we broaden  this definition to include any LA model that does not overtly  direct learner behavior based on any theoretical pedagogical  model. The pedagogy is part of the input dataset or the system that  captures the pedagogic behaviors of users. In contrast, LA that  make pedagogies explicit in the design address them through the  goals and objectives of the design, and this is seen through  pedagogic behavior enabled by the LA and system, as well as the  consequence of that behavior [14; 24]. However, in our brief  review of the emerging field, there are very little LA designs that  develop an explicit pedagogical model. Many pedagogical models  in LA papers are implicit, or do not focus on any particular model.  Although this allows the flexibility and creativity of the  stakeholders in its use and sustainability of the system, it may also  lead to confusion as well as uninformed, non-ideal or misaligned  practice. In this paper, we present a pedagogical framework for  LA in collaborative inquiry tasks and demonstrate trial outcomes  in a teamwork competency awareness program.   We begin with a brief literature review of the different categories  of pedagogical frameworks in learning analytics. Next, we  introduce the background of the research problem and the studys  context, namely, the increasing focus on 21st century  competencies, and the competency of teamwork. Our pedagogical  framework was based on several informing theories and the  following sub-section (3.2) provides a theoretical understanding.  Subsequently, 4 elaborates on how the framework was used and  implemented in a school setting. We detail each stage of the  framework. 5 describes the evaluation of the pedagogical  framework, drawn from the qualitative accounts of participant  students and teachers. In 6, we elaborate on our findings, their  implications and future work, and conclude by highlighting  several potential advantages of the TSDL framework for the field  of LA (7).   2. PEDAGOGICAL FRAMEWORKS FOR  LEARNING ANALYTICS  One of the key challenges in LA is the lack of pedagogic theory  [10]. This sparsity of theory is in part attributable to numerous  extant LA studies subtly embedding pedagogical models and  strategies in the design [11; 42] and/or only providing a broad  overarching frame of the general theory [1]. Some LA designs  even claim pedagogical neutrality [10].   Knight, Buckingham Shum and Littleton [21] highlight that no  learning analytics design can exist without pedagogical and  epistemological assumptions. These assumptions determine what  methods and types of learning analytics are used. The study   Publication rights licensed to ACM. ACM acknowledges that this  contribution was authored or co-authored by an employee, contractor or  affiliate of a national government. As such, the Government retains a  nonexclusive, royalty-free right to publish or reproduce this article, or to  allow others to do so, for Government purposes only.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom    Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883914     mailto:antonette.x@nie.edu.sg http://dx.doi.org/10.1145/2883851.2883914   surmises three forms of learning analytics that stem from various  pedagogical lenses: mastering content (generally from  transactional and constructivist lenses), evidencing membership  and processes (affect-based, apprenticeship, and connectivist  lenses), and success is use (connectivist and pragmatist lenses).   Besides epistemological assumptions, pedagogical frameworks  can be represented more practically through learning designs.  These are the sequence of learning tasks, resources, and supports  that a teacher constructs for students over part of, or the entire,  academic semester [24, p.1441-1442]. There are many kinds of  learning designs with differing levels of granularity. Chiefly,  learning designs should make explicit the planned pedagogical  action without going into the details of the specific instructional  activity such as in a typical lesson plan. This allows learning  designs to be used as a framework for the design of analytics to  support educators in learning and teaching decisions. The Social  Networks Adapting Pedagogical Practice tool based on the socio- constructivist model of learning is one such instance [24].   Wise et al. [44] provides another example of a pedagogical  framework aimed at informing learning analytics intervention  design. Based largely on constructivism, meta-cognition, and self- regulated learning, four principles (integration, agency, reference  frame and dialogue) and three processes (grounding, goal-setting  and reflection) are conceived. Moreover, pedagogical frameworks  can and should be made explicit throughout the lifecycle of the  teaching and learning curriculum. For instance, in recent work,  Rodriguez et al. [31] designed a pedagogical model that aligned  scripting and monitoring explicitly throughout the whole  development and use process, from planning, to technological  deployment, enactment and evaluation of the implementation.   These examples, which explicitly describe their pedagogical  frameworks, clearly provide direction for educators and  stakeholders. If such pedagogical underpinnings are hidden, it  limits the potential of the approach and can cause misaligned  practice. Moreover, as these approaches shape the reality they  measure, explicitly stating such frameworks makes clear what  counts as important. However, not many such examples exist, and  this paper intends to present a pedagogical framework for LA in  the context of inquiry-based collaborative tasks. The background  of our study, the larger research problem and agenda are described  next.   3. RESEARCH PROBLEM AND STUDY  CONTEXT: ASSESSMENT OF TWENTY- FIRST CENTURY COMPETENCIES  As a response to globalization and the rapid technological changes  in the world, educators and organizations have identified that  academic knowledge only is not sufficient for learners to thrive in  the world. Rather, a suite of competencies, commonly termed as  21st century competencies, are needed. Many competencies have  been proposed and highlighted by numerous international  organizations and national frameworks. Among all these  frameworks, there are many similar competencies namely,  collaboration, communication, digital literacy, citizenship,  problem solving, critical thinking, creativity and productivity [40].  Also, a number of more recent frameworks and studies attempt to  concentrate on the gap area of assessment, while focusing on a  few key competencies. For example, the Partnership for 21st  century learning (www.p21.org) consortium narrowed its focus to  4 essential skills, namely, creativity, critical thinking,  collaboration and communication.   The study presented in this paper focuses on collaboration in  small groups in group-based inquiry tasks. This is common across  many 21st century competency frameworks and is pertinent in this  day and age for school and work. We term this skill--teamwork  competency-- a multi-dimensional construct that focuses on the  process of members working in a team [34; 37]. Despite many  past studies of teamwork and collaboration, the what to measure  (teamwork conceptualizations) and how to measure (methods  including analytics) and design of teamwork activities  (pedagogical model) have yet to be firmly established to date.    In the next section, we first provide a brief description of our  conceptualization of teamwork and the associated analytics  methods, after which we turn to focus more squarely on the  development, implementation and evaluation of the TSDL  pedagogical framework.   3.1 Teamwork competency conceptualization  and analytics  Based on past literature and pilot tests, six dimensions of  teamwork competency are established: coordination [9; 27],  mutual performance monitoring [35], team decision making [13;  38], constructive conflict [38], team emotional support [6], and  team commitment [25]. (A detailed explication of these six  dimensions is beyond the scope of this paper, but documented in  the authors prior and upcoming work [22]).   For the measure of teamwork competency, we adopted a multi- method approach to add rigor and objectivity. In addition,  contemporary technological affordances, particularly learning  analytics, are being explored for its potential value-add to future  scalability   Researchers have increasingly developed several learning  analytics techniques and applications to assess competencies as  the LA field matures. To assess multiliteracies, Dawson and  Siemens [5] identified several learning analytics methods such as  modeling and knowledge domain mapping to measure literacies  relating to experimentation as well as structured mapping and  prediction to measure products and creation multiliteracies.    Ferguson and Shum [11] define five types of analytics that are  relevant in this social age: social network analytics, discourse  analytics, content analytics, disposition analytics and context  analytics. We posit that a combination of these analytics can be  meaningfully designed and implemented for the assessment of  teamwork. For instance, social network analytics through  gathering interaction data could measure the participation and the  network of collaborating student teams.   Discourse analytics are also another potent way to identify and  infer teamwork competency in online group communication [2;  32]. For instance, Crowston et al. [4] focused on group  maintenance behaviors in online groups and showed good  performance of a discourse-based system using Natural Language  Processing (NLP) rules. They examined the role of group  maintenance behaviors in the messages of online discussion lists  by applying NLP rules to automate coding, which could be  reviewed and corrected by human coders. In their approach, codes  were applied based on specific features that were evidences of  theoretical constructs of interest. It involved the steps of pre- processing, tokenization, POS tagging and rule-writing.   Dispositional analytics can also make visible teamwork  competencies. Dispositional analytics make use of the traditional  surveys of the social sciences and represent them as a visual   http://www.p21.org/   analytic [2]. This is demonstrated by Buckingham Shum and  Crick [2] who conceptualized a learning power model of learning  dispositions. This was empirically validated and visualized using  the Learning Warehouse system with self-reports of participants  from many schools and countries. The analytic could be  visualized to show aggregations across groups of learners or  across a person over time.   Self-reports can be performed for oneself as well as for others.  Besides self-ratings, there is much potential in peer ratings, where  participants evaluate how they perceive another person. Peer  ratings can allow for a more triangulated and fairer measurement  in teamwork [12; 28]. Self and peer ratings are also more scalable  than having experts (such as teachers) rate students. A Self and  Peer Assessment Resource Kit (SPARK) was developed for  college students which provided them with a confidential  assessment tool to develop their teamwork skills [12]. Most  students found it valuable as it was a fair system of assessing team  contributions, especially when the goals of using SPARK were  aligned with the learning outcomes of the course. This form of  self and peer assessment were also found to encourage the  development of teamwork skills such as team cooperation,  commitment and team engagement [43].   3.2 Pedagogical model  The learning design of collaborative inquiry tasks can be a  complex process. There have been several approaches to facilitate  effective teamwork and collaboration. Generally, teamwork  activities have their pedagogical roots in collaborative learning  which is a socio-constructivist approach of learning [7; 15; 41].  As learners discuss and negotiate, they learn from each other.    Some research has highlighted antecedents and/or task  characteristics for successful teamwork i.e., complex, open-ended,  and ill-structured tasks, interdependence (the extent to which team  members must rely and work together to perform the task), and  individual and group accountability [20; 36].    Besides these characteristics, augmenting group behavior using  analytics is an emerging and important component in research.  Some work in this has been carried out in the area of group  awareness. In social group awareness, past research has used  visual analytics for both quantitative (e.g. amount of discussion,  extent of participation, perception of collaboration) and qualitative  (e.g. agreement, quality of group discussions) awareness  information [16]. However, a gap area in the research is that  analytics use is unregulated. It is up to students to decide if they  will view the awareness information and also it is up to them to  choose how they will process and use this visualization. This  could explain some of the variability in the results of past research  experiments. Further research needs to provide a pedagogical  model for such group awareness information to be more  effectively used by students [17].   In a face-to-face collaboration study, Rummel et al. [33] found  that observational learning enhanced collaboration skills more  than a collaboration script. This pedagogical model was more  effective when elaboration support was conducted. Elaboration  support was implemented through having instructional prompts to  focus students to relevant underlying principles, and a reflective  self-explanation where participants would recall the collaboration  process and explain to themselves what aspects had been  important for the collaboration to be successful [33, p.79]. This  suggests the importance of reflection in collaborative inquiry  tasks.   Although observational learning has been shown to be effective,  research highlights that it is difficult and time-intensive to develop  ideal models of collaboration in ill-structured tasks [33]. Instead, a  pedagogical approach that focuses the effort on the learners  metacognitive processes would be helpful for internalizing  teamwork competency. Reflecting on collaboration processes  seems to be an effective pedagogical strategy to enhance  collaboration skills [26; 28]. Moreover, reflection is one of the  two fundamental objectives in LA, the other being prediction [14].  Reflection is the critical self-evaluation of users own data to  obtain self-knowledge (and may also include other data) while  prediction focuses on modeling activities for other activities and  interventions.   One of the foundational learning theories that is based on  experience and reflection is experiential learning [23].  Experiential learning theory posits that humans learn through a  transformation of experience [23]. The theory emphasizes a  cycle of four stages: concrete experience, reflective observation,  abstract conceptualization and active experimentation. It has three  major principles: learning is a process not an outcome, learning is  grounded in experience, and that learning requires the resolution  of the dialectics (experience and conceptualization, observation  and action). It has been tested in individual learner contexts as  well as in teams [19].   Another related conceptualization is the learning analytics process  model [39] which alludes to a concrete experience but emphasizes  that awareness is derived from data. In awareness, data is  visualized in various ways such as radar charts, activity streams,  tabular overviews, to help learners see their activity/interactions  better. Verbert et al. [39] also posit a continuous loop of  awareness, reflection, sensemaking and impact. Reflection is  important to help learners think about the data. This is closely  followed by sensemaking which helps generate new knowledge  and insights. Lastly, impact aims to create new meaning or change  behaviors.    Besides individual reflection, collaborative reflection or co- reflection among team members is a valuable learning process  [18; 45]. In a collaborative inquiry task, the world includes the  views of the team members. An individual reflecting on his own  behavior may not be aware of how he is perceived by other group  members and a group reflection would be helpful to help learners  learn from each other. This is closely related to socially shared  regulation [18] and recent research has delineated three learning  design principles for providing support for shared regulation:  enhancing awareness of learners own and others process of  learning; making visible the externalization of students and  others learning process and interaction; and prompting and  stimulating regulatory processes.   Integrating these principles, we posit that an experiential,  awareness and reflection approach would be an effective  pedagogical model to nurture students teamwork competency.  We term this the Team and Self Diagnostic Learning (TSDL)  framework which primarily aims to develop students teamwork  competencies and collaboration skills.   3.2.1 Team and self diagnostic learning framework  The key informing pedagogies of TSDL are experiential learning  [23], collaborative learning [7; 15; 41], and the learning analytics  process model [39].    The framework broadly follows the experiential learning cycle  [23] and integrates learning designs with learning analytics [24].     The TSDL framework comprises four stages: team-based concrete  experience, self and team awareness building, self and team  reflection and sensemaking, and self and team growth and change  (See Figure 1).                                  Figure 1: Team and Self Diagnostic Learning Framework.    Learners begin with team-based concrete experiences namely,  the collaborative inquiry task. This can be in multiple forms such  as a team-building ice-breaker activity, the collaborative writing  of a report, and a group brainstorming chat. During the process of  the experience, learners will at times perform individual work,  and at times engage with other team members and learn from each  other.    After the concrete experience, we propose that team members can  be made more aware of their experience using visual analytics  (such as through discourse and dispositional analytics). These  make visible the activities in formats such as data aggregations  and can trigger an intended change in the learner [8]. Basically,  this stage intends to build the awareness of the individuals and  the teams learning process.   The next stage is self and team reflection and sensemaking.  This is a deliberate set of activities to enable learner reflection,  abstract conceptualization and sensemaking of the awareness  information. Learners need to evaluate the visual analytics, ask  and answer reflective questions, diagnose their learning and create  new insights. Goal-setting and future-oriented questions are  particular effective strategies [29; 44]. This should be done  individually and as a team.   The last stage is self and team growth and change. The  successful resolution of the dialectics of concrete observation and  abstract conceptualization causes internal change in the learner  [23]. When learners make sense of their behaviors, and realize  areas of change and areas to change, they grow and can enact new  behaviors and attitudes. Learners better self-awareness and  change in team behavior can be seen through the differences in  earlier and later perceptions and behavior.   Several studies have explicated and shown the cycle of concrete  experience, awareness, reflection and sensemaking, and growth  and change to be a powerful pedagogical approach that is  effective in nurturing cognitive and non-cognitive skills, although  these processes have not been previously consolidated into a  single framework and termed TSDL [17; 28-30].   For instance, a series of studies by Phielix and colleagues [28; 29]  shows the potential of the pedagogical approach for secondary  school students. Phielix et al. [28] developed two tools, a peer   feedback tool (Radar) and a reflection tool (Reflector) for high  school students working together on a collaborative writing task.  Participants in dyads or groups of threes and fours, worked over  three sessions to complete their task. Students in the experimental  group performed peer feedback and reflected after each session  (three time points in total). The research found that teams using  the two tools (which encouraged awareness and reflection) had  higher social and cognitive behavior (between the first and second  session), and higher social group performance than the control  condition. The study also compared between Radar-only groups  and groups with Radar and Reflector, but did not find many  significant differences in the self-reported scores. This was  possibly due to the reflection questions which highlighted  individual contributions.   A follow-up study was conducted where the reflection questions  were modified to be more future-oriented, prompting students to  goal-set for improved team behaviors [29]. The reflection activity  consists of a series of six reflection questions which students type  out. For instance, what is your opinion of how you functioned in  the group The last question is Set specific goals (who, what,  when) to improve group performance [29, p. 1094]. Reflection  was done individually as well as together as a team. As compared  to the control group, peer feedback and reflection resulted in  higher self-reported social performance between the first and  second session, and between the first and third session. Social  performance included team development, group satisfaction, and  less group conflict, which are related to teamwork dimensions.  These suggest that the TSDL framework could be an effective  pedagogical model to enhance teamwork competency.   4. FRAMEWORK IN USE: A TRIAL  TEAMWORK COMPETENCY  AWARENESS PROGRAM  A two session (a total of 135 minutes) teamwork competency  awareness program was designed for 14 year old students in a  school. Seven classes with a total of 272 students participated in  the study and the program was carried out class by class over 4  months as part of the students curriculum in 2014. This trial was  designed as a blended learning experience using 1-1 student to  computers ratio. Students in a class were randomly grouped into  teams of 3 or 4 members. In this trial, the researchers led the  activities while the teachers co-facilitated the sessions.   The goals of the program were for students to engage in the  practice of teamwork and gain awareness of teamwork processes.  We hoped that students would learn to work better in future  collaborative inquiry tasks.   The different stages of the framework and how it was  implemented in our context are described in the following  sections.   4.1 Team-based concrete experience  This is the starting stage of the TSDL framework. Students gained  team-based concrete experience in session 1 when they engaged  in collaborative inquiry tasks in an online environment. We  designed an online group chat where students were provided with  their tasks and where they discussed and submitted their answer.   Students began with an icebreaker task to help them know their  team members and familiarize themselves with the online chat  system. This was followed by a dilemma task. The dilemma task  was an open-ended task with a scenario that challenged students  to make choices to save the environment, the people in a home for   Self and Team  Awareness   Building (visual  analytics)      Self and Team  Reflection    and  Sensemaking   Team-based  Concrete   Experience     Self and Team  Growth    and   Change     the aged, and/or their fathers livelihood. Students had to come up  with one final decision as a team regarding how the problem  could be solved. Instructions were given by a human-controlled  ChatAdmin who typed standardized instruction messages in the  chat. After this collaboration experience, students rated  themselves and their peers on an online survey portal that we  developed. The survey items were constructed based on the  teamwork competency dimensions mentioned in section 3.1.  Figures 2 and 3 show the online chat environment and the survey  interface respectively.   4.2 Self and team awareness building  The next stages of self and team awareness building and self and  team reflection and sensemaking took place in session 2. We  needed time to calculate and produce the visual analytics. For this  trial, we were only able to provide a visual analytic based on the  dispositional analytics (i.e., self and peer ratings).     Figure 2: Online chat environment.        Figure 3: Survey interface for self and peer rating.      The visual analytic was termed a teamwork micro-profile rather  than a profile to acknowledge that teamwork processes can  change, and is not a permanent status of a students teamwork  competency. It is based on the micro-time context, which  examines a short period of group processes [3]. The micro- profiles were created from the rating survey based on a Likert  scale from 1 to 5 where 5 indicates strongly agree. Ratings of  3.5 and above are generally considered high. The visualization  included the individual students numeric rating scores of each   dimension, separated into self, peer and overall (of peer and self).  A radar chart of the students scores was also presented to allow  students to see their strengths and weaknesses easily.   Additionally, an overall similarity score was calculated. This is to  enable students to see the difference between their own ratings  and those of their peers. We developed this score to compare self  and peer scores in line with [43] as it would make the differences  between team members more obvious and offer critical reflection.  The calculation is:   (Self scores  Peers scores) for each dimension  Number of dimensions     A negative value indicates that the student rated himself lower  than his peers (team members). A positive value indicates that the  student rated himself higher than his peers. A good range is  between -0.5 to +0.5 as it shows a high degree of similarity.   Figure 4 displays our visual analytic of teamwork competency.  Students were given their personal micro-profile as a colored  printout in class. They were given some time to look at it and  subsequently briefed about the aspects of the micro-profile. This  was to generate self and team awareness building.           Figure 4: Visual analytic of teamwork competency: teamwork   competency micro-profile for a student.     4.3 Self and team reflection and sensemaking  Subsequently, self and team reflection and sensemaking took  place. Students were asked to sit in their teams, and provided with  a reflection worksheet comprising 4 questions derived from past  research [29].    For individual reflection, students were asked to reflect on:    (1) What differences do you see between the rating that you  received from your peers and your self-rating      (2) Why do or dont you agree with your peers concerning your  rating    For team reflection, students reflected in their groups on:    (3) What does the group think about its functioning in general  Discuss and formulate a conclusion shared by all the group  members.   (4) Set specific goals (who, what, when) to improve group  performance   The objective of this explicitly scaffolded self- and team-based  reflection task was to create the pedagogical conditions for  students to assimilate and synthesize their reflections of their  experience in terms of both conceptual understandings and very  importantly, concrete goal-setting and plans to productively adapt  or modify their teamwork behaviors [23; 29]. To end session 2,  the facilitator consolidated what students learned by asking  students to share in class and closed the session.    One limitation of the trial cycle reported here is that the self and  team growth and change, if any, were not recorded and  documented in a structured manner, mainly because the focus of  the program was on students gaining awareness of their own  teamwork competencies and to learn to work better in future  collaborative tasks. We aim to address this limitation in future  work. Nevertheless, for the trial cycle at hand, students writing in  the reflection worksheets provided some evidence of their  growing awareness and desire for change.   5. PEDAGOGICAL FRAMEWWORK  EVALUATION  A qualitative analysis was performed to evaluate the impact and  effectiveness of the TSDL framework from the perspectives of  both the students and teachers.    5.1 Data sources and analysis method  The key data sources were focus group discussions (FGDs) with  students and interviews with teachers. Students written reflection  worksheets were also examined. There was a total of 6 FGDS  from 6 teams (3 to 4 students each) from different classes. Each  FGD lasted between 45mins to 1 hour and was video-recorded.  We also had written or face-to-face interviews with the teachers  from all the 7 classes. For this analysis, a subset of the 272  participant reflection sheets was examined.  Thematic analysis was carried out where the data was examined  and information categorized into codes and then larger themes.   5.2 Findings  5.2.1 Overall program  Generally students and teachers were receptive towards the  program. Overall, students felt that the program helped them to  know how others saw them, and understand themselves better in  terms of the teamwork dimensions.  [I was able to] find out what others think about me. I was able to  find out what I can do more. Student X.  Some students wanted to go through the program again. Student Y  said that she would like to get to talk with other people and know  other students ideas. Student Z shared, I think it is useful to  experience how other people work in a group together with a  different group.   Teachers also found the program useful for their students to be  more self-aware and relate to others better.   Overall the program is just nice, timing is not too draggy.  Students enjoyed the first session. As for session 2, it's good for  the students to know and express themselves according to their  profile. Teacher A.  The program is okay, but depends on how you execute it.  Teacher B.  The execution of the program depended a lot on the skill of the  facilitator being able to guide and manage the classroom  dynamics. In this trial, the researchers led the program. However,  a few teachers suggested that the teachers could play a greater role  in leading and facilitating this (and the researchers also  concurred). This is because the teachers know their students better  which could help in facilitating the whole activity.  The TSDL framework was not made explicit to both the teachers  and students, and during the FGDs and interviews, we asked the  participants about their perceptions towards the different stages of  the framework.   5.2.2 Team-based concrete experience  As session 1 required students' to use the computer, students  especially liked this experience. It was a break from their normal  classroom lessons. Although there were a few who thought it was  boring, the majority found it quite fun. It was something new  and interesting for them, and they could go out of their normal  class. A teacher shared that some students were keyboard  warriors and liked communicating online.  Students were generally receptive towards the collaborative  inquiry task, although there were a few who found it confusing, or  were not engaged with the task or, did not like the topic. Students  suggested task scenarios that could give them more life lessons  or be related to current affairs.  We found that students were not new to teamwork as they had to  work in teams for other projects in school. However, for many,  what was different was who they were working with, as the  research team randomly grouped the students into teams. Many of  the students found themselves in teams with classmates they had  never worked with before. Some students were unhappy about this  initially, but they got used to this and completed their team  activity. Still, there were others who enjoyed the opportunity to  perform a task with students they normally would not group with.   5.2.3 Visual analytic - self and team awareness  building  Students were provided with a personal micro-profile of their  teamwork competency based on self and peer ratings. We had  feedback relating to the accuracy of students ratings, as well as the  clarity and interpretation of the visual analytic.  Some students questioned the accuracy of the peer ratings as they  felt that certain team members may not have rated them honestly.  Similarly, teachers also felt that certain students might not have  rated their team members accurately. This brings into question  that perhaps students need to be taught how to rate others.  Nevertheless, most students generally agreed with their self and  peer ratings and during the session there were very little questions  addressed to the facilitator regarding this. Moreover, during the  earlier stage, students were also instructed to rate their friends  fairly.     As for the visual analytic, there were 3 parts to the analytic, the  radar chart comparing self, peer, and overall; the numbers shown  in a table; and an overall similarity score.  Regarding the dimensions of teamwork, the facilitator explained  and provided realistic examples to students which helped in  students becoming more aware of what the concepts meant.  Students found the radar chart a powerful visual comparison  between self, peer and overall ratings. They could see how their  peers thought of them as compared to how they thought of  themselves during the task. A student felt that this radar chart  could be shown as individual charts, in addition to the comparison  chart to make it even clearer.  The table was also useful to see the actual numbers. A teacher  remarked, my class is quite analytical; they like to see the small  details. Students liked it when they scored highly on the  dimensions.  The most confusion was regarding the overall similarity score as  the score had negative values. Students were not used to a  negative number and tended to interpret that as a negative aspect  of their teamwork. Facilitators had to repeatedly explain that it  was the magnitude that mattered, not so much the valence. This  value can probably be improved in the future with a more user- friendly visual.   5.2.4 Self and team reflection and sensemaking  In this stage, students were guided in their reflection and  sensemaking with a reflection worksheet with four questions as  mentioned earlier. Students written individual reflections ranged  widely. There were students who agreed with their peer ratings:   I do [agree with my peers concerning my rating] as I spammed  the group.  I think that maybe my teammates didnt see that I was  committed to the task so they rated me lower than I did. My team  might have a less biased point of view and may actually be more  accurate.  There were also students who disagreed with their peer ratings:  I dont agree with them as they have different views.  I dont agree because I did contribute a fair bit.  Many students felt that their peers rated them higher than they  actually should, I dont agree with my peers rating as I should  have got [a] lower [score] because I did not contribute a lot.  However, there were other students who seemed to care less or  were more philosophical in their responses. One student wrote  the ratings do not matter to me while another explained I do  not mind what they rate me as I think that it does not matter that  much and I think that since they do not know me that well, what  they rate might be wrong or might be true, there is no definite  answer.  For the team reflections, we found that many students wrote the  same answer for the whole team in their reflections, suggesting a  consensus in their team reflection. E.g., I want to be more helpful  and communicate better was written by all members of team G.  Some students were slightly vague or did not answer the question,  for instance, be more supportive and have more teamwork.  There were others too that felt that they did not need to change  and could function as what they had functioned.  Nevertheless, most students reflected and stated specific goals to  improve future team performance, for instance:   I will try to make peace with everyone and try to get them to  discuss and give opinions. When the team is not discussing the  topic, I will remind them to stay on task.  I can cooperate better with my teammates and listen to their  views more often. I will be committing more to the team and be  more active. I will be giving suggestions to the members to  improve the answer and participate in the discussion more.  We would put all our differences aside and work together as a  team.  These positive goal-directed responses indicated that students  became aware and understood how they could grow their  teamwork competency.  Overall, the activity for this stage was slightly more difficult for  students. Students were not as responsive as compared to the  earlier stages. During the FGD, a student shared that he found the  reflection questions straightforward but explained that it was  hard to think of something, to write something down when  he did not have any opinion of it.  Another student, student X,  stated that students need to know the purpose of the reflection and  suggested having more examples to help them reflect, and also to  structure it in a format likened to that of a classroom discussion.   Still, most students during the FGD shared that they were clear  about the six dimensions of teamwork and the activity helped  them to understand more about teamwork. One student shared that  she became more aware that she did not know how to share and  explain her ideas and the reasons for why she disagreed with her  teammates. This is the area she would like to work on to enhance  her teamwork competency.  These findings suggested that students individually and as a team  were able to make-sense of the dimensions of teamwork  competency and set goals to change.    5.2.4.1 Teachers views of the reflection and  sensemaking stage  A teacher acknowledged that while important, students are not  used to reflecting and expressing their opinions. He found that this  was also the case in other lessons as students were not prepared or  reluctant to share their opinions. He attributed it to the students  maturity and believed that it would require a lot of time and effort  to develop the metacognitive skills of the students.  Other teachers were less skeptical and felt that this part of the  program could be emphasized more. Teacher A commented that it  served two purposes. It was useful for students to know  themselves, and it sets them to think about how to work in a  group. It was also useful to the teachers because we are able to  better identify what kind of team players our kids are. With that  we can customize grouping to increase efficacy and learning.  Teacher C felt that there could be more room for discussion to  allow all groups a chance to present to know what students  are thinking. She was concerned that the visual analytic could be  too remote and suggested allocating more time for reflections  over a series of lessons to give teachers time to analyze students  micro-profile, in order to provide more specific advice to  students, so that students can work better in their teams.  Similarly, another teacher commented that students might know  the number [the survey scores in their micro-profile] but they  need more time to digest the significance so that it can be useful.  On the whole, the findings pointed to the fact that teachers  recognized the challenges of implementing the reflection and  sense-making stage, but at the same time, they generally  concurred on the importance of the activity and highlighted that     more time was needed for students to reflect and make greater  sense of their teamwork competency.   6. DISCUSSION, IMPLICATIONS AND  FUTURE WORK  The findings reveal both the challenges and potentialities  associated with the trial implementation of the TSDL pedagogical  framework in a teamwork competency awareness program. Both  students and teachers were found to be generally receptive  towards each stage of the framework, despite identified  challenges. In this regard, there was evidence that the broad goals  of the program were met, in that students were able to (a) gain  awareness of their personal teamwork competency and (b) state  possible ways to improve their teamwork. The pedagogical  framework contributed to a large extent in overtly scaffolding the  activities, which in turn points to the pedagogical value and  usefulness of the model.  In the beginning of this paper, we argued for a more explicit  pedagogical model for LA. Our work demonstrates one such  model where the pedagogical activities were planned as stages  according to the TSDL framework. Moreover, in the framework,  LA served as a visual analytic to build the awareness of students  teamwork competency, and for subsequent reflection and  sensemaking. This work spurred us to consider two questions: Is  such a directed theoretical model necessary Would this be  considered a good alignment of LA and learning design  To this end, we would argue that such explicit models are  necessary, and that these frameworks should show good  alignment of LA and the learning design, echoing the works from  [24; 31]. Our findings indicated some advantages of such a stance.  The framework provided the general direction for the program  and theoretical clarity of the learning process. The LA and  learning design was also adequately coupled and prevented  serious misalignments in implementation. More importantly, we  saw that the use of the TSDL framework brought about perceptual  change in students teamwork competency, meeting the goals of  the program.  Our findings also revealed that different aspects of the  pedagogical framework were welcomed by students and teachers.  Students enjoyed the team-based concrete experience, and found  the reflection and sense-making activity difficult. On the other  hand, teachers recognized the reflection and sense-making stage  as important and wanted more time for their students to fully  engage in this.  The depth and duration for the implementation of the different  stages in the TSDL framework is an area that the research team  found challenging. We were constrained by the amount of  curriculum time that the school provided us with to carry out the  program. The research team was cognizant of the trade-off of time  especially for the reflection and sensemaking stage. We  acknowledge that more time should be provided for the reflection  stage, where possible, and will take this into account in the  planning and design of our follow-up trial cycle iterations.  Relating to this issue of curriculum time, we are planning for  greater integration of our program with the schools normal  curriculum. We hope to embed the TSDL framework into a  curriculum subject that employs collaborative inquiry tasks. Plans  are underway but this integration would inadvertently require  other types of concrete experiences, visual analytics as well as  reflection and sensemaking activities. This is complex and  challenging, at the same time offering more room for research, as  requiring careful and principled execution. The principle of   integration as conceptualized in [44] would be helpful in tying  analytics to the curriculum and authentic learning goals. We also  foresee that we might need to develop more fine-grained  instructional activity for each stage of the framework for different  contexts, as guided by the overarching theoretical frame.   At this stage in our research, we only managed to employ  dispositional analytics. The plan is to include other forms of  analytics, with the upcoming phase being discourse analytics.  However, the nature of semi-automated text analysis has been  tedious and challenging for the research team to date. The team is  in the process of devising a reliable analytic engine for the  indicators of teamwork dimensions. The discourse analytics will  add another layer to the existing visual analytic, such as through a  scaled score of the sum of each coded message of each dimension  in the micro-profile. Also, for the dispositional analytics, we are  working to improve the scale validation results of the  questionnaire items and also to enable a real-time system. Besides  the discourse layer, we see potential in using trace data of the  students online usage (such as searches, browsing websites) and  also in identifying the network of interactions among students to  provide further evidences of teamwork and collaboration.   In this study, we focused on TSDL framework for students, but  the teachers were also involved in the whole process as  facilitators. The TSDL framework can also be theoretically  extended for teachers such that teachers are provided with a clear  set of principles of their role in the learning process. In our  implementation, teachers were provided with a class micro-profile  to see their students teamwork competency scores and help them  flag out students that might need early or adaptive intervention.  This was well received by teachers, many of whom found the  class micro-profile to be a useful form of validating their more  tacit and/or intuitive observations of their students. Many teachers  were able to guess the names of the students before they saw the  actual names. Further work would go toward equipping teachers  with learning design guidelines especially in this area of  teamwork competency.   This study proposed and implemented a pedagogical framework  focused on the 21st century competency of teamwork. The  findings are limited to one specific instance. Still, the findings  lean towards a collective appreciation for the pedagogical  usefulness of the program and TSDL framework, although there  are challenges to be addressed. This serves as the impetus for us  to move forward by taking into account the students and  teachers suggested refinements to further improve and develop  the program. There may be potential for the TSDL framework to  be applied to other team-related outcomes too, such as other  cognitive skills and knowledge, which constitutes an area that  may benefit from future research.   7. CONCLUSION  Many pedagogical models in LA papers are implicit which could  result in misaligned practice. This paper presents an explicit  pedagogical model for teamwork competency, the TSDL  framework, and describes its implementation and evaluation by  students and teachers in the context of collaborative inquiry tasks.  The framework was implemented in a teamwork competency  awareness program for 7 classes of 14 year old students. This  paper qualitatively evaluates the program from students and  teachers perspectives. Findings reveal positive perceptions of the  stages of the framework suggesting its pedagogical value. Some  challenges associated with its implementation within school-based  learning contexts were also highlighted. In light of the findings,     we make the case that this framework goes some length to provide  theoretical clarity of the learning process, and also aligns learning  analytics and the learning design. The current work provides trial  outcomes of a teamwork competency awareness program that  used dispositional analytics, and further efforts are underway to  develop the discourse layer of the analytic engine. Future work  will also be dedicated to application and refinement of the  framework for other contexts and participants, both learners and  teachers alike.   8. ACKNOWLEDGMENTS  This paper refers to data and analysis from the following research  projects: NRF2015-EDU001-IHL08 (funded by the Singapore  National Research Foundation, eduLab Research Program),  OER62/12EK and OER09/15EK (funded by the Education  Research Funding Program, National Institute of Education,  Nanyang Technological University, Singapore). The views  expressed in this paper are the authors and do not necessarily  represent the views of the National Institute of Education.    9. REFERENCES  [1] Ali, L., Hatala, M., Gaevi, D., and Jovanovi, J., 2012.   A qualitative evaluation of evolution of a learning  analytics tool. Computers & Education 58, 1, 470-489.   [2] Buckingham Shum, S. and Deakin Crick, R., 2012.  Learning dispositions and transferable competencies:  pedagogy, modelling and learning analytics. In  Proceedings of the 2nd International Conference on  Learning Analytics and Knowledge ACM, 92-101.   [3] Chiu, M.M., 2008. Effects of argumentation on group  micro-creativity: Statistical discourse analyses of algebra  students collaborative problem solving. Contemporary  Educational Psychology 33, 3, 382-402.   [4] Crowston, K., Allen, E.E., and Heckman, R., 2011. Using  natural language processing technology for qualitative  data analysis. International Journal of Social Research  Methodology 15, 6 (2012/11/01), 523-543. DOI=  http://dx.doi.org/10.1080/13645579.2011.625764.   [5] Dawson, S. and Siemens, G., 2014. Analytics to literacies:  The development of a learning analytics framework for  multiliteracies assessment. In The International Review of  Research in Open and Distributed Learning, 285-305.   [6] Drach-Zahavy, A. and Somech, A., 2002. Team  heterogeneity and its relationship with team support and  team effectiveness. Journal of Educational Administration  40, 1, 44-66.   [7] Duffy, T. and Jonassen, D., 1992. Constructivism and the  technology of instruction: a conversation. Lawrence  Erlbaum Associates Publishers, Hillsdale, N.J.   [8] Duval, E., 2011. Attention please!: learning analytics for  visualization and recommendation. In Proceedings of the  1st International Conference on Learning Analytics and  Knowledge ACM, 9-17.   [9] Erkens, G. and Janssen, J., 2008. Automatic coding of  dialogue acts in collaboration protocols. International  Journal of Computer-Supported Collaborative Learning  3, 4, 447-470.   [10] Ferguson, R., 2012. Learning analytics: drivers,  developments and challenges. International Journal of  Technology Enhanced Learning 4, 5-6, 304-317.   [11] Ferguson, R. and Buckingham Shum, S., 2012. Social  learning analytics: five approaches. In Proceedings of the  2nd international conference on learning analytics and  knowledge ACM, 23-33.   [12] Freeman, M. and Mckenzie, J., 2002. SPARK, a  confidential webbased template for self and peer  assessment of student teamwork: benefits of evaluating  across different subjects. British Journal of Educational  Technology 33, 5, 551-569.   [13] Garrison, D.R., Anderson, T., and Archer, W., 2001.  Critical thinking, cognitive presence, and computer  conferencing in distance education. American Journal of  distance education 15, 1, 7-23.   [14] Greller, W. and Drachsler, H., 2012. Translating learning  into numbers: A generic framework for learning analytics.  Journal of Educational Technology & Society 15, 3, 42- 57.   [15] Hung, D. and Nichani, M., 2001. Constructivism and e- learning: Balancing between the individual and social  levels of cognition. Educational Technology 41, 2, 40-44.   [16] Janssen, J. and Bodemer, D., 2013. Coordinated  Computer-Supported Collaborative Learning: Awareness  and Awareness Tools. Educational psychologist 48, 1  (2013/01/01), 40-55. DOI=  http://dx.doi.org/10.1080/00461520.2012.749153.   [17] Janssen, J., Erkens, G., and Kirschner, P.A., 2011. Group  awareness tools: Its what you do with it that matters.  Computers in Human Behavior 27, 3, 1046-1058.   [18] Jrvel, S., Kirschner, P.A., Panadero, E., Malmberg, J.,  Phielix, C., Jaspers, J., Koivuniemi, M., and Jrvenoja, H.,  2015. Enhancing socially shared regulation in  collaborative learning groups: designing for CSCL  regulation tools. Educational Technology Research and  Development 63, 1 (2015/02/01), 125-142. DOI=  http://dx.doi.org/10.1007/s11423-014-9358-1.   [19] Kayes, A.B., Kayes, D.C., and Kolb, D.A., 2005.  Experiential learning in teams. Simulation & Gaming 36,  3 (September 1, 2005), 330-354. DOI=  http://dx.doi.org/10.1177/1046878105279012.   [20] Kirschner, P.A., Strijbos, J.-W., Kreijns, K., and Beers,  P.J., 2004. Designing electronic collaborative learning  environments. Educational Technology Research and  Development 52, 3, 47-66.   [21] Knight, S., Shum, S.B., and Littleton, K., 2014.  Epistemology, assessment, pedagogy: where learning  meets analytics in the middle space. Journal of Learning  Analytics 1, 2, 23-47.   [22] Koh, E., Hong, H., and Seah, J., 2014. An Analytic Frame  and Multi-method Approach to Measure Teamwork  Competency. In 14th International Conference on  Advanced Learning Technologies (ICALT) IEEE, Athens,  264-266. DOI= http://dx.doi.org/doi:  10.1109/ICALT.2014.82.   [23] Kolb, D.A., 1984. Experiential learning: Experience as  the source of learning and development. Prentice-Hall  Englewood Cliffs, NJ.   [24] Lockyer, L., Heathcote, E., and Dawson, S., 2013.  Informing pedagogical action: Aligning learning analytics  with learning design. American Behavioral Scientist 57,  10, 1439-1459.   [25] Loughry, M.L., Ohland, M.W., and Moore, D.D., 2007.  Development of a Theory-Based Assessment of Team  Member Effectiveness. Educational and Psychological  Measurement 67, 3, 505-524. DOI=  http://dx.doi.org/10.1177/0013164406292085.   [26] Mayne, L., 2012. Reflective writing as a tool for assessing  teamwork in bioscience: Insights into student performance   http://dx.doi.org/10.1080/13645579.2011.625764 http://dx.doi.org/10.1080/00461520.2012.749153 http://dx.doi.org/10.1007/s11423-014-9358-1 http://dx.doi.org/10.1177/1046878105279012 http://dx.doi.org/doi: http://dx.doi.org/10.1177/0013164406292085   and understanding of teamwork. Biochemistry and  Molecular Biology Education 40, 4, 234-240.   [27] Meier, A., Spada, H., and Rummel, N., 2007. A rating  scheme for assessing the quality of computer-supported  collaboration processes. International Journal of  Computer-Supported Collaborative Learning 2, 1, 63-86.   [28] Phielix, C., Prins, F.J., and Kirschner, P.A., 2010.  Awareness of group performance in a CSCL-environment:  Effects of peer feedback and reflection. Computers in  Human Behavior 26, 2, 151-161. DOI=  http://dx.doi.org/10.1016/j.chb.2009.10.011.   [29] Phielix, C., Prins, F.J., Kirschner, P.A., Erkens, G., and  Jaspers, J., 2011. Group awareness of social and cognitive  performance in a CSCL environment: Effects of a peer  feedback and reflection tool. Computers in Human  Behavior 27, 3, 1087-1102.   [30] Prins, F.J., Sluijsmans, D.M., Kirschner, P.A., and  Strijbos, J.-W., 2005. Formative peer assessment in a  CSCL environment: A case study. Assessment &  Evaluation in Higher Education 30, 4, 417-444.   [31] Rodrguez-Triana, M.J., Martnez-Mons, A., Asensio- Prez, J.I., and Dimitriadis, Y., 2015. Scripting and  monitoring meet each other: Aligning learning analytics  and learning design to support teachers in orchestrating  CSCL situations. British Journal of Educational  Technology 46, 2, 330-343.   [32] Ros, C.P., Wang, Y.-C., Cui, Y., Arguello, J., Stegmann,  K., Weinberger, A., and Fischer, F., 2008. Analyzing  collaborative learning processes automatically: Exploiting  the advances of computational linguistics in computer- supported collaborative learning. International Journal of  Computer-Supported Collaborative Learning 3, 3  (2008/09/01), 237-271. DOI=  http://dx.doi.org/10.1007/s11412-007-9034-0.   [33] Rummel, N., Spada, H., and Hauser, S., 2009. Learning to  collaborate while being scripted or by observing a model.  International Journal of Computer-Supported  Collaborative Learning 4, 1, 69-92.   [34] Salas, E., Rosen, M.A., Burke, C.S., and Goodwin, G.F.,  2009. The wisdom of collectives in organizations: An  update of the teamwork competencies. In Team  Effectiveness in Complex Organizations. Cross- Disciplinary Perspectives and Approaches, E. SALAS,  G.F. GOODWIN and C.S. BURKE Eds. Routledge/Taylor  & Francis Group, New York, 39-79.   [35] Salas, E., Sims, D.E., and Burke, C.S., 2005. Is there a  Big Five in teamwork Small Group Research 36, 5,  555-599.   [36] Strijbos, J.-W., Martens, R.L., and Jochems, W.M.G.,  2004. Designing for interaction: Six steps to designing  computer-supported group-based learning. Computers &  Education 42, 4, 403-424. DOI=  http://dx.doi.org/http://dx.doi.org/10.1016/j.compedu.2003 .10.004.   [37] Valentine, M.A., Nembhard, I.M., and Edmondson, A.C.,  2012. Measuring teamwork in health care settings: A  review of survey instruments. Harvard Business School.   [38] Van Den Bossche, P., Gijselaers, W.H., Segers, M., and  Kirschner, P.A., 2006. Social and Cognitive Factors  Driving Teamwork in Collaborative Learning  Environments: Team Learning Beliefs and Behaviors.  Small Group Research 37, 5 (October 1, 2006), 490-521.  DOI= http://dx.doi.org/10.1177/1046496406292938.   [39] Verbert, K., Duval, E., Klerkx, J., Govaerts, S., and  Santos, J.L., 2013. Learning analytics dashboard  applications. American Behavioral Scientist 57, 10, 1500- 1509.   [40] Voogt, J., Erstad, O., Dede, C., and Mishra, P., 2013.  Challenges to learning and schooling in the digital  networked world of the 21st century. Journal of Computer  Assisted Learning 29, 5, 403-413.   [41] Vygotsky, L.S., 1978. Mind and society: The development  of higher mental processes Cambridge, MA: Harvard  University Press.   [42] Whitelock, D., Twiner, A., Richardson, J.T., Field, D., and  Pulman, S., 2015. OpenEssayist: a supply and demand  learning analytics tool for drafting academic essays. In  Proceedings of the Fifth International Conference on  Learning Analytics And Knowledge ACM, 208-212.   [43] Willey, K. and Freeman, M., 2006. Improving teamwork  and engagement: the case for self and peer assessment.  Australasian Journal of Engineering Education 12, 2-19.   [44] Wise, A.F., 2014. Designing pedagogical interventions to  support student use of learning analytics. In Proceedings  of the Fourth International Conference on Learning  Analytics And Knowledge ACM, 203-211.   [45] Yukawa, J., 2006. Co-reflection in online learning:  Collaborative critical thinking as narrative. International  Journal of Computer-Supported Collaborative Learning  1, 2, 203-228.        http://dx.doi.org/10.1016/j.chb.2009.10.011 http://dx.doi.org/10.1007/s11412-007-9034-0 http://dx.doi.org/http:/dx.doi.org/10.1016/j.compedu.2003.10.004 http://dx.doi.org/http:/dx.doi.org/10.1016/j.compedu.2003.10.004 http://dx.doi.org/10.1177/1046496406292938   1. INTRODUCTION  2. PEDAGOGICAL FRAMEWORKS FOR LEARNING ANALYTICS  3. RESEARCH PROBLEM AND STUDY CONTEXT: ASSESSMENT OF TWENTY-FIRST CENTURY COMPETENCIES  3.1 Teamwork competency conceptualization and analytics  3.2 Pedagogical model  3.2.1 Team and self diagnostic learning framework   Figure 1: Team and Self Diagnostic Learning Framework.   4. FRAMEWORK IN USE: A TRIAL TEAMWORK COMPETENCY AWARENESS PROGRAM  4.1 Team-based concrete experience  4.2 Self and team awareness building  Figure 2: Online chat environment.  Figure 3: Survey interface for self and peer rating.  Figure 4: Visual analytic of teamwork competency: teamwork competency micro-profile for a student.  4.3 Self and team reflection and sensemaking   5. PEDAGOGICAL FRAMEWWORK EVALUATION  5.1 Data sources and analysis method  5.2 Findings  5.2.1 Overall program  5.2.2 Team-based concrete experience  5.2.3 Visual analytic - self and team awareness building  5.2.4 Self and team reflection and sensemaking  5.2.4.1 Teachers views of the reflection and sensemaking stage     6. DISCUSSION, IMPLICATIONS AND FUTURE WORK  7. CONCLUSION  8. ACKNOWLEDGMENTS  9. REFERENCES   "}
{"index":{"_id":"11"}}
{"datatype":"inproceedings","key":"Cukurova:2016:AFC:2883851.2883900","author":"Cukurova, Mutlu and Avramides, Katerina and Spikol, Daniel and Luckin, Rose and Mavrikis, Manolis","title":"An Analysis Framework for Collaborative Problem Solving in Practice-based Learning Activities: A Mixed-method Approach","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"84--88","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883900","doi":"10.1145/2883851.2883900","acmid":"2883900","publisher":"ACM","address":"New York, NY, USA","keywords":"analysis framework, collaborative learning, practice-based learning, problem solving","abstract":"Systematic investigation of the collaborative problem solving process in open-ended, hands-on, physical computing design tasks requires a framework that highlights the main process features, stages and actions that then can be used to provide 'meaningful' learning analytics data. This paper presents an analysis framework that can be used to identify crucial aspects of the collaborative problem solving process in practice-based learning activities. We deployed a mixed-methods approach that allowed us to generate an analysis framework that is theoretically robust, and generalizable. Additionally, the framework is grounded in data and hence applicable to real-life learning contexts. This paper presents how our framework was developed and how it can be used to analyse data. We argue for the value of effective analysis frameworks in the generation and presentation of learning analytics for practice-based learning activities.","pdf":"An Analysis Framework for Collaborative Problem  Solving in Practice-based Learning Activities:    A Mixed-method Approach     Mutlu Cukurova1, Katerina Avramides1, Daniel Spikol2, Rose Luckin1, Manolis Mavrikis1   1London Knowledge Lab, UCL Institute of Education, University College London, London, United Kingdom  2 Faculty of Technology and Society, Dept. of  Media Technology, Malm University, Malm, Sweden   1{m.cukurova, k.avramides, r.luckin, m.mavrikis}@ioe.ac.uk, daniel.spikol@mah.se   ABSTRACT  Systematic investigation of the collaborative problem solving  process in open-ended, hands-on, physical computing design tasks  requires a framework that highlights the main process features,  stages and actions that then can be used to provide meaningful  learning analytics data. This paper presents an analysis framework  that can be used to identify crucial aspects of the collaborative  problem solving process in practice-based learning activities. We  deployed a mixed-methods approach that allowed us to generate  an analysis framework that is theoretically robust, and  generalizable. Additionally, the framework is grounded in data  and hence applicable to real-life learning contexts. This paper  presents how our framework was developed and how it can be  used to analyse data. We argue for the value of effective analysis  frameworks in the generation and presentation of learning  analytics for practice-based learning activities.   Categories and Subject Descriptors   Applied computing~Computer-assisted instruction      Applied computing~Collaborative learning      Applied computing~Computer-managed instruction   General Terms  Measurement, Design, Experimentation, Human Factors.   Keywords  Collaborative learning, problem solving, practice-based learning,  analysis framework   1. INTRODUCTION  Learning Analytics (LA) rely upon effective frameworks that  drive the analysis of data in a manner that answers the questions  posed by researchers and educators. These questions can be  diverse, as can the educational practices that are subject to  analysis. We are particularly interested in collaborative problem  solving processes in practice-based learning activities. Practice- based learning activities differ considerably in what they ask  students to do and what they are trying to teach. In the research   study reported here, we focus on open-ended, hands-on, physical  computing design tasks. This type of practice-based learning  activity is commonly used to improve collaborative problem  solving processes [13], which in turn improve learning in this type  of practice-based learning activities.  Practice-based learning activities have the potential to help  educators to achieve high tier institutional and policy goals such  as developing 21st century skills in STEM subjects at scale. They  are becoming increasingly popular in both secondary and post- secondary learning institutions, particularly after the introduction  of the Makers Movement [3]. However, 21st century skills  including collaborative problem solving skills are complex and  non-linear in nature. Hence, although practice-based learning  activities are widely recognised as an essential aspect of teaching  these skills in STEM subjects, both by educators and by  researchers [11]; our understanding of practice-based learning still  remains scant and there is still little agreement on how it should  be used effectively and how it should be supported [1].   LA is introducing a number of new techniques and frameworks  for studying learning in general, nonetheless practice-based  learning activities are one of those educational areas to which  learning analytics have yet to contribute significantly [18].  Systematic investigation of the collaborative problem solving  process in practice-based learning activities requires a framework  that highlights the main process features, stages and actions.  Those identified process actions and stages can then be used to  provide meaningful LA data. The main purpose of this paper is  to provide such a framework.    2. ANALYSIS FRAMEWORKS IN LA  The development of appropriate frameworks for analysing  students engagement processes is a key aspect of research in LA  [2, 7, 8, 10, 15]. Designing and validating analysis frameworks for  LA is a challenging task and requires expertise in multiple  research domains including educational design, educational  psychology, human-computer interaction and computer sciences.  There appear to be two fundamental approaches to the  development of analysis frameworks in the literature. The first  approach starts with a theoretical model. Researchers derive a  theoretical model of the process they are investigating, either from  the previous literature or from expert opinions (Delphi Method),  and deploy it to data generated from real world contexts to  validate, reshape or refute it (see for instance [5]). This approach  has the drawback of overlooking the essential aspects of what  makes a learning process unique. The second approach is to use a  grounded-theory approach. In a grounded-theory approach,  researchers attempt to ignore all prior assumptions and categories  and to describe the process as it emerges from the data (see for  instance [6]).  However, this approach could lead to generation of  very data-specific frameworks, which cannot be used in other     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  permissions@acm.org  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom     2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00    DOI: http://dx.doi.org/10.1145/2883851.2883900     contexts. We believe, a mixed methods approach (see for instance  [10]) could lead to more effective analysis frameworks that  highlight the main process features, stages and actions of a  learning process. They can be both theory-driven and therefore  broad enough to observe learning processes on the basis of  theoretical assumptions, and data-driven and therefore grounded  enough to be applicable to the real-life learning contexts.     2.1 Development Process of the Analysis  Framework  Here we present the methodology we adopted to develop a mixed- methods analysis framework for the collaborative problem solving  process in practice-based learning. There are three main stages:  identification of the theory-driven framework; adaptation of the  theoretical framework to fit the research purposes and merging the  fine-grained actions from the data to the adapted theoretical  framework (figure 1).       Figure 1: Three-stage development process      2.1.1 Identification of a theory-driven framework  from the literature  We started with a framework that was developed by the OECD to  assess collaborative problem solving competence [12]. This  framework met our requirements, because it is a rare attempt to  merge collaboration and problem solving skills in one framework.  The approach taken by the OECD also takes into account a very  broad educational context, because their focus is upon the  evaluation of knowledge and skills at an international comparison  level.  The OECD defines collaborative problem solving competency as  the capacity of an individual to effectively engage in a process  whereby two or more agents attempt to solve a problem by  sharing the understanding and effort required to come to a  solution and pooling their knowledge, skills and efforts to reach  that solution [12, p.6]. The definition identifies three core  competencies particularly related to collaboration and four  competencies particularly related to problem solving.  Competencies related to collaboration: 1. Establishing and  maintaining shared understanding; 2. Taking appropriate action to  solve the problem; 3. Establishing and maintaining team  organization. Competencies related to problem solving: 1.  Exploring and Understanding; 2. Representing and Formulating;  3. Planning and Executing; 4. Monitoring and Reflecting. These  competencies are then used as dimensions in the OECDs  assessment framework for collaborative problem solving (see  Table 1).       Table 1: Matrix of Collaborative Problem Solving for PISA   (1) Establishing   and maintaining  shared  understanding   (2) Taking  appropriate action  to solve the  problem   (3) Establishing and  maintaining team  organization      (A) Exploring and  Understanding   (A1) Discovering  perspectives and  abilities of team  members   (A2) Discovering  the type of  collaborative  interaction to solve  the problem, along  with goals   (A3) Understanding  roles to solve  problem         (B) Representing  and Formulating   (B1) Building a  shared  representation and  negotiating the  meaning of the  problem (common  ground)      B2) Identifying and  describing tasks to  be completed   (B3) Describe roles  and team  organization  (communication  protocol/rules of  engagement)       (C) Planning and  Executing   (C1)  Communicating  with team members  about the actions to  be/ being performed      (C2) Enacting plans   (C3) Following rules  of engagement, (e.g.,  prompting other team  members to perform  their tasks.)      (D) Monitoring  and Reflecting   (D1) Monitoring  and repairing the  shared  understanding   (D2) Monitoring  results of actions  and evaluating  success in solving  the problem   (D3) Monitoring,  providing feedback  and adapting the  team organization  and roles   2.1.2 Adaptation of the OECD framework to fit our  research purposes  The framework was originally developed for assessment purposes.  However, it does not meet the requirements for analysing the  process of collaborative problem solving, because it does not  include the component of knowledge deficiency.    Table 2: Combined Matrix that merges PBL and CPS  concepts adapted from PISA    (1) Establishing and  maintaining shared   understanding   (2) Taking  appropriate action   to solve the problem   (3) Establishing and  maintaining team   organization   (A) Identifying  facts   (A1) Discovering  perspectives and  abilities of team   members, making  knowledge explicit   (A2) Discovering the  type of collaborative  interaction to solve  the problem, along   with goals   (A3) Understanding  roles to solve problem   (B)  Representing  and  Formulating   (B1) Building a  shared representation  and negotiating the   meaning of the  problem (common   ground)   (B2) Identifying and  describing tasks to be   completed   (B3) Describe roles  and team organization   (communication  protocol/rules of   engagement)   (C) Generating  Hypotheses   (C1) Critically  analyzing the problem   representation   (C2) Generating and  Communicating   potential solution  paths   (C3) Present  Hypothesis, encourage  feedback from others  and offer feedback on   others hypotheses   (D) Planning  and Executing   (D1) Communicating  with team members  about the actions to  be/ being performed   (D2) Enacting plans (D3) Following rules  of engagement, (e.g.,  prompting other team  members to perform   their tasks.)   (E) Identifying  Knowledge and  Skill  Deficiencies   (E1) Comparing the  teams knowledge and   skills with the  proposed actions   (E2) Identifying and  specifying individual   deficiencies   (E3) Specification of  team deficiencies and  their relationship to  proposed problem   solution   (F) Monitoring,  Reflecting and  Applying   (F1) Monitoring and  repairing the shared   understanding   (F2) Monitoring  results of actions and  evaluating success in  solving the problem   (F3) Monitoring,  providing feedback   and adapting the team  organization and roles      IdentificationIdentification  AdaptationAdaptation  MergingMerging    Knowledge deficiency is an important aspect of the problem  solving process [4]. To address both the assessment and the  tuition aspects of the collaborative problem solving process, we  adapted and integrated the knowledge deficiency element of  Hmelo-Silver (2004)s tuition framework into the OECDs  assessment framework (Please see table 2). A detailed discussion  of this adaptation process as well as its implications were  discussed in a recent research conference [8].   2.1.3 Merging the data-driven fine-grained actions  of collaborative problem solving  A multi-step qualitative methodology was used, taking into  account the procedures and techniques developed in qualitative  content analysis methods [17]. Our dataset consisted of video  recordings from two different events,  a) A Hackathon event in which three groups of five secondary  school students (aged 14-15 years) were assigned to work on an  open-ended, hands-on project using physical computing (Arduino  modules1),  b) A workshop event in which two groups of adult pairs worked  on two different open-ended, hands-on physical computing  projects using an Arduino-like kit (SAM Labs2).  First, fine-grained actions from the data are identified. These were  observable actions that occurred during the collaborative problem  solving process. 31 fine-grained actions were identified. To  validate these actions, two coders, applied these 31 actions to a  different set of data collected from the same event. This procedure  was suggested as a way of testing the validity of actions generated  with a grounded theory approach by Mayring [9] as cited in Meier  et al. [10]. Where there was disagreement, the researchers  discussed the data and revised the categorisations accordingly.  Then, the emerging actions were assigned to the coordinates of  the theory-driven framework previously developed. First the fine- grained actions were assigned to three core competencies  particularly related to collaboration, and then they were assigned  to six competencies particularly related to problem solving.  Merging those two activities resulted with table 3.  We argue that the mixed methods approach to generating analysis  frameworks is a productive one. It helps to generate frameworks,  which are theoretically robust, and generalizable while being  grounded in data and applicable to real-life learning contexts. The  real value of this framework (table 3) is that it has observable  actions as its codes rather than broad definitions. Such broad  definitions are hard to identify, track and interpret in data analyses  both for human annotators and to a greater extent for machines,  which makes them hard to use in learning analytics research.  The analysis framework has been generated as part of a bigger  research project and our next step is to apply this framework at  scale using data collected from practice-based learning activities.  The goal is to observe if there are any fundamental differences in  different groups of students. Once we establish that machine  collected data correlates with the human analysis of the  collaborative problem solving process, we plan to use the  Learning Analytics System (LAS) we design to compare  successful students to novice ones, and STEM experts to STEM  students in order to map which actions and stages of the  collaborative problem solving process differ for those different  groups. We aim to use this information to generate visualisations                                                                    1 https://www.arduino.cc/  2 https://samlabs.me/   to support the collaborative problem solving process in open- ended, hands-on physical computing design tasks. It is important  to make it clear here that we do not hope to provide a measure of  determination but a way of effectively supporting practice-based  learning activities in STEM teaching. In this sense we see learning  analytics as a means of seeking to augment human intellect [14]  rather than define it.   Table 3: Combined Matrix that merges fine-grained actions  and theoretically suggested stages   3. DATA COLLECTION AND ANALYSIS  WITH THE FRAMEWORK  As has been mentioned, the framework was created in the context  of a larger EU project X (http://blinded.review), which is  commissioned by the EC 7th Framework Programme. The overall  aim of the project is to develop learning analytics tools for hands- on, open-ended STEM learning activities using the Arduino  platform.  Within the project we have generated tools that can  detect some of the fine-grained actions in the analysis framework.  Given the complexity of the collaborative problem-solving  process even at the fine-grained level identified in our analysis  framework, we cannot automatically track each of those actions  with the current technology. However, we can automatically track  some of the fine-grained actions, which are presented in this  section.       (1) Establishing and  maintaining shared  understanding   (2) Taking  appropriate  action to solve the  problem   (3) Establishing  and maintaining  team  organization   (A)  Identifying   facts   (A1) Vocalizing  knowledge; Confirming  shared understanding;   Communicating regarding  an answer to a question;   Asking questions to verify  a suggested solution;   presenting skills   (A2) Identifying a  problem (a   situation which  stops/hampers   students from the  natural progression   of the practice- based activity)        (A3) Confirming  the actions to be  taken, engaging   with rules     (B)  Representing   and  Formulating   (B1) Sharing the identified  problem with other   teammates; Explaining an  hypothesis/suggestion in   detail   (B2)  Communicating  about actions to   take   (B3) Assigning  roles to team  mates; Giving   responsibilities to  team mates     (C)   Generating  Hypotheses     (C1) Critically analyzing a   problem; Critically  analyzing a suggestion   (C2) Suggesting a  solution to a   problem;  Hypothesizing  regarding the   source of problem   (C3) Suggesting  an improved  version of an  hypothesis   suggested by  others          (D) Planning  and   Executing          (D1) Negotiating on  actions to take; Approving   a suggested solution          (D2) Taking  actions to progress   (D3) Prompting  other team  members to   perform their  tasks; Taking   actions related to  suggestions of   other teammates   (E)  Identifying  Knowledge  and Skill   Deficiencies   (E1) Identifying individual  deficiencies   (E2) Making  knowledge or skill  deficiency explicit   (E3) Identifying a  team mistake     (F)   Monitoring,  Reflecting   and Applying   (F1) Verifying what each  other knows; Asking   questions regarding the  actions being taken;  Observing an agreed  action being taken;   Observing the attempts of  another teammate to solve   a problem   (F2) Testing a  solution to check   its validity;  Reflecting on   previous actions;  Correcting simple  mistakes of others   (F3) Warning  teammates  regarding a   possible mistake     3.1 Learning Analytics System (LAS)  The LAS collects data from both ambient and live sources. The  ambient collection of data includes a computer vision system  integrated in specially designed furniture. The learning  environment is designed to foster collaboration and includes an  integrated screen and round shape to allow people to share and  work together.  A new Arduino electronics platform with plug-and  play electrical components, and visual programming tool has been  developed to allow learners to more easily and build and program  artifacts. The system collects what components are plugged in and  how the students manipulate the programming environment. A  lightweight mobile application has been developed to allow  learners to plan, document, and reflect on their projects with text  and multimedia. The mobile system also allows teachers to mark  critical incidents, and researchers to time stamp the different  stages of the learners project, based on the framework. This on the  fly first round of coding provides a foundation for the hand coding  of the video documentation post activity. A set of sentiment  buttons has been developed with thundercloud and sunshine icons  to allow the students to mark critical events in their activities. The  system is seen below in figure 2. The live data will provide the  self-declared data from students and teachers including the cycle  of planning, documenting, and reflecting through the mobile  system. The computer vision system and the log files of the  Arduino software collect the ambient data. This data set includes  the capture of objects, the positions of people, arm movement,  faces and audio levels from the vision system; and from the  Arduino the log files of components are collected as are the  manipulations in the visual programming platform. In table 4, the  detailed types of data collected, the instruments of collection, the  type of events, the description, the extraction methods as well as  fine-grained actions we aim to track are summarised.     Figure 2: LAS in action   The LAS is now up and running, and the first round of trials with  learners are being conducted. The framework presented in this  research paper is being used as a guide to design the algorithms  for processing and visualising the data.   4. DISCUSSION  The analysis framework we generated offers a flexible approach  to the analysis of data collected regarding the collaborative  problem solving process in practice-based learning activities.  Although this type of analysis usually needs to be completed by  humans, we see our framework as an initial step to generate useful  analysis frameworks for data collected and analysed by a  machine. At this point we make the distinction between  frameworks that can only be completed by humans as we strive to  understand how learning happens and those employed in data  analysis by a machine. However, this does not mean that they do  not contribute to our understanding of such learning processes.    Table 4: The detailed types of data collected, the instruments of collection, the type of events, description, the extraction methods,  and the codes       Data Collector Computer Vision System Arduino IDE (Visual Electronics  Platform)   Mobile System Sentiment Buttons Video Capture   Types of Data  /Events   DATA: Shared Gaze, Body Closeness,  hand tracking, object manipulation,  motion around the special furniture  and audio levels  EVENTS: people and objects   DATA: Number of components and  inputs  EVENTS: Arduino modules and  codes   DATA: Number of posts,  transitions between activities  EVENTS: self-documentation   DATA and  EVENTS: Critical  Incidents   Occasion of Codes/  Qualitative analysis   Brief Description More granular details of who around  the table and what their arms are  doing, how close are they to each  other, what are they are looking at, and  the motion around the table -location  of bodies, direction of gaze   How students designed and built the  tasks - types and number of  components and manipulation in the  IDE   How did the students plan,  document, and reflect on the task -  Teacher critical incident marks  Researcher coded activities   Marking of critical  incidents   Coding Annotating  moments on the  video/audio   Extraction  Methods   Frequency of hand movements   Time spent looking at screen   Time spent looking away   Frequency of components plugged in  Frequency of interactions with IDE   Number of Mobile submits  Number of words in text submitted  Duration of different activities           Content of text and multimedia   Frequency and  duration between  types of incidents   Rating of codes, Amount  of time   Types of Analysis Machine Machine Machine and human Machine and human Human   Fine-grained  actions aimed to  be tracked   (D2) Taking actions to progress,  (D3) Prompting other team members  to perform their tasks; Taking actions  related to suggestions of other  teammates, (F1) Observing an agreed  action being taken; Observing the  attempts of another teammate to solve  a problem   (A1) Presenting skills, (A2)  Identifying a problem (a situation  which stops/hampers students from  the natural progression of the  practice-based activity), (E2) Making  knowledge or skill deficiency  explicit, (E3) Identifying a team  mistake   (F1) Verifying what each other  knows; Asking questions regarding  the actions being taken; (F2)  Testing a solution to check its  validity; Reflecting on previous  actions; Correcting simple mistakes  of others, (F3) Warning teammates  regarding a possible mistake, (E1)  Identifying individual deficiencies,  (C1) Critically analyzing a  problem; Critically analyzing a  suggestion, (C2) Suggesting a  solution to a problem;  Hypothesizing regarding the source  of a problem, (C3) Suggesting an  improved hypothesis    (A2) Identifying a  problem (a situation  which stops/hampers  students from the  natural progression  of the practice-based  activity), (F3)  Warning teammates  regarding a possible  mistake   All identified fine- grained actions        In the same way that computers were not invented in order to  make word-processing or the Internet possible, but once the space  of possible computer applications was rendered accessible, design  processes went into overdrive creating all the software we now  rely on everyday, analysis frameworks for learning analytics and  associated tools have the potential to lead to the production and  use of effective technologies for teaching and learning.  One possible criticism of our mixed-methods approach could be  that there is a low possibility of researchers generating fine- grained actions from the data while bracketing out all a priori  assumptions and categories. This is obviously a much broader  question than the scope of this research study and it relates to all  research studies that use a grounded theory approach. It is clear to  us that somehow, every observation is theory-driven. We  recognize the difference between looking and identifying what  one looks at. Whilst the first one is a physical action, the latter  requires a theoretical framework and clearly the generation of  fine-grained actions from data requires the identification of  collaborative problem solving actions. However, our focus on the  data-driven, fine-grained actions is particular to the observable  actions of what we, as educators and researchers, think of as part  of the collaborative problem solving process.   5. CONCLUSIONS AND FUTURE WORK   In this paper we present the development of an analysis  framework for the collaborative problem solving process in  practice-based learning activities (Table 3). We then, briefly  present the learning analytics system we developed to collect data  regarding the fine-grained actions and dimensions of collaborative  learning processes from practice-based learning activities (Table  4). We argue that the three-stage development process we  describe in this paper is a useful one to generate analysis  frameworks that can be used to identify crucial aspects of the  learning process in practice-based learning activities. The stages,  dimensions and fine-grained actions of our analysis framework  can be used to provide meaningful LA data. Although, the  framework is generated in the context of practice-based learning  environments, we believe the development process we generated  can be used in different educational contexts particularly in face- to-face learning environments where detecting and tracking  student interactions is extremely challenging. Our next research  aim is to apply this framework at scale and generate learning  analytics data for teachers and students, which will support their  teaching and learning process during practice-based activities.    6. ACKNOWLEDGMENTS   This work was funded by the PELARS project (GA No. 619738)  under the Seventh Framework Programme of the European  Commission.    REFERENCES  [1] Abrahams, I., & Millar, R. (2008). Does practical work really   work A study of the effectiveness of practical work as a  teaching and learning method in school science. International  Journal of Science Education, 30(14), 1945-1969.    [2] Gibson, A., Kitto, K., & Wills, J. (2014). A cognitive  processing framework for learning analytics. Paper presented  at the LAK'14 Proceedings of the Fourth International  Conference on Learning Analytics and Knowledge, New  York. Doi: 10.1145/2567574.2567610   [3] Halverson, E. R., & Sheridan, K. M. (2014). The Maker  Movement in Education. Harvard Educational Review, 84(4),  495-506.   [4] Hmelo-Silver, C. E. (2004). Problem-based Learning: What  and How do Students Learn. Educational Psychology Review,  16(3).   [5] Jarvela, S., & Hakkinen, P. (2003). The levels of web-based  discussions: using perspective-taking theory as an analytical  tool. Cognition in a digital world, 77-95.   [6] Koschmann, T., Zemel, A., Conlee-Stevens, M., Young, N.,  Robbs, J., & Barnhart, A. (2003). Problematizing the Problem.  Computer-Supported Collaborative Learning, 2, 37-46.    [7] Lee, Y. C. E., Chan, C. K. K., & van Aalst, J. (2006).  Students assessing their own collaborative knowledge  building. Computer-Supported Collaborative Learning, 1, 57- 87.   [8] Luckin, R., Mavrikis, M., Avramides, K., & Cukurova, M.  (2015). Analysing Project Based Learning Scenarios to inform  the design of Learning Analytics: Learning from related  concepts. Paper presented at the AIED2015 Artificial  Intelligence in Education, Madrid.   [9] Mayring, P. (2003). Qualitative content analysis. Foundations  and techniques. Weinheim: Beltz.   [10] Meier, A., Spada, H., & Rummel, N. (2007). A rating scheme  for assessing the quality of computer-supported collaboration  process. Computer-Supported Collaborative Learning, 2, 63- 86.   [11]  Millar, R. (2004). The role of practical work in the teaching  and learning of science. High school science laboratories:  Role and vision.   [12] OECD. (2015). Draft Collaborative Problem Solving  Framework. Retrieved from  http://www.oecd.org/pisa/pisaproducts/Draft PISA 2015  Collaborative Problem Solving Framework .pdf   [13] Rummel, N., & Spada, H. (2005). Learning to Collaborate:  An instructional approach to promoting collaborative problem  solving in computer-mediated settings. Journal of the  Learning Sciences, 14(2), 201-241.  DOI:10.1207/s15327809jls1402_2   [14] Santos, J. L., Verbert, K., Govaerts, S., & Duval, E. (2011).  Visualising PLE usage. Paper presented at the EFEPLE11 1st  Workshop on Exploring the Fitness and Evolvability of  Personal Learning Environments.   [15] Scheffel, M., Drachsler, H., & Specht, M. (2015). Developing  an Evaluation Framework of Quality Indicators for Learning  Analytics. Paper presented at the LAK'15 Proceedings of the  Fifth International Conference on Learning Analytics and  Knowledge, New York.   [16] Segal, J. W., Chipman, S. F., & Glaser, R. (2014). Thinking  and Learning Skills (Vol. 1). London: Routledge.   [17] Spannagel, C., Glaser-Zikuda, M., & Schroeder, U. (2005).  Application of Qualitative Content Analysis in User-Program  Interaction Research. FQS Forum: Qualitative Social  Research, 6(2), 29.   [18] Worsley, M., & Blikstein, P. (2014). Analyzing Engineering  Design through the Lens of Computation. Journal of  Learning Analytics, 1(2), 151-186.    "}
{"index":{"_id":"12"}}
{"datatype":"inproceedings","key":"Drachsler:2016:PAD:2883851.2883893","author":"Drachsler, Hendrik and Greller, Wolfgang","title":"Privacy and Analytics: It's a DELICATE Issue a Checklist for Trusted Learning Analytics","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"89--98","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883893","doi":"10.1145/2883851.2883893","acmid":"2883893","publisher":"ACM","address":"New York, NY, USA","keywords":"data management, educational data mining, ethics, implementation, learning analytics, legal aspects, privacy, trust","abstract":"The widespread adoption of Learning Analytics (LA) and Educational Data Mining (EDM) has somewhat stagnated recently, and in some prominent cases even been reversed following concerns by governments, stakeholders and civil rights groups about privacy and ethics applied to the handling of personal data. In this ongoing discussion, fears and realities are often indistinguishably mixed up, leading to an atmosphere of uncertainty among potential beneficiaries of Learning Analytics, as well as hesitations among institutional managers who aim to innovate their institution's learning support by implementing data and analytics with a view on improving student success. In this paper, we try to get to the heart of the matter, by analysing the most common views and the propositions made by the LA community to solve them. We conclude the paper with an eight-point checklist named DELICATE that can be applied by researchers, policy makers and institutional managers to facilitate a trusted implementation of Learning Analytics.","pdf":"Privacy and Analytics  its a DELICATE Issue  A Checklist for Trusted Learning Analytics  Hendrik Drachsler   Welten Institute  Open University of the Netherlands   Heerlen, The Netherlands  hendrik.drachsler@ou.nl   Wolfgang Greller  University of Education   Vienna, Austria  wolfgang.greller@phwien.ac.at   ABSTRACT  The widespread adoption of Learning Analytics (LA) and  Educational Data Mining (EDM) has somewhat stagnated recent- ly, and in some prominent cases even been reversed following  concerns by governments, stakeholders and civil rights groups  about privacy and ethics applied to the handling of personal data.  In this ongoing discussion, fears and realities are often indistin- guishably mixed up, leading to an atmosphere of uncertainty  among potential beneficiaries of Learning Analytics, as well as  hesitations among institutional managers who aim to innovate  their institutions learning support by implementing data and  analytics with a view on improving student success. In this paper,  we try to get to the heart of the matter, by analysing the most  common views and the propositions made by the LA community  to solve them. We conclude the paper with an eight-point check- list named DELICATE that can be applied by researchers, policy  makers and institutional managers to facilitate a trusted imple- mentation of Learning Analytics.    CCS Concepts  Security and privacy  Privacy protections  General and  reference~Design  Security and privacy~Social aspects of  security and privacy  Security and privacy~Privacy protections   Applied computing~E-learning   Keywords  Learning Analytics, data management, educational data mining,  implementation, privacy, ethics, trust, legal aspects,    1. INTRODUCTION  In 2011, Learning Analytics has been hailed by the Horizon  Report [1] as a revolutionary game-changer for teaching and  learning. Rapid moves and developments by enthusiasts have,  however, slowed down recently due to rising concerns about the  impact analytics has on individuals, their identity and integrity, as  has already been identified early on by [2]. These second thoughts  about Learning Analytics, in our view, originate from and run in  parallel to the fears expressed in the wider context of Internet  safety, surveillance, and commercial exploitation of data and  labour on the Internet.   Since then, vivid academic discussions are taking place on how to  provide acceptable approaches for the institutional adoption of  Learning Analytics. A number of initiatives have been created to   address issues of ethics and privacy in relation to Learning  Analytics. At LAK15, the first workshop on Ethics and Privacy  in Learning Analytics (EP4LA) has been organised jointly by the  EU FP7 project Learning Analytics Community Exchange1 and  the SURF SIG Learning Analytics2, who also organised similar  events at other conferences in the Netherlands (Utrecht), US,  (Washington), and France (Paris).   How pertinent the issue is can be seen in prominent recent  examples like inBloom in the US [3] below. In spite of receiving  more than $100m of grant funding by the Gates and Carnegie  foundations, and despite the potential benefits, the inBloom  analytics system was closed down for good in April 2014, after  parents and pressure groups expressed sincere concerns about the  misuse of data, the repurposing of data for commercial interests,  as well as general safety from cyber-attacks. Another, very similar  case concerned the Snappet foundation in the Netherlands which  provided tablets to some 400 public primary schools with pre- installed apps for maths and language learning. Snappet re- purposed the collected usage data and used it to classify and  predict individual student success. It then provided schools with  information on educational interventions. However, an investig- ation by the national organisation for the protection of personal  data (CBP) identified the collected datasets as personal data that  needed to be treated according to the Dutch laws on privacy about  individuals. The fact that the data collected affected young  children and provided insights into their performance at school  has been used by the CBP to classify this data as highly sensitive  and demanding the highest privacy standards [4] below.   These two cases demonstrate how sensitive the issue of privacy  and ethical use of educational data is, particularly when dealing  with underage children. It has to be said, though, that the scares of  unprotected privacy do not confine themselves to minors, but  appear through all age groups. This shows that ignoring the fears  and public perception of the application of analytics, no matter  how benevolent the intent, can lead to a lack of acceptance,  protests, and even failure of entire Learning Analytics implemen- tations.   Acceptance of technological solutions using data in education  depends to a great extent on the data subjects being sufficiently  aware of the consequences of using the system, the validity and  relevance of the results obtained, and the level of transparency of  the data model  e.g., how data are collected, stored, processed  and shared (cf. the technology acceptance model [5]). A big  challenge for Learning Analytics in this respect is the complexity  of the data collection and algorithmic analysis processes. The  applied technologies are not trivial and it can be rather difficult to                                                                     1 http://www.laceproject.eu/blog/about-todays-ethics-and-privacy- in-learning-analytics-ep4la/  2 https://www.surfspace.nl/sig/18-learning-analytics/   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than the author(s) must be honored. Abstracting  with credit is permitted. To copy otherwise, or republish, to post on servers or  to redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from permissions@acm.org.    LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom  Copyright is held by the owner/author(s). Publication rights licensed to ACM.  ACM 978-1-4503-4190-5/16/04...$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883893   http://www.laceproject.eu/blog/about-todays-ethics-and-privacy-in-learning-analytics-ep4la/ http://www.laceproject.eu/blog/about-todays-ethics-and-privacy-in-learning-analytics-ep4la/ https://www.surfspace.nl/sig/18-learning-analytics/   provide non-technical educational stakeholders (learners, teachers,  managers, and external parties like education authorities or  parents) with an understanding of how and what data are being  collected, how they are processed, and how reliable the results of  the analysis are.   The above cases, and others, justify a thorough and open  discussion about the issues surrounding the safe and transparent  use of Learning Analytics in an educational context. To contribute  to this wider academic exchange is the aim and objective of this  paper. We approach this complex issue primarily from an  institutional and policy perspective with a view to provide some  guidance for educational managers, decision makers, and data  curators for K12, Higher Education, and work-based learning  when implementing privacy-conform and ethically agreed solu- tions for Learning Analytics. To this end, we developed an eight  point checklist named DELICATE that can serve as a reflection  aid.   In the remaining parts of this paper, and leading up to the men- tioned DELICATE checklist, we work our way through various  relevant criteria of ethics and privacy. We added the criteria from  each subsection into a matrix that formed the foundation for the  DELICATE checklist Our goal is to provide a practical tool that  can be used by implementers to quickly check the privacy risks  that the introduction of data processes could throw up, and how to  deal with them. We, therefore, first summarise the current state of  the art on ethics and privacy for Learning Analytics. Thereafter,  we try and clean up the substantial overlap in understanding  between these concepts, which often leads to confusion when  discussing the implications of Learning Analytics. After clarifying  the relevant working concepts, we analyse some of the most  prominent fears and doubts about Learning Analytics and try to  demystify them. Finally, we conclude the paper with the  DELICATE checklist that aims to provide a means to support  educational organisations in becoming trusted Learning Analytics  users.   2. RELATED WORK   In an empirical expert study, Scheffel et al. [6] identified data  privacy as the most important aspect for increasing quality and  trust in Learning Analytics. This followed an earlier survey  among the emerging Learning Analytics community [7], which  showed similar opinions, with about two thirds of the surveyed  experts believing that Learning Analytics will affect privacy and  personal affairs. Despite the apparent prominence of the issue in  public and academic thinking, only few papers have been  published in this area to date (e.g., [8][9][10][11][12]), and, even  fewer policies or guidelines regarding privacy, legal protection or  ethical implications [14][15] were developed and publicised.   While the law relating to personally identifiable information is  widely understood, there has been insufficient attention on priv- acy from a user-centred perspective to reassure users of proper  conduct when providing them with data services. There are no  clearly defined best practices for the anonymisation, reuse, storage  and security of educational data. Still, the need for an ethical and  privacy-wise acceptable data analysis has been widely acknow- ledged, including by some national authorities. In late 2015, the  SURF foundation of the Dutch Universities released a very  comprehensive review of the legal state of educational data in the  Netherlands that is in line with European law and shows ways on  how Learning Analytics can take advantage of educational data  without affecting the legal rights of individuals [13]. Kennisnet,  the Dutch innovation foundation for schools, has also set up an  initiative and consultation on privacy by design in a user-centric   approach3. User-centric design in this context aims at putting  the data subjects in control of the data. In previous attempts to  organise academic and educational consent in this area, the UK  CETIS institute published a consultation paper [16] and the JISC,  more recently, developed a code of practice for Learning  Analytics [14].   The user-centric aspect also underlies the most recent paper by  Slade & Prinsloo [11], which explores the topic from a student  vulnerability side using a substantial set of recent literature. Vul- nerability is an interesting perspective to take in the management  of privacy and ethics. Unfortunately, in their paper, it is not  explained what these vulnerabilities actually are in the context of  Learning Analytics. What are the concrete risks beyond, for  example, that students may get targeted advertisements How do  these vulnerabilities balance against others  non-analytic ones  (that is, for example, the vulnerability of dropping out of school  education) Furthermore, it is not clear from their elaboration  whether the authors see vulnerabilities as part of the  learning  contract  or fiduciary duty of the institution. Nevertheless, they  quite rightly state that privacy management goes beyond the  traditional binary of opting in or opting out and go on to propose  a framework for learner agency [ibid.] that explores facets of  control such as privacy self-management, timing and focus, and  contextual integrity. With this, they clearly demand a more pro- active engagement with students and stakeholders to inform and  more directly involve them in the ways individual and aggregated  data are being managed and used.    In another state-of-the-art approach, Steiner et al. [9] provide a  thorough and clearly articulated piece of work in the context of a  EU FP7 project called LEAs box4. After analysing previous  propositions in depth, they synthesise them into a privacy and data  protection framework with eight foundational requirements: (1)  data privacy; (2) purpose and data ownership; (3) consent; (4)  transparency and trust; (5) access and control; (6) accountability  and assessment; (7) data quality; and, (8) data management and  security. They conclude that technology and tools developed and  used in Learning Analytics contexts need to be in line with these  foundations and see them as fundamental requirements for a  proper code of conduct.   The review of related recent literature already provided us with a  rich set of criteria that have been added into the concept matrix  when compiling the DELICATE checklist for Learning Analytics  practioners. Elements from this part of the study included in the  checklist are questions relating to the purpose of the data  collection, data ownership and access, legal grounding, informed  consent, anonymisation, but also ethical aspects like the  transparency of the data collection process.   3. ETHICS, PRIVACY, AND LEGAL  FRAMEWORKS   In order to be able to discuss the challenges of ethics and privacy  for Learning Analytics, we first need a better understanding of  both concepts as well as their relationship towards each other.  Below, we will express our summarised views on what constitutes  ethics and what is typically meant when talking about privacy. In  short, we can say that ethics is a moral code of norms and  conventions that exists in society externally to a person, whereas                                                                     3https://www.kennisnet.nl/fileadmin/kennisnet/publicatie/Personal iseren_in_het_Leren_een_Internationale_Schets.pdf  4 http://www.leas-box.eu   https://www.kennisnet.nl/fileadmin/kennisnet/publicatie/Personaliseren_in_het_Leren_een_Internationale_Schets.pdf https://www.kennisnet.nl/fileadmin/kennisnet/publicatie/Personaliseren_in_het_Leren_een_Internationale_Schets.pdf http://www.leas-box.eu/   privacy is an intrinsic part of a persons identity and integrity. The  understanding of what constitutes ethical behaviour varies and  fluctuates strongly over time and cultures. Privacy, on the other  hand, is first and foremost context bound [16], in that it forms the  boundary of ones person or identity against other entities. Thus,  the understanding of privacy can diverge greatly between, e.g.,  persons living in large one-room family households and people  living in large space single-occupancies, even when they belong  to the same culture at the same time. Perceived violation of  privacy can occur, when the ethical code of the surrounding  society conflicts with the personal boundaries.    In common language and popular thinking, there exists a substan- tial overlap between ethics and privacy, which sometimes leads to  confusion when discussing the effects of Learning Analytics on  either of them. Both concepts have in common that they can be  approached from various perspectives, especially sociologically,  psychologically, and even philosophically and religiously. They  manifest themselves differently in a legalistic context and change  over time.   3.1 Ethics  Ethics is the philosophy of moral that involves systematising, de- fending, and recommending concepts of right and wrong conduct.  In that sense, ethics is rather different to privacy. In fact, privacy  is a living concept made out of continuous personal boundary  negotiations with the surrounding ethical environment.   Research ethics have become a pressing and hot topic in recent  years, first and foremost arising from discussions around codes of  conduct in the biomedical sciences such as the human genome  [19], but also, more recently, in the shape of responsible research  and innovation (RRI) which is being promoted by the European  Commission5.  Historically, the first basic written principles for ethical research  originated from the Nuremberg trials in 1949, and were used to  convict leading Nazi medics for their atrocities during the Second  World War [16]. This so-called Nuremberg Code is the first mani- fest for research ethics. It contains ten internationally recog- nised principles for the experimentation on humans:    1. Data subjects must be voluntary, well-informed, and  consent to their research participation.   2. The experiment should aim at positive results for  society.   3. It should be based on previous knowledge that justifies  the experiment.   4. The experiment should avoid unnecessary physical and  mental suffering.   5. It should not be conducted when there is any reason to  believe that it implies a risk of death or disabling injury.   6. The risks of the experiment should be in proportion to  (that is, not exceed) the expected humanitarian benefits.   7. Preparations and facilities must be provided that  adequately protect the subjects against the experiments  risks.   8. The staff who conduct or take part in the experiment  must be fully trained and scientifically qualified.   9. The human subjects must be free to immediately quit  the experiment at any point when they feel physically or  mentally unable to go on.                                                                     5 http://ec.europa.eu/programmes/horizon2020/en/h2020-  section/responsible-research-innovation    10. Likewise, the medical staff must stop the experiment at  any point when they observe that continuation would be  dangerous.   The Nuremberg Code stimulated a major initiative in 1964 to  promote responsible research on human subjects for biomedical  purposes in the Helsinki Declaration [20]. It represents a set of  ethical principles developed by the World Medical Association,  but it is widely regarded as the cornerstone in ethical human re- search. The Helsinki Declaration has later been developed into the  Belmont Report [21] in 1978 by the US National Commission for  the Protection of Human Subjects of Biomedical and Behavioural  Science. Both documents build on the basic principles of the  Nuremberg Code. Together, they provide the foundation for the  modern, ethically agreed conduct of researchers mainly in the  medical fields. These are nowadays also taken to apply to  technological research and data collection about human subjects.  The basic principles can be summarised as:    Voluntary participation in research;   Informed consent of the participants, and, with respect   to minors, the informed consent of their parents or  guardians;    Experimental results are for the greater good of society;   Not putting participants in situations where they might   be at risk of harm (either physical or psychological) as a  result of participation in the research;    Protected privacy and confidentiality of the information;    Option to opt-out;   To make it clear what ethical and what unethical research may  constitute it is helpful to recall some prominent examples of  unethical research from the past. Two infamous cases have been  the Milgram experiment at Yale University [22], and the Stanford  Prison experiment [23]. Those experiments are in their nature  very different to the evaluation of any data tools in the way they  caused harm to their participants. The negative impact of those  experiments has been very direct and even physical to the  participants of the experiments.  This is rather different to the effects that Learning Analytics  research has on its research subjects. For the sake of argument,  however, one could envisage dividing a class of students into  control and experimental groups for A/B testing, and then,  providing a positive stimulus to the participants of group A and a  negative stimulus to the others. Such an experimental setup could  be considered as unethical, as it would disadvantage group B.  Harm in such a case would not be physical, but by deprivation of  beneficial learning opportunities.  To prevent such negative impact from research and data usage,  ethical committees  or IRB as they are called in the US  are  charged with ensuring the protection of fundamental rights of all  subjects participating in experiments (i.e., humans, but also  animals). In most Western universities, nowadays, any human- subject research has to pass the ethical committee, and research  ethics has also become part of the training of young scientists.  From this, we dare say, that the risk of having unethical research  being conducted at a Western university can be considered rather  limited.   That being said, with the rise of Big Data and cloud computing  new ethical challenges emerged, spurring the call for ethical  committees at Big Data companies like facebook. A recent trigger  was the facebook contagion study from 2014 [24], where a team  of company researchers manipulated the newsfeed of over  650,000 facebook users without notification or informed consent.   http://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation http://ec.europa.eu/programmes/horizon2020/en/h2020-section/responsible-research-innovation   The reaction to this manipulation has been massive among the  user community and beyond. However, ethics is a volatile human  made concept and what we see after the facebook study, is, that  researchers now discuss the pros and cons of the study. Some  people argue that the study has indeed been unethical, but, at the  same time, contributed new insights into human behaviour [25].  In this context, it remains questionable whether self-regulation of  profit-oriented companies via ethical committees would work in  practice.   By contrast, as has been indicated in the previous paragraphs,  after decade-long debates and risk assessments, the public educa- tion and research systems are much more advanced in applied  ethics and reflective practice than private enterprises. Still, as the  above cases of inBloom and Snappet have shown, the educational  domain is extremely sensitive, and, therefore, universities and also  the increasing amount of private companies now operating within  education need to be very careful with using student data for  Learning Analytics.   It is important to mention, that ethical approval is typically only  required for experiments. If a system is rolled out directly and  becomes part of the operational infrastructure of an institution,  ethical approval is not always sought. The rollout of the new  system then has to comply with the privacy and data protection  laws according to national legislation.    While producing the DELCIATE checklist, this section on ethical  foundations provided several aspects that were added into the  matrix, e.g., explaining the added value and intent, balancing risks  versus benefits, informed consent, option to opt out, training and  qualification of staff.    3.2 Privacy  The right to privacy is a basic human right and an established  element of the legal systems in developed countries. Already in  1890, Warren & Brandeis [26] wrote an article about The Right  to Privacy, where they explained that privacy is the  right to be  let alone , and focused on protecting individuals. This right is  often challenged in the context of the yellow press with regards to  royals and celebrities.   The concept of privacy as a right to be let alone was further  developed by Westin in 1968 [27] who made it clear that new  technologies change the balance of power between privacy and  societal technologies. From this, Westin went on to specify  privacy as the right of informational self-determination and as a  vital part for restricting government surveillance in order to  protect democratic processes.    According to Westin [ibid.], each individual is continually  engaged in a personal adjustment process in which they balance  the desire for privacy with the desire for disclosure and interaction  with environmental conditions and social norms. Flaherty [28]  took the informational self-determination further and claimed that  networked computer systems pose a threat to privacy. He first  specified 'data protection' as an aspect of privacy, which involves   the collection, use, and dissemination of personal information .  This concept forms the foundation for fair information practices  used by governments globally. Flaherty promoted the idea of  privacy as information control. Roessler [29] later operationalised  the right to privacy across three dimensions: 1. Informational  privacy, 2. Decisional privacy, 3. Local privacy. It is important to  note that privacy is not the same as anonymity or data security.  They are related concepts that have an effect on privacy, but do  not represent privacy as such.   Another important aspect of privacy especially in the age of Big  Data is Contextual Integrity. Contextual Integrity is a concept that  has arisen in recent years to provide guidance on how to respond  to conflicts between values and interests, and to provide a  systematic setting for understanding privacy [30]. It is not  proposed as a full definition of privacy, but as a framework for  evaluating the flow of information between agents (individuals  and other entities) with a particular emphasis on explaining why  certain patterns of flow provoke public outcry in the name of  privacy (and why some do not). Contextual Integrity defines a  context specified by roles, activities, norms, and values that  interact with one another. The actors in this context are: senders,  receivers, subjects and the attributes are data fields.   Contextual Integrity is very much at odds with the Big Data  business model that actually aims to collect and integrate as many  data sources as possible and gain new insights from those data  through overarching mining and analyses. It uses data that has  been collected under different pretexts and circumstances. This  repurposing of data is totally against the concept of Contextual  Integrity as described.   In other works, Hildebrand [31] examines the concept of privacy  from a legal perspective and concludes that privacy concerns the  freedom from unreasonable constraints that creates the freedom to  reconstruct ones identity. Data mining might therefore be seen  as impacting the personal development of identity.   Again, this section on privacy has contributed to our DELICATE  matrix and checklist. Relevant things we added from it were: data  ownership and user control, transparency about data collection,  data protection, the need to renew a data contract as it is a  dynamic concept, repurposing vs. contextual integrity, control of  access to data, option to opt out.    3.3 Legal Frameworks  The European position towards Learning Analytics has been  expressed in the European Commissions report: New Modes of  Learning and Teaching in Higher Education [32]. In recommend- ation 14, the Commission clearly stated: Member States should  ensure that legal frameworks allow higher education institutions  to collect and analyse learning data. The full and informed con- sent of students must be a requirement and the data should only  be used for educational purposes, and, in recommendation 15:  Online platforms should inform users about their privacy and  data protection policy in a clear and understandable way. Individ- uals should always have the choice to anonymise their data.6  They base these recommendations on the EU Data Protection  Directive 95/46/EC [33], i.e., the European law on personal data  protection, comparable to the OECDs Fair Information Practice  Principles [34]. Both are widely accepted frameworks and are  mirrored in the laws of many U.S. states and other nations and  international organisations.   Directive 95/46/EC defines personal data as any information  relating to an identified or identifiable natural person ('Data Sub- ject'); an identifiable person is one who can be identified, directly  or indirectly, in particular by reference to an identification num- ber or to one or more factors specific to his physical, physio- logical, mental, economic, cultural or social identity. The Direct- ive is only applicable if automated processing of personal data is  employed or if it is part of a filing system (Article 3). This                                                                     6 http://ec.europa.eu/education/library/reports/modernisation-  universities_en.pdf   http://ec.europa.eu/education/library/reports/modernisation-universities_en.pdf http://ec.europa.eu/education/library/reports/modernisation-universities_en.pdf   means, in reverse, that if data is properly anonymised, the EU  directive 95/46/EC does not apply. However, experts around the  world are adamant that 100% anonymisation is not possible (see  Section 4.3 below).    With this Directive, the EC is following the previously mentioned  ethical frameworks such as the Nuremberg Code and the Helsinki  Declaration by demanding consent, legal compliance, and that  research should be beneficial for society. But it also specifies  some general obligations in Article 6 such as: Use of personal  data need to be:      processed fairly and lawfully;    for specified, explicit and legitimate purposes;    safeguarded from secondary use and further processing;    adequate, relevant and not excessive;    accurate and up to date;    stored no longer than necessary;   It is important to highlight that these principles are equally valid  whether or not the data subjects provided full consent to access  and use their data.  With Article 6, EU Directive 95/46/EC addresses a very important  aspect towards Big Data business models [35] in restricting the  use of data for limited purposes only. Big Data business models  are driven by the collection and storing of infinite amounts of data  without an expiry date and for later re-purposing. Thus, most of  the time, the data is being collected in different contexts, and  found later to be of benefit for other information needs. This is  diametrically opposed to existing data legislation like the EU  Directive 95/46/EC and the concept of Contextual Integrity as  explained in the above section on privacy. The reuse of collected  data for other (unspecified) purposes is against the information  rights of an individual.   The commercial habit of indiscriminate collection and repurpos- ing of data, therefore, strengthened the idea of adding another  legal aspect to the EU Directive 95/46/EC which entails the right  to be forgotten. This legal concept was put into effect on some  occasions in the European Union since 2006. It arose from the  desire of individuals to  determine the development of their life in  an autonomous way, without being perpetually or periodically  stigmatised as a consequence of a specific action performed in the  past.  [36]. Following the high-profile case of EU vs. Google in  2014, search data can now be removed on request by the user.  There are, however, serious challenges connected with the right to  removal of personal digital footprints and data shadows  the  former being the traces left by users in electronic systems, the  latter representing data about an individual left by others (e.g.  tagged photos in facebook) [37]. More often than not it is unclear  against whom such right can be claimed [ibid.]. Things are being  complicated even more as recent national and European legis- lation works towards more data storage for security purposes, as is  apparent in the European Data Retention Directive, or, more  formally known as Directive 2006/24/EC [38].   Besides the evolution of the legal frameworks that underlie  Learning Analytics, the sector of private education service  providers also started initiatives towards self-regulation in a move  to appease critics after the inBloom case. Some 187 K12 school  service providers in the US have signed a Privacy Pledge7 to  protect data gained from educational data subjects and especially  the school sector. In the UK, the JISC recently published a first                                                                     7 http://studentprivacypledge.org/page_id=45   draft of a Code of Practice [14] for Learning Analytics. One of the  examples mentioned in the Code is the Ethical use of Student  Data for Learning Analytics Policy which has been established at  the Open University UK [15]. The JISC Code of Practice and the  Policy by the Open University are good starting points and blue- prints for other implementations to follow. In the Netherlands too,  the SURF foundation recently released a guiding paper how to  treat educational data in a privacy conform way [13].   This section has been very influential on the compilation of our  DELICATE checklist. It reiterates items that we already came  across in previous subsections, e.g., informed consent, which has  been further strengthened through the legal frameworks men- tioned in this part. Other perspectives that have been added to the  DELICATE matrixes were: lifespan of data, legal grounding,  anonymisation, and, the requirement to limit the scope of the  analytics to what is essential and necessary to fulfil a defined  purpose.    4. FEARS TOWARDS LEARNING  ANALYTICS    Researchers and institutions dealing with Learning Analytics are  facing up to privacy as a big concern. Many people are not really  aware of the legal boundaries and ethical limits to what they can  do within the sphere of privacy protection. Institutions, on the one  hand, have a fiduciary duty and need to demonstrate care for the  well-being and positive development of students, leading them to  success in their studies [17]. On the other hand, there is wide- spread fear of negative consequences from the application of  Learning Analytics, such as negative press or loss of reputation  and brand value, or even legal liabilities. Despite the enormous  promise of Learning Analytics to innovate and change the educa- tional system, there are hesitations regarding, among other things,  the unfair and unjustified discrimination of data subjects; violation  of personal privacy rights; unintended and indirect pressure to  perform according to artificial indicators; intransparency of the  Learning Analytics systems; loss of control due to advanced  intelligent systems that force certain decisions; the impossibility  to fully anonymise data; safeguarding access to data; and, the  reuse of data for non-intended purposes. This is a non-exhaustive  list, but we need to take all concerns seriously if we are to  establish proper implementations for Learning Analytics.   From what has been said, one could conclude that privacy and  legal rights are a burden which we need to combat in the same  formal arena, perhaps by amending the laws. Some people  already argued for an adjustment of existing privacy rights to the  new age of Big Data, like facebook founder Mark Zuckerberg  when he famously stated: Privacy is dead! [42]. True, privacy  got heavily affected by the latest technology developments and  the ever rising computer processing power. But, in fact, these  basic human rights have been established for very good reasons  and we should firstly care to build technologies that support  human learning within the existing ethical and legal boundaries,  created many years before the Big Data boom, in the Belmont  report and Directive 95/46/EC [21][33].   Similarly, we would refrain from solving a weakness in a new  learning technology by proposing technical fixes or technological  solutions, such as standardisation approaches, e.g., the emerging  IMS Caliper specification8 (cf. also [12]). Instead, we prefer to see  this as a soft issue, rooted in human factors, such as angst,  scepticism, misunderstandings, and critical concerns. Within this                                                                    8 http://www.imsglobal.org/activity/caliperram    http://studentprivacypledge.org/page_id=45 http://www.imsglobal.org/activity/caliperram   paper, we, therefore, aim to provide some answers to the fog of  misunderstandings around privacy and legal obligations dealing  with the semantics of the concepts and translating them into clear  action points through the suggested DELICATE checklist. We  hope to show that ethical and privacy supported Learning  Analytics is indeed possible, and we would like to encourage the  Learning Analytics community to turn the privacy burden into a  privacy quality label.   There are a wide variety of anxieties expressed with regards to the  analysis, sharing and exploitation of personal data, including  learner records and digital traces. These are not confined to  education alone. Rather, such fears are transferred from other  environments (commercial, governmental, social) with little  differentiation of the respective domain. Compared to these, wed  argue, the institutional environments in education can be con- sidered as relatively safe havens. This is due to the considerate  nature of the education system being in charge of young peoples  personal development, the long-standing experience and practice  of ethical behaviours (including the promotion of such behaviours  to the learners), and the public scrutiny and transparency which  goes far beyond any other sector. Learners, therefore, should be  able to feel in a safe space, where they are allowed to make mis- takes without the fear of consequences or unnecessary publicity.   Below, we summarise the most widespread and most critical  topics, concerns or arguments against applied Learning Analytics.  These have been collected from participants of the earlier  mentioned workshop series EP4LA9 that took place at different  locations between October 2014 and March 2015. The organisers  received an overwhelming interest from all over the world, which  resulted in six international workshops on ethics and privacy with  legal and Learning Analytics experts who discussed current regul- ations and norms for most pressing questions. We later sourced  the results of the workshops to further refine our DELICATE  checklist.   4.1 Power-relationship, data/user exploitation  One of the criticisms levelled against analytics and Big Data in  general is the asymmetrical power relationship it entails between  the data controller and the data subject [17]. This can lead to a  feeling of being powerless and exploited. That this concern  reaches wider than Learning Analytics is demonstrated by events  like the 2015 summit of the International Society of Information  Sciences (#IS4IS) in Vienna, dedicated to the Information Soci- ety at the crossroads10. One key argument in this context was the  exploitation of digital labour, as is the case with facebook  which  is widely regarded as the worlds biggest advertising company   monetising user data and contributions for commercial profits  [40]. Another criticism is the fact that data models are not neutral,  but reflect and reversely influence an ideal identity. This means  that data subjects are given a (designed) data identity by others,  which can cause friction, especially in the context of the global  economic and cultural divisions between Western and developing  world: people are operating within structured power relations  that they are powerless to contest [41]. To a smaller extent there  is also such an asymmetry in power between Learning Analytics  providers and users.   While these concerns are real and serious, things present them- selves differently in an institutional learning context. Naturally,                                                                     9http://www.laceproject.eu/blog/about-todays-ethics-and-privacy-  in-learning-analytics-ep4la/  10 http://summit.is4is.org/is4is-summit-vienna-2015    there has always been an asymmetrical relationship between the  institution, the teacher, and the learner. However, the benevolent  fiduciary mission and walled garden of education should install  confidence and trust also in the use of digital assets and data.  Nevertheless, challenges here are the increased openness of teach- ing tools, using cloud services and social networks, as well as the  pressures to commercialise and commodify education [42].   One of the main objectives we strive for with the DELICATE list  and surrounding discussion is to encourage data intensive learning  providers to develop into Trusted Knowledge Organisations that  can demonstrate a responsible, transparent, and secure treatment  of personal data.    4.2 Data ownership  At present, there is no clear regulation for data ownership of any  party, i.e. neither the student, the university or a third party  provider. Data ownership is a very difficult legal concept. It is  assumed, that the digital footprints someone leaves behind belong  to the data subject, i.e. the users themselves, as long as it isnt  specified differently in the terms of service of the provider of the  digital system. An additional factor is that the data subject cannot  manage all those tons of data breadcrumbs on a daily basis. Thus,  data subjects are in need of having service providers take care of  the data storage and management. Furthermore, a single data  subject has no ownership rights on an aggregated data model  computed out of their digital footprints. But, according to EU  Data Protection Directive 95/46/EC article 12, the data subject  always has the right to know about all the information that has  been collected about them. Just recently, a law student from  Austria brought down the so-called Safe Harbour data transfer  agreement between the European Union and the United States  used by more than 4.000 companies, including Google, Facebook,  and IBM11. He won a law suit that took more than two years and  forced facebook and other Big Data providers to keep all data  collected from a data subject in the same country and provide an  overview of this data to the users12.  Data ownership becomes even more complex, however, when we  consider the processing of data. If there is a computational model  developed from a collection of data traces in a system, can a  student still opt-out of such a data model that is being generated  around their data traces In search for answers, data ownership  remains a very complicated issue that is mainly dominated by the  technical power the developers and service providers offer to the  data subjects. There are visions to change this power relationship  by enabling individuals to carry and curate their own datasets  through personal data stores. This idea parallels earlier such  user-centric solutions, like openID for generic authentication  across the web. It could fundamentally turn the whole data  movement in education around if it were to become real. One such  attempt has already been initiated with the MITs openPDS13,  which allows users to collect, store, and give fine-grained access  to their data all while protecting their privacy. Kristy Kitto has  made another attempt in 2015 in a blog post at the LACE project  where she made a call towards a manifesto for data ownership14.                                                                      11 http://www.reuters.com/article/2015/10/07/eu-ireland-privacy-  schrems-idUSL8N1272Z820151007   12 http://europe-v-facebook.org   13 http://openpds.media.mit.edu/   14 http://www.laceproject.eu/blog/towards-a-manifesto-for-data-  ownership/    http://www.laceproject.eu/blog/about-todays-ethics-and-privacy-in-learning-analytics-ep4la/ http://www.laceproject.eu/blog/about-todays-ethics-and-privacy-in-learning-analytics-ep4la/ http://summit.is4is.org/is4is-summit-vienna-2015 http://www.reuters.com/article/2015/10/07/eu-ireland-privacy-schrems-idUSL8N1272Z820151007 http://www.reuters.com/article/2015/10/07/eu-ireland-privacy-schrems-idUSL8N1272Z820151007 http://europe-v-facebook.org/ http://openpds.media.mit.edu/ http://www.laceproject.eu/blog/towards-a-manifesto-for-data-ownership/ http://www.laceproject.eu/blog/towards-a-manifesto-for-data-ownership/   For the DELICATE checklist we added from this section the  demands to ask for consent, and to strictly monitor who has access  to data.    4.3 Anonymity and data security  According to PricewaterhouseCoopers15, on average, around  117,339 attempts of information heists were made per day in  2014. These statistics reveal the severity of the problem of data  security. Such attacks bring even big players like Apple, Sony, or  Microsoft into trouble and they suffer from not being able to  protect their online systems in an adequate way. Cyber-attacks are  perhaps an even greater threat for universities and smaller  educational providers who have fewer resources to establish and  maintain appropriate data security measures. Naturally, this then  becomes an issue for the protection of student and staff Learning  Analytics data.   Anonymisation is often seen as the easy way out of data  protection obligations. Institutions use various methods of de- identification to distance data from real identities and allow analy- sis to proceed [43]. Most of the time, though, data controllers con- sider replacing identifiers as sufficient to make data anonymous.  But many studies have shown that this kind of anonymisation can,  at best, be considered as pseudonymisation [44]. Anonymised data  can rather easily be de-anonymised when they are merged with  other information sources. Famous approaches include de-anony- misation of medical datasets: names of patients were not included  but demographic information that could be linked to electoral  registers allowed the retrieval of names and contact information  for the medical records [45]. Among those relatively easy ap- proaches are also more computational ones as presented in  [46][47]. They showed how Netflix users could be re-identified  from the dataset of a Netflix competition by combining it with  data from the movie platform IMDB. Those and other examples  show that robust anonymisation is hardly achievable with the in- crease of computational methods, data integration and calculation  power. Some residual risk of identification has to be taken into  account for educational data and Learning Analytics. Data  security and encryption, therefore, has a vital role to play in the  acceptance of such systems and combined security strategies,  including anonymisation, can go a long way to protect individual  privacy from illegal access.   As a further measure to make data less sensitive over time and to  protect privacy, data degradation has been suggested whereby a  timestamp is introduced when data should be deleted in order to  prevent further use [48]. This approach lets data decay over time  and in that way protects privacy and informational self- determination of data subjects.   It is rather obvious that this section mainly contributed to the  criteria matrix of the checklist in the areas of anonymsation and  (physical) data security. In that way, it helped weighting the  criteria collected in previous sections. Furthermore, timestamping  data for expiry or access was seen as another reasonable point to  make.   4.4 Privacy and data identity  It can be argued that our (external) identity is made up by the sum  of our actions and appearances. Arora [44] critically contrasts the  two notions of system identity versus social identity, the latter  encapsulating who we are in our social environment, while the                                                                     15 http://www.pwc.com/gx/en/consulting-services/information-  security-survey/key-findings.jhtml    former represents the collected digital data. This has become even  more sensitive with the collection of personal biometrical  information. Logging, tracking and storing individuals online,  therefore, can be considered an intrusion into our identity and a  violation of our perceived selves, i.e. the information produced by  our actions and appearances  exploited for other purposes by  third parties without consent or even knowledge. Profiling actions  by companies and governments have been under fire for some  time. Their impact on the formation and declaration of a persons  own identity, the democratic society, and civic self-determination  have been highlighted by some scholars (e.g. [31][48]). Repurpos- ing and fragmentation of personal digital identities isnt the only  criticism raised in the context of personal integrity, but the fact  that individuals are typecast into data identities not of their own  choosing and measured against benchmarks, indicators, or  algorithmic stereotypes that are out of their control [41]. What is  more, if one doesnt fit into a data model, it is often applied on a  probabilistic basis [49].   Here again, the special relationship in education can ease the  problem. Students are in a learning contract with the institution  or training provider they sign up with. For the duration of this  relationship, the teacher and institution need to be trusted to act  responsibly and in the favour of its clients. This is unlike the  ambiguous commercial relationship that for-profit enterprises  have towards their investors and shareholders versus customers.   The development of DELICATE has been influenced by these  thoughts via the call for transparency of data design and collec- tion, control of data usage by third parties, and the data contract as  a dynamic concept that needs to be approved by the data subjects  in case of relevant changes.    4.5 Transparency and trust   It is often said that lack of transparency can cause unease and  concern with data subjects. However, it is rarely defined how this  transparency should manifest itself. Commercial providers like  Google keep their algorithms secret, and, yet, as long as results  are relevant and in line with users expectations, there is trust in  the service, despite it being a black box. On the other hand,  Google Takeout, which allows insight and export of the user  dataset from all Google services, is of no great use to ordinary end  users, as they are unable to understand or re-use that data. Here  then, again, we see the asymmetrical power relationship: while  transparency of the user leads to commodification of their data,  the reverse isnt true for large companies.   The Korean-born German philosopher Byung-Chul Han [50]  states that transparency turns social systems of trust into control  systems because information is too readily available. Learning  Analytics is perceived as making learners/teachers transparent and  open for criticism, while keeping the system itself opaque and out  of scrutiny. There is widespread anxiety in the education com- munity that data and information retrieved from data subjects may  be used against outliers, and, thus, leads to more conformity and  uniformity [2]. As such, analytics is perceived by some as an  engine for controlling and correcting behaviours.   The issue in education can best be tackled by being clear and open  about the purpose of data collection and the compliance with the  existing legal frameworks. A code of conduct can clarify to the  stakeholders and data subjects what the intentions for analytics  are, how long data is being kept, and what internal procedures are  available to contest negative consequences. As an additional mea- sure, the focus of analytics should be put on providing informa- tion for human decision making, prediction and self-reflection  rather than accountability. Playing analytics results back to the   http://www.pwc.com/gx/en/consulting-services/information-security-survey/key-findings.jhtml http://www.pwc.com/gx/en/consulting-services/information-security-survey/key-findings.jhtml   data subject and letting them decide for themselves, whether to  ask for pedagogic support and intervention or not, puts the learner  in control, something that is anyway desirable as an educational  outcome.   This section again emphasises the aim of DELICATE to establish  trusted Learning Analytics in support of all stakeholders to make  more of their educational eco-system rather than manifesting a  monitoring and surveillance system that will destroy any trusted  teacher-learner relationship and scares them of making mistakes  and learn from them.     5. INTRODUCING DELICATE  The DELCIATE checklist is derived from the intensive study of  the legal texts mentioned above, a thorough literature review and  several workshops with experts, e.g., the six EP4LA workshops  with over 100 experts involved16. Relevant information from  those sources has been entered into a matrix on ethics and privacy  criteria that are relevant for establishing trusted Learning  Analytics. The criteria that have been added to the matrix have  been further weighted in case they have been mentioned multiple  times in the various subsections of our studies. We used this  weighting to identify the importance of a criterion to be added to  the DELICATE checklist. In a later step, we sorted the collected  criteria into clusters and labelled them. Lastly, we presented these  clusters of criteria to a group of experts from the LACE project  for feedback and verification. In a final step, we designed the  DELICATE checklist into a memorable format in order to reach  the goal of providing a largely self-explanatory practical tool for  establishing trusted Learning Analytics within any data-driven  educational organisation.    We strongly believe that trusted Learning Analytics are a key  vision to establish a learning organisation that has the bravery to  talk about mistakes and failures and learn from them. In order to  establish this level of trust, regulations need to be in place that  guards the personal information rights but also empowers the  organisation to gain insights for its improvement. We believe that  the DELICATE checklist can be a practical means towards this  vision. It can be applied as a quality checklist to review if the data  processing procedures fulfil the trusted Learning Analytics regul- ations of an educational institution. Similar to medical checklists  or aviation checklists [51], it safeguards critical processes and  objectives by raising the attention and awareness of the involved  stakeholders and guides them through the procedure. The  DELICATE checklist can be used in the context of a value- sensitive design approach as specified by Friedman in 1997 [52].  Value-sensitive design is the idea that ethical agreements and  existing privacy laws need to be embedded where and when it is  relevant for the design and usage of a system like Learning  Analytics  starting early on in the design and implementation  process, and close to where the technology is being rolled out.   The final version of the DELICATE checklist contains eight  action points that should be considered by managers and decision  makers planning the implementation of Learning Analytics  solutions either for their own institution or with an external  provider.  Figure 1 shows the full overview of the checklist and all  its relevant sub questions. The full version of the checklist can be  downloaded from the LACE website17.                                                                      16 http://www.laceproject.eu/blog/about-todays-ethics-and-  privacy-in-learning-analytics-ep4la/  17 http://www.laceproject.eu/ethics-privacy/    6. CONCLUSIONS  In order to use educational data for Learning Analytics in an  acceptable and compliant way, and to overcome the fears  connected to data aggregation and processing, policies and  guidelines need to be developed that protect the data from abuse  and ensure treatment in a trustful way.     Figure 1: The DELICATE Checklist.  Drachsler & Greller,  2016.  We need to see data protection not as a mere legal requirement,  but should embed the care about privacy deeply into Learning  Analytics tools and increase the trust of data subjects in these  systems. Privacy should not been seen as a burden but rather as a  valuable service we can offer to build trusting relationships with  our stakeholders. To build this kind of relationship, a high degree  of transparency, combined with reassuring explanations  referencing relevant legislation like the EU Directive 95/46/EC  are needed. Therefore, the contract between learners and their  educational providers needs to be reviewed and adjusted to reach  this level of trust and with it the backing to release the full  potential of Learning Analytics.  In conclusion, we believe that Learning Analytics projects should  follow a value-sensitive design process, which allows considering  ethical and privacy values on the same level as functional  requirements. Thereby, the aforementioned ethical considerations  are not seen as an unfortunate constraint, but help to develop a  system that achieves its aims not only in a technical but also in an  ethical and humane manner. At the same time, we request that the  education sector as a whole and institutions in particular need to   http://www.laceproject.eu/blog/about-todays-ethics-and-privacy-in-learning-analytics-ep4la/ http://www.laceproject.eu/blog/about-todays-ethics-and-privacy-in-learning-analytics-ep4la/ http://www.laceproject.eu/ethics-privacy/   get better in distinguishing themselves from commercial profit- oriented enterprises and advertise their mission of care and  support for individuals to revive trust in the system.    We hope that the DELICATE checklist will be a helpful  instrument for any educational institution to demystify the ethics  and privacy discussions around Learning Analytics. As we have  tried to show in this article, there are ways to design and provide  privacy conform Learning Analytics that can benefit all  stakeholders and keep control with the users themselves and  within the established trusted relationship between them and the  institution.   ACKNOWLEDGMENTS  The efforts of Hendrik Drachsler have been partly funded by the  EU FP7 LACE-project.eu (grant number 619424). We thank all  the enthusiastic participants around the planet who attended the  EP4LA workshop series. Those smart discussions supported us in  crafting the DELICATE checklist.    REFERENCES  [1] Johnson, L., Smith, R., Willis, H., Levine, A., & Haywood,   K. (2011). The 2011 Horizon Report. Austin, Texas: The  New Media Consortium.   [2] Greller, W., & Drachsler, H. (2012). Translating Learning  into Numbers: A Generic Framework for Learning Analytics.  Educational Technology & Society, 15 (3), 4257.    [3] New York Times, (2014) InBloom Student Data Repository  to Close. April 21, 2014. Available at:  http://bits.blogs.nytimes.com/2014/04/21/inbloom-student- data-repository-to-close/_r=0    [4] College Bescherming Persoonsgegevens. Onderzoek CBP  naar de verwerking van persoonsgegevens door Snappet.  2014. Available at:  https://cbpweb.nl/sites/default/files/downloads/mijn_privacy/ rap_2013_snappet.pdf.   [5] Venkatesh, V., & Davis, F. D. (2000). A theoretical  extension of the technology acceptance model: Four  longitudinal field studies. Management science, 46(2), 186- 204. DOI 10.1287/mnsc.46.2.186.11926.   [6] Scheffel, M., Drachsler, H., Stoyanov S., & Specht, M.  (2014). Quality Indicators for Learning Analytics.  Educational Technology & Society, 17 (4), 117132.   [7] Drachsler, H., & Greller, W. (2012). The pulse of learning  analytics: understandings and expectations from the  stakeholders. In Proceedings of the 2nd international  conference on learning analytics and knowledge (pp. 120- 129). ACM. DOI 10.1145/2330601.2330634.   [8] Prinsloo, P. & Slade, S. (2013). An evaluation of policy  frameworks for addressing ethical considerations in Learning  Analytics. In Proceedings of the 3rd Learning Analytics and  Knowledge conference, Leuven, Belgium. DOI:  10.1145/2460296.2460344   [9] Slade, S. & Prinsloo, P. (2013). Learning analytics: ethical  issues and dilemmas. American Behavioral Scientist, 57 (10),  pp. 15101529. DOI: 10.1177/0002764213479366   [10] Pardo, A. & Siemens, G. (2014). Ethical and privacy  principles for Learning Analytics. British Journal of  Educational Technology, 45(3), 438450.  DOI: 10.1111/bjet.12152.   [11] Prinsloo, P. & Slade, S. (2015). Student privacy self- management: implications for Learning Analytics. In  Proceedings of the 5th international conference on learning  analytics and knowledge Pooghkeepsie, New York, USA.  DOI: 10.1145/2723576.2723585   [12] Hoel, T. & Chen, W. (2015). Privacy in Learning Analytics   Implications for System Architecture. In: Watanabe, T. and  Seta, K. (Eds.) (2015). Proceedings of the 11th International  Conference on Knowledge Management.   [13] Engelfriet, E., Jeunink, E., Maderveld, J. (2015).  Handreiking Learning analytics onder de Wet bescherming  persoonsgegevens. SURF report.  https://www.surf.nl/kennisbank/2015/learning-analytics- onder-de-wet-bescherming-persoonsgegevens.html    [14] Sclater, N., & Bailey, P. (2015). Code of practice for learning  analytics. Available at: https://www.jisc.ac.uk/guides/code- of-practice-for-learning-analytics    [15] Open University UK (2014). Policy on Ethical use of Student  Data for Learning Analytics. Available:  http://www.open.ac.uk/students/charter/sites/www.open.ac.u k.students.charter/files/files/ecms/web-content/ethical-use- of-student-data-policy.pdf    [16] Kay, D., Korn, N., and Oppenheim, C. (2012). CETIS  Analytics Series: Legal, Risk and Ethical Aspects of  Analytics in Higher Education, serial number: ISSN 2051- 9214 Vol 1, No 6. Available at:  http://publications.cetis.org.uk/2012/500   [17] Slade, S., & Prinsloo, P. (2015). Student vulnerability,  agency and learning analytics: an exploration. Journal of  Learning Analytics, Special Issue on Ethics and Privacy.   [18] Steiner, C., Kickmeier-Rust, M.D., and Albert, D. (2015).  LEA in Private: A Privacy and Data Protection Framework  for a Learning Analytics Toolbox, Journal of Learning  Analytics, Special Issue on Ethics and Privacy.   [19] Lauss, G., Bialobrzeski, A., Korkhaus, M., Snell, K.,  Starkbaum, J., Vermeer, A.E., Weigel, J., Gottweis, H.,  Heln, I., Taupitz, J. & Dabrock, P. (2013). Beyond Genetic  Privacy. Past, Present and Future of Bioinformation Control  Regimes. Available at: http://private- gen.eu/uploads/media/PRIVATE_Gen_FINAL- REPORT_2013_02.pdf    [20] World Medical Association. (2001). World Medical  Association Declaration of Helsinki. Ethical principles for  medical research involving human subjects. Bulletin of the  World Health Organization, 79(4), 373.   [21] National Commission for the Protection of Human Subjects  of Biomedical and Behavioral Research, Bethesda, MD.  (1978). The Belmont report: Ethical principles and guidelines  for the protection of human subjects of research. ERIC  Clearinghouse.   [22] Milgram, Stanley. Das Milgram-Experiment. Reinbek:  Rowohlt, 1974.   [23] Zimbardo, P. G., Maslach, C., & Haney, C. (2000).  Reflections on the Stanford prison experiment: Genesis,  transformations, consequences. Obedience to authority:  Current perspectives on the Milgram paradigm, 193-237.   [24] Kramer, A. D., Guillory, J. E., & Hancock, J. T. (2014).  Experimental evidence of massive-scale emotional contagion  through social networks. Proceedings of the National   http://bits.blogs.nytimes.com/2014/04/21/inbloom-student-data-repository-to-close/_r=0 http://bits.blogs.nytimes.com/2014/04/21/inbloom-student-data-repository-to-close/_r=0 https://cbpweb.nl/sites/default/files/downloads/mijn_privacy/rap_2013_snappet.pdf https://cbpweb.nl/sites/default/files/downloads/mijn_privacy/rap_2013_snappet.pdf http://dx.doi.org/10.1145/2460296.2460344 http://dx.doi.org/10.1145/2723576.2723585 https://www.surf.nl/kennisbank/2015/learning-analytics-onder-de-wet-bescherming-persoonsgegevens.html https://www.surf.nl/kennisbank/2015/learning-analytics-onder-de-wet-bescherming-persoonsgegevens.html https://www.jisc.ac.uk/guides/code-of-practice-for-learning-analytics https://www.jisc.ac.uk/guides/code-of-practice-for-learning-analytics http://www.open.ac.uk/students/charter/sites/www.open.ac.uk.students.charter/files/files/ecms/web-content/ethical-use-of-student-data-policy.pdf http://www.open.ac.uk/students/charter/sites/www.open.ac.uk.students.charter/files/files/ecms/web-content/ethical-use-of-student-data-policy.pdf http://www.open.ac.uk/students/charter/sites/www.open.ac.uk.students.charter/files/files/ecms/web-content/ethical-use-of-student-data-policy.pdf http://publications.cetis.org.uk/2012/500 http://publications.cetis.org.uk/2012/500 http://private-gen.eu/uploads/media/PRIVATE_Gen_FINAL-REPORT_2013_02.pdf http://private-gen.eu/uploads/media/PRIVATE_Gen_FINAL-REPORT_2013_02.pdf http://private-gen.eu/uploads/media/PRIVATE_Gen_FINAL-REPORT_2013_02.pdf   Academy of Sciences, 111(24), 8788-8790. DOI:  10.1073/pnas.1320040111.    [25] Kleinsman, J., & Buckley, S. (2015). Facebook Study: A  Little Bit Unethical But Worth It Journal of Bioethical  inquiry, 12, Issue 2, 179-182, DOI: 10.1007/s11673-015- 9621-0.   [26] Warren, S. D., & Brandeis, L. D. (1890). The right to  privacy. Harvard law review, 193-220.   [27] Westin, A. F. (1968). Privacy and freedom. Washington and  Lee Law Review, 25(1), 166.   [28] Flaherty, D. H. (1989). Protecting privacy in surveillance  societies: The federal republic of Germany, Sweden, France,  Canada, and the United States. UNC Press Books.   [29] Roessler, B. 2005. The Value of Privacy, Oxford: Polity  Press.   [30] Nissenbaum, H. (2004). Privacy as Contextual Integrity.  Washington Law Review 79(1). Available:  https://www.nyu.edu/projects/nissenbaum/papers/washington lawreview.pdf    [31] Hildebrandt, M. (2006). 3. Privacy and Identity. Privacy and  the criminal law, 43.   [32] European Commission (2014). New modes of learning and  teaching in higher education. Luxembourg: Publications  Office of the European Union 2014, 68 pp. ISBN 978-92-79- 39789-9 DOI:10.2766/81897  http://ec.europa.eu/education/library/reports/modernisation- universities_en.pdf    [33] European Union: EU Data Protection Directive 95/46/EC.  http://eur-lex.europa.eu/legal- content/EN/ALL/uri=CELEX:31995L0046.   [34] Organisation for Economic Co-operation and Development.  (2002). OECD Guidelines on the Protection of Privacy and  Transborder Flows of Personal Data. OECD Publishing.  http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprote ctionofprivacyandtransborderflowsofpersonaldata.htm     [35] Mayer-Schnberger, V., & Cukier, K. (2013). Big data: A  revolution that will transform how we live, work, and think.  Houghton Mifflin Harcourt.   [36] Mantelero, A. (2013). The EU Proposal for a General Data  Protection Regulation and the roots of the 'right to be  forgotten'. Computer Law & Security Review 29 (3): 229 235. Available at:  https://www.academia.edu/3635569/The_EU_Proposal_for_ a_General_Data_Protection_Regulation_and_the_roots_of_t he_right_to_be_forgotten_    [37] Koops, B. J. (2011). Forgetting footprints, shunning  shadows: A critical analysis of the 'right to be forgotten' in  big data practice. In: SCRIPTed, Vol. 8, No. 3, pp. 229-256,  2011; Tilburg Law School Research Paper No. 08/2012.  Available at: http://dx.doi.org/10.2139/ssrn.1986719    [38] European Union: EU Data Retention Directive 2006/24/EC  http://eur-lex.europa.eu/legal- content/EN/TXT/uri=celex:32006L0024    [39] The Guardian (2010), Privacy no longer a social norm, says  facebook founder. (January, 11,2010)  http://www.theguardian.com/technology/2010/jan/11/facebo ok-privacy    [40] Fuchs, C. (2015). Digital Labour and Karl Marx. Routledge.  [41] Arora, P. (2015). Bottom of the Data Pyramid: Big Data and   the Global South. In: Discover Society, vol. 23. Available at:  http://discoversociety.org/2015/08/03/bottom-of-the-data- pyramid-big-data-and-the-global-south/    [42] Casey, J., & Greller, W. (2015). Jane Austen and the Belly of  the Beast Part 2 - Language and Power: Commodification,  Technology and the Open Agenda in Higher Education. In:  ISIS Summit Vienna 2015The Information Society at the  Crossroads. Multidisciplinary Digital Publishing Institute.  Available at: http://sciforum.net/conference/isis-summit- vienna-2015/paper/2910/download/pdf    [43] Tene, O., & Polonetsky, J. (2012). Privacy in the age of big  data: a time for big decisions. Stanford Law Review Online,  64, 63. Available at:  https://www.stanfordlawreview.org/online/privacy- paradox/big-data    [44] Ohm, P. (2010). Broken Promises of Privacy: Responding to  the Surprising Failure of Anonymization, 57 UCLA L. Rev.  1701.   [45] Sweeney L. (2000). Simple Demographics Often Identify  People Uniquely. Carnegie Mellon, Data Privacy Working  Paper 3. http://dataprivacylab.org/projects/identifiability/.    [46] Narayanan, A., Shmatikov, V. (2008). Robust De- anonymization of Large Datasets (How to Break Anonymity  of the Netflix Prize Dataset). The University of Texas at  Austin February 5, 2008. http://arxiv.org/pdf/cs/0610105.pdf    [47] Narayanan, A., and Felten, E.W. (2014). No silver bullet:  De-identification still doesnt work.     http://randomwalker.info/publications/no-silver-bullet-de- identification.pdf    [48] Heerde, H.J.W. van (2010). Privacy-aware data management  by means of data degradation - Making private data less  sensitive over time. Ph.D. thesis, University of Twente,  Enschede. http://www.vanheerde.eu/phdthesis.pdf.   [49] Hildebrandt, M. (2008). Profiling and the identity of the  European citizen. In: Profiling the European citizen (pp. 303- 343). Springer Netherlands. DOI: 10.1007/978-1-4020-6914- 7.   [50] Han, B.-C. (2015). The Transparency Society. Stanford  University Press. ISBN: 9780804794602   [51] Clay-Williams R., Colligan L. (2015). Back to basics:  checklists in aviation and healthcare. BMJ Qual Saf 2015,  24: 428  431. DOI:10.1136/bmjqs-2015-003957   [52] Friedman, B., ed. (1997). Human values and the design of  computer technology. Cambridge University Press.          https://www.nyu.edu/projects/nissenbaum/papers/washingtonlawreview.pdf https://www.nyu.edu/projects/nissenbaum/papers/washingtonlawreview.pdf http://ec.europa.eu/education/library/reports/modernisation-universities_en.pdf http://ec.europa.eu/education/library/reports/modernisation-universities_en.pdf http://eur-lex.europa.eu/legal-content/EN/ALL/uri=CELEX:31995L0046 http://eur-lex.europa.eu/legal-content/EN/ALL/uri=CELEX:31995L0046 http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm http://www.oecd.org/sti/ieconomy/oecdguidelinesontheprotectionofprivacyandtransborderflowsofpersonaldata.htm https://www.academia.edu/3635569/The_EU_Proposal_for_a_General_Data_Protection_Regulation_and_the_roots_of_the_right_to_be_forgotten_ https://www.academia.edu/3635569/The_EU_Proposal_for_a_General_Data_Protection_Regulation_and_the_roots_of_the_right_to_be_forgotten_ https://www.academia.edu/3635569/The_EU_Proposal_for_a_General_Data_Protection_Regulation_and_the_roots_of_the_right_to_be_forgotten_ http://dx.doi.org/10.2139/ssrn.1986719 http://eur-lex.europa.eu/legal-content/EN/TXT/uri=celex:32006L0024 http://eur-lex.europa.eu/legal-content/EN/TXT/uri=celex:32006L0024 http://www.theguardian.com/technology/2010/jan/11/facebook-privacy http://www.theguardian.com/technology/2010/jan/11/facebook-privacy http://discoversociety.org/2015/08/03/bottom-of-the-data-pyramid-big-data-and-the-global-south/ http://discoversociety.org/2015/08/03/bottom-of-the-data-pyramid-big-data-and-the-global-south/ http://sciforum.net/conference/isis-summit-vienna-2015/paper/2910/download/pdf http://sciforum.net/conference/isis-summit-vienna-2015/paper/2910/download/pdf https://www.stanfordlawreview.org/online/privacy-paradox/big-data https://www.stanfordlawreview.org/online/privacy-paradox/big-data http://dataprivacylab.org/projects/identifiability/ http://arxiv.org/pdf/cs/0610105.pdf http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf http://randomwalker.info/publications/no-silver-bullet-de-identification.pdf http://www.vanheerde.eu/phdthesis.pdf   "}
{"index":{"_id":"13"}}
{"datatype":"inproceedings","key":"Cooper:2016:ACA:2883851.2883946","author":"Cooper, Martyn and Ferguson, Rebecca and Wolff, Annika","title":"What Can Analytics Contribute to Accessibility in e-Learning Systems and to Disabled Students' Learning?","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"99--103","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883946","doi":"10.1145/2883851.2883946","acmid":"2883946","publisher":"ACM","address":"New York, NY, USA","keywords":"HCI, accessibility, higher education, learning analytics, metrics, technology enhanced learning","abstract":"This paper explores the potential of analytics for improving accessibility of e-learning and supporting disabled learners in their studies. A comparative analysis of completion rates of disabled and non-disabled students in a large five-year dataset is presented and a wide variation in comparative retention rates is characterized. Learning analytics enable us to identify and understand such discrepancies and, in future, could be used to focus interventions to improve retention of disabled students. An agenda for onward research, focused on Critical Learning Paths, is outlined. This paper is intended to stimulate a wider interest in the potential benefits of learning analytics for institutions as they try to assure the accessibility of their e-learning and provision of support for disabled students.","pdf":"What Can Analytics Contribute to Accessibility in   e-Learning Systems and to Disabled Students Learning   Martyn Cooper  Institute of Educational Technology   The Open University  Walton Hall, Milton Keynes,    MK7 6AA  UK  Martyn.Cooper@open.ac.uk   Rebecca Ferguson  Institute of Educational Technology   The Open University  Walton Hall, Milton Keynes,    MK7 6AA  UK  Rebecca.Ferguson@open.ac.uk   Annika Wolff  Centre for Research in Computing   The Open University  Walton Hall, Milton Keynes,    MK7 6AA  UK  Annika.Wolff@open.ac.uk     ABSTRACT  This paper explores the potential of analytics for improving  accessibility of e-learning and supporting disabled learners in their  studies. A comparative analysis of completion rates of disabled  and non-disabled students in a large five-year dataset is presented  and a wide variation in comparative retention rates is  characterized. Learning analytics enable us to identify and  understand such discrepancies and, in future, could be used to  focus interventions to improve retention of disabled students. An  agenda for onward research, focused on Critical Learning Paths, is  outlined. This paper is intended to stimulate a wider interest in the  potential benefits of learning analytics for institutions as they try  to assure the accessibility of their e-learning and provision of  support for disabled students.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   collaborative learning, distance learning.   K.4.2 [Computers and Society]: Social Issues  assistive  technologies for persons with disabilities, handicapped persons /  special needs.  General Terms  Design; Human Factors; Measurement   Keywords  Learning Analytics; Metrics; Accessibility; HCI; Technology  Enhanced Learning; Higher Education    1. INTRODUCTION  More and more universities are now rolling out learning analytics  across the institution. To date, there has been little attention paid  to the benefits that learning analytics may bring to disabled  students, to those who support them in their learning, and to those  responsible for ensuring that courses are accessible.    This paper reports research at The Open University (OU) that  explores the utility and validity of using learning analytics to  identify accessibility deficits in courses and course components  and to target support for disabled students.   2. ACCESSIBILITY AND ANALYTICS  A review of the key points of reference for learning analytics [4]  shows that, when identifying technical and research challenges,  disability and accessibility have not been subjects of particular  interest to researchers [4, 12]. Where these subjects have been  addressed is within the academic analytics literature in the context  of retention and success rates, [10]. Academic analytics marry  large data sets with statistical techniques and predictive modeling  to improve decision making [3]. Retention and success rates are  areas of particular concern, and work on these topics draws on  large-scale studies that often predate modern analytics  approaches; for example, Tintos studies of factors affecting  student persistence [13]. Such studies typically segment students  using categories including gender, ethnicity and class. Disability  is one variable amongst many [13].   3. DISABILITY AND ACCESSIBILITY  Disabilities have traditionally been described with reference to the  medical conditions from which they were seen to arise. This  medical model of disability is encapsulated in the international  classification of impairments, disabilities and handicaps produced  by the World Health Organisation (WHO) [16].  The main alternative to the medical model of disability is the  social model [14], which considers that disability is caused by the  ways in which society is organised and responds to people. It is  not an inevitable consequence of impairment but the product of  physical, organisational and attitudinal barriers. This view has  been highly influential in shaping policy, practice and attitudes.  In 2001, the WHO revised its definitions, in part as a response to  the social model, and in part because the medical model was of  limited use in defining effective responses to the needs of disabled  people. It described disability as the outcome or result of a  complex relationship between an individuals health condition and  personal factors, and of the external factors that represent the  circumstances in which the individual lives [17].  The IMS Global Learning Consortium offered more education- specific definitions when introducing its work on the development  of technical standards for accessibility in e-learning. It defined  disability as a mismatch between the needs of the learner and the  education offered. Accessibility is the ability of the learning  environment to adjust to the needs of all learners and is  determined by the flexibility of the education environment (with  respect to presentation, control methods, access modality, and  learner supports) and the availability of adequate alternative-but- equivalent content and activities [7].   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee. Request permissions from Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom   Copyright is held by the owner/author(s). Publication rights licensed to  ACM.   ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883946     The term accessibility is often used in the context of web design,  and the W3C web standards body states, Web accessibility means  that people with disabilities can perceive, understand, navigate,  and interact with the Web, and that they can contribute to the  Web [18]. A design can be said to be accessible if it facilitates  full interaction by all users, irrespective of the assistive  technologies or access approaches that may be adopted by some.    3.1 Disability flags and access profiles  Key to the development of learning analytics for accessibility is  knowledge of which students have declared a disability. This  <disability flag> carries no additional information. However, data  that many universities are required to collect and report enables  the use of a richer data element, <disability type>.  Disability type is medical-model based. The UKs Higher  Education Statistics Agency defines disability in relation to 12  broad categories, including sight impairment, hearing impairment  and dyslexia. These categories do not map meaningfully to  disabled peoples functional requirements for interacting with web  resources but may still be useful in terms of analytics.  An alternative is the Access for All 3.0 Personal Needs and  Preferences (PNP) specification developed within the IMS Global  Learning Consortium, which went to public draft in September  2012 [7]. This provides a candidate approach that could enable  comprehensive profiles of individuals access approaches and  assistive technologies to be stored within a user database.  Disabled students could generate their own functional profiles,  possibly with the help of disability advisors, by inputting their  access approaches and requirements. These profiles would help in  identifying accessibility problems and where and how they arise.   3.2 The law and external standards  Most developed countries now have legislation which impacts on  the access of disabled people to education. These laws vary in  nature from country to country. The UK, for example, has anti- discrimination legislation rather than accessibility legislation.  This is the case in most developed countries, although the US has  some accessibility legislation that is linked to public procurement.  In the UK, the key current legislation is the Equality Act (2010)  [5], which builds on other legislation, including the Disability  Discrimination Act (2005). In essence, these require educational  institutions not to discriminate against disabled students on the  basis of their disabilities; to make reasonable adjustments in  order to meet disabled students needs in all aspects of their  education, and to anticipate the needs of disabled students.  The law is understandably not specific about what the phrase  reasonable adjustments means in terms of accessibility of online  offerings. However, it is widely accepted that the Web Content  Accessibility Guidelines (WCAG) 2.0 recommended by the W3C  are the baseline if a case is taken to court [19]. These guidelines  are targeted at web developers and cover technical accessibility.  In addition, accessibility in e-learning should include accessibility  of learning design and the pedagogically appropriate use of  alternative formats, which may employ diverse media.  Accessibility should be built into everyday practices throughout  the product lifecycle from conception and specification, through  development, to delivery and maintenance. Recognising this, the  British Standards Institute developed BS 8878: 2010 Web  Accessibility Code of Practice [2]. This facilitates a pragmatic  application of WCAG 2.0 within a process-based approach.   4. ANALYTICS FOR ACCESSIBILITY   Researchers investigating drop-out rates of campus-based students  have drawn on institutional records and survey data when  developing models of persistence and attrition [1, 13]. Similar  approaches have been used to predict drop-out rates on online  courses [8, 15]. Student attributes, skills and contexts have been  used to develop predictive models [12]. Researchers can now  make use of online interaction data in order to identify links  between participation in various activities, rates of participation at  various stages of the course, and drop-out rates [9].  Little of this work has taken disability into account, although age,  gender, ethnicity, family background and study habits have all  been considered [1, 13]. This is a significant omission, because  prominent theoretical models of academic attrition imply that  disability could be a significant variable if it results in students  feeling they do not fit in, or that their studies are putting too much  pressure on their resources and well-being [1, 13].  Retention issues are key institutional drivers for the adoption of  learning analytics. If disabled students encounter critical elements  or required modes of study that are not accessible to them, then  this will impact negatively on their chances of passing a course  and may lead to them dropping out before the final assessment.    5. DESCRIPTION OF DATASET  In order to explore whether learning analytics can be used to  identify modules with such accessibility deficits, a large dataset  was compiled and analysed, in a project focused on the retention  of STEM (science, maths, engineering and technology) students.   This dataset contained data from OU modules (units requiring 300  or 600 study hours). It included five years of data (2009-2013)  from two OU faculties, Maths Computing & Technology (MCT)  and Science, covering all module presentations for which  complete data were available (N=1,338). For each module  presentation, numbers of students declaring a disability (disabled  students) or not (non-disabled students) were recorded, as were  percentages of both groups completing and passing. Students were  considered to have completed a module if they submitted the final  summative assessment. Pass and completion rates of disabled and  non-disabled students could then be compared.   Overall, analysis of the dataset indicated that the OU had  improved the accessibility of its modules over time. Only three  modules in the dataset that were presented after 2012 showed the  completion rate of disabled students to be markedly poorer than  that of non-disabled students (indicated by an odds ratio of >2.95).   6. RESULTS  6.1 Results of quantitative analysis  The average completion rate for non-disabled students in the  dataset was 75.3% and for disabled students it was 69.5%.  However, many factors other than disability impact on completion  rates. The objective of our analytics approach was to identify  modules in which accessibility was a dominant factor in  determining the completion rates of disabled students and then to  focus remedial efforts on those modules. Table 1 shows that in  68% of module presentations a greater proportion of non-disabled  students completed the module than disabled students. However,  it might be expected that non-disabled students would do better  than disabled students, so it is perhaps more significant to note  that in 30.6% of cases the converse was true. The range of  differences in completion rate (non-disabled students minus  disabled students) was from -49.5 percentage points to 96.0  percentage points.     Table 1: Rates at which disabled and non-disabled   students complete modules    No. module presentations  % of module  presentations   Higher % of non-disabled  students complete 923 68.0%   Higher % of disabled  students complete 415 30.6%   Equal % students  complete 19 1.4%   The more extreme results were mainly found in module  presentations with a low population of disabled students. This  effect of population size can be illustrated with an example. If a  module has only two disabled students and one drops out then the  completion rate is 50%. If the same module has many non- disabled students, their completion rate will be very close to the  mean of the course, say 70%. This results in a difference of 20  percentage points. However, this difference would be due to one  student dropping out, not necessarily because of accessibility.  To avoid this distortion, in subsequent analysis modules with  fewer than 25 disabled students were not considered, reducing the  number of module presentations in the dataset to 668.   Table 2: Rates at which disabled and non-disabled   students complete modules (modules > 25 disabled students)    No. module presentations  % of module  presentations   Higher % of non-disabled  students complete 537 80.4%   Higher % of disabled  students complete 129 19.3%   Equal % students  complete 2 0.3%   In this modified dataset, differences in completion rate on a  module (non-disabled students minus disabled students) ranged  from -22.1% points to 35.0 percentage points.   Initial analysis considered differences between the percentage  completion of these groups for each module presentation. This  approach was refined by the use of odds ratios, a standard  statistical approach that enables comparisons to be made across  different modules when the underlying phenomenon varies.   When using odds ratios, if the probability of the members of  Group 1 achieving an outcome is p, with 0 indicating it will never  happen and 1 indicating it is certain to happen, then the odds of  this are p/(1  p). If the probability of the members of Group 2  exhibiting that outcome is q, then the odds of this are q/(1  q).  The odds ratio is the ratio between these odds [p/(1  p)]/[q/(1   q)], which equals [p(1  q)]/[q(1  p)]. Odds ratios vary from 0  (when p = 0 or q = 1) to infinity (when p = 1 or q = 0). An odds  ratio of 1 means there is no difference in the odds of the two  groups members achieving the outcome (when p = q). An odds  ratio less than 1 means the members of Group 1 are less likely to  achieve the outcome than the members of Group 2; while an odds  ratio greater than 1 means the members of Group 1 are more  likely to exhibit the outcome than are the members of Group 2  Odds ratios are often used to generalise from a sample population.  Here, though, we calculated and compared the odds ratio for all  students on each module presentation, classifying non-disabled  students as Group 1 and disabled students as Group 2. The odds  ratio is therefore >1 when non-disabled students perform better   than disabled students on a module. The larger the odds ratio is,  the greater the disparity between the two groups.   We suggest that when the odds ratios are above a certain threshold  they indicate a module is presenting accessibility issues that are  significantly impacting on the performance of disabled students.  The plot in Figure 1 was used to set a threshold of 3.0. Although  the plot is difficult to interpret at this scale, the main thing to note  is that most data points lie between 1.0 and 3.0. Working with an  odds ratio of 3.0 was therefore considered likely to filter out cases  where factors other than disability would have the most  significant effects on student performance.   This approach points to cases where significant accessibility  issues may exist; it says nothing about what those accessibility  barriers might be. It can therefore be used to decide where  accessibility reviews should be carried out. This has the advantage  of focusing the limited accessibility expertise available across a  university on the modules where it is likely to have most impact.    Figure 1: Distribution plot of odds ratios of completion rates;  each dot represents a module presentation (distributed along   the x axis in no particular order); y axis is the odds ratio   6.2 Comparison with qualitative data  In order to contextualise this quantitative approach to identifying  possible accessibility deficits, qualitative data were also analysed  in a small-scale investigation.   At the end of each module presentation, OU students are asked to  complete a survey that explores their study experience. This  survey includes three opportunities to provide free-text comments.  Since 2012, the survey includes a question about the experience of  disabled students, so this section of our research focused on  module presentations that had used this form of the survey.  Free-text survey responses from six module presentations were  analysed qualitatively, with the focus on responses from students  who had declared a disability. For three of the sample modules  quantitative analysis had indicated accessibility challenges; for  three modules no problems had been identified.    6.3 Results of qualitative analysis  Table 4 summarises the qualitative analysis of survey responses  (the coder had not seen results of the quantitative analysis).  Ranking of the selected modules for accessibility on the basis of  free-text survey responses by disabled students was markedly  different to the ranking by odds ratio of completion rates  comparing disabled and non-disabled students. Reasons for this  difference, and its implications, are discussed in the next section.     0.0  1.0  2.0  3.0  4.0  5.0  6.0    Table 3: Modules selected for qualitative analysis ranked in  descending order of odds ratio.    M represents a mathematics module and S a science module    Module  Code   Disabled   No Yes   Total No.  %   complete No.  %   complete  Odds  Ratios   M1 247 221 67.0 26 26.9 5.517   S2 145 118 71.2 27 44.4 3.096   S1 230 199 70.9 31 45.2 2.954   S3 291 229 64.2 62 53.2 1.578   M2 320 282 70.2 38 63.2 1.372   S4 148 117 76.1 31 71.0 1.301   The selection criteria used to obtain this sample (Table 3) were:    Modules were at Level 3, comparable to modules taken  by students in the final year of undergraduate study    Presentation had at least 25 disabled students registered   Field trips and summer schools (face-to-face lab work)   were excluded   Modules thought to include significant accessibility   barriers had a completion rate odds ratio of >2.95   Three modules, selected from many, thought to be non-  problematic from an accessibility perspective had a  completion rate odds ratio of < 1.6.   Unfortunately, for three of the selected modules the data were  incomplete because an unrelated data-cleansing process had  caused the over-writing of some responses.   Table 4: Relative ranking of modules by access issues reported  in survey responses (with summary of reasons for the   ranking) compared with ranking by odds ratio   Module  Code   Summary of Reason for  Ranking from Survey Analysis   Survey  analysis  ranking   (worst = 1)   Ranking by  odds ratio  completion  (worst = 1)   M2   Two-thirds of students with  declared disabilities had  complaints that could relate to  their disabilities.    1 5   S1  Difficult for dyslexics to follow,  videos lacking subtitles, no  alternative to using website.    2 3   S4 Incomplete data. Lack of printed material was a problem for some 3 6   S3   Incomplete data. One had  problems with online copy.  However, three noted other  options were available.   4 4   M1 Data incomplete. No problems identified. 5 1   S2 No problems AND students were very positive. 6 2   7. DISCUSSION  Comparative analysis of completion rates between disabled and  non-disabled students, over a large set of module presentations,  confirmed that disabled students are less likely to complete a  module than non-disabled students. However, it revealed wide  variation between modules. Identifying modules with the greatest  disparity between performance of disabled and non-disabled  students indicates where significant accessibility challenges lie.  This approach was not valid when the numbers of disabled  students on a module was low. We suggested that a minimum of  25 disabled students on a module was appropriate. This figure  halved the number of modules analysed in this study and limits  the contexts in which the approach would be applicable. In the  context of a large university like the OU, this approach would  identify the accessibility deficits that impact on a significant  majority of disabled students. At many universities, far fewer  courses would have sufficient disabled student numbers to make  the approach valid.  However, one context in which we suggest  that it would have particular utility and validity is that of massive  open online courses (MOOCs).   Disabled students do raise issues relating to module accessibility  when surveyed. These include difficulty with reading material on  screen, and lack of subtitles on videos from external providers.  However, we run into a number of problems if we rely on end-of- module surveys to reveal where accessibility problems may be  significantly impacting on disabled students learning:  (1) Those who are most profoundly affected by accessibility   deficits and decide to withdraw are unlikely to complete an  end-of-module survey.    (2) Student responses do not align with drop out rates (Table 4).   (3) Problems reported by disabled students are not necessarily   the main problems.   (4) On modules with a high drop out rate, disabled students may   report no problems.  (5) Declared disability is a very broad classification. A   problem that is insurmountable for blind students may have  no effect on others who are hard-of-hearing or dyslexic.   Analytics provide another way of approaching the problem of  identifying where major accessibility deficits lie. First, we identify  quantitatively the modules with disproportionately high drop out  rates of disabled students. Having done that, we can (in future)  carry out critical learning path analysis of those modules, and  compare the critical learning paths of disabled students and non- disabled students. This potentially enables us to pinpoint where  the accessibility problems lie that are really impacting on learning.   8. ONGOING AND FUTURE WORK  8.1 Analytics to support disabled students  A major theme in learning analytics has been to facilitate the  offering of targeted support to the students most in need of it.  Declared disability could have significant utility in this process. If  tutor dashboards, when highlighting those at risk, also identified  which students had declared a disability, this could prompt an  enquiry as to whether this disability, or accessibility issues with  the course, were having a significant impact on performance. A  tutor could then support the student to make use of appropriate  access approaches. This could trigger a negotiation about  reasonable adjustments or prompt the student to obtain support  from specialist disabled student support services.  This could be done at different levels, depending on the use of  such dashboards and the levels of information collected and made     available to the learning analytics system. If the learning analytics  system just uses the <disability flag> then only a basic prompt  that this student has declared a disability is possible. However, if  the system models disabled students needs and preferences for  access approaches then it is possible to point both tutor and  student to more specifically appropriate guidance.    8.2 Analytics to reveal critical learning paths   Students working on an online system can typically choose how  and when they access the learning materials provided. Some  learning activities are inevitably more critical for success than  others. Machine learning methods can be applied to historical  module data in order to discover which types of activity are  important for a particular module at a particular time. For  example, being inactive in a forum in the first week of a module  presentation can be implicated in lower performance in module A,  yet have no impact in module B. Through mapping the activity  space of a module and identifying the pathways of successful and  unsuccessful students through the activities, it is possible to  identify critical activities along the pathway to success and,  conversely, to identify strategies that are not successful.   Current work at the OU (building on [15]) represents multiple  student paths through a module as a probabilistic Markov chain of  independent transitions between one activity on the virtual  learning environment (VLE) and another. A graphical  representation of the Markov chain for a given module can  highlight how many students made a transition between certain  activities and how this impacted on their final outcome. This  shows the critical pathways to success.  Currently, this work is being investigated at the level of VLE  activity type. If any critical activities pose accessibility  problems, then this could limit progress for some disabled  students. Comparing pathways of successful and unsuccessful  disabled students with the pathways of others could highlight  problems with a modules activities. In addition, the accessibility  of activities that fall on the pathways of successful students will  have more impact on the success of disabled students than  activities that appear to have little impact on student success. Thus  this approach could be used to prioritise remedial accessibility  work on a module.   9. CONCLUSIONS  Disabled students experience a wide range of challenges in their  study. Educational institutions need to extend accessibility of their  courses to reduce those challenges. This paper has shown that an  analytics approach based on comparative analysis of completion  rates between disabled and non-disabled students could identify  where accessibility deficits are having real impact on learning and  thus focus remedial attention. Based on ongoing work on critical  learning path analysis, the paper has outlined how analytics could  be used to identify module components that are presenting  accessibility challenges. Where learning analytics dashboards are  being used to support students directly and enable their tutors to  support them, these approaches could be extended to target  effective support for disabled students. It is hoped that this paper  will stimulate others involved in the research, development and  roll-out of learning analytics to work towards realising their  potential to meet the needs of disabled students.     10. REFERENCES  [1] Bean, J. P. & Metzner, B. S., A conceptual model of   nontraditional undergraduate student attrition. Review of  Educational Research, 55, 4, (1985), 485-540.   [2] British Standards International, BS 8878:2010 Web  Accessibility  Code of Practice. 2010. Available from:  http://www.bsi-publications.com.    [3] Campbell, J. P. & Oblinger, D. G., Academic Analytics.  Educause, 2007.    [4] Dawson, S., Gaevi, D., Siemens, G., & Joksimovic, S.  (2014). Current state and future trends: a citation analysis of  the learning analytics field. LAK 14, Indianapolis, USA.   [5] HMSO, The Equality Act 2010. Available from  http://www.homeoffice.gov.uk/equalities/equality-act/.    [6] IMS Global Learning Consortium, IMS AccessForAll Meta- data Overview. 2004. http://www.imsglobal.org/accessibility.   [7] IMS Global Learning Consortium, Access for All (AfA),  Version 3.0 Specification, Public Draft 1.0. 2012. Available  from http://www.imsglobal.org/accessibility/index.html    [8] Morris, L.V., Finnegan, C.L. and Wu, S.-S., Tracking student  behavior, persistence, and achievement in online courses.  The Internet and Higher Education, 8, 3, (2005), 221-231.   [9] Nistor, N. & Neubauer, K., From participation to dropout:  quantitative participation patterns in online university  courses. Computers & Education, 55, 2, (2010), 663-672.   [10] Richardson, J.T.E. (2009): The academic attainment of  students with disabilities in UK higher education, Studies in  Higher Education, 34:2, 123-137   [11] Richardson, J.T.E., Course completion and attainment in  disabled students taking courses with The Open University  UK. Open Learning, 25, 2, (2010), 81-94.   [12] Romero, C. and Ventura, S., Educational data mining: a  survey from 1995 to 2005. Expert Systems with Applications,  33, 1, (2007), 135-146.   [13] Tinto, V., Colleges as communities: taking research on  student persistence seriously. The Review of Higher  Education, 21, 2, (1997), 167-177.   [14] UPIAS, Fundamental Principles of Disability. Union of  Physically Impaired against Segregation, London, 1976.    [15] Wolff, A., Zdrahal, Z., Nikolov, A. & Pantucek, M. (2013)  Improving retention: predicting at-risk students by analysing  clicking behaviour in a virtual learning environment, LAK13,  Leuven, Belgium.   [16] World Health Organization, International classification of  impairments, disabilities, and handicaps. WHO, Geneva,  1980.    [17] World Health Organization, International Classification of  Functioning, Disability and Health. WHO, Geneva, 2001.   [18] World Wide Web Consortium, Introduction to Web  Accessibility. 2005. Available at http://www.w3.org/WAI/    [19] World Wide Web Consortium, Web Content Accessibility  Guidelines 2.0. 2008. http://www.w3.org/TR/WCAG/.     "}
{"index":{"_id":"14"}}
{"datatype":"inproceedings","key":"Grawemeyer:2016:AOB:2883851.2883936","author":"Grawemeyer, Beate and Mavrikis, Manolis and Holmes, Wayne and Gutierrez-Santos, Sergio and Wiedmann, Michael and Rummel, Nikol","title":"Affecting Off-task Behaviour: How Affect-aware Feedback Can Improve Student Learning","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"104--113","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883936","doi":"10.1145/2883851.2883936","acmid":"2883936","publisher":"ACM","address":"New York, NY, USA","keywords":"affect, exploratory learning environments, feedback","abstract":"This paper describes the development and evaluation of an affect-aware intelligent support component that is part of a learning environment known as iTalk2Learn. The intelligent support component is able to tailor feedback according to a student's affective state, which is deduced both from speech and interaction. The affect prediction is used to determine which type of feedback is provided and how that feedback is presented (interruptive or non-interruptive). The system includes two Bayesian networks that were trained with data gathered in a series of ecologically-valid Wizard-of-Oz studies, where the effect of the type of feedback and the presentation of feedback on students' affective states was investigated. This paper reports results from an experiment that compared a version that provided affect-aware feedback (affect condition) with one that provided feedback based on performance only (non-affect condition). Results show that students who were in the affect condition were less bored and less off-task, with the latter being statically significant. Importantly, students in both conditions made learning gains that were statistically significant, while students in the affect condition had higher learning gains than those in the non-affect condition, although this result was not statistically significant in this study's sample. Taken all together, the results point to the potential and positive impact of affect-aware intelligent support.","pdf":"Affecting Off-Task Behaviour: How Affect-aware Feedback Can Improve Student Learning  Beate Grawemeyer London Knowledge Lab  Birkbeck, University of London London, UK  beate@dcs.bbk.ac.uk  Manolis Mavrikis London Knowledge Lab  UCL Institute of Education University College London, UK  m.mavrikis@ioe.ac.uk  Wayne Holmes London Knowledge Lab  UCL Institute of Education University College London, UK  w.holmes@ioe.ac.uk Sergio Gutierrez-Santos  London Knowledge Lab Birkbeck, University of London  London, UK sergut@dcs.bbk.ac.uk  Michael Wiedmann Institute of Educational  Research Ruhr-Universitt Bochum, DE michael.wiedmann@rub.de  Nikol Rummel Institute of Educational  Research Ruhr-Universitt Bochum, DE  nikol.rummel@rub.de  ABSTRACT This paper describes the development and evaluation of an aect-aware intelligent support component that is part of a learning environment known as iTalk2Learn. The intelligent support component is able to tailor feedback according to a students aective state, which is deduced both from speech and interaction. The aect prediction is used to determine which type of feedback is provided and how that feedback is presented (interruptive or non-interruptive). The system includes two Bayesian networks that were trained with data gathered in a series of ecologically-valid Wizard-of-Oz stud- ies, where the eect of the type of feedback and the presen- tation of feedback on students aective states was investi- gated. This paper reports results from an experiment that compared a version that provided aect-aware feedback (af- fect condition) with one that provided feedback based on performance only (non-aect condition). Results show that students who were in the aect condition were less bored and less o-task, with the latter being statically significant. Im- portantly, students in both conditions made learning gains that were statistically significant, while students in the af- fect condition had higher learning gains than those in the non-aect condition, although this result was not statisti- cally significant in this studys sample. Taken all together, the results point to the potential and positive impact of aect-aware intelligent support.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.1 [Computer Uses in Education]: Computer-assisted in- struction  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883936  Keywords Aect, Feedback, Exploratory Learning Environments  1. INTRODUCTION The aim of our research is to provide intelligent support to  students, taking into account their aective states in order to enhance their learning experience and performance.  It is well understood that aect interacts with and in- fluences the learning process [19, 10, 3]. While positive aective states (such as surprise, satisfaction or curiosity) contribute towards learning, negative ones (including frus- tration or disillusionment at realising misconceptions) can undermine learning. Any learning experience is typically full of transitions between positive and negative aective states. For example, while a student may be interested in a partic- ular learning task, any misconceptions that they have might lead to frustration or disillusionment as they are forced to reconsider their existing understanding. However, if this negative aective state is reconciled, the student might be- come deeply engaged with the task once again. DMello et al., for example, elaborate on how confusion, which superfi- cially might be considered a negative aective state, is likely to promote learning under appropriate conditions [10].  It is important therefore, to deepen our understanding of the role of aective states for learning, and to be able to move students out of states that inhibit learning. Pekrun [24] discusses achievement emotions or aective states that arise in a learning situation. Achievement emotions are states that are linked to learning, instruction, and achieve- ment. We focus on a subset of aective states identified by Pekrun: flow/enjoyment, surprise, frustration, and bore- dom. We also add confusion, which has been identified else- where as an important aective state during learning [25], for tutor support and for learning in general [10].  As described in Woolf et al. [32], students can become overwhelmed (very confused or frustrated) during learning, which may increase cognitive load [30]. However, appropri- ate feedback might help to overcome such problems. Carenini et al. [4] describe how eective support or feedback needs to answer three main questions: (i) when the support should be provided during learning; (ii) what the support should contain; and (iii) how it should be presented.  http://dx.doi.org/10.1145/2883851.2883936   In this paper we focus on the latter two questions: what and how support or feedback should be provided based on the students aective state.  We report on the development of intelligent feedback that is able to tailor the type of feedback as well as the presen- tation of the feedback in order to enhance a students learn- ing experience. It includes a Bayesian network for deter- mine the most eective feedback type, as well as a Bayesian network for detecting the most eective presentation of the feedback. Both networks are trained with data from Wizard- of-Oz studies where the impact of a students aective state on the eectiveness of the feedback type and on the presen- tation of feedback was investigated (c.f. [15, 13]).  In those studies, we learned that a students aective state can be enhanced when the feedback type is matched to the aective state of the student. For example, when students were confused, aect boosts and specific instructive feedback were more eective in helping students [15]. Additionally, adapting the presentation of the feedback according to the students aective state is also important, especially when they are confused or frustrated. For these particular af- fective states, high-interruptive feedback is more eective, especially as the cost of not viewing the feedback is likely to be a negative aective state [13]. However, when students are in flow, low-interruptive feedback is preferred [21].  In the next section an overview of related literature is provided. Section 3 describes the development of the aect- aware intelligent support. Section 4 outlines the evaluation of the support. Results of the evaluation are reported in section 5. A detailed discussion that highlights the impor- tance of including aect in learner modelling is provided in section 6; while section 7 concludes the paper.  2. RELATED WORK Most of the related work in the field focuses on detecting  aect in dierent input stimuli, ranging from spoken dia- logue (e.g. [31]) to keyboard and mouse interactions [27], including a combination of dierent stimuli such as conver- sational cues, body language and facial expression [8].  Only a limited amount of research has been undertaken to investigate how a students aect or motivation can been taken into account to provide learning material or motiva- tional feedback. One early example is del Soldato & du Boulay [7] who use a students motivation to decide whether to provide the next task or to provide hints. Another exam- ple is Santos et al. [28] who show that aect as well as mo- tivation and self-ecacy impact the eectiveness of motiva- tional feedback and recommendations. Additionally, Woolf et al. [32] developed an aective pedagogical agent which is able to mirror a students aective state, or acknowledge a students aective state if it is negative. Another exam- ple is Conati & MacLaren [6], who developed a pedagogical agent to provide support according to the aective state of the student and their personal goal. Also, Shen et al. [29] recommend learning material to the student based on their aective state. DMello et al. [9] developed a system that is able to respond to students via a conversation that takes into account the aective state of the student.  A limited number of researchers have looked at how the presentation of information or feedback could be adapted according to certain user characteristics. For example, in the area of information visualisation, Carenini et al. [4] de- scribe a study that looks at tailoring visual prompts, based  on task complexity, user characteristics (such as perceptual speed, visual working memory, and verbal working memory) and delivery times. Also, Grawemeyer & Cox [12] describe a system that is able to recommend a particular represen- tation (bar chart, plot chart, pie chart, sector graph, eulers diagram, or table) based on the users expertise with repre- sentations, their preferences for particular representations, the task, the information to be presented and the repre- sentations semantics. In the slightly dierent, but related, use case of exploratory visual data analysis, researchers have looked at inferring the users intended task and recommend- ing alternative visualisations that may help in their analy- sis [11]. Further, Ahn & Brusilovsky [1] describe a system which adapts the visualisation of search results dynamically, based on a users emerging interests.  In this paper we describe how we add to this literature by developing and evaluating intelligent support within a learning environment that is able to adapt the type of feed- back as well as the presentation (high- or low-interruptive) of feedback according to a students aective state in order to enhance their learning experience and learning gains.  3. THE ITALK2LEARN PLATFORM iTalk2Learn is a learning platform for children aged 8-12  years old who are learning fractions. It combines structured practice with more open-ended activities in an exploratory learning environment called Fractions Lab [22]. The over- all aim is to foster robust learning through providing ex- ploratory exercises that help develop conceptual knowledge interleaved with structured practice activities that foster procedural knowledge. In this paper, we focus on the sup- port provided while students are undertaking these learning activities (i.e. not the sequencing of those activities).  Figure 1: Exploratory learning environment (Frac- tions Lab).  Figure 1 shows the Fractions Lab interface. The learning task is displayed at the top of the screen. Students are asked to solve the task by choosing fraction representations (from the right-hand side menu) which they manipulate in order to construct an answer to the given task. Adaptive support is provided to the students based on their screen interactions and their speech. The platform is designed to detect and    analyse childrens speech in near real time (c.f. [17]). Figure 2 shows the components of the adaptive support.  Drawing on our previous work [16] the support consists of three main layers: the analysis or evidence detection layer, the reasoning layer, and the feedback generation layer.  Figure 2: Components of the adaptive support.  The input to the analysis layer includes the interaction with Fractions Lab (e.g. what fraction was generated or changed), the result of the perceived task diculty classifier (PTDC), which is based on the students speech (e.g. over-, appropriately or under-challenged), and the data from the speech recognition software (spoken words).  In the analysis layer, the students interactions with the platform are identified. This layer includes the aective state detector, where a students aective state is detected from their speech (results from PTDC and speech recogni- tion software) and their interaction with Fractions Lab.  Based on the evidence detection component, the reason- ing layer decides if and what feedback should be provided. This layer contains a student model and an aective state reasoner. The student model includes the aective state of the student and information about actions the student has performed, such as representation selected, as well as information about what feedback has been provided to the student and if that feedback was followed. The aective state reasoner uses the information from the student model to decide what type of feedback should be provided.  The feedback generation layer receives the output from the reasoning layer. It includes an aective state presenta- tion model, which is used to decide how the feedback should be presented to the student. The feedback can either be pre- sented in a low-interruptive way (a light bulb glows, indicat- ing that feedback is available, which can then be accessed or ignored by the student) or in a high-interruptive way (the students are interrupted by a pop-up window which has to be dismissed before the student can continue).  Detailed information about the dierent components are provided below.  3.1 Learner model The learner model spans all three main components and  can be seen as the heart of the intelligent support. It in- cludes the following information about the current student:   The aective state of the student (that has been cal- culated by the aective state detector, based on the students interaction and speech).   Reasoning stage of the student (beginning, middle or end of the task).   Interactions with the learning environment (e.g. rep- resentation selected or fraction changed).   The type of feedback that has been provided to the student in the past.   If the feedback was viewed by the student or not.   If the feedback provided was followed or not.  This information is used by the dierent components to de- termine what type of support should be provided to the stu- dent, and how that support should be provided.  3.2 Analysis layer (affect detection) The detection of the students aective state is based on  their interaction with the exploratory learning environment (Fractions Lab) as well as on their speech as follows:   The students interaction with the platform is used to provide a probability about their aective state. The data used for this calculation is based on whether or not they viewed the feedback and whether or not they followed the advice provided by the feedback. It is a Bayesian network that is trained with data from for- mative evaluations in Wizard-of-Oz studies [20].   A perceived task diculty classifier (PTDC) extracts prosodic features from the students speech, in order to determine if the student is over-, appropriately or under-challenged [17]. Speech and pause histograms are used by the perceived task diculty recognition [18].   The speech recognition software [26] detects whether students are speaking or not and produces an array of spoken words. This array is used to detect certain keywords that are associated with a particular aective state. We apply a naive Bayes classifier for classifying the aective state from those words [14].  The aective state detector determines the overall aective state of the student, based on weightings of the dierent input components. For example, a higher weight is given to detecting keywords from the speech that are associated with a particular aective state, followed by the interaction data.  3.3 Reasoning layer (affective state reasoner) The aective state reasoner uses the information from the  student model to decide what type of feedback should be pro- vided. We explore dierent types of feedback that are known from the literature to support students in their learning and that fit our context. The following dierent feedback types were provided:   AFFECT BOOSTS - aect boosts. As described in [32], aect boosts can help to enhance students mo- tivation in solving a particular learning task. These in- cluded prompts that acknowledged, for example, that a task is dicult or that the student may be confused and encourages them to keep trying.     AFFIRMATION PROMPTS - task completion prompts. This feedback is provided when students have completed the task successfully, in order to give them an indication that they have finished the task and should start working on the next task.   INSTRUCTIVE FEEDBACK - instructive task- dependent feedback. This feedback provided de- tailed instructions, what subtask or action to perform in order to solve the task.   OTHER PROBLEM SOLVING FEEDBACK - task-dependent feedback. This support was cen- tred on helping students to solve a particular problem that they are facing during their interaction by pro- viding either questions to challenge their thinking or specific hints designed to help them identify the next step themselves.   REFLECTIVE PROMPTS - reflecting on task performance and learning. Self-explanation can be viewed as a tool to help a student address mis- understandings [5] and as a window into a students thinking.   TALK ALOUD PROMPTS - talking aloud. With respect to learning in particular, the hypothesis that automatic speech recognition (ASR) can facilitate learn- ing is based on educational research that has shown benefits of verbalization for learning (e.g., [2]).   TASK SEQUENCE PROMPTS - completing the task. This feedback is centred on providing sup- port when students try to go to the next task when they have not completed the current task.  Table 1 shows examples of the dierent feedback types. Based on the information from the learner model, the aective state reasoner decides what type of feedback should be pre- sented to the student. The aective state reasoner is a Bayesian network based on data gathered in Wizard-of-Oz studies [15] that investigated the impact of the dierent feed- back types on the aective state of students. In those stud- ies, students were given a series of fractions tasks and were provided with feedback, of the types described above, by the researchers (the wizards) as if it was being provided by the system. The decision about what type of feedback to provide was based on a script. For more information, the reader is referred to [20].  During those studies, the students aective states were annotated by using the Baker-Rodrigo Ocumpaugh Method Protocol (BROMP) and the HART mobile app that facili- tates coding in the classroom [23]. Two researchers also in- dependently annotated the aective states after the Wizard- of-Oz studies using screen and voice recordings. This was then compared against the field annotations. Kappa be- tween the consolidated annotation and the HART data was .71, p<.05.  Figure 3 shows the Bayesian network of the aective state reasoner. We trained the network with the data from the Wizard-of-Oz studies (265 data points). For the trained network we employed a 10-fold cross-validation that showed promising results (accuracy = 79.25%; Kappa = 0.50; re- call true = 0.62; recall false = 0.87) and that encouraged us to proceed to the implementation of the system proper.  Figure 3: Bayesian network of the aective state reasoner.  The aective state reasoner receives the aective state of the student (based on speech and interaction) as well as infor- mation about whether the previous feedback was followed. For each feedback type, the enhanced aective state is pre- dicted. This is used to determine which feedback type will be most eective at enhancing the aective state at any given time.  3.4 Feedback layer (affective state presenta- tion model)  The aim of the aective state presentation model is to present the feedback in a way that enhances the students aective state. In our learning environment, the feedback can be presented in a low-interruptive way by highlighting a light bulb, which indicates feedback available (see Fig- ure 4). Additionally, the feedback can be presented in a high-interruptive way by providing a pop-up window that includes the relevant feedback (See Figure 5).  Figure 4: Low-interruptive feedback: the light bulb graphic (at the top of the screen) glows, indicating that feedback is available.  We conducted a second Wizard-of-Oz study that investi- gated if there was a dierence in a students aective state when the feedback was either presented in a low-interruptive way through the light bulb, or in the high-interruptive way through the pop-up window [13]. Similar to the Wizard- of-Oz studies described earlier, dierent types of feedback were provided to the students. However, in this study this feedback was either presented in a low-interruptive or in a high-interruptive way. We annotated the students aec- tive states after the studies based on video and speech data (Kappa = .52, p<.001).  The data from the study was used to train a Bayesian network that is able to predict an enhanced aective state    Feedback type Example AFFECT BOOSTS Well done. Youre working really hard! AFFIRMATION PROMPTS The way that you worked that out was excellent. Now go to  the next task. INSTRUCTIVE FEEDBACK Use the comparison box to compare your fractions. OTHER PROBLEM SOLVING FEEDBACK  Good. What do you need to do now, to complete the fraction  REFLECTIVE PROMPTS What do you notice about the two fractions TALK ALOUD PROMPTS Please explain what are you doing. TASK SEQUENCE PROMPTS Are you sure that you have answered the task fully Please  read the task again.  Table 1: Examples of feedback types  Figure 5: High-interruptive feedback: pop-up win- dow that includes a feedback message.  Figure 6: Bayesian network of the aective state presentation model.  by adapting the presentation of the feedback. Figure 6 shows the Bayesian network of the aective state presen- tation model.  The dataset contained 266 cases of aective states that occurred before and after feedback was presented (either high- or non-interruptive) as well as student interaction data (whether or not previous feedback had been followed). Using this data set and employing a 10-fold cross-validation gives encouraging results (accuracy = 82.38%; Kappa = 0.53; re- call true= 0.65; false= 0.87).  The Bayesian network of the aective state presentation model is similar to the Bayesian network of the aective state reasoner, but diers in adapting the presentation of the feedback rather than the feedback type.  The aective state presentation model receives the af- fective state of the student as well as information about  whether previous feedback was followed. Based on this, the most eective presentation of the feedback is detected - one that aims to enhance the aective state of the student.  4. EVALUATION As mentioned, we were interested in investigating the po-  tential of the learner model and, as an extension, the adap- tive support to student learning. The iTalk2learn project ran a series of formative and summative evaluation stud- ies that considered a range of questions. Of relevance here, was a study looking particularly into whether feedback tai- lored to the aective state of the students enhanced their learning experience and performance. We were particularly interested in the following sub-questions:   Can speech and interaction be used eectively as in- puts to detect a students aective states   Can a students learning experience be enhanced when the feedback is tailored to their aective state   Can a students behaviour towards the task be en- hanced when the feedback is tailored to their aective state   Do students have higher learning gains when feedback is adapted to their aective state  To address these questions, we evaluated the system and intelligent support by comparing one version that included the aect-aware support with a version where the aect- aware support was switched o.  4.1 Participants 77 participants took part in the evaluation. They were  all primary school students, aged between 8 and 10 years old. Parental consent, for their involvement in the study, was obtained for all students.  4.2 Procedure The participating students were roughly stratified, accord-  ing to previous teacher assessments of the childrens math- ematical ability, and then randomly allocated to two sub- groups (approximately equal in size, with each group hav- ing approximately the same number of high, middle and low achieving students). The first group (N = 41) was assigned to the aect condition: the students were given access to the full iTalk2Learn system, which uses the students aective    state and their performance to determine the appropriate feedback and its presentation. The second group of students (N = 36) was assigned to the non-aect condition: they were given access to a version of the iTalk2Learn system in which feedback is based on the students performance only.  Two sessions, one for each condition, were undertaken in each school. At the beginning of each session, students com- pleted an online questionnaire that assessed their knowledge of fractions (the pre-test). This was followed by 40 minutes during which the students engaged with fractions tasks in a version of the iTalk2Learn system that, according to the experimental condition, included either the aect-aware or the non-aect-aware support.  Based on results from our Wizard-of-Oz studies [20], two sets of support were identified that dier in their impact for enhancing a students learning experience by adapting feedback based on their aective states:   No impact for adaptation based on aect:   TALK ALOUD PROMPTS were based on inter- action only and were provided in the aect con- dition only, when students did not say anything for 30 seconds.   TASK SEQUENCE PROMPTS were based on in- teraction only and were provided when students try to go to the next task when they have not completed the current task.   AFFIRMATION PROMPTS were based on per- formance and were provided when students suc- cessfully completed the task.  All of these feedback types were provided in both con- ditions in a high-interruptive way (pop-up window).   High impact for adaptation based on aect:   AFFECT BOOSTS were based on a students af- fective state and were provided in the aect con- dition only.   INSTRUCTIVE FEEDBACK, OTHER PROB- LEM SOLVING FEEDBACK and REFLECTIVE PROMPTS were tailored based on aect within the aect condition, and based on performance within the non-aect condition.  In the aect condition the presentation of the feed- back (high-interruptive (pop-up) or low-interruptive (light bulb)) was based on the students aective state. In contrast, in the non-aect condition these feedback types were presented in a low-interruptive way (light bulb). An exception to this was the REFLECTIVE prompts, which were provided in a high-interruptive way at the end of the task in the non-aect group.  While the students engaged with the system, the aective states of a subset of the students (aect condition: N = 26; non-aect condition: N = 22) were monitored and noted using the Baker-Rodrigo Ocumpaugh Monitoring Protocol (BROMP) and the Human Aect Recording Tool (HART) Android mobile app. BROMP gives strict guidelines on how the aective states of students are detected (by e.g. body posture, facial expression and engagement with the learning environment). The HART mobile app was then used to note the aective states detected with the BROMP protocol.  After the 40 minutes, the students completed a second online questionnaire that again assessed their knowledge of fractions (a post-test similar to the pre-test).  5. RESULTS  5.1 Affect detection In the aect condition, the students aective states were  detected automatically by the system, as it analysed their speech and their interaction as described earlier. Addition- ally, the aective states were annotated by a researcher us- ing the HART mobile app with the BROMP method, also as described earlier. The aective states that were detected automatically include flow, confusion, frustration, boredom, and surprise. With the BROMP method the aective states that the researchers detected included the automatically de- tected ones and two additional aective state: delight and eureka.  Both of those data sources include time stamps, identi- fying when the particular aective state occurred. The af- fective state from the automatic detection and the HART annotations were matched according to their time stamp (with a 30 seconds window).  There was a moderate agreement between the automatic detection and the HART annotations, Kappa = .53, p<.001 (74.07% agreement). The dierence is partly due to the two aective states that were detected with the HART tool but that were not included in the automatic detection (i.e. de- light and eureka). Additionally, we knew from our formative phase that surprise and boredom are dicult to detect au- tomatically and/or rare. Excluding those aective states, a high agreement between the automatic detection and the HART annotations is achieved, Kappa = .62, p<.001 (80.00% agreement). However, this result should be read with cau- tion. We ignore the human annotation that the system can- not detect and assume that the annotated states either side of the ignored states are less transient, which together prob- ably suggest a higher agreement than is really the case. Nev- ertheless, the result is acceptable, especially given that the eect of a misclassification is an intervention that in the best case can help the student and in the worst case can be ignored.  5.2 Adapting feedback message types In order to investigate dierences between the conditions  in respect to the adaptations, we outline dierences in adapt- ing the types of feedback messages below. In the aect con- dition, 1971 feedback messages were provided to students. In the non-aect condition students received 2007 messages. Figure 7 shows how these messages were distributed across the dierent feedback types.  In order to investigate dierences between the two con- ditions (aect and non-aect), a multivariate ANOVA was conducted for the dierent feedback types. Using Pillais trace, there was a significant eect of the condition on the number of dierent types of feedback messages received, V = .929, F(5,71) = 187.045, p < .001. Separate t-tests on each feedback type revealed significant eects of adapting message type based on aect. There was a dierence in how often AFFIRMATION prompts were provided between the aect (M = 2.51, SD = 2.09) and the non-aect (M = 5.33, SD = 2.41) group (t(75) = -5.50, p < .001, d = -1.25). There was also a large dierence in how much INSTRUCTIVE    Figure 7: Feedback types provided in the aect and non-aect condition.  feedback was provided between the aect (M = 10.32, SD = 7.04) and the non-aect (M = 37.14, SD = 11.75) group (t(55.703) = -11.94, p < .001, d = -2.769). There were also large dierences between conditions for OTHER PROBLEM SOLVING support (aect: M = 6.05 , SD = 2.55 ; non- aect: M = 0.97 , SD = 2.21; t(74.991) = 9.36, p < .001, d = 2.129), REFLECTIVE prompts (aect: M = 7.80, SD = 3.49 ; non-aect: M = 5.53 , SD = 2.21; t(68.501) = 3.46, p < .001, d = 0.846), and TASK SEQUENCE prompts (af- fect: M = 3.12 , SD = 2.60 ; non-aect: M = 6.78 , SD = 4.22; t(56.679) = -4.50, p < .001, d = 1.044). All of these results were statistically significant.  As described earlier, AFFECT BOOSTS and TALK ALOUD prompts were only provided in the aect condition (aect boosts: M = 0.80, SD = 1.40; talk aloud prompts: M = 17.46, SD = 5.92) and thus could not be compared with the non-aect condition.  5.3 Adapting presentation of feedback As described earlier, the feedback message was either dis-  played in a low-interruptive (light bulb) or in a high-interruptive way (pop-up). The way in which the feedback was displayed depended on the impact for adapting feedback based on af- fective states and, if they were in the aect condition, on the students aective state.  When feedback was low-interruptive (a glowing light bulb), students could either click on the light bulb and receive the feedback or they could ignore the light bulb and not receive the feedback. In the aect condition, 955 feedback messages were ignored (M = 23.00, SD = 7.54). In the non-aect con- dition, students ignored 1044 feedback messages (M = 29.00, SD = 11.05). An independent t-test showed that students ignored fewer messages in the aect condition than in the non-aect condition (t(60.624) = -2.61, p < .05, d = -0.634), a result that was statistically significant.  5.4 Affective states and task behaviour We were particular interested in identifying if a students  aect and their task behaviour can be enhanced through adapting the feedback (type and presentation) according to their aective states.  As described earlier, for a subset of students (aect con-  dition: N = 26; non-aect condition: N = 22) the aec- tive states and task behaviour were monitored by using the Baker-Rodrigo Ocumpaugh Monitoring Protocol (BROMP) and the Human Aect Recording Tool (HART) Android mo- bile app [23]. For each student, a set of aective states and task behaviour was annotated. Based on these annotations, the percentage of time that a student was in a particular af- fective state and their task behaviour were calculated. This was used for further analysis as described below.  5.4.1 Affect  Figure 8 shows the dierent types of aective states that were detected in the aect and non-aect condition.  Figure 8: Students aective states during the main evaluation session in the aect and non-aect con- dition.  In both conditions students were mainly in flow (aect: M = 58.12, SD = 22.23; non-aect: M = 52.98, SD = 17.41; d = 0.257). This was followed by confusion (aect: M = 28.77, SD = 23.28; non-aect: M = 27.36, SD = 18.21; d = 0.067) and boredom (aect: M = 9.54, SD = 13.33; non- aect: M = 16.08, SD = 7.45, d = -0.606). Only a few were frustrated (aect: M = 2.01, SD = 3.15; non-aect: M = 1.54, SD = 2.36; d = 0.169), surprised (aect: M = 1.03, SD = 1.83; non-aect: M = 0.74, SD = 2.07; d = 0.148), or delighted (aect: M = 0.53, SD = 1.33; non-aect: M = 1.19, SD = 2.50; d = -0.33).  In order to investigate dierences between the two con- ditions (aect and non-aect), a multivariate ANOVA was conducted for the dierent aective states. Using Pillais trace, no significant eect of the condition on the aective states could be detected, V = .188, F(6,41) = 1.586, p > .05. However, the medium eect size in boredom (d = 0.606) can be seen as an indicator that students were indeed less bored within the aect condition, just not significantly so in this sample. The medium eect can be seen as an indicator that we might find this eect in other samples. Adapting feed- back based on aect can decrease boredom.  5.4.2 Task behaviour  Figure 9 shows the dierent types of behaviour that oc- curred during the evaluation.  In both conditions, students were mainly on task (aect: M = 83.58, SD = 13.33; non-aect: M = 82.42, SD = 8.29,    Figure 9: Students task behaviour during the main evaluation session in the aect and non-aect con- dition.  d = 0.105). Fewer students did have an on task conversation (aect: M = 7.24, SD = 7.86; non-aect: M = 7.36, SD = 6.02, d = -0.017), were o-task (aect: M = 5.39, SD = 6.48; non-aect: M = 9.87, SD=6.03, d = -0.716), or reflecting on the task (aect: M = 3.38, SD = 9.86; non-aect: M = 0.23, SD = 0.75, d = 0.451). Very few were gaming the system (aect: M = 0.41, SD = 1.45; non-aect: M = 0.12, SD = 0.55, d = 0.264).  In order to investigate dierences between the two condi- tions (aect and non-aect), two multivariate ANOVAs were conducted: one for o-task, and one for on-task behaviours. Using Pillais trace, no significant eect of the condition on on-task behaviours could be detected, V = .125, F(3,44) = 2.094, p > .05. However, there was a significant eect of condition on o-task behaviours, V = .135, F(2,45) = 3.519, p < .05. Follow-up t-tests revealed a large dierence on stu- dents o-task behaviour. Students in the aect condition were less o-task than students in the non-aect condition, t(46) = -2.46, p<.05, d = -.716, a result that was statisti- cally significant.  5.5 Learning gains Figure 10 shows the students performance when answer-  ing fractions tasks before and after they had used the learn- ing environment in the dierent conditions.  In the aect condition students increased their knowledge of fractions from M = 2.49 (SD = 1.65) to M = 3.83 (SD = 1.46). In the non-aect condition students increased their knowledge from M = 2.44 (SD = 1.58) to M = 3.33 (SD = 1.71). An ANOVA repeated measures showed an increase of knowledge in both groups (F(1,75) = 43.94, p<.001, 2p = .369), a result that was statistically significant.  Although, the dierence in learning gains between the groups was not significant (F(1,75) = 1.81, p>.05), the over- all tendency of the aect condition showing higher learning gains warrants further investigation.  6. DISCUSSION The results of our evaluation will be discussed in respect  to our main research questions.  Figure 10: Students learning gains in the aect and non-aect condition.  6.1 Can speech and interaction be used as in- put to detect a students affective states  In our system, the automatic detection of a students aec- tive states is based on their speech and their interaction with the learning environment. This automatic detection was compared to the aective states that were annotated with the HART mobile app using the BROMP method (where the aective states are detected by looking at a students facial expression, body posture and engagement with the learning environment). The comparison revealed a medium agree- ment when taken into account all aective states (flow, con- fusion, frustration, boredom, surprise, delight and eureka).  Out of those aective states delight and eureka were not included in the automatic aect detection. Additionally, we knew from our formative phase that surprise and boredom are dicult to detect automatically and are rare. Other researchers (e.g. [3]) also reported on the low frequency of some aective states, such as surprise. Excluding these aective states a high agreement between the human anno- tation and automatic detection was indicative of the quality of the latter.  Nevertheless, our analysis shows that the detection of af- fective states using information from speech and interaction data is very promising and warrants further research.  6.2 Can a students learning experience be en- hanced when the feedback is tailored to their affective state  In both conditions, students were mainly in flow, followed by confusion. This is similar to results from other research, such as [3]. Flow and confusion contribute towards learning and can be seen as positive aective states.  Students in the aect condition were less bored than stu- dents in the non-aect condition. While this eect was not significant, given the exploratory nature of this work, we see this as a first indicator that tailoring feedback to aective states can enhance students learning experience. Future analyses will investigate how tailoring feedback could en- hance the students learning experience. For example: was    tailoring of feedback able to transfer students from a nega- tive aective state (such as it boredom) into a positive af- fective state (such as flow)  The adaptation of the presentation of the feedback might also have been important, as students in the non-aect con- dition might have ignored feedback presented in the low- interruptive way and therefore might have moved from e.g. confusion state to a negative aective state, e.g. boredom. This needs further investigation.  6.3 Can a students behaviour towards the task be enhanced when the feedback is tailored to their affective state  Students in both conditions were mainly on task. This might be due to the nature of the exploratory learning en- vironment, which appeared to engage the students.  However, there was a dierence between the groups in o- task behaviour. Students in the non-aect group were more o-task than students in the aect condition, a result that was statistically significant. Here, the adaptations of the feedback types as well as the presentation of the feedback based on the students aective state seem to have had an eect on their engagement with the task. Students that are bored or frustrated might show o-task behaviour. It looks as if the adaptations based on a students aect are able to reduce such negative aective states, which reduce o-task behaviour.  Anecdotal evidence from class observations and discus- sions among and with students suggests that students might have found the task more interesting when feedback was adapted according to their aective states.  6.4 Do students have higher learning gains when feedback is adapted to their affective state  In both groups, student knowledge of fractions was en- hanced. This is a positive result that demonstrates the qual- ity of the tasks and the overall level of support even with- out aect-aware feedback. However, looking at the result in terms of the impact of the aect detection, the dierence between the groups in how much knowledge they gained was not statistically significant but the higher-learning gains in combination with the rest of our results and particularly the significantly higher self-reported interest are encouraging.  As described earlier, the adaptation of the presentation of the feedback might have been important for an increase in learning gains, as students in the non-aect condition might have ignored feedback that was presented in the low- interruptive way.  Also, the range of dierent types of feedback was spread more evenly in the aect condition than in the non-aect condition. In the non-aect condition mainly INSTRUC- TIVE FEEDBACK was provided, based on the students performance. Meanwhile, in the aect condition the feed- back types were tailored according to the students aect, and much more OTHER PROBLEM SOLVING FEEDBACK and REFLECTIVE PROMPTS were provided. Also inter- esting to see is that there were fewer TASK SEQUENCE PROMPTS in the aect condition than in the non-aect condition, which indicates that fewer students attempted to skip the task in the aect condition than in the non-aect condition.  This indicates that the aect aware support is able to lead students into positive aective states, such as flow where  they tend to benefit from reflective or non-instructive more open-ended problem solving feedback.  Because of the dierent types of feedback, students might have gained dierent types of knowledge in the dierent con- ditions. More INSTRUCTIVE FEEDBACK was provided to students in the non-aect condition, which might have led to an emphasis on procedural knowledge gains. Mean- while, more OTHER PROBLEM SOLVING FEEDBACK and REFLECTIVE PROMPTS were provided to students in the aect condition, which might have led to conceptual knowledge gains. This again warrants further investigation.  7. CONCLUSION We have developed an adaptive environment that provides  intelligent support according to a students aective states. It includes two Bayesian networks, which are able to predict an enhancement in a students aective state by adapting the type of feedback and the presentation of the feedback according to their aective states.  The intelligent aect-aware support is included in a learn- ing platform, where it can be switched on or o. This fea- ture was used to evaluate the aect-aware support (aect condition), by comparing it to support that was based on performance only (non-aect condition). During our evalua- tion the students aective states were annotated while they were using the system in either conditions. The results show that students in the aect condition showed less o-task be- haviour then students in the non-aect condition, a result that was statistically significant. Additionally, the results indicate that in the aect-aware condition students were less bored than students in the non-aect condition. These are important findings as o-task behaviour and boredom can have a negative impact on learning.  The results also underpin the eectiveness of the perfor- mance of the training data of the Bayesian networks, as stu- dents in the aect condition were more engaged (less o-task and less bored) than students in the non-aect condition.  Future work includes the refinement of the Bayesian net- works with the newly collected data. Additionally, we plan to analyse our data further by looking at a students aective states and their interactions with the learning environment.  8. ACKNOWLEDGMENTS This research has been funded by the European Union in  the Seventh Framework Programme (FP7/2007-2013) in the iTalk2Learn project (318051).  9. REFERENCES [1] J. Ahn and P. Brusilovsky. Adaptive visualization for  exploratory information retrieval. Information Processing and Management, 49:11391164, 2013.  [2] M. Askeland. Sound-based strategy training in multiplication. European Journal of Special Needs Education, 27(2):201217, 2012.  [3] R. S. J. d. Baker, S. K. DMello, M. T. Rodrigo, and A. C. Graesser. Better to be frustrated than bored: The incidence, persistence, and impact of learners cognitive-aective states during interactions with three dierent computer-based learning environments. Int. J. Hum.-Comput. Stud., 68(4):223241, apr 2010.  [4] G. Carenini, C. Conati, E. Hoque, B. Steichen, D. Toker, and J. Enns. Highlighting interventions and    user dierences: Informing adaptive information visualization support. In CHI 14 Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 18351844, 2014.  [5] M. Chi. Self-explaining expository texts: The dual processes of generating inferences and repairing mental models. In R. Glaser, editor, Advances in instructional psychology, pages 161238. Mahwah, NJ: Lawrence Erbaum Associates, 2000.  [6] C. Conati and H. MacLaren. Empirically building and evaluating a probabilistic model of user aect. User Modeling and User-Adapted Interaction, 2009.  [7] T. del Soldato and B. du Boulay. Implementation of motivational tactics in tutoring systems. Journal of Artificial Intelligence in Education, 6(4), 1996.  [8] S. DMello and A. Graesser. Multimodal semi-automated aect detection from conversational cues, gross body language, and facial features. User Modeling and User-adapted Interaction, 20:147187, 2010.  [9] S. DMello and A. Graesser. Autotutor and aective autotutor: Learning by talking with cognitively and emotionally intelligent computers that talk back. ACM Transactions on Interactive Intelligent Systems, 2(4):138, 2013.  [10] S. K. DMello, B. Lehman, R. Pekrun, and A. C. Graesser. Confusion can be beneficial for learning. Learning & Instruction, 29(1):153170, 2014.  [11] D. Gotz and Z. Wen. Behaviour driven visualization recommendation. In Intelligent User Interfaces, IUI 09, pages 315324, 2009.  [12] B. Grawemeyer and R. Cox. Graphical data displays and database queries: Helping users select the right display for the task. In Smart Graphics, 5th International Symposium, pages 5364, 2005.  [13] B. Grawemeyer, W. Holmes, S. Gutirrez-Santos, A. Hansen, K. Loibl, and M. Mavrikis. Light-bulb moment towards adaptive presentation of feedback based on students aective state. In IUI 2015, 2015.  [14] B. Grawemeyer, M. Mavrikis, A. Hansen, C. Mazziotti, and S. Gutirrez-Santos. Employing speech to contribute to modelling and adapting to students aective states. In EC-TEL 2014, 2014.  [15] B. Grawemeyer, M. Mavrikis, W. Holmes, A. Hansen, K. Loibl, and S. Gutirrez-Santos. Aect matters: Exploring the impact of feedback during mathematical tasks in an exploratory environmen. In AIED 2015, 2015.  [16] S. Gutirrez-Santos, M. Mavrikis, and G. Magoulas. A separation of concerns for engineering intelligent support for exploratory learning environments. Journal of Research and Practice in Information Technology, 44(3):347360, 2012.  [17] R. Janning, C. Schatten, and L. Schmidt-Thieme. Feature analysis for aect recognition supporting task sequencing in adaptive intelligent tutoring systems. In EC-TEL 2014, 2014.  [18] R. Janning, C. Schatten, and L. Schmidt-Thieme. Perceived task-diculty recognition from log-file information for the use in adaptive intelligent tutoring systems. International Journal of Artificial Intelligence in Education, in press.  [19] B. Kort, R. Reilly, and R. Picard. An aective model of the interplay between emotions and learning. In IEEE International Conference on Advanced Learning Technologies, number 43-46, 2001.  [20] M. Mavrikis, B. Grawemeyer, A. Hansen, and S. Gutirrez-Santos. Exploring the potential of speech recognition to support problem solving and reflection - wizards go to school in the elementary maths classroom. In EC-TEL 2014, pages 263276, 2014.  [21] M. Mavrikis, S. Gutirrez-Santos, E. Geraniou, and R. Noss. Design requirements, student perception indicators and validation metrics for intelligent exploratory learning environments. Personal and Ubiquitous Computing, 17(8):16051620, 2013.  [22] C. Mazziotti, W. Holmes, M. Wiedmann, K. Loibl, N. Rummel, M. Mavrikis, A. Hansen, and B. Grawemeyer. Robust student knowledge: Adapting to individual student needs as they explore the concepts and practice the procedures of fractions. In Workshop on Intelligent Support in Exploratory and Open-ended Learning Environments Learning Analytics for Project Based and Experiential Learning Scenarios at AIED 2015, 2015.  [23] J. Ocumpaugh, R. Baker, and M. Rodrigo. Baker-rodrigo observation method protocol (bromp) 1.0. training manual version 1.0. Technical report, New York, NY: EdLab. Manila, Philippines: Ateneo Laboratory for the Learning Sciences., 2012.  [24] R. Pekrun. The control-value theory of achievement emotions: Assumptions, corollaries, and implications for educational research and practice. J. Edu. Psych. Rev., pages 315341, 2006.  [25] K. Porayska-Pomsta, M. Mavrikis, and H. Pain. Diagnosing and acting on student aect: the tutors perspective. User Modeling and User-Adapted Interaction, 18(1):125173, 2008.  [26] Sail-Labs. http://www.sail-labs.com. [27] S. Salmeron-Majadas, O. Santos, and J. Boticario.  Exploring indicators from keyboard and mouse interactions to predict the user aective state. In EDM 2014, 2014.  [28] O. Santos, M. Saneiro, S. Salmeron-Majadas, and B. J.G. A methodological approach to elicit aective educational recommendataions. In IEEE 14th International Conference on Advanced Learning Technologies, 2014.  [29] L. Shen, M. Wang, and R. Shen. Aective e-learning: Using emotional data to improve learning in pervasive learning environment. Educational Technology & Society, 12(2):176189, 2009.  [30] J. Sweller, J. G. van Merrienboer, and G. W. Paas. Cognitive Architecture and Instructional Design. Educational Psychology Review, 10:251  296+, 1998.  [31] T. Vogt and E. Andr. Comparing feature sets for acted and spontaneous speech in view of automatic emotion recognition. In Multimedia and Expo (ICME05), pages 474477, 2005.  [32] B. Woolf, W. Burleson, I. Arroyo, T. Dragon, D. Cooper, and R. Picard. Aect-aware tutors: recognising and responding to student aect. Int. J. Learning Technology, 4(3-4):129164, 2009.    Introduction  Related work  The iTalk2Learn platform  Learner model  Analysis layer (affect detection)  Reasoning layer (affective state reasoner)  Feedback layer (affective state presentation model)   Evaluation  Participants  Procedure   Results  Affect detection  Adapting feedback message types  Adapting presentation of feedback  Affective states and task behaviour  Affect  Task behaviour   Learning gains   Discussion  Can speech and interaction be used as input to detect a student's affective states  Can a student's learning experience be enhanced when the feedback is tailored to their affective state  Can a student's behaviour towards the task be enhanced when the feedback is tailored to their affective state  Do students have higher learning gains when feedback is adapted to their affective state   Conclusion  Acknowledgments  References   "}
{"index":{"_id":"15"}}
{"datatype":"inproceedings","key":"Allen:2016:IBE:2883851.2883939","author":"Allen, Laura K. and Mills, Caitlin and Jacovina, Matthew E. and Crossley, Scott and D'Mello, Sidney and McNamara, Danielle S.","title":"Investigating Boredom and Engagement During Writing Using Multiple Sources of Information: The Essay, the Writer, and Keystrokes","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"114--123","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883939","doi":"10.1145/2883851.2883939","acmid":"2883939","publisher":"ACM","address":"New York, NY, USA","keywords":"corpus linguistics, intelligent tutoring systems, natural language processing, stealth assessment, writing","abstract":"Writing training systems have been developed to provide students with instruction and deliberate practice on their writing. Although generally successful in providing accurate scores, a common criticism of these systems is their lack of personalization and adaptive instruction. In particular, these systems tend to place the strongest emphasis on delivering accurate scores, and therefore, tend to overlook additional indices that may contribute to students' success, such as their affective states during writing practice. This study takes an initial step toward addressing this gap by building a predictive model of students' affect using information that can potentially be collected by computer systems. We used individual difference measures, text indices, and keystroke analyses to predict engagement and boredom in 132 writing sessions. The results suggest that these three categories of indices were successful in modeling students' affective states during writing. Taken together, indices related to students' academic abilities, text properties, and keystroke logs were able classify high and low engagement and boredom in writing sessions with accuracies between 76.5% and 77.3%. These results suggest that information readily available in writing training systems can inform affect detectors and ultimately improve student models within intelligent tutoring systems.","pdf":"      Investigating Boredom and Engagement during Writing  Using Multiple Sources of Information:   The Essay, The Writer, and Keystrokes      Laura K. Allen  Arizona State University   PO Box 872111     Tempe, AZ, 85287   LauraKAllen@asu.edu      Scott Crossley  Georgia State University   34 Peachtree Ave, St 1200  Atlanta, GA 30303   scrossley@gsu.edu  Caitlin Mills   University of Notre Dame   118 Haggar Hall     Notre Dame, IN, 46556   cmills4@nd.edu      Sidney DMello  University of Notre Dame   118 Haggar Hall     Notre Dame, IN, 46556   sdmello@nd.edu  Matthew E. Jacovina  Arizona State University   PO Box 872111     Tempe, AZ, 85287   Matthew.Jacovina@asu.edu      Danielle S. McNamara   Arizona State University   PO Box 872111     Tempe, AZ, 85287   Danielle.McNamara@asu.edu         ABSTRACT  Writing training systems have been developed to provide students  with instruction and deliberate practice on their writing. Although  generally successful in providing accurate scores, a common  criticism of these systems is their lack of personalization and  adaptive instruction. In particular, these systems tend to place the  strongest emphasis on delivering accurate scores, and therefore,  tend to overlook additional indices that may contribute to students  success, such as their affective states during writing practice. This  study takes an initial step toward addressing this gap by building a  predictive model of students affect using information that can  potentially be collected by computer systems. We used individual  difference measures, text indices, and keystroke analyses to predict  engagement and boredom in 132 writing sessions. The results  suggest that these three categories of indices were successful in  modeling students affective states during writing. Taken together,  indices related to students academic abilities, text properties, and  keystroke logs were able classify high and low engagement and  boredom in writing sessions with accuracies between 76.5% and  77.3%. These results suggest that information readily available in  writing training systems can inform affect detectors and ultimately  improve student models within intelligent tutoring systems.    Categories and Subject Descriptors   K.3.1 [Computer Uses in Education] Computer-assisted  instruction (CAI); I.2.7 [Natural Language Processing] Text  analysis, discourse; J.5 [Computer Applications: Arts and  Humanities]: Linguistics   General Terms  Algorithms, Measurement, Performance, Languages, Theory   Keywords  Intelligent Tutoring Systems, Natural Language Processing, stealth  assessment, corpus linguistics, writing   1 INTRODUCTION  An individuals ability to effectively communicate ideas through  text is an increasingly important skill in todays society. Indeed, in  both educational and professional contexts, writing skills have  become necessary for success [1-2]. Unfortunately, strong writing  skills can be extremely challenging for students to develop and  refine. This is largely due to the complex host of skills required to  produce high-quality texts, such as strategically managing memory,  developing strong vocabulary knowledge, setting goals, and  producing coherent arguments [3-4]. Given the difficulty of  developing these skills, it is not surprising that students consistently  underachieve on tests of writing proficiency [e.g., 5-6].   In order for students to successfully develop the skills needed to  produce high-quality texts, they need to be provided with explicit  instruction and feedback. Specifically, research on writing  instruction suggests that students benefit most from a combination  of strategy instruction [7] and extended practice with individualized  feedback [8]. One significant problem with these  recommendations, however, relates to the difficulty of  implementing them within typical classrooms. The time needed to  prepare classroom materials, teach courses, and read, edit, and  provide personalized feedback on students essays can be  overwhelming for teachers. This is particularly true today, as  reports indicate that teachers are now faced with increasingly large   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883939             class sizes and, as a result, have less time to devote to instruction,  planning, and grading [9].    To help alleviate some of the difficulties facing writing instructors,  researchers and technology developers have placed an increased  focus on designing computer-based systems that can provide  students with automated writing instruction and practice [10-11].  These writing training systems have been developed with a number  of different goals in mind, ranging from the automatic scoring of  essays to the delivery of personalized feedback and the instruction  of writing strategies [12-15]. Automated essay scoring (AES)  systems, for instance, focus on the assessment of the structure,  content, and quality of student essays [11; 16]. These systems  largely rely on natural language processing (NLP) and machine  learning techniques to accurately model the scores assigned by  expert raters [17].  These standalone AES systems have more recently been  incorporated into educational learning environments, such as  automated writing evaluation (AWE) systems [18-21] and  intelligent tutoring systems (ITSs) [13]. The goal of these systems  extends beyond the assessment of essay quality  rather, they focus  on providing students with personalized feedback, as well as (in  some cases) explicit instruction.  Despite their general success [e.g., 11-14], however, these systems  have not gone without criticism [e.g., 16; 22-24]. In particular,  critics have noted that AES assessments often miss out on  components of rhetorical effectiveness and argumentation, and the  feedback can often be impersonal and lacking in human sensitivity.  The concerns noted about these systems are valid, and pose new  challenges for developers of writing training systems. In particular,  researchers have begun to shift their focus from simply providing  accurate essay scores to providing feedback and adaptive  instruction that is more nuanced and focuses on specific  characteristics of individual students.      To illustrate the importance of this goal, consider two students,  Kevin and Cecile, who write and submit essays to a particular  writing system. While Kevin may be deeply engaged and interested  in the topic of the essay prompt, his focus on the content might  cause him to lose sight of some of the details that could improve  his essay score, such as choosing more appropriate words and  correcting spelling errors. Cecile, on the other hand, may produce  an essay that is generally lacking in basic errors; however, her  disinterest and boredom in the assignment may be apparent in her  lack of compelling arguments and attention grabbing techniques.  In this example, both students receive the same score from the  system; however, their different affective states while writing the  essay suggests that they may benefit from different feedback and  adaptive instruction. Kevin may benefit from targeted feedback that  acknowledges his effort and investment in the task, but that  encourages him to take an additional look at the essay to improve  grammar errors and word choices. Cecile, on the other hand, may  benefit from feedback that reminds her of the importance of the  topic, or suggests she engage in a game-based practice activity to  increase her motivation.     One way to adjust to these differences among students and learning  sessions is to embed assessments that are based on more than their  essay scores. These measures can be hidden from users (i.e.,  stealth assessments [25-26]) and can inform more specific  instruction and feedback that is tailored to students strengths and  weaknesses, as well as their potential affective states and learning  preferences. Recent research suggests that affect is present through  the writing process, and different affective states can predict the  quality of students writing outcomes [27-28]. There has been some   success with respect to detecting affective states in computer-based  learning environments (e.g., as reviewed in [29-30]); however,  limited attention has been paid to developing affect detectors for  systems that develop writing proficiency.    In the current paper, we address this gap in the literature by  examining the efficacy of indices commonly collected in writing  training systems to detect students affective states during  individual writing sessions. In particular, we examine whether  individual difference measures, linguistic and semantic properties  of the generated text, and keystroke measures can be used to model  affective states. Second, we aim to determine whether each of these  index types (i.e., individual differences, text properties, and  keystroke measures) contribute unique predictive power in  modeling affect during writing. Our ultimate goal is to use these  models to provide more individualized tutoring and feedback to  students.     1.1 Adaptive Feedback and Instruction  In an effort to provide more adaptive instruction and feedback to  students, computer-based learning environments often rely on  measures of performance (and other relevant indices) that can be  collected without disrupting the learning task [25-26]. These  stealth assessments can take many forms, from the trajectories of  a users mouse movements to the linguistic structure of their text  responses. Most importantly, once these assessments have been  developed, they can be used to improve student models, which can  inform feedback delivery and instructional recommendations [31].   1.1.1 Individual Difference Measures  One potential method for increasing the validity and  personalization of AWE systems (and other computer-based  learning environments) is to first take into consideration any  information that is already known about the student users. Given  that one of the strongest criticisms facing developers of writing  training systems is that the systems are impersonal, the inclusion of  student-level information, such as literacy skills and cognitive  abilities, may increase the sensitivity of algorithms to model  student users. Indeed, relevant to this study, individual differences  have been shown to be important predictors of affective states  during writing [27]. This process of contextualizing the writing  assessment based on individual differences is an important step,  given that a primary goal of systems is to provide more adaptive  and personalized feedback to students.    Recently, researchers have begun to consider the inclusion of  individual differences in algorithms for predicting essay score [32].  In particular, Crossley and colleagues investigated the efficacy of  improving traditional AES methods (i.e., statistical modeling  human scores based on linguistic essay indices) by incorporating  student-level indices into the model. The results of their study  indicated that the combination of text and student indices led to  scoring accuracies that were comparable to the industry-standard  AES systems. Given the results of this study, it may be reasonable  to assume that individual differences among students may similarly  contribute to the accuracy of algorithms designed to measure  student affect, rather than their essay scores.    1.1.2 Natural Language Processing and Writing  Indices related to text indices from the essays provide additional  sources of information. Natural Language Processing (NLP)  techniques are commonly employed in AES systems in order to  extract various linguistic and semantic indices of students essays.  These indices have been used extensively in prior research on           writing, particularly with the aim of improving models for  predicting expert ratings of essay quality [11; 16; 33-35].    More recently, researchers have begun to investigate whether these  NLP techniques can similarly be used to model individual  differences among students. Allen and McNamara (2015) [36], for  example, used indices related to the lexical properties of students  essays to successfully model their scores on an unrelated  vocabulary knowledge assessment. Overall, these (and other)  previous studies suggest that NLP techniques are an extremely  powerful source of student data and can be used to inform stealth  assessments to improve student models. Despite the wealth of  previous research in this area, however, there is, to our knowledge,  no current research testing the efficacy of these text indices to  predict students affective states during writing.    1.1.3 Keystroke Analyses for Writing  A final source of system data that may be useful for modeling  students affective states is the keystroke data related to the  physical process of writing. Although researchers have made a  significant effort to leverage the indices of texts to better  understand writing quality and individual differences (as reviewed  above), there has been significantly less research on students  online writing processes. Specifically, most of the previous  research on writing has focused on students finished writing  products and not the moment-by-moment writing process. Studying  the writing process can help to reveal qualities of a writer and  written texts that are more difficult to measure with product  measures alone. Keystroke logging tools have been developed to  record the keys that writers press while typing. These tools provide  a unique means to study the processes associated with writing [37- 38], including investigations of struggling and expert writers [39]  and a preliminary study on the detection on affective states during  writing [37]. These tools are consistently improving; for example,  InputLog, a prominent logging tool, can interface with NLP tools,  affording analyses that include both keystroke information and  linguistic information, such as parts of speech [40].   To illustrate the potentially important value of these keystroke  analyses, consider the process of entering a state of flow during  writing [41]. How might your patterns in keystroke timing vary  when you enter into this flow state, as compared to when you are  struggling to generate ideas or are bored These differences may  play a key role in the modeling students affective states beyond the  written text itself. Additionally, these indices may be able to help  researchers identify and better understand the various states of  productivity during writing, which can ultimately inform  personalized feedback and instructional adaptations.    In a recent study, Bixler and DMello (2013) [37] conducted an  initial investigation of these questions. In particular, they collected  individual difference measures and keystroke data from student  writers to detect on-line affective states during writing (i.e., self- reported affective states in 15-second intervals). Results of their  analyses indicated that the combination of these behavioral  measures and student-level indices was able to detect boredom,  engagement, and neutral states between 11% and 38% above  baseline. Additionally, their results were able to generalize to new  individuals.    1.2 Writing Pal  One aim of the current research is to improve the adaptability of the  Writing Pal (W-Pal) system. W-Pal is an intelligent tutoring system  (ITS) that was developed to deliver explicit writing strategy  instruction and practice to high school and early college students   [13]. In contrast to the majority of writing training systems (see [10]  for a review), W-Pal strongly focuses on the teaching of strategies  for high-quality writing, in addition to providing multiple forms of  practice (i.e., strategy-specific practice and holistic essay writing  practice).   Strategy instruction in the W-Pal system covers the three primary  phases of the writing process: prewriting, drafting, and revising. In  the system, these strategies are taught in the context of individual  instructional modules that include: Freewriting and Planning  (prewriting); Introduction Building, Body Building, and  Conclusion Building (drafting); and Paraphrasing, Cohesion  Building, and Revising (revising; see Figure 1 for a screenshot of  the main W-Pal interface). Each of these instructional modules  contains multiple lesson videos, which are each narrated by an  animated pedagogical agent (see Figure 2 for example screenshots  of the videos) who describes and provides examples of specific  strategies that are important for writing.    Figure 2: Screenshots of the W-Pal Lesson Videos  Once students have viewed the lesson videos, they can unlock mini- games that provide them with opportunities to practice the writing  strategies in isolation before applying them in the context of a  complete essay. In W-Pal, students can practice the strategies with  identification mini-games, where they are asked to select the best  answer to a particular question, or generative mini-games, where  they produce natural language (typed) responses related to the  strategies they are practicing.    1.2.1 W-Pal Essay Scoring and Feedback  An important component of the W-Pal system is the automated  writing evaluation (AWE) component (i.e., the essay practice   Figure 1: Main Interface of the W-Pal System           component). This aspect of W-Pal contains a word processor in  which students can write essays in response to a set of SAT-style  prompts. Additionally, teachers have the option of adding their own  prompts to the system. Once a student has completed an essay, it is  submitted to the W-Pal system for grading. The W-Pal algorithm  [33] then calculates a variety of linguistic indices related to the  submitted essay and provides both summative and formative  feedback to the student (see Figure 3 for a screenshot of the  feedback screen).    The summative feedback provided by W-Pal consists of a holistic  essay score that ranges from 1 to 6 (described to students as Poor  to Great). The formative feedback, on the other hand, provides  information about the writing strategies that students can use to  improve the quality of their essays. After they have read the  feedback messages, students have the option to revise their essays  based on the feedback that they received.    Figure 3: Screenshot of the W-Pal Feedback   Formative feedback is an important component of writing  development, as it provides important knowledge to writers about  components of high-quality writing, as well as actionable  recommendations for how to improve. Examples of these  recommendations include: generating ideas and examples,  maintaining cohesion through explicit text connections, and  employing sophisticated words. The automated formative feedback  in W-Pal was specifically developed with this in mind, and provides  recommendations that relate to multiple writing strategies.   Previous research evaluating the efficacy of the W-Pal system has  found that this training results in improved essay scores, increased  strategy knowledge, and improved revising strategies [42-43].   1.3 Current Study  The purpose of the current study is to investigate the degree to  which the affective states that students experience during writing  sessions can be classified based on measures readily collected by  the W-Pal system. In particular, our aim is to use individual  difference measures, text indices (e.g., linguistic and semantic text  properties), and keystroke measures to classify whether a student  experienced either high or low general levels of boredom and  engagement over the course of an entire writing session. The  overarching aim of this line of research is to develop stealth  assessments of students affective states during the writing process,  which will ideally help to update student models in the W-Pal  system. Increasing the sensitivity of W-Pal to students affect is  expected to improve its adaptability through the development of  more nuanced and personalized feedback and recommendations.    To accomplish our initial goal, we collected essays from  undergraduate students, along with a number of individual  difference measures. Students provided retrospective judgments of  their affective states during the writing process by viewing a video  that displayed the students face along with a screen-capture video  of the computer interface used to write the essay. The linguistic and  semantic properties of the essays were calculated using two NLP  tools, Coh-Metrix [44], and SEANCE. Coh-Metrix calculates  information related to linguistic indices of text, whereas SEANCE  provides information related to semantic and affective information.  Finally, we recorded the keystrokes logged during the writing  process and calculated indices related to specific aspects of these  keystrokes. We hypothesized that the individual differences, text  properties, and keystroke indices would all provide unique  predictive power in classifying students writing sessions as high  or low engagement and high or low boredom. Additionally, we  predicted that engagement and boredom would be best classified  by different combinations of indices. For instance, general  engagement over the course of a writing assignment might be a  more fleeting affective state, detectable by rapid bursts of  activity, whereas boredom might manifest in more stable individual  difference measures, such as a general aversion to writing.    2 METHODS  2.1 Participants  Participants were 44 undergraduate students from a university in  the United States. Of these students, 68% were female, 45% were  Caucasian, 52% were African American, and 3% reported Other.  The students reported a mean age of 19.9 years. All students  participated in the study for course credit.   2.2 Individual Difference Measures  Participants were asked to self-report their ACT scores as a  measure of their scholastic aptitude. Additionally, their  apprehension towards writing was assessed with the Writing  Apprehension test (WAT) [45]. The WAT is a 26-item self-report  survey that prompts students to respond to multiple questions on a  5-point Likert scale related to their feelings toward the writing  process. The WAT scores are negatively related to apprehension  levels; thus, lower scores are indicative of more writing  apprehension. Students exposure to print was assessed using the  Author Recognition Test [46]. Participants were shown a list of 42  popular authors, such as J.R.R. Tolkien or Dean Koontz) and were  asked to check each author they recognized. Students scores were  simply the number of authors that were correctly recognized.     2.3 Data Collection Procedure  The participants were allotted 10 minutes to complete an essay on  each of the three essay topics. For each, the students were first  asked to select one of the subtopics described above. The  participants typed their essays on a computer where each keystroke  was logged, along with a timestamp, and the number of  milliseconds that had passed since the last keystroke. Video of  participants faces and computer screens were also recorded. In all,  participants completed three essays on the three topics (30 mins  writing time total).   2.3.1 Retrospective Affect Judgment  The participants provided self-reports of their affective states  immediately following the writing sessions. The judgments of the  writing sessions began by playing a video of the participants face  along with their screen capture video on a computer monitor,  similar to a cued-recall procedure [47-48]. The screen capture video  included the writing prompt and dynamically presented the text as           it had been written by the participants in order to provide them with  the context of the writing session. Participants were instructed to  make judgments on the affective states that were present at any  moment during the writing session by manually pausing the videos.  Additionally, they were instructed to make affect judgments at each  15-second interval  in these instances, the videos were  automatically paused. Participants provided their judgments on a  computer interface that allowed them to select one out of 15  affective states from an alphabetized drop down list. These states  included: anger, anxious, boredom, confusion, contempt, curiosity,  delight, disgust, fear, flow, frustration, happiness, neutral, sadness,  and surprise. Altogether, these affective judgments were made  based on the participants facial expressions, contextual cues from  the screen capture, the definitions of the affective states (presented  on a piece of paper), and their memories of the writing session.   The affect judgment task yielded 5,551 affect judgments across the  44 participants. The fourteen affective states cumulatively  accounted for 78.9% of the judgments, and neutral was reported for  the remaining 21.1% of the judgments. Importantly, the most  frequent affective state reported was engagement (flow) with an  occurrence rate of 35.4%, followed by boredom at 26.4%. Together  these two states accounted for over half of the affective  observations. In the current study, we chose to focus on  engagement and boredom because they comprised the majority of  the observations, and the remaining affective states were either  reported at very low frequencies or were inconsistently reported  across participants. Boredom and engagement were also found to  predict essay quality in previous research [28].   2.4 Corpus  The current corpus consisted of 132 essays, which were collected  from a previous experiment that examined the role of affect during  the writing process [27]. The experiment had a repeated measures  design that prompted students to write essays on three different  topics: academic, socially charged, and personal/emotional  experiences. The order of the essay topics was counterbalanced  across students using a 3  3 Latin Square design.   Participants were allowed to choose the subtopic of their essays  from a list of options in order to maximize their engagement in the  writing. The academic essay topics were adapted from the ACT  test (standardized test in the U.S.) and the subtopics included: time  spent in high school, the use of class discussions, and social skills  that are taught in schools. The socially charged essay subtopics  related to: abortion, gays in the military, and the death penalty.  Finally, the subtopics for the personal/emotional experience  essays included writing about an intense experience involving one  of the six basic emotions (i.e., anger, disgust, fear, happiness,  sadness, and surprise).    2.5 Essay Scoring  Two researchers scored the essays using a modified version of the  SAT rubric [49]. The SAT is a standardized test commonly used  for college admission in the United States. Essay quality was  measured on a 6-point scale with a score of 1 indicating little to no  mastery and several major flaws, 3 indicative a development of  mastery, but with one or more major flaws, and a 6 indicating clear  and consistent mastery with minor errors. The essays were  randomly divided between the two raters who independently scored  the entire corpus of essays. The resulting scores were then  standardized within each rater in order to remove potential rater  biases. Interrater reliability, computed on a random subset of essays  scored by both raters, was r = .91.   2.6 Text Analyses  Linguistic and semantic indices of students essays were obtained  from component scores reported by the Coh-Metrix and SEANCE  tools as discussed in greater detail below. In addition, the total  number of words was computed for each essay, as this index is a  strong predictor of essay quality [33].    2.6.1 Coh-Metrix  Coh-Metrix [44] is a computational text analysis tool that was  developed, in part, to provide deeper measures of text difficulty.  This tool analyzes texts at the word, sentence, and discourse levels;  thus, it can potentially offer more information about the specific  difficulties of a particular text. Previous work with Coh-Metrix  suggests that multiple aspects of a text coordinate to affect  subsequent comprehension. To account for these multiple textual  aspects, Graesser and colleagues (2011) [50] developed the Coh- Metrix Easability Components. These components provide  measures of the principal sources of text difficulty and are well  aligned with an existing multilevel framework [51].  Narrativity. The narrativity of a text reflects the degree to which a  story is being told, using characters, places, events, and other things  familiar to readers. Highly narrative texts are typically easier to  read.  Syntactic Simplicity. Syntactically simple texts contain shorter  sentences and more familiar and simple syntax. These texts are  typically easier to comprehend.  Word Concreteness. This component refers to texts that contain  concrete and meaningful words that can easily evoke mental  images. Increases in word concreteness correspond to easier and  more understandable texts.  Referential Cohesion. Referential cohesion reflects the degree to  which words and ideas overlap across a text. Texts that are high in  referential cohesion represent explicit connections between ideas  and are, consequently, easier to read.  Deep Cohesion. Deep cohesion refers to the presence of causal,  intentional, and temporal connectives in a text. Texts with more  deep cohesion afford readers to form strong representations of  causal events and are typically easier to comprehend.   2.6.2 SEANCE  The SEntiment ANalysis and Cognition Engine (SEANCE) is a  sentiment analysis tool that relies on a number of pre-existing  sentiment, social positioning, and cognition dictionaries. Unlike  other sentiment analysis tools commonly used in learning analytic  studies (i.e., LIWC) [52], SEANCE is freely available and contains  part of speech (POS) tags and valence indices. The tool is available  at http://www.kristopherkyle.com/seance.html.    SEANCE indices are taken from available source databases such as  SenticNet and EmoLex. For many of these dictionaries, SEANCE  provides a negation feature (i.e., a contextual valence shifter) that  ignores positive terms that are negated. The negation feature, which  is based on Hutto and Gilbert [53], checks for negation words in the  three words preceding a target word.    SEANCE also includes the Stanford part of speech (POS) tagger  [41] included in Stanford CoreNLP. The POS tagger allows for  POS tagged specific indices for nouns, verbs, and adjectives. POS  tagging is an important component of sentiment analysis because  unique aspects of sentiment may reside more strongly in adjectives  or in verbs and adverbs.            The SEANCE tool can report on almost 3,000 indices, but because  such a large number of indices can be unwieldy, SEANCE also  reports on 20 components derived from the SEANCE indices:  negative adjectives, social order, action, positive adjectives, joy,  affect for friends and family, fear and disgust, politeness, polarity  nouns, polarity verbs, virtue adverbs, positive nouns, respect, trust  verbs, failure, well being, economy, certainty, positive verbs, and  objects. We focus on the scores from these components.   2.7 Text Analyses  To assess whether students online writing behaviors (i.e., their  keystroke patterns) were related to their self-reported affective  states, we calculated a number of keystroke indices. These indices  are described in Table 1.    Table 1: Keystroke Indices   Description   Verbosity Number of keystrokes per essay   Backspaces Number of backspaces per essay  Largest Latency Largest time difference between   keystrokes during essay writing  Smallest Latency Smallest time difference between   keystrokes during essay writing  Mean Latency The mean of all the differences in time   between keystrokes per essay (not  including initial pause)   Median Latency The median of all the differences in time  between keystrokes per essay (not  including initial pause)   Initial Pause The length of the first pause of an essay  writing session   0.5 Second Pauses The number of pauses above .5 seconds  and below 1 second   1 Second Pauses The number of pauses above 1 second  and below 1.5 seconds   1.5 Second Pauses The number of pauses above 1.5 seconds  and below 2 seconds   2 Second Pauses The number of pauses above 2 seconds  and below 3 seconds   3 Second Pauses The number of pauses above 3 seconds    2.8 Statistical Analyses  Statistical analyses were conducted to investigate the ability of  individual differences, text properties, and keystroke indices to  classify students affective states during writing. As mentioned in  the Retrospective Affect Judgment section above, our analyses  focused solely on classifying boredom and engagement. Thus, to  determine whether an essay writing session was considered high or  low in boredom and high or low in engagement, we first conducted  a median split analysis on students affect ratings. To do this, we  calculated affect proportion scores per essay, such that the sum of  all affect proportion scores per essay was one. We then classified  essay writing sessions as high (mean = 0.306) or low (mean =  0.011) boredom, and high (mean= 0.623) or low (mean = 0.082)  engagement based on a median split of the respective distributions.  Note that we treated each individual essay writing session  separately, rather than accounting for within-subject variability.    We next conducted a number of statistical analyses to determine  which indices would best classify student affect. The indices were  divided into student-level indices (i.e., ACT, WAT, and Author  Recognition scores), essay indices (i.e., Coh-Metrix and SEANCE  component scores; essay quality), and keystroke indices.    Visual inspections of the data were conducted to assess normal  distributions. These inspections were followed by square root  transformations to ensure that the data was normally distributed.  Multicollinearity of the variables was assessed as pair-wise  correlations r > .90. In the case that indices demonstrated  multicollinearity, the index that correlated most strongly with the  relevant affect proportion score was retained in the analysis.   We first conducted MANOVAs to identify which indices exhibited  significant differences across the high and low boredom and  engagement groups. The MANOVAs were followed by stepwise  discriminant function analyses (DFAs). In the DFAs, we used only  the indices that demonstrated significant differences between the  high and low boredom and high and low engagement groups in the  MANOVA. We first conducted the DFA analysis on the entire  corpus, and then validated the model using leave-one-out-cross- validation (LOOCV). In LOOCV, one essay was removed from the  corpus for each analysis and the remaining essays were used as the  training set. We tested the accuracy of the DFA model by  examining its ability to classify the omitted essay.  The process was  repeated until each essay was omitted once in the test set. This  analysis therefore allowed us to test the models classifications on  an independent essay (i.e., data that is not in the training set). If  results on training and testing on all essays (i.e., no separate test  set) and the LOOCV set are similar, confidence in model stability  is increased.    3 RESULTS  3.1 Boredom  A MANOVA was conducted comparing the differences in  individual differences, text properties, and keystroke indices  between essay writing sessions that were reported high and low in  boredom. No two predictors correlated above r = .90; therefore, no  indices were removed from the analysis.    The results of the MANOVA analysis indicated that 12 indices  were significantly different across high and low boredom writing  sessions (see Table 2 for descriptive statistics of the 12 indices).   The stepwise DFA retained six variables related to individual  differences, essay properties, and keystroke indices: WAT scores,  3 Second Pauses, Narrativity Component Score, Polarity Noun  Component, Number of Words, and Median Latency. The results  revealed that the DFA using these six indices correctly allocated  102 of the 132 essays on the entire set, 2 (df=6, n=132)=57.27 p<  .001, for an accuracy of 77.3% (the chance level for this analysis is  50%) For the LOOCV analysis, the DFA allocated 101 of the 132  essays for an accuracy of 76.5% (see the confusion matrix reported  in Table 3 for results). It appears that students who reported more  boredom during writing were also less likely to have apprehension  towards writing. Additionally, the boredom ratings were related to  a lower frequency of long pauses while writing, shorter pauses in  general, and shorter essays that contained fewer narrative elements,  but a higher number of nouns related to polarity.                   Table 2: Descriptive statistics [Means and (SD)] for variables  included in DFA   Variable  Low  Boredom   High  Boredom   Narrativity Component 85.80 (15.57) 78.68 (23.31)  WAT scores 68.98 (14.29) 58.38 (14.95)  Largest Latency 25995.92   (21834.62)  39233.18  (36521.24)   Median Latency 199.56  (59.85)   173.10  (39.20)   0.5 Second Pauses 114.91  (46.65)   87.94     (30.65)   1 Second Pauses 29.29 (10.97) 22.96 (9.17)  1.5 Second Pauses 12.69 (4.82) 10.49 (5.30)  3 Second Pauses 18.59 (6.42) 16.10 (5.33)  Number of Words 212.30   (85.87)  186.13  (62.39)   Action Component  65.20 (24.54) 56.30 (17.93)  Polarity Nouns  38.21 (33.89) 52.20 (41.05)  Trust Verbs  19.44 (13.89) 25.59 (13.94)     Table 3: Confusion matrix for DFA classifying low and high   boredom    Low    Boredom  High   Boredom  Whole Set Low    Boredom 49 15    High   Boredom   15 53          Low    Boredom  High   Boredom  LOOCV Low    Boredom 48 15    High   Boredom   15 53      3.2 Engagement  Our second analysis examined the degree to which the indices  could classify essay sessions as having high or low degrees of  engagement/flow. A MANOVA was first conducted to determine  which indices were significantly different across the high and low  engagement essay sessions. This analysis yielded nine significant  indices (see Table 4 for descriptive statistics of the 9 indices).    A stepwise DFA was calculated to investigate whether these nine  indices accurately classified the writing sessions according to self- reported engagement. The resulting DFA model retained three  variables: Author Recognition Test scores (+), Median Latency (-),  and WAT scores (+). This model correctly allocated 101 of the 132  students in the total set, 2 (df=3,n=132)=42.790 p< .001, for an  accuracy of 76.5% (the chance level for this analysis is 50%). For  the LOOCV analysis, the DFA allocated 96 of the 132 students for  an accuracy of 72.7% (see the confusion matrix reported in Table   5). Thus, students who experienced a higher proportion of  engagement during writing had a greater exposure to print, as well  as less apprehension towards writing. These more engaged students  had shorter pauses than writers who reported lower levels of  engagement.  Table 4: Descriptive statistics [Means and (SD)] for variables   included in DFA   Variable  Low  Engagement   High  Engagement   ACT Scores 20.12 (3.62) 22.25 (4.52)  WAT scores 58.46 (15.02) 68.43 (14.47)  Author Recognition  Test scores 3.92 (1.76) 5.73 (2.52)   Overall Essay Z-Score -.0.28 (0.85) 0.34 (1.00)  Verbosity 1306.62   (465.90)  1621.48  (636.94)   Backspaces 164.82  (104.07)   218.09  (153.72)   Mean Latency 440.27  (153.25)   358.04  (114.33)   Median Latency 205.84  (53.17)   166.62  (42.64)   Number of Words 179.32  (63.55)   217.73  (81.72)     Table 5: Confusion matrix for DFA classifying low and high   engagement    Low    Engagement  High   Engagement  Whole Set Low   Engagement 54 11    High  Engagement 20 47          Low    Boredom  High   Boredom   LOOCV Low  Engagement 51 14    High  Engagement 22 45   4 DISCUSSION  Writing training systems have been developed to provide students  with instruction and deliberate practice on their writing [10]. While  generally successful in providing accurate summative feedback  [11-12], a common criticism of these systems is their lack of  personalization and adaptive instruction [22-24]. The objective of  most writing training systems is to provide accurate scores that  match an experts ratings of the essays quality. These systems tend  to overlook additional variables that may ultimately contribute to  students success, such as their affective states during writing  practice.    Teachers can observe and interpret students affect before and after  they compose a writing assignment. They can then use these  judgements to guide what feedback they give, and how they convey           that feedback. We believe that it is both possible and desirable for  automated systems to do much the same. But accomplishing this  goal will require a more comprehensive picture of the writer and  the writing process. This study takes an initial step toward this goal  by building a predictive model of students affect using information  that can potentially be easily collected by computer systems.    We used individual difference measures, text indices (calculated  via NLP tools), and keystroke analyses to predict affect. The  MANOVAs revealed that there were 12 indices that significantly  differentiated between low and high boredom writing sessions, and  9 indices that significantly differentiated between the low and high  engagement groups. This is an important finding because it  indicates that students affective ratings can be detected by  analyzing information about the students, as well aspects of the  final product (i.e., text indices) and the writing process (i.e.,  keystrokes). Further, the DFA analyses revealed that boredom  ratings were predicted by all three categories of indices  namely,  students who frequently reported feeling bored during the writing  session reported higher levels of writing apprehension, wrote  shorter and less narrative essays, and had a lower frequency of long  pauses. Engagement, on the other hand, was largely characterized  by student and process-level indices, such as lower writing  apprehension and shorter pause lengths.      Importantly, these DFAs revealed both similarities and differences  between the boredom and engagement ratings. First, both the high  boredom and high engagement groups were classified by shorter  pause lengths. This is an interesting finding and potentially  suggests that high levels of boredom and engagement may have  been co-present during specific writing sessions. In particular,  nearly half (43%) of the writing sessions were categorized as  having high boredom and high engagement (23%) or low boredom  and low engagement (20%). Thus, there may have been specific  students who experienced higher degrees of affect in general during  their writing sessions, as opposed to reporting neutral affective  states.     This pause finding points to multiple promising areas of future  research. First, follow-up studies that examine individual  differences may reveal student profiles associated with varying  levels of engagement and boredom during their writing sessions.  A  second follow-up study relates to the detection of affect during the  writing session. In the current study, we categorized whether an  essay writing session had a high or low proportion of boredom and  engagement ratings. However, future studies should focus on the  development of affect detectors that can signal the system when it  predicts a student is experiencing certain emotions that warrant  feedback (e.g., boredom).    In addition to this similarity in pause times, the DFAs indicated  some differences between the engagement and boredom ratings.  While high levels of boredom were associated with lower  apprehension and shorter, less narrative essays, high engagement  was predicted by lower writing apprehension and author  recognition scores. This suggests that students feelings of boredom  may be more strongly related to the text that they produce, as it is  related to both text indices and keystroke indices; engagement, on  the other hand, may be more strongly influenced by more stable  traits of the students. This has important implications for future  system adaptability. If this finding were to be replicated in follow-                                                                   1 To ensure that our models were not largely biased by within-  subject variability, we conducted leave-one-subject-out cross- validation for the classification analyses of engagement and  boredom. The most accurate classifications in each case came   up studies, it suggests that boredom and engagement should  potentially be addressed in different ways. Boredom, for instance,  might require more online feedback, whereas low levels of  engagement might be addressed through the assignment of more  motivating prompts prior to the writing session.    As a final note, in the current study, we focused on students  individual writing sessions, and did not account for within-subject  variability associated with students multiple writing sessions.1  This methodological choice was made because the majority of  current AES systems focus on assessing writing quality at the  individual session level and do not account for students previous  performance. Although it may be the case that students prior  performance and affective states can increase the strength of the  feedback and instructional adaptation in these systems, this remains  an empirical question. Future studies should be conducted to  compare the effectiveness of AES systems that do and do not  account for previous student performance in their models.    Overall, our results suggest that individual differences, text indices,  and keystroke logs can be utilized to develop models of students  affective states during writing sessions. Taken together, indices  related to students academic abilities, text properties, and  keystroke logs were able to reliably predict the general affective  states that students experienced during writing. These results are  important because they suggest that students affect can manifest in  the ways that they produce essays, both in the indices of the texts  themselves, as well as in their typing patterns. In the current study,  we focused solely on engagement and boredom. However, future  studies will be conducted to examine additional affective states, as  well as other individual differences that may help to improve the  adaptability of writing training systems.    In conclusion, the current study utilized multiple components  related to the writing process to investigate the efficacy of writing  training systems to inform stealth assessments of students affective  states. Our eventual goal is to use these stealth assessments to  enhance our student models in the W-Pal system, which will allow  us to provide students with more personalized feedback and  instruction. More broadly, the current study suggests that individual  differences, text indices, and online writing measures (such as  keystroke analyses) can be used as a step towards more adaptive  educational technologies for writing. Although this is only a first  step, and a number of studies remain to be conducted, this study  provides a strong initial foundation because it demonstrates the  feasibility of such measures for modeling affect.    5 ACKNOWLEDGMENTS  This research was supported in part by: NSF ITR 0325428, HCC  0834847, DRL 1235958, DRL 1417997, IIS 1523091, and IES  R305A120707. Opinions, conclusions, or recommendations do not  necessarily reflect the views of the IES or NSF. We also thank Rod  Roscoe, Cecile Perret, and Jianmin Dai for their help with the data  collection and analysis and developing the ideas found in this paper.   6 REFERENCES  [1] Geiser, S. and Studley, R. 2001. UC and SAT: Predictive   validity and differential impact of the SAT I and SAT II at the  University of California. Oakland, CA: University of  California.   from a Bayes Net classifier with 70.1% accuracy for the  Engagement model and a Logistic Regression with 70.4% for the  Boredom model.            [2] Powell, P. 2009. Retention and writing instruction:  Implications for access and pedagogy. College Composition  and Communication, 66, 664-682.   [3] Flower, L. S. and Hayes, J. 1981. A cognitive process theory  of writing. College Composition and Communication, 32,  365-387.   [4] Hayes, J. 1996. A new framework for understanding  cognition and affect in writing. In C. M. Levy & L. S.  Ransdell (Eds.), The science of writing: Theories, methods,  individual differences and applications. Erlbaum, Mahwah,  NJ, 1-27.   [5] National Assessment of Educational Progress. 2007. The  nations report card: Writing 2007. Retrieved Nov. 20, 2010,  nces.ed.gov/nationsreportcard/writing/   [6] National Assessment of Educational Progress. 2011. The  nations report card: Writing 2011. Retrieved Nov. 5, 2012,  nces.ed.gov/nationsreportcard/writing.   [7] Graham, S. and Perin, D. 2007. A meta-analysis of writing  instruction for adolescent students. Journal of Educational  Psychology, 99, 445-476.   [8] Kellogg, R., and Raulerson, B. 2007. Improving the writing  skills of college students. Psychonomic Bulletin and Review,  14, 237-242.    [9] National Commission on Writing. 2003. The neglected R.  College Entrance Examination Board, New York.    [10] Allen, L. K., Jacovina, M. E., and McNamara, D. S. 2015.  Computer-based writing instruction. In C. A. MacArthur, S.  Graham, and J. Fitzgerald (Eds.), Handbook of writing  research (2nd ed.) (pp. 316-329). New York: Guilford Press.   [11] Shermis, M., and Burstein, J. (Eds.). 2003. Automated essay  scoring: A cross-disciplinary perspective. Erlbaum, Mahwah,  NJ.    [12] Dikli, S. 2006. An overview of automated scoring of essays.  Journal of Technology, Learning, and Assessment, 5.    [13] Roscoe, R. D., Allen, L. K., Weston, J. L., Crossley, S. A.,  and McNamara, D. S. 2014. The Writing Pal intelligent  tutoring system: Usability testing and development.  Computers and Composition, 34, 39-59.    [14] Weigle, S. C. 2013. English language learners and automated  scoring of essays: Critical considerations. Assessing Writing,  1, 8599.   [15] Xi, X. 2010. Automated scoring and feedback systems:  Where are we and where are we heading Language Testing,  27, 291300.   [16] Deane, P. 2013. On the relation between automated essay  scoring and modern views of the writing construct. Assessing  Writing, 18, 724.   [17] Warschauer, M., and Ware, P. 2006. Automated writing  evaluation: Defining the classroom research agenda.  Language Teaching Research, 10, 124.   [18] Attali, Y., and Burstein, J. 2006. Automated essay scoring  with e-rater V.2. Journal of Technology, Learning, and  Assessment, 4(3). Retrieved from www.jtla.org   [19] Crossley, S. A., Varner, L. K., Roscoe, R. D., and  McNamara, D. S. 2013. Using automated indices of cohesion  to evaluate an intelligent tutoring system and an automated  writing evaluation system. In: K. Yacef et al (Eds.),   Proceedings of the 16th International Conference on  Artificial Intelligence in Education (AIED) (pp. 269-278).  Springer, Heidelberg, Berlin, 269-278.   [20] Grimes, D., and Warschauer, M. 2010. Utility in a fallible  tool: A multi-site case study of automated writing evaluation.  Journal of Technology, Learning, and Assessment, 8, 443.   [21] Shermis, M. D., Burstein, J., Elliot, N., Miel, S., and Foltz, P.  W. 2015. Automated writing evaluation: An expanding body  of knowledge. In C. A. MacArthur, S. Graham, & J.  Fitzgerald, Handbook of writing research (2nd ed.). New  York: Guilford Press.   [22] Haswell, R. H. 2006. Automatons and automated scoring:  Drudges, black boxes, and dei ex machina. In: P. F. Ericsson  & R. H. Haswell (Eds.), Machine scoring of student essays:  Truth and consequences (pp. 5778). Logan, UT: Utah State  University Press.    [23] Hearst, M. 2002. The debate on automated essay scoring.  Intelligent Systems and their Applications, IEEE, 15, 22-37.   [24] Perelman, L. 2012. Construct validity, length, score, and time  in holistically graded writing assessments: The case against  automated essay scoring (AES). In: C. Bazerman, C. Dean, J.  Early, K. Lunsford, S. Null, P. Rogers, & A. Stansell (Eds.),  International advances in writing research: Cultures, places,  measures (pp. 121131). Fort Collins, Colorado: WAC  Clearinghouse/Anderson, SC: Parlor Press.   [25] Shute, V. J. 2011. Stealth assessment in computer-based  games to support learning. In S. Tobias & J. D. Fletcher  (Eds.), Computer games and instruction (pp. 503-524).  Charlotte, NC: Information Age Publishers.   [26] Shute, V. J., and Kim, Y. J. 2013. Formative and stealth  assessment. In J. M. Spector, M. D. Merrill, J. Elen, and M.  J. Bishop (Eds.), Handbook of research on educational  communications and technology (4th ed.) (pp. 311-323).  Lawrence Erlbaum Associates, Taylor & Francis Group,  New York, NY.   [27] DMello, S.K., and Mills, C. 2014. Emotions during  emotional and non-emotional writing. Motivation and  Emotion, 38, 140-156.   [28] Mills, C. and DMello, S. K. 2013. Emotions during writing  about socially-charged issues: Effects of the (mis) alignment  of personal positions with instructed positions. In:  Proceedings of 26th Florida Artificial Intelligence Research  Society Conference (pp. 509-514). Menlo Park, CA: AAAI  Press.   [29] Baker, R., and Ocumpaugh, J. 2015. Interaction-based affect  detection in educational software. In R. Calvo, S. D'Mello, J.  Gratch & A. Kappas (Eds.), The Oxford handbook of  affective computing (pp. 233-245). New York: Oxford  University Press.   [30] D'Mello, S., and Graesser, A. 2015. Feeling, thinking, and  computing with affect-aware learning technologies. In R.  Calvo, S. D'Mello, J. Gratch & A. Kappas (Eds.), The Oxford  handbook of affective computing (pp. 419-434). New York:  Oxford University Press.   [31] Brusilovsky, P. 1994. The construction and application of  student models in intelligent tutoring systems. Journal of  Computer and Systems Science International, 23, 70-89.   [32] Crossley, S. A., Allen, L. K., Snow, E. L., and McNamara,  D. S. 2015. Pssst...Textual features...There is more to           Automatic Essay Scoring than just you!. In J. Baron, G.  Lynch, N. Maziarz, P. Blikstein, A. Merceron, & G. Siemens  (Eds.), Proceedings of the 5th International Learning  Analytics & Knowledge Conference (LAK'15) (pp. 203-207).  Poughkeepsie, NY.   [33] McNamara, D. S., Crossley, S. A., Roscoe, R. D., Allen, L.  K., and Dai, J. 2015. Hierarchical classification approach to  automated essay scoring. Assessing Writing, 23, 35-59.   [34] Varner, L. K., Roscoe, R. D., and McNamara, D. S. 2013.  Evaluative misalignment of 10th-grade student and teacher  criteria for essay quality: An automated textual analysis.  Journal of Writing Research, 5, 35-59.   [35] Crossley, S. A., Kyle, K., Allen, L. K., Guo, L., and  McNamara, D. S. 2014. Linguistic microfeatures to predict  L2 writing proficiency: A case study in automated writing  evaluation. Journal of Writing Assessment.   [36] Allen, L. K., and McNamara, D. S. 2015. You are your  words: Modeling students vocabulary knowledge with  natural language processing. In: O. C. Santos et al.  (Eds.), Proceedings of the 8th International Conference on  Educational Data Mining (EDM 2015). Madrid, Spain.   [37] Bixler, R. and DMello, S. 2013. Detecting boredom and  engagement during writing with keystroke analysis, task  appraisals, and stable traits. In: Proceedings of the 2013  International Conference on Intelligent User Interfaces (pp.  225-234). New York, NY: ACM.   [38] Leijten, M., and Van Waes, L. 2013. Keystroke logging in  writing research: Using Inputlog to analyze and visualize  writing processes Written Communication, 30, 358-392.   [39] Van Waes, L., Leijten, M., Lindgren, E., and Wengelin, A.  2015. Keystroke logging in writing research: Analyzing  online writing processes. In C. A. MacArthur, S. Graham, &  J. Fitzgerald, Handbook of writing research (2nd ed.). New  York: Guilford Press.   [40] Leijten, M., van Horenbeeck, E., and Van Waes, L. 2015.  Analyzing writing process data: A linguistic perspective. In  G. Cislaru (Ed.), Writing(s) at the crossroads: The  process/product interface. Philadelphia: John Benjamins  Publishing Company.   [41] Kellogg, R. T. 2006. Professional writing expertise. In K. A.  Ericsson, N. Charness, P. J. Feltovich, and R. R. Hoffman  (Eds.), The Cambridge handbook of expertise and expert  performance (pp. 389-402). Cambridge University Press.   [42] Allen, L. K., Crossley, S. A., Snow, E. L., and McNamara,  D. S. 2014. Game-based writing strategy tutoring for second  language learners: Game enjoyment as a key to  engagement. Language Learning and Technology, 18, 124- 150.   [43] Roscoe, R. D., Snow, E. L., Allen, L. K., and McNamara, D.  S. 2015. Automated detection of essay revising patterns:  application for intelligent feedback in a writing  tutor. Technology, Instruction, Cognition, and Learning,  10, 59-79.   [44] McNamara, D. S., Graesser, A. C., McCarthy, P., and Cai, Z.  2014. Automated evaluation of text and discourse with Coh- Metrix. Cambridge: Cambridge University Press.   [45] Daly, J., and Miller, M. 1975. The empirical development of  an instrument to measure writing apprehension. Research in  the Teaching of English 9(3), 242-249.   [46] Cunningham, A. E., and Stanovich, K. E. 1997. Early  reading acquisition and its relation to reading experience and  ability 10 years later. Developmental Psychology, 33(6), 934- 945.   [47] DMello, S. and Graesser, A. C. 2011. The half-life of  cognitive-affective states during complex learning. Cognition  and Emotion, 25, 1299-1308.   [48] Rosenberg, E. L. and Ekman, P. 1994. Coherence between  expressive and experiential system of emotions. Cognition  and Emotion, 8, 201-229.   [49] McNamara, D. S., Crossley, S. A., and McCarthy, P. M.  2010. Linguistic indices of writing quality. Written  Communication, 27, 57-86.   [50] Graesser, A.C., McNamara, D.S., and Kulikowich, J.M.  2011. Coh Metrix: Providing multilevel analyses of text  characteristics. Educational Researcher, 40, 223-234.    [51] Graesser, A.C. and McNamara, D.S. 2011. Computational  analyses of multilevel discourse comprehension. Topics in  Cognitive Science, 2, 371-398.   [52] Pennebaker, J. W., Booth, R. J., & Francis, M. E. (2007).  Linguistic Inquiry and Word Count: LIWC [computer  software]. Austin, TX.   [53] Hutto C, Gilbert E. 2014. Vader: a parsimonious rule-based  model for sentiment analysis of social media text. In:  International AAAI conference on weblogs and social media,  (pp. 216225).       "}
{"index":{"_id":"16"}}
{"datatype":"inproceedings","key":"Martinez-Maldonado:2016:ISL:2883851.2883873","author":"Martinez-Maldonado, Roberto and Schneider, Bertrand and Charleer, Sven and Shum, Simon Buckingham and Klerkx, Joris and Duval, Erik","title":"Interactive Surfaces and Learning Analytics: Data, Orchestration Aspects, Pedagogical Uses and Challenges","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"124--133","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883873","doi":"10.1145/2883851.2883873","acmid":"2883873","publisher":"ACM","address":"New York, NY, USA","keywords":"awareness, dashboard, design, face-to-face, groupware, studies in the wild, visualizations","abstract":"The proliferation of varied types of multi-user interactive surfaces (such as digital whiteboards, tabletops and tangible interfaces) is opening a new range of applications in face-to-face (f2f) contexts. They offer unique opportunities for Learning Analytics (LA) by facilitating multi-user sensemaking of automatically captured digital footprints of students' f2f interactions. This paper presents an analysis of current research exploring learning analytics associated with the use of surface devices. We use a framework to analyse our first-hand experiences, and the small number of related deployments according to four dimensions: the orchestration aspects involved; the phases of the pedagogical practice that are supported; the target actors; and the levels of iteration of the LA process. The contribution of the paper is twofold: 1) a synthesis of conclusions that identify the degree of maturity, challenges and pedagogical opportunities of the existing applications of learning analytics and interactive surfaces; and 2) an analysis framework that can be used to characterise the design space of similar areas and LA applications.","pdf":"Interactive Surfaces and Learning Analytics: Data,  Orchestration Aspects, Pedagogical Uses and Challenges   Roberto Martinez-Maldonado1, Bertrand Schneider2,   Sven Charleer3, Simon Buckingham Shum1, Joris Klerkx3, Erik Duval3   1University of Technology Sydney, Australia   {roberto.martinez-maldonado,   simon.buckinghamshum}@uts.edu.au   2Stanford University, USA  schneibe@stanford.edu   3KU Leuven, Belgium   {sven.charleer, joris.klerkx,  erik.duval}@cs.kuleuven.be     ABSTRACT  The proliferation of varied types of multi-user interactive surfaces  (such as digital whiteboards, tabletops and tangible interfaces) is  opening a new range of applications in face-to-face (f2f) contexts.  They offer unique opportunities for Learning Analytics (LA) by  facilitating multi-user sensemaking of automatically captured  digital footprints of students f2f interactions. This paper presents  an analysis of current research exploring learning analytics  associated with the use of surface devices. We use a framework to  analyse our first-hand experiences, and the small number of  related deployments according to four dimensions: the  orchestration aspects involved; the phases of the pedagogical  practice that are supported; the target actors; and the levels of  iteration of the LA process. The contribution of the paper is two- fold: 1) a synthesis of conclusions that identify the degree of  maturity, challenges and pedagogical opportunities of the existing  applications of learning analytics and interactive surfaces; and 2)  an analysis framework that can be used to characterise the design  space of similar areas and LA applications.   CCS Concepts    Information systems  Information systems applications   Collaborative and social computing systems and tools   Human-centered computing  HCI   Interaction devices.   Keywords  Design; groupware; visualisations; design; dashboard; studies in  the wild, awareness; face-to-face   1. INTRODUCTION  While there has been a growing interest in the potential role of  Learning Analytics (LA) in mobile learning and online activities,  to a large extent, students learning still happens in face-to-face  (f2f) settings [1]. Blended learning and massive online courses  have become popular targets for LA solutions [11], but they are  primarily, or wholly, focused on the non-f2f, online part of  students engagement in learning activities. However, the  development of effective f2f communication and collaboration  skills remain key 21st century competencies for employability and   lifelong learning [13]. Group tasks typically require negotiation,  brainstorming, and argumentation, usually in the service of some  form of artefact design. These can be powerful vehicles for  authentic learning, and typically have a major f2f element [18]. It  has been emphasised that LA research has a particular perspective  of attempting to understand learning as a whole, in their full  complexity [26], which also includes f2f students activity.   Emerging technologies such as touch and tangible interaction,  gesture recognition and object tracking, have the potential to help  support f2f students activity from a LA perspective. These  technologies have been increasingly moving from research to  commercial applications over the last two decades in the form of  varied types of interactive surfaces [9]. In this paper, we focus on  multi-user interactive surfaces which are devices that allow touch  and/or tangible interaction by one or more users. These include  interactive tabletops, interactive whiteboards (IWB), tangible  interfaces and smaller-scale devices such as tablets, which can  allow transitions between individual and group work.   The proliferation of surface devices is opening a broader range of  possible applications to facilitate and enrich face-to-face activities  in educational contexts [12]. Affordances of interactive surfaces  commonly include the provision of a work space that offers  multiple direct input points so users can manipulate digital  content with fingers or through physical trackable objects, while  they communicate via speech, facial expressions, and gestures [9].  Less explored affordances of these devices include the unique  opportunity they offer to automatically capture students digital  footprints that can be analysed and used to make f2f interactions  visible. Their intrinsic multi-user capabilities can assist in  enhancing collocated exploration, discussion and sensemaking of  LA indicators. Furthermore, Oviatts research [19] argues for the  critical role of creative sketching in learning, placing renewed  emphasis on digital pens and surfaces.   These underexplored opportunities motivate the need to define  key dimensions of a new design space, where surface technology  and LA tools can meet to address f2f learning challenges. Such an  explicit design space may not only be helpful as an instrument for  describing and coordinating current research in the area (e.g. by  identifying potential uses of the technology and challenges; and  avoiding duplicate work), but may also provide a conceptual basis  for the development of new LA tools targeting unmet needs.   Designing and deploying LA tools using surface devices require  a comprehensive understanding of interaction design and the  possibilities that these technologies offer, not just for learning and  teaching, but also for learning analytics. The design space should  also consider the pedagogical underpinning, the possible target  actors, the teaching strategies, data sources, and the degree of  maturity of development in this area. This paper presents a  synthesis of conclusions drawn from an empirical analysis of the     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions  from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883873     current research and authentic deployments of LA tools utilising  interactive surfaces. To describe the design space in this emerging  area, we use an analytical framework, drawing on principles from  four sources: i) a framework of classroom orchestration [20]; ii) a  framework to support the implementation of teaching practices  [10]; iii) the actors who are commonly targeted by LA tools [25]   and iv) the iterative process they commonly followed by these to  use and respond to LA tools [27]. We analyse the technological  and educational aspects of our first-hand experiences, and the  small number of authentic deployments, of learning analytics  utilising different types of interactive surfaces. In parallel, our  approach allows us to analyse the maturity of the multi-user  interactive surface technology and LA solutions by drawing a  contrast between interesting pieces of research conducted in  controlled lab conditions and authentic classroom deployments.   The contribution of the paper is two-fold: 1) a series of  conclusions that identify the degree of maturity, orchestration  aspects, challenges and pedagogical approaches of the existing  applications of learning analytics in interactive surface-based  settings; and 2) a combined framework that can be used to  characterise the learning analytics design space and maturity in  other areas of application. Moreover, the framework connects  pedagogical principles with the practical metaphor of classroom  orchestration in the context of the deployment of LA tools.   The rest of the paper is structured as follows. The next section  provides an overview about touch and tangible interaction; and a  definition of orchestration technology and its links with learning  analytics. Then, we present the theoretical underpinning of our  combined framework. After this, we describe the analysis of  selected case studies using the framework, making an emphasis on  the particular orchestration challenges, pedagogical uses and  advantages of using interactive surfaces for LA purposes. We  conclude with a discussion of the application of our framework,  the maturity of the area and opportunities for future research.     2. RELATED WORK  2.1 Touch and Tangible Surfaces in Education  Surface computing is still a maturing technology, which has  become a more natural alternative to traditional mouse and  keyboard input by allowing users multi-touch interaction using  fingers, hands or special pens [2]. This shift in input technology  has opened the interaction space allowing a wide range of new  collaborative and ubiquitous applications, especially for those  tasks that are more effectively performed face to face [18]. Some  example tasks that have been supported by interactive surfaces  include group planning, diagramming, designing, data  exploration, brainstorming, knowledge building, and information  curation. However, whilst advancements in hardware have been  rapid, application software for large surface devices is still in early  stages compared with, for example, the market of mobile devices.   In terms of educational contexts, there has been a great interest in  using large surface devices for supporting collaborative learning  pedagogies. IWBs have been used to conduct whole class  activities [9], both vertical and horizontal large touch screens  have been used to conduct small group work [12], and multiple  tablets have been interconnected to support tasks in pairs [28], or  to show a user interface just for the teacher [12]. Moreover, the  use of tangible objects on surface interfaces has been regarded by  practitioners and researches as a particularly important feature for  the cognitive development of young students coordination and  3D orientation [6]. Tangible interfaces are promising for tasks that   require the manipulation of objects, which is not possible in flat  displays (examples have included narrative, biochemistry and  simulation systems [6]). Increased interest has also been posed in  the digital affordances of surface devices to support handwriting  and sketching [19]. These are often more fluid ways for students  to communicate and generate ideas, compared with the use of  mice, and physical or on-screen keyboards.   Another use of large surface devices has been collaborative data  visualisation, but not much has been done to support collaborative  sensemaking of educational data. At the same time, there has not  been much work in exploiting data captured by these devices in  similar ways as it has been done with online systems. The capture  and identification of f2f users actions can impose particular  challenges that are not present in non-f2f scenarios. For example,  all online actions can be easily recorded and identified by asking  the user for login credentials. Although overcoming data  collection challenges can be a current challenge, there is an  enormous potential to support f2f students work in ways that  have not been yet possible.    Overall, there has been a steadily increasing interest in using  touch and tangible devices. Rather than having a wave of novel  technology occupying the classroom, we are seeing a slower  paced increase of surface technology used in several areas of life,  including learning and teaching. It is timely to start considering  the potential of the support that learning analytics can offer - in  situ- and, conversely, the new areas of application that these  emerging devices can bring to learning analytics.   2.2 Technology for Classroom Orchestration    The metaphor of orchestration takes account of the variability and  complexity of classrooms and the key role of teachers in adapting  the available pedagogic and technological resources to help  students achieve their intended learning goals [7]. Orchestration  technology may support the management of the orchestration or  some part of it. This includes, for example, interfaces that help  teachers manage the class workflow, enhance their awareness or  track students progress. The metaphor was further embraced by  other researchers to explain several other aspects that need to be  attended before and/or after the actual deployment of learning  tasks, not only in the classroom, but also in online or blended  learning scenarios [20]. This includes, for example, tools that  support teachers to deploy their learning designs or reflection,  assessment and re-design after the activity is completed.   In short, this perspective empowers teachers as drivers of  classroom activities and advocates for the use of simple  technologies that may have important effects. The effectiveness of  orchestration and the extent to which teachers can respond to the  ways students perform their tasks is critical because it directly  impacts these students activities, and therefore, their students  learning. Moreover, the metaphor has also been extended by the  notion of distributed orchestration [24], considering that students  and other actors of the learning process, can also be responsible  for part of or all the orchestration tasks. Thus, this makes  orchestration also applicable to self-managed learning scenarios.   Our work takes an approach based on orchestration as it is a  dynamic perspective that attends authentic issues considering that  learning activities that occur in the classroom may be affected by  unanticipated processes and contingencies. Differently to learning  theories that focus on cognitive aspects, orchestration is  concerned with practical issues and tasks that are not directly  linked with learning but can shape learning. This makes     Figure 1. A combination of frameworks creates this 4- dimensional framework, used to analyse the current state of   learning analytics on data from interactive surfaces     orchestration very relevant for deploying LA tools in authentic  learning settings. Learning analytics can have a key role in  supporting f2f and blended learning activities. To achieve this, a  clear understanding of orchestration aspects is needed to create  effective LA solutions in those f2f settings where teachers or  students need to adapt to unexpected problems, on the fly.   3. THE COMBINED FRAMEWORK  The combined framework we used to analyse the current research  and deployments that combine LA tools and interactive surfaces is  defined by four dimensions: a) a set of orchestration aspects that  the LA tools provide support to, b) the phases of the pedagogical  practice that are supported, c) the target actors of the learning  analytics and d) the levels of iteration of the learning analytics and  pedagogical processes (see Figure 1). The combined framework  forms a 4-dimensional matrix which can categorise the LA  deployments. Each of these dimensions, and their theoretical  underpinnings, are described in the rest of this section.    3.1 Elements of the framework  3.1.1 Orchestration aspects  Prieto et al. [20] developed a framework that identifies five  orchestration aspects. For the first dimension of our analysis  framework, we considered the first four functional aspects of  orchestration (see Figure 1, a). These aspects can be linked with  tasks that either teachers or students should perform, and thus, LA  solutions can be created to support the actors in performing such  functions or tasks. The fifth aspect refers to the roles of teachers,  students and other actors, and was considered into a separate  dimension, linked to the target actors of the LA tools (Figure 1,  c). The four functional orchestration aspects are the following:   Design and planning. Learning design includes the preparation of  the educational materials, pedagogical approaches, social  dynamics, tasks, scripts, strategies and any other resources that are  needed to create opportunities of learning for students. Teachers  commonly have a crucial role in learning design and co-design.  There can also be other actors specialised in learning design,  particularly in higher education. Alternatively, students can also  design or co-design their own learning tasks. The design process  is not necessarily linear, as design and planning can co-occur  while the actual activity unfolds or after it is completed [20]. In  terms of learning analytics, awareness and/or analytical tools may  support fine tuning of learning designs by providing visualisations  of students data, indicators about how planned tasks actually  occurred or insights from the community of practice.   Regulation and management. This aspect refers to the  coordination of the ongoing teaching process and/or the self- regulation of the learning activity. This includes the management  of time for each students task, class duration, task distribution  and social arrangements. In short, this aspect is focused on the  coordination of the workflow of the learning activity. This  regulation can be performed through social interaction (e.g. the  teacher directing the flow of the class or students managing their  own workflow based on feedback from LA systems) or be partly  handed over to some Comp. controlled mechanism [20]. LA tools  can support the actors responsible of the management of the  learning processes and their constraints by providing, for  example, key information about the execution of the workflow so  they could modulate it according to the demands of the activity.  Adaptation, flexibility and intervention. This aspect refers to the  capacity of the educational technology, the class script or the   learning activities to be flexibly adapted to unexpected classroom  events and the emergence of new tasks. This can include the  actors creating improvised tasks or adapting the planned tasks  during the enactment. Similarly, the systems can offer flexible  functions to handle those adaptations. LA tools can support this  process by providing teachers or students with key information  that would allow them to manually intervene or adapt specific  learning tasks, or for the learning system to automatically adapt  the tasks to particular students needs or provide automated  interventions to tune the order or the approach to the tasks.  Awareness and assessment. Awareness, and formative/summative  assessment tools are clearly critical to orchestrating learning.  While awareness of different sorts pervades all sensemaking  activity around data [27], for this aspect we focus on those  awareness mechanisms that are particularly linked to the students  learning activity. This includes tools that can provide key insights  into students learning processes so actors can modify their  teaching strategies, the provision of feedback, the pedagogical  approach or the students learning styles. These can be simple tools  such as basic visualisations of group progress, or more complex  student modelling or predictive approaches.    3.1.2 Phases of pedagogical practice  The second dimension is derived from the Implementing  Collaborative Learning in the Classroom (ICLC) framework by  Kaendler et al. [10]. This points at the teacher competencies that  are needed across the implementation phases of learning strategies  in classroom sessions. It defines five teacher competencies:  planning, monitoring, supporting, consolidating and reflecting,  which span three phases of teaching practice: pre-active, inter- active, and post-active (Figure 1, b). The authors map planning to  the pre-active-phase, monitoring, supporting and consolidating to  the inter-active phase, and reflecting to the post-active-phase.  Although the teacher competencies could be matched with the  orchestration aspects described above, the metaphor of  orchestration is not only concerned with the ability of the teacher  to perform tasks according to their professional knowledge.  Rather, it is concerned with how different types of technology can  support teachers, or students themselves, to manage multi-layered  activities in a multi-constraints context [7]. Additionally and very  important is that almost all orchestration aspects can be relevant  before, during or after the activity [20]. For example, planning  and learning design commonly occur in the pre-active phase, but  it can be that a teacher has to adapt the intended design on the fly,  or accomplish some re-designing work in the post-active phase.      In summary, by combining both frameworks we can map surface- based LA in terms of what orchestration support they provide and  when. We have not yet specified for whom, considered next.   3.1.3 Target Actors  LA solutions can be oriented towards different actors of the  learning process, including students, teachers, intelligent agents,  administrators, etc [5]. At the same time, LA studies can be  conducted for research, or prototype system design purposes,  without being deployed in authentic learning scenarios. Learning  analytics can also support learning designers to take informed  decisions about changes that the course may require based on  evidence. Therefore, to understand the design space of learning  analytics in a specific area, and its degree of maturity in terms of  real deployments, we should differentiate the actors who are being  targeted as end-users of the LA tools. Based on the users targeted  by the current deployments covered in our analysis, we divide the  target users into three groups (Figure 1, c): Students, Educators  (including lecturers, tutors, learning designers) and Researchers  (including individuals and also the community of research).   3.1.4 Levels of the Iterative Process  The fourth dimension introduces the notion of iteration, at two  levels: Micro and Macro (Figure 1, d). Micro refers to the  iterative process of LA support within each pedagogical phase.  This has been described by Verbert et al.s [27] as the process  users follow to: have access to data (i. awareness); ask questions  and assess the relevance of the data (ii. reflection); answer  questions, getting new insights (iii. sensemaking); to finally  induce new meaning or behavioural change (iv. impact). This  four-stage iterative process occurs while users interact with a LA  tool in a given phase. Iteration at a macro-level is concerned with  the workflow as the phases (pre-active, inter-active, and post- active) are repeated over multiple sessions. This is crucial so that  LA tools can provide support spanning multiple sessions.   In summary, the combined framework considers that the pre- active, inter-active, and post-active phases form a linear workflow  for one specific session (e.g. a classroom session, an experimental  trial, an online-based task). Each orchestration aspect can be  supported in any of these phases (e.g. planning is not restricted to  the pre-active phase, but can occur in the inter-active and post- active phases). Finally, LA support can be targeted at different  actors in each phase and macro-iteration.    4. Analysis of Case Studies  In this section, we analyse a series of case studies of LA  applications that use interactive surfaces to support different   orchestration aspects. Table 1 presents an overview of the design  space defined by the dimensions of our framework. The table  maps the Projects analysed (column 1), the Orchestration Aspects  addressed (2-5); the Pedagogical Phases that are supported (6-8);  and whether they involve certain levels of Iteration (9-10). The  actors targeted in each deployment are represented by letters: E  for educators, teachers, tutors and learning designers; S for  students and R for researchers. Beyond the dimensions of the  combined framework, in the case studies we seek to identify the  forms in which the data is communicated to the actors, such as  whether it is presented in a raw format (e.g. statistics, algorithms  results, patterns), through visual representations (e.g., dashboards,  visualisations, alerts, notifications) or by direct automated actions.  We also pay attention to the topology of LA tools classified by the  type of information they offer, including information about 1) the  task/class progress, 2) students interaction, 3) quality of the  students solution and 4) learning (including conceptual change,  learning to collaborate or learning about the process).   In the following subsections, we provide a concise description of  our first-hand experiences from seven case studies (the first 7  rows in Table 1). These illustrate how the dimensions of the  combined framework are interwoven to help understand the  technologies used and pedagogical aspects tackled by the LA  solutions. To facilitate the presentation of the cases, we grouped  them by the main actors that are targeted in each (teachers,  learning designers, students and researchers, respectively). Lastly,  we briefly describe other LA applications where some sort of  surface technology has been used (last four rows of Table 1).    4.1 Supporting Awareness for Teachers  We start by describing two case studies of analytics support for  enhancing teachers awareness.   4.1.1 MTFeedback: driving teachers attention  The first case study consisted in providing support to enhance  teachers classroom Awareness and Assessment on the fly (inter- active phase). The pedagogical intentions of the teachers were that  students could engage in collaborative discussions and visually  represent their proposed solutions to posed challenging problems.  The teacher aimed to conduct this activity face-to-face to support  students and provide direct feedback to promote verbal discussion  and argumentation. The setting used was the MTClassroom  (Figure 2, left). This is a multi-surface classroom environment  composed of 4-5 large interconnected tabletops and three vertical  displays. Each tabletop was enriched with a Kinect sensor that  differentiates individual touches. This allows the capture of an  identified log of student's actions at each table. A total of 6   Table 1. Analysis of the current design space of learning analytics applications utilising interactive surfaces.    Target actors: E=Educators, S=Students, and R=Researchers        Project    Orchestration Aspects Pedagogical Phases Iteration  Design &  Planning    Regulation &  Management   Adaptation,  Flexibility, &  Intervention    Awareness &  Assessment    Pre- active   Inter- active   Post- active   Micro- level   Macro -level   MTFeedback [15]    E  *  *   Analytics for redesign [17] E E  E *  *  *   CoCo Design table [16] E, R    *   *   Navi Surface [4]    E,S  * * *   LARAe.TT [3]  E,S  E,S  * * *    Co-located eye-tracking [23]    R  *     Motion sensors [22]    R  *      Script awareness tool [14]  E E   *  *   Do Lenh [8]    E, S  *  *    Monitoring tablets [28]    E  *  *   Learning catalytics [21]    E  *  *         teachers and more than 300 students were involved  in a series of realistic studies conducted during  three regular semester courses. Three types of tasks  were facilitated by the tabletops: collaborative  concept mapping, brainstorming and scripted group  meetings. All the tabletops and the vertical displays  were controlled by a teacher's tablet-based  dashboard (Figure 2, right). This also showed  visualisations that conveyed student's information  in two dimensions: individual participation and  group progress in their task. It also showed  notifications from the MTFeedback subsystem.  This analysed student's artefacts in the backend to  generate both positive and negative notifications  according to the groups misconceptions or underperformance,  automatically identified based on thresholds set by the teacher.   Empirical evaluations studied if the visualisations and  notifications shown in the dashboard effectively supported  teachers micro-level iterative LA process, by enhancing their  classroom awareness and thus allowing them to take more  informed decisions when selecting the groups that required more  attention [15]. Results indicated that the system helped to  seamlessly capture traces of students activity, thus allowing the  generation of live visualisations and notifications for the teacher.  The deployment of the teacher's dashboard on a tablet allowed  free mobility to the teacher while having access to control and  monitoring tools. The visualisations and the notifications allowed  teachers to attend to groups that needed immediate support and  provide formative and/or corrective feedback, which translated  into students conceptual changes.    4.1.2 Learning Analytics for Redesign  The second study consisted in providing LA support in two forms  [17]. First, in the post-active phase, enhancing teachers  Awareness and helping her Assess how the initial intentions  played in the classroom. Second, in the pre-active phase of the  next class session, providing insights into the aspects of the  learning tasks that need to be Redesigned. The setting was the  MTClassroom as for the first case. This study focused on one  teacher designing and then re-designing 1-hour tutorials for two  different subjects in two consecutive university semesters. The  first tutorial involved 236 students distributed in 14 classroom  sessions. The second involved 140 students distributed in 8  sessions. The goals and the topic of both tutorials were similar: to  promote discussion, and deep understanding of political dynamics  for students to learn how to address organisational issues. Both  tutorials had a similar macroscript which basically consisted of  two small-group concept mapping tasks. The captured data  included: application logs, snapshots of the evolution of each  groups concept map and how the teacher advanced the class  according to her script. Three semi-structured interviews were  held with the teacher after the tutorials to capture teachers  intentions and reflections. Teachers intentions were grouped into  three categories: class script progress (A), students participation  (B), and students achievement (C) in all sessions. In the first the  LA support was presented to the teacher in the form of  visualisations (graphs), workflow diagrams, and raw numerical  results about each of the three pedagogical intention categories.  This supported teachers reflection in the post-active phase of the  first macro-level iteration of the learning analytics cycle [17]. The  next two interviews focused on capturing the teachers re-design  decisions as part of the pre-active phase of the next iteration. For   example, regarding the class script (A), the teacher was provided  with a fuzzy workflow diagram (e.g. see Figure 3). She identified  that in most tutorials, students spent too much time in the first  task, not giving enough time for completing the second task.  Concerning students participation (B), a bar chart was shown to  the teacher, indicating that within most groups participation had  not always been equally distributed. A third example (for category  C) is illustrated by the results from a correlation analysis which  suggested that a hierarchical concentric arrangement of students'  concept maps was connected with the achievement of better  solutions. These insights were informative for the teacher to re- design the tutorials. For the next tutorial sessions, the teacher  provided an initial scaffolding solution for students to progress  more quickly and focus on the subsequent higher-level tasks. The  teacher also developed a strategy to encourage all students to use  the tabletop, and to follow a specific concentric layout.  In this study, the surface devices allowed the automated collection  of classroom evidence. The data was exploited to generate visual  and non-visual information to help a teacher compare her planned  intended goals with how they actually played in the classroom.  This example illustrates the synergy between surface technology  and learning analytics to provide continued macro-iterative  support to teachers awareness and planning, across sessions.   4.2 Analytics for Learning Designers  One of the functionalities of using large interactive surfaces is that  they invite all team members to interact with the shared device,  making their actions visible. The next subsection describes a case  study of analytics support for learning design.    4.2.1 CoCoDT: collaborative educational design  This case study consisted in supporting Design and Planning in  the pre-active phase. The goal was to understand how surface  technology and minimalist visual analytics can support high level  learning design. The setting was the Design Studio [16]. Figure 5  shows this multi-surface space providing a set of digital and non- digital tools, including: a tabletop, an IWB, tablets, a white-wall,  a dashboard, and various paper-based materials. The tabletop and   Figure 3. Planned time limits for 5 tasks (top row) and the  enactment of the design for 14 tutorials (bottom row) [17]         Figure 2. Left: An ongoing small-group session in the MTClassroom. The  teacher is holding a tablet-dashboard while providing feedback to one team.  Right: The dashboard showing visualisations of participation for 4 groups        the IWB run an application called CoCoDT. It offers a large  interface customised to support rapid construction of candidate  designs as part of the early stage conceptual design of university  courses. The tool shows a flipped timeline where users can  arrange learning tasks on a weekly basis. This allows the  manipulation of iconic digital objects to configure spatiotemporal  characteristics of learning tasks and their workflow.  The dashboard shows live visualisations of the candidate designs  created in the surface devices. This information includes a list of  the learning tasks added to each candidate design, a pie chart that  shows how students time would be divided among learning  spaces (face-to-face and online), and a histogram showing the  students weekly workload (see Figure 5, right). The goal of  presenting a dashboard with visualisations of multiple candidate  designs is to support teachers' high level comparison and promote  understanding of the impact of substituting certain learning tasks  for equivalent tasks on students workload and direct contact time.   Four teams of three teachers and learning designers participated in  an observational lab study. The goal of each team was to produce  two candidate high-level designs of a university course, satisficing  some competing design goals. Results of the study showed that  the dashboard was one of the features that was most valued by  participants. It provided an overall view of the tasks within each  design and helped most groups in keeping themselves on track  toward their design goals by having continuous access to  indicators of their designs. Moreover, the participants valued the  combination of large devices to have a view of the designs,  smaller sized tablet devices to seek information as needed, and the  dashboard to keep awareness of the changes on their designs.    4.3 Collaborative LA Data Exploration  Collaborative tools have been used to help small groups keep a  shared view and articulate their insights more fluidly than with  single-user displays. Surface devices can be used to support  collaborative reflection on educational data. Next, we describe  two case studies of collaborative LA exploration.    4.3.1 Navi Surface  This case study aimed to support students by enhancing their  Awareness about their achievements to help them self-Regulate  their own learning. The approach relies on a students dashboard  that can be used in the inter-active and/or the post-active phases  (see Figure 4). The third author and his colleagues used the notion  of badges to create Navi Surface [4]. Badges are used to abstract  important aspects of students learning processes, including  intended learning outcomes and produced artefacts such as blog  posts, shared documents. Navi Surface is a tabletop-based tool  that allows teachers and students to navigate students   achievements for a university  course. Users can navigate  through the tool to get more  information about how and why  badges were awarded to which  students, based on the learning  traces that were captured during  the course. Multiple items can be  accessed simultaneously, enabling  group interaction with the data.  The teacher can guide the process  by dragging items onto the  interface to promote discussion  about what students have  achieved, while students can also   interact and steer the conversation.   Navi Surface was evaluated with 14 students (4 groups of 2, 3 and  4 members) who used the tool in groups and individually, and  were able to access their personal data and that of others.  Preliminary observations showed that the interface promotes  engagement, group interaction and evaluation of achievements.  This can be explained as follows. Most dashboards provide a  single-user experience, requiring motivation (either intrinsic or  extrinsic) from a student to access the LA data. The public nature  of a tabletop (as opposed to a more private personal screen)  creates a more inviting environment, facilitating a multi-user  experience for students and their teachers to collaboratively  explore LA data. The tabletop played a key role as catalyst of  discussion and participants considered the approach as a fun way  to interact with LA data. By contrast, when students used the  tabletop alone, a more hesitant interaction was observed. These  observations suggest that the collaborative nature of the surface  device promoted social discourse which may   4.3.2 LARAe.TT  The second case study of this section includes the use of  LARAe.TT [3]. Similarly to Navi Surface, this tabletop tool aims  to support students Awareness and Reflection in the inter-active  and post-active phase, particularly for Inquiry-Based Learning  (IBL) activities. In IBL, teachers encourage learners to pose  questions and formulate hypotheses about a given topic, and  accomplish independent investigations to support their  conclusions. LARAe.TT visualises the paths that students follow  through their inquiry-based learning activities. The tool is  grounded on IBL process model which distinguishes the next six  phases: problem identification, operationalisation, data collection,  data analysis, interpretation and communication. Thus, students  assume an active role to regulate their own learning as each  learner can follow her own path. LARAe.TT allows students and  their teacher(s) to discuss and retrace individual steps taken by  students. They can look up related content such as hypotheses that  were formulated, evidence data that was gathered, etc. Figure 7  shows the LARAe.TT interface, with the visual representations of   Figure 4. Students using Navi Surface in pairs to explore their  achievements through a collaborative badge visualisation      Figure 5. Left: A group of deigners looking at the dashboard while designing 2 candidate  designs (A and B) in the Design Studio. Right: The dashboard showing: a) the tasks   included in each design, b) the proportion of tasks by learning space, and c) the weekly  distribution of students time between online and f2f work     students paths in the centre. The application provides a series of  dropzones that allow students and teachers to physically drag  particular activities to see more details of it in the form of text or  pictures that are evidence of student activity for a particular IBL  phase. Additionally, dragging a student name into a personal drop  zone (coloured squares the figure) allows students to explore and  filter their data according to the position of participants at the  tabletop.  LARAe.TT was presented and evaluated with 15 participants  (teachers, students and researchers) at a workshop. The evaluation  explored how the tabletop application can assist both students and  teachers during the IBL process. It was clear that it could facilitate  students to assess their own progress and manage the distribution  of work. LARAe.TT would not only help students explore  personal achievements, but would also let them compare, reflect  on and learn from the activities of their  peers. Teachers on the other hand could  invite students to the tabletop to initiate  a discussion, to intervene, discuss  progress, ask for clarification and  reasoning, assess activities and point  out peer activities for comparison.  Overall, Navi Surface and LARAe.TT  illustrate a very particular orchestration  use for interactive surfaces to support  reflection and post hoc assessment. The  physicality of the tabletop and the  design of the interface provide a unique  opportunity to support collective f2f  exploration of students data with the  purpose of facilitating discussion  between students and their teacher.    4.4 Multi-Modal Learning  Analytics for Researchers  The previous case studies suggest that  interactive surfaces provide rich  opportunities to support students f2f  interactions and teachers orchestration.  At the same time, they also provide  researchers with a wealth of information  to better understand the nature of social  learning in the inter and post-active  phases: researchers can use many data   collection tools to capture students interactions as they are  learning new concepts by using cameras, microphones, motion  sensors, mobile eye-trackers, galvanic skin response sensors, and  emotion detection tools. We see interactive surfaces as  environments where rich learning episodes can occur, which  makes them ideal candidates for using multi-modal sensors. We  illustrate this idea with the two examples below.    4.4.1 Mobile eye-trackers and Joint Visual Attention  This case study is about capturing a fundamental building block  of students interaction: Joint Visual Attention (JVA). JVA is  known by developmental psychologist and learning scientists to  be a pre-requisite for any kind of high-quality collaboration,  because it allows a group to build a common ground to effectively  solve a problem. The second author and his colleagues [23] have  developed innovative ways to capture JVA around interactive  surfaces. Their methodology involves using fiducial markers  (Figure 6) to remap students gaze onto a ground truth. Since the  fiducial markers are part of the tangible interface, the interactive  surface becomes an essential part of being able to collect and  meaningfully analyse the eye-tracking data. Having both gazes on  the same physical plane allowed the researchers to determine  whether students were jointly looking at the same location at the  same time. They found that the number of times that JVA is  achieved is not only correlated with students quality of  collaboration, but also reflects higher performances on the  problem-solving task as well as higher learning gains. This kind  of data stream allows researchers to generate reliable footprints of  collaboration quality, and separate productive from less  productive groups of students. This data could potentially be  collected in real-time to help teachers decide which groups need  attention and which ones do not need help.  One interesting aspect of multi-modal sensors is that they do not   just allow researchers to more easily collect  quantitative data, but also facilitate  qualitative analyses. The next step of this line  of research is to look at videos augmented  with gaze information (Figure 6 is showing  one frame of this kind of video) to support  qualitative analysis of students interactions.  This kind of analyses was previously difficult  to conduct, because it required researchers to  position multiple cameras around a group to  infer whether two students were  simultaneously looking at the same location.  Sensors can now provide this information to  researchers, which can help speeding up the  pace of qualitative work.    4.4.2 Motion sensors and students  physical mobility    This last case study is about capturing  another key aspect of f2f interactions:  students ability to use their physical body to  express ideas and manage collaborative  processes. These movements can be manually  coded or captured using a motion sensor. For  example, Schneider & Blikstein [22] used a  Kinect sensor to collect data from a study  conducted with 38 students interacting with a  tangible interface, resulting in 1 million data  points describing their body postures. They   Figure 6. Two students analysing a static  version of a Tangible interface. Red lines   show the points used for remapping  students gazes onto a ground truth   (middle figure)       Figure 7 LARAe.TT Activities are shown in the centre of the  screen. The top dropzone lets users expand an activity to get   more details. Each user has a coloured, personal drop zone for  highlighting activities     then fed this matrix into a simple clustering algorithm to obtain  the following prototypical body positions (Figure 8). Not  surprisingly, they found that the time spent by students in the  active posture (left graph of Figure 8) was positively associated  with their learning gains while the passive posture (right graph)  was negatively correlated with them. More interestingly, they  found that the number of times students transitioned from one  posture to another was the strongest predictor for learning. This  suggests that the most successful students were the ones who not  only acted, but also systematically stepped back to reflect on their  actions and think about their next steps. With traditional  qualitative approaches, it would have taken months to identify and  code this kind of behaviour. Using sensors and unsupervised  machine-learning, it took an order of magnitude less time to  isolate this productive learning behaviour.   In conclusion, results suggest that surface devices, augmented  with multi-modal sensors, provide researchers with rich  opportunities to collect massive datasets about students learning  experiences. Those datasets can be then mined using machine- learning algorithms, or used to augment videos and facilitate  qualitative analyses of students interactions.     4.5 Other cases  Other case studies that we analysed are the following. The first  author and colleagues investigated the impact of showing the  teacher visualisations about the enactment of the macro-script  during a class session through a Script Awareness tool [14]. This  is the only example we are aware of, that directly supported the  orchestration aspects of Adaptation and Flexibility to enhance the  Management of the workflow of a multi-surface classroom.  Lenhs work [8] was very similar to the first case study described  above. His system captured from each small group using multiple  tangible tables in a classroom. Then, a public dashboard was  displayed on an IWB for all students and their teacher to be aware  of their progress on the task in comparison with other groups of  students. Recent work by Wang et al. [28] proposed similar  visualisations of the progress of the task for students working with  and sharing tablets (instead of tabletops). Similar cases of learning  analytics applied to interactive surfaces are slowly emerging to  support BYOD (bring your own device) strategies. An example is  Learning Catalytics [21] which provides some visual analytics to  teachers about students progress and their misconceptions while  collaborating in the classroom using tablets or mobiles.     5. DISCUSSION  This section presents a synthesis of conclusions that identify the  degree of maturity, challenges and pedagogical opportunities of  learning analytics and interactive surfaces. In the next subsections,  we discuss different aspects of the case studies presented above,  the implications of defining this design space, the particular  affordances of surface devices and the kinds of analytics that are  promising to support f2f collaborative learning challenges.     5.1 Towards real time analytics in the field  A basic affordance of large surfaces is that (used well) they more  readily support the ergonomic (perceptual, physical, cognitive,  social) characteristics of groups than a small surface. So, it is not  surprising that groupwork is a common denominator in most of  the cases reviewed, but with the difference that they in some cases  support novel kinds of interactivity, and critically, make them  traceable. The case studies showed varied ways to capture  students interactions, enabling teachers to provide enhanced  feedback while orchestrating a classroom, and permitting the  collaborative exploration of student data. The combination of  these technologies has the potential to open up new lines of  research by allowing automatic processing and mining from large  amounts of heterogeneous traces of f2f data (such as physical  actions, gaze, body mobility, speech, etc). Critically, these  technologies are not only analytics tools for researchers, but show  promise for providing real-time feedback of activity to students  and educators. The people who constitute the learning system are  provided with data about their own process, whereas before, they  were the object of study by researchers, who were the only people  with the tools to capture and render such data. Manually analysing  this kind of f2f data through more classical video coding and  observational approaches is time-consuming. As surface analytics  matures, real-time analytics could become practical in authentic  classroom settings at runtime.     5.2 Learning analytics approaches  In the cases reviewed, interactive dashboards and visualisations  were the most common ways to show educational data to  educators and students. The focus was on providing information  about  the task [3; 4; 8; 15; 28] and class [14] progress (Case 1),  students interaction with the shared device (Case 2) [15; 17; 22],  the class design [16; 17], and, to a lesser extent, the quality of the  students solution (Case 3) [17]. Only two studies provided  notifications [15; 21] to the teacher during the inter-active phase  to aid the decision making of the teacher in the classroom. Finally,  detailed and more complex analytics that give information about  more abstract aspects of learning such as achievement [4] and   collaboration [23] have mostly been lab studies  (Case 4).   5.3 Current applications and  learning tasks  5.3.1 Suitable learning tasks  The most suitable tasks for surface technology  seem to be those that involve a combination of  talk, discussion, manipulation of digital or  physical objects in a spatiotemporal   Table 2 Maturity of learning analytics applications utilising interactive  surfaces. E=educators, S=Students, R=Researchers and n= number of studies   Orchestration aspects  Pedagogical phases   Pre-active Inter-active Post-active   Adaptation, Flexibility, & Intervention  E   Regulation & Management  E2,S2 E2,S2  Awareness & Assessment E R2, E4,S3 E,S   Design & Planning E2,R  E     Figure 8. The results of the clustering algorithm on students  body posture. The left centroid is active, with both hands on the  table; the middle one is semi-active, with one hand on the table;   the right one is passive, with both arms crossed       representation plane, and/or that require larger sized displays. The  tasks in the case studies included collaborative concept mapping  [15; 17], brainstorming [15], team meetings [15], data exploration  [3; 4], logistics training [8; 23], and a physiology challenge [22].       5.3.2 Classroom dashboards  The use of dashboards and visualisations in the classroom is still  in its infancy. With the increasing use of digital surfaces in the  classroom (e.g. tablets), it will be very common in the near future  to see more implementations of systems that visualise key aspects  of student's activity and/or performance or simply visualise or  notify them for cases where students are disengaged,  underperforming or not collaborating with their peers. This  information could also be helpful for the students themselves to  self-regulate their interaction and learning activities.    5.3.3 Analytics for collaborative design  The use of learning analytics to support learning design is also an  underexplored area of application. The data captured by  interactive surfaces and the orchestration technology can be  valuable to facilitate teachers reflection on their design [16; 17],  even if the time constraints of the class make it challenging to  make big changes on the original plan, they can re-design for the  following sessions. One case study [17] illustrated how  orchestration support can be provided by learning analytics at a  macro level of iteration (across sessions), showing analytics about  the planned curriculum compared to how it actually occurred.    5.3.4 Sensors and multi-modal analytics  Regarding more complex, multi-modal analytics approaches, the  challenge is to feed these data back to students (and teachers) to  help them make better informed decision and to support students  collaboration. Gaze awareness tools where students in a remote  collaboration can see the gaze of their partner in real time on the  screen can be highly beneficial to students. This allows them to  monitor the visual activity of their partner, and anticipate their  contributions, which leads to higher quality of collaboration and  higher learning gains [23].   5.3.5 Interfaces for teachers and students   Visualisations of individual learner traces on shared surface  devices can help in bootstrapping dialogue between teacher and  students. On the one hand, they allow learners to gain insight into  the learning activities of themselves and their peers and the effects  these have, while allowing teachers to stay aware of the subtle  interactions in their course. In addition, teachers and students can  jointly agree on appropriate learning strategies to follow, based on  collaborative discussion around real factual data [3].   5.4 Maturity and under-attended aspects   Table 2 presents an overview of the orchestration aspects, actors  and pedagogical phases currently addressed by the analysed case  studies. Most effort has been placed on supporting the  orchestration aspects of Awareness and Assessment and in the  inter-active and post-active phases of the learning activities (rows  2 and 3). By contrast, other cells are empty or are populated just  by 1-2 exemplars. The orchestration aspect that refers to  Adaptation, Flexibility, & Intervention has barely been explored.  There is potential to develop solutions that can, for example,  perform automatic or semi-automatic interventions in students  activities. There are still under-attended actors as well. For  example, providing LA tools to enhance students awareness or  other orchestration aspects in the physical classroom has not been   deeply explored. Table 1 (columns 9 and 10) also shows that there  is potential to provide iterative support at a macro level. This can  include providing continued LA support across sessions -  bridging the physical world where interactive surfaces can capture  some traces of f2f activity, with the digital remote access to  resources. An alternative indicator of the maturity of this area of  application is to observe to what extent the LA solutions can be  readily deployed in authentic classrooms. Most of the examples  analysed describe lab-based scenarios, indicating that this area is  rapidly growing but is still exploratory. The only examples of LA  classroom tools mostly supported the orchestration aspect of  Awareness through teachers [15] or public [8] dashboards.    6. CONCLUSION  This paper presented a description of the orchestration aspects,  challenges and pedagogical opportunities of applying learning  analytics solutions utilising interactive surfaces to facilitate a  range of f2f tasks. As illustrated in Tables 1 and 2, this area of is  still immature as the technology is co-evolving along with  pedagogical practices that are beginning to recognise the value  that these pervasive devices may offer. Our analysis framework  helped us characterise the design space in terms of orchestration  aspects that need to be addressed, along with the pedagogical  phases that teachers or students need to accomplish in order to  prepare for classroom sessions. This framework is promising to  help decompose other LA deployments, especially for those  scenarios that can be complex, involving iterative support across  different classroom sessions and considering different tools, and  multiple sessions, LA target users and orchestration aspects.   The paper points at future work still needed to support students  directly, exploit further unexplored affordances of interactive  surfaces (such as sketching), and also support other orchestration  aspects, such as adaptation, flexibility, intervention, management,  design and planning. Besides, most LA support through  interactive surfaces has focused on providing visualisations and  dashboards. Other analytics techniques look particularly  promising for surface tools, given the activity data they are good  at capturing. These may include multi-modal analytics (e.g. traces  of physical actions, or learning analytics approaches for tasks that  require handwriting and sketching using interactive surface),  analytics from heterogeneous sources of data (e.g. coming from  different devices or education software), and the provision of  (semi) automated systems interventions, alarms, or feedback.     7. ACKNOWLEDGMENTS  The research projects described in this paper were supported by:  the European Communitys 7th Framework Programme  (FP7/2007-2013) (318499-weSPOT project), the Erasmus+  programme, Key Action 2 Strategic Partnerships, of the European  Union (2015-1-UK01-KA203-013767-ABLE project), the  Australian Research Council (Grant FL100100203), the National  Science Foundation (NSF #0835854) and the Leading House  Technologies for Vocation Education, funded by the Swiss State  Secretariat for Education, Research and Innovation.   8. REFERENCES  [1] Bowers, J., and Kumar, P. 2015. Students' Perceptions of   Teaching and Social Presence: A Comparative Analysis of  Face-to-Face and Online Learning Environments. Int. J. Web- Based Learn. Teach. Technol., 10, 1, 27-44.  DOI=10.4018/ijwltt.2015010103     [2] Brown, J., Wilson, J., Gossage, S., Hack, C., and Biddle, R.  2013. Surface Computing and Collaborative Analysis Work.  Synt. Lectures on Human-Centered Informatics, 6, 4, 1-168.  DOI=10.2200/S00492ED1V01Y201303HCI019   [3] Charleer, S., Klerkx, J., and Duval, E. 2015. Exploring  Inquiry-Based Learning Analytics through Interactive  Surfaces. In Proceedings of the Workshop on Visual Aspects  of Learning Analytics held at the Int. Conf. LAK'15, 1-4.    [4] Charleer, S., Klerkx, J., Odriozola, S., Luis, J., and Duval, E.  2013. Improving awareness and reflection through  collaborative, interactive visualizations of badges. In  Proceedings of Workshop on Awareness and Refl. in Tech.  Enhanced Learning, 69-81.    [5] Chatti, M. A., Dyckhoff, A. L., Schroeder, U., and Ths, H.  2012. A reference model for learning analytics. Int. J. of Tech.  Enhanced Learning, 4, 5 (May 2012), 318-331. DOI=  10.1504/IJTEL.2012.051815   [6] Dillenbourg, P., and Evans, M. 2011. Interactive tabletops in  education. Int. J. of Comp.Supp. Collab. Learning, 6, 4 (Dec  2011), 491-514. DOI=10.1007/s11412-011-9127-7   [7] Dillenbourg, P., Zufferey, G., Alavi, H., Jermann, P., Do- Lenh, S., Bonnard, Q., Kaplan, F. 2011. Classroom  orchestration: The third circle of usability. In Proceedings of  the Int. Conf. on Comp. Supp. Collab. Learning (Hong Kong,  4-8 July 2011). CSCL '11. NY: Springer 510-517.    [8] Do-Lenh, S. (2012). Supporting Reflection and Classroom  Orchestration with Tangible Tabletops. PhD thesis. EPFL,  Switzerland: CRAFT group, School of Computer Science.    [9] Evans, M., and Rick, J. 2014. Supporting Learning with  Interactive Surfaces and Spaces. In J. M. Spector, M. D.  Merrill, J. Elen & M. J. Bishop, Eds., Handbook of Research  on Educ. Communications and Technol. Springer, NY, 689- 701. DOI=10.1007/978-1-4614-3185-5_55   [10] Kaendler, C., Wiedmann, M., Rummel, N., and Spada, H.  2015. Teacher Competencies for the Implementation of  Collaborative Learning in the Classroom: a Framework and  Research Review. Educ. Psychology Review, 27, 3 (Sep  2015), 505-536. DOI=10.1007/s10648-014-9288-9   [11] Kay, J., Reimann, P., Diebold, E., and Kummerfeld, B. 2013.  MOOCs: So Many Learners, So Much Potential. IEEE  Intelligent Systems, 3 (May 2013), 70-77.  DOI=10.1109/MIS.2013.66   [12] Kharrufa, A., Martinez-Maldonado, R., Kay, J., and Olivier,  P. 2013. Extending tabletop application design to the  classroom. In Proceedings of the Int. Conf. on Interactive  Tabletops and Surfaces (St Andrews, UK, 6-9 October 2013).  ITS '13. NY: ACM, 115-124.  DOI=10.1145/2512349.2512816   [13] Lee, K., Tsai, P. S., Chai, C. S., and Koh, J. H. L. 2014.  Students' perceptions of self-directed learning and  collaborative learning with and without technology. J. of  Comp. Assisted Learning, 30, 5 (Oct 2014), 425-437.  DOI=10.1111/jcal.12055   [14] Martinez-Maldonado, R., Clayphan, A., and Kay, J. 2015.  Deploying and Visualising Teachers Scripts of Small Group  Activities in a Multi-Surface Classroom Ecology: a study in- the-wild. Comp. Supp. Cooperative Work, 24, 2 (Feb 2015),  177-221. DOI=10.1007/s10606-015-9217-6   [15] Martinez-Maldonado, R., Clayphan, A., Yacef, K., and Kay,  J. 2015. MTFeedback: providing notifications to enhance  teacher awareness of small group work in the classroom. IEEE  TLT, 8, 2 (Jun 2015), 187-200.  DOI=10.1109/tlt.2014.2365027   [16] Martinez-Maldonado, R., Goodyear, P., Dimitriadis, Y.,  Thompson, K., Carvalho, L., Prieto, L. P., and Parisio, M.  2015. Learning about Collaborative Design for Learning in a  Multi-Surface Design Studio. In Proceedings of the Int. Conf.  on Comp.-Supp. Collab. Learning (Gothenburg, Sweden, 7- 11 June 2015). CSCL '15. NY: Springer, 174-181.    [17] Martinez-Maldonado, R., Kay, J., Yacef, K., Edbauer, M.-T.,  and Dimitriadis, Y. 2012. Orchestrating a multi-tabletop  classroom: from activity design to enactment and reflection. In  Proceedings of the Int. Conf. on Interactive Tabletops and  Surfaces 2012 (Cambridge, USA, 11-14 November 2012).  ITS '12. NY: ACM, 119-128.  DOI=10.1145/2396636.2396655   [18] Olson, J. S., Teasley, S., Covi, L., and Olson, G. 2002. The  (currently) unique advantages of collocated work. In P. J.  Hinds & S. Kiesler, Eds., Distributed work: New research on  working across distance using technology. MIT Press,  Cambridge, MA, 113-136.    [19] Oviatt, S. 2013. The design of future educational interfaces:  Routledge.   [20] Prieto, L. P., Dlab, M. H., Gutirrez, I., Abdulwahed, M.,  and Balid, W. 2011. Orchestrating technology enhanced  learning: a literature review and a conceptual framework. Int.  J. of Technol. Enhanced Learning, 3, 6, 583-598.  DOI=10.1504/ijtel.2011.045449   [21] Schell, J., Lukoff, B., and Mazur, E. 2013. Catalyzing learner  engagement using cutting-edge classroom response systems in  higher education. Cutting-edge Tech.in Higher Ed. E, 233- 261. DOI=10.1108/S2044-9968(2013)000006E011   [22] Schneider, B., and Blikstein, P. 2015. Using exploratory  tangible user interfaces for supporting collaborative learning  of probability. IEEE TLT. in press.    [23] Schneider, B., Kshitij Sharma, E., Sbastien Cuendet, E.,  Guillaume Zufferey, E., Pierre Dillenbourg, E., and Pea, R. D.  2015. 3D tangibles facilitate joint visual attention in dyads. In  Proceedings of the Int. Conf. on Comp. Supp. Collab.  Learning (Hong Kong, 4-8 July 2011). CSCL '11. NY:  Springer, 158-165.    [24] Sharples, M. 2013. Shared orchestration within and beyond  the classroom. Comp. & Educ., 69, 1 (Nov 2013), 504-506.  DOI=10.1016/j.compedu.2013.04.014   [25] Siemens, G. 2012. Learning analytics: envisioning a research  discipline and a domain of practice. In Proceedings of the Int.  Conf. on Learning Analytics and Knowledge (Vancouver,  Canada, April 29 - May 2). LAK '12. NY: ACM, 4-8.  DOI=10.1145/2330601.2330605   [26] Siemens, G., and Baker, R. S. J. d. 2012. Learning analytics  and Educ. data mining: towards communication and  collaboration. In Proceedings of the Int. Conf. on Learning  Analytics and Knowledge (Vancouver, Canada, April 29 -  May 2). LAK '12. NY: ACM, 252-254.  DOI=10.1145/2330601.2330661   [27] Verbert, K., Duval, E., Klerkx, J., Govaerts, S., and Santos,  J. L. 2013. Learning Analytics Dashboard Applications.  American Behavioral Scientist, 57, 10 (Feb, 2013), 1500- 1509. DOI=10.1177/0002764213479363   [28] Wang, P., Tchounikine, P., and Quignard, M. 2015. A Model  to Support Monitoring for Classroom Orchestration in a  Tablet-Based CSCL Activity. In G. Conole, T. Klobuar, C.  Rensing, J. Konert & . Lavou, Eds., Design for Teach. and  Learning in a Networked World. Springer, 491-496.  DOI=10.1007/978-3-319-24258-3_45     "}
{"index":{"_id":"17"}}
{"datatype":"inproceedings","key":"Papousek:2016:EAP:2883851.2883884","author":"Papouvsek, Jan and Stanislav, V'it and Pel'anek, Radek","title":"Evaluation of an Adaptive Practice System for Learning Geography Facts","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"134--142","numpages":"9","url":"http://doi.acm.org/10.1145/2883851.2883884","doi":"10.1145/2883851.2883884","acmid":"2883884","publisher":"ACM","address":"New York, NY, USA","keywords":"attrition bias, computerized adaptive practice, engagement, evaluation, learning curve, survival analysis","abstract":"Computerized educational systems are increasingly provided as open online services which provide adaptive personalized learning experience. To fully exploit potential of such systems, it is necessary to thoroughly evaluate different design choices. However, both openness and adaptivity make proper evaluation difficult. We provide a detailed report on evaluation of an online system for adaptive practice of geography, and use this case study to highlight methodological issues with evaluation of open online learning systems, particularly attrition bias. To facilitate evaluation of learning, we propose to use randomized reference questions. We illustrate application of survival analysis and learning curves for declarative knowledge. The result provide an interesting insight into the impact of adaptivity on learner behaviour and learning.","pdf":"Evaluation of an Adaptive Practice System for Learning  Geography Facts  Jan Papouek  Faculty of Informatics  Masaryk University  Botanick 68a  602 00 Brno, Czech Republic  jan.papousek@mail.muni.cz  Vt Stanislav  Faculty of Informatics  Masaryk University  Botanick 68a  602 00 Brno, Czech Republic  slaweet@mail.muni.cz  Radek Pelnek  Faculty of Informatics  Masaryk University  Botanick 68a  602 00 Brno, Czech Republic  xpelanek@mail.muni.cz  ABSTRACT Computerized educational systems are increasingly provided as open online services which provide adaptive personalized learning experience. To fully exploit potential of such sys- tems, it is necessary to thoroughly evaluate dierent de- sign choices. However, both openness and adaptivity make proper evaluation dicult. We provide a detailed report on evaluation of an online system for adaptive practice of geog- raphy, and use this case study to highlight methodological issues with evaluation of open online learning systems, par- ticularly attrition bias. To facilitate evaluation of learning, we propose to use randomized reference questions. We il- lustrate application of survival analysis and learning curves for declarative knowledge. The result provide an interesting insight into the impact of adaptivity on learner behaviour and learning.  CCS Concepts Human-centered computing!User studies; Applied computing ! Computer-assisted instruction;  Keywords attrition bias, computerized adaptive practice, engagement, evaluation, learning curve, survival analysis  1. INTRODUCTION Open online educational systems are becoming a key part  of education  systems like Khan academy, Duolingo, or edX are today used by millions of learners and in the future the role of such systems is expected to grow. One advantage of computerized educational systems is adaptivity  their behaviour can be personalized for a particular learner. To assess the contribution of such educational systems and to tune their behaviour (e.g., choose a proper learner model for guiding the adaptive behaviour), we need to evaluate  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883884  them. However, both openness and adaptivity significantly complicate the evaluation process.  The open nature of these systems means that they can be used by anybody, anywhere. This has several consequences for evaluation. Standard evaluation methods (like pre-test, post-test) are not applicable. The learner population is typ- ically very heterogeneous, often comprising students using the system compulsory within classroom, students using the system voluntary as part of their preparation for an exam, adult learners who want to refresh their knowledge, and also people who just stumbled upon the system while browsing an internet or following a suggestion of a friend on a so- cial network. The motivation of learners to use the system thus widely diers, the distribution of time in the system is typically highly skewed (most learners use the system for only a short time) and the departure from the system is not random. This creates attrition bias, which complicates eval- uation of learning within the system. Adaptive behaviour of systems further complicates the evaluation  each learner proceeds through the system using dierent learning mate- rials and questions and it is not easy to use these adaptively constructed questions for evaluation of learning gains. More- over, feedback loop between a learner model and collection of data for evaluation [13] further complicates the evalua- tion.  Evaluation of adaptive systems has been studied before. Evaluation of recommender systems [5] faces many similar issues. Specifically for educational systems, previous re- search [10, 4] discussed wide coverage of dierent methods and evaluation aspect, but only on a high level without dis- cussing specific details. Layered evaluation [20, 1] has been proposed as a basic framework for evaluation of adaptive systems. Current research, however, focuses mainly on eval- uation of learner models (the first layer), which can be done using historical data and evaluation of predictive accuracy measured by metrics [21]. There has been attempts to use historical data to assess impact on learners [3], but reliable assessment of this impact needs a proper randomized con- trol trial and such experiments are for open online adaptive systems currently rather rare.  Let us overview specific methods for evaluation of edu- cational systems and discuss their properties and applica- bility in the context of open and adaptive systems. The gold standard for evaluation of educational interventions is a randomized control trial together with pre-test and post- test to evaluate learning gains. In our setting this is however not feasible due to the complete lack of control over learners  http://dx.doi.org/10.1145/2883851.2883884   using the system online. Another way to obtain high-quality evaluation would be to use results from external tests (e.g., university exams), but in most cases it is infeasible to obtain such data (learners using open systems typically do not take any external test). It may be possible to use voluntary tests within the system to asses learning, but the motivation to take the test influences results (self-selection bias).  A realistic approach is to use learning curves [11] which map learning during the usage of the system. Interpreta- tion of learning curves is, however, complicated by issues with aggregation and attrition bias [12, 15]. Attrition bias is a key issue in evaluation of educational systems, methods from survival analysis may be useful [2]. We can also use learner modeling techniques and use model based detectors of learning. However, such approach is dependent on qual- ity of models and their simplifying assumptions, which may influence results. Another relatively easily realizable (but imprecise) approach is to evaluate only aspects of the sys- tem which are more easily measurable than learning and use them as proxy metrics, e.g., number of answers or learner feedback [17]. However, previous research [9] suggests that such metrics may not be directly correlated with learning.  We propose to use periodic reference questions which are constructed fully randomly. Similar approach based on usage of random items have been used for evaluation previ- ously in [7, 8]. We analyze reference questions using learning curves. Because of a random aspect of these questions, there is no influence from the adaptive algorithm, and thus we can fairly compare dierent conditions. However, there still re- mains attrition bias, which needs to be taken into account.  In this work we explore methodological evaluation issues using a specific case study  a widely used system for adap- tive practice of geography facts [18, 17]. Using this system we performed a randomized control trial  a comparison of 4 dierent strategies for question construction ranging from fully adaptive to fully random. We explore factors influenc- ing the length of stay within the system, where we employ techniques from survival analysis (particularly fit to Weibull distribution). We also explore an application of learning curves describing learners progress of declarative knowledge (learning curves have previously been used mainly for eval- uating procedural skills [11]). Using these techniques we illustrate when and how the adaptivity is important for the studied system. Our main point, however, is not limited for a particular system. Exploration of techniques and method- ological issues of evalutation is generally relevant to any open and adaptive educational system, e.g., our results highlight the role of attrition bias and show how this bias can influ- ence learning curves in dierent directions (in the context of a single educational system).  2. EXPERIMENTAL SETTING  2.1 The Used System For our experiments we use an online adaptive system  providing practice of geographical facts (names and loca- tion of countries, cities, . . . ), available at outlinemaps.org. The system estimates learners knowledge and based on this estimate it adaptively constructs questions of suitable dif- ficulty [18]. The system uses open questions (Where is France) and multiple-choice questions (What is the name of the highlighted country) with 2 to 6 options. Learners answer questions with the use of an interactive outline map.  These questions are asked in sequences of length 10, and al- though learners can quit a sequence anytime, they tend to finish it. After each sequence, a summary overview about practiced items is shown. Learners can also access a visu- alization of their knowledge using an open learner model. During a school year we collect roughly 1 000 000 answers from 10 000 users per month. Part of the data is publicly available1 [16].  The adaptive behaviour of the system is based on models of learners knowledge which provide for each learner and place current prediction of knowledge (probability of correct answer). These models have been described and evaluated in previous work [14, 22], here we use them as a black box.  An important factor influencing the evaluation and inter- pretation of results are dierent contexts within the system. Learners can use the system with a lot of dierent contexts (maps, types of places) and these contexts dier widely in their diculty (prior knowledge) and the number of places available to practice (from 10 to 120). Distribution of an- swers is highly uneven, most learners practice few popular maps (Figure 1).  Figure 1: The most commonly answered contexts. For each of these context we also report the average error rate for the first answer of each learner.  2.2 Users of the System This system is a typical open educational system avail-  able to anybody free of charge. We have no control over the number of answered questions, the time when learners practice, or whether they ever return to the system after one session of practice. The majority of learners is from the Czech Republic (84%) and Slovakia (8%) since the interface was originally only in Czech. However, English, German, and Spanish are currently also available.  2.3 Experimental Conditions The system uses a target error rate (20 %) and adap-  tively constructs questions in such a way that the learners achieved performance is close to this target [17]. Firstly, the algorithm selects the stem of the question (which place to ask about). Secondly, it chooses the number of options for a multiple choice question and particular distractors. In our experiments we evaluate four versions of the question con- struction algorithm; for both construction steps we consider 1www.fi.muni.cz/adaptivelearning/data/slepemapy/  outlinemaps.org www.fi.muni.cz/adaptivelearning/data/slepemapy/   an adaptive condition and a random condition: adaptive- adaptive (A-A), adaptive-random (A-R), random-adaptive (R-A), random-random (R-R). New learners were assigned to conditions randomly upon entering the system. Learners present in our system before this experiment are provided with the A-A condition and are not taken into account for further analysis.  Adaptive version of item selection (A-* ) computes a score for each item taking into account its diculty, number of a learners answers about it and time elapsed since the last learners answer about it. Random version of item selection (R-* ) picks the stem randomly. As for construction of op- tions, adaptive version (*-A) computes a number of options to make the question as close to the target diculty as it is possible and uses the most competitive distractors. Ran- dom version (*-R) chooses a number of options and options themselves fully randomly. Both version provide multiple- choice questions with from 2 to 6 options or completely open ones. It is worth noting that A-A condition prefers open questions, on the other side questions constructed by the R-R condition are mostly multiple-choice. Distractors for this condition are non-competitive, so it provides the easiest practice from all studied conditions (Figure 2, top).  To provide better intuition behind the used experimental conditions, we discuss specific example of question construc- tion for a new learner who chooses to practice African coun- tries. The first construction step in A-* condition prefers Algeria (estimated error rate 25%) to Madagascar (6%  too easy) or Zimbabwe (55%  too dicult), whereas R-* condi- tion selects countries with uniform probability. In the second step, if R-A has Zimbabwe from the first step, it reduces its diculty by selecting only 2 options (Zimbabwe and 1 com- petitive distractor - Zambia), whereas A-A has Algeria from the first step, Algeria has appropriate diculty, and the al- gorithm thus selects either open question or a high number of options (6) with competitive distractors (Egypt, Libya, Dem. Rep. Congo, South Sudan, and Sudan). Regard- less of whether the first step selected Algeria, Zimbabwe, or some other country, both *-R conditions select random number of options and random distractors (e.g., 4-options question with distractors Morocco, Tanzania, and Ghana).  2.4 Collected Data In case of this experiment running from the end of Au-  gust to October 2015 we have collected more than 1 300 000 answers from roughly 20 000 learners. For each context sepa- rately every 1st, 11th, 21st, . . . are reference questions, open questions about an item randomly chosen from the context. These questions result to 1st, 2nd, 3rd, . . . reference answers which are used to track learning. It should be noted that 1st reference answer comes before the question construction algorithm has any chance to influence the practice for the given context. We also ask learners to evaluate the diculty of questions. After 30, 70, 120, and 200 answers the system shows the dialogWhat is the diculty of asked questions, learners choose one of the following options: Too Easy, Appropriate, Too Dicult. Within this experiment we analyze roughly 16 000 records. To make our research repro- ducible we make the analyzed data set available2, together with a brief description and terms of use.  2www.fi.muni.cz/adaptivelearning/data/slepemapy/ 2015-ab-random-parts.zip  3. EVALUATION: ENGAGEMENT At first we evaluate impact of individual conditions on stu-  dent engagement. Students engagement depends not only on system behaviour, but also on their motivation. As the system is open to anyone, its users vary in many aspects including their motivation for using the system.  Some learners use the system in school lessons. Learners in schools are mostly aected by external motivation fac- tors as they are constrained by the time allocated by their teacher and might not be genuinely interested in practicing of geography. We can detect in-school usage based on the IP address (a group of at least n learners who started using the system from the same IP address is identified as an in- school group). The in-school usage represents about 20% of the collected data.  There are also learners preparing for their school exams at home. These learners are probably more focused on mas- tering a particular map and not motivated to return to the system after the exam. Finally, some learners use the system just for fun. These learners do not have external motivation and thus are most likely to be aected by the system be- haviour (e.g., leave the system if the practice is too dicult or not challenging enough). Although we have anecdotal ev- idence of these learner groups in the system, we do not have enough data to reliably distinguish between the latter two groups.  3.1 Statistics for Conditions The experimental conditions dier in learners error rate  (Figure 2). Conditions A-R and R-R have overall lower er- ror rate because they are more likely to use fewer options. R-* conditions exhibit decline in the error rate through- out the use of the system (Figure 2, bottom), whereas A-* conditions by definition keep the error rate more constant. Especially the A-A error rate is distributed closely around the target error rate (Figure 2, top). On the other hand, R-R error rate distribution is skewed towards 0%.  Error rate is influenced by average item diculty which varies largely among dierent contexts. For the 10 most practiced contexts the error rate on the first reference ques- tion is between 30% and 80% (Figure 1). The relation of av- erage item diculty and error rate is dierent for dierent experiment conditions on dierent contexts. In R-* con- ditions the error rate is highly influenced by average item diculty of the context. A-* conditions can decrease error rate by asking multiple-choice questions with fewer alterna- tives. However, when every item on a given contexts has prediction below the target error rate, then there is no way to increase the error rate.  3.2 Explicit Feedback Figure 3 shows the results of learners explicit feedback  about diculty of questions. The most appropriately di- cult questions among the experimental conditions are asked by the A-A condition. The other three conditions exhibit increased number of Too Easy evaluations. In particular, both *-R conditions have increased number of Too Easy compared to their *-A counterparts.  Explicit feedback also reflects error rate dierences among contexts. Random conditions are more varied in this re- spect than adaptive conditions in both the first and the sec- ond question construction step. There is more space for adaptivity to make a dierence in contexts with average  www.fi.muni.cz/adaptivelearning/data/slepemapy/2015-ab-random-parts.zip www.fi.muni.cz/adaptivelearning/data/slepemapy/2015-ab-random-parts.zip   Figure 2: Comparison of error rates for the four conditions. Top: Histogram of the overall error rate. Bottom: Error rate as function of the number of attempts.  item diculty far from the target error rate. Especially in contexts with low average diculty and thus non-trivial prior knowledge (e.g., Czech regions, European countries or cities), there are more Too Easy evaluations in R-R condi- tion (Czech regions: 33%, European countries: 41%, Euro- pean cities: 33%) than in A-A (Czech regions: 26%, Euro- pean countries: 24%, European cities: 24%). For A-R and R-A conditions the percentages of Too Easy evaluations are somewhere in between A-A and R-R. On the other hand in contexts with lower prior knowledge (e.g., Czech moun- tains) the amount of Too Easy evaluations is less diverse among conditions (A-A: 23%; R-R: 25% for Czech moun- tains)  As we ask for the evaluations repeatedly, the proportion of Appropriate evaluations is slightly rising with the num- ber of questions answered by the learner. It is most likely caused by dissatisfied learner leaving the system (attrition bias). This eect, however, does not occur in A-R, where proportion of Appropriate evaluations stays at the same level, but proportion of Too Dicult declines in favor of Too Easy.  3.3 Survival Analysis To compare attractiveness of dierent conditions we an-  alyze the number of answers within the system. The key ob- servation is that the distribution of answers is very skewed  Figure 3: Explicit feedback by learners about ques- tion diculty for the four conditions. Black lines show 95% confidence intervals.  and thus it is not suitable to compare conditions using av- erages (or even other measures of central tendency like the median). It is useful to employ techniques from survival analysis. Survival analysis deals with questions like What proportion of population will survive past a given time, typically in the context of medical data. Once we interpret survivalasactive usage of a system, it is directly relevant to evaluation of educational systems.  Figure 4 (left) shows a survivor graph, i.e., proportion of learner population surviving the given number of ques- tions. There are clear discrete steps after multiples of 10, these are due to the properties of the analyzed system de- scribed in Section 2.1 which presents a summary overview of a learners progress after each sequence of 10 questions (and thus creates natural points to leave the system). Once the length of stay is analyzed for groups of 10 questions, the graph becomes smooth. Figure 4 (right) shows this pre- processed variant in the form of probability density function with fitted Weibull distribution. This is a standard distri- bution in survival analysis, previous research shows that it also fits well dwell time on web pages [6], and it has also been used to fit MOOC data [23]. Our results indicate that the Weibull distribution is useful also for fitting the number of answers within an open educational system.  Table 1: Fitted parameter of the Weibull distribu- tion.  Condition k   A-A 0.762 6.673 A-R 0.793 5.849 R-A 0.746 6.349 R-R 0.751 5.882  Probability density function of the Weibull distribution is  f(x, k,) = k   ( x   )k1e(x/) k (k, > 0, x  0). The distri-  bution has two parameters: k is the shape parameter and  is the scale parameter. Values k < 1 correspond to negative aging (infant mortality), i.e., the probability of leaving de- creases with the length of stay, for k = 1 we get exponential distribution (constant rate of leaving), values k > 1 corre- spond to positive aging. Fitted parameter values for our four conditions are in Table 1. In all cases we have k < 1, i.e., negative aging (which is typical for online systems [6]). The    Figure 4: Left: Survivor graph (proportion of learners that answers at least k questions). Right: Probability density function for number of attempts (groups of 10) fitted by the Weibull distribution for the A-A condition (for other conditions the fit is very similar).  Table 2: Probability of return to the system. The 95% confidence interval is in all cases 0.9% around the given value.  Condition Probability  A-A 15.1% A-R 13.9% R-A 14.3% R-R 13.1%  table shows that adaptivity in the first question construc- tion step is related to the k parameter (adaptivity reduces infant mortality), whereas adaptivity in the second step is related to the  parameter (the length of stay). This be- haviour can be seen also on the survivor graph (Figure 4 left), where at the beginning we have higher survivor rates for conditions A-A and A-R, whereas in the long run we have top performance for A-A and R-A.  We have also analyzed probability that a learner returns to the system (as a return to the system we consider oc- currence of two attempts with pause between them of least 10 hours). Table 2 shows the comparison of conditions, we see that adaptivity increases chance of return. The rela- tive dierence between A-A and R-R condition is 15%, i.e., adaptability has large impact on learners decision to return to the system.  4. EVALUATION: LEARNING Our system provides practice of items which are indepen-  dent on each other. We assume practice of an item A does not have any impact on knowledge of an item B, or the impact is negligible. Although the provided practice seems to be similar to testing, data collected using question con- struction algorithm can not be simply used to evaluate pro- gression of learners performance across dierent conditions, because these conditions dier in questions they ask, e.g., R-R condition provides easy, mainly multiple-choice ques- tions, on the other side in case of A-A condition questions are more dicult and mostly open. For the same reason we  can not use a model providing estimation of learners per- formance based on data for this purpose, e.g., answers on too easy or too dicult questions do not contain the same amount of information as in case where the probability of correct answer is close to 50%, so it takes much longer time to estimate learners knowledge. Estimation and improve- ment of learners knowledge happen in the same time and we have to be sure one condition is not disadvantaged be- cause of poor behaviour in estimation, since it can perform well in learning. In the following section we focus on analyz- ing answers to reference questions which represent objective data collected independently on studied conditions (they are constructed randomly).  4.1 Learning Curves To measure learners knowledge to analyze learning we  look at learners performance based on answers to reference questions. For each context we build series of learners ref- erence answers, we merge these series together and compute an average error rate for each attempt. By this technique we are able to analyze progression of performance for con- text(s) and a group of learners, but we can not simply do the same on a level of one item and one learner.  As the analysis in previous section shows, there is high attrition in the data (learners are leaving the system at dif- ferent points of time). Due to this attrition it is not straight- forward to construct and compare learning curves. We con- sider three approaches to construction of learning curves, each has dierent advantages and disadvantages:  1. All learners: In this case we include all learners. Since learners have varied number of answers, individual points of the learning curve are computed from dif- ferent samples of learners. Previous research [12] has shown how this may lead to flat learning curves in case of mastery learning.  2. Filtered learners: We construct a curve for n answers and we include only learners having at least n answers. Now each point of the learning curve is computed from the same sample of learners, but this sample may be biased.    3. Filtered learners, reverse: Similarly to the previous case, but in case of learners who have more than n answers, we use the last n answers.  4.2 Attrition bias The interpretation of learning curves is complicated by  attrition bias. Attrition bias is a type of selection bias which is often present for example in medical experiments. In the context of educational systems and evaluation using learning curves, previous research identified mastery attri- tion bias [12, 15]  when learners, who master the studied topic stop practice (e.g., due to the use of mastery learn- ing in the educational system), the learning curves become significantly flatter and can even mask learning.  Mastery is, however, not the only source of attrition. Par- ticularly in open online systems, dierences in motivation may also play a significant role. If a system, for example, oers rather dicult questions, this may disengage and deter weaker learners and we may obtain an opposite of mastery attrition (self-selection) which causes learning curves to be steeper.  Our results suggest that these two eects may be present at the same time within one system and it may be hard to disentangle them and interpret learning curves correctly. For the analysis presented in Figure 5 we consider dierent groups of learners based on the number of answers to refer- ence questions. The figure shows for each group an average error rate on the first reference question (which corresponds to the prior knowledge for the group). The results show that there are dierences between these groups, i.e., learn- ers attrition is not random. Particularly interesting aspect of the figure is that the attrition diers between conditions. We see, for example, a big dierence between learners hav- ing at least 3 reference answers in R-A and R-R conditions. Learners having at least 3 reference answers in A-R condi- tion are those with above average prior knowledge, whereas in R-A condition those learners have below average prior knowledge. This phenomenon should be taken into account in case of learning curve based on learners having at least n answers (Figure 6, middle and right). It can easily happen we compare learning of dierent groups of learners, e.g., in this case low performers vs. high performers. The same phe- nomenon is present even though we do not use any filtering, but it is combined with mastery attrition bias.  Figure 5: The first attempt error rate depending on how many reference questions the learner answered.  We do not come up with a solution to prevent the men-  tioned problems, but based on these observations we decided to analyze several types of learning curves and check how dierent methodologies aect the relative order of analyzed conditions with respect to learning. The advantage of the first approach without any filtering is that we do not ar- bitrarily omit any learner. In case of filtering out learners with less number of reference answers, the given number n we take the first n or the last n learners answers (Figure 6, middle and right). In case of the last n answer we see how learners learn before they quit the practice. Since we assume many learners quit the practice once they master the topic, this curve has generally much lower error rate.  4.3 Results Figure 6 (top) shows the resulting learning curves. Con-  fidence intervals are computed for each data point indepen- dently. The confidence intervals for individual points over- lap, but we get repeatedly similar results (ordering of exper- imental conditions). Based on previous research on learning curves [11], we fit the power law function to the data, i.e., error rate(k) = akb, where k is the order of the attempt.  Another potential source of bias in learning curves is aggregation across dierent contexts (knowledge compo- nents) [11]. This issue is again more prominent in open educational systems, where learners can freely choose top- ics. Imagine that one of our experimental conditions is more interesting to learners for easier contexts, whereas other is more interesting for dicult contexts. This would mean a lower error rates in the aggregated learning curves for the first condition. However, this dierence between learning curves would not be due to dierences in learning, but due to dierent impact of conditions on engagement. It is thus useful to analyze also learning curves for individual contexts. A disadvantage of these disaggregated results is that they are constructed from smaller amount of data and thus the learning curves are more noisy. Figure 7 shows examples of such curves for a few popular contexts within our system.  The use of error rate as a measure of learning is a standard (and in our setting natural) choice. It is, however, not the only possible choice. We can take into account also other aspects of learners behaviour, e.g., the response time. Our previous research shows that the time learners spent by an- swering questions relates to their future success [19]. Even though the system does not motivate learners to have low re- sponse time (in fact it does not even indicate in any way that the response time is measured), we observe an improvement of response time and systematic dierences between studied conditions (Figure 8). With respect to this measure we get the best results for the R-A condition.  Although none of the presented learning curves is ideal, the main results are consistent across dierent analysis methods. In all cases the conditions with adaptive construc- tion of options (A-A, R-A) beat the conditions with random options (A-R, R-R). The item selection part does not seem to have large eect on learning. When we see dierences between the A-A and R-A conditions, the R-A condition is slightly better, i.e., it seems that with respect to learning the adaptive choice of stems could be improved.  5. DISCUSSION Our work focuses on evaluation of an open online educa-  tional system. In such systems it is important to evaluate impact of system behaviour on both learners motivation and    Figure 6: Learning curves based on reference questions. Top: Raw data with 95% confidence intervals. Bottom: Fitted power law functions. Columns correspond to dierent types of filtering.  Figure 7: Learning curves (fitted power law functions) for specific contexts: European states, Czech mountains and Czech cities.  learning. To analyze the length of the stay within the sys- tem we utilize techniques from survival analysis, particularly we fit the Weibull distribution and show that the fitted pa- rameters provide insight into the impact of adaptive system behaviour on learners use of the system. We also show that attrition diers for dierent conditions (versions of the used system), which creates attrition bias that complicates anal- ysis of learning within the system. Previous work [12, 15] has considered mastery attrition bias, but our results show that attrition bias is not just due to mastery.  To evaluate learning we work with learning curves. To use learning curves in adaptive system we employ refer- ence questions which are constructed randomly; this allows us to perform fair comparison of dierent question construc- tion algorithms. However, the constructed learning curves still give a simplified view of learning. A particular disad- vantage of learning curves is that they take into account only order of questions and not the time that passed be- tween attempts. This aspect may be particularly important for declarative knowledge (as opposed to procedural skills for which learning curves have been used so far). In the current data set, practice is currently mostly massed (80%  of answers are within the first session of a learner), so the used simplification should not have significant impact on presented results.  To incorporate the timing information into the analysis, it may be useful to study learning surface, e.g., in the form of graph depicted in Figure 9, which visualizes data from our experiment. The figure shows error rate depending on both order of an attempt and the time from a previous at- tempt. Such analysis may help us to evaluate also long term eects of dierent learning situations. A proper way to con- struct and compare such learning surfaces is an interesting direction for future work.  The results of our evaluation demonstrate the advantage of adaptive behaviour over a baseline, random selection of questions. More interestingly, the results show a part of the question construction for which the adaptivity is im- portant. For the studied setting it turns out that adaptiv- ity is important mainly for choosing number of options for multiple-choice questions and for choosing distractor (i.e., fine-tuning question diculty), not in the choice of a ques- tion stem (the target factual knowledge). With respect to the length of the stay within the system the adaptive choice    Figure 8: Learning curve for response times (the re- ported time for each attempt is the median of cor- responding times).  Figure 9: Learning surface showing error rate de- pending on both number of attempts and time from previous attempt.  of a stem is related to initial mortality, whereas the adap- tive choice of options is related to the overall length of the stay. With respect to learning the main factor is adaptivity in choice of options. For the choice of stem we get the same or even better results for random selection. This suggests that the algorithm for the choice of a question stem [18, 17] may need to be improved.  6. ACKNOWLEDGMENTS This publication was written with the support of the Spe-  cific University Research provided by the Ministry of Edu- cation, Youth and Sports of the Czech Republic.  7. REFERENCES [1] P. Brusilovsky, C. Karagiannidis, and D. Sampson.  Layered evaluation of adaptive learning systems. International Journal of Continuing Engineering Education and Life Long Learning, 14(4-5):402421, 2004.  [2] M. Eagle and T. Barnes. Survival analysis on duration data in intelligent tutors. In Intelligent Tutoring Systems, pages 178187. Springer, 2014.  [3] J. P. Gonzalez-Brenes and Y. Huang. Your model is predictive  but is it useful theoretical and empirical  considerations of a new paradigm for adaptive tutoring evaluation. Educational Data Mining, 2015.  [4] J. Greer and M. Mark. Evaluation methods for intelligent tutoring systems revisited. International Journal of Artificial Intelligence in Education, pages 16, 2015.  [5] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. Evaluating collaborative filtering recommender systems. ACM Transactions on Information Systems (TOIS), 22(1):553, 2004.  [6] C. Liu, R. W. White, and S. Dumais. Understanding web browsing behaviors through weibull analysis of dwell time. In Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval, pages 379386. ACM, 2010.  [7] Y.-E. Liu, T. Mandel, E. Brunskill, and Z. Popovic. Towards automatic experimentation of educational knowledge. In Human Factors in Computing Systems, pages 33493358. ACM, 2014.  [8] Y.-E. Liu, T. Mandel, E. Brunskill, and Z. Popovic. Trading o scientific knowledge and user learning with multi-armed bandits. In Educational Data Mining 2014, 2014.  [9] D. Lomas, K. Patel, J. L. Forlizzi, and K. R. Koedinger. Optimizing challenge in an educational game using large-scale design experiments. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 8998. ACM, 2013.  [10] M. A. Mark, J. E. Greer, et al. Evaluation methodologies for intelligent tutoring systems. Journal of Artificial Intelligence in Education, 4:129129, 1993.  [11] B. Martin, A. Mitrovic, K. R. Koedinger, and S. Mathan. Evaluating and improving adaptive educational systems with learning curves. User Modeling and User-Adapted Interaction, 21(3):249283, 2011.  [12] R. C. Murray, S. Ritter, T. Nixon, R. Schwiebert, R. G. Hausmann, B. Towle, S. E. Fancsali, and A. Vuong. Revealing the learning in learning curves. In Artificial Intelligence in Education, pages 473482. Springer, 2013.  [13] J. Niznan, R. Pelanek, and J. Papousek. Exploring the role of small dierences in predictive accuracy using simulated data. In AIED Workshop on Simulated Learners, 2015.  [14] J. Niznan, R. Pelanek, and J. Rihak. Student models for prior knowledge estimation. In Educational Data Mining, 2015.  [15] T. Nixon, S. Fancsali, and S. Ritter. The complex dynamics of aggregate learning curves. In Educational Data Mining, 2013.  [16] J. Papousek, R. Pelanek, and V. Stanislav. Adaptive geography practice data set. Journal of Learning Analytics, 2015. To appear.  [17] J. Papousek and R. Pelanek. Impact of adaptive educational system behaviour on student motivation. In Artificial Intelligence in Education, volume 9112, pages 348357, 2015.  [18] J. Papousek, R. Pelanek, and V. Stanislav. Adaptive practice of facts in domains with varied prior knowledge. In Educational Data Mining, pages 613,    2014. [19] J. Papousek, R. Pelanek, J. Rihak, and V. Stanislav.  An analysis of response times in adaptive practice of geography facts. In Educational Data Mining, 2015.  [20] A. Paramythis, S. Weibelzahl, and J. Mastho. Layered evaluation of interactive adaptive systems: framework and formative methods. User Modeling and User-Adapted Interaction, 20(5):383453, 2010.  [21] R. Pelanek. Metrics for evaluation of student models. Journal of Educational Data Mining, 7(2), 2015.  [22] R. Pelanek. Modeling students memory for application in adaptive educational systems. In Educational Data Mining, 2015.  [23] D. Yang, T. Sinha, D. Adamson, and C. P. Rose. Turn on, tune in, drop out: Anticipating student dropouts in massive open online courses. In Proceedings of the 2013 NIPS Data-Driven Education Workshop, volume 11, page 14, 2013.    Introduction  Experimental Setting  The Used System  Users of the System  Experimental Conditions  Collected Data   Evaluation: Engagement  Statistics for Conditions  Explicit Feedback  Survival Analysis   Evaluation: Learning  Learning Curves  Attrition bias  Results   Discussion  Acknowledgments  References   "}
{"index":{"_id":"18"}}
{"datatype":"inproceedings","key":"Karkalas:2016:TAE:2883851.2883943","author":"Karkalas, Sokratis and Mavrikis, Manolis and Labs, Oliver","title":"Towards Analytics for Educational Interactive e-Books: The Case of the Reflective Designer Analytics Platform (RDAP)","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"143--147","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883943","doi":"10.1145/2883851.2883943","acmid":"2883943","publisher":"ACM","address":"New York, NY, USA","abstract":"This paper presents an analytics dashboard that has been developed for designers of interactive e-books. This is part of the EU-funded MC Squared project that is developing a platform for authoring interactive educational e-books. The primary objective is to develop technologies and resources that enhance creative thinking for both designers (authors) and learners. The learning material is expected to offer learners opportunities to engage creatively with mathematical problems and develop creative mathematical thinking. The analytics dashboard is designed to increase authors' awareness so that they can make informed decisions on how to redesign and improve the e-books. This paper presents architectural and design decisions on key features of the dashboard and discusses future steps with respect to the potential for exploratory data analysis.","pdf":"Towards Analytics for Educational Interactive e-Books  The case of the Reflective Designer Analytics Platform (RDAP)  Sokratis Karkalas London Knowledge Lab  UCL Institute of Education London WC1N 3QS, UK s.karkalas@ucl.ac.uk  Manolis Mavrikis London Knowledge Lab  UCL Institute of Education London WC1N 3QS, UK  m.mavrikis@ucl.ac.uk  Oliver Labs University of Potsdam  Am Neuen Palais 10, House 9 14469 Potsdam, Germany  labs@cermat.org  ABSTRACT This paper presents an analytics dashboard that has been developed for designers of interactive e-books. This is part of the EU-funded MC Squared project that is developing a platform for authoring interactive educational e-books. The primary objective is to develop technologies and re- sources that enhance creative thinking for both designers (authors) and learners. The learning material is expected to oer learners opportunities to engage creatively with mathe- matical problems and develop creative mathematical think- ing. The analytics dashboard is designed to increase au- thors awareness so that they can make informed decisions on how to redesign and improve the e-books. This paper presents architectural and design decisions on key features of the dashboard and discusses future steps with respect to the potential for exploratory data analysis.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3 [Computers and Education]: Miscellaneous  1. INTRODUCTION The growing interest on learning analytics dashboards is  partly due to their potential of providing, both in real time and retrospectively, an opportunity for awareness and decision- making that is otherwise dicult if not impossible. As ed- ucational applications are being adopted and used at scale, understanding their usage and their impact on learning is important. We are interested in the particular genre of dig- ital, interactive books (or e-books) that are beginning to be established as a possible alternative to static textbooks oering several advantages both practical (such as portabil- ity or low cost) and pedagogical (such as interactivity and potential for formative feedback) [4, 9, 6]. We see carefully designed dashboards as having a lot to oer in the design cycle of educational resources. While the emergence of au- thoring software for e-books is making it easier to create or modify e-books based on their preferences, there is very  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom   2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883943  little work to make this process more evidence-based. Simi- lar to the growing interest in the possible synergies between learning analytics, learning design and teacher inquiry [3] we are observing a need for informing the design and re- design of resources based on empirical data from their usage. While with the advent of data science and analytics in gen- eral, there are several analytical tools that are making their appearance, most are not targeted to educational e-books. Publishers and authors are interested in (and to some extent only have access to) high level information such the number of pages read, average reading times and other details that reveal reading patterns that can correlate with sales figures. However, from an educational point of view teachers, design- ers require a more in-depth analysis of learners interaction with the e-books. The ease of data collection oers unprece- dented opportunities in enabling e-book authors to make informed decisions as to how these books can be improved and serve better their original design goals. The latter is one of the goals of the EU-funded M C Squared project that is investigating the design and use of digital, interactive, cre- ative, mathematics e-books. A key characteristic of these e-books is the inclusion of dynamic, interactive widgets that target creative mathematical thinking and problem solving rather than procedural knowledge [6]. The project engages the members of several so-called Communities of Interest (CoI) across EU in the creation of e-books and parallel re- quirements elicitation and analysis.  This paper presents our approach towards a dashboard and associated visualisations particularly targeted to assist designers to reflect on the use of their e-books. First, in Section 2, we present related work. Section 3 describes our overall design methodology and initial requirements after a series of workshops, sustained online interaction and engage- ment of key members of the CoIs. Section 4 and 5 present our underlying architecture and the dashboard itself. Sec- tion 6 concludes the paper.  2. RELATED WORK There is a growing trend of learning analytics dashboards  for online, face-to-face, and blended learning settings, largely targeted to teachers. As we cannot review them all here, we refer the reader to relevant reviews in the field (e.g. [15]). The closest area of work is that in collaborative or open- ended digital learning environments that demonstrated the potential of tools for increasing awareness, supporting reflec- tion and facilitating decision-making and intervention [10, 5]. GLASS [7] is a web-based modular system that is based on data that follows the CAM schema [17]. It is a versatile  http://dx.doi.org/10.1145/2883851.2883943   solution that can be used with any CAM datasource. It fo- cuses mainly on user activity and the detection of the most common events. A system that is more closely related to our project is eLAT [2]. This is a framework and an im- plementation of a Learning Analytics Platform that is de- signed exclusively for teachers. The aim is to oer teachers opportunities for exploratory data analysis and the ability to evaluate and reflect on teaching practices and interven- tions.  3. METHODOLOGY AND REQUIREMENTS As mentioned in the Introduction, this work is set in the  context of the EU-funded M C Squared project that en- gages several designers across EU in Communities of In- terest that design mathematical e-books. For the develop- ment of the dashboard, we are following a methodology the largely resembles the Learning Awareness Tools User eXpe- rience (LATUX) workflow [10]. Although that paper focuses mostly in the design and deployment of awareness tools in the classroom, the workflow applies in the case of targeting designers as it consists of an iterative process of five stages commonly found in software engineering and user experience approaches  problem definition, low- and higher-fidelity prototyping, pilot studies and validation in-the-wild.  Despite the fact that our focus of attention is e-book de- signers e.g. rather that teachers, through our iterative de- sign process it has become clear that we need to also pay attention to the pedagogical requirements behind the de- sign of the tools as they have potential for classroom use by teachers. Nevertheless, in this paper, we focus on the de- sign, prototyping and early pilot studies targeting our main stakeholders  tech savvy designers (but not necessarily de- velopers) and authors of e-books. in some cases these are teachers. The starting point in this part is the identifica- tion of usage scenarios for the e-books as specified by the CoIs in the project. We first need to take into account that digital resources like e-books are being used either directly in the classroom or in blended learning scenarios (e.g. for practice exercises at home) or in a flipped learning model where students read and interact with the e-book content online (e.g. at home) and complete other parts of the e- book in the classroom with the help of other students or the teacher. So neither context can be excluded. Based on the above usage scenario, in early stages of our design cycle we identified the process of analysing e-book interaction as hav- ing similar characteristics as exploratory data analysis [14].  We also identified the following high-level requirements. Designers should be able to:  1. utilise the service at any time and from anywhere with- out any restrictions and dependencies on technologies and platforms.  2. retrieve, process and analyse data about any chosen period of an e-books lifetime, which can be changed as one is working the dashboard.  3. perform dierent types of analysis of the same data at dierent times throughout a session.  4. analyse an e-book at dierent levels of granularity (book, page, widget, user).  5. go back in time and inspect past data (flashback oper- ations).  It is also worth clarifying that while, in principle, a dash- board like the one we are presenting here could be used by teachers to support their work in the classroom, our focus here is primarily authors (who could, of course, be teach- ers). For previous related work on dashboard for teachers we refer the reader to related work (e.g. [11, 5]). So an addi- tional requirement is being able to monitor the usage of an e-book unit in real-time. Early iterations and feedback on low-fidelity prototypes from members of the CoIs helps us convert these requirements to a specification and eventually to a higher-fidelity prototype that is the version we present in this paper.  4. ARCHITECTURE The analytics platform is designed as an external plug-  gable application that can provide its full functionality in a totally service oriented manner through standardised in- terfaces. It comprises two main parts: The analytics data repository and the dashboard. These two parts are not phys- ically or logically interdependent. The data repository com- prises a dedicated DBMS instance and a set of RESTful web services that can receive, validate and process xAPI mes- sages 1. The data services are optimised to handle dierent types of requests and decompose incoming data in case it is sent as a batch. That implies that the learning platform can optionally implement its own optimisers and take ad- vantage of these optimisations. A simple scenario would be to perform temporary caching whenever possible and send user actions cumulatively as a batch. This is much more ef- ficient than sending each individual action event separately. The following figure (fig. 1) depicts the architecture for that part of the application:  Figure 1: Data Repository  The dashboard communicates directly with the data repos- itory. It constantly checks to see if there is new data recorded and discretely informs the user. It limits the number of data requests through caching and analysis services can process local data and construct visualisations (fig.2).  Integration with the learning platform requires nothing more than a url to be passed along with a set of launch pa- rameters for the dashboard. These parameters are needed so that the dashboard can construct dynamically the struc- ture that represents the learning environment. The assump- tion here is that we always expect to have entities like users  1http://www.adlnet.gov/tla/experience-api    Figure 2: The Dashboard  (students), widgets (activities) and webpages (containers for these activities). This structure has a dual purpose in the system. It can be used to dynamically create the necessary visualisations at the correct level and it can also be used by the authors to navigate themselves through these visualisa- tions during the analysis. The information passed in these parameters is expected to match the identifiers in the user data that will be received from the repository.  4.1 Data Management As mentioned above this is a web-based application de-  signed to process large volumes of data in real-time and de- liver configurable analytics to authors and teachers. This is a service that may need to be utilised both in a synchronous and in an asynchronous manner. In any case it is not known in advance what will be requested by the user. That implies that data preprocessing in the server is not a viable option. Large datasets might have to be transferred, processed and delivered in real-time to the client-end of the application. When requirements like these apply, it is obvious that data management becomes a matter of crucial importance and therefore requires special attention. In order to have the complete picture of what influenced the design decisions, we must also consider the constraints. The constraints follow:  1. The service must be delivered in a distributed manner over the web. That satisfies the first requirement but imposes a problem of potential bandwidth limitations that may aect the ability to transfer large quantities of data in a timely fashion between tiers.  2. The service must be communicated through a web browser without any dependencies on components that are not inherently supported. This is a consequence of 1 that satisfies the second part of the first require- ment but imposes an additional problem. That is the potential memory limitations of the browser and its subsequent inability to store large quantities of data.  3. Another problem related to 2 is the fact that JavaScript engines in browsers follow a single-threaded model. That means that concurrency and its respective per- formance gains are typically not possible. Fortunately, HTML5 oers the ability to distribute processing through web workers.  4. The data processing cannot be performed in the server. All the requirements apart from 1 (especially 7) con- verge on that.  Considering all the above, the decision was to create a web-based platform with a sophisticated data management sub-component that oers the following:  1. It provides data caching capabilities. It maintains data in local JavaScript databases. Data is synchronised with the source in an asynchronous mode using Ajax. That guarantees that this operation is a non-blocking process in case it takes a noticeable time to complete.  2. It provides the ability for incremental updates. If more data is needed (or less), it is not required to download the entire dataset again but only synchronise the miss- ing parts.  3. It is discreet enough to inform the author about the availability of new data without interrupting what is currently being processed.  4. It oers a clear distinction between synchronisation, analysis and presentation. That helps the user operate the system in a more ecient way.  5. It oers the option to process dierent time ranges within the range the cached data covers. That means that if all the required data has been downloaded, the rest of the session can be completed in a purely dis- connected mode.  It is obvious that the main objectives here were to keep the amount of data to transfer and the number of round-trips to the server to the minimum so that we can utilise in the most ecient way the available bandwidth. The user is able to perform as many operations as needed on the local data without incurring additional network trac and workload to the data services. This connection-less approach makes the application more scalable, since the data services are able to process more requests, and more responsive, since all the processing takes place at the client side. The au- thors are given full control over what is synchronised and processed. All that is needed is sensible decisions and care- ful handling. The application provides all the information about the amount of data that is available. It also provides the ability to select a time range that corresponds to the period that needs to be analysed. The authors must have a certain degree of IT literacy so that they can understand the limitations of the system and use it responsibly.  4.2 Distinguishing Features RDAP is loosely coupled with the learning environment  it relates to. Communication takes place through the stan- dardised xAPI specification that looks a lot like ActivityS- treams 2 but allows more flexibility in the structure and the definition of verbs. It also allows the inclusion of widget- specific data that may follow totally dierent data mod- els. That provides flexibility without compromising diver- sity. Another dierence is in RDAP the primary focus is to enable the author of the material to revisit the initial design and use the feedback from the dashboard to verify the ex- tend to which the objectives have been met. In this process the author is expected to identify flaws in the design that had as an eect the appearance of unexpected patterns in students behaviour. Finally, the tree-like dynamic struc- ture of the learning platform in RDAP provides the ability  2http://www.w3.org/TR/activitystreams-core/    to easily navigate in a random manner between dierent lev- els of specificity and thus perform exploratory data analysis with minimal cognitive overhead.  5. THE DASHBOARD The dashboard is initially empty. There is no data that  can be used for analysis and visualisation. The only infor- mation that is available is the structure of the e-book and the timestamps that define the start and the end of the time period recorded in the analytics data repository. The avail- able controls that can be used for parameterisation and ex- ecution of commands are organised in areas called ribbons. There are currently three ribbons available in the applica- tion (Toolbar, Configuration and Event Log). The figure below displays the configuration ribbon. This ribbon hosts controls that can be used primarily for data-related settings and operations. The green area in the data range part is a special slider control that is equipped with two handles. The entire area covered by the control corresponds to the available data in the server. The two text fields above the slider display the starting and ending dates of this period and the text in the green area displays the duration. If at the same time the e-book under investigation is being used by students, the tool gets automatically updated with the changes. If the author wants to analyse a smaller period than that the handles can be used to adjust the starting and ending dates.  Figure 3: Data Configuration  As the user adjusts the data range period, the display range period gets automatically updated. The display range corresponds to the data that will eventually be analysed and display results. After the adjustments the author can press the synchronize button to start the data synchronisation process. After the completion of this operation the new data is stored in the local databases and becomes available. The next thing to do is to select which part of this data needs to be analysed using the display range slider and press dis- play. After the analysis is completed the visualisations are displayed in the dashboard. The tool bar ribbon can be used for further analysis of existing data. The inspection part of the ribbon hosts another slider that can be used for flash- back operations. This slider is initially empty and inactive. The first time it gets activated is when the display button is pressed and a successful analysis completes. When that happens it takes the time period of the currently selected display range. As the slider moves back and forth the au- thor can see immediate changes in the visualisations. The changes are so fast that appear like animations to the human eye. If step-by-step flashback is needed then the dropdown list and the respective buttons in the inspection part must be used.  5.1 The Visualisations The available visualisations are categorised and presented  in three tabs: Widgets, Users and Usage. The first tab focuses on the structure of the learning environment. On  the left we can see the structure of the unit (fig. 4). In this case the e-book consists of three pages each one of which contains two widgets. The nodes in the tree are selectable. The author can use them to navigate to dierent levels of the e-book and display the respective visualisations.  Figure 4: Multi-level Analysis. The structure of the e-book is shown on the left and the user can select nodes to see the corresponding data visualisations.  The third tab focuses on widget usage but from a dierent perspective. It shows in the same graph how the users relate to widgets in terms of intensity of usage. A cross-tabulation table is used to present this information. The intersection of a row and a column shows information about how intensely a particular widget is used by a particular user. The intensity of the colour in the box corresponds to the proportion of indicators generated by that user for this particular widget in relation to the total number of indicators for this widget. Widgets may be given dierent colors depending on how heavily they are used. If the total number of indicators for a widget is greater than the average activity per widget then the respective column is displayed in hues of green. If it is more than 80% of the average it is displayed in hues of blue and if it is less than that it is displayed in hues of red. If there are no indicators at all the column is displayed in white color.  Figure 5: Widget Usage  These three tabs present data from dierent perspectives. There is often an overlap between them and two or more vi- sualisations can complement each other and provide a view that is more representative of what really happens in the classroom. That implies that if there are ties between them wherever possible, that may support a more exploratory type of data analysis, which is desirable. An example of that approach can be seen on the way the Usage tab is linked to the Widgets. If the author identifies something in the former that requires further investigation, she can click on the header of the table column (widget) and move directly to the page that displays the set of more detailed    visualisations that correspond to this particular widget.  6. CONCLUSION This paper presented the Reflective Designer Analytics  Platform (RDAP) that helps learning material authors re- flect on their designs and how they meet their original ob- jectives. A prototype was designed and tested with the MC Squared learning platform that utilises interactive e-books that include dynamic widgets aimed at enhancing students mathematical problem solving and creativity. RDAP is de- signed to operate as a standalone, platform-independent ap- plication that communicates with partner systems through standardised interfaces and data formats. It features a highly ecient data management mechanism that enables incre- mental synchronisation of data and disconnected operation at the client side. This eliminates server bottlenecks, pre- vents excessive network load, increases the analytical ca- pacity of the tool and delivers the results through a highly responsive user interface. From an analytical viewpoint the strong points of the system are the ability to analyse data from diverse and dissimilar widgets (learning activities) and the ability to switch between dierent levels of specificity with ease (shallow, deep analysis). The latter is especially advantageous in exploratory data analysis.  The system has been thoroughly tested for usability and analytic potential. Preliminary evaluation results provided us with positive feedback that feeds into discussions of new features and modifications. One key part we need to pay attention to is cognitive overload and therefore allowing the configuration of the dashboard according to users prefer- ence seems to be important. As mentioned, our focus here has been on designers conducting retrospective data analysis but, technically, the system can be used for real time mon- itoring and therefore we plan to extend it with appropriate visualisations to support teachers as well. We are, therefore, at the time of this writing testing and improving scaleabil- ity issues. Finally, because of the extensibility of the system we can easily include new visualisations and plan to include more advanced algorithms to help analysts identify impor- tant events or patterns that are worth exploring further.  ACKNOWLEDGEMENTS The M C Squared project has received funding from the European Union Seventh Framework Programme (FP7/2007- 2013) under grant agreement N610467.  7. REFERENCES [1] P. Bradford, M. Porciello, N. Balkon, and D. Backus.  The blackboard learning system: The be all and end all in educational instruction Journal of Educational Technology Systems, 35(3):301314, 2007.  [2] A. L. Dyckho, D. Zielke, M. Bultmann, M. A. Chatti, and U. Schroeder. Design and implementation of a learning analytics toolkit for teachers. Journal of Educational Technology & Society, 15(3):5876, 2012.  [3] V. Emin-Martinez, H. Cecilie, R. Triana, M. Jesus, W. Barbara, M. Yishay;, D. Mihai, F. Rebecca, and J.-P. Pernin. Towards teacher-led design inquiry of learning. 2014.  [4] E. Gardiner and R. Musto. The Oxford companion to the book, chapter The electronic book. Oxford University Press, Oxford, 2010.  [5] S. Gutierrez-Santos, E. Geraniou, D. Pearce-Lazard, and A. Poulovassilis. Design of Teacher Assistance Tools in an Exploratory Learning Environment for Algebraic Generalization. IEEE Transactions on Learning Technologies, 5(4):366376, 2012.  [6] C. Kynigos. Designing constructionist e-books: New mediations for creative mathematical thinking Constructivist Foundations, 10(3):305313, 2015.  [7] D. Leony, A. Pardo, L. de la Fuente Valentn, D. S. de Castro, and C. D. Kloos. Glass: a learning analytics visualization tool. In Proceedings of the 2nd international conference on learning analytics and knowledge, pages 162163. ACM, 2012.  [8] J. Lewis. Usability testing. In Handbook of Human Factors and Ergonomics, pages 12751316. John Wiley, New York, NY, 2006.  [9] R. Martin. The road ahead: ebooks, etextbooks and publishers electronic resources. In M. Brown, M. Hartnett, and T. Stewart, editors, Future Challenges, Sustainable futures: Proceedings ASCILITE 2012.  [10] R. Martinez-Maldonado, A. Pardo, N. Mirriahi, K. Yacef, J. Kay, and A. Clayphan. The latux workflow: designing and deploying awareness tools in technology-enabled learning settings. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 110. ACM, 2015.  [11] Noss et al. The design of a system to support exploratory learning of algebraic generalisation. Computers & Education, 59(1):6381, 2012.  [12] J. A. Ruiperez-Valiente, P. J. Munoz-Merino, D. Leony, and C. D. Kloos. Alas-ka: A learning analytics extension for better understanding the learning process in the khan academy platform. Computers in Human Behavior, 47:139148, 2015.  [13] M. Scheel, H. Drachsler, and M. Specht. Developing an evaluation framework of quality indicators for learning analytics. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 1620. ACM, 2015.  [14] J. W. Tukey. Exploratory Data Analysis. Addison-Wesley, 1977.  [15] K. Verbert, S. Govaerts, E. Duval, J. L. Santos, F. Van Assche, G. Parra, and J. Klerkx. Learning dashboards: an overview and future research opportunities. Personal and Ubiquitous Computing, 18(6):14991514, 2014.  [16] A. Vozniuk, S. Govaerts, and D. Gillet. Towards portable learning analytics dashboards. In Advanced Learning Technologies (ICALT), 2013 IEEE 13th International Conference on, pages 412416. IEEE, 2013.  [17] M. Wolpers, J. Najjar, K. Verbert, and E. Duval. Tracking actual usage: the attention metadata approach. Journal of Educational Technology & Society, 10(3):106121, 2007.  [18] H. Zhang, K. Almeroth, A. Knight, M. Bulger, and R. Mayer. Moodog: Tracking students online learning activities. In World Conference on Educational Multimedia, Hypermedia and Telecommunications, volume 2007, pages 44154422, 2007.    Introduction  Related Work  Methodology and Requirements  Architecture  Data Management  Distinguishing Features   The Dashboard  The Visualisations   Conclusion  References   "}
{"index":{"_id":"19"}}
{"datatype":"inproceedings","key":"Prieto:2016:TAT:2883851.2883927","author":"Prieto, Luis P. and Sharma, Kshitij and Dillenbourg, Pierre and Jes'us, Mar'ia","title":"Teaching Analytics: Towards Automatic Extraction of Orchestration Graphs Using Wearable Sensors","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"148--157","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883927","doi":"10.1145/2883851.2883927","acmid":"2883927","publisher":"ACM","address":"New York, NY, USA","keywords":"activity detection, multimodal learning analytics, teacher reflection, teaching analytics, wearable sensors","abstract":"'Teaching analytics' is the application of learning analytics techniques to understand teaching and learning processes, and eventually enable supportive interventions. However, in the case of (often, half-improvised) teaching in face-to-face classrooms, such interventions would require first an understanding of what the teacher actually did, as the starting point for teacher reflection and inquiry. Currently, such teacher enactment characterization requires costly manual coding by researchers. This paper presents a case study exploring the potential of machine learning techniques to automatically extract teaching actions during classroom enactment, from five data sources collected using wearable sensors (eye-tracking, EEG, accelerometer, audio and video). Our results highlight the feasibility of this approach, with high levels of accuracy in determining the social plane of interaction (90%, ?","pdf":"Teaching Analytics: Towards Automatic Extraction of Orchestration Graphs Using Wearable Sensors  Luis P. Prieto CHILI Lab, EPFL  RLC D1 740, Station 20 1015 Lausanne, Switzerland  luis.prieto@epfl.ch  Kshitij Sharma CHILI Lab, EPFL  RLC D1 740, Station 20 1015 Lausanne, Switzerland kshitij.sharma@epfl.ch  Pierre Dillenbourg CHILI Lab, EPFL  RLC D1 740, Station 20 1015 Lausanne, Switzerland  pierre.dillenbourg@epfl.ch Mara Jess  Rodrguez-Triana REACT Lab, EPFL  ME A3 30, Station 9 1015 Lausanne, Switzerland  maria.rodrigueztriana@epfl.ch  ABSTRACT Teaching analytics is the application of learning analytics techniques to understand teaching and learning processes, and eventually enable supportive interventions. However, in the case of (often, half-improvised) teaching in face-to-face classrooms, such interventions would require first an under- standing of what the teacher actually did, as the starting point for teacher reflection and inquiry. Currently, such teacher enactment characterization requires costly manual coding by researchers. This paper presents a case study exploring the potential of machine learning techniques to automatically extract teaching actions during classroom en- actment, from five data sources collected using wearable sen- sors (eye-tracking, EEG, accelerometer, audio and video). Our results highlight the feasibility of this approach, with high levels of accuracy in determining the social plane of interaction (90%, =0.8). The reliable detection of con- crete teaching activity (e.g., explanation vs. questioning) accurately still remains challenging (67%, =0.56), a fact that will prompt further research on multimodal features and models for teaching activity extraction, as well as the collection of a larger multimodal dataset to improve the ac- curacy and generalizability of these methods.  Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in EducationCollaborative learning ; J.1 [Computer Appli- cations]: Administrative Data ProcessingEducation  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883927  Keywords Teaching analytics, Multimodal learning analytics, Activity detection, Wearable sensors, Teacher reflection  1. INTRODUCTION Aiding educators in understanding and improving teach-  ing and learning processes is one of the main aims of learn- ing analytics [41]. Such teacher-oriented learning analytics eorts, exemplified by tools such as eLAT [13] or LOCO- Analyst [23], so far have focused mostly on online or blended learning scenarios, using almost exclusively the traces and information available in such digital platforms. There is, however, an emergent trend within the community that also looks into capturing and modelling the physical interactions that make up the learning process in face-to-face situations, using a variety of data sources to complement the usual dig- ital traces (multimodal learning analytics  MMLA [29])  These eorts into supporting teaching practice through analytics (also known as teaching analytics [1, 44, 45]) are often portrayed as a cycle involving the gathering of data from the learning situation, analyzing it and performing an intervention as a result of the new understanding of the situ- ation [14, 5]. Indeed, most of the existing eorts in teaching analytics focus on the first steps of this cycle, namely, the data gathering, analysis and visualization of learning pro- cesses (maybe due to the recent emergence of this commu- nity).  However, the crucial step of supporting teacher interven- tions based on learning analytics insights remains under- supported, starting from its very first step: knowing what the initial state of the teaching practice was, so as to know what needs to be changed. In the case of online learning, the teacher plan (either explicit or reified into the learning environment) is normally available, and the lack of enact- ment flexibility of most currently-used platforms somehow guarantees that such plan was executed. However, in face- to-face (or blended) learning, even if the lesson plan is avail- able, how do we know what (and how) the teacher actually did during the lesson, so that we can know how to intervene to improve it  To answer this question, we need to go beyond technical,  http://dx.doi.org/10.1145/2883851.2883927   low-level measures that are often hard to interpret, towards more sense-making indicators [40], i.e., indicators that have a pedagogical meaning. The way this is normally achieved in research is through manual video coding by a human (see, e.g., [33]), which is costly in terms of time and eort.  Therefore, bringing together teaching analytics and MMLA, the overall question we try to explore in this paper is: can we automatically characterize teaching practice in a face-to-face situation, in pedagogical terms Such automated character- ization can be crucial to the wide and scalable application of learning analytics to teacher reflection and inquiry, an area of application of increasing importance for this community [26, 31].  To explore this question, the paper presents a case study in which we use data from multiple wearable sensors (includ- ing accelerometers, EEG or eye-trackers) and machine learn- ing techniques to automatically characterize the teacher ac- tivity and the social plane of interaction of one teacher across 4 sessions of collaborative learning with primary school stu- dents. The next section introduces the main related work in teaching and multimodal learning analytics; later on, we outline how the general research question above has been operationalized, in the form of automatically generating an orchestration graph of the teacher enactment. Afterwards, the context, methods, analyses and results of our case study are described, we discuss its main implications and limita- tions, and we outline the most likely avenues for further research opened by this study.  2. RELATED WORK  2.1 Teaching Analytics Within learning analytics general aim of improving our  understanding of teaching and learning [43] and optimising learning and the environments in which it occurs [16], teach- ing analytics is conceived as a sub-field that focuses on the design, development, evaluation of visual analytics methods and tools for teachers, to understand learning and teaching processes [45]. This particular use of learning analytics is of- ten mentioned in connection with teacher inquiry (or teacher reflection) processes [26, 31].  Although the previous definition of teaching analytics con- siders both learning and teaching processes (and products) as subjects of analysis, so far teaching analytics research has been mostly focusing on analyzing student learning/behavior, and providing feedback to the teacher (see [48, 47]). Thus, many of these research works depict learning analytics for teachers rather than teaching analytics in a literal sense. Although the focus on student learning is certainly needed (as student learning is the main goal of any educational sce- nario), a complementary teacher-oriented view is also neces- sary to understand how and why some of the student learn- ing processes take place, and assess the most adequate in- tervention.  Interestingly, most of the works in teaching analytics (both theoretical and implementations), that consider the teach- ing side, do so through the inclusion of teacher-generated artifacts, especially the teachers plan for the lesson. This plan can be either explicit, as it often happens in proposals that combine learning design (LD) approaches and learning analytics [38, 21, 25, 15, 22, 42] or implicit in the resources and structure of the learning environment (e.g., in [36]).  However, how do we know whether the design of the les-  son was actually followed, or what events not specified there could have an influence in the learning process Despite the fact that teaching practice (especially, the face-to-face class- room) has often been seen as improvisational [39], few works attempt the characterization of the actual enactment of the lesson, often in very specific episodes: examining teachers tool usage patterns (e.g., while using an educational digital library tool [51]); through explicit audience-provided feed- back during lectures [37]; or through the visual analysis of the reasoning behind expert teacher assessments [19].  As it often happens elsewhere in learning analytics, most of current teaching analytics research is restricted to the analysis of easily-accessible data from digital platforms, thus creating a certain streetlight eect [17] (in this context, an- alyzing learning only in places where there is an abundance of data, even if it is not the where most of the learning actu- ally occurs). To address this well-known limitation of tradi- tional learning analytics approaches (especially, in face-to- face learning scenarios), there is an emergent trend of com- plementing the easily-available digital traces from learning platforms with other data captured from the physical world: multimodal learning analytics.  2.2 Multimodal Learning and Teaching Ana- lytics  A hidden assumption present in much of learning ana- lytics research is that it is about the usage of pre-existing, machine-readable data [16], very often in the context of on- line learning. Noticing this blind spot, along with the re- alization that all learning is, almost by definition, blended [30], always containing some amount of physical embodi- ment (even if subdued by computer-mediated interaction), has prompted the proposal of multimodal approaches to learning analytics (MMLA, see [27, 29]). This flavor of learn- ing analytics is used to aid in understanding and supporting more free-form, creative learning activities that are not as constrained as online ones [3], or that are more process- oriented in nature (such as project-based learning [49]).  Typical examples of MMLA include Worsley & Bliksteins work to understand creative construction activities using hu- man annotations, speech, gesture and electro-dermal activa- tion data [50]. Working on co-located, collaborative problem solving, Ochoa et al. [28] used video, audio and pen stroke information, extracting simple features to discriminate be- tween experts and non-experts. More recently, other authors have proposed Feeler [12], a system that uses EEG in con- junction with application logs to promote student reflection about learning.  This kind of multimodal approaches to analyze learning processes in the physical world are not yet widespread in the sub-area of teaching analytics. Isolated examples include the iKlassroom conceptual proposal [46], which features a map of the classroom to help contextualize the real-time data about the learners in a lecture; Also in the context of univer- sity lectures, Raca and Dillenbourg [35] take an unobtrusive computer vision approach to assess student attention from their posture and other behavioral cues. Again, we can see a larger focus on modelling student actions and information, and a dearth of studies that characterize teacher practice in the classroom using such multimodal approaches.  There exists, nonetheless, a wealth of research in the field of sensors (especially using inertial sensors such as accelerom- eters), very often applied to the fields of health and enter-    tainment [32]. However, the large majority of the initia- tives in this field target low-level, physical activities such as walking, running, sitting, etc. Such activities are unlikely to prompt interesting reflection from a teacher: we need novel ways of characterizing teacher practice in terms that make sense [40] for teaching practitioners. Only very recently, re- searchers in education are starting to turn to higher-level fea- tures and the modelling of pedagogically-meaningful interac- tions [2]; these eorts, however, are still confined to particu- lar kinds of classroom episodes and pedagogical approaches (e.g., question turns in dialogic learning), and many chal- lenges of the data gathering setup are in the process of be- ing tackled (e.g., for accurate automated speech recognition [10]).  3. OPERATIONALIZING TEACHER PRAC- TICE: ORCHESTRATION  From the related work outlined above, we see that teach- ing analytics that look at actual (blended or face-to-face) teacher enactment of learning situations is an essential miss- ing piece of support for the teacher reflection/inquiry cy- cle. However, general-purpose physical activities commonly used in wearable sensors (like walking and sitting) are bound to have little significance for a teacher. Hence, we need a generic but still pedagogically-meaningful way of character- izing the variety of pedagogical approaches that often co- exist in everyday classroom practice.  One potential way of characterizing teacher practice (es- pecially when using technology) is that of orchestration, de- fined as the process of productively coordinating support- ive interventions across multiple learning activities occurring at multiple social levels [8]. In line with this definition of teaching practice as orchestration, graphical and computa- tional representations of the orchestration of a lesson can be made (what may be called orchestration graphs [7]). This kind of graph, representing time horizontally and so- cial plane (individual, group, or whole-class) vertically, can be used to model the student learner activities, but also the teachers supporting actions (such as explanation, monitor- ing, repairs, etc.) [34], and have been used extensively in computer-supported collaborative learning (CSCL), both to express the teacher plan (or script) [9] and the improvised actions during enactment [34].  So far, these kinds of representations are being generated post-hoc by researchers, on the basis of observations or the manual coding of videos of the lesson (see figure 1, middle). This kind of process, if done in a detailed manner, is very time-consuming, and the time required to do such analyses makes the feedback cycle of teacher practice and reflection unnecessarily long. Indeed, automated characterization of teacher practice along the lines of these orchestration graphs could be a great enabler for teacher inquiry processes in conjunction with other teaching analytics more focused on the learner.  With these elements in mind, and taking into account that other eorts are already looking into the multimodal characterization of student activities in physical classrooms [11, 50], we can operationalize our general research question about automatically characterizing teaching practice, into a more concrete one: can we use multimodal teaching analyt- ics to extract automatically the (teacher-side) orchestration graph of the enactment of a lesson. Towards this aim, we  set ourselves to: 1) explore dierent sensors and modalities (as well as dierent features extracted from them) in order to assess their predictive power to build the teaching ac- tivity and social plane of the orchestration graph; and 2) generate predictive models that use those data sources and features to automatically characterize the teaching activity and social plane of a lessons enactment. Below, we present an exploratory case study in which data from five dierent modalities are used to characterize the teaching activities and social planes of interaction of a single teacher, across four sessions with primary school students.  4. CASE STUDY  4.1 Context The data for our study was gathered during an open doors  day at our lab, in which entire classes of primary students from nearby schools are shown novel educational technolo- gies. In this case, the visits were structured as simulated math lessons in a room equipped as a multi-tabletop class- room (see figure 2). Four sessions of 35-40 min were held with four dierent cohorts of 19-21 students per session, in which a researcher (wearing a number of sensors) acted as the main teacher-facilitator of the session. In two of the sessions the researcher had an assistant (to provide more variety in orchestration load in the otherwise very similar situations), and in all of the sessions 1-2 of the usual school teachers accompanied the children, acting as observers.  The four sessions had a similar lesson plan (see also fig- ure 1) and similar usages of classroom technology, including the use of tangible, paper-based geometry exercises to be solved in small groups, and a whole-class collaborative and competitive game that used those same tabletops and the classroom projector, based around the same geometrical no- tions (rotation, translation, coordinate systems). Although the general plan for the sessions was the same (alternat- ing phases of small-group student work and whole-class syn- chronization points to keep the whole class engaged), the activities and social interaction for each of the sessions were left fluid, improvised over the skeleton of the lesson plan (as it often happens in everyday teaching practice in primary schools). More concretely, the activities were organized in the following coarse sequence: 1) Explanation of the activity and technology involved; 2) Questioning of students about the mathematical concepts to be seen (to have an idea of their level of prior knowledge); 3) In small groups, use the tabletops to solve very basic geometry exercises, in order to get familiar with the concepts and technology presented; 4) Play a whole-class game in which students first collab- orate in small groups to rotate/translate geometric figures, and then there is a whole-class resolution phase to see which team better protected their area; 5) The teacher does a final round of questions to assess the students new understanding of the concepts.  4.2 Data Gathering and Feature Extraction During the four sessions described above, the main teacher/  facilitator wore several sensors in order to capture relevant teaching practice data (taking into account that teaching has an important cognitive aspect, but also a physical one): a single-electrode, portable electroencephalogram (EEG) de- vice, mobile eye-tracker googgles (which recorded not only the eye movements, but also a subjective video+audio stream),    Figure 1: Example orchestration graphs, including the representation of the intended lesson plan (top), the actual teaching enactment as coded by a human researcher (middle), and the orchestration graph predicted by the best-performing models of our study (bottom)  Figure 2: Partial view of the classroom in one of the case study sessions. In the center, the teacher wearing the eye-tracking and EEG devices  plus a smartphone located in his pocket, set to record 3- axis accelerometer data as he moved around the classroom. From these sensors, five dierent data streams were con- sidered: 1) Eye-tracking variables (e.g., saccades, fixations, pupil size); 2) EEG data (including the raw electrode read- ing, the usual EEG bands, attention, etc.); 3) Accelerometer readings; 4) Subjective video feed depicting the field of view of the teacher (taken from the eye-trackers camera); and 5) Subjective audio feed (also obtained from the eye-tracker recording).  From these five data streams, up to 144 features were extracted (see Table 1). In general, we explored generic, relatively simple features used for multiple purposes (e.g., simple face detection in the video, audio energy or envelope, general fixation/saccade features), rather than going into more advanced, expensive techniques like speech segmenta- tion and recognition, or the definition of areas of interest, visual object recognition, etc.  Given the disparity of sampling rates of the dierent de- vices, each data stream was divided into equal-length, rolling windows of 10 seconds, using a 5s. slide. Then, the dierent features were calculated for each 10s window1 (e.g., by av- erages, deviations, maximum values, etc.), a technique com- monly used in the activity detection field [32]. The feature extraction and data analysis pipeline was developed using SMIs BeGaze software (for eye-tracking feature extraction), MatlabR for audio processing (including the Matlab Audio Analysis Library [20]), and the DLib2 library to perform ba- sic face detection on the video stream. Then, the data from all the features were joined and analyzed using R.  The subjective audio/video stream has then been man- ually coded by a human researcher, assigning to each 10s window a value for the teaching activity being done (ex- planation, questioning, monitoring, repairs or task distribu- tion/transition), as well as the social plane of the teachers interaction at that moment (individual, group or whole- class). The dierent machine learning models described in the following section have been trained and validated against this ground truth.  Given the limits of this multimodal dataset (with only 1 subject, over 4 sessions), the models have been trained over the data of 3 of the sessions, with 25 iterations of bootstrap resampling, in order to tune the model parameters and have a first estimation of in-session performance. Finally, the models have been tested against the data of the remaining session, to give a more realistic estimation of the perfor- mance of the model when predicting about data from a ses- sion that the algorithm has never been trained against. This  1During exploratory data analysis, other window lengths (e.g., one second) were also used, with similar or worse pre- dictive performance. 2http://dlib.net/  http://dlib.net/   Data sources Eye-tracking EEG Accelerometers (Subjective) Video (Subjective) Audio  Pupil diameter (+sd) Electrode X value (+sd) Image blurriness Zero-cross rate Nr. long fixations Attention Y value (+sd) (+sd,median,max) Energy Saccade speed Meditation Z value (+sd) Nr. blurry frames Energy entropy  Fixation duration Delta band Jerk (+sd) Nr. blurry episodes Spectral centroid Fixation dispersion Theta band Jerk FFT Length blurry episodes Spectral spread Saccade duration Low Alpha band (30 coefs.) (+sd,median,max) Spectral entropy Saccade amplitude High Alpha band Length clear episodes Spectral flux Saccade length Low Beta band (+sd,median,max) Spectral rollo Saccade velocity High Beta band Nr. faces per frame Mel-Frequency Cepstrum  Low Gamma band (+sd,median,max) (MFCC, 13 coefs.) Mid Gamma band Nr. of frames with faces Harmonic ratio Blink strength Nr. of face episodes Fundamental frequency  Length of face episodes Chroma vector (12 coefs.) (+sd,median,max) Auto-correlation (6 coefs.)  Length of face-free episodes Envelope (+sd) (+sd,median,max) Envelope skew, kurtosis  Faces per clear episode Linear Predictor (+sd) coecients (6 coefs.)  Line Spectral Frequency coecients (6 coefs.)  Table 1: Overview of the features extracted for the dierent data sources in the study. In general, averages of the value over a 10s window are taken; (+sd,median,max) denotes that standard deviations, medians and maximum values were also extracted  process has been repeated holding up for testing each one of the sessions, and then averaging the performance in this out of session testing. For model comparison, the Kappa () statistic has been used, as it considers not only the accu- racy of the model, but also how much better than a random predictor it is.  4.3 Results: Predicting Teaching Activity Given our first goal of trying to understand the predic-  tive value of each data source and its features (in this case, for detecting whether the teacher was explaining, vs. mon- itoring the work of students), we first tried predicting the teaching activity using data from a single data source at a time. During our prior exploratory analyses with dier- ent machine learning models from the families commonly used for classification problems, we had found that random forests (RF, [4]) performed the best (or close to the best) in almost all combinations of data in this dataset. The re- sults of this mono-modal prediction using random forests are shown at the beginning of table 2. We can see that the eye-tracking, audio and video streams perform much better than EEG and accelerometer data, from which we get not much better accuracy than if we selected randomly. Audio features performs the best out of the five data sources, but still the predictions are not very accurate (55.9%, =0.41).  To understand the value of having such a rich multimodal dataset, we also trained a random forest on our whole dataset and features. We can see that the accuracy of this full mul- timodal prediction is larger, but the performance on out- of-session episodes is still not very high (63.8%, =0.52). However, in such a large set of features, it is clear that not all of them will be equally predictive, and many will be just noise. In order to understand which variables are the most informative (with the hope of increasing the performance of our teaching activity extraction), we have triangulated among two methods: a) We have calculated the eect size  of each of the features with respect to teaching activity [6], in order to find those with the highest distinguishablity; b) We computed the permutation variable importance of each fea- ture in the random forest (which estimates which variables are most important over all the decision trees of the RF). The results of this ranking, in table 3 (left), show that the pupil diameter mean size, taken from the eye-tracker, is the most important variable according to both measures. Aside from this, we can also note that the simple face detection features we calculated (e.g., the maximum number of faces that appeared in a frame of the episode) also feature among the top variables in distinguishing among dierent teacher activities. The fact that also a few audio features made it into this list (e.g., envelope skewness, which captures the overall asymmetry of the wave, and can be related with the teachers voice volume), highlights the value of having mul- timodal data in order to predict not-so-obvious, higher-level activities such as these.  An important issue to note is that in all the aforemen- tioned models, each episode is used for training and predic- tion in an independent manner (regardless of their order and position in time). However, teaching activities (as well as learning activities) form sequences over time, the same way that lesson plans are normally structured as sequences. As a first attempt to exploit the temporal structure of the sessions and the activities, we developed another random forest pre- dictor using the most important multimodal features (which itself gave a slight increase in performance). Then, we cor- rected these RF prediction probabilities (i.e., how likely we are to be in a certain teaching activity, given the current in- put features) with those of the transitions between activities from a 10s window to the next (by building a discrete-time Markov chain DTMC from the training data). This mul- timodal Markov-improved random forest predictor gave us a slight increase in accuracy (up to =0.56, see the last row    In-session perf. Out-of-session perf. Data source Features Best model Accuracy  Accuracy   Eye-tracking only All Random Forest 50.2% 0.34 45.7% 0.28 EEG only All Random Forest 34.1% 0.11 29% 0.06  Accelerometer only All Random Forest 44% 0.25 31% 0.09 Audio only All Random Forest 58.2% 0.45 55.9% 0.41 Video only All Random Forest 50.8% 0.35 45.7% 0.28  All All Random Forest 67.4% 0.57 63.2% 0.51 Audio+video All (Random Forest) 64.2% 0.53 61.7% 0.49  All Top 7 (SVM) 61% 0.49 All Top 80 RF+Markov Chain 67.3% 0.56  Table 2: Performance of dierent models, data sources and features in predicting teacher activities  in table 2), but still did not manage to make the prediction overly reliable.  Another aspect worth exploring in such multimodal anal- yses, which often feature hundreds of dierent features from dierent data sources, is the cost-benefit analysis of: a) gath- ering such multimodal data (e.g., the cost of the device itself, and the eort of setting it up for recording a session); and b) the computational cost (in terms of time and computational power) required to pre-process, analyze and predict the mul- timodal data. In order to explore these aspects, we show in table 2 two additional predictors and their performance: we can see that a simpler random forest predictor using only the audio and video streams from the subjective head-mounted camera, already provides a performance that is not so dif- ferent from that of the full multimodal dataset (without the Markov correction). On the other hand, a much faster and simpler model, using a support vector machine (SVM) and only the top seven variables in terms of importance, also provides a similar level of performance (accuracy of 61%, =0.49).  4.4 Results: Predicting Social Plane of Inter- action  In general, we have followed the same sequence of analyses and modelling as described above for the automated extrac- tion of teacher activities. In this case, however, we are trying to discriminate between the moments in which the teacher is interacting individually, in small group or with the whole classroom of students. Given that the lesson plan of the ses- sions (and their actual enactment) included very few inter- actions at the individual level, in the analyses below we will try to distinguish only among two social planes, small group and whole-class. Again, we have used the  statistic when predicting out-of-session (i.e., on data of a session the model has not been trained with) as the main yardstick to com- pare performance of the predictive models. In this case, our exploratory modelling concluded that generalized boosted models (GBM, concretely, stochastic gradient-boosted deci- sion trees [18]) performed better, and is thus used below for comparison purposes.  The performance of predictive models based on a single data source can be found at the beginning of table Table 4. There, we can see that again the predictive models based on eye-tracking, audio and video data perform much better than those based on EEG or accelerometer data. In this case, surprisingly, eye-tracking features perform the best out of the five data sources (achieving already 86.1% accuracy, =0.72).  Regarding the added value of having amultimodal dataset, a GBM model fed with all the features from the five data sources achieved better performance than the eye-tracking- only model (89.6% accuracy, =0.79). To understand which of the features in our multimodal dataset might have most predictive potential for the social level of interaction, we again triangulated among the eect size calculation of each features (to assess distinguishability) and the variable im- portance that can be extracted from the multimodal GBM predictor. In table 3 (right) we can see that, again, the pupil diameter mean is the most important feature by all accounts. The rest of this feature ranking is dominated by video-based features, both based on face-detection, as well as those based in blur of the image (indicating teacher moving the head and, hence, the field of view). Using the variables from this rank- ing to achieve a more ecient multimodal predictor aorded only marginal benefits over the full-featured one (89.9% ac- curacy, =0.8, using the first 81 variables). In the case of predicting social level of interaction, our attempts of incor- porating the time structure of the session through Markov chains did not provide any additional increase in accuracy.  Regarding the cost-benefit (or simplicity vs. value) trade- os when building these multimodal predictors, again we found that a predictor based in the audiovisual information only (see table 4) provided a successful extraction of the social level around 85% of the time (=0.69), comparable to the one based on eye-tracking, and not much worse than the most accurate model. Similarly, a much simpler and faster (but still multimodal) support vector machine based on the top five features of our ranking (table 3, right) obtains an accuracy comparable to that of the full dataset (88.2%, =0.76).  5. DISCUSSION The results presented above provide a first exploration of  the multimodal data streams and the feature space avail- able for researchers in teacher analytics, in order to achieve an automated characterization of teacher activity in ped- agogical terms (rather than just physical ones), and show how reasonable accuracy can be achieved by using machine learning techniques, even with such simple and generic fea- tures. Indeed, our models were able to distinguish between two dierent social planes of interaction, close to 90% of the time, and the resulting orchestration graph so generated looked remarkably similar to the actual one, as coded by a human (see figure 1, bottom) and, certainly, was closer to what happened in the classroom than just using the lesson plan as a proxy.    Teacher activity Social plane of interaction Feature Rank ES Rank RF Feature Rank ES Rank GBM  Avg. pupil diameter (ET) 1 1 Avg. pupil diameter (ET) 1 1 Max. nr. faces/frame (VD) 2 4 Std. dev. blur in frame (VD) 2 3  Std. dev. faces per frame (VD) 3  Max. nr. faces/frame (VD) 3 5 Envelope skewness (AU) 4 2 Max. len. episode w/o faces (VD) 4 14  Std. dev. faces/clear frame (VD) 5 6 Std. dev. faces/frame (VD) 5  Total nr. faces/window (VD) 6 3 Std. dev. faces/clear frame (VD) 6 9 3rd MFCC coecient (AU) 7 16 Med. len. episode w/o faces (VD) 7 51  Table 3: Most predictive variables in the multimodal feature set, according to the ranking obtained by mea- suring eect size (ES) and importance in the best-performing whole dataset model (RF/GBM). Along with the features, in parentheses, the data source to which it belongs (ET=eye-tracking, AU=audio, VD=video)  In-session perf. Out-of-session perf. Data source Features Best model Accuracy  Accuracy   Eye-tracking only All Gradient Boosted T. 87.5% 0.75 86.1% 0.72 EEG only All Gradient Boosted T. 55.1% 0.08 50.9% 0.02  Accelerometer only All Gradient Boosted T. 67.6% 0.34 61.2% 0.19 Audio only All Gradient Boosted T. 81.4% 0.62 79.3% 0.58 Video only All Gradient Boosted T. 81.7% 0.63 81.9% 0.63  All All Gradient Boosted T. 90.6% 0.81 89.6% 0.79 Audio+video All Gradient Boosted T. 86.1% 0.72 84.8% 0.69  All Top 5 (SVM) 88.2% 0.76 All Top 81 Gradient Boosted T. 90.6% 0.81 89.9% 0.80  Table 4: Performance of dierent models, data sources and features in predicting social plane of interaction  Distinguishing among the dierent teaching activities, how- ever, still remains dicult. Looking at the erroneously- predicted episodes, we find that certain kinds of error were more common than others (e.g., the activities of monitor- ing students work and providing repairs when one of them asks a question, which flow very fluidly into each other3). Such results hint at the necessity of developing new sets of features (e.g., based on basic automated speech analysis), but also of developing further our coding schemes so that they provide as much pedagogical value as possible, while remaining distinguishable.  In our exploration of the dierent multimodal data sources, we have found that basic features based on accelerometer and EEG signals provide very poor information to distin- guish teaching activity and social plane (which is to be ex- pected, as they are rather noisy and contaminated by even minimal physical movement, which can be irrelevant for this kind of characterization). On the other hand, we found that eye-tracking data had a surprising amount of useful informa- tion, especially the mean pupil diameter of each 10s window. This finding can be interpreted in the sense that such mea- sures are known to be related to emotional response and cognitive load factors, that is, this measure may capture the dierent levels of cognitive load elicited by the dierent teaching activities at dierent social planes (a hypothesis supported by our previous research in measuring cognitive load in the classroom [33]). However, it is also worth noting that eye-tracking measures (and the features extracted from them) are also most prone to be subject-dependent, which may pose a limitation if we are looking for models that are generalizable across teachers.  3For instance, the same random forest models, if applied to a coding scheme in which monitoring and repair are joined into a single category, achieve an accuracy of 75% (=0.6).  This exploratory case study also enabled us to uncover interesting trade-os between the accuracy of the machine learning models and the cost, eort and convenience of gath- ering and analyzing the dierent data sources. For instance, we found that using just the subjective audiovisual feed (eas- ily attainable using a small camera such as those used for sports, head-mounted cameras like Google Glass, or even a simple mobile phone) provided already quite good accu- racy, even if the eye-tracking measures provided an addi- tional edge (and have other advantages from the point of view of research, such as providing access to the subjects cognitive load). Even more convenient (and cheaper) would be the use of fixed cameras (e.g., such as the ones used to as- sess student attention in [35]), although the subjective feeds are more likely to capture the teachers experience and ac- tivities (e.g., occlusions, dead angles), and has side benefits for teacher inquiry processes (as it is easier to remember or understand a situation, or empathize with it  as we found during the coding of the videos). Regarding the cost in com- puting power we found that, although the best performance was attained by rather complex, black-box models like ran- dom forests, much simpler and faster models like support vector machines, using only a handful of multimodal fea- tures, also provided reasonable results (which can be use- ful, for instance, if real-time extraction of the orchestration graph were needed).  Despite the interest of these findings, this exploratory case study also suers from a number of limitations, the most important of which is the data set used (featuring only one subject, across four sessions that were rather similar to each other). This fact make the accuracies and performance re- ported in this paper very tied to the concrete subject and situation of our case study (i.e., low potential generalizabil- ity to other teachers, or to very dierent classroom situa-    tions). This is especially true of some features we found very important (like pupil diameter), although it may be less so for many of the audiovisual features that helped dis- tinguish between activities and social planes (e.g., faces in the field of view, or certain audio features). On the pos- itive side, our finding that audio/video data may provide adequate performance, along with the increasing aordabil- ity of such means of data gathering, make us hope for more scalable approaches that can be widely deployed, as other recent work in this area [2] also demonstrate.  Another limitation of this study is the feature set and machine learning methods explored. Even if we used more than a hundred features from five data streams, the fea- tures used were rather simplistic and generic, in many cases not tuned specifically for such teaching practice discovery. Although our results already provide a first step into sepa- rating useful features in each of the kinds of data, more work is still needed to distill more targeted features (maybe from other sensor data streams) that can help us distinguish more clearly between certain similar activities. Furthermore, the teaching practice categorization used in this study is only one example, and other characterizations are also possible, especially for researchers or practitioners interested in con- crete pedagogical approaches such as collaborative learning, or inquiry-based learning (which will also prompt new ex- ploration eorts into dierent sets of useful multimodal fea- tures). Also, further exploration is needed in applying more complex algorithms (e.g., deep/recurrent neural networks), which have recently shown promising capabilities in dealing with rich multimodal data (e.g., [24]).  Finally, it should be noted that one important aspect of orchestration, of which this study has barely scratched the surface, is that of the time structure of the teaching activi- ties, and of the dierent signals used, as teaching is by nature a sequence of actions over time. We anticipate that look- ing into the rhythm and pulse of each classroom situation, and exploiting time-series tools such as semi-Markov models (which model the time spent in one state before jumping to the next one), will probably provide additional increases in the accuracy of our automated teaching activity extraction.  6. CONCLUSIONS AND FUTURE WORK We started this paper by noting the scarcity of teaching  analytics research that actually studies teacher practice, and especially teacher actions in the face-to-face classroom, be- yond their lesson plans. As a first step into incorporating analyses of teaching activities to existing learning analytics and teacher reflection processes, we have explored a multi- modal approach to automatically extracting the orchestra- tion graph of a face-to-face collaborative learning session, based on the data of several wearable sensors.  The results of our study show that the approach is fea- sible and achieves reasonably good accuracy (especially to discriminate the for social plane of interaction). Also, the dierent models and data sources used show how this kind of approach is feasible, not only for researcher teams with ad- vanced equipment, but also for mass deployment by practi- tioners themselves (by using a simple subjective audiovisual feed). However, our study also demonstrates the diculty in discriminating between certain teaching activities, solely on the basis of a set of generic (and rather simple) features. More eorts are needed into extracting new discriminating features from these and other data sources.  These favorable results, and the hope that even a personal camera or a mobile phone could achieve useful results, open the door for larger-scale, wearable-based studies into teach- ing practice, either based on the generic characterization of practice as orchestration (presented in this paper), or through dierent ones based on more concrete pedagogical approaches. Regarding the categorization of teaching prac- tice, we are currently validating the interest of the teacher community in this kind of approach to reflection and teach- ing analytics through example visualizations extracted from this dataset4.  In our current and future work we are also continuing the exploration of more accurate statistical models (especially, those aimed at the time structure of the problem), and of more discriminating features for automatic activity and so- cial plane extraction. However, our most prominent current work relates to the recording of a larger multimodal dataset of teaching sessions by multiple teachers, especially in pri- mary schools but also at other educational levels. We hope this expanded dataset will help us overcome the largest limi- tation of the work presented here, namely, the limited gener- alizability of the results beyond a concrete classroom. Once this dataset is gathered, we expect to share it, along with the processing and analytical code used, with the teaching analytics community, as we believe openness and collabora- tion will be crucial in using this kind of multimodal analytics to study orchestration (since expertise from pedagogy, psy- chology, and a wide array of data and signal processing fields is required to make sense of the heterogeneous data), and to overcome the current scalability limitations of research into face-to-face orchestration.  7. ACKNOWLEDGMENTS This research was supported by a Marie Curie Fellow-  ship within the 7th European Community Framework Pro- gramme (MIOCTI, FP7-PEOPLE-2012-IEF project no. 327384).  8. REFERENCES [1] M. Bienkowski, M. Feng, and B. Means. Enhancing  teaching and learning through educational data mining and learning analytics: An issue brief. Technical report, U.S. Department of Education, Oce of Educational Technology, 2012.  [2] N. Blanchard, S. DMello, M. Nystrand, and A. M. Olney. Automatic classification of question & answer discourse segments from teachers speech in classrooms. In Proceedings of the 8th International Conference on Educational Data Mining (EDM 2015), International Educational Data Mining Society, 2015.  [3] P. Blikstein. Multimodal learning analytics. In Proceedings of the third international conference on learning analytics and knowledge, pages 102106. ACM, 2013.  [4] L. Breiman. Random forests. Machine learning, 45(1):532, 2001.  [5] D. Clow. The learning analytics cycle: closing the loop eectively. In Proceedings of the 2nd international conference on learning analytics and knowledge, pages 134138. ACM, 2012.  [6] J. Cohen. Statistical power analysis for the behavioral sciences (rev. Lawrence Erlbaum Associates, Inc, 1977.  4See http://classroom-mirror.meteor.com.  http://classroom-mirror.meteor.com   [7] P. Dillenbourg. Orchestration Graphs: Modeling Scalable Education. EPFL Press, Lausanne, Switzerland, 1 edition, 2015.  [8] P. Dillenbourg, S. Jarvela, and F. Fischer. The evolution of research on computer-supported collaborative learning. In Technology-enhanced learning, pages 319. Springer, 2009.  [9] P. Dillenbourg and P. Tchounikine. Flexibility in macro-scripts for computer-supported collaborative learning. Journal of computer assisted learning, 23(1):113, 2007.  [10] S. K. DMello, A. M. Olney, N. Blanchard, B. Samei, X. Sun, B. Ward, and S. Kelly. Multimodal capture of teacher-student interactions for automated dialogic analysis in live classrooms. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, pages 557566. ACM, 2015.  [11] F. Domnguez, K. Chiluiza, V. Echeverria, and X. Ochoa. Multimodal selfies: Designing a multimodal recording device for students in traditional classrooms. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, pages 567574. ACM, 2015.  [12] E. Durall and T. Leinonen. Feeler: supporting awareness and reflection about learning through EEG data. In Proceedings of the 5th Workshop on Awareness and Reflection in Technology Enhanced Learning In conjunction with the 10th European Conference on Technology Enhanced Learning, pages 6773, 2015.  [13] A. L. Dyckho, D. Zielke, M. Bultmann, M. A. Chatti, and U. Schroeder. Design and implementation of a learning analytics toolkit for teachers. Journal of Educational Technology & Society, 15(3):5876, 2012.  [14] T. Elias. Learning analytics: The definitions, the processes, and the potential. 2011.  [15] V. Emin-Martinez, C. Hansen, M. J. Rodrguez-Triana, B. Wasson, Y. Mor, R. Ferguson, and J.-P. Pernin. Towards teacher-led design inquiry of learning. E-learning papers. Special issue on Learning Analytics and Assessment, 36:314, 2014.  [16] R. Ferguson. Learning analytics: drivers, developments and challenges. International Journal of Technology Enhanced Learning, 4(5-6):304317, 2012.  [17] D. H. Freedman. Why scientific studies are so often wrong: The streetlight eect. Discover Magazine, 26, 2010.  [18] J. H. Friedman. Stochastic gradient boosting. Computational Statistics & Data Analysis, 38(4):367378, 2002.  [19] G. Gauthier. Using teaching analytics to inform assessment practices in technology mediated problem solving tasks. In Vatrapu et al. [48].  [20] T. Giannakopoulos and A. Pikrakis. Introduction to Audio Analysis: A MATLAB R Approach. Academic Press, 2014.  [21] Y. Hayashi and R. Mizoguchi. Articulation of scenario construction of lessons based on ontological engineering. In Vatrapu et al. [47].  [22] D. Hernandez-Leo, J. I. Asensio-Perez, M. Derntl, L. P. Prieto, and J. Chacon. Ilde: Community environment for conceptualizing, authoring and  deploying learning activities. In C. Rensing, S. de Freitas, T. Ley, and P. Munoz-Merino, editors, Open Learning and Teaching in Educational Communities, volume 8719 of Lecture Notes in Computer Science, pages 490493. Springer International Publishing, 2014.  [23] J. Jovanovic, D. Gasevic, C. Brooks, V. Devedzic, and M. Hatala. Loco-analyst: A tool for raising teachers awareness in online learning environments. In Creating New Learning Experiences on a Global Scale, pages 112126. Springer, 2007.  [24] B.-K. Kim, H. Lee, J. Roh, and S.-Y. Lee. Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition. In Proceedings of the 2015 ACM on International Conference on Multimodal Interaction, ICMI 15, pages 427434, New York, NY, USA, 2015. ACM.  [25] D. Laurillard. Teaching as a design science: Building pedagogical patterns for learning and technology. Routledge, 2012.  [26] Y. Mor, R. Ferguson, and B. Wasson. Editorial: Learning design, teacher inquiry into student learning and learning analytics: A call for action. British Journal of Educational Technology, 46(2):221229, 2015.  [27] L.-P. Morency, S. Oviatt, S. Scherer, N. Weibel, and M. Worsley. Icmi 2013 grand challenge workshop on multimodal learning analytics. In Proceedings of the 15th ACM on International conference on multimodal interaction, pages 373378. ACM, 2013.  [28] X. Ochoa, K. Chiluiza, G. Mendez, G. Luzardo, B. Guaman, and J. Castells. Expertise estimation based on simple multimodal features. In Proceedings of the 15th ACM on International conference on multimodal interaction, pages 583590. ACM, 2013.  [29] X. Ochoa, M. Worsley, K. Chiluiza, and S. Luz. Mla14: Third multimodal learning analytics workshop and grand challenges. In Proceedings of the 16th International Conference on Multimodal Interaction, pages 531532. ACM, 2014.  [30] M. Oliver and K. Trigwell. Can blended learningbe redeemed E-learning and Digital Media, 2(1):1726, 2005.  [31] D. Persico and F. Pozzi. Informing learning design with learning analytics to improve teacher inquiry. British Journal of Educational Technology, 46(2):230248, 2015.  [32] S. J. Preece, J. Y. Goulermas, L. P. Kenney, D. Howard, K. Meijer, and R. Crompton. Activity identification using body-mounted sensorsa review of classification techniques. Physiological measurement, 30(4):R1, 2009.  [33] L. P. Prieto, K. Sharma, and P. Dillenbourg. Studying teacher orchestration load in technology-enhanced classrooms. In Design for Teaching and Learning in a Networked World, pages 268281. Springer, 2015.  [34] L. P. Prieto, S. Villagra-Sobrino, I. M. Jorrn-Abellan, A. Martnez-Mones, and Y. Dimitriadis. Recurrent routines: Analyzing and supporting orchestration in technology-enhanced primary classrooms. Computers & Education, 57(1):12141227, 2011.  [35] M. Raca and P. Dillenbourg. System for assessing    classroom attention. In Proceedings of the Third International Conference on Learning Analytics and Knowledge, pages 265269. ACM, 2013.  [36] S. Rebholz, P. Libbrecht, and W. Muller. Learning analytics as an investigation tool for teaching practitioners. In Vatrapu et al. [47].  [37] V. Rivera-Pelayo, E. Lacic, V. Zacharias, and R. Studer. LIM App: Reflecting on Audience Feedback for Improving Presentation Skills. In D. Hernandez-Leo, T. Ley, R. Klamma, and A. Harrer, editors, Scaling up Learning for Sustained Impact, number 8095 in Lecture Notes in Computer Science, pages 514519. Springer Berlin Heidelberg, Sept. 2013.  [38] M. J. Rodrguez-Triana, A. Martnez-Mones, J. I. Asensio-Perez, and Y. Dimitriadis. Script-aware monitoring model: Using teachers pedagogical intentions to guide learning analytics. In Vatrapu et al. [47].  [39] R. K. Sawyer. Creative teaching: Collaborative discussion as disciplined improvisation. Educational researcher, 33(2):1220, 2004.  [40] G. Siemens. Sensemaking: Beyond analytics as a technical activity. Presentation at the EDUCAUSE ELI 2012 Online Spring Focus Session.  [41] G. Siemens and P. Long. Penetrating the fog: Analytics in learning and education. EDUCAUSE review, 46(5):30, 2011.  [42] K. Thompson, R. Martinez-Maldonado, D. Wardak, P. Goodyear, and L. Carvalho. Analysing F2F Collaborative Design and Learning : Experiences in a Design Studio A Place for Learning. In L. P. Prieto, Y. Dimitriadis, A. Harrer, M. Milrad, and J. D. Slotta, editors, Proceedings of the Orchestrated Collaborative Classroom Workshop 2015 co-located with 11th International Conference on Computer Supported Collaborative Learning, pages 2529, Gothenburg, Sweden, 2015.  [43] M. van Harmelen and D. Workman. Analytics for learning and teaching. JISC CETIS Analytics Series, 1(3):141, 2012.  [44] R. Vatrapu, C. Teplovs, N. Fujita, and S. Bull. Towards visual analytics for teachers dynamic diagnostic pedagogical decision-making. In Proceedings of the 1st International Conference on Learning Analytics and Knowledge, pages 9398. ACM, 2011.  [45] R. K. Vatrapu. Towards semiology of Teaching Analytics. In Workshop Towards Theory and Practice of Teaching Analytics, at the European Conference on Technology Enhanced Learning, TAPTA12, Saarbrucken, Germany, 2012.  [46] R. K. Vatrapu, K. Kocherla, and K. Pantazos. iklassroom: Real-time, real-place teaching analytics. In Vatrapu et al. [48].  [47] R. K. Vatrapu, P. Reimann, W. Halb, and S. Bull, editors. Proceedings of the Workshop Towards Theory and Practice of Teaching Analytics (TaPTA 2012), number 894 in CEUR Workshop Proceedings, Aachen, 2012.  [48] R. K. Vatrapu, P. Reimann, W. Halb, and S. Bull, editors. Proceedings of the 2nd International Workshop on Teaching Analytics (IWTA 2013), number 985 in CEUR Workshop Proceedings, Aachen, 2013.  [49] M. Worsley. Multimodal learning analytics: enabling the future of learning through multimodal data analysis and interfaces. In Proceedings of the 14th ACM international conference on Multimodal interaction, pages 353356. ACM, 2012.  [50] M. Worsley and P. Blikstein. Leveraging multimodal learning analytics to dierentiate student learning strategies. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 360367. ACM, 2015.  [51] B. Xu and M. Recker. Teaching Analytics: A clustering and triangulation study of digital library user data. Educational Technology & Society, 15(3):103115, 2012.    Introduction  Related Work  Teaching Analytics  Multimodal Learning and Teaching Analytics   Operationalizing `Teacher Practice': Orchestration  Case study  Context  Data Gathering and Feature Extraction  Results: Predicting Teaching Activity  Results: Predicting Social Plane of Interaction   Discussion  Conclusions and future work  Acknowledgments  References   "}
{"index":{"_id":"20"}}
{"datatype":"inproceedings","key":"McPherson:2016:SPD:2883851.2883945","author":"McPherson, Jen and Tong, Huong Ly and Fatt, Scott J. and Liu, Danny Y. T.","title":"Student Perspectives on Data Provision and Use: Starting to Unpack Disciplinary Differences","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"158--167","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883945","doi":"10.1145/2883851.2883945","acmid":"2883945","publisher":"ACM","address":"New York, NY, USA","keywords":"disciplinary differences, knowledge, learning analytics, legitimation code theory, sociology of education, student needs","abstract":"How can we best align learning analytics practices with disciplinary knowledge practices in order to support student learning? Although learning analytics itself is an interdisciplinary field, it tends to take a 'one-size-fits-all' approach to the collection, measurement, and reporting of data, overlooking disciplinary knowledge practices. In line with a recent trend in higher education research, this paper considers the contribution of a realist sociology of education to the field of learning analytics, drawing on findings from recent student focus groups at an Australian university. It examines what learners say about their data needs with reference to organizing principles underlying knowledge practices within their disciplines. The key contribution of this paper is a framework that could be used as the basis for aligning the provision and/or use of data in relation to curriculum, pedagogy, and assessment with disciplinary knowledge practices. The framework extends recent research in Legitimation Code Theory, which understands disciplinary differences in terms of the principles that underpin knowledge-building. The preliminary analysis presented here both provides a tool for ensuring a fit between learning analytics practices and disciplinary practices and standards for achievement, and signals disciplinarity as an important consideration in learning analytics practices.","pdf":"Student perspectives on data provision and use: Starting  to unpack disciplinary differences  Jen McPherson  Faculty of Business and Economics   Macquarie University  North Ryde, NSW, Australia    jen.mcpherson@mq.edu.au     Scott J. Fatt  Undergraduate Merit Scholar   Macquarie University  North Ryde, NSW, Australia   scott.fatt@students.mq.edu.au  Huong Ly Tong  Undergraduate Merit Scholar   Macquarie University  North Ryde, NSW, Australia   huong-ly.tong@students.mq.edu.au     Danny Y.T. Liu  Learning and Teaching Centre   Macquarie University  North Ryde, NSW, Australia  danny.liu@mq.edu.au   ABSTRACT  How can we best align learning analytics practices with  disciplinary knowledge practices in order to support student  learning Although learning analytics itself is an interdisciplinary  field, it tends to take a one-size-fits-all approach to the collection,  measurement, and reporting of data, overlooking disciplinary  knowledge practices. In line with a recent trend in higher education  research, this paper considers the contribution of a realist sociology  of education to the field of learning analytics, drawing on findings  from recent student focus groups at an Australian university. It  examines what learners say about their data needs with reference to  organizing principles underlying knowledge practices within their  disciplines. The key contribution of this paper is a framework that  could be used as the basis for aligning the provision and/or use of  data in relation to curriculum, pedagogy, and assessment with  disciplinary knowledge practices. The framework extends recent  research in Legitimation Code Theory, which understands  disciplinary differences in terms of the principles that underpin  knowledge-building. The preliminary analysis presented here both  provides a tool for ensuring a fit between learning analytics  practices and disciplinary practices and standards for achievement,  and signals disciplinarity as an important consideration in learning  analytics practices.   CSS Concepts   Information systems  Decision support systems  Applied  computing  Education  Human-centered computing   Collaborative and social computing  Social and professional   topics  User characteristics  Software and its engineering   Requirements analysis.   Keywords  Disciplinary differences; student needs; learning analytics;  knowledge; Legitimation Code Theory; sociology of education.   1. INTRODUCTION  If the aim of higher education is to initiate students into the  knowledge practices of knowledge societies [35; 40] through  developing students agency as professionals in their chosen  discipline [8], then what does this mean for learning analytics  practices Recent research in learning analytics points to the  importance of considering disciplinary differences in developing  predictive models [17; 42]. Further, recent research also  underscores the importance of both pedagogy and epistemology  [16; 21]. That is, if the purposes of learning analytics are to  [understand and optimize] learning and the environments in which  it occurs [39], then that purpose is better served when data  collection, measurement, and reporting are grounded within  existing educational research [16]. To date, educational psychology  and sociocultural theory have been valuable sources of theoretical  frameworks to inform learning analytics practices, although both  tend to overlook knowledge in favor of knowing or knowers [28].    A realist sociology of education provides a new lens through which  to view relations between knowledge practices and learning  analytics practices, and analytical and theoretical tools for  describing disciplinary differences. Research from this perspective  indicates that where some intellectual and educational fields tend  to progress through knower-building, emphasizing who you are  [27, p94, p110] (for example, marketing [1], media, and cultural  studies [27]), other fields tend to progress through knowledge- building, emphasizing what and how you know [27; 38] (for  example, law [12], accounting [31], physics, and economics [27]).  It is also possible for a field to progress through both knower- building and knowledge-building, with emphasis on both who you  are and what and how you know (for example, music [27],  English literature [19], and architecture [7]). Put simply, the  differences between sciences and the humanities could be described  as a code clash between knowledge and knower codes [27, p243],  although principles for knower-building and knowledge-building  are not homogenous within knower code and knowledge code  fields [27].    Importantly for learning analytics, principles for knower-building  and knowledge-building also define the dominant basis for  achievement within a discipline [27]. Achievement within knower- code disciplines tends to be defined in terms of learners  dispositions, including aptitude, attitude and personal expression  [27, p78]. Achievement in knowledge-code disciplines tends to be  defined in terms of disciplinary knowledge, skills, and procedures.  Learning analytics practices that take these principles into account  are more likely to support students in participating in learning   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883945     practices that align with disciplinary practices and standards for  achievement.    This paper will present an analytical framework or language of  description [5, p132] that makes disciplinary differences in the use  of analytics for curriculum, pedagogy, and assessment visible. The  analytical framework will frame different uses of data for  knowledge-building and knower-building practices, drawing on  data from a recent series of focus groups at Macquarie University.  The analytical framework is the first stage of a pilot study of  disciplinary differences in the use of data to support learning. The  focus groups were part of a larger project that emphasizes the  importance of student input into learning analytics, a perspective  that recent research suggests is sometimes overlooked in learning  analytics development [13; 14; 33].   The analytical framework will be developed through detailed  analysis of focus group transcripts, drawing on and extending a  framework developed by Chen [10] using Legitimation Code  Theory. Legitimation Code Theory provides analytical tools for  examining knowledge practices, allowing their organizing  principles to be conceptualized, and their effects to be explored  [27, p3]. Chens framework will be adapted to examine what  students say about the provision and use of data. Reflecting our  process in developing the framework for our pilot study, each  dimension of the framework will be presented in turn, supported by  examples from our focus group data. These examples reveal  differences in the provision and/or use of data in relation to  curriculum, pedagogy, and assessment that have significance for  learning analytics design and practice.   1.1 Learning practices, learning analytics  practices, and knowledge practices  As a practice, learning analytics condenses learning practices into  data which are then manipulated in symbolic relations and  relocalized [34] in learning practices, for example as prompts or  visual representations. Data are objects within a system of  representation, and as such, learning analytics creates a semiotic  system that is emergent from but not reducible to, interpersonal  exchange. Learning practices are located within broader networks  of practices which determine the properties of classroom teaching  and knowledge practices [11, p23]. The practice of learning  analytics is similarly located within broader networks of practices,  and it both enables and requires learners to engage in new kinds of  learning practices, for example, the interpretation and critical  analysis of data representations [22]. The effectiveness of learning  analytics systems rests on identifying actionable data that can  improve learning and teaching and course design, rather than  simply data that are easy to capture [37, p12]. At issue here is what  constitutes improvement, although it is noted that this is a question  that extends beyond learning analytics to educational research in  general.    Research within a realist sociology of education argues for a central  focus on knowledge, considering what is learned, as well as the  processes by which learners make shifts from congruent to more  abstract meanings. Legitimation Code Theory provides tools for  examining knowledge practices. Maton [27] provides a  comprehensive overview of two key dimensions of Legitimation  Code Theory: specialization and semantics, and outlines three other  dimensions of Legitimation Code Theory: autonomy, density, and  temporality. A brief summary of key principles in Legitimation  Code Theory can be found in Van Krieken et al. [41].    1.2 Specialization codes  This paper draws on the specialization dimension of Legitimation  Code Theory, which holds that:   [E]ducational practices and contexts represent messages as  to both what is valid to know and how, and also who is an  ideal actor (learner or teacher). That is, every practice or  knowledge claim is by someone (the subject) and is about,  or oriented towards, something (the object) [10, p131]  (italics in original).   The analysis presented here employs two key concepts that together  generate a range of specialization codes: epistemic relations, or  relations between practices and their object or focus and social  relations, or relations between practices and their subject [27,  p52]. The first refers to what can legitimately be described as  knowledge, and the second to who can claim to be a legitimate  knower [27, p52] (italics in original).    As is now standard within Legitimation Code Theory, epistemic  and social relations can each be described in terms of their relative  strength, from stronger (+) to weaker (-). As will become important  in the discussion of extracts from focus groups in Section 3, the  relative strength of epistemic relations and social relations actually  expresses the strength of classification and framingtwo concepts  that are central to Bernsteins [4; 5] work on pedagogic discourse.  Legitimation Code Theory builds explicitly on the work of  Bernstein, among other sociological theories [27]. Briefly,  classification (C) expresses power relations [5] and refers to  'relative strength of boundaries between contexts or categories' [27,  p29]. Framing (F) expresses control relations [5] and refers to 'the  locus of control within contexts or categories (where stronger  framing indicates greater control from above)' [27, p29]. Each can  be described in terms of relative strength: stronger or weaker  classification (+/-C) and stronger or weaker framing (+/-F).  Stronger epistemic relations (ER+) is therefore shorthand for  stronger classification (+C) and stronger framing (+F), and weaker  epistemic relations (ER-) are in effect weaker classification (-C)  and weaker framing (-F). Likewise, stronger social relations (SR+)  can be expressed as stronger classification and framing (+C/+F)  and weaker social relations (SR-) as (-C/-F) [27, p75-76].    Together, the relative strength of epistemic and social relations on  which our analytical framework is based generate four different  modalities or specialization codes. As defined by Maton [27, p30- 31]:     Knowledge codes (ER+, SR-) emphasize the possession of  specialized knowledge of specific objects of study  as the  basis of achievement and the attributes of actors are  downplayed;    Knower codes (ER-, SR+) emphasize the attributes of actors  ... as measures of achievement;    Elite codes (ER+, SR+) emphasize both specialized  knowledge and attributes of actors; and     Relativist codes (ER-, SR-) emphasize neither.   Knowledge code fields progress through knowledge-building,  knower code fields through knower-building, elite code fields  through both knowledge-building and knower building, and  relativist code fields through neither. As noted by Maton [27], in  any context, more than one code may be present and there may be  struggles over which is dominant (p77). There may also be code  clashes, for example between a learners disposition and the  dominant code of their educational context [9], or code shifts, such  as those at different stages of a curriculum [27].      2. METHODOLOGY  The analytical framework to be presented here has been developed  through detailed analysis of transcripts from a series of focus  groups with students at Macquarie University in mid-2015. The  focus groups were designed to gather information about learners  and their learning practices in order to investigate how meaningful  data can be made more accessible to students to improve their  learning. During focus groups, students were first provided with a  definition of learning analytics [39], shown everyday examples of  recommendation engines (eBay and Netflix), then asked what data  related to their learning they would like to have and why they would  like to have it. They were then asked how they would like to receive  this information, and to respond to examples of data dashboards  (Blackboard Analytics, a sample from Corrin and de Barba [13],  and Purdues Course Signals [3]). Although not discussed in detail  here, students were also asked what kind of data they would be  willing to share.    2.1 Participants  A total of 33 students (26 female and 7 male) participated in one  hour focus groups. Participants were recruited through several  channels including announcements posted through the universitys  learning management system, emails to students on established  mailing lists, advertisements posted on the universitys Facebook  page, posters, and flyers. Recruitment continued until data  saturation was reachedthat is, when an informationally  representative sample of students had participated in focus groups.  Participants included 24 undergraduate and 9 postgraduate students  from four out of the five faculties at Macquarie University: 9  students from the Faculty of Arts, 11 students from the Faculty of  Business and Economics, 10 from the Faculty of Human Sciences,  and 4 from the Faculty of Science and Engineering. There were no  participants from the universitys relatively new Faculty of  Medicine and Health Sciences. Although a statistically  representative sample was not required to achieve our research  objectives, the number of participants from each faculty is roughly  proportional to the distribution of students across faculties. Section  3 includes extracts from transcripts. In these extracts, pseudonyms  have been used to refer to participants. Extracts are from  undergraduate students unless stated otherwise.    2.2 Analysis  With students consent, focus groups were recorded and  transcribed, and transcripts were analyzed in NVivo. What students  said about the provision or use of data related to their learning was  examined for messages about what is valid to learn and who is an  ideal learner. The analysis builds on Chens [9, p83] external  language of description (summarized in Table 1) and explained  below. Where Chens framework was developed to analyze  students online learning experiences, it is extended here to  examine the realization of epistemic and social relations in what  students say about the provision or use of data in relation to  curriculum, pedagogy, and assessment practices.   A language of description enables a dialogue between empirical  data and theory, and once established, it makes the analysis visible  to and reproducible by other researchers [27, p137]. As shown in  Chens language of description in Table 1, an emphasis on  epistemic relations in her data was realized in what students said in  relation to content knowledge (curriculum), the teaching of content  knowledge (pedagogy), and explicit evaluative criteria  (assessment). An emphasis on social relations was realized in what  students said in relation to their personal knowledge and experience  (curriculum), personal dimensions of learning (pedagogy), and  self-evaluation (assessment). Following convention within   Legitimation Code Theory, the relative strength or degree of  emphasis on epistemic relations is indicated using ER+/- and the  strength or degree of emphasis on social relations is indicated using  SR+/-.    Table 1: Epistemic and social relations in the focus group data  (adapted from [9, p83]).   Concept Emphasis on   E p  is te  m ic     re la  ti o  n s   Curriculum Content knowledge ER+/-   Pedagogy Teaching of content  knowledge   ER+/-   Assessment Explicit evaluative criteria ER+/-   S o  ci a  l   re la  ti o  n s   Curriculum Learners personal  knowledge and experience   SR+/-   Pedagogy Personal dimensions of  learning   SR+/-   Assessment Learners self-evaluation SR+/-      3. RESULTS AND DISCUSSION  Our analytical framework or language of description was  developed to examine the realization of epistemic and social  relations in our focus group data. As is typical in this kind of  analysis, this involved repeated movements between theory and  data [27, p137]: in our case, this was achieved collaboratively  using Google Docs. Although based on what students have said  about their needs and interests in relation to the provision and/or  use of data, the analytical framework also provides a framework for  differentiating between learning analytics practices that support  knowledge-building and those that support knower-building. We  have used Chens [9] analytical categories (Table 1), but extended  the definition of each category to include what students say about  data or data-driven interventions in relation to curriculum,  pedagogy, and assessment practices.    Extracts from focus group transcripts will be provided to illustrate  each analytical category. Extracts have been selected from the  analysis that exemplify the nature and scope of each category, and  are intended to be informationally rather than statistically  representative of participants contributions to focus groups.    3.1 Curriculum  Following Chens [9] framework (Table 1), comments about the  provision or use of data in relation to curriculum were examined  for a focus on either content knowledge or learners personal  knowledge and experience.    3.1.1 Content knowledge  Student comments that focused on content knowledge were  categorized according to the strength of epistemic relations, or in  other words, the extent to which boundaries between disciplinary  categories are maintained (classification) and the extent to which  control over sequencing, selection, and pacing of content comes  from academic staff (framing). Those statements that associated the  provision and/or use of data with strong classification and framing  of content knowledge (+C, +F) were coded as ER+, while those that  associated the provision or use of data with weak classification and  framing of content knowledge (-C, -F) were coded as ER-.       Curriculum ER+  As shown in Table 2, the indicator for Curriculum ER+ in the focus  group data is the provision or use of data that emphasizes content  knowledge as defining the curriculum and/or legitimate educational  knowledge. Students identified a need for or interest in:    Data that would assist in finding resources that would assist in  understanding key concepts: youve always got lots of  readings to do and you need to know which readings are very   relevant to you, first to better understand the key concepts   covered in the unit and also to prepare us for the assignments  (Yuxi, postgraduate applied linguistics student, FG4);    Data that would help in finding assignment readings: [It  would] probably take the keywords of what your assignment is   about and be like, here's some suggestions for you to get   started (Lauren, first year arts and criminology student,  FG10);    Data that would support exam preparation: So I guess would it  be possible to collect data on things like that, keywords [in   sample exam papers] (Lauren, first year arts and criminology  student, FG10);    Resources used by high-achieving students:  compare which  resources and how many resources [correlate with] higher   scores and also which resources used how will benefit in what   way as well (Helen, first year psychology student, FG8).   Some of these needs could be met through discourse-centric  learning analytics that focus on the advancement of subject  knowledge [20, p188]. Examples of analytics identified by  students were at course level, and include recommendations that  rank relevance of readings based on key concepts in a subject, and  recommendations based on keywords in assignment topics or  sample exam papers.     Curriculum ER-  The indicator for Curriculum ER- in the focus group data is the  provision or use of data that downplays content knowledge in  defining the curriculum and/or what is legitimate educational  knowledge, as shown in the bottom row of Table 2. Here a  distinction is made between learning analytics practices that imply  that content knowledge is legitimate educational knowledge (ER-)  and the previous category where learning analytics practices  emphasize content knowledge as legitimate educational knowledge  (ER+).    In this analytical category, classification and framing are weaker:  objects of study are less bounded and clearly defined, and academic  staff are positioned as having less control over sequencing,  selection, and pacing of content. The needs or interests identified  by students included:    Data on graduate destinations that could be used to inform  subject choices: like how many people got a job in this field  using that degree . I reckon [having data on what subjects   people that got jobs did] would be like pretty helpful (Nadia,  first year arts media student, FG2);    Data on level of difficulty that could be used to inform subject  choices: knowing like the pass rate on like really, really hard  units If you knew the actual pass rate, the most popular grade   that people got, that would be really useful in choosing if I want   to do it or not (Nadia, first year arts media student, FG2).   In these examples, it can be seen that there is a sense of choice over  subject selection and curriculum, although the choices are informed  or mediated by linked data that condenses markers of academic  success within a discipline: getting a job in the field (first year arts  media student, FG2) or grades. Data that support subject choices  based on graduate destinations emphasize the idea that selection,  sequencing, and pacing in a curriculum are arbitrary, downplaying  the organizing principles of content knowledge as the basis for  selection, sequencing, and pacing.   Table 2: Curriculum and epistemic relations   Emphasis Strength Indicators   C on  te nt   k no  w le  dg e   ER+ Provision or use of data that  emphasizes content knowledge as  defining the curriculum and/or  legitimate educational knowledge.   ER- Provision or use of data that downplays  content knowledge in defining the  curriculum and/or what is legitimate  educational knowledge.      3.1.2 Learners personal knowledge and experience  Student comments about personal knowledge and experience were  categorized according to the strength of social relations. Here, the  strength of classification and framing is associated with the  primary experience of knowers [27, p29-30] and the attributes of  knowers, rather than knowledge. For example, in the first two  examples shown in Curriculum SR+, Jingyis emphasis is on the  judgement of students in choosing readings, rather than the content  of the reading, and Alices on the dispositions of students, or what  they are good at.    Curriculum SR+  Table 3 shows that the indicator for Curriculum SR+ in the focus  group data was the provision or use of data that condenses personal  experience, preferences, and opinions as legitimate educational  knowledge. The needs or interests identified by students in this  category included:    Data on popular or useful resources: I know that what kind of  articles other students are reading I think is very useful,   especially when we are doing our assignments (Jingyi,  postgraduate business and economics student, FG11);    Recommendations on subject selections:  it would be good  based on our grades that when we did really well [then we get   advice on] which units we should further undertake, like the   units that we're good at (Alice, second year finance student,  FG3);    Recommendations on subject selections or program choices  linked to interests: it might help with subject selection, other  degrees like if I was thinking about changing my degree or   something like that because you could see that hey look, you're   studying this degree but you spend a lot of your time looking at   this; would this maybe be appropriate for you (Vesna, fourth  year law and economics student, FG11);    Recommendations on subject selections based on similarity of  students experience:  it would be great if  they have a  database where students have done well in this subject and then   a lot of students will undertake this other subject as well, to     continue on because it's like similar (Alice, second year  finance student, FG3);    Representations of popular study pathways: You could do like  a pretty interactive map of the subjects people followed on with.   So if you kind of had your subject selected, it would be like a   bunch of people went over there, and then ... really bolded,   because lots of people went and followed and did [another   unit]. ... Because they're a paired subject, and you can see that.   Then, over here, lots of people went and did that. Then you'd   see a clear kind of path, people following on to these kind of   capstones (Isabelle, first year science and arts student, FG9).   In this category, recommendations at a personal level are based on  the choices of other students. Students frequently commented that  they would like data on what other students are reading. Several  equated popularity with usefulness, as in the first example above  from Jingyi, or as Nadia, a first year arts and media student,  explains: if you're going to write an essay and there's heaps of  resources that you don't know which one to choose, like you can   find the most popular one which will be like the most useful one  (FG2). As there could be other reasons for popularity unrelated to  disciplinary content (for example, popular readings may be short or  easy to read rather than relevant), these are coded as Curriculum  SR+. This is in contrast to recommendations that rank readings  based on relevance to key concepts in the data coded at Curriculum  ER+.   Recommendations at program/department level (for example, in  relation to student pathways) are linked to other data that condenses  students experience or attributes, producing recommendations  based on students strengths or interests. Students position  themselves as playing a significant role in determining what  legitimate knowledge is [9, p128], choosing a path through the  curriculum based on their own or other students experience or  preferences.     Curriculum SR-  The indicator for Curriculum SR- is the provision or use of data that  downplays personal experience, preference, and opinions,  distinguishing these from legitimate educational knowledge.  Comments coded in this category were student concerns about data  that presents personal experience, preferences, and opinions as  legitimate educational knowledge, rather than comments about  needs or interests. Peter, a mature-age postgraduate applied  linguistics student, for example doubted the status of what other  students have done: It may not be the best idea to work on the  basis of what other students have done, but something that I   suggested  was having discipline specific mentors that were able   to advise (FG6).    Table 3: Curriculum and social relations   Emphasis Strength Indicators   Le ar  ne rs   p er  so na  l  kn  ow le  dg e   an d   ex pe  rie nc  e SR+ Provision or use of data that  condenses personal experience,  preferences and opinions as legitimate  educational knowledge.   SR- Provision or use of data that  downplays personal experience,  preferences, and opinions,  distinguishing these from legitimate  educational knowledge.      3.2 Pedagogy  Student comments about the use or provision of data in relation to  pedagogy were examined for a focus on either the teaching of  content knowledge (epistemic relations) or the personal dimension  of the learning process and self-evaluation of the learning process  (social relations). Here, Chens [9] categories have been extended  to take into account the potential of analytics for self-evaluation of  the process of learning (pedagogy). Although Chens language of  description includes self-evaluation, this is limited to the products  of learning (assessment). Even though an assessment task may  assess learning processes as well as products, a basic distinction is  made here between self-regulation of learning (coded at pedagogy)  and learning processes and products associated specifically with  assessment (coded at assessment).    3.2.1 Teaching of content knowledge  As with student comments on content knowledge, student  comments on the teaching of content knowledge were categorized  according to the strength of epistemic relations. Distinctions within  this category are associated with the extent to which procedures for  disciplinary learning are explicit.     Pedagogy ER+  Table 4 shows that the indicator for Pedagogy ER+ is the provision  or use of data that emphasizes content knowledge (what) and makes  procedures for disciplinary learning (how) explicit to students. The  needs or interests identified in this category included:    Data that emphasize study habits appropriate to a discipline:   that would be useful as well, how much time they spend doing   readings, because it varies between subjects. I do a lot of law   subjects, so there's a huge emphasis on the readings whereas   my boyfriend is doing economics he doesnt have many   readings at all. So the way that we study has to be totally   different (Emma, second year psychology and law student,  FG4);    Data on students use of other discipline-specific resources that  emphasize procedures for learning content knowledge: for  exam prep, sometimes people post links to, oh this website was   really helpful, it had great multi choice practice especially in   psych where you dont really get past papers at all (Emma,  second year psychology and law student, FG4).    The needs and interests identified in this category can be  differentiated from those in Curriculum SR+ by an emphasis in  Pedagogy ER+ on explicit discipline-specific procedures for  learning; for example, how much time they spend doing readings  because it varies between subjects (Emma, second year  psychology and law student, FG4) as opposed to a focus on what in  Curriculum SR+.    Pedagogy ER-  The indicator for Pedagogy ER- is the provision or use of data that  emphasizes content knowledge (what) but where procedures for  disciplinary learning (how) are implicit to students, as shown in the  bottom row of Table 4. The needs or interests identified in this  category included data that help students to manage content  learning, with emphasis on what is learned. Examples of needs and  interests include:    Data that indicate activities completed:  back on a unit I did  in the first semester at uni they had actual tick boxes next to all   of thepretty much every resource. That was really fantastic   just to go, oh yeah, I've done that  So having that you need to   this, this is optional, but if you could do that that would be good,     just so that I dont have to write a zillion to do lists every time   I go to study (Emma, second year psychology and law student,  FG4);    Data that indicate activities to be completed: Ideally it would  be a tab in [the learning management system]. So it says, like,   semester two, and it has your list of subjects  you'd have a   semester overview  then you'd have just a running list,   starting with the subject name, ideally. You knowhave week   three reading done, and that would be something that the unit   convenor would set out  So you could have it kind of as a   running feed, as you went through. You'd be able to easily move   forward in a, kind of, auto-prioritized way, without just getting   stuck into one subject. Or you'd be able to see if you were   ignoring one subject, if it was just in one big list (Isabelle, first  year science and arts student, FG9).   While procedures in both of these examples are explicit in the sense  that they specify what students should do and when they should do  it, procedures for disciplinary learning are implicit because the data  provides prompts that are generic to any discipline. These can be  contrasted with examples in Pedagogy ER+ where procedures for  disciplinary learning are explicit.   Table 4: Pedagogy and epistemic relations   Emphasis Strength Indicators   Th e   te ac  hi ng   o f c  on te  nt    kn ow  le dg  e   ER+ Provision or use of data that  emphasizes content knowledge (what)  and makes procedures for disciplinary  learning (how) explicit to students.   ER- Provision or use of data that  emphasizes content knowledge (what)  but where procedures for disciplinary  learning (how) are implicit to  students.       3.2.2 Personal dimension of the learning process and  self-evaluation of the learning process   Student comments that emphasized the personal dimensions of  learning and self-evaluation of learning were examined for the  strength of social relations. Distinctions within this category relate  to the presence or absence of external criteria or standards against  which the personal dimension of the learning process can be  judged.     Pedagogy SR+  The indicator for Pedagogy SR+ is the provision or use of data that  condenses learners choices on how to study or study habits as can  be seen in Table 5. This can be contrasted with procedures for  disciplinary learning in Pedagogy ER+, given that students choices  about how to study may not necessarily match procedures for  disciplinary learning. Further, the emphasis in this category is on  how to study, rather than what and how in Pedagogy ER+.  Comments in Pedagogy SR+ relate to data that support the self- evaluation of learning processes, decisions about how to go about  learning, and social dimensions of learning. Needs and interests  identified in this category emphasize learners dispositions, without  reference to external criteria or standards against which learning is  measured. Examples of needs and interests at a personal level  include:    Data that support time management:  knowing how much  I've done and how that compares to where I should be. So just   setting that goal and making sure I fulfil all my study   commitments so that I can use my time properly (Emma,  second year psychology and law student, FG4);    Data that support scheduling: So if you could set up what you  want to do and having that schedule, personally for me that   would help, being able to go, okay it's Monday afternoon, it's   time to do torts (Emma, second year psychology and law  student, FG4);    Data on own or other students class attendance or online  lecture downloads: I kind of struggle between choosing  whether I should go to a lecture or just listen to it at home    So knowing which of the two is more useful and like which one   people go to the most would be more timesaving (Nadia, first  year arts media student, FG2);    Data on time spent on resources:  just being able to go, oh  yeah, I found this useful or this isn't reallyspend a lot of time   on this one or maybe just skip this (Emma, second year  psychology and law student, FG4);    Data that help in identifying like-minded students: It would be  nice if I can find the people who are reading similar things with   me and I know that he or she is interested in the same stuff as   me (Ailun, postgraduate applied linguistics student, FG3).   In the first quote in this list, Emma is looking to manage her time  against her own goals rather than an external standardin her  words, where I should be in relation to the goals she has set for  herself. This example suggests that from the learners perspective,  representations of her own activity in visual form can act as a proxy  for external standards. Making events and processes visible helps  to change those events [30, p2]. A numerical view is often regarded  as an objective one [32]: the numbers themselves are apparently  neutral, where in reality they are an interpretation of reality, one  that is heavily weighted in favor of what the [analyst] is able to  measure and chooses to measure [32, p480] depending on their  choice of measurement tool. Visual representations of data of this  type condense learning activity as 'things'semiotic objects which  take on a sense of objectivity.    In the shift towards representation, that is, when learning activity is  represented visually as a thing, truth becomes encoded in the  conventions of visual display. As Kress and van Leeuwen observe,  visual modality rests on culturally and historically determined  standards of what is real and what is not, and not on the objective  correspondence of the visual image to a reality defined  independently of it [23, p52]. Data has limits as a measure of  student performance or intention: Corrin and de Barba, for  example, note the partiality of data, and register concern about  heavy reliance on quantitative representations of student activity  [13, p1]. As noted by Sharples et al. [37, p13], students learning  behavior depends upon a complex interaction of personal,  emotional, social, and economic factors and these may not be  directly observable or measurable. As with representations of  interpersonal exchange in educational practice [29], visual or other  abstract representations of learning practices (e.g. models and  formulae) are partial because they only partially relocalize, and  cannot logically contain, the constellational identity [6, p.xix] of  learning interactions condensed within them.       Pedagogy SR-  The indicator for Pedagogy SR- in the data is the provision or use  of data that link study habits with grades or subject requirements to  provide an external standard for self-evaluation of learning, as  shown in the bottom row of Table 5. Needs and interests identified  by students in this category included:    Data on amount of time spent studying by high-achieving  students: So like, I dont know, if the person who topped the  unit spent X amount of time on the [learning management   system] page and made sure they did that, I reckon that would   be pretty useful to kind of make sure I'm aiming for that  (Emma, second year psychology and law student, FG4);    Data on study strategies of high-achieving students: I reckon  something that would be useful, too, is knowing what times the   top students like start working on assignments, like is it two   weeks before it's due, three weeks, as soon as they get it. That'd   be heaps useful (Nadia, first year arts media student, FG2)    Data on study pathways or load taken by high-achieving  students: I plan to move down from four to three subjects later  on, because it is going to be really full on and I've got other   things to do. To get an idea of whether thatI'm stepping out   of the herd and that's a bad idea. Or if I'm stepping away from   the high achievers in doing that. To be able to guide my plans.  (Isabelle, first year science and arts student, FG9)    Personalized warnings: Sometimes you fall behind and it's  good to have a bit of advance notice if you're not quite keeping   up, if you're falling behind on the lectures. It would be good to   have some kind of warning in place, I dont know, just for me  (Emma, second year psychology and law student, FG4)   As in the category Curriculum ER-, it can be seen that analytics  suggested by students in these examples are mediated by linked  data that condenses markers of academic success.    Table 5: Pedagogy and social relations   Emphasis Strength Indicators   Pe rs  on al   d im  en si  on  o  f  th  e  le  ar ni  ng  p  ro ce  ss  a  nd    se lf-  ev al  ua tio  n  of    le  ar ni  ng  p  ro ce  ss    SR+ Provision or use of data that  condenses learners choices on how  to study or study habits.   SR- Provision or use of data that links  study habits with grades or subject  requirements to provide an external  standard for self-evaluation of  learning.      3.3 Assessment  Student comments about the use or provision of data in relation to  assessment were examined for a focus on either explicit evaluative  criteria (epistemic relations) or self-evaluation of the products of  learning (social relations). As noted above, a distinction was made  between self-evaluation of the products of learning (in this  category) and self-evaluation of the process of learning (in  Pedagogy SR+/-), acknowledging that the process of learning may  also be assessable.   3.3.1  Explicit evaluative criteria  As in previous categories that focus on epistemic relations, student  comments on explicit evaluative criteria were also categorized   according to the strength of epistemic relations. Distinctions within  this category are associated with the presence or absence of  evaluative criteria such as marking criteria.     Assessment ER+  Moving on to Table 6, it can be seen that the indicator for  Assessment ER+ in the data is the provision or use of data that  emphasizes explicit evaluative criteria in judging products of  learning. The needs or interests identified in this category included:    Data that provide personalized feedback with reference to  marking criteria: But even if you click on it and just be like,  here's some feedback from the lecturer or your tutor here. Why   are you going good Why are you not going good Why have   you failed (Lauren, first year arts and criminology student,  FG10)    Data that provides personalized feedback with reference to  subject learning outcomes:  summative can help  [but] it's  really just how am I going and in terms of class average or   cohort average. It's how do I compare with others, which I   dont think is the best way of making a comparison. I think that   the best way of making a comparison is what do I expect   Where should I be in relation to what I should be learning If   I'm not at that stage, how can I get there (Peter, postgraduate  applied linguistics student, FG6)   As can be seen in the comments from Peter, a distinctive feature of  this analytical category is comparing performance against criteria  rather than against the performance of other students. In this  instance, while he starts out to say that he thinks the best point of  comparison is what do I expect which would be consistent with  Assessment SR-, he reformulates this as where he should be in  relation to what [he] should be learning. This statement has been  interpreted with reference to his other contributions to discussion,  and what I should be learning is taken to be an external standard.     Assessment ER-  As shown in the bottom row of Table 6, the indicator for  Assessment ER- in the data is the provision or use of data that  downplays explicit evaluative criteria. The needs or interests  identified by students in this category included:    Direct interventions based on performance:  you could use  that data to send automated things and say, just to let you know   that you would have to get an average mark of 89 per cent in   the next three assessments if you want to pass the subject; if   they were linking in your analytics, sayingyeah, like I've had   a look. Just want to remind you of where your average is   compared to the cohort. Just want to let you knowif it was   like an automated thingPS, you're on a track to fail (Isabelle,  first year science and arts student, FG9); Probably they could  send something like notifications if you're doing really badly in   the subject as well, to warn you (Tim, second year business  and economics student, FG2);    Indirect interventions based on performance: Maybe as an  option you can request your ranking and they'll send it to you   after you request it (Hongfei, first year business and  economics student, FG2); Some people don't want to check the  rankings (Ishrak, first year information technology student,  S06, FG2); Maybe if there was like an option on [the learning  management system], like when you go on the homepage you   can like click your profile or something like that then it's like   your grades and everything's just there (Nadia, first year arts  media student, FG2).     The terms direct and indirect are used here to differentiate  between targeted personalized interventions such as a warning  message by email or some other channel (direct), and less direct  interventions such as a data dashboard that students can choose to  access or not (indirect). Although a grade is a measure of success,  criteria for success are condensed into a single numerical figure,  differentiating this category from Assessment ER+.   Table 6: Assessment and epistemic relations   Emphasis Strength Indicators   Ex pl  ic it   ev al  ua tiv  e  cr  ite ria     ER+ Provision or use of data that  emphasizes explicit evaluative criteria  in judging products of learning   ER- Provision or use of data that downplays  explicit evaluative criteria      3.3.2 Self-evaluation of the products of learning  Student comments that emphasized self-evaluation of the products  of learning were examined for the strength of social relations.  Paralleling distinctions in self-evaluation of the process of learning  in Pedagogy SR+/-, the two strengths of social relations (SR+ and  SR-) in this category could be distinguished by the presence or  absence of external criteria or standards against which the products  of learning can be judged, or in other words, the presence of  external criteria (Assessment SR-) versus the absence of external  criteria or standards (Assessment SR+). As distinct from  Assessment ER+, this category includes comments related to self- assessment, rather than assessment by academic staff.     Assessment SR+  The indicator for Assessment SR+ in the data is the provision or  use of data that emphasizes learner beliefs in evaluating the  legitimacy of products of learning (Table 7). Comments coded in  this category included:    Beliefs about assessment criteria and criteria for academic  success: I think that there is more to academic success than  just getting numbers on a transcript. I think that that's - that's   come about because talking to friends at different universities,   it feels like sometimes the amount of work needed to get those   numbers varies across the board, and what that means  it's   definitely possible that there's not this one standard, one level   playing field that everyone is in (Vesna, fourth year law and  economics student, FG11);    Beliefs about evaluation: Because I know people say that a  particular unit's really hardlike [unit name] for example, it   has quite a high fail rate, but I managed to get a high distinction   in it (Jessie, third year commerce and arts student, FG5).    Assessment SR-  Finally, as can be seen in the bottom row of Table 7, the indicator  for Assessment SR- in the data is the provision or use of data that  allows for self-evaluation of products of learning with reference to  shared criteria external to the learner. The needs or interests  identified by students in this category included:    Comparisons with current cohort: Just something that I would  want to know. I want to know if my 60 grade that I thought okay   that's an alright pass, but I'd want to know if the class average   was 85 or 55  So whether that's relatively good or not  (Jessie, third year commerce and arts student, FG5);    Individual comparisons with previous cohorts:  you're  performing in the twentieth percentile. People in the twentieth   percentile last year got this final mark (Isabelle, first year  science and arts student, FG9);    Cohort level comparisons with previous cohorts:  the whole  cohort's data is consolidated and then they tell us, how well   your cohort did in comparison to other cohorts (Alice, second  year finance student, FG3).   In contrast to the examples in Assessment SR+, the emphasis here  is on shared criteria. In these examples, the shared criteria are the  standards set by the current or previous cohort. While external  standards in Pedagogy SR- are based on the performance of  particular groups of students (e.g. high-achieving students),  standards in this category refer to the performance of more general  cohorts of students. Here, students evaluate themselves with  reference to the standard set by that cohort. Where the emphasis in  the category Assessment ER- is on particular criteria for success  (grades and rankings), emphasis in Assessment SR- is on self- evaluation with reference to shared criteria (the standard set by the  cohort).   Table 7: Assessment and social relations   Emphasis Strength Indicators   Se lf-  ev al  ua tio  n  of    pr  od uc  ts  o  f l ea  rn in  g   SR+ Provision or use of data that  emphasizes learner beliefs in  evaluating the legitimacy of products  of learning   SR- Provision or use of data that allows for  self-evaluation of products of learning  with reference to shared criteria  external to the learner      4. CONCLUSIONS  A realist sociology of education brings knowledge into focus,  making visible principles of knowledge-building and knower- building within disciplines. These principles have significance for  learning analytics, because they define the primary basis for  achievement within disciplines.    Our paper has drawn on student perspectives on the provision and  use of data. As noted earlier, recent literature has identified gaps in  our understanding of student perspectives on learning analytics. A  small number of studies have begun to address this gap: Sclater  [36], for example, documents student participation in identifying  user requirements for a student-facing learning analytics app, while  Corrin and de Barba [13; 14] focus on students interpretation of  feedback delivered through dashboards. The latter study also fits  within a related strand of research literature that calls for a more  nuanced [15, p232] approach to analytics; for example, one that  seeks to understand learning analytics within disciplinary contexts  [17] and that moves beyond the predictive analytics paradigm [26].  We have sought to connect these two strands of research in an  analytical framework or language of description that differentiates  between different uses of data in relation to curriculum, pedagogy,  and assessment on the basis of their underlying epistemic or social  relations. Our theoretical approach parallels other recent work that  applies tools from Legitimation Code Theory in investigating the  integration of educational technology [18]. While the focus of  research by Howard et al. is on the use of laptops and information     and communication technologies in the classroom, their  recommendation that [r]elevant and appropriate technology uses  that match with underlying principles of subject areas, such as the  nature of success and relations to knowledge embodied in  technology-related practices, need to be identified and explicated  [18, p368] also has significance within the field of learning  analytics.    The analytical framework or language of description presented here  is a step towards a more granular understanding of learning  analytics practices with reference to principles of knowledge- building and knower-building practices. These principles are at the  center of disciplinarity: disciplinary differences in pedagogy,  curriculum, and assessment reflect core features, values and  underlying principles of subject areas [18, p362] (italics in  original). A language of description such as the one developed in  this paper exemplifies a knowledge-code (ER+, SR-): in making  the analysis visible to other researchers, who you are is  downplayed as the basis for legitimacy  Anyone who  understands the theory can see if the analysis is consistent with the  data and conclusions borne out by evidence [27, p137].    Our next step will be to apply this analytical framework to more  detailed examination of our focus group data to explore patterns of  disciplinary differences. Preliminary analysis suggests that student  perspectives on the provision or use of data frequently runs counter  to the dominant code suggested by their disciplinary affiliation.  These code clashes sound a warning for the field of learning  analytics, highlighting the potential of learning analytics practices  not only in the construction, reproduction, and transformation of  disciplinary identities, but also in their destruction.    Ideally, learning analytics would contribute to building students  capacity for reflexivity, or making conscious learning choices  within the context of their discipline. As sense-makers with  agency [25, p178] students can use data to support their inner  dialogues [24; 25] or internal conversations [2]. The practice of  learning analytics generates tools that enable students to self- regulate their learning, but at the same time, has the potential to  displace disciplinarity and create a vacuum of legitimacy [10].  Instead of a concern with knowledge-building consistent with their  discipline, students may be co-opted into utilitarian and  organizational discourses in which data-driven learning practices  are legitimated with reference to progress, rational economic  behavior, and quantification, and organizational demands for  quality, efficiency, and accountability. Depending on the degree of  autonomy over curriculum, that is, who controls curriculum  decisions and on what basis, it may be shaped by external drivers,  for example, discourses of employability which increasingly  [privilege] a knower code  with the emphasis on attributes and  ways of being [38, p6]. Without a more nuanced approach [15,  p232], learning analytics has similar potential to shape the  curriculum, through enabling new kinds of learning practices that  favor efficient and accountable ways of being over disciplinary  knowledge-building or knower-building.   5. ACKNOWLEDGEMENTS  This work was funded as part of a larger Innovation and  Scholarship Program project, supported by the Education Studio at  Macquarie University. H.L.T. and S.J.F. were supported through  Macquaries undergraduate Merit Scholars program. We wish to  thank the students who participated in our focus groups, and the rest  of the project team, for offering their insights. We would also like  to thank three anonymous reviewers for their valuable feedback.   6. REFERENCES  [1] Arbee, A., Hugo, W. and Thomson, C. (2014)   Epistemological access in marketing: A demonstration of the  use of Legitimation Code Theory in higher education.  Journal of Education, 59(39-63).   [2] Archer, M. (2003) Structure, agency and the internal  conversation. Cambridge University Press, Cambridge.   [3] Arnold, K. E. (2010) Signals: Applying Academic Analytics.  Educause Quarterly, 33(1), n1.   [4] Bernstein, B. (1977) Class, codes and control, Vol III: The  structuring of pedagogic discourse. Routledge, London.   [5] Bernstein, B. (2000) Pedagogy, symbolic control and  identity: Theory, research, critique.  Revised edition.  Rowman and Littlefield, Lanham.   [6] Bhaskar, R. (1975) A realist theory of science. Routledge.  [7] Carvalho, L., Dong, A. and Maton, K. (2009) Legitimating   design: A sociology of knowledge account of the field.  Design Studies, 30(5), 483-502.  http://dx.doi.org/10.1016/j.destud.2008.11.005   [8] Case, J. M. (2013) Researching Student Learning in Higher  Education: a social realist approach. Routledge, London.   [9] Chen, R. T.-H. (2010) Knowledge and knowers in online  learning: Investigating the effects of online flexible learning   on student sojourners. Thesis: University of Wollongong,  Wollongong.   [10] Chen, R. T.-H., Maton, K. and Bennett, S. (2011). Absenting  discipline: Constructivist approaches in online learning. In F.  Christie and K. Maton (Eds.), Disciplinarity: Functional  linguistic and sociological perspectives (pp. 129-150).  London: Continuum.   [11] Chouliaraki, L. and Fairclough, N. (1999) Discourse in late  modernity: Rethinking Critical Discourse Analysis.  Edinburgh University Press, Edinburgh.   [12] Clarence, S. L. (2013) Enabling cumulative knowledge- building through teaching: A legitimation code theory   analysis of pedagogic practice in law and political science.  Thesis: Rhodes University, Grahamstown, South Africa.   [13] Corrin, L. and de Barba, P. (2014) Exploring students  interpretation of feedback delivered through learning  analytics dashboards. In Proceedings of the 31st Annual  Conference of the Australasian Society for Computers in   Learning in Tertiary Education (ascilite 2014), Dunedin.  [14] Corrin, L. and de Barba, P. (2015) How do students interpret   feedback delivered via dashboards In Proceedings of the  Fifth International Conference on Learning Analytics And   Knowledge. http://dx.doi.org/10.1145/2723576.2723662  [15] Dawson, S., Gaevi, D., Siemens, G. and Joksimovic, S.   (2014) Current state and future trends: A citation network  analysis of the learning analytics field. In Proceedings of the  Fourth International Conference on Learning Analytics And   Knowledge, Indianapolis.  http://dx.doi.org/10.1145/2567574.2567585   [16] Gaevi, D., Dawson, S. and Siemens, G. (2015) Lets not  forget: Learning analytics are about learning. TechTrends,  59(1), 64-71. http://dx.doi.org/10.1007/s11528-014-0822-x   [17] Gaevi, D., Dawson, S., Rogers, T. and Gasevic, D. (2016)  Learning analytics should not promote one size fits all: The     effects of instructional conditions in predicting academic  success. The Internet and Higher Education, 28, 68-84.  http://dx.doi.org/10.1016/j.iheduc.2015.10.002   [18] Howard, S. K., Chan, A. and Caputi, P. (2015) More than  beliefs: Subject areas and teachers integration of laptops in  secondary teaching. British Journal of Educational  Technology, 14(2), 360-369.  http://dx.doi.org/10.1111/bjet.12139   [19] Jackson, D. (2014) Knowledge and knowers by Karl Maton:  A review essay. Journal of Education, 59, 127-146.   [20] Knight, S. and Littleton, K. (2015) Discourse-centric  learning analytics: Mapping the terrain. Journal of Learning  Analytics, 2(1), 185-209.   [21] Knight, S. J., Buckingham Shum, S. and Littleton, K. (2014)  Epistemology, assessment, pedagogy: Where learning meets  analytics in the middle space. Journal of Learning Analytics,  1(2), 23-47.   [22] Kop, R. and Fournier, H. (2011) New dimensions to self- directed learning in an open networked learning  environment. International Journal of Self-Directed  Learning, 7(2), 2-20.   [23] Kress, G. and van Leeuwen, T. (1990) Reading images.  Deakin University Press, Geelong, Victoria.   [24] Linell, P. (2009) Rethinking language, mind, and world  dialogically. Information Age Publishing, Charlotte, NC.   [25] Linell, P. (2014) Interactivities, intersubjectivities and  language: On dialogism and phenomenology. Language and  Dialogue, 4(2), 165-193.  http://dx.doi.org/10.1075/ld.4.2.01lin   [26] Liu, D. Y. T., Rogers, T. and Pardo, A. (2015) Learning  analytics - are we at risk of missing the point In  Proceedings of the 32nd ascilite conference, Perth.   [27] Maton, K. (2014) Knowledge and knowers: Towards a  realist sociology of education. Routledge, London.   [28] Maton, K., Carvalho, L. and Dong, A. (2016). LCT in praxis:  Creating an e-learning environment for informal learning of  principled knowledge. In K. Maton, S. Hood and S. Shay  (Eds.), Knowledge-building: Educational studies in  Legitimation Code Theory. London: Routledge.   [29] McPherson, J. (2014) Comparing apples with apples:  Professional accounting practices in university classroom   discourse. Thesis: University of Sydney, Sydney.  [30] Miller, P. (1994). Accounting as social and institutional   practice: An introduction. In A. G. Hopwood and P. Miller  (Eds.), Accounting as social and institutional practice (pp. 1- 39). Cambridge: Cambridge University Press.   [31] Mkhize, T. (2015) An Analysis of the Certificate of the  Theory of Accounting Knowledge and Knower Structures: A   case study of professional knowledge. Thesis: Rhodes  University, South Africa.   [32] Morgan, G. (1988) Accounting as reality construction:  Towards a new epistemology for accounting practice.  Accounting, Organizations and Society, 13(5), 477-485.  http://dx.doi.org/10.1016/0361-3682(88)90018-9   [33] Newland, B., Martin, L. and Ringan, N. (2015) Learning  analytics in UK HE 2015: A HeLF survey report: HeLF   Heads of e-Learning Forum. Available:  http://www.helf.ac.uk/    [34] Pennycook, A. (2010) Language as a local practice.  Routledge, London.   [35] Scardamaila, M. and Bereiter, C. (2006). Knowledge  building: Theory, pedagogy and technology. In K. Sawyer  (Ed.), Cambridge handbook of the learning sciences (pp. 97- 118). New York: Cambridge University Press.   [36] Sclater, N. (2015) What do students want from a learning  analytics app Available:  http://analytics.jiscinvolve.org/wp/2015/04/29/what-do- students-want-from-a-learning-analytics-app/   [37] Sharples, M., Adam, A., Ferguson, R., Gaved, M.,  McAndrew, P., Rienties, B., Walker, M. and Whitelock, D.  Innovating Pedagogy 2014: Open University Innovation   Report 3. The Open University, Milton Keynes, 2014.   [38] Shay, S. (2015) Curricula at the boundaries. Higher   Education, 1-13. http://dx.doi.org/10.1007/s10734-015-9917- 3   [39] Siemens, G. and Gaevi, D. (2012) Guest editorial-learning  and knowledge analytics. Journal of Educational Technology  & Society, 15(3), 1-2.   [40] Stehr, N. (1994) Knowledge Societies: The Transformation  of Labour, Property and Knowledge in Contemporary   Societies. Sage, London.  [41] Van Krieken, R., Smith, P., Habibis, B., Smith, P., Hutchins,   B., Martin, G. and Maton, K. (2010) Sociology: Themes and  perspectives. Pearson, Sydney.   [42] Wolff, A., Zdrahal, Z., Nikolov, A. and Pantucek, M. (2013)  Improving retention: predicting at-risk students by analysing  clicking behaviour in a virtual learning environment. In  Proceedings of the 3rd international conference on learning   analytics and knowledge, Leuven.  http://dx.doi.org/10.1145/2460296.2460324             "}
{"index":{"_id":"21"}}
{"datatype":"inproceedings","key":"Mavrikis:2016:DET:2883851.2883909","author":"Mavrikis, Manolis and Gutierrez-Santos, Sergio and Poulovassilis, Alex","title":"Design and Evaluation of Teacher Assistance Tools for Exploratory Learning Environments","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"168--172","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883909","doi":"10.1145/2883851.2883909","acmid":"2883909","publisher":"ACM","address":"New York, NY, USA","keywords":"exploratory learning, teacher assistance tools","abstract":"We present our approach to designing and evaluating tools that can assist teachers in classroom settings where students are using Exploratory Learning Environments (ELEs), using as our case study the MiGen system, which targets 1114 year old students' learning of algebra. We discuss the challenging role of teachers in exploratory learning settings and motivate the need for visualisation and notification tools that can assist teachers in focusing their attention across the whole class and inform their interventions. We present the design and evaluation approach followed during the development of MiGen's Teacher Assistance tools, drawing parallels with the recently proposed LATUX workflow but also discussing how we go beyond this to include a large number of teacher participants in our evaluation activities, so as to gain the benefit of different view points. We discuss the results of the evaluations, which show that participants appreciated the capabilities of the tools and were mostly able to use them quickly and accurately.","pdf":"Design and Evaluation of Teacher Assistance Tools for  Exploratory Learning Environments  Manolis Mavrikis  London Knowledge Lab  UCL Institute of Education  m.mavrikis@ucl.ac.uk  Sergio Gutierrez-Santos  London Knowledge Lab  Birkbeck, Univ. of London  sergut@dcs.bbk.ac.uk  Alex Poulovassilis  London Knowledge Lab  Birkbeck, Univ. of London  ap@dcs.bbk.ac.uk  ABSTRACT We present our approach to designing and evaluating tools that can assist teachers in classroom settings where students are using Exploratory Learning Environments (ELEs), us- ing as our case study the MiGen system, which targets 11- 14 year old students learning of algebra. We discuss the challenging role of teachers in exploratory learning settings and motivate the need for visualisation and notification tools that can assist teachers in focusing their attention across the whole class and inform their interventions. We present the design and evaluation approach followed during the develop- ment of MiGens Teacher Assistance tools, drawing parallels with the recently proposed LATUX workflow but also dis- cussing how we go beyond this to include a large number of teacher participants in our evaluation activities, so as to gain the benefit of dierent view points. We discuss the results of the evaluations, which show that participants ap- preciated the capabilities of the tools and were mostly able to use them quickly and accurately.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3 [Computers and Education]: Computer Uses in Educa- tion - Computer assisted instruction, computer managed  Keywords teacher assistance tools, exploratory learning  1. INTRODUCTION  Design and evaluation of learning analytics tools targeted at the teacher is not an easy task and, as discussed in [9], re- quires the adoption of interdisciplinary techniques and meth- ods. We present here our approach to designing and evalu- ating tools that can assist teachers in a classroom where stu- dents are using Exploratory Learning Environments (ELEs). Examples of ELEs include simulators, microworlds, virtual  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom   2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883909  labs, and educational games, all of which give considerable freedom to students to learn in a variety of dierent ways.  The role of teachers in such an exploratory learning set- ting is that of a facilitator, or orchestrator [11, 3]. This role would be relatively easy in one-to-one student-tutor in- teraction, but scaling it up to the number of students present in a typical classroom poses several challenges, further com- pounded by the use of technology [3]. Given the open-ended nature of the tasks that the students are working on, teach- ers can only be aware of what a small number of students are doing at any one time as they walk around the classroom. The computer screens of students who are not in their imme- diate vicinity are typically not visible to the teacher, and it is therefore hard to know which students are making progress, which are o-task, and which are in diculty and in need of additional support.  Our case study here is the MiGen system (see migen.org), which includes an intelligent microworld called eXpresser, designed to support 11-14 year old students development of algebraic ways of thinking. As part of the MiGen sys- tem, we have designed a suite of visualisation and notifica- tion tools, which we refer to as the Teacher Assistance (TA) tools, whose aim is to assist teachers in focusing their at- tention across the whole class as students are working with eXpresser, and to inform teachers own interventions.  Earlier work has described the architectural design and implementation of the TA tools, focusing specifically on one tool, the Student Tracking (ST) tool [6]. In contrast, the present paper presents the design and evaluation approach we followed for the whole suite of TA tools. Our evaluation approach resembles, retrospectively, the LATUX (Learning Awareness Tools User eXperience) workflow recently put forward for designing and deploying awareness tools in the classroom [9]. LATUX proposes an iterative process of five phases common in software engineering and user experience approaches  Problem Definition, Low and Higher-fidelity Prototyping, Pilot studies and Validation in-the-wild.  In this paper we go beyond the Validation in-the-wild stage. We discuss our eorts to ensure the quality of our TA tools and of the user experience, as well as identifying metrics that provide us with indicators of the usability of the TA tools and of our success in addressing the require- ments emerging from earlier stages. Extending the LATUX approach, we discuss our eorts to include a large number of teacher participants in our summative evaluation activi- ties, so as to gain the benefit of dierent view points, while recognising that this can be a costly and time-consuming eort.    The structure of the paper is as follows. Section 1 has introduced and motivated the research. Section 2 gives an overview of the context and functionalities of the MiGen system, and of related work in ELEs and support for the teacher. Section 3 discusses the methodology we have adopted in designing, developing and evaluating the TA tools. We also discuss teachers requirements from these tools, in the form of a set of usage scenarios. Section 4 describes the TA tools themselves, as well as a time-stop functionality that we developed specially for facilitating user evaluation with large numbers of teacher participants. Sections 5 presents our evaluation. Section 6 discusses the results and gives our concluding remarks and directions for further research.  2. BACKGROUND AND RELATED WORK  2.1 The MiGen system and eXpresser The MiGen project has designed and developed an intel-  ligent, exploratory environment to support 11 to 14-year- old students learning of algebraic generalisation. Using the eXpresser mathematical microworld, students are asked to construct two-dimensional tiled models and associated alge- braic rules. In order to build a model, students need to cre- ate building blocks out of unit-square coloured tiles, based on their perception of the models structure, and to repeat each building block so as to create a pattern that forms part of their overall model. The algebraic rules they con- struct relate to the number of tiles of each colour required to paint each pattern and their model overall.  The eXpresser has an animation facility which allows students to explore the generality of their models and rules. This applies automatically dierent random values to the variables used by the student and displays the resulting in- stances of the model in a separate pane of their screen. Tasks are designed so as to contextualise students interaction with the eXpresser, and include a set of goals for students to achieve, e.g. check that your model is always coloured.  ELEs such as MiGens eXpresser have the potential to support students exploration while at the same time fos- tering progressive building of knowledge. For example, eX- presser has been specifically designed to support students with some well-known and well-researched challenges on learn- ing algebra. In particular, eXpresser has certain epistemic aordances aimed at enhancing students understanding of algebraic generalisation [12].  As students are undertaking the current task that they have been set using the eXpresser, a series of indicators are automatically detected by the system and stored in a database hosted on the MiGen Server (see [6] for details). The indicators that are meaningful and useful for teachers in their role in the classroom have been identified through an iterative process undertaken collaboratively with our group of teacher collaborators (see Section 3). There are two cat- egories of indicators: task independent (TI) and task depen- dent (TD). TI indicators refer to aspects of the students interaction that are related to the eXpresser microworld it- self and do not depend on the specific task the student is working on, e.g. placed a tile on the canvas, created a pattern. In contrast, the detection of TD indicators uses knowledge of the task that the student is working on and requires intelligent reasoning by the system, e.g. student has made a plausible building block for this task, student has coloured their pattern generally.  The TA Tools (see Section 4) receive real-time notifica- tions from the MiGen Server relating to occurrences of TI and TD indicators for each student, and each TA tool visu- alises a selection of this information to the teacher.  2.2 Related Work To our knowledge, MiGens TA tools represent the first  work targeted at providing teachers with information about students progress and state during exploratory learning ac- tivities in the classroom, notifying them of students attain- ment of key indicators, and aiming to inform the teachers own interventions in the class. This novelty of our TA tools has presented a number of methodological challenges, which we discuss in Section 3. In the past few years, several sim- ilar initiatives have appeared, including a recent approach that built on the work described here and that aimed to help teachers working with the Metafora platform and its associated tools, which target science and mathematics ed- ucation [4]. Most related work investigates students collab- orative interactions while collaborating at interactive table- tops [9] or other multi-touch technology(e.g. [14]). The trend towards teacher support is recently growing also in the learn- ing analytics community [5, 15] and there is high synergetic potential between that work and the work reported here. Other earlier initiatives include using Web log data gener- ated by course management systems (e.g. WebCT) to help instructors become aware of students activities in distance learning classes [13]; or class-wide collaborative activities supported by hand-held devices [2]. However, with the no- table exception of a few works, e.g. [1] that visualises stu- dents inferred plans in an ELE for chemistry, none of this work focuses on exploratory learning activities.  3. METHODOLOGY In our experience over many years of working with maths  teachers in participatory design and research projects, teach- ers are not accustomed to having access to tools such as MiGens TA tools when teaching. Their typical instinct is to walk around the classroom in order to monitor how in- dividual students are progressing and to help them. This makes it dicult, from the outset, to elicit from teachers a set of requirements for tools to support them. Instead, it was necessary to adopt an iterative participatory methodology, comprising successive phases of prototyping, requirements elicitation, incremental development, and evaluation.  Our earlier work, described in [6], presented the methods and outcomes of a first requirements elicitation and design phase for the TA tools. During this first phase, mockups and prototypes of possible visualisations for the TA tools were developed and discussed in several meetings of the projects Teacher Advisory group. As reported in [6], this advisory group comprised around 20 maths teachers and mathemat- ics educators from a broad spectrum of secondary schools in the greater London area, who attended regular project team meetings and gave their input throughout the project. However, the time that these teachers had available to actu- ally use prototypes of the tools in their classrooms was lim- ited, and so collaboration with a core group of four teachers who trialled the tools played a prominent role in subsequent phases. Two teachers in particular piloted successive limited prototypes of the tools in classroom sessions.  Given these challenges, to help us get even to a stage of meaningful problem identification and elicit teachers re-    quirements for the TA tools, we had to short-circuit the LATUX workflow and to perform a rapid iteration of the whole cycle with prototype tools that were a little more than low-fidely prototypes  they were not paper-based but fully functional (albeit limited in their scope). We referred to this approach as iterative context engineering (see [10]) because we engineered settings that gave teacher partici- pants the opportunity to experience first-hand what it would mean to have access to such TA tools and, therefore, to oer deeper insights in susbequent one-to-one interviews. Based on these interviews, but also through feedback from our gen- eral teacher advisory group, a set of Usage Scenarios for the whole suite of TA tools were identified and the tools were refined accordingly. These Usage Scenarios are as follows:  1. Finding out which students need the teachers imme- diate help.  2. Finding out which students are progressing satisfacto- rily towards completing the task and which students may be in diculty.  3. Finding out which students are currently disengaged from the task.  4. Identifying common conceptual and procedural di- culties that students are facing in order to provide more explanation to the class as a whole.  5. Finding out which students have finished the task.  6. Finding out which students have achieved which task goals.  7. Providing appropriate support and guidance to indi- vidual students: (i) during the lesson, and (ii) after the lesson.  8. Reflecting on the achievements of the class and plan- ning the next lesson.  The establishment of these usage scenarios informed sub- sequent development iterations and allowed us to orient the evaluation activities around them. In particular, we followed a process that resembles Stages 3 (higher-fidelity prototyp- ing), 4 (pilot) and 5 (validation-in-the-wild) of the LATUX workflow. Having held several focus groups, and undertaken small pilots, our challenge was to get input and evaluate the potential of the TA tools against these usage scenarios by a large number of teachers in a cost-eective way. Given the constraints of funded research and of the limited time that schools have for engaging in a research project, it was not possible to undertake a full cycle of using the whole MiGen system and the TA tools in the classroom over several hours. We therefore needed a methodology that would allow us to evaluate the tools on our premises with several people at a time but in a way that would provide the participants with a realistic experience of using the tools in the classroom. We present this approach and its results in Section 5.  4. THE TEACHER ASSISTANCE TOOLS MiGens suite of TA tools consists of the Student Tracking  (ST), Classroom Dynamics (CD), and Goal Achievements (GA) tools. A fourth tool  the Grouping Tool [8]  is not discussed here as it does not relate directly to monitoring students activities and progress.  The ST tool is the most detailed TA tool, and the one developed first chronologically. It monitors the occurrence  of task-independent and task-dependent indicators gener- ated by each student as they interact with the eXpresser. Coloured indicators are displayed in chronological order in a top-down timeline for each student. Green/red indicators show productive/problematic interaction with respect to the task set. Yellow indicators show that the students interac- tion may be positive or negative depending on context. Blue indicators relate to feedback provided to the student by the system. The teacher can select which indicators should be shown and which hidden, depending on her current needs. For reasons of space, we refer the reader to [6] for further description and screenshots of the ST tool.  The CD tool gives the teacher an at-a-glance overview of which students are currently engaged with the task and which may be in diculty and in need of help from the teacher (see Figure 1, left-hand side). It represents each student present in the classroom by a colour-coded circle, containing the students initials. Hovering over a circle with the cursor displays the students full name. Clicking on a circle shows the students current model and rule (see Fig- ure 1, right-hand side).  Figure 1: Class Dynamics tool. On the left, a classroom with the students sitting at tables. On the right, a U-shaped classroom; in this case, the teacher has clicked on one of the students to see their model and rule. The colour of a students circle reflects the students current activity status as perceived by the system. Green circles show students working productively on the task set. Amber circles show students who have not interacted with eXpresser for some time (by default, five minutes). Red circles show students who may benefit from immediate help.  In the CD tool, the circles representing the students can be dragged and moved around on the canvas. This enables teachers to set up the display so that the position of the circles matches the students spatial positions in the class- room. This helps the teacher to match the information being displayed in the CD tool with her own observations. It also helps the teacher to identify situations that may be location- dependent. For example, if several students seated at the same table show as Amber this may indicate that they are distracting each other.  The GA tool shows a tabular display of students and task goals (see Figure 2). Each row of the table shows the progress of one student (identified by their initials) in com- pleting the task goals. Each column shows the completion status of one task goal across all students. Hovering over a cell with the cursor displays a full description of the goal, the name of the student, and the achievement status of that goal for that student.  A cross-tool functionality we term time-stop is supported    Figure 2: Goal Achievement tool. Green and white cells show whether a goal has been achieved or not. Amber shows that the goal has been achieved earlier but the students current construction is not meeting the goal criteria.  by all the TA tools. It allows the user to select a specific point in time, t, with respect to which the ST, CD and GA visualisations are generated. The tools ignore all indicator occurrences after that time point, allowing analysis of the classroom situation at that particular time. If the time point t selected is in the future, or if no time point is selected, the tools show the current situation by default.  5. EVALUATION As mentioned in Section 3, early stages of our design  approach established several usage scenarios through fully functional but limited prototypes piloted with a small group of teachers. Having undertaken these pilots, and having tested the TA tools thoroughly, we engaged first in a higher fidelity prototyping and focus groups with several teachers. After taking into account participants feedback, we then engaged in a small pilot and subsequently a validation in- the-wild classroom-based trial. This trial involved one of our teacher collaborators at her school and aimed at allow- ing her to compare the dierence in the teachers experience compared to a lesson in which she did have use of the TA tools. We present the results in [7].  In order to get input from several teachers and to further evaluate the tools we held a 2-hour session with a cohort of 11 Maths teachers on the PGCE programme at the Institute of Education. Each participant had an installation of the MiGen system running on their computer. In the first half of the session, they were introduced to the MiGen system as a whole, the eXpresser, and the TA tools. They were then asked to work through several construction examples using eXpresser so as to gain familiarity with how students would use it in a lesson and the kinds of feedback the system would give to students. In the second half of the session, each of the TA tools was introduced to the participants, using real data drawn from a previous classroom pilot. Participants were asked to use the TA tools and the time-stop functionality to answer the following questions relating to Usage Scenarios 16 at dierent time points in the lesson, simulating the use of the tools in an actual classroom.   Q1. The session started 10 minutes ago (10 minutes into the lesson). If you chose a student to help imme- diately, which student(s) would you choose and why   Q2. Based on your experience and previous sessions, you would have expected by now (10 min. on) that students have achieved at least two goals. With a quick glance of the tools would you say that the class is going according to plan or would you intervene and why   Q3. We are at 30 minutes on. Based on your experi- ence and previous sessions, you expected that students would have finished by now so that you can progress on the next task. With a quick glance of the tools do you think that the class is at that stage and why   Q4. Sometimes students are o-task (e.g. play games). At 30 minutes on, find two students that are disen- gaged/distracted.   Q5. We are at 30 minutes. Some students need help and you are trying to identify others who have finished and can help them. Can you give two examples of students who have finished  Participants were also asked how long they thought it would take them to answer these questions in an actual les- son, our aim being not only to determine if participants were able to use the TA tools to answer the questions correctly, but also how they perceived the amount of time that it would take them to answer the questions in a classroom situation.  All participants provided correct answers to all questions without any assistance from the research team. The graph in Figure 3 summarises their responses relating to the per- ceived length of time required to answer each question. We see that for all the questions no participant responded a long or a lot of time. The questions regarded as requiring the least time to answer were Questions 1, 4 and 5, most proba- bly because they pertain to individual students and could be answered by consulting just one tool (the CD tool). Ques- tions 2 and 3 may have appeared to participants as needing more time to answer because they refer to the classroom as a whole and because, in order to answer them, partici- pants may have consulted the GA too, and also the ST tool in Question 2 for a more detailed view of how students are progressing with their constructions.  Figure 3: Participants perceived time to answer evaluation questions  On the whole, we consider these responses as being en- couraging, particularly as no question was perceived as re- quiring a long or a lot of time, which was a key aim in the design of the TA tools.    6. DISCUSSION AND CONCLUSION In this paper, we have described a suite of Teacher Assis-  tance (TA) tools that target an exploratory learning setting, and their summative evaluation. Our overall approach re- sembles and validates, retrospectively, the LATUX workflow but also elaborates on a possible extension of the validation- in-wild stage. Our particular methodological contribution was to extend the number of teacher participants and the type of data that can be gathered to evaluate tools such as our TA tools. The provision of time-stop functionality across all of the TA tools allowed us to use real interaction data from earlier classroom trials and to present these data to several participants via the TA tools frozen at particu- lar moments in time, simulating in this way the experience of using the tools in an actual classroom. This approach helped us conduct evaluations with a far greater number of teachers than those who would ever be able to participate in classroom trials either due to their constraints or to limited resources on our behalf. As such we consider it a type of poor mans evaluation at scale.  The evaluation results are encouraging. The results show that teachers exposed to the TA tools understand the tools capabilities and are able to use them eectively in answer- ing most of the usage-scenario based questions. Answering most of the evaluation questions was relatively speedy, which achieves one of our objectives and is a key requirement for using the TA tools in a real classroom.  The TA tools presented in this paper are general in their design and similar tools could be used to monitor the activ- ities of students interacting with other exploratory learning environments, provided that the environment detects appro- priate interaction indicators. This would need to include, as a minimum, indicators relating to students current activity status, waiting for help from the teacher, and goal achieve- ments status: these are the indicators that drive the CD and GA tool visualisations which we have found that, in practice, teachers consult most often during a lesson.  Having received more feedback on the TA tools from teach- ers since they were made more widely available, it is evident that tools such as these allow teachers to use ELEs in the classroom in ways that were not possible before. Visualisa- tion and notification tools that are developed specifically to support the teacher in the classroom are better able to pro- vide a sense of awareness than other general-purpose screen monitoring tools. Moreover, with the increased emphasis on evidence-based teaching, such TA tools empower teach- ers to provide evidence of learning, even in a context that is less subject to formal assessment, and to engage in their own inquiry into more conceptual student learning. Our fu- ture work involves investigating how the TA tools could be adapted to support teachers in even more complex learning scenarios, such as blended learning, and in their own profes- sional development as teachers.  7. REFERENCES [1] O. Amir and K. Gal. Plan recognition and  visualization in exploratory learning environments. ACM Transactions on Interactive Intelligent Systems, 3(3), 2013. DOI: 10.1145/2533670.2533674.  [2] C. Cortez, M. Nussbaum, G. Woywood, and R. Aravena. Learning to collaborate by collaborating: a face-to-face collaborative activity for measuring and  learning basics about teamwork. Journal of Computer Assisted Learning, 25(2):126142, Apr. 2009.  [3] P. Dillenbourg. Design for classroom orchestration. Computers & Education, 69:485  492, 2013.  [4] T. Dragon, M. Mavrikis, B. McLaren, A. Harrer, C. Kynigos, R. Wegerif, and Y. Yang. Metafora: A web-based platform for learning to learn together in science and mathematics. IEEE TLT, 6(3), 2013.  [5] R. C. Garcia, A. Pardo, C. D. Kloos, K. Niemann, M. Scheel, and M. Wolpers. Peeking into the black box: visualising learning activities. International Journal on Technology Enhanced Learning, 4(1/2):99120, 2012.  [6] S. Gutierrez-Santos, E. Geraniou, D. Pearce-Lazard, and A. Poulovassilis. Design of Teacher Assistance Tools in an Exploratory Learning Environment for Algebraic Generalization. IEEE Transactions on Learning Technologies, 5(4):366376, 2012.  [7] S. Gutierrez-Santos, M. Mavrikis, G. E., and A. Poulovassilis. Usage scenarios and evaluation of teacher assistance tools for exploratory learning environments (under review). Technical report, 2012. Available at http://www.dcs.bbk.ac.uk/research/ techreps/2012/bbkcs-12-02.pdf.  [8] S. Gutierrez-Santos, M. Mavrikis, E. Geraniou, and A. Poulovassilis. Similarity-based grouping to support teachers on collaborative activities in exploratory learning environments. IEEE Transactions on Emerging Topics in Computing, in press.  [9] R. Martinez-Maldonado, A. Pardo, N. Mirriahi, K. Yacef, J. Kay, and A. Clayphan. The LATUX workflow: Designing and deploying awareness tools in technology-enabled learning settings. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 110, 2015.  [10] M. Mavrikis et al. Iterative context engineering to inform the design of intelligent exploratory learning environments for the classroom. In R. Luckin et al., editors, Handbook of Design in Educational Technology, pages 8092. Routledge, 2013.  [11] M. Mavrikis, S. Gutierrez-Santos, E. Geraniou, and R. Noss. Design requirements, student perception indicators and validation metrics for intelligent exploratory learning environments. Personal and Ubiquitous Computing, 17:16051620, 2013. DOI: http://dx.doi.org/10.1007/s00779-012-0524-3.  [12] M. Mavrikis, R. Noss, C. Hoyles, and E. Geraniou. Sowing the seeds of algebraic generalization: designing epistemic aordances for an intelligent microworld. Journal of Computer Assisted Learning, 2012.  [13] R. Mazza and V. Dimitrova. CourseVis: A graphical student monitoring tool for supporting instructors in web-based distance courses. International Journal of Man-Machine Studies, 65(2):125139, 2007.  [14] E. Mercier, G. Vourloumi, and S. Higgins. Student interactions and the development of ideas in multi-touch and paper-based collaborative mathematical problem solving. BJET, 2015.  [15] V. A. R. Zaldivar, A. Pardo, D. Burgos, and C. D. Kloos. Monitoring student progress using virtual appliances: A case study. Computers & Education, 58(4):10581067, 2012.    "}
{"index":{"_id":"22"}}
{"datatype":"inproceedings","key":"Oster:2016:LAR:2883851.2883925","author":"Oster, Meghan and Lonn, Steven and Pistilli, Matthew D. and Brown, Michael G.","title":"The Learning Analytics Readiness Instrument","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"173--182","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883925","doi":"10.1145/2883851.2883925","acmid":"2883925","publisher":"ACM","address":"New York, NY, USA","keywords":"ethics, higher education, learning analytics, readiness, reflection, survey design","abstract":"Little is known about the processes institutions use when discerning their readiness to implement learning analytics. This study aims to address this gap in the literature by using survey data from the beta version of the Learning Analytics Readiness Instrument (LARI) [1]. Twenty-four institutions were surveyed and 560 respondents participated. Five distinct factors were identified from a factor analysis of the results: Culture; Data Management Expertise; Data Analysis Expertise; Communication and Policy Application; and, Training. Data were analyzed using both the role of those completing the survey and the Carnegie classification of the institutions as lenses. Generally, information technology professionals and institutions classified as Research Universities--Very High research activity had significantly different scores on the identified factors. Working within a framework of organizational learning, this paper details the concept of readiness as a reflective process, as well as how the implementation and application of analytics should be done so with ethical considerations in mind. Limitations of the study, as well as next steps for research in this area, are also discussed.","pdf":"The Learning Analytics Readiness Instrument  Meghan Oster   University of Michigan  Ann Arbor, MI 48109   USA  omeghan@umich.edu   Steven Lonn  University of Michigan   Ann Arbor, MI 48109   USA  slonn@umich.edu    Matthew D. Pistilli  Indiana University Purdue University   Indianapolis  Indianapolis, IN 46202   USA  mpistill@iupui.edu  Michael G. Brown  University of Michigan  Ann Arbor, MI 48109   USA  mbrowng@umich.edu  ABSTRACT  Little is known about the processes institutions use when  discerning their readiness to implement learning analytics. This  study aims to address this gap in the literature by using survey  data from the beta version of the Learning Analytics Readiness  Instrument (LARI) [1].  Twenty-four institutions were surveyed  and 560 respondents participated. Five distinct factors were  identified from a factor analysis of the results: Culture; Data  Management Expertise; Data Analysis Expertise;  Communication and Policy Application; and, Training.  Data  were analyzed using both the role of those completing the  survey and the Carnegie classification of the institutions as  lenses. Generally, information technology professionals and  institutions classified as Research UniversitiesVery High  research activity had significantly different scores on the  identified factors. Working within a framework of  organizational learning, this paper details the concept of  readiness as a reflective process, as well as how the  implementation and application of analytics should be done so  with ethical considerations in mind. Limitations of the study, as  well as next steps for research in this area, are also discussed.   Categories and Subject Descriptors  J.1 [Administrative Data Processing]: Education    General Terms  Measurement, Reliability, Experimentation, Standardization    Keywords  Learning Analytics, Readiness, Survey Design, Higher  Education, Reflection, Ethics    1. INTRODUCTION  The Learning Analytics Readiness Instrument (LARI) [1] was  created to address the space between the various inventories of  analytics tools available and the EDUCAUSE Maturity Index  [3], which measures the extent to which institutions have   implemented analytics systems. These inventories simply  informed institutions about what could be used while the  Maturity Index assumed that institutions had an analytics system  in place. These resources left an important gap: a diagnostic tool  that could inform administrators about the various components  required of analytics systems that should be in place and/or  considered to ensure as strong an implementation as possible.  Arnold, Lonn, and Pistilli [1] combined their experiences  working with various systems and institutions with their  individual research on the field and began to narrow a list of  requirements that would help ensure that learning analytics,  when used at an institution, could reach its potential and desired  effects. The original (alpha; [1]) analysis of the tool and its  outcomes indicated that the LARI had a promising future, but  required a deeper analysis at scale and additional specificity for  participating institutions. After completing the alpha analysis,  the team continued to refine the tool and developed a beta  version. This paper presents the results from the beta LARI and  sets the stage for how institutions can utilize the tool to  responsibly and successfully implement learning analytics.    1.1 Readiness as a Reflective Process  The process of adopting analytics should, ideally, begin with  self-reflection. As Arnold et al. [1] write, it is imperative that  institutions considering [the implementation of] learning  analytics reflect upon their readiness to do so (p. 263). They  continue, indicating that multiple facets of an institution should  be involved, creating a culture of awareness of and acceptance  for learning analytics, as well as having a shared vision for  support of student success (p. 264). Swenson [32], then, puts  this cultural importance into a broader process, examining who  has the power to:    make decisions about the learning analytics model and data;    legitimize some student knowledge or data and not others;    focus on potential intervention strategies and not others;    give voice to certain students and not others; and,    validate some student stories and not others (p. 249).  Norris, Baer, and Offerman [23] go further, discussing the need  for taking information learned through analytics efforts and  putting it into action. Specifically, they note that implementing  analytics is more about leading and navigating significant  changes in organizational culture and behavior than acquiring  technology solutions (p. 1). Baepler and Murdoch [2] extend  this thinking, stating that the use of analytics is strategic in  nature, and points to the need for early [collection of] student  data, prompt analysis, and immediate access by students, faculty,  and advisors who can make smart choices to influence learning  (p. 1). In addition, Baepler and Murdoch emphasize the need for  the application of analytics to bring about change within the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom   Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883925       classroom and for the students. Clow [7] argues, however, that  such change is difficult without closing the feedback loop in  which analytics interventions ultimately reach learners and their  instructors. Both of these components  the relatively rapid  collection, analysis, and dissemination of data and the use of  outcomes to change classroom environments and student (and,  potentially, faculty and staff) behavior  are reliant upon the  culture of an institution being one that is open to radical change  and a new form of influence.   According to Pistilli, Willis, and Campbell [24], the  institutional application of analytics can result in a major shift  for colleges and universities with regard to the culture fostered  around undergraduate learning (p. 88). They describe an  institutional environment where data is gathered from multiple  points, analyzed and provided to students as a form of feedback,  and where that feedback is also absorbed by instructors for  subsequent change on the instructors behalves. Once both  groups have had the opportunity to alter their behaviors,  members of the administration then have the ability to review  the various processes and changes and opt to implement them  further or continue to make tweaks to maximize the potential  positive change. Bonfliglio et al. [5], however, indicate that  moving an institution to one that is concerned about student  learning, rather than one focused on the instructor or the needs  of the university, requires a great deal of organizational  reflection and analysis.   1.2 Using Ethics as a Lens  Learning analytics as a field is relatively young, only having  been named by Goldstein and Katz in 2005 as academic  analytics [13] and later differentiated from similar terms [34].  However, even in its infancy, it was apparent that an ethical  framework was necessary in order for the field to not simply  grow, but to do so in a way that allowed for that growth to be  accepted by the broader higher education community. Swenson  [32] discusses this eloquently, calling for ethical actions that  [guide] learning analytics by the philosophy of use, the  motivation for use, and working towards a desire to do better  by the institution, the instructor, and the discipline as a whole  (p. 247). While written after the LARI was conceived, the  sentiments of examining the rationale and motivation behind  implementing analytics, along with improving an institutions  chances of successfully applying analytics, are precisely what  these authors set out to do.    The disruption that accompanied the advent of learning analytics  brought with it a need for institutions to better understand what  they were trying to achieve and how those goals would be  achieved through the actions of many key areas of each  institution. The ethics associated with the application of  learning analytics need to be examined, but Swensons [32]  point is that unless a similar ethical tack is taken with regard to  an institutions adoption of analytics, any processes may  become marginalized, if not forgotten altogether.   Beyond the practical components of implementing learning  analytics, the researchers in the current study focused on the  ethical implementation and use of analytics, informed by the  ethical frames proposed and discussed by Swenson [32], Willis,  et al. [37], and Slade and Prinsloo [31]. While it was imperative  that the application of analytics in an effort to improve student  success be done so ethically, the broad use of analytics  from  the rationale behind implementation to the actual  implementation itself  also should have an ethically-driven   approach. The focus is to teach people ethical considerations  associated with and driven by learning analytics [e.g., the ethical  considerations for stakeholders [15]), and subsequently re- inform the ethical application of analytics as the field matures.  Using the above ethical frames, it is apparent to these authors  that the concept of beneficence, or doing the greatest amount of  good for the greatest amount of people, is often behind both the  philosophy of analytics use and feedback provided to  institutions considering an investment in analytics. Having any  amount of beneficence in ones approach to this field requires a  great deal of readiness, a concept often looked at through  militaristic terms of being prepared to handle nearly any  contingency that might occur in the field of battle. In higher  education, while the consequences are not as dire, the sentiment  remains: in order to do something well, one must first be ready  to do it.   Ultimately, what is an ethical lens for readiness with regard to  learning analytics These authors describe readiness as a  necessary condition for institutions to be able to perform  educational functions consistent with their individual missions  and purposes, particularly towards student success. Furthermore,  it requires the appropriate management and application of  resources and personnel that results in the ability to effectively  educate students and enhance the higher education experience.  Finally, readiness needs to be considered as an open frame  one  that is transparent in nature and inclusionary in application.  Leaders at all levels of an institution need to be both consumers  and protectors of information learned and applied. Institutions  need to take care to treat student and faculty records and data  appropriately and ethically, and ensure that all are informed and  trained to the extent necessary to contribute to their educational  missions. The final  production  version of the LARI, based on  the results presented in this paper, will therefore provide  feedback to participating organizations that will not only reflect  the growth and stability considerations for each factor, but also  lenses appropriate for cultivating ethically attuned perspectives  with regard to learning analytics at each institution.   2. CONCEPTUAL FRAMEWORK  This work is largely informed by the literature on organizational  adaptation and organizational learning in postsecondary/higher  education institutions (HEIs). Within HEIs, structural units (i.e.,  academic departments, information technology units) often are  loosely coupled in an open system arrangement [4] [29]. While  alignment facilitates individual and organizational autonomy as  well as experimentation without major consequence to the rest  of the organization in instances of failure [35], diffusion from  one part of the institution occurs unevenly, sporadically, and  unpredictably [6]. This can hamper or silo new and rapidly- changing areas of innovation, particularly in cross-disciplinary  areas like learning analytics. Furthermore, the open arrangement  of HEIs often produces various decentralized authority and  decision-making structures [16] that could affect the governance  and access of key data sources.    HEIs, particularly large, research-oriented organizations,  possess multiple institutional missions [19] that can create  ambiguity for individual actors [18]. This ambiguity can be  exacerbated when an HEI faces changes in its external  environment [6], ultimately resulting in organizational  transformation [19]. Therefore, staff and faculty may be unclear  about if and how learning analytics relates to the institutional  mission(s) for student success, particularly if the institution is     adopting learning analytics in reaction to their external  environment (e.g., peer institutions adopting analytics  solutions). HEIs respond to complexity in their environment by  producing new complex local structures for task management  [6]. An institution might develop multiple analytics initiatives  that, if incongruent with one another, can result in contentious  misunderstandings, conflicts, and even power struggles over  values, beliefs, and implicit assumptions [16] (p. 8). These  contentious misunderstandings either persist, allowing for local  translations of practice that are idiosyncratic and fractured, or  organizational actors work across (real and perceived)  organizational boundaries to learn coordination. During this  phase, through direct experience actors learn and develop new  routines to manage the transformed organization [20]. Balancing  external constraints and internal limitations is critical,  particularly in rapidly changing domains such as learning  analytics [15]. Institutions may be able to circumvent the  process of local translation and cross boundary coordination by  focusing on their readiness to adopt a technology or practice  before implementation. Gathering stakeholder perceptions, an  institutional repertoire of current practices, and an assessment of  analytics-related skills is critical in determining whether an  institution has the capacity to proceed with implementation, or if  new skills are required to accommodate changes [13] [26] [38].   The authors frame the process of implementation as a series of  cycles of expansive organizational learning; one where  organizations and teams ascend from abstract and simple  explanatory relationships to a concrete system of multifaceted  objects and practices [12]. This model, in turn, is grounded in  the cultural-history theory of activity (i.e., activity theory) [9],  which delineates how actors, tools, goals, communities, rules,  and divisions of labor mutually constitute outcomes associated  with the central objects and practices. The expansive learning  model (Figure 1) consists of seven steps, or actions: 1)  Questioning, criticizing, or rejecting some aspects of the  accepted practice; 2) Analyzing the situation both historically  (origin & evolution) and empirically (inner systemic relations);  3) Modeling the new relationship in a public and shareable  medium; 4) Examining the new model by running, operating,  and experimenting with it; 5) Implementing the model in terms  of practical applications, enrichments, and conceptual  extensions; 6) Reflecting on and evaluating the new processes;  and, 7) Consolidating the outcomes of these actions into new  and stable practices. The LARI, as designed, attempts to capture  snapshots of institutional transition between each of these  activities as well as their development for capacity among each  crucial population of stakeholders. Given the dynamic nature of  learning analytics, expansive cycles of learning allow for the  inclusion and implementation of new techniques, tools, and  theories as they emerge. Within the activity system of expansive  organizational learning, multiple constituencies may be engaged  at different time points, levels of effort, and leadership.   3. METHODOLOGY   3.1 Survey Design  The LARI was originally designed based on a broad definition  of learning analytics utilizing four key principles: Institutions  should examine 1) rich, learning-related data sets, 2) as they are  exposed to various analytics techniques, 3) in an effort to  support teachers and/or learners, 4) as those populations move  toward intervention, action, and increased success [1]. This  approach, situating the survey instrument at the intersection of  institutional  big data  and student success, also acknowledged  that knowledge, understanding, and applicable technologies and  techniques for applying learning analytics would evolve over  time as the field matured and discourse about these issues  became more commonplace within higher education. The LARI  is therefore designed to facilitate an iterative and cyclical  organizational learning model to help move an institution along  its path of increasing learning analytics sophistication [30].   The expansive learning model allows for iterative cycles and   successive periods of local innovation and reorganization  [11]  (p. 323). Large cycles of expansion typically last several years,  particularly when many different units within an HEI are  involved. It is also unclear whether this model can support  sustained learning or is primarily useful for contexts in which  rapid change is occurring. For example, Engestrm et al. [12]  suggest that technological investments, teams (and  organizations) likely will face decisions more nuanced than yes,  no, or delay, and furthermore, in each successive cycle,  previously rejected innovations are likely to reappear in  modified forms. Thus, as new faculty members, administrators,  IT professionals, and others within an institution engage in the  learning analytics conversation on campus, new, revised, and/or  old practices are all likely to be revisited as the field matures and  expands. The LARI is designed to facilitate these learning cycles  by taking input from across an institution and providing  actionable feedback, benchmarked against similar institutions  that have faced similar challenges and decision points.   3.2 Survey Distribution Procedure  Originally, the LARI comprised 139 questions [1]. From the  feedback received from participants of the alpha survey and an  exploratory factor analysis, the LARI instrument was reduced to  90 items with factor loadings unique to each of the five factors,  and 55.7% of the variance explained. The 90-item LARI  possessed a Chronbachs alpha of 0.946, indicating a high level  of internal consistency. The factors identified in the alpha  version were: ability, data, culture & process, governance &  infrastructure, and overall readiness perception [1]. The 90-item  LARI (beta) was then distributed to a large group of  administrators, faculty, IT staff, students, and others engaged in  learning analytics at participating institutions to gather  additional feedback and further reduce the number of items  through an exploratory factor analysis. Because of the nature of  the survey, the authors sought out those associated with  institutions and/or familiar with successful implementations of   Figure 1: Expansive learning cycle (adapted from [11] p. 384)     learning analytics who were largely associated with a  consortium of research institutions in the American Midwest.  Individuals with other institutions also were targeted based on  their experience and knowledge.    The authors of the LARI beta survey used a convenience  sampling method to collect data for this version. The authors  solicited colleagues from a variety of institutions and asked for a  list of individuals at their respective institutions who were  involved in data initiatives. As a result of this method, the beta  survey respondents were faculty, staff, administrators, and  students who were knowledgeable of their individual  institutions readiness to implement a complicated data initiative  focused on student success, broadly defined at each participating  institution. The authors then emailed the identified individuals  and asked them to participate in the survey. Follow-up emails  were used to increase the response rate. The survey was  administered from August 2014 through October 2014 via  Qualtrics, an online survey tool.    3.3 Survey Participants  In total, 560 individuals from 24 institutions responded to the  survey (See Table 1 for a breakout of respondents). Overall,  response rates within institutions ranged from 50% to 100%.  The majority of institutional response rates were above 80%.  The average number of participants per institution was 23, with  a range of 12-67 survey respondents from individual  institutions. Most institutions have a Carnegie classification of  Research University-Very High research activity (RUVH). In  order to create categories of individuals' roles (i.e., primary job  category) that were large enough for statistical analysis, similar  roles were collapsed together. The groups that were collapsed  were initially analyzed to determine if they were statistically  different on the variables of interest using t-tests. No significant  differences were found between the roles that were collapsed. Of  the 560 respondents, 29.1% (n = 163) identified as academic  deans or faculty members, 25.0% (n = 140) identified as  institutional leaders or administrators, and 19.5% (n = 109)  indicated they were information technology (IT) professionals. It  should be noted that five of the 24 institutions in the current  LARI administration participated in the alpha as well. Because  factor analysis measures whether the same amount of variation  in the data can be represented equally  [or if there are]   ordered or patterned variations in the data, we believe that prior  participation did not measurably influence these results [27] (p.  16). Further, in the alpha analysis, 24 responses from the five  institutions were included, whereas the current analysis included  133 respondents from the five previously surveyed institutions.    3.4 Factor Analysis Procedure  In order to further explore the domains that were established in  the alpha version of the survey and continue to reduce the  numbers of items included in the survey, an exploratory factor  analysis using principal axis factoring technique was completed  on the data obtained from the beta survey administration. Only  items on a five-point scale with response rates greater than 70%  were included in the factor analysis, initially removing 15 items  (NB: Some questions had multiple components, resulting in  more than 90 items included in the exploratory analysis).    Since the alpha administration of the LARI, many, if not all,  HEIs have invested in data collection efforts and mechanisms  that track various data points throughout students time in  higher education [8]. This was evident in the fact that all  institutions involved in the beta administration earned a high  score on the data readiness factor. Because the concept of data  collection being a necessary component for the success of any  initiative at an institution has become a ubiquitous prerequisite  of nearly any effort, whether or not those efforts involve the  utilization of learning analytics, the authors removed these six  items as they were no longer relevant to learning analytics  readiness. Five items not on a Likert scale also were removed.  All other questions were coded such that 1 indicated the lowest  end of the scale while 5 indicated the highest.    The first factor analysis of 65 items would have retained 35  factors had the traditional cut-off of an eigenvalue of 1.0 been  used, while the screeplot indicated only two factors. Since the  screeplot and traditional cut-off level offered substantially  different conclusions, factors were explored by forcing items  into two, three, four, five, etc., factors in order to determine a  best-fit solution. After thoroughly exploring the factor analyses  and their factor loading scores, five factors produced the most  sensible solution with similar questions grouping together.    Once the data were forced into five factors, each rotated loading  matrix, using Promax rotation, was examined for items that did   Table 1: Number of Participants by Role and Institution Type   Role  Institution Type   Associate Institutions  (n=4)   Masters Institutions  (n=5)   Research University   Very High (n=15)   Total   (N=24)   Academic Dean/Faculty  29    (34.5%)  38    (28.6%)  96    (28.0%)  163    (29.1%)   Faculty Development  1    (1.2%)  5    (3.8%)  13    (3.8%)  19    (3.4%)  Institutional  Administrator/Leader   20   (23.8%)   29   (21.8%)   91   (26.5%)   140   (25.0%)   Institutional Researcher  4    (4.8%)  5    (3.8%)  26    (7.6%)  35   (6.3%)  Information   Technology Professionals   9   (10.7%)   23   (17.3%)   77   (22.5%)   109   (19.5%)   Student   Affairs Professionals   3   (3.6%)   6   (4.5%)   18   (5.3%)   27   (4.8%)   Other  3    (3.6%)  6    (4.5%)  18    (5.3%)  27    (4.8%)   Missing  9    (10.7%)  14    (10.5%)  8    (2.3%)  31    (5.5%)   Total 84  (100.0%)  133    (100.0%)  343    (100.0%)  560    (100.0%)  Note: Percentages sum by columns.     not load or cross-loaded. Promax rotation was used because it  was believed that the individual factors were correlated [39]. If  all rotated loading values were below |0.30| for a particular item,  it was determined that the item did not load [33]. It was  removed from the analysis, and the factor analysis executed  again, continuing to force the analysis into five factors. Once all  non-loading items were removed, the rotated loading matrix was  examined for cross-loading items, defined as having two or  more values that loaded above |0.30| in multiple factors [33].    After cross-loading items were identified, they were removed  from the factor analysis and the factor analysis performed again.  This process (examining for non-loading and cross-loading  questions) was completed until a solution was found that was  free of non-loading and cross-loading items. Through this  process, five factors were retained from items loading onto a  single factor with a values greater than |0.30| and no cross- loading items. The five retained factors illustrate underlying  domains of 1) culture, 2) data management expertise, 3) data  analysis expertise, 4) communication and policy application, and  5) training. These are explained further in section 4.    Finally, as a sensitivity check of the overall factor analysis, a  conditional factor analysis using principal axis factoring and  Promax rotation on the items included in the final factor analysis  was completed to ensure that the overall factor analysis fit each  institutional type and role. While some questions loaded onto  different factors, in general, the items were similarly grouped  within each institutional type and role. This analysis suggests  that the overall factor structure (Section 4) is stable for the  overall sample, each institutional type, and each respondent role.    4. RESULTS  4.1 Factor Analysis and Description  The final factor analysis contained 46 items with an overall  Cronbachs alpha of 0.956, indicating a high level of internal  consistency. The solution accounted for 87.4% of the variance.  Descriptive information for each factor is presented in Table 2.  The skewness indicates that all factors had a negatively skewed  distribution. Kurtosis indicates that the culture, communication  and policy application, and training factors have heavy tails  while the data management expertise and data analysis expertise  factors have approximately normal kurtosis. Additionally, the  Cronbachs alpha statistics indicate each individual factor has  high internal consistency. All five factors utilize a five-point  Likert scale from Strongly Disagree (1) to Strongly Agree (5).   4.1.1 Culture   The first factor, Culture (Eigenvalue = 15.57), measures  respondents' perceptions regarding the extent to the different  types of data collected at the institution, and views and  perceptions within the institution regarding data and data use.  An example question includes, My institution has a culture  that accepts the use of data to make decisions.    4.1.2 Data Management Expertise   The second factor, Data Management Expertise Factor  (Eigenvalue = 3.71), measures respondents' perceptions  regarding the institution's investment in data collection and  management, as well as in professional staff who possess skills  and experience related to data management. An example  question includes, My institution has the ability to store  increasingly large volumes of data.    4.1.3 Data Analysis Expertise   The third factor, Data Analysis Expertise (Eigenvalue = 2.16),  measures respondents' perceptions regarding professional staff at  the institution who possess skills and experience in areas critical  for successfully implementing necessary analyses, such as,   Institution has professionals with mathematical/statistical  experience in manipulating and transforming data and/or  variables in large, complex data sets.    4.1.4 Communication and Policy Application   The fourth factor, Communication and Policy Application  (Eigenvalue = 1.66), measures respondents' perceptions  regarding the extent to which the institution is able to effectively  communicate to stakeholders about learning analytics and work  within current policies.  For example,  Institution has  professionals with business acumen in marketing/publicity.    4.1.5 Training   The fifth factor, Training (Eigenvalue = 1.27), measures  respondents' perceptions regarding the extent to which the  institution had the resources to appropriately educate end-users  (of all levels) on learning analytics-powered reports and  applications. For example,  My institution has professionals  with customer-facing support experience in training diverse  constituents on the use of new systems.     4.2 Linear Regression Results  Ordinary least squares (OLS) regression was used to determine  the linear relationship between the independent variables  (institutional type and role) and the dependent variable (mean  score of each factor). All linear regressions models were  statistically significant. Table 3 provides these results.    4.2.1 Culture  For the culture factor, on average, academic deans and faculty  members along with institutional research (IR) professionals,  had significantly lower scores than IT professionals while  holding institutional type constant (p<0.01 each). Specifically,  IR researchers, on average, had culture scores a quarter of a  point (0.28) lower than IT professionals when holding  institutional type constant. Similarly, academic deans and  faculty members had, on average, culture scores that were also   Table 2: Descriptive Statistics for   Five Learning Analytics Readiness Factors   Factor   (count of items) Median Skewness Kurtosis   Culture   (n = 22)   3.27 -0.171 3.48 0.9169   Data  Management  Expertise   (n = 8)   3.75 -0.294 2.94 0.9017   Data Analysis  Expertise   (n = 6)   4.00 -0.369 2.85 0.9559   Communication  and Policy  Application   (n = 7)   4.00 -0.633 4.38 0.9180   Training  (n = 3)   3.67 -0.649 3.59 0.9140        approximately a quarter of a point lower (0.23) than IT  professionals, all else being equal. Regarding institutional type,  Associate institutions had, on average, a significantly lower  score by 0.25 on the culture factor than the RUVH institutions  (p<0.001). However, respondents from Masters institutions had  significantly higher culture scores than those from RUVH  institutions by 0.27 (p<0.001).    4.2.2 Data Management Expertise  Similar to the culture factor, academic deans and faculty, on  average, reported a significantly lower data management  expertise factor score than IT professionals, such that the  average data management expertise score for academic deans  and faculty was 0.38 points lower than IT professionals  (p<0.001). Further, Associate (p<0.001) and Masters  (p<0.001) institutions were significantly lower on the data  management expertise factor than RUVH institutions.  Specifically, respondents from Associate institutions were .0.83  points lower on data management expertise while respondents  from Masters institutions indicated only 0.15 points lower than  RUVH institutions while holding the role variable constant.    4.2.3 Data Analysis Expertise  Again for the data analysis factor, on average, academic deans  and faculty members, reported a significantly lower data analysis  expertise score than IT professionals, such that their average  score was 0.22 points lower than IT professionals (p<0.05)  Also, Associate and Masters institutions were significantly  lower, on average, on the data analysis expertise factor than  RUVH institutions (p<0.001 each). Specifically, respondents  from Associate institution gave scores that were almost a point   lower than RUVH institutions (0.85) and respondents from  Masters institutions gave scores that were 0.43 points lower  than RUVH institutions, all else being equal.    4.2.4 Communication and Policy Application  For the communication and policy application factor, academic  deans and faculty members gave scores that were just over four- tenths of a point (0.42) lower, on average, than IT professionals  (p<0.001). Additionally, on average, respondents from  Associate (p<0.001) and Masters institutions (p<0.05) reported  significantly lower scores on communication and policy  application than RUVH institutions, such that Associate  institution respondents provided scores that were slightly greater  than four-tenths of a point lower (0.43), and Masters institution  respondents provided scores that were fifteen-hundredths of a  point lower than RUVH institutions.    4.2.5 Training  For the training factor, on average, all roles were significantly  lower than IT professionals (p<0.05 each). Specifically,  academic deans and faculty members provided scores that were  0.85 points lower, on average, compared to IT professionals on  the training factors. Faculty development respondents provided  responses that were 0.43 points lower than IT professionals on  training. Institutional administrators and leaders gave responses  that were over a half a point lower, on average, than IT  professionals on training (0.62). Institutional researchers  provided responses that were almost a full point lower on  training than IT professionals, on average (0.85). Student affairs  professionals, on average, gave respondents that were 0.73  points lower than IT professionals, on training. Respondents in   Table 3: Linear Regression Models of Each Factor Score Mean Regressed on Role and Carnegie Classification Variables    Factors     Culture  Data   Management  Expertise   Data  Analysis  Expertise   Communication  and Policy   Application  Training   (n = 539) (n = 537) (n = 522) (n = 528) (n = 524)   Role (vs. IT professionals)   Academic deans & faculty -0.23** -0.38*** -0.22* -0.42*** -0.85***   Faculty development -0.15 -0.15 -0.02 -0.02 -0.43*   Institutional admin/leaders -0.09 -0.15 -0.12 -0.17* -0.62***   Institutional researchers -0.28** -0.16 0.1 0.18 -0.85***   Student affairs professionals -0.14 -0.21 -0.08 -0.23 -0.73***   Other -0.02 -0.2 -0.12 -0.19 -0.46*   Missing -0.19 -0.2 0.01 -0.32** -0.59***   Carnegie Classification (vs. Research Universities  Very High)   Assoc/Pub-R-L, Bac/A&S, Bac/Diverse -0.25*** -0.83*** -0.85*** -0.43*** -0.12   Master's L, RU/H 0.27*** -0.15** -0.43*** -0.15* -0.07           Intercept 3.37*** 4.05*** 4.19*** 4.33*** 4.10***   F 8.98*** 21.06*** 14.96*** 10.60*** 11.03***   R2 0.11 0.22 0.18 0.13 0.13   * = p<0.05, ** = p<0.01, *** = p<0.001.      the 'other' category provided responses that were almost half a  point lower than IT professionals (0.46). Finally, individuals  who did not identify their role provided training scores that were  0.59 points lower than IT professionals. However, neither  Associate institution nor Masters institution respondents were  significantly different than RUVH institution participants.   5. DISCUSSION  Before an institution can implement learning analytics, staff  have to collectively understand their institution's readiness to do  so. Unfortunately, while a great deal of knowledge about  learning analytics is beginning to emerge, very little is known  about institutional readiness to implement. This gap served as  the impetus for the LARIs development in 2014 [1]. After the  original survey was analyzed psychometrically and revised  appropriately, the LARI was administered in its beta form in an  effort to continue to validate the items, reduce the number of  items, and evaluate institutions perceived capacity to implement  learning analytics. This paper presented the analysis from the  beta administration of the LARI survey, and discusses the  subsequent factor analysis completed on data collected from on  a larger sample. Further, it allowed for a better understanding of  the association between the conceptual factors identified by the  factor analysis, individuals roles in an institution, and Carnegie  classifications of the institutions at which they worked. As such,  this study extends prior work describing actors and dimensions  of learning analytics growth within an institution [7] [15] [30].  The fact that learning analytics knowledge is emerging at a rapid  pace absent information about quality implementation and  scaling processes served as the basis to explore how different  institutions, as well as different individuals (and their primary  work responsibilities) within an institution, perceived their  individual and institutional capacity to effectively implement  learning analytics. As discussed above, all OLS regression  models were found to be significant. Of particular note,  academic deans and faculty members differed significantly from  IT professionals on all factors. Additionally, institutional  researchers had a significantly lower culture score than IT  professionals and institutional administrators/leaders, and the  missing category have significantly lower scores on the  communication and policy application factors. All roles had, on  average, scores on the training factor that were significantly  lower than IT professionals. Taken together, these findings  indicate that information technology professionals have a higher  probability of rating their institutions readiness to implement  learning analytics higher than other roles within the institution.  This suggests that members of IT departments have a perception  of institutional readiness that is removed and potentially  discordant with other members of the HEI community,  particularly deans, faculty, and institutional researchers. Given  IT professionals' daily interaction with institutional data in a  variety of collection, maintenance, and reporting functions, this  discrepancy is not entirely surprising. Yet, bridging these  institutional perceptions is critical to ensuring a cohesive and  collaborative implementation.     With regard to differences by institutional Carnegie  classification, Associate institutions and Masters institutions  had significantly lower scores on the data management  expertise, data analysis expertise, and communication and policy  application factors.  A lack of data management expertise and  analysis expertise at the Associate institution level is not  surprising; many institutions of this type lack the infrastructure   necessary to manage their own data systems [22], and few have  comprehensive institutional research (or equivalent) offices.  Interestingly, the Masters institutions scores, on average, were  significantly higher on the culture factor than the RUVH  institutions. This finding could indicate that smaller institutions  or institutions that are more focused on teaching, as compared to  RUVH institutions, could have a perceived culture to implement  learning analytics that is different from RUVH institutions.  More research is necessary to fully explore these trends as well  as a focus on how institutions communicate internally (and, to a  lesser extent, externally) concerning policies and practices  related to the use of learning analytics data and related tools.   Overall, these findings indicate that information technology  administrators and individuals at institutions classified as  RUVH rate their individual institutions capacity to implement  learning analytics higher than other roles and Carnegie  classifications.  These findings add to the understanding of how  learning analytics are implemented, because it now is evident  that researchers and practitioners need to understand that  institutional characteristics and faculty/staff roles are associated  differently with readiness. Ultimately, we can conclude that  individuals who plan to implement learning analytics must now  consider institutional and role characteristics across a variety of  domains before moving forward with their plans.     5.1 Research Implications  As stated throughout this section, understanding personnel and  institutional characteristics that are associated with the perceived  capacity to implement learning analytics is an important area of  research because little is actually known about the process of  learning analytics implementation, particularly the different  issues, policies, abilities, and supporting infrastructure across  different types of institutions. As the field continues to become  more prevalent in higher education (and education in general),  understanding the path to effective implementation will only  become more important. Scholars and practitioners should  continue to examine institutional characteristics that influence  readiness to implement learning analytics, particularly those  associated with smaller institutions, a population largely missing  from this study. Additionally, demographic characteristics of  those who participate in the survey should be examined more  closely. Adding control variables such as sex, race/ethnicity,  age, and length of time in current position would allow for a  more statistically controlled regression analysis. Researchers  should also evaluate the actual implementation of learning  analytics, and how that implementation manifests. In order to  compare an institutions capacity to implement learning  analytics and their actual success in implementing learning  analytics, institutions would need to first administer the LARI  and then implement learning analytics, at which point the  implementation could be evaluated. This analysis would confirm  or disconfirm the actual usefulness of the LARI.   5.2 Practice Implications  Gauging an institutions readiness to implement learning  analytics is an important step in the process of employing  learning analytics at an institution. Therefore, the validation of  the LARI through factor analysis is an important practical  implication. This tool is the first and only of its kind that can be  used by administrators within higher education to understand an  institutions readiness status with regard to implementing  learning analytics specifically (there are existing and newly     developed survey and focus group instruments by EDUCAUSE  and others that evaluate the analytics landscape broadly and how  HEIs are building institutional analytics maturity). Additionally,  the identified factors help institutions pinpoint the specific areas  where efforts should be focused as they begin work on setting  learning analytics initiatives in motion locally. Further, as  institutions use the LARI, it is important to consider their  institutional characteristics (which are difficult to change).  Because Carnegie classification was consistently related to  factors of perceived institutional readiness to implement  learning analytics, practitioners who use this survey need to be  mindful of how their results may be influenced by institutional  factors that are likely impossible to change.    5.2.1 Implications for Institutional Learning  The LARI instrument, as designed, is best suited to facilitate an  institutional conversation regarding learning analytics and how  best to implement learning analytics to meet institutional goals.  However, while there may be broad-based agreement on those  goals, an institutional culture that  understands and values data- informed decision-making processes  is a necessary prerequisite  before learning analytics systems can be deployed and  corresponding intervention points identified [8] (p. 3). This  supposition is confirmed by the LARI beta analysis presented in  this paper where culture emerged as the primary, and strongest,  factor when evaluating institutional readiness. Yet, it is also  relevant to acknowledge that 22 items comprise this factor,  representing a wide variety of topics related to data collection  and utility. Alignment and agreement across data and  institutional culture requires several cycles of expansive  organizational learning [11] where multiple constituencies have  the opportunity to question, examine, and reflect on these issues.    Just as educational researchers have long known that reflective  practices can promote knowledge building for both individuals  and groups of learners [28] by identifying deficiencies in  cognition, improving understanding, and identifying the  characteristics of good performance in a domain [36], the LARI  is designed to facilitate similar reflective practices within HEIs  in continual iterative cycles. It is reasonable for an institution to  self-assess using the LARI multiple times before deploying  learning analytics systems at an enterprise scale, particularly if  there was widespread skepticism or disagreement about the  institutional culture in the first distribution of the instrument.  Conversations, symposia, and pilot projects across an institution  would likely facilitate organizational learning within this rapidly  changing and still emergent domain [12]. In fact, the process of  defining, testing, and refining the LARI instrument itself reflects  the processes of reflection and expansive learning.    As the field of learning analytics has matured, so has the LARI  evolved to better reflect the state of the field and available  technological options for data management and visualization, as  well as a more nuanced approach to data analysis. Among other  evolutions in the LARI, it should be reiterated that the data  factor that was the largest component in the alpha analysis [1]  has fallen out of the instrument completely.    5.2.2 Scaffolding Institutional Learning  Organizational learning can be ambiguous and confusing across  roles in a HEI, particularly in circumstances where changes in  the external environment have precipitated organizational  transformation [6] [18]. To that end, the LARI beta  administration also included factor-level feedback to institutions  designed to facilitate productive planning and monitoring of   institutional-level reflective practices [25]. In essence, the  feedback was an initial attempt at providing scaffolding based  on the experiences and successes of other institutions' processes  and implementations of learning analytics. For example, the  researchers suggested that institutions whose scores indicated a  low readiness level for the culture factor initiate an educational  initiative aimed at raising awareness of the power of analytics  and data-driven decision making while simultaneously inviting  stakeholders from across the institution to engage in setting  definitions, policies and practices.    It is important that institutional feedback provided from the  LARI not only be normed to the relative size and capacities of  institutional types (using Carnegie classifications) but also  leverages the ethical lens utilized in creating and analyzing the  LARI. It is critical that the messages delivered to participating  institutions prompt leadership within the HEI to surface   philosophical assumptions, ideological perceptions, and  normative values underlying and/or guiding how people relate to  and exist with technology  [17] in general, and with data more  specifically. Furthermore, institutional scaffolds for learning  analytics readiness should facilitate the development of the  institutional interpretation of the  obligation of knowing   espoused by Willis et al. [37]  each institution needs to  interpret how such obligations should manifest for particular  audiences (e.g., students, faculty, administrators) within the  cultural and political norms of the institution and the current  understanding of learning analytics. Such interpretations will  undoubtedly evolve over time as institutional learning about  learning analytics progresses.    In this beta phase of the LARI project, many participating  institutions noted that this feedback was helpful and spurred  their institution towards informed and productive conversations  and decisions with regard to learning analytics strategy and  institutional orientation. As the LARI nears a state of full  production and automated feedback, the investigators will not  only reframe the feedback to match the revised factors, but also  ensure that these scaffolds can be updated as the field evolves.    5.3 Limitations  While the results for this administration are encouraging and  useful, this study is not without its limitations.  The data used  for this study did not comprise a nationally representative  sample of institutions, and not all Carnegie classifications were  represented. Further, within institutions, a representative sample  of roles was not surveyed.  Also, because the sample had a large  number of RUVH institutions, it is likely more representative of  that type of institution than any other within the sample.  Individuals also self-selected to participate in the survey,  presenting a potential bias in the perceptions from survey  respondents, particularly within those institutions that had a  comparatively low number of respondents.    This is also survey data that gauged the perceived ability of the  institution to implement learning analytics as determined by  several different individuals, not the actual ability of the  institution to implement an analytics solution. Even though  perception is an important factor in an institutions true ability  to implement a large, complex data project, it is not the only  factor that influences their ability to do so. Many unobserved  characteristics of the institutions that were not captured by the  survey exist, and these variables could have influenced the  regression models. These limitations indicate that these results  and implications should be used with caution.      5.4 Next Steps  The investigators believe that the LARI is now ready for  production deployment, and as such will have it hosted in a  custom web-based environment allowing multiple members of  an institution to respond to the reduced instrument much in the  same way the LARI was released in its alpha and beta forms.  Institutional results and corresponding factor- and item-level  feedback will be released to all survey respondents as discussed  in section 5.2.2. Institutional responses from this version of the  tool will be analyzed, and investigators will solicit periodic  input to improve feedback, particularly as new trends and  innovations emerge. Opportunities to investigate if and how  institutions use the feedback, how they ultimately implement  learning analytics, and if increased student success was realized  will be solicited as well.    6. CONCLUSION  Learning analytics  both as a field and as a suite of products   are only going to become more prevalent as time moves  forward, and this study aimed to answer the extent to which  institutional roles and Carnegie classifications are associated  with different levels of perceived readiness to implement  learning analytics. On average, IT administrators and institutions  classified as RUVH had higher scores on each of the identified  factors. These findings suggest that different roles within  institutions and different institution types are associated  varyingly with perceived levels of readiness to implement  learning analytics. Thus, when administrators begin to consider  learning analytics, the perceived capacity to successfully execute  a large, complex data initiative could be influenced by the roles  of individuals involved and the institutions characteristics.    Readiness is not an end point. Rather, these authors view  readiness as a measure of the participating institution to engage  or further engage in implementing, deploying, and  understanding learning analytics activities. Learning analytics  continues to be an emergent field with multiple disciplinary ties  to traditional areas of expertise (e.g., learning sciences, human- computer interaction, computer science) and other emergent  fields simultaneously operating in a state of rapid evolution  (e.g., data science). HEIs can ill afford to be complacent or  satisfied with early successes in learning analytics. To be   ready  requires continued investment and commitment to foster  experimentation and innovation for all institutions seeking to  positively impact student success in the 21st century.    7. ACKNOWLEDGEMENTS   The authors would like to acknowledge several parties that  contributed to the creation of this paper. First, Kim Arnold at  the University of Wisconsin  Madison was greatly involved in  the administration of the beta LARI and research surrounding  this paper. Second, the Bill & Melinda Gates Foundation  generously provided funding for much of this research under a  grant awarded to Purdue University. Third, Jacqueline Bichsel  and her colleagues at EDUCAUSE Center for Applied Research  allowed us to base the LARI off of the Maturity Index. Fourth,  many ideas for this work came from a 2013 EDUCASE  Learning Initiative session whose participants contributed to the  thinking surrounding the concept of readiness, as well as from  feedback received from presentations on the LARI in various  venues. Finally, faculty, staff, and students in the LED Lab at  the University of Michigan provided input and insight into the  formation of the instrument and institutional feedback.    8. REFERENCES   [1] Arnold, K. E., Lonn, S., & Pistilli, M. D. (2014). An   exercise in institutional reflection: The Learning Analytics  Readiness Instrument (LARI). In A. Pardo & S. Teasley  (Eds.), Proceedings from the 4th International Conference  on Learning Analytics and Knowledge (pp. 163-167). New  York: ACM. DOI: 10.1145/2567574.2567621   [2] Baepler, P., & Murdoch, C. J. (2010) Academic analytics  and data mining in higher education. International Journal  for the Scholarship of Teaching and Learning, 4(2), art. 17.  Available: http://digitalcommons.georgiasouthern.edu/ij- sotl/vol4/iss2/17   [3] Bichsel, J. (2012). Analytics in higher education: Benefits,  barriers, progress, and recommendations (research report).  Louisville, CO: EDUCAUSE Center for Applied Research.  Available:  http://net.educause.edu/ir/library/pdf/ERS1207/ers1207.pdf    [4] Birnbaum, R (1988). How colleges work: The cybernetics  of academic organization and leadership. San Francisco:  Jossey-Bass.   [5] Bonfiglio, R., Hanson, G. S., Fried, J., Roberts, G., &  Skinner, J. (2006). Assessing internal environments. In R.  P. Keeling (Ed.), Learning reconsidered 2: Implementing a  campus-wide focus on the student experience (pp. 4349).  Champaign, IL: Human Kinetics. Available: http://www.  myacpa.org/pub/documents/LearningReconsidered2.pdf   [6] Cameron, K. S. (1984). Organizational adaptation and  higher education. The Journal of Higher Education, 122- 144. DOI: 10.2307/1981182   [7] Clow, D. (2012). The learning analytics cycle: Closing the  loop effectively. In S. B. Shum, D. Gaevi, & R. Ferguson  (Eds.) Proceedings of the 2nd International Conference on  Learning Analytics and Knowledge (pp. 134-138). New  York: ACM. DOI: 10.1145/2330601.2330636   [8] ECAR-Analytics Working Group. The Predictive Learning  Analytics Revolution: Leveraging Learning Data for  Student Success. ECAR working group paper. Louisville,  CO: ECAR, October 7, 2015. Available:  http://net.educause.edu/ir/library/eli_so/ewg1510.pdf   [9] Engestrm, Y. (1987). Learning by expanding: An activity- theoretical approach to developmental research. Helsinki:  Orienta-Konsultit.   [10] Engestrm, Y., Engestrm, R., & Krkkinen, M. (1995).  Polycontextuality and boundary crossing in expert  cognition: Learning and problem solving in complex work  activities. Learning and Instruction, 5(4), 319-336.  DOI: 10.1016/0959-4752(95)00021-6   [11] Engestrm, Y., Kerosuo, H., & Kajamaa, A. (2007).  Beyond discontinuity: Expansive organizational learning  remembered. Management Learning, 38(3), 319-336. DOI:  10.1177/1350507607079032   [12] Engestrm, Y., Miettinen, R., & Punamki, R. L. (Eds.).  (1999). Perspectives on activity theory. New York, NY:  Cambridge University Press.   [13] Fisher, D., & Atkinson-Grosjean, J. (2002). Brokers on the  boundary: Academy-industry liaison in Canadian  universities. Higher Education, 44(3-4), 449-467. DOI:  10.1023/A:1019842322513     [14] Goldstein, P. J., & Katz, R. N (2005). Academic analytics:  The uses of management information and technology in  higher education. Littleton, CO: EDUCAUSE Center for  Applied Research. Available: http://www.educause.edu/ir/  library/pdf/ers0508/rs/ers0508w.pdf   [15] Greller, W., & Drachsler, H. (2012). Translating learning  into numbers: A generic framework for learning analytics.  Educational Technology & Society, 15 (3), 4257.   [16] Hubbard, L., Mehan, H., & Stein, M. K. (2006). Reform as  Learning: School reform, organizational culture, and  community politics in San Diego. New York: Routledge.   [17] Katz, S. B. & Rhodes, V. W. (2010). Beyond ethical  frames of technical Relations: Digital being in the  workplace world. In R. Spilka (Ed.), Digital Literacy for  Technical Communication (pp. 230-256). New York:  Routledge.   [18] Kezar, A. (2001). Understanding and facilitating  organizational change in the 21st century. ASHE-ERIC  higher education report, 28(4), 147.   [19] Kraatz, M. S. (1998). Learning by association  Interorganizational networks and adaptation to  environmental change. Academy of Management Journal,  41(6), 621-643. DOI: 10.2307/256961   [20] Levitt, B., & March, J. G. (1988). Organizational learning.  Annual Review of Sociology, 319- 340.    [21] MacDonald, G. P. (2013). Theorizing university identity  development: Multiple perspectives and common goals.  Higher Education, 65(2), 153-166. DOI: 10.1007/s10734- 012-9526-3   [22] Norris, D. M. & Baer, L. L. (2013) Building organizational  capacity for analytics. Louisville, CO: EDUCAUSE.  http://net.educause.edu/ir/library/pdf/PUB9012.pdf    [23] Norris, D., Baer, L., & Offerman, M. (2009). A national  agenda for action analytics. Available:  http://lindabaer.efoliomn.com/uploads/settinganationalagen daforactionanalytics101509.pdf    [24] Pistilli, M. D., Willis, III, J. E., & Campbell, J. P. (2014).   Analytics through an institutional lens: Definition, theory,  design, and impact. In J. A. Larusson & B. White (eds.),  Learning analytics: From research to practice (pp. 79- 101). New York: Springer Science + Business Media. DOI:  10.1007/978-1-4614-3305-7_5   [25] Quintana, C., Reiser, B. J., Davis, E. A., Krajcik, J., Fretz,  E., Duncan, R. G., et al. (2004). A scaffolding design  framework for software to support science inquiry. The  Journal of the Learning Sciences, 13(3), 337-386.  doi:10.1207/s15327809jls1303_4   [26] Roth, W. M., & Tobin, K. (2002). Redesigning an  urban   teacher education program: An activity theory perspective.  Mind, Culture, and Activity, 9(2), 108-131. DOI:  10.1207/S15327884MCA0902_03   [27] Rummel, R. J. (1970). Applied factor analysis. Evanston,  IL: Northwestern University Press.   [28] Scardamalia, M., & Bereiter, C. (1991). Higher levels of  agency for children in knowledge building: A challenge for  the design of new knowledge media. The Journal of the  Learning Sciences, 1(1), 37-68. DOI:  10.1207/s15327809jls0101_3    [29] Scott, W. R. & Davis, G. F. (2007). Organizations and  Organizing: Rational, natural, and open systems  perspectives. Upper Saddle River, NJ: Pearson.   [30] Siemens, G., Dawson, S., & Lynch, G. (2013). Improving  the quality and productivity of the higher education sector:  Policy and strategy for systems-level development of  learning analytics. Canberra, Australia: Office of Learning  and Teaching, Australian Government. Available:  http://solaresearch.org/Policy_Strategy_Analytics.pdf   [31] Slade, S., & Prinsloo, P. (2013). Learning analytics:  Ethical issues and dilemmas. American Behavioral  Scientist, 57, 1510-1529.    [32] Swenson, J. (2014). Establishing an ethical literacy for  learning analytics. In A. Pardo & S. Teasley (Eds.),  Proceedings from the 4th International Conference on  Learning Analytics and Knowledge (pp. 264-250). New  York: ACM. DOI: 10.1145/2567574.2567613   [33] Tabachnick, B. G., & Fidell, L. S. (2007). Using  multivariate statistics. (5th ed.). Boston, MA:  Pearson.   [34] van Barneveld, A., Arnold, K. E., & Campbell, J. P. (2012,  January). Analytics in higher education: Establishing a  common language. ELI paper 1: 2012. Available http://  www.educause.edu/Resources/AnalyticsinHigherEducation Esta/245405   [35] Weick, K. E. (1976). Educational organizations as loosely  coupled systems. Administrative Science Quarterly, 21(1),  1-19. DOI: 10.2307/2391875   [36] White, B. Y., & Frederiksen, J. R. (1998). Inquiry,  modeling, and metacognition: Making science accessible to  all students. Cognition and Instruction, 16(1), 3-118. DOI:  10.1207/s1532690xci1601_2   [37] Willis, J. E., III, Campbell, J. P., & Pistilli, M. D. (2013,  May). Ethics, big data, and analytics: A model for  application. EDUCAUSE Review Online. Available:  http://www.educause.edu/ero/article/ethics-big-data-and- analytics-model-application   [38] Yeo, F. (1997). Teacher preparation and inner-city schools:  Sustaining educational failure. The Urban Review, 29(2),  127-143. DOI: 10.1023/A:1024686607759   [39] Yong, A.G., & Pearce, S. (2013). A beginners guide to  factor analysis: Focusing on exploratory factor analysis.  Tutorials in Quantitative Methods for Psychology, 9(2),  79-94.      "}
{"index":{"_id":"23"}}
{"datatype":"inproceedings","key":"Manai:2016:RIT:2883851.2883942","author":"Manai, Ouajdi and Yamada, Hiroyuki and Thorn, Christopher","title":"Real-time Indicators and Targeted Supports: Using Online Platform Data to Accelerate Student Learning","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"183--187","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883942","doi":"10.1145/2883851.2883942","acmid":"2883942","publisher":"ACM","address":"New York, NY, USA","keywords":"cognitive and non-cognitive factors, community college developmental mathematics, hierarchical linear modeling, learning analytics, networked improvement community, online engagement","abstract":"Statway is one of the Community College Pathways initiatives designed to promote students' success in their developmental math sequence and reduce the time required to earn college credit. A recent causal analysis confirmed that Statway dramatically increased students' success rates in half the time across two different cohorts. These impressive results were also obtained across gender and race/ethnicity groups. However, there is still room for improvement. Students who did not succeed in Statway often did not complete the first of the two-course sequence. Therefore, the objective of this study is to formulate a series of indicators from self-report and online learning system data, alerting instructors to students' progress during the first weeks of the first course in the Statway sequence.","pdf":"Real-time indicators and targeted supports: Using online  platform data to accelerate student learning   Ouajdi Manai  The Carnegie Foundation for the   Advancement of Teaching  51 Vista Ln, Stanford,    CA 94305 USA  manai@carnegiefoundation.org   Hiroyuki Yamada  The Carnegie Foundation for the   Advancement of Teaching  51 Vista Ln, Stanford,    CA 94305 USA  yamada@carnegiefoundation.org   Christopher Thorn  The Carnegie Foundation for the   Advancement of Teaching  51 Vista Ln, Stanford,    CA 94305 USA  thorn@carnegiefoundation.org        ABSTRACT  Statway is one of the Community College Pathways initiatives  designed to promote students success in their developmental  math sequence and reduce the time required to earn college credit.  A recent causal analysis confirmed that Statway dramatically  increased students success rates in half the time across two  different cohorts. These impressive results were also  obtained   across  gender and race/ethnicity groups. However,  there is still room for improvement. Students who did not succeed  in Statway often did not complete the first of the two-course  sequence. Therefore, the objective of this study is to formulate a  series of indicators from self-report and online learning system  data, alerting instructors to students progress during the first  weeks of the first course in the Statway sequence.       CCS Concepts  Applied computing  Education  Learning management  systems     Applied computing  Education  Computer-assisted  instructions   Keywords  Learning analytics; hierarchical linear modeling; cognitive and  non-cognitive factors; online engagement; community college  developmental mathematics; networked improvement community   1. OBJECTIVE  The objective of this study is to formulate a series of indicators,  available during the first weeks of the course, to alert instructors  to their students engagement and progress in a timely manner for  possible targeted interventions. These indicators are distinct from  other early warning systems in that they integrate conceptual  knowledge, productive dispositions, and academic behaviors by  leveraging self-report and online learning platform data to monitor  students engagement and progress. In this paper, we report  preliminary results obtained from Fall 2013 Statway courses and  describe our plan of analysis.      2. THORETICAL FRAMEWORK  Approximately 60 percent of the nations incoming community  college students are referred to at least one developmental math  course, of which 80 percent will not earn college-level math credit  within three years [1]. Without achieving college math credit, they  cannot transfer into four-year degree programs or qualify for entry  into preparation programs in a wide range of occupational- technical specialties. As a result, millions of students each year  fail to acquire essential mathematics skills and are unable to  progress toward their career and life goals.    To address this national problem, the Community College  Pathways (CCP) program was developed and implemented  through Networked Improvement Communities (NICs) involving  college faculty, administrators, researchers, designers, and content  experts [5, 6, 9]. Statway is one of the CCP initiatives designed as  a year-long course that allows students to simultaneously  complete their developmental mathematics and college-level  statistics requirements to receive college credit. A causal analytic  study, using a propensity score matching technique [16] with a  hierarchical linear modeling (HLM) approach [12, 15], confirmed  that Statway tripled student success in half the time across two  different cohorts, and that this effect held across gender and  race/ethnicity groups [26].    However, there is still room to improve students success in  Statway. A recent study suggested that over  half of non-success  cases across the two-course sequence stemmed from students  failing the first course and not proceeding into the second course  [18]. Our approach for addressing this issue is to leverage existing  data, formulate indicators that identify students in need of support,  and alert instructors to their students engagement and  understanding in a timely manner for possible targeted  interventions [27]. In particular, we focus on the first several  weeks of the course, during which the majority of students who  enroll in developmental math courses fail to attend class [1].   To formulate a series of indicators that track students progress  over an academic term as early as possible, we leverage three sets  of data. One set of self-report data is used to measure what we call  productive persistence, a set of non-cognitive factors thought to  affect community college developmental math student success  [17, 27]. For instance, many students have had negative prior  math experiences, which lead them to develop the belief that I  am not a math person [10]. These beliefs can trigger anxiety and  poor learning strategies when students are faced with difficult or  confusing math problems [2, 3, 11]. This phenomenon is  compounded for some students (e.g., women, African Americans)  who are members of groups that have been stereotyped as not   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM mustbe  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom.   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883942       good at math [7, 19, 24]. Also, for students who question  whether they belong in a class or  college, it can be difficult to  fully commit to the behaviors that may be necessary for success,  such as joining study groups or asking professors for help [22,  23].    A second set of data assesses cognitive factors in the form of  students mathematical conceptual knowledge (MCK). Research  shows that when students learn math concepts, achievement gains  are visible not only in their conceptual understanding but also in  their procedural skill; however, gains in procedural skill do not  necessarily translate into conceptual understanding [4]. In fact, it  is possible that students are able to execute procedures adeptly  without having sound knowledge of the concepts that underlie  them. For this reason, and since existing college math placement  tests students take before entering community college (i.e.,  Accuplacer, Compass, MDTP) almost exclusively assess  procedural skill, it is important to seek a better understanding of  the conceptual knowledge (or lack thereof) of students entering  Statway.   Lastly, we leverage data from an online learning system. These  data represent any aspects of students interaction with the online  platform, such as reading pages, practicing problems, and  completing assignments, which can be used to operationally  define students engagement and understanding [21]. We focused  on students patterns of deviation from a standard pattern or  norm, which is a promising predictor of failure [14]. The norm  can be defined as a classroom norm or a norm that commonly  emerged across colleges.   Our aim is to integrate all three sources of data into an efficient  system of indicators to provide instructors with actionable insights  in a timely manner.   The technical infrastructure supporting this work aims at  collecting students data from multiple sources -- An online  portal, a survey application, and an online instructional platform.  The portal serves as a gateway and integrates with the other  components through API calls and LTI launches, the online portal  orchestrates the business logic responsible for sequencing various  content of the course. A data pipeline is also in place to capture all  students (and faculty) productions longitudinally. Figure 1  summarizes this whole process.      Figure 1: System topology for data collection.   3. METHODS/DATA SOURCES   In this paper, we present preliminary results using data available  from Fall 2013 Statway courses taken by approximately 1600  students from 26 colleges. However, our plan is to develop and  refine our indicator system with data from 2015 Statway courses  in the near future.   The first source of data is a self-report survey, which is  administered twice. At Week 1, self-report data on productive  persistence are collected along with student background   characteristics (e.g., demographics). At Week 4, only productive  persistence items are administered.   Data on MCK are also collected during Week 1. To avoid possible  priming effects of MCK completion on responses to productive  persistence items, students complete the MCK assessment after  the week 1 survey. The current version of MCK consists of 21  items, measuring key constructs that have the potential to impede  students learning of mathematics and statistics and have been  demonstrated to be difficult for developmental students: meaning  of fractional and decimal values, proportional reasoning, basic  algebraic notation, and reasoning about the effects of  mathematical operations. A Rasch scale score [25] is computed to  represent students knowledge of mathematical concepts at the  outset of  Statway.   The third source of data is from the online learning platform.  Statway courses in Fall 2013 were hosted by Carnegie  Mellons Online Learning Initiative (OLI) platform. The Statway  curriculum consists of 12 modules, the first six of which are  taught during the first course. Within each module, there are  material pages to review, practice activities (i.e., Learn By Doing  and Did I Get This), and checkpoints that are graded for  students understanding. These platform components generated  over 12 million rows of data, which  were used to measure  students engagement and proficiency as part of the indicator  system.    To develop effective and efficient indicators [27] from self-report,  conceptual knowledge, and system use data, we leveraged two  types of student end-of-term outcome variables: (a) a summative  assessment administered to all Statway students across colleges  and (b) students course grades. Students performance on the  summative assessment, which was a sort of final exam, was  factored into their grades by faculty. The correlation between  summative assessment scores and grades (on a four point scale)  was approximately .60 [20]. We wanted to  ensure the  development of quality indicators using these two types of  outcomes.   Our analytic plan for creating indicators is as follows. First, we  develop a Week 1 indicator using data on productive persistence  and MCK by running analyses with the end-of-term outcomes  described above. Under productive persistence, we focus on  students' self-perceptions as math learners and doers, such as fixed  mindset and stereotype threat. To measure fixed mindset, students  were asked to respond to the statement,  Being a 'math person' or  not is something about you that you really can't change. Some  people are good at math and other people aren't , by indicating  how much they agree or disagree with the statement. Responses  were coded as:  Strongly agree  = High;  Agree  and  Mostly  agree  = Medium; and  Mostly disagree ,  Disagree , and   Strongly disagree  = Low. The survey item assessing stereotype  threat asked students whether they thought others at their school  would be surprised or not surprised if people like them succeeded  in school. Responses were coded as:  Extremely surprised  and   Very surprised  = High;  Moderately surprised  and  Slightly  surprised  = Medium;  Not surprised at all  = Low. We also use  the first modules checkpoint performance as an intermediate  outcome. We consider each module checkpoint as key milestone  for students, and accordingly examine checkpoint data whenever  appropriate.   Second, we leverage data from the online learning platform to  track student progress on a daily basis with a focus on any  patterns of students deviations from a normative pattern in     engagement and understanding. Their engagement patterns in a  given module will also be submitted to analyses with the  corresponding module checkpoint.     Third, we examined productive persistence data collected during  Week 4, with a particular focus on students' perceptions of their  own classroom setting as a mathematical learning environment.  By the fourth week of class, we expect students to have developed  their perceptions of the classroom they belong to. Hence, data  measuring, for instance, belonging uncertainty, will be  informative for identifying students in need. In fact, earlier  research confirmed this hypothesis: those who reported a high  level of belonging uncertainty were significantly more likely to  drop out of the course [27]. To assess their level of belonging  uncertainty, students were asked,  When you think about your  college, how often, if ever, do you wonder: 'Maybe I don't belong  here'  Responses were coded as:  Always  = High;  Frequently   and  Sometimes  = Medium;  Hardly ever  and  Never  = Low.   Fourth, we continue monitoring data from the online platform to  detect any patterns of deviations. A prior study suggests that  students who attempted a checkpoint after failing were more  likely to achieve a higher score on the summative assessment than  those who did not [13]. Accordingly, we monitor this type of  attempt behavior for each checkpoint as part of our indicator  system.   We apply a hierarchical linear modeling approach [15] to the  above analyses due to the hierarchical nature of our data (i.e.,  students nested within classrooms, which are further nested within  colleges). By taking this approach, we want to appropriately  capture variation at student, classroom, and college levels.   4. RESULTS  Our preliminary results are informative toward developing our  indicator system. Table 1 presents the descriptive statistics of the  summative assessment administered at the end of the fall term in  2013 by each of the productive persistence construct groups. As  expected, students productive persistence has a strong  relationship with their end-of-term outcomes. More specifically,  lower levels of fixed mindset, stereotype threat, and belonging  uncertainty during the first and fourth weeks of class are  positively associated with summative assessment scores at the end  of term. Consistent with the results in Table 1, Figure 2  demonstrates that the student success rate of Statway 1 (i.e., the  first course in the Statway sequence) is associated with the level  of each productive persistence construct.   Table 1: Descriptive Statistics of Summative Assessment  Rasch Scale Score Per Productive Persistence Construct   Note. High, Medium, and Low represent the levels of each  productive persistence construct. For the definitions of the levels,  see 3. METHODS/DATA SOURCES.  M = mean, SD = standard deviation.   Figure 2: Student success rate of Statway 1 per productive  persistence construct. High, Medium, and Low represent the  levels of each productive persistence construct. For the  definitions of the levels, see 3. METHODS/DATA SOURCES.   Figure 2 presents the student success rate of Statway 1 by the  level of math conceptual knowledge (MCK) and the level of  online engagement during the first module. Engagement was  defined as number of student attempts at practice activities (i.e.,  Learn By Doing and Did I Get This) and checkpoints.  Interestingly, there are interaction effects between  the level of  MCK and the level of online engagement on student success rate.  That is, students who demonstrated low levels of math conceptual  knowledge but engaged with the online platform during the first  module tended to achieve success rates comparable to students  who demonstrated higher levels of math conceptual knowledge  but never engaged with the online platform.      Figure 3: Student success rate of Statway 1 by level of online  engagement during the first module and MCK group. No  engagement means students have attempted 0% of practice  activities and checkpoints. Some engagement means students  have done some (between 1-99%) of these activities. Full  engagement means students have done 100% of these  activities.   5. SIGNIFICANCE OF THE STUDY  This study demonstrated the ways in which we can leverage both  self-report and online learning system data to identify students in  need of support. For instance, we identified a few self-report  survey items measuring non-cognitive factors as promising  predictors of student outcomes, suggesting that we can inform  actionable insights with only a few items data. More specifically,  if we identify students who show higher levels of fixed mindset  and/or stereotype threat during the first week of class, we can  promote a growth mindset (as opposed to a fixed mindset) by  engaging them in growth mindset activities (formulated by the    Fixed  mindset   at Week 1   Stereotype  threat   at Week 1   Belonging  uncertainty  at Week 4   Level M SD M SD M SD  High 0.55 0.68 0.54 0.67 0.42 0.84  Medium 0.69 0.77 0.61 0.77 0.63 0.78  Low 0.85 0.82 0.90 0.79 0.85 0.80     networked improvement community [NIC] members) and  providing them with feedback that establishes high standards  while assuring student that they are capable of meeting them [8].  For instance, if students exhibit higher levels of belonging  uncertainty during the fourth week of class, we can provide group  activities that facilitate building a learning community for all  students in the classroom (interventions are formulated by the  NIC members). Furthermore, the results suggest that the use of the  online learning platform may provide scaffolding for students  with low levels of math conceptual knowledge.   The results shown in this paper are still descriptive and  preliminary. As aforementioned, we will apply a hierarchial linear  modeling approach to better formulate our indicator system  because of the hierarchical nature of our data (i.e., students nested  within Statway courses within colleges). We also plan to work on  more real-time analytics at more granular levels (e.g., how  students math conceptual knowledge, productive persistence, and  online engagement interact with each other to manifest in patterns  of success or failure, when students start deviating from classroom  norms, whether they attempt a module checkpoint right after  failing it). In this paper, we defined 1-99% attempts as some  engagement to differentiate it from no or full engagement (see  Figure 3); however, we plan on dividing this intermediate level of  engagement into more meaningful sub-levels for further  actionable insights. In the future, we plan on expanding the  current analysis plan by including data beyond the first month.  For example, which aspects of productive persistence can we  leverage as data to identify students in need after the first month  Which online behavioral patterns or deviations should we focus  on to alert instructors of  targeted supports for students Overall,  we aim to establish the best practices for leveraging different  kinds of data over time to provide support for students success.   6. ACKNOWLEDGMENTS  This program of work is supported by Carnegie Corporation of  New York, The Bill & Melinda Gates Foundation, The William  and Flora Hewlett Foundation, The Kresge Foundation, and  Lumina Foundation in cooperation with the Carnegie Foundation  for the Advancement of Teaching.   7. REFERENCES  [1] Bailey, T., Jeong, D. W., & Cho, S.-W. (2010). Referral,   enrollment, and completion in developmental education  sequences in community colleges. Economics of Education  Review, 29, 255270.   [2] Beilock, S. L., Gunderson, E. A., Ramirez, G., & Levine, S.  C. (2010). Female teachers' math anxiety affects girls' math  achievement. Proceedings of the National Academy of  Sciences, USA, 107, 1060-1063.   [3] Blackwell, L. S., Trzesniewski, K. H. & Dweck, C. S.  (2007). Implicit theories of intelligence predict achievement  across an adolescent transition: A longitudinal study and an  intervention. Child Development, 78, 246-263.   [4] Boaler, J. (1998). Open and closed mathematics: Student  experiences and understandings. Journal for Research in  Mathematics Education, 29, 4162.   [5] Bryk, A. S., Gomez, L. M., & Grunow, A. (2011). Getting  ideas into action: Building networked improvement  communities in education. In M. T. Hallinan (Ed.), Frontiers  in sociology of education (pp. 127-162). New York, NY:  Springer.   [6] Bryk, A. S., Gomez, L. M., Grunow, A., & LeMahieu, P. G.  (2015). Learning to improve: How America's schools can get  better at getting better. Cambridge, MA: Harvard Education  Press.   [7] Cohen, G. L., Garcia, J., Purdie-Vaughns, V., Apfel, N., &  Brzustoski, P. (2009). Recursive processes in self- affirmation: Intervening to close the minority achievement  gap. Science, 324, 400-403.   [8] Cohen, G. L., Steele, C. M., & Ross, L. D. (1999). The  mentors dilemma: Providing critical feedback across the  racial divide. Personality and Social Psychology Bulletin, 25,  13021318.   [9] Dolle, J. R., Gomez, L. M., Russell, J. L., & Bryk, A. S.  (2013). More than a network: building communities for  educational improvement. In B. J. Fishman, W. R. Penuel,  A. R. Allen, & B. H. Cheng (Eds.), Design-based  implementation research: Theories, methods, and exemplars.  National Society for the Study of Education Yearbook. New  York, NY: Teachers College Record.   [10] Dweck, C.S. (2006). Mindset. New York, NY: Random  House.   [11] Haynes, T. L., Perry, R. P., Stupnisky, R. H., & Daniels, L.  M. (2009). A review of attributional retraining treatments:  Fostering engagement and persistence in vulnerable college  students. In Smart, J. C. (Ed.), Higher education: Handbook  of theory and research (pp. 227-272). New York, NY:  Springe   [12] Hong, G., & Raudenbush, S. W. (2005). Effects of  kindergarten retention policy on childrens cognitive growth  in reading and mathematics. Educational Evaluation and  Policy Analysis, 27, 205-224.    [13] Krumm, A. E., D'Angelo, C., Podkul, T. E., Feng, M.,  Yamada, H., Beattie, R., Hough, H., & Thorn, C. (2015, 14- 15 March). Practical measures of learning behaviors. Paper  presented at the ACM Conference on Learning @ Scale  (L@S'15), Vancouver, BC, Canada.   [14] Krumm, A. E., Waddington, R. J., Teasley, S. D., & Lonn, S.  (2014). Using learning analytics to support academic  advising in undergraduate engineering education. In J. A.  Larusson & B. White (Eds.). Learning Analytics: From  Research to Practice (pp. 103-119). New York: Springer.   [15] Raudenbush, S. W., & Bryk, A. S. (2002). Hierarchical linear  models: Applications and data analysis methods (2nd ed.).  Thousand Oaks, CA: Sage Publications.   [16] Rosenbaum, P. R., & Rubin, D. B. (1983). The central role of  the propensity score in observational studies for causal  effects. Biometrika, 70, 41-55.   [17] Silva, E., & White, T. (2013). Pathways to improvement:  Using psychological strategies to help college students  master developmental math. Carnegie Foundation for the  Advancement of Teaching. Stanford, CA.   [18] Sowers, N., & Yamada, H. (2015). Pathways Impact Report.  Stanford, CA: Carnegie Foundation for the Advancement of  Teaching.   [19] Steele, C. M., & Aronson, J. (1995). Stereotype threat and  the intellectual test performance of African-Americans.  Journal of Personality and Social Psychology, 69, 797-811.      [20] Strother, S., & Sowers, N. (2014). Community College  Pathways: Summative Assessments and Student Learning.  Stanford, CA: Carnegie Foundation for the Advancement of  Teaching.   [21] U.S. Department of Education. (2013). Expanding evidence  approaches for learning in a digital world. Washington, DC:  Author.  www.ed.gov/edblogs/technology/files/2013/02/Expanding- Evidence-Approaches.pdf   [22] Walton, G. M., & Cohen, G. L. (2007). A question of  belonging: Race, social fit, and achievement. Journal of  Personality and Social Psychology, 92, 82-96.    [23] Walton, G. M., & Cohen, G. L. (2011). A brief social- belonging intervention improves academic and health   outcomes among minority students. Science, 331, 1447 1451.   [24] Walton, G. M., & Spencer, S. J. (2009). Latent ability:  Grades and test scores systematically underestimate the  intellectual ability of negatively stereotyped students.  Psychological Science, 20, 1132-1139.    [25] Wright, B. D., & Masters, G. N. (1982). Rating scale  analysis: Rasch measurement. Chicago: MESA Press.   [26] Yamada, H. (2014). Community College Pathways Program  Success: Assessing the First Two Years Effectiveness of  Statway. Stanford, CA: Carnegie Foundation for the  Advancement of Teaching.     [27] Yeager, D., Bryk, A., Muhich, J., Hausman H., & Morales,  L. (2013) Practical measurement. Carnegie Foundation for  the Advancement of Teaching. Stanford, CA.          "}
{"index":{"_id":"24"}}
{"datatype":"inproceedings","key":"Wise:2016:BOC:2883851.2883916","author":"Wise, Alyssa Friend and Cui, Yi and Vytasek, Jovita","title":"Bringing Order to Chaos in MOOC Discussion Forums with Content-related Thread Identification","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"188--197","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883916","doi":"10.1145/2883851.2883916","acmid":"2883916","publisher":"ACM","address":"New York, NY, USA","keywords":"discussion forum, machine learning, massive open online courses, natural language processing, social interaction","abstract":"This study addresses the issues of overload and chaos in MOOC discussion forums by developing a model to categorize and identify threads based on whether or not they are substantially related to the course content. Content-related posts were defined as those that give/seek help for the learning of course material and share/comment on relevant resources. A linguistic model was built based on manually-coded starting posts in threads from a statistics MOOC (n","pdf":"Bringing Order to Chaos in MOOC Discussion Forums  with Content-Related Thread Identification   Alyssa Friend Wise  Simon Fraser University   250-13450 102nd Avenue  Surrey, B.C. V3T 0A3 Canada    1-778-782-8046  alyssa_wise@sfu.ca   Yi Cui  Simon Fraser University   250-13450 102nd Avenue  Surrey, B.C. V3T 0A3 Canada    1-778-782-8046  yca231@sfu.ca   Jovita Vytasek  Simon Fraser University   250-13450 102nd Avenue  Surrey, B.C. V3T 0A3 Canada    1-778-782-8046  jvytasek@sfu.ca        ABSTRACT  This study addresses the issues of overload and chaos in MOOC   discussion forums by developing a model to categorize and   identify threads based on whether or not they are substantially   related to the course content. Content-related posts were defined   as those that give/seek help for the learning of course material and   share/comment on relevant resources. A linguistic model was built   based on manually-coded starting posts in threads from a statistics   MOOC (n=837) and tested on thread starting posts from the   second offering of the same course (n=304) and a different   statistics course (n=298). The number of views and votes threads   received were tested to see if they helped classification. Results   showed that content-related posts in the statistics MOOC had   distinct linguistic features which appeared to be unrelated to the   subject-matter domain; the linguistic model demonstrated good   cross-course reliability (all recall and precision > .77) and was   useful across all time segments of the courses; number of views   and votes were not helpful for classification.     CCS Concepts   Information systems ~ Content analysis and feature  selection    Information systems ~ Clustering and  classification    Applied computing ~ E-learning    Computing  methodologies ~ Natural language processing    Computing  methodologies ~ Model development and analysis   Keywords  Massive open online courses; social interaction; discussion forum;   natural language processing; machine learning   1. INTRODUCTION  Massive Open Online Courses (MOOCs) are online learning   environments that are open to anyone with web access [7].   Generally charging no cost for participation and setting no   prerequisite requirements, the courses often attract thousands, or   even hundreds of thousands of registrations. In the past several   years, MOOCs have seen dramatic growth in terms of available   platforms, participating institutions, courses offered, and learners   involved [10, 28].   With such large scale enrollments and the support of the Internet,   MOOCs have the potential to provide abundant interaction   opportunities for learners. Interaction is a key component in the   quality of online learning, providing important support to learners   [31]. Specifically, providing a good interaction environment is   considered an important criterion for the quality of MOOCs [17].   While many forms of interaction are theoretically possible,   currently discussion forums are the de facto primary venue in   MOOCs for leaner-learner and learner-instructor interactions.   Discussion forums are valued by instructors as an important   instrument for understanding and intervening in learning activities   [29, 16], while learners use them for giving and getting help for   challenges they encounter in their learning [2, 30].   In order for these activities to happen effectively, instructors and   learners need to be able to find the messages relevant to their   purposes. However, due to the large number of participants in   MOOCs, discussion forums are often plagued by information   overload and chaos [24, 3]. On top of this, a large proportion of   MOOC posts are not directly related to the course [3]. As a result,   forums become overwhelming and confusing for users to navigate   [15]. This is an exacerbation of problems seen in traditional   discussion forums for over a decade [13, 9, 26]. Such problems   can lead to extremely low levels of responsiveness [14] which   means the desired understanding, intervening, help giving and   getting activities are not optimized [12].   Currently, there are very limited means for addressing overload   and disorder in MOOC discussion forums. One commonly used   strategy is to set sub-forums for different purposes. However,   misplaced posts are common in MOOCs [27], indicating that this   strategy hasnt been very successful. An alternative strategy has  been to ask learners to tag their posts, making it easier for others   to identify different types of postings. But just like with sub-  forums, there is no guarantee that learners will tag their posts in   accurate and consistent ways. In addition, many MOOC forums   allow learners to sort posts by the number of views and votes   made by other users. However, such forms of peer   recommendation are subject to bias through the disproportionate   effect of early support (the rich get richer phenomenon) and  positioning effects [20]. More importantly, since these features   simply indicate the general popularity of messages, their value  in distinguishing the type of information different posts contain is   questionable [6]. This indicates the need for novel tools that can   Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that copies   bear this notice and the full citation on the first page. Copyrights for   components of this work owned by others than the author(s) must be   honored. Abstracting with credit is permitted. To copy otherwise, or   republish, to post on servers or to redistribute to lists, requires prior   specific permission and/or a fee. Request permissions from   permissions@acm.org.    LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom    Copyright is held by the owner/author(s). Publication rights licensed to   ACM.  ACM 978-1-4503-4190-5/16/04$15.00   DOI: dx.doi.org/10.1145/2883851.2883916     more effectively assist instructors and learners in navigating the   complicated landscape of MOOC discussion forums to find the   posts they are looking for.   In this study, we address the overload problem in MOOC   discussion forums by developing a model to automatically classify   threads as substantially related to the course material or not. In the   following section we first review prior efforts to address overload   in MOOC discussion forums and justify why helping instructors  and learners easily distinguish content-related and non-content-  related posts is a useful and novel contribution to this problem   space. We then describe how the concept was operationalized to   manually code threads from three statistics MOOC discussion   forums according to whether or not they were substantially related   to the course subject, and subsequently develop and test a   classification model to automatically identify content-related   threads. Through this work, we aim to build a foundation for tools   that can help MOOC instructors and learners locate forum threads   addressing the learning of course material more easily.   2. LITERATURE REVIEW  Research efforts targeting overload and chaos in MOOC   discussion forums generally fall into two categories. One is   machine-oriented solutions that use automatic tools to diagnose   learners posts and provide prescribed help or resource as  appropriate. For example, [1] targeted the lack of responsiveness   in MOOC forums with an automatic tool that detects confusion in   posts and recommends relevant video clips to the learner.   Automated approaches have value in providing personalized   instant intervention to learners in need of simple answers to   straightforward questions; however, in many cases, learners  needs are more complex or idiosyncratic, may not be sufficiently   addressed by preexisting material, and pointing learners to videos   they have already viewed could be frustrating, creating a negative   affective state for learning [8]. In such cases, responsive human   interaction may be a more useful resource for support [25]. In   addition, by reducing the need and opportunity for human   interaction, automated solutions risk exacerbating the lack of   community in MOOCs, which has been a prominent issue   associated with learning difficulties and dropouts [18].   The second category of approaches to addressing MOOC   discussion forum disorder also begins with the automated analysis   of discussion forum posts, but with the goal of supporting human   interaction. These approaches have generally been either   instructor-oriented or learner-oriented. Instructor-oriented   approaches aim to assist instructors in making efficient and   effective interventions in discussion forums. For example, [4] and   [5] both aimed at optimizing instructors intervention decisions  with post recommendation systems. They built models based on   instructors intervention histories and tested how well the models  could identify where instructors had chosen to intervene. While   such models allow instructors to replicate their current   intervention patterns more efficiently in the future, this approach   overlooks the important question of if the existing patterns are   desirable in the first place. Given that typically instructors may   not have reviewed all existing posts before deciding where to   intervene, and that subjectiveness and arbitrariness are common in   these decisions [4], models based on prior intervention history   alone may be insufficient, or even harmful, in meeting the goal of   identifying posts in which intervention should occur.    Taking a different approach to support instructor intervention, [27]   aimed to use existing discussion forum structures to inform   instructors reading decisions. They assumed that threads in the  same sub-forums generally involve the same types of interactions   and thus have common features that can help identify misplaced   ones. Using the sub-forum titles (e.g. Lectures, Assignments,  Meetups) as labels, they extracted five types of language- independent thread features: thread structure (e.g. length and   breadth), underlying social network (e.g. number and density of   users within a thread), popularity (e.g. number of views and votes),   temporal dynamics (e.g. message rate), and content (e.g. quantity   of text and hyperlinks). Using this feature set, the researchers built   a model to identify misplaced threads in each sub-forum. Their   results showed that non-linguistic features could be useful for   identifying small talk threads (the majority of which were found  in Meetups), but had limited utility in differentiating between  other post categories.    Finally, [16] addressed instructors intervention decision from a  social network perspective. Instead of classifying and filtering   posts based on their content, they aimed at identifying a small   group of prominent learners to amplify the effect of (an inherently   limited number of) instructor interventions. They modeled MOOC   forums as a social network and developed an algorithm to identify   the most influential learners. The underlying assumption was that   by making responses to these learners posts, the effects of the  instructors intervention would disseminate to a large number of  other learners, though this logic was not tested in that study. If   effective, this approach has the potential to broadcast instructors  influence most widely with minimum intervention cost; however   it overlooks the specific learning needs of less prominent or   networked learners. Additionally the most influential learners may   not be the ones who most need the instructors help.   In contrast to instructor-oriented approaches which have looked   for general, high-impact ways to intervene, learner-oriented   approaches have focused more on addressing specific learners  needs and promoting personalized interactions. For example, [34]   aimed at assisting learners reading choices and developed a  model that recommends discussion threads that match learners  interests as reflected by their previous activities. Similarly, [33]   built a question recommendation system that analyzes learners  intellectual and behavioral characteristics, and recommends   people to answer questions considering relevance to their   interests, level of intellectual challenge, and anticipated workload.   These approaches provide learners with personalized interactions   and make effective use of learners participation efforts in  discussion forums; they are thus one productive solution path to   pursue. However, sorting posts based on historical learning   behaviors creates a self-reinforcing narrowing of the field of   vision that may not sufficiently reflect learners evolving interests  and needs. It also doesnt support them to explore for diversified  learning opportunities. Thus modeling based only on prior activity   and interests may not be desirable as the sole solution strategy.   Another approach that can help address learners needs is topic- based post classification, which could help learners to browse a   more organized set of posts for ones that may interest or benefit   them. Topic-based classification has been an objective of several   prior research efforts; however, the scope of the topics has not   been clearly or consistently defined. [3] categorized posts as   course-related (course-specific discussions and course logistics)  versus small talk (for social purposes). They built a model based  on topical features of these two categories to classify and rank   posts according to their relevance to the course. While effective   for its given purpose, their categorization did not distinguish     between discussions of the learning of course material and those   about logistical and technical issues, which are substantially   different. Such distinction was made in [30]s framework for post  topics in MOOC forums, but this initial study did not go beyond   defining categories to create models to identify such posts. Thus   principled identification of learning-content-related MOOC posts   remains a promising, but as yet not fully realized, line of research.    3. THE CURRENT STUDY  In the current research, we extend the work of these prior studies   to address information overload in MOOC discussion forums by   providing a clear and theoretically justifiable way to categorize   posts and build a classification model based on this   categorization. Our approach is designed to address the needs of   both instructors and learners in finding relevant posts by focusing   on the identification of those which are substantively related to   the content of the course material. While there are certainly other   reasons that students and instructors may come to a discussion   forum (for example to report or find out about technical/logistic   problems or make social connections), we argue that   communication about course content-related issues is (a) an   important function of MOOC discussion forums; (b) where the   greatest direct learning-related activity takes place; and (c) a   particular kind of interaction, different in character from technical   and social exchanges. Thus identification of content-related posts   is the focus of this work.    From instructors perspective, this can be helpful in directing their  attention and domain expertise where it can be of most value.   Other posts, such as those for social purposes, may not be directed   at or need instructor intervention [27]. Moreover, posts that do   require non-peer responses involve diversified issues, many of   which would be best addressed by other members of the MOOC   instructional team such as TA and technical staff [4].  Finally,   even if an instructor chooses to engage with both content- and   non-content-related posts, it may be most efficient to do in   batches, focusing their cognitive efforts on similar tasks at a time.   From the learners perspective, identifying threads based on  whether they address questions about the course material is also   desirable. While MOOC learners are diversified in learning goals,   engagement patterns, and workload commitment, for the majority   of learners who do more than just simply enroll, the purpose in   taking a MOOC is learning-related, with logistic and social   concerns serving supporting roles [19]. Thus, allowing learners to   find the posts that address course-content-related issues can   support learning. Importantly, by making the category all course- content-related posts, rather than those on specific topics as might   be found with a search functionality, our approach allows learners   to benefit from valuable peer questions they would not have   thought to ask themselves. From the question-asking side, this   approach also helps threads that substantially involve the course   material get more attention and potential responses. In this way,   the tool aims to foster learning interactions in the discussion   forum.   3.1 Study Framing  The content of discussions in forums can be analyzed at various   levels, such as thread, post, or sentence. When users navigate   discussion forums, they usually browse the starting post in a   thread to determine whether or not they are interested. Although a   discussion thread may change direction as new participants join   [30], the starting post reflects the primary intention of the thread   initiator and largely scopes the content of the subsequent replies.   Therefore, the starting post of a discussion thread is a useful unit   of analysis for the purposes of this study.   In this work we define content-related starting posts as those that   seek/provide help, opinions, or resources directly related to the   course subject. This includes posts that ask or answer subject-  related questions, share subject-related opinions, and those that   share/comment on external resources related to course subject.   Given the focus on identifying content-related starting posts, the   rest of the threads are considered simply to be non-content-  related, although we acknowledge that this broad framing could in   the future be divided into subcategories such as socializing,   technical problems, logistical questions etc. This binary   classification goes beyond the work of [3], building on [30] to   provide a refined criterion for distinguishing forum activities   substantially related to learning of course material from other   forum activities. To our knowledge, no previous work has focused   in this way on identifying content-related discussion threads to   assist human interaction in MOOCs.   Finally, we limit the scope of this initial work to MOOCs in the   domain of statistics. Such limitation provides a useful test-bed for   the viability of the approach in a local context, before addressing   potentially more complicated questions about generalizability   across domains. Statistics was chosen as the area of focus both   because (like many MOOCs) it addresses a technical domain, and   because several sets of relevant data for model development and   testing were available. While a practical concern, accessibility to   sufficient data from real MOOC discussions in particular kinds of   courses is imperative for valid modeling and testing.    3.2 Research Issues  As content-related and non-content-related posts address   substantially different topics, forum users may use language   differently in such postings. Therefore, as a first step it is worth   exploring whether or not content- and non-content-related posts in   statistic MOOC discussion forums have distinct linguistic features   that could be used for classification: RQ 1: Do starting posts of  content-related threads in a statistics MOOC discussion forum  have distinguishing linguistic features from starting posts of non- content-related threads   If distinctive linguistic features are found, it may then be possible   to use these to build a classifier that detects whether or not a post   is content-related based on these: RQ 2: Can linguistic features  be used to create a model that reliably identifies starting posts of  content-related threads in a statistics MOOC discussion forum   Apart from the linguistic features of starting posts, peer   recommendations such as views and votes are potential indicators   (or distractors) in the identification of content-related threads.   These are important to test since they are common post-sorting   features used by learners: RQ 3: Are the number of views and  votes useful in predicting content-related starting posts   If a reliable classifier is built, the next question is the degree to   which it is useful on data generated in similar situations. The most   similar situation in the case of MOOCs would be a new group of   students taking the same course. The next degree of generalization   would be to look at data from a new group of students taking a   different course on the same general topic: RQ 4: Does the model  generalize to another offering of the same statistics MOOC and a  different statistics MOOC     Finally, the proportion of content- and non-content-related posts   in discussion forums may vary over time due to changes in   learners interests and needs as a MOOC proceeds [3]. For  example, when a course just begins, learners may generate many   posts to initiate study groups and ask for software-related   technical assistance; when the course is about to conclude,   learners may flock in to express gratitude and ask about credential   issuance. Variation in the proportions of topics discussed may   affect the performance of a linguistic model. For this reason, an   important concern for the applicability of a model generated on   completed course data to a live MOOC situation, is how well it   performs on data from different time segments in a course: RQ 5:  How well does the model identify starting posts of content-related  threads from particular time segments of a course   4. METHODS  4.1 Data Source  This study was conducted on data from three completed MOOCs   offered in 2013 and 2014 on Stanford open-source platform   Lagunita (initially called Stanford OpenEdX). The initial   examination of linguistic features and modeling efforts were   conducted on data from an offering of Statistics in Medicine  (StatMed13). The generalizability of this model was tested on  data from a later offering of the same course (StatMed14), and a  different statistics course Statistical Learning (StatLearn). The  usefulness of the number of views and votes was tested on all   three courses; time segments could be evaluated only for the   external test sets StatMed14 and StatLearn.   4.1.1 Course Contexts  Statistics in Medicine is an introductory course on probability and  statistics with a special focus on statistics in medical studies.   There was no prerequisite for taking the course. Optional modules   in the course covered advanced math topics and basic data   analysis in R. The course was 9 weeks long and the estimated   weekly workload was 8 to 12 hours. Course materials included   lecture videos and optional readings. Assessment consisted of   quizzes, homework assignments, and a final exam. The course   provided a discussion forum for interaction in nine topic areas   (Figure 1.a). Learners were invited to post questions and   comments about the course in the forums for response by other   learners, the TAs and the instructor.    Discussion Topic Areas     Course Material Feedback      External Resources      General  Discussion Topic Areas   Homework  Course Material Feedback    Introductions  General    Platform Feedback  Platform Feedback    Study Group  Quiz and Review    Tech Support  R and RStudio   (a)     Video (b) Tech Support   Figure 1: Forum structures for (a) StatMed and (b) StatLearn.   Statistical Learning is an introductory-level course in supervised  learning with a focus on regression and classification methods.   The prerequisites included introductory courses in statistics, linear   algebra, and computing. In this course, computing was done in R.   Course materials included lecture videos and readings. The course   was 9 weeks long with an estimated weekly workload of 3 hours.   Assessment was based on the completion of quizzes. The course   provided a discussion forum for interaction in six topic areas   (Figure 1.b). Students were invited to post questions and   comments about the course in the forum.   4.1.2 The MOOCPosts Dataset   The MOOCPosts Dataset  1  was obtained from researchers at   Stanford University. The dataset consists of a selection of   randomly chosen forum posts from several MOOC courses. The   subset of the data pertaining to StatMed13, StatMed14 and  StatLearn was used in this study. Forum information provided in  the dataset included the following: thread id; post id; post position   in thread (starting post or reply post); post text; post creation date   and time; number of times post was viewed; and number of votes   post received. Thread titles were not included in the data set. A   number of manual post annotations were also provided, but not   used in this research. The number of threads and posts provided   for each course are shown in Table 1.   Table 1: Number of threads and posts in the provided dataset   Course Name  # of Threads  # of Posts  (starting posts & replies)  StatMed13 844 3320   StatMed14 310 1218   StatLearn 626 3030   4.2 Data Preparation  The entire set of starting posts for StatMed13 was coded in order  to have sufficient data to train the model. Smaller sets of starting   posts from StatMed14 and StatLearn were coded to serve as  external test sets. This included all 310 of the starting posts from   StatMed14, and 300 randomly selected starting posts from  StatLearn. Reply posts were used only for contextual information  during manual coding when necessary.   Seven duplicated posts and one post containing foreign language   were removed from the datasets. An additional seven posts were   removed during the coding process because it was not possible to   make a coding decision without the (missing) thread titles. After   all post removals, 99% of data remained in the analysis, with the   number of starting posts in StatMed13, StatMed14 and  StatLearn datasets being 837, 304 and 298 respectively. All  anonymization codes were cleaned from post text before coding.    4.3 Coding   Each starting post was coded by two researchers as relating   substantively to the course material or not according to the   definition set out in Section 3.1. A coding guide with detailed   category descriptions and examples was provided to both coders.    A rule of leniency towards the content-related category was   adopted for borderline cases so as to maximally capture content-   related linguistic features. Coder training was conducted on data   not included in this study in cycles of 25 posts until reliability as   indexed by Krippendorff's alpha was stable at an acceptable level                                                                    1 http://datastage.stanford.edu/StanfordMoocPosts/.     Table 2: Top 30 features of content-related and non-content-related starting posts organized by category    Category Content-related Non-content-related   Course Subject  value, mean*, calculate, probability, p, difference*,   standard, data, test      Learning Process understand, example, mean*, difference*, question*     Question Words how*, what*, which*, why*, does*, is*, are* was*   Connectors  of, of_the, in, in_the, about, between, that, then, if,   which,* why*, how*, what*  On   Existence / Condition is*, are*, does*, not was*, had*, have*   Course Tasks and   Platform Resources  question*   final, homework, submit, the_final, answers, quizzes,   exam, work*, download, videos, course, the_course,   this_course   Pronouns We my,  I_have, but_I, I_had, your, all*   Quality / Quantity  again, all*, time*   Effort / Action  work*,  had*,have*,  made   Appreciation  great, thank_you, thank   *An asterisk indicates a word that appears in more than one category due to different uses.  (  >.70). Study data was then coded in subsets of 100 to 169   posts for all courses. All differences were discussed and   reconciled before the coders proceeded to the next subset. Coding   reliability was good for all three courses: StatMed13 ( =.77);  StatMed14, ( =.82); StatLearn, ( =.84).   4.4 Feature Extraction and Modeling  Lightside Researchers Workbench v2.3.1 was used to perform  feature extraction on coded starting posts in StatMed13, using the  basic bag-of-words feature set and a rare threshold of 5. Unigrams   and bigrams alone were most useful for characterizing and   modeling posts. Stopwords were not removed from the feature   lists because they were found to be helpful for modeling. This   follows a general trend in the information retrieval systems to   include stopwords in feature sets [22]. A total of 2410 features   were extracted. After Arabic numbers, symbols, and features   substantially incorporated by other features (e.g. m is  incorporated by I_m) were removed, 2236 features remained.   All 2236 features were then used to train a binary L2 regularized   logistic regression classification model (thread starting post as   content-related or not).  The model was first evaluated by ten-fold   cross-validation, and then on independent test sets from   StatMed14 and StatLearn. Additional modeling was conducted to  test the usefulness of the number of views and votes that starting   posts received for classification. The predictive ability of the   model for posts from different time segments in the course was   tested by dividing the test datasets for StatMed14 and StatLearn  respectively into three equal subsets according to posts time of  creation.   5. RESULTS  Content-related and non-content-related posts were relatively   equal in proportion across all three courses with the percentage of   content-related starting posts in StatMed13, StatMed14, and  StatLearn being 47%, 54%, and 51% respectively.   5.1 Research Question 1: Linguistic Features  of Thread Starting Posts  Stark differences were found between the top 30 features (ranked   by kappa) of content- and non-content-related starting posts. At a   basic level, the lists were composed of completely distinct terms.   To probe this distinction further, features were organized into   categories based on examination of their uses in the text (Table 2).   The vast majority (85%) of the features across both categories did   not appear to be specific to the courses subject domain. Many  features of content-related starting posts were associated with the   process of learning, involved question words or terms describing   relationships between ideas (connectors). For content-related   starting posts only, several linguistic features related to the course   domain were found. The features of non-content-related starting   posts did not appear to be related to the course domain. Rather   they were terms related to course tasks, resources, and those that   refer to the course itself. In addition, several terms referring to   effort/action, quality/quantity, appreciation, and pronouns were   found in this category. Both categories contained several terms   related to existence and condition.   Within categories common to both content- and non-content-  related features, stark differences were also observed. The only   content-related feature in the Course Task and Platform category   (question) refers to a more specific form of course task than the  six non-content-related features (e.g. homework, exam).  Similarly, the category of Pronouns was dominated by non-  content-related features containing a variety of first-person   singular references (e. g. I) while the only content-related  Pronoun feature is the first person plural pronoun we. Existence  /Condition was the only feature category that did not show stark   difference between content- and non-content-related features.   Collectively these findings indicate content-related starting posts   in StatMed13 have distinguishing linguistic features from non- content-related ones, irrespective of specific course vocabulary.     5.2 Research Question 2: Identifying Content- Related Threads  The model created showed reasonably good reliability in   identifying content-related starting posts in StatMed13   (accuracy=0.80, kappa=0.61). Recall was 79%: 315 out of 397   content-related posts were identified. Precision was 79%: an   additional 82 posts were (incorrectly) classified as content-related,   leading to a total of 397 that were assigned this label2. These   results show that, at a basic level, linguistic modeling can reliably   identify starting posts of content-related threads in a statistics   MOOC discussion forum.   5.3 Research Question 3: Predictive Value of  the Number of Views and Votes   Adding the number of views and votes each starting post received   as additional features to the base model did not produce   substantial improvement (Table 3). In addition, models created   using only these features produced very poor classification results   (all kappa < .13). These results indicate the number of views and   votes are not useful for identifying content-related starting posts.   Table 3: Reliability statistics of the base model and with  additional features for StatMed13 (cross-validation)    Base model + #of views + #of votes   Accuracy 0.80 0.80 0.80   Kappa 0.61 0.60 0.61   Recall 0.79 0.79 0.78   Precision 0.79 0.79 0.80   5.4 Research Question 4: Testing the Models  Cross-Course Generalizability  To examine the generalizability of the model, it was first tested on   the data from a second offering of the same course, StatMed14.  Results were consistent with those from the initial data-set   (accuracy=0.81, kappa=0.62). Recall was 85%: 140 out of 165   content-related starting posts were identified. Precision was 81%:   an additional 32 posts were (incorrectly) classified as content-  related, leading to a total of 172 that were assigned this label.   Similar to StatMed13, adding the number of views ad votes did  not improve the model (see Table 4). As StatMed14 is a second  offering of StatMed13, its not surprising that the posts have  highly similar contents and thus similar linguistic features. While   these results are promising, we need to go beyond multiple   offerings of the same class to look at if the model can work across   different courses in the same subject area.   To find out how well the model performs on a different course on   a similar subject, the model was tested on the coded starting posts   from StatLearn. StatLearn was an advanced course for learners  with introductory knowledge of statistics. Although StatMed13  and StatLearn both contained math and statistics content, they  were independent courses. Nonetheless the results were again   largely consistent with those of the previous trials (accuracy=0.80,   kappa=0.60). Recall was 90%: 137 out of 153 content-related   posts were identified. Precision was 76%: an additional 44 posts                                                                     2 It is coincidence that the number of posts coded as content-  related and the number identified by the model were equal.   were (incorrectly) classified as content-related, leading to a total   of 181 that were assigned this label. These results show that the   model has reasonably good generalizability to a different course   on a similar subject. Similar to the two StatMed courses, adding  the number of views and votes did not improve the model (see   Table 5).   Table 4: Reliability statistics of the base model and with  additional features for StatMed14 (test set)    Base model + #of views + #of votes   Accuracy 0.81 0.81 0.81   Kappa 0.62 0.61 0.61   Recall 0.85 0.84 0.86   Precision 0.81 0.81 0.80      Table 5: Reliability statistics of the base model and with  additional features for StatLearn (test set)    Base model + #of views + #of votes   Accuracy 0.80 0.80 0.82   Kappa 0.60 0.59 0.63   Recall 0.90 0.90 0.90   Precision 0.76 0.75 0.78   5.5 Research Question 5: Testing the Model  on Different Time Segments in a Course  The proportion of content-related starting posts in different time   segments varied in both StatMed14 and StatLearn (Table 6). In  StatLearn, the distribution pattern was similar to that anticipated:  a higher concentration of content-related starting posts in the   middle of the course. In StatMed14, however, the proportion of  content-related starting posts increased continually over the time   segments. This may be related to the fact that StatMed14 had a  final exam at the end of the course while StatLearn did not.   Table 6: Results of StatMed13 model on data from different  time segments in StatMed14 and StatLearn   Course Dataset % C A K R P   StatMed14   1st third   (n=102)  47% 0.83 0.67 0.92 0.77   2nd third   (n=101)  53% 0.76 0.53 0.72 0.81   3rd third   (n=101)  62% 0.84 0.66 0.90 0.85   StatLearn   1st third   (n=100)  38% 0.75 0.50 0.84 0.63   2nd third   (n=99)  65% 0.77 0.46 0.89 0.78   3rd third   (n=99)  52% 0.88 0.76 0.94 0.84   % C =percent of content-related starting posts in data subset,   A=accuracy, K=kappa, R=recall, P=precision   To test how well the model classified posts from different time   segments in the courses, the model was applied to each of three   temporal subsets in StatMed14 and StatLearn (Table 6). The     model achieved consistently good classification results   (accuracy > 0.75, kappa > 0.46) across time segments. Overall   model reliability as indexed by kappa was lowest for the middle   time segments in both courses. For StatMed14 this seemed to be  related to a reduced level of recall. For StatLearn, both recall and  precision remained high for the middle time segment, thus the   depression in kappa is due to the greater proportion of actual     content-related posts in the subset3. The first time segment of   StatLearn also showed a substantially lower level of precision  than all other segments across the two courses (0.63 compared   with 0.77 and higher). This may be related to the low level of   content-related posts in this segment.   6. DISCUSSION  This research investigated the automatic identification of content-  related threads as an approach to address problems of information   overload and chaos in MOOC discussion forums. Building on   prior findings that MOOC discussion forum posts can be   categorized as pertaining to the learning of course material or not   [30] and that classification tools can sort posts for users based on   relevance [3], this study extended the line of research by building   a classification model to support students and teachers in finding   content-related posts. Results showed that content-related starting   posts in a statistics MOOC had distinguishing linguistic features   from non-content-related ones that could be used reliably for   identification. Highly weighted extracted features were mainly   terms that did not appear to relate to the topic of statistics; the   number of views and votes a post received were not useful   features for the classification task. The model generated   demonstrated good generalizability to both a second offering of   the same course and a different course on statistics; classification   was also successful across different time segments of the courses.    6.1 Notable Features of the Model  Several findings in our study diverge from previous work. First,   the success of our model upholds the value of simple modeling.   Literature shows that many models built for classification   purposes in MOOC discussion forums have been based on   complicated feature sets [27, 3], but our model was built using   only unigram and bigram linguistic features and achieved   consistently good classification results across three course   (accuracy>.80, kappa>.59, recall>.79, precision>.78). This   indicates that complex feature combinations are not necessarily   needed for useful classification results.    Second, our findings affirm the indicative power of linguistic   features for topic-based post classification in MOOC discussion   forums. Notably, in contrast to the customary practice of   excluding stopwords when choosing linguistic feature for   classification modeling of MOOC posts [1, 4], our test on feature   combinations found that including stopwords improved the model.   In fact, a large proportion of the top features of content- and non-  content-related starting posts were commonly used stopwords.   Therefore it is worthwhile to further explore and interpret the   usefulness of stopwords in topic-based classification.                                                                      3 This exemplifies the critique that has been levied against the   kappa statistic as being overly sensitive to the base rate  proportions, resulting in a low value even when the actual  ratings themselves have high levels of accuracy.   Finally, our findings that the number of views and votes were not   helpful for our classification purposes suggest the need to   reexamine the role of these features in MOOC discussion forums.   Our results echo previous findings [6] that these features are not   good indicators of the content-relatedness of MOOC discussion   posts. In prior work, [27] found the number of views and votes to   be useful in detecting social small talk threads, but not for  distinguishing between other kinds of posts as we sought to do   here. Notably this is different from the situation in more formal   education contexts. For example, in a traditional discussion forum   in a university class the content of messages which were liked  (equivalent to a vote) were of higher cognitive complexity than   those messages which were not liked [21]. The power of learning   context difference is very apparent here: while [21] studied a   small graduate-level course which emphasized online discussions   as a site for social knowledge construction, MOOC discussion   forums are designed to support more diversified functions   including learning support, course management, technical   support, feedbacks, and social interaction.  Given such varied   purposes of use, the indicative power of the number of views and   all-purpose votes are questionable. Instead, it may be fruitful to   have a (limited) variety of types of votes available to users; for   example critical content issue and pressing logistical / technical   concern. Such tailored votes could be useful indicators in future   prediction models.   6.2 Practical Benefits   The model built using the extracted features had not only good   internal validity, but also good generalizability on a second   offering of the same course. This is of particular practical value,   as given the high cost of development, many MOOC instructors   plan to offer the same course multiple times [15]. Moreover, the   model showed almost equivalent reliability on another   independent statistics MOOC, indicating that within this topic   area, there seems to be linguistic consistencies in the way   questions are asked across different courses. These findings   suggest that the developed model could be applicable to a variety   of courses on similar topics offered by different institutions.   Importantly, the verification that the model works for different   time segments in MOOCs suggests substantial value for real-time   application.     The model can be applied to MOOC teaching and learning in   multiple ways. The most straightforward approach is to use it to   drive a live thread-sorting feature in the interface that can help   instructors and learners to navigate the discussion forums and find   threads related to the learning of course material more efficiently.   This can inform instructors intervention decisions in a live course  situation and help learners find learning interaction opportunities   more easily. To give a sense of how this would change   instructors and learners experiences using the forums, consider  that the proportion of content-related posts in StatMed13 was  47%. If an instructor or learner read all discussion threads   indiscriminately, more than half of their effort would be   consumed by ones not related to the course content. With the help   of a filter based on our model, users could narrow their task to   less than half the total threads in the forum (397 out of 837   starting posts were labeled as content-related) of which almost   80% would be content-related ones. In courses where the actual   proportion of content-related posts is lower (for example [6]) use   of the model would bring even larger benefits.   Another use of the model could be a live tagging tool that   analyzes the posts being composed and prompts learners to tag     them as content-related or not, so as to make them easier to locate.   Taken a step further, the model could be used to identify learners   who have particular posting patterns, for example asking some   number of content-related questions or asking a large volume of   exclusively non-content-related questions. Such patterns may be   indicative of particular goal-orientations (e.g. performance,   mastery, work-avoidance) which are more or less beneficial for   learning [32]. This creates the potential for targeted interventions   that could help encourage learners to adjust their engagement   patterns. However, whether it is appropriate to adopt such a   proactive learning-oriented intervention will be largely dependent   on an instructors perspectives on learning and instruction and the  overarching goals of the course.    Moreover, the model could also be applied as a collector that   helps instructors and researchers gather content-related posts from   concluded courses. As content-related posts contain interactions   related to the learning of course material, these posts are quality   data for analysis aiming at improving course design and   understanding learning in MOOCs.     6.3 Potential Domain-Generality  Findings showed that content-related and non-content-related   starting posts in a statistics MOOC had distinct linguistic features.   The top features of content-related starting posts were terms   related to the process of learning and understanding, question   words, terms that connect ideas, and the course topic of statistics   while the top features of non-content-related starting posts were   terms related to the course tasks and platform, effort / action,   appreciation and pronouns. Notably the vast majority of features   across both categories were language related to the course-taking  process (learning, asking, connecting, using the platform,   fulfilling tasks) or interpersonal concerns (pronouns, appreciation)   rather than topic-specific language about statistics. While   somewhat counterintuitive, the contradiction of the notion that   common linguistic features are vulnerable to domain influence   [27] is actually quite logical. Although content-related posts may   contain more statistical language generally, given the diversity of  statistics topics about which questions might be asked, there are   few particular statistics words that will be used frequently enough  to be identified as a top feature. Thus it is the language that   surrounds the asking of such questions (interrogatives, learning   process words, connectors) that are used again and again   regardless of the specific focus of the question being asked.    Though not surprising, the predominance of seemingly domain-  unrelated features is powerful with respect to the potential for a   linguistic model of content-relatedness to generalize across   courses and domains of learning. This runs in contrast to the   recent result by [11] who found that generalized analytics built to   predict academic success (course completion and grade) across   multiple courses both had less predictive power than those built   for specific courses and consistently misidentified the most   relevant predictors in a given situation. They concluded that   attention to the instructional conditions and the ways technology   is used in particular courses must be considered in deciding what   predictive factors to include and how to model them. [11] raises   an important concern for the learning analytics community: the   tension between a desire for portable models that give leverage to   our work and the need to take into account the particularities of   individual learning contexts. Acknowledging this concern, we   limit claim of the potential generality of our model for MOOC   discussions to the condition that it is being uses to classify starting   posts in forums designed for the purpose of Q&A4. As we have   discussed elsewhere [6], there are a variety of other uses to which   discussion forums might be put. The similarity between the   linguistic patterns generated in such situations and here cannot be   assumed and thus would need to be investigated empirically. In   addition, within a Q&A format, even if the key words used are not   domain-specific, there may be differences in the ways in which   learners in different domains ask questions (for example compare   the non-domain specific words in the following questions: Is it  possible to write this algebra function with one definition  versus I am wondering about if, when and how literature can  inspire people to violence). We are currently exploring the  domain generality of the model generated on the statistics   discussions by applying it to increasingly distant subject areas.   6.4 Limitations and Future Research  This study has limitations with respect to the conceptualization,   coding and modeling of content-relatedness, and the use of  thread starting posts as the target for modeling.   First, the choice to classify based on content-relatedness was   made based on common concerns of educators from a learning   perspective. Posts on other topics, such as course logistics may be   important to instructors in other ways and can be targeted   accordingly. In defining the scope of content-related there was a  challenge in being inclusive enough to maximally meet the   practical needs of forum users and at the same time control the   level of noise in sorting results. Thus the delineation of the   content/non-content boundary required decisions of inclusion and   exclusion that could vary depending on a forum user perspective.  The categorization in this research was developed based on   negotiations among the three researchers, all experienced   educators and online teachers. For example, posts that discussed   external resources related to the course material were considered   content-related in that such comments may reflect important ideas   or misconceptions about the course content and thus are worthy of   instructors attention. Similarly, posts that shared resources  related to the course material were also considered content-related   as they could provide learners with extended learning   opportunities. However, in reality, instructors and learners may   want to target a more narrow conception of content-relatedness   due to the limited time and energy they have available.    Second, there were several practical challenges in labeling posts   as content- versus non-content-related. The most notable of these   occurred with error-reporting posts (in which students reported   aspects of the course materials that they thought were in error).   Error-reporting posts were difficult to code for several reasons.   First, there were several types of possible errors. Some were low-  level, such as typos and speaking errors in lecture videos. These   were ostensibly related to the course content but conceptually   were unlikely to lead to meaningful learning. Other posts reported   apparent higher-level errors, such as misapplication or inaccurate   interpretation of concepts, but these posts were not always correct   in what they perceived as an error. These posts revealing a                                                                     4 It could be argued that generalizability should also be limited to   the specific discussion tool used (EdX Forum), however it is not  clear that the structure of this tool influences the language used  within it. In addition as noted in [23], the majority of discussion  forum software currently in use employ a very similar form.     learners misunderstanding are critical ones for the instructor (or  other peers) to address. Finally some posts (correctly or   incorrectly) reported errors related to course logistics, policy, or   technical issues, but used similar error reporting language to the  other error types. Moreover, learners reported perceived errors in   diverse linguistic styles. Some were assertive and straightforward   while others conveyed confusion and/or suspicion. This did not   seem to be related to whether what they were reporting was   actually an error or not. Given all these factors, making coding   decisions for these posts demanded methodological skills, course   subject knowledge, and understanding of specific course and   discourse context, and in many cases had to be made based on   checking available reply posts for more context. The difficulties   and ambiguity surrounding such posts for human raters raise   questions about their quality as a benchmark for machine   classification. In future work a separate error reporting category  would worth exploring.   Third, similar challenges arose from the discovery that learners   often had more than one purpose in making a post, combining   content-related and non-content-related concerns. Since coding   criteria was based on the presence (or absence) of content-related   comments, such posts were not hard for manual coding. However,   the posts themselves contained mixed linguistic features which   could cause confusion in the training and testing of the model. It   is thus not surprising that many of the misclassified posts were   ones which contained multiple (content- and non-content-related)   topics. This is a more difficult problem to address since it is   impractical (though attractive) to oblige students to address one   issue per post. One potential solution could be to manually   segment a data corpus of messages a posteriori (for instance at the   sentence level) for initial model training and subsequently use this   model to detect and flag the presence of both content- and non-  content-related topics in a post or to calculate the percentage of   content-relatedness in each post.   Finally, the model was built with starting post as the analysis unit,   but it is possible for threads that start with a non-content related   post to evolve into a content-related discussion (or visa-versa).   Thus, the current approach could miss emergent content-related   reply streams or incorrectly identify threads which diverge from   an initial content-related focus. Future research can examine the   frequency of such occurrences and, if warranted, include reply   posts into the data to provide more nuanced diagnostics about the   discussions so as to better detect content-related portions of   discussions within threads. Alternatively, it is also possible to use   features extracted from replies to content- and non-content-related   starting posts as additional indicators to assist in the   categorization of threads overall.   7. CONCLUSION  Due to the quantity and disorganization of posts in MOOC   discussion forums, learners and instructors often face an   overwhelming workload to find the relevant ones to read and   which to reply [3, 15]. Specifically, a central goal of MOOCs is   learning, yet a relatively small percentage of posts are   substantially related to understanding the subject matter of the   course. This study targeted the automatic identification of such   content-related posts as a potential tool to help students and   instructors navigate MOOC discussion forums more effectively.   The resulting model, which reliably used linguistic features to   identify content-related starting posts across several statistics   courses and time periods within these courses, makes both   theoretical and practical contributions. Theoretically, the study   enhances understanding of the linguistic qualities of discussion   posts associated with content-relatedness. It also raises doubts   about the use of number of views and votes in MOOC forums as   universal indicators of posts quality. Practically, it is a step toward   a tool for using this information to help learners and instructors   more effectively find content-related posts in MOOCs and thus   derive the intended learning benefits these online discussions have   the potential to provide.   8. ACKNOWLEDGMENTS  We thank Stanford University and the MOOCPosts Dataset team   for their assistance in accessing and working with the data. We   also acknowledge Wan Qi Jin for her helpful discussions with us   about the ideas expressed in this work.   9. REFERENCES  [1] Agrawal, A., Venkatraman, J., Leonard, S., and Paepcke, A.   2015. YouEDU: addressing confusion in MOOC discussion   forums by recommending instructional video clips. In  Proceedings of the 8th International Conference on  Education Data Mining (Madrid, Spain, June 26 - 29, 2015).  ACM, New York, NY, USA, 297-304.    [2] Breslow, L., Pritchard, D. E., DeBoer, J., Stump, G. S., Ho,  A. D., and Seaton, D. T. 2013. Studying learning in the   worldwide classroom research into edX's first MOOC.   Research & Practice In Assessment, 8, 13-25.    [3] Brinton, C.G., Chiang, M., Jain, S., Lam, H., Liu, Z., and  Wong, F.M.F. 2014. Learning about social learning in   MOOCs: from statistical analysis to generative model. IEEE  Transactions on Learning Technologies, 7, 4, 346-359.  DOI= 10.1109/TLT.2014.2337900.   [4] Chandrasekaran, M. K., Kan, M. Y., Tan, B. C., and  Ragupathi, K. 2015. Learning instructor intervention from   MOOC forums: early results and issues. In Proceedings of  the 8th International Conference on Education Data Mining  (Madrid, Spain, June 26-29, 2015). ACM, New York, NY,   USA, 218-225.     [5] Chaturvedi, S., Goldwasser, D., and Daum III, H. 2014.  Predicting instructors intervention in MOOC forums. In  Proceedings of the 52nd Annual Meeting of the Association  for Computational Linguistics (Baltimore, USA, June 23 -  25, 2014). ACL, Baltimore, Maryland, USA, 1501-1511.    [6] Cui, Y., and Wise, A. F. 2015. Identifying content-related  threads in MOOC discussion forums. In Proceedings of the  2nd ACM Conference on Learning @ Scale (Vancouver,  Canada, March 14 - 18, 2015). ACM, New York, NY, USA,  299-303. DOI= 10.1145/2724660.2728679.   [7] DeBoer, J., Ho, A.D., Stump, G.S. and Breslow, L. 2014.  Changing course: reconceptualizing educational variables  for MOOCs. Educational Researcher, 43, 2 (March. 2014),  74-84. DOI= 10.3102/0013189X14523038.   [8] DMello, S., and Graesser, A. 2012. Dynamics of affective  states during complex learning. Learning and Instruction,  22, 2 (April. 2012), 145-157. DOI=  10.1016/j.learninstruc.2011.10.001.   [9] Dringus, L. P., and Ellis, T. 2005. Using data mining as a  strategy for assessing asynchronous discussion forums.   Computers & Education, 45, 141-160.      [10] Gaebel, M. 2013. EUA occational papers: Massive Open  Online Courses. White Paper. European University  Association, Brussel, Belgium.   [11] Gaevi, D. Dawson, S., Rogers, T. and Gaevi, D. 2015.  Learning analytics should not promote one size fits all: the   effects of instructional conditions in predicating academic   success. The Internet and Higher Education, DOI=  10.1016/j.iheduc.2015.10.002   [12] Gtl, C., Rizzardini, R. H., Chang, V., and Morales, M.  2014. Attrition in MOOC: lessons learned from drop-out   students. Learning Technology for Education in Cloud.  MOOC and Big Data Communications in Computer and  Information Science 446 (2014), 37-48. DOI= 10.1007/978- 3-319-10671-7_4.   [13] Herring, S. 1999. Interactional coherence in CMC. Journal  of Computer-Mediated Communication, 4, 4. DOI=  10.1111/j.1083-6101.1999.tb00106.x.   [14] Huang, J., Dasgupta, A., Ghosh, A., Manning, J., and  Sanders, M. 2014. Superposter behavior in MOOC forums.   In Proceedings of the 1st ACM conference on Learning @  scale (Atlanta, USA. March 4 - 5, 2014). ACM, New York,  NY, USA, 117-126. DOI= 10.1145/2556325.2566249.   [15] Hollands, F. M., and Tirthali, D. 2014. MOOCs:expectations  and reality. Report. Center for Benefit-Cost Studies of  Education, Teachers College, Columbia University.    [16] Jiang, Z., Zhang, Y., Liu, C., and Li, X. 2015. Influence  analysis by heterogeneous network in MOOC forums: what   can we discover  In Proceedings of the 8th International  Conference on Education Data Mining (Madrid, Spain, June  26 - 29, 2015). ACM, New York, NY, USA, 242-249.    [17] Khalil, H. and Ebner, M. 2013. How satisfied are you with  your MOOC - a research study on interaction in huge  online courses. In Proceedings of EdMedia 2013 (Victoria,  Canada, June 24, 2013). AACE. 830-839.   [18] Khalil, H. and Ebner, M. 2014. MOOCs completion rates  and possible methods to improve retention - a literature   review. In Proceedings of EdMedia 2014 (Tampere, Finland,  June 23 2014). AACE 1305-1313.   [19] Kizilcec, R. F., Piech, C., and Schneider, E. 2013.  Deconstructing disengagement: analyzing learner   subpopulations in massive open online courses. In  Proceedings of the 3rd International Conference on  Learning Analytics and Knowledge (Leuven, Belgium, 8 - 12  April, 2013). ACM New York, NY, USA, 170-179. DOI=   0.1145/2460296.2460330.   [20] Lerman, K., and Hogg, T. 2014. Leveraging position bias to  improve peer recommendation. PLoS ONE, 9, 6. DOI=  10.1371/journal.pone.0098914.   [21] Makos, A., Lee, K., & Zingaro, D. 2014. Examining the  characteristics of student postings that are liked and linked in   a CSCL environment. British Journal of Educational  Technology, 46, 6, 1281-1294.   [22] Manning, C. D., Raghavan, P., and Schtze, H. 2008.  Introduction to information retrieval. Cambridge: Cambridge  University Press.    [23] Marbouti, F. and Wise, A. F. 2015. Starburst: a new  graphical interface to support productive engagement with   others posts in online discussions. Educational Technology  Research & Development, 1-27. DOI= 10.1007/s11423-015- 9400-y.   [24] McGuire, R. 2013. Building a sense of community in  MOOCs. Campus Technology, 26, 12, 31-33. Retrieved  October 11, 2015, from   https://campustechnology.com/articles/2013/09/03/building-  a-sense-of-community-in-moocs.aspx.    [25] Moore, M.G. 1989. Editorial: three types of interaction.  American Journal of Distance Education, 3, 2, 1-7. DOI=  10.1080/08923648909526659.   [26] Peters, V. L., and Hewitt, J. 2010. An investigation of  student practices in asynchronous computer conferencing   courses. Computers & Education, 54, 951-961. DOI=  10.1016/j.compedu.2009.09.030.   [27] Rossi, L.A and Gnawali, O. 2014. Language independent  analysis and classification of discussion threads in Coursera   MOOC forums. In Proceedings of 2014 IEEE 15th  International Conference on Information Reuse and  Integration (San Francisco, USA, August 13 - 14, 2014).  IEEE, 654-661. DOI= 10.1109/IRI.2014.7051952.   [28] Shah, D. 2014. MOOCs in 2014: breaking down the  numbers. EdSurge News.  Retrieved March 3, 2015, from  https://www.edsurge.com/n/2014-12-26-moocs-in-2014-  breaking-down-the-numbers.   [29] Stephens-Martinez, K., Hearst, M. A., and Fox, A. 2014.  Monitoring MOOCs: which information sources do   instructors value. In Proceedings of the 1st ACM  Conference on Learning@ Scale (Atlanta, USA. March 4 - 5,  2014). ACM, New York, NY, USA, 79-88. DOI=   10.1145/2556325.2566246.   [30] Stump, G. S., DeBoer, J., Whittinghill, J., and Breslow, L.  2013. Development of a framework to classify MOOC   discussion forum posts: methodology and challenges. In   Proceedings of NIPS 2013 Workshop on Data Driven  Education (Lake Tahoe, United States, December 5 - 8,  2013). NIPS Foundation, 1-20.   [31] Trentin, G. 2000. The quality-interactivity relationship in  distance education. Educational Technology, 40, 1, 17-27.    [32] Wise, A., Marbouti, F., Hsiao, Y. and Hausknecht, S. 2012.  A survey of factors contributing to learners listening  behaviors in asynchronous online discussions. Journal of  Educational Computing Research, 47, 4, 461-480.    [33] Yang, D., Adamson, D., and Ros, C. 2014. Question  recommendation with constraints for massive open online   courses. In Proceedings of the 8th ACM Conference on  Recommender systems (Foster City, USA, October 6 -10,  2014). ACM, New York, NY, USA. 49-56. DOI=   10.1145/2645710.2645748.   [34] Yang, D., Piergallini, M., Howley, I., and Rose, C. 2014.  Forum thread recommendation for massive open online   courses. In Proceedings of the 7th International Conference  on Educational Data Mining (London, UK, July 4 - 7, 2014).  ACM, New York, NY, USA, 257-260    "}
{"index":{"_id":"25"}}
{"datatype":"inproceedings","key":"Hecking:2016:ISS:2883851.2883924","author":"Hecking, Tobias and Chounta, Irene-Angelica and Hoppe, H. Ulrich","title":"Investigating Social and Semantic User Roles in MOOC Discussion Forums","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"198--207","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883924","doi":"10.1145/2883851.2883924","acmid":"2883924","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, blockmodeling, discussion forums, socio-semantic analysis","abstract":"This paper describes the analysis of the social and semantic structure of discussion forums in massive open online courses (MOOCs) in terms of information exchange and user roles. To that end, we analyse a network of forum users based on information-giving relations extracted from the forum data. Connection patterns that appear in the information exchange network of forum users are used to define specific user roles in a social context. Semantic roles are derived by identifying thematic areas in which an actor seeks for information (problem areas) and the areas of interest in which an actor provides information to others (expertise). The interplay of social and semantic roles is analysed using a socio-semantic blockmodelling approach. The results show that social and semantic roles are not strongly interdependent. This indicates that communication patterns and interests of users develop simultaneously only to a moderate extent. In addition to the case study, the methodological contribution is in combining traditional blockmodelling with semantic information to characterise participant roles.","pdf":"Investigating Social and Semantic User Roles   in MOOC Discussion Forums   Tobias Hecking  University of Duisburg-Essen   Lotharstrae 63/65  47048 Duisburg, Germany  hecking@collide.info   Irene-Angelica Chounta  University of Duisburg-Essen   Lotharstrae 63/65  47048 Duisburg, Germany  chounta@collide.info   H. Ulrich Hoppe  University of Duisburg-Essen   Lotharstrae 63/65  47048 Duisburg, Germany   hoppe@collide.info       ABSTRACT  This paper describes the analysis of the social and semantic  structure of discussion forums in massive open online courses  (MOOCs) in terms of information exchange and user roles. To  that end, we analyse a network of forum users based on  information-giving relations extracted from the forum data.  Connection patterns that appear in the information exchange  network of forum users are used to define specific user roles in a  social context. Semantic roles are derived by identifying thematic  areas in which an actor seeks for information (problem areas) and  the areas of interest in which an actor provides information to  others (expertise). The interplay of social and semantic roles is  analysed using a socio-semantic blockmodelling approach. The  results show that social and semantic roles are not strongly  interdependent. This indicates that communication patterns and  interests of users develop simultaneously only to a moderate  extent. In addition to the case study, the methodological  contribution is in combining traditional blockmodelling with  semantic information to characterise participant roles.   Categories and Subject Descriptors  H.1.2 [User/Machine Systems]: Human Factors; K.3.1  [Computer Uses in Education]: collaborative learning; H.4.2  [Types of Systems]: Decision support   General Terms  Algorithms, Measurement, Experimentation, Theory.   Keywords  Discussion Forums, MOOCs, Blockmodeling, Socio-semantic  analysis.   1. INTRODUCTION  For online learning courses with no direct interaction between  learners and tutors, discussion forums are commonly used as  communication channels for information exchange between peers.   This is especially the case with massive open online courses  (MOOCs) where, in the absence of individual support by a tutor,  threaded discussion forums are often the only means for  information exchange and peer-to-peer-support provided by the  MOOC platform.   The use of discussion forums in MOOCs has to be considered in a  differentiated way. On one hand, only a small fraction of all  participants in a MOOC use the forum to communicate [23] but  forum activity often goes along with higher engagement in the  course and completion rates [2, 10]. On the other hand, supported  learner discussions in MOOCs have the potential of involving a  large community in sustainable collaborative knowledge building  in a social context (c.f. [12, 30]). In order to provide the necessary  support, there is a strong need for a better understanding of the  structure and function of the existing discussion forums. Further  insights regarding information exchange in discussion forums in  online courses can contribute to improvements with respect to the  design and application of discussion forums or to the development  of new types of communication channels for learners in online  courses.  In this paper we aim to explore the characteristics of structured  information exchange in a MOOC discussion forum. In particular,  a forum community can be structured at least in two dimensions,  namely, the social dimension and the semantic dimension. The  social dimension is represented as a social network with relations  between the users based on their communication  i.e. who is  talking to whom. The semantic dimension reflects the semantic  content the actors discuss in the discussion forum  i.e. who is  talking about what.   Most of existing research on discussion forums in the learning  context focuses on either the social or the semantic dimension.  However, in order to get a more complete picture of the  community based on discussion forums, a combined analysis of  both dimensions is necessary. This requires mixed techniques  from social network analysis and content analysis. To that end, we  investigate the discussion forum of a MOOC with respect to the  social and semantic structure of information exchange and user  roles. The course was named Introduction to Corporate Finance,  it took place over a six-week period (11/2013 to 12/2013) and was  offered at the Coursera1 platform.  As a first step, information-seeking and corresponding  information-giving posts are identified using automatic post  classification. The classified posts are used to model a directed  network of forum users and the information-giving relations  between them. In addition, on the semantic level a user is  represented by the thematic areas of interest in which the actor                                                                    1 https://www.coursera.org/   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  permissions@acm.org.  LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom  Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883924       seeks for information (problem areas) and the areas of interest in  which the actor provides information to others (expertise). While  a social network of forum users explicitly models person-to- person relations, on the semantic level similar interests of two  actors do not necessarily imply a social relation. Blockmodelling  as an existing technique [9] for role modelling of users based on  connection patterns to other users in a social network is extended  by incorporating the semantic models of the users based on their  information-seeking and information-giving interests. We call this  approach socio-semantic blockmodelling. A role can be  interpreted as a group of participants with similar connection  patterns in a thematic context. Inferred relations between those  roles give insights to the main structure of information exchange  between users of different roles in the MOOC discussion forum.  In particular the proposed approach is used to answer the  following research questions:    To what extent does the community of actors in the  discussion forum exhibits a social role structure  discoverable by blockmodels To what extent is the  community structured in semantically coherent  subgroups or sub-communities of interests     To what extent are the social and semantic structure  interdependent    Can socio-semantic blockmodelling reveal the basic  structure of the forum communication in a meaningful  way   The paper is structured as follows: After this introduction Section  2 gives an overview on related work and current developments in  the research on discussion forums in MOOCs. Section 3 describes  the extraction of an information exchange network from Coursera  forum data. An introduction to blockmodelling and a detailed  description of the proposed extension is given in Section 4. The  main findings are summarised in Section 5 and further discussed  in Section 5, which concludes the paper.    2. BACKGROUND AND RELATED WORK  Discussion forums have been widely used in MOOCs to   facilitate communication between participants and to scaffold  collaboration. The collection of posts in a MOOC discussion  forums accounts for the information flow between learners from  various knowledge backgrounds and is an indicator for  collaborative knowledge building between learners with diverse  knowledge backgrounds [33].  Related research has shown that  users engagement in MOOC discussion forum tend to differ.  While many users are not active at all or use the forum purpose- specific (i.e. participants use forums either to ask for assignments  solutions or to get rapid and trustworthy response to specific  questions) [28], MOOC forums are usually dominated by few,  highly active users, who can influence other participants and  stimulate and sustain the discussions [21, 35]. This diverse  behaviour results in different roles of users that can be described  in various aspects using different analysis techniques.    Techniques used for the analysis of MOOC discussion forums can  be characterised as content related or communication related.  Content related analyses aim to uncover the nature of forum  contributions from the post content [31]. Cui and Wise [7] apply  content analysis and machine learning to identify forum threads  where participants discuss the course content which is important  for the investigation of information exchange. Content analysis  can also be used to characterise forum users based on the types of  contributions they made [3, 24]. For communication related  analyses, often social network analysis techniques are applied.   Social networks between users based on common discussion  threads can serve to investigate the coherence of the underlying  social network [15], detection of communication patterns [14] as  well as community support [26]. However, when it comes to role  modelling based on social relations in discussion forums finer  grained network modelling is required such that the concrete post/  reply communication between individuals are adequately  reflected. In discussion forums with nested threads, these relations  can be observed directly from the thread structure [29]. However,  in forums with a more linear thread structure, such as the Coursera  forums investigated in this paper, the identification of direct  communication between users requires content-analytic  appraoches such as discussion act tagging [3]. User roles can then  be inferred based on communication behaviour which is reflected  in the position of an actor in the social network. There are  different possibilities to define user roles in social communication  networks. Abnar et al. [1] use centrality measures in  subcommunities to identify roles such as leaders and mediators in  a forum communication network. In [20]  users are characterised  with respect to the number of help-giving and help-seeking posts  giving higher values to those users who reach more others with  their posts to those who repeatedly target a small set of  communication partners.    In the work described in this paper, techniques of network and  content analysis are combined to characterise roles of users by  blending the position in the information exchange network with  semantic similarity based on content analysis of the threads they  were active in (see Section 4). We have found a somewhat similar  approach in the work of Yang et al. [36] who combine network  data with post content in a single model to identify  subcommunities of learners based on discussion topics and reply  relations in the forum. However, Yang et al. assume that there is  an interplay between users interests and social relations that is  inherently encoded in the model. In our work, however, we  investigate the possible interdependence between social relations  and semantic similarity more closely with respect to user roles in  a network that represents course related information exchange  more explicitly. Also, using block modelling approach, we do not  assume that users with the same role have to form a cohesive  subcommunity.    3. NETWORK EXTRACTION FROM  FORUM DATA  The dataset comprises forum posts from the Coursera MOOC on  Introduction to Cooperate Finance conducted in six weeks  between 11/2013 and 12/2013. Overall there were 8336 posts in  870 different threads by 1540 different users. 1436 posts were  made by anonymous users. Many of the discussion threads are  used by the course participants to introduce themselves, to seek  for learning groups with peers of the same mother tongue, etc. We  explicitly restricted the analysis to discussion threads dedicated  specifically to issues regarding lectures, exercises and quizzes for  the analysis since we are only interested in tracking information  giving and information seeking related to the course content. This  resulted in a dataset of 540 threads with 5533 posts from 945  different users. It is important to note that all anonymous posts  were counted as posts of a single artificial anonymous user.  The starting point of the analysis is the set of forum threads of the  discussion forum. These threads contain a sequence of posts  where for each post the unique identifier, post content, and the  authors identity is available. The analysis relies on the social  network of users who participated in content-related, knowledge  exchange in the discussion forum. In contrast to most of the     existing studies (see Section 2), the network should reflect the  directed relations between users who ask for information and  users who reply to these specific information requests. Thus, the  initial task is to extract this network from the raw forum thread  data and can be structured in three successive steps, namely (1)  post classification, (2) post linking, (3) transformation to a social  knowledge exchange network. An example of the procedure for  the example discussion thread in Table 1 is shown in Figure 1.    Table 1: Example of a discussion thread with three users.   Post User Content Post type   P1 User A I have a problem with     Information- seeking   P2 User B Have you tried the  following    Information-giving   P3 User A That helps. Thank you. Other   P4 User C An alternative solution     Information-giving        Figure 1: Basic scheme of network extraction from forum  posts.   3.1 Post Classification  In order to identify the information giving and information  seeking relations between the users, the first step is to identify the  posts that can be considered as information-giving or information- seeking. In previous studies different types of posts in MOOC  discussion forums are described [3, 16, 22, 24]. In this study the  classification schemes for MOOC discussion forums described by  Arguello and Shaffer [3] and the similar classification of Liu,  Kidzinski and Dillenbourg [24] are generalised to three classes of  posts: information-seeking (all types of questions, clarification  requests, report of an issue), information-giving (answers, issue  resolutions, hints and recommendations), and other posts. For this  task an automated classification model was trained on 500 posts  that were hand-classified by three experts. The validity of the  classification scheme was ensured with a high interrater  agreement among all three raters according to Fleiss-Kappa (! = .78, ' < .005).   The organisation of the course forum in sub forums is used to  filter the dataset prior to the automatic post classification. In  previous work [20], we proposed forum post classification on the  entire dataset incorporating threads that, likely, do not contain  content-related discussions. Social posts, like self-introduction or  requests for study groups, were also classified with considerable  accuracy using since the sub forum is a good predictor for those  posts. In this study, however, information on the sub forum in  which a discussion thread occurs is used to restrict the analysis to   sub forums that are explicitly dedicated to content-related issues  such as assignments and lectures. Posts were encoded by  structural features (position in the thread, number of votes) and  content related features (text length, occurrences of questions  words, question/exclamation marks, and specific phrases such as  need help or helps you). The best results, based on 10-fold  cross validation, were obtained by a random forest classifier [5]  when bagging with 10 iterations was applied. Information seeking  posts can be classified with high F1-score = 0.77. For information  giving posts the F1-score is also moderately high F1-score = 0.66.  However, posts of type other often lead to misclassifications as  the confusion matrix in Table 2 shows.   Table 2: Confusion matrix for post classification.    True inf.  seeking   True inf.  giving     True  other   Class  precision   Pred. inf.  seeking 75 16 2 0.81   Pred. inf.  giving 27 88 30 0.61   pred.   others 0 17 36 0.68   class  recall 0.74 0.73 0.53      In order to reduce the effect of misclassified other posts, the final  classification was improved using the iterative classification  algorithm described by Duinn and Bridge [27]. This algorithm  uses the results from the classifier described above to compute for  each post the number of preceding posts of each class. Then an  additional classifier is trained to incorporate this information  updates the initial classification, which leads to improved results  since misclassifications such as classification as information  giving posts without a preceding information-seeking post can be  avoided. This increases F1-scores for information-giving posts  increases to 0.79 and for information-seeking posts to 0.71 based  on evaluation on another 200 hand-classified posts.   3.2 Network Extraction  Based on the classified posts, we initialise the network of  information seeking and related information giving posts.   As a first step, we remove the anonymous user and isolated users  (users who did not receive a reply to their posts). This resulted in  a network of 647 of the original 1540 users. These users  contributed 4096 posts in 502 threads that spread over 27 of the  40 sub forums. Out of these, 1523 posts were classified as  questions, 1832 posts were classified as answers resulting in  1303 links between the users. 741 posts were classified as other,  and thus, not reflected in the edges of the resulting network. On  average, each user in the network made 4.34 posts (SD=7.246) in  2.61 threads (SD=3.608) over 1.71 forums (SD=1.281). From  these posts, on average per user, 1.61 were classified as  questions (SD=2.740), 1.94 were classified as answers  (SD=4.042) and 0.78 were classified as other (SD=1.603). As it  is shown from these distributions, users have a limited activity in  the discussion forum throughout the course and they do not get  involved or spread over many threads and forums.  In the following, all posts classified as other are filtered out  from each thread such that only the information seeking and  information giving posts remain. As an intermediate step before  the social network between users can be created, a network of  posts has to be built (see Figure 1). The basis for this is the     observation that the users in Coursera discussion forum usually  maintain the structure of a thread themselves, such that the  relations between posts are recognizable. Most content related  threads start with a request for information. This initial request is  either directly answered by another user or further questions  follow until an information giving post occurs in the sequence.  After a sequence of information giving post sometimes further  questions are posted. Comments are attached to a single post. This  helps to relate posts to previous posts even if the discussion has  proceeded and other posts occurred in between. Sequences of  comments attached to a parent posts can be seen as sub-threads  that can contain both types of posts with the parent post as initial  post. Consequently, a forum thread and the corresponding sub- threads based on comments can be decomposed into alternating  sequences of information seeking and information giving posts.  This structure enables the linking of information giving to  previous information seeking posts by linking the posts of each  information-giving sequence to the posts of the most recent  sequence of information-seeking posts in a thread.   In the resulting forum post network each post node is annotated  with the author of the post and a timestamp. Next, each post node,  labelled with the same author, is collapsed into a single node  representing the user (Figure 1) resulting in the final knowledge  exchange network between forum users, similar to the approach  described in [19].     4. APPROACH: SOCIO-SEMANTIC  BLOCKMODELLING  Blockmodelling [9] is a method to reduce a network to a macro  structure by grouping actors groups based on their connection  patterns and modelling relations between them. Those groups are  commonly interpreted as roles or positions since it is assumed that  similar connection patterns indicate the similar function. Figure 2  gives an example of a blockmodel with three roles and relations  between them that reflects the hierarchical structure of the  network. In this section the existing techniques for  blockmodelling based on similarities of connection patterns of  users are described first. The extensions we made incorporate the  semantic similarity of users based on their interest in thematic  areas. This new approach is described in Subsections 4.3 and 4.4.   4.1 Blockmodelling Foundations  In general, blockmodelling groups actors based on a certain notion  of similarity. These groups reflect the roles of the actors and do  not necessarily have to be cohesive, in the sense that actors of the  same role are densely interconnected among themselves. A  blockmodel fitted to the network structure can be used to infer  relations between those groups of actors. In generalized  blockmodelling approach [9] one distinguishes between various  types of relations that can exist between two groups/roles  indicating different types of connection patterns between the  actors of the roles. The most important types of relations for this  work are depicted in Figure 3.   A complete directed relation between two groups A and B is  given if all actors in A have an outgoing relation to all actors in B.  This indicates the strongest possible relationship between two  groups. Regular relations can be seen as a relaxation of a complete  relation. If a regular relation from group A to group B exists, all  actors in A point to at least one actor in B and all actors in B have  at least one ingoing relation to actors in A. Regular relations are  very important for this work since they  reflect information flow.  For information-giving relations between actors, regular relations   between groups can be interpreted as existing information flow  from group A to group B. Note that complete relations are a  special case of regular relations. If no relations between actors in  group A and group B are present, the relationship between the  groups is considered as null relation.     Figure 2: Example network with regular and structural  equivalences.   It is important to note that in forum networks there is often no  perfect fit of the relations between groups of users to the  mentioned relation types. For example, if groups A and B both  contain more than one member and there is only one relation from  an actor in A to an actor in B, the group relation is far from being  regular or complete. However, it can also not be considered as  null-relation as it is defined. In cases were none of the described  relations are applicable, the relation is chosen that can be applied  with minimal modifications of the links between the actors in A  and B. The total number of such modifications is referred as the  blockmodel error.  An important fact that is often ignored is that blockmodelling can  clearly be distinguished from the more common sub-community  detection [13] in social network analysis. Even though, both,  blockmodelling and sub-community detection group users in to  clusters the objectives of these methods are quite different.  Community detection methods aim to find densely connected  substructures in the network by finding a clustering such that the  number of connections within the cluster exceeds the number of  connections between actors of different clusters as much as  possible. Blockmodelling does not require any connections  between actors of the same cluster at all, although they are not  forbidden (see group B in Figure 2). Moreover, in a blockmodel  users belong to the same group since they have similar connection  patterns to users in other groups. Thus, a cluster can be interpreted  as users with similar position or role in the network. In order to  highlight this difference compared to sub-communities based on  dense intra-cluster relations, in the following the groups found by  user similarity are referred to as roles.     Figure 3: Relation types between two groups of actors.     4.2 Graph-based Actor Similarity  Graph based similarity derives actor similarity directly from the  graph structure. This is the traditional approach for  blockmodelling. The benefit of this approach is that actors are  grouped to roles/positions such that the relations described before  between groups of actors are inherently induced by the grouping  of the actors. Graph-based similarity measures that are commonly  applied for blockmodelling are structural and regular similarity.    4.2.1 Structural Similarity  Structural similarity [25] is related to the position of the actors  within the network. Structural similarity can be assesses by  correlations between the connections of each pair of actors. If two  actors are structurally equivalent (maximum structural similarity)  they have ingoing relations from the same set of actors and  outgoing relations to the same set of actors. For example, actors 3  and 4 in Figure 2 are structural equivalent. This means they have  the same position and can be replaced by a single node without  information loss. A perfect assignment based on structural  similarity, i.e. all actors in one role are structural equivalent, leads  to a perfectly fitting blockmodel with only complete and null  blocks. However, finding such a model in forum networks is quite  unlikely. Thus, this type of similarity is not used in the  blockmodels described later in favor of regular similarity  described next.   4.2.2 Regular Similarity  In contrast to structural similarity, regular similarity [34] between  two actors does not explicitly take into account mutual  connections to concrete instances of actors in the network.  Moreover, the regular similarity between two actors measures to  what extent these two have the same connections to classes of  actors. Thus, actors with a high regular similarity are considered  to have the same role in the network. The problem then becomes  assigning roles to actors such that actors within the same role are  as similar as possible with respect to the roles of the actors they  are connected to. If there is an assignment of actors to roles such  that actors within a role are regular equivalent (maximum regular  similarity), the fitted blockmodel has only regular and null blocks  without any errors. For example in Figure 2 a perfect fitting  blockmodel would result from the regular equivalence classes  {{1,2,3,4},{5,6},{7,8}}. In order to compute regular similarity in  this work the REGE algorithm [4] is applied.   4.3 Semantic Similarity  In contrast to graph based similarity described before, semantic  similarity is not computed from the connection patterns in the  social network. Users can have certain properties like interests,  age, gender, etc.. The similarity of two users is calculated based  on the distance of the users property set or vector in a certain  feature space. Thus, blockmodels based on this type of similarity  can be considered as feature based blockmodels [32]. In those  blockmodels roles are induced to the social network from external  observations instead of direct inference from the network  structure.   In our approach, the semantic similarity of users is calculated  from the thematic areas in which they provide information and the  thematic areas in which they seek for information (Figure 4).  More formally, the notion of semantic similarity in MOOC  discussion forums can be described as follows:   Given two users ux and uy. Each user provides (P) information in  subsets of all forum threads +,-, +.-  + and seeks (S) for  information in+,0, +.0  +. The similarity regarding the   information providing interests or expertise can then be calculated  as in equation 1.   123456- 78,, 8.9 =    max 1237 ,,!, +.-9 #$,%'$(  max7|+,-|, *+.-*9  (1)      The term 1237 ,,!, +.-9 corresponds to the similarities between the  ith thread in which user ux provides information and the set of  threads in which user uy provides information. The calculation for  the similarity of their information seeking interest  1234560 78,, 8.9 of two users can be calculated by their sets of  threads in which they ask for information accordingly.  The final semantic similarity of users ux and uy will be defined as  the average of their expertise similarity and the similarity of their  information seeking interests, as given in equation 2.   12345678,, 8.9 =   123456- 78,, 8.9 +  1234560 78,, 8.9  2   (2)     The distinction between information giving and information  seeking interests is crucial for role semantic modelling. A role, in  terms of thematic interests, can be interpreted as users who are  information providers for the themes X and pull information from  themes Y. Furthermore, if the distinction between information  giving and information seeking would not be made, the resulting  blockmodel is likely to contain mostly relations from a certain  role to the role itself and would hardly allow for a distinction  between social and semantic roles since communication in one  thematic area implies corresponding connections in the  information exchange network.        Figure 4: Semantic similarity of two users based on the  similarity of threads in which they provide information  (orange) and seek for information (green).     For the calculation of the similarity between threads which is a  prerequisite for the calculation of the semantic similarity between  users, one has several options. Forum threads can be considered as  documents. Then, one possibility would be to calculate their  semantic similarities based on latent semantic indexing (LSI) [8],  which is a well-known technique from information retrieval. LSI,  in general, derives the similarities between threads based on a  principal component analysis of the columns of a term-document  matrix. An alternative approach, which is used in this work, is to  extract meaningful concepts from the forum threads first and then  calculate the similarity of threads from the average semantic     similarity of the assigned concepts. Concept similarity is  calculated by the UMBC semantic similarity service [17], which  combines latent semantics analysis on large corpora with word net  similarity of the assigned concepts. The concept extraction is done  by the Social Tagging Engine provided by Thompson Reuthers  Open Calais2. It extracts concepts from textual documents by  comparing the documents to to Wikipedia pages. This has several  benefits compared to other approaches for keyword extraction.  First, the concepts do not have to be exactly mentioned in the  thread posts. The assigned concepts generalize the keywords to  higher-order concepts using Wikipedia page titles as a controlled  vocabulary, which can be seen as inherent resolution of  synonyms, polysemy, and disambiguation.  This solves also the  problem of short text and inexact language which is common in  discussion forums. Additionally, this approach has the advantage  of simultaneously assigning meaningful concepts to the threads  which is very helpful for the interpretability of the semantic  clusters found in later steps.   4.4 Socio-semantic Approach  Next, we show how regular similarity (social role modelling) and  semantic similarity (semantic role modelling) can be combined  into a hybrid approach that we call socio-semantic  blockmodelling. The goal is, given an allocation of users to roles,  to identify regular relations between semantic coherent (but not  necessarily socially coherent) roles in the knowledge exchange  network extracted for the forum data. A directed regular relation  from a role A to a role B in a regular similarity blockmodel  indicates information flow from role A to role B since all users in  A give information to at least one user in B and all users in B  receive information of at least one user in A (c.f. Section 4.2.2).  Semantic similarity, as described in Section 4.3, identifies  semantic coherent roles but with possibly heterogeneous  communication patterns. For example, a graph based role  summarizes people who have many outgoing connections  (information providers) to people of a role with many ingoing  connections (information consumers). A semantic role can  characterise users who have problems with topic X or who have  an expertise on topic Y. The combination of both can then be seen  as a social role in semantic context.  On the one hand, if the semantic structure of the community is not  strongly interleaved with the structure of information exchange, it  might be very hard to find regular relations between roles and the  resulting blockmodel is very inaccurate. On the other hand, if the  blockmodel is solely created from role assignments based on  regular similarity, the resulting blockmodel is likely to be more  accurate than a blockmodel derived from semantic similarity since  the roles are discovered using the same criterion that is used to  identify role relations. However, regular similarity identifies role  relations based on communication patterns while ignoring the  interests and semantic coherence of users within a role. The  problem then is to find a good assignment of users to roles such  that the resulting blockmodel is as accurate as possible in terms of  regular role relations (information flow) and a high semantic  coherence within a role. To achieve this, our socio-semantic  approach to blockmodelling combines regular and semantic  similarity in the assignment of users to roles. The roles in this  context can be interpreted differently. For example, information  providers for topic X discovered by the semantic approach, can be  subdivided into different types based on their connection patterns  in the network discovered based on regular similarity.                                                                    2 http://www.opencalais.com/   Combining user features with network structure [32], and finding  the optimal blockmodel with respect to multiple objectives by  optimizing role allocations is a hard problem [6, 18]. An indirect  approach where regular and semantic similarities can be mixed  into a joint similarity by weighted average (equation 3) gives good  results and is feasible for big datasets. Further, varying the values  for the weighting factors allows for investigating the  interdependency between both semantic and social (regular)  similarity, which will be reported in Section 5.   1234-.456/0, 12 = 3455  123455/0, 12 + 3456  123456/0, 12  /3455 + 34562  (3)     Based on this formulation of similarity a blockmodel is derived as  follows:   1. Build a hierarchical clustering based on  1234-.456/0, 12 for each pair of users.   2. Determine the number of roles by cluster bootstrapping  [11], a method that estimates the optimal number of  clusters given distances/similarities of objects and a  clustering function by minimising cluster instability.    3. Assign the role relations such that the blockmodel error  is minimal described in Section 4.1.   The sparsity of the network is a problem since it biases the  inference of relations towards null relations (see Section 4.1). If  the density of a network is too small, assigning null relations  always gives a small blockmodel error. For this reason, the  acceptable error for introducing a regular relation between two  roles is enhanced in relation to the network density as suggested  in [37].     5. RESULTS  Regular similarity inherently assigns users to roles such that the  relations between the roles are either (almost) regular or (almost)  null/non-existent relations. The questions we aim to answer in the  following Section 5.1 is to what extent the social and semantic  structure of the community is interleaved. More concretely, how  well does role assignment based on semantic similarity induces a  blockmodel that has a small error according to regular relations  between roles and whether roles deriving from regular similarity  are also semantically coherent. In Section 5.2 the community in  the MOOC discussion forum is analysed using the hybrid  blockmodelling approach introduced in Section 4.4.   5.1 Semantic vs. Social Structuring  In the following, the relation between the social structure of the  social information exchange network and the semantic structure  based on the similarity of interests/expertise of the users in  thematic areas in the discussion forum is investigated.    Table 3: Correlations between different types of similarities    structural regular semantic   structural 1 -0.19 -0.16   regular -0.19 1 0.36   semantic -0.16 0.36 1     First, we conducted a correlation analysis between the graph- based (social) similarities described in Section 4.2 and semantic  similarity of users (Section 4.3). If social and semantic structure     were highly correlated, role assignment based on graph-based and  semantic similarity would result in very similar blockmodels.  Thus, the parameter settings in equation 3 would have no strong  effect on the result. The Spearman rank correlations between the  different types of user similarities are reported in Table 3. All  correlations are statistically significant (p << .05). There is a low  positive correlation between regular and semantic similarity. This  means that there is no strong interdependence between the  semantic structure based on the information giving and  information seeking interests (semantically induced roles) and the  information flow between roles based on connection patterns  (regular similarity induced roles) in the discussion forum of the  Cooperate Finance MOOC. This indicates that direct  communication between users does not influence their interests  significantly and, vice versa, interests do not affect the social  structure of the community. Structural equivalence correlates on a  very low level negatively with the other similarity measures.  Thus, concrete connections between users can be considered as  independent from the regular role structures and users interests.  In order to further investigate the relations between social and  semantic role structures, we generated blockmodels with a  different emphasis of regular (social) and semantic similarity by  varying the paramters reg and sem (equation 3). For each  blockmodel the normalized blockmodel error (bm_err) is  provided. The semantic dissimilarity of a role is evaluated by the  ratio of the average semantic distance of users within the same  role and the average distance of users of different roles  (wb_ratio). Consequently, a good blockmodel should have a low  values for bm_err and wb_ratio.     Figure 5: Blockmodel error (top) and ratio of average  semantic distance within roles and between roles (bottom) for  different ratios of reg and sem.  The results are presented in Figure 5. For both cases, bw_ratio  and bm_err, there is a state transition between role assignments  that emphasize more on social similarity and role assignments that  emphasize on the semantic similarity of users. The results are  compared to the average wb_ratio and bm_err of 50 blockmodels  based on a random assignment of users to roles. Even if the social  and semantic structure of the community is not strongly related,  there is at least some influence such that, even for the extreme  cases, pure semantic and pure social blockmodels are still better   than random role assignment. These findings support the  assumption that socio-semantic coevolution takes place in the  discussion forum to some extent. Furthermore, this shows that the  community bears a structure in, both, the social dimension and the  semantic dimension. The proposed hybrid blockmodelling  approach described in Section 4.4 can be applied to map the  information flow between different socio-semantic roles, as be  described in Section 5.2.   5.2 Socio-semantic Blockmodelling  In the following, the socio-semantic structure of the forum  communication is analysed based on a hybrid blockmodel. For our  analysis we take into account the semantic coherence of roles as  well as the blockmodel error in terms of regular relations. In order  to do this, first a good level of emphasis of social and regular  similarity according to equation 3 has to be found. Figure 6  depicts the ratio between the blockmodel error bm_error and the  coherence of the roles (1 - wb_ratio) for different values for reg  and sem. As (1 - wb_ratio) has to be as large as possible and  bm_error as small as possible, a good mixture is given for  reg=1 and sem=2.      Figure 6: Ratio between blockmodel error and semantic  coherence of the roles.   The resulting blockmodel is depicted in Figure 7. The nodes  represent the three discovered roles and the edges represent  regular relations between them. The node size corresponds to the  number of users assigned to the role and the edge width to the  number of links present between the roles.   It is shown that there is one dominant role (role 1) comprising of  305 users. It has regular relations not only with the other roles but  also with itself. This means that there is information flow from  role 1 to role 2 and also information flow within the role indicated  by the self-loop. The two smaller roles 2 and 3 have different  connection patterns. Role 2 has only ingoing regular relations to  the other roles and role 3 has only outgoing relations. This  indicates there is a smaller set of users who can be characterised  as information-seekers (role 2) and others as information- providers (role 3). This is further validated by the mean inreach  and outreach of the users (columns 3 and 4 of Table 4). As shown  in [20], in- and outreach combine the post quantity of a user with  the number of connections the user has to others. Users with a  high inreach post many information- seeking posts and receive  information from many different other users. Outreach is defined     similar for outgoing information- giving relations. Thus, inreach  corresponds to information- seeking behaviour and high outreach  to information-giving behaviour. The value for the mean outreach  is very small for role 2 and the value for inreach is small for role  3. However, the largest values for both measures can be found for  role 1. Role 1 can be seen as the core community comprising  information providers and information seekers as well as users  who are both. Roles 2 and 3 can then be seen as users who are  more specialised in their communication behaviour.    On the semantic level the roles can be differentiated with respect  to the thematic areas in which they provide information  (expertise) and areas in which they seek for information (first two  columns of Table 4. For role 1, there is no clear semantic  distinction between information giving and seeking interests  which is also reflected by the self-loop in the blockmodel (Figure  7). The concept Mathematical finance is associated to every role  since it is an important general concept that has been assigned to  many threads by the concept extraction described in Section 4.3.  This is reasonable since many of the assignments in the course  deal with calculations of various values related to corporate  finance. Consequently this concept cannot be used to characterise  the particular roles.    The most frequent other concepts that were extracted from forum  threads in which users of role 1 appear as information seekers and  givers are Investment, Depreciation, and Taxation. These  are some of the main concepts covered during the course.  Participants had to calculate depreciation and investment rates, as  part of their assignments. Issues regarding the calculation itself  and formal requirements (such as the rounding of real numbers)  were discussed among the participants. In particular, the correct  formulas were heavily discussed, such that users of role 1 appear  as both, information givers and information seekers.   The information seeking role 2 has no key concepts assigned to  their information giving interests. These users seek for  information especially in areas related to investments. They  receive help form users in role 1 and role 3 on this topic. The  nature of role 2 is further underlined by the fact that many of the  threads they are active in are additionally annotated with the  keyword question.   Role 3 can be interpreted as experts for the topics related to  investment appraisal. However, despite from being a relatively  small role in terms of number of users and the mean outreach is  moderately high, users in this role could either be the ones who  provide some information to a course topic they are good in and  then stop participating in the forum or show a kind of elder  statesman behaviour in the sense that they occasionally  contribute to the information exchange in the forum as experts in  topics that are of wide interest for the whole community.    In general, it can be said that three discovered socio-semantic  roles structure reflects the general assumptions on MOOC  discussion forums very well. There is a core community (role 1)  that is more engaged in the main discussion topics than other  roles, which can be seen by the higher values for in- and outreach.  There is also communication within this role. The other roles (role  2 and 3) correspond to the users who participate in the forum  communication occasionally and are either information givers or  information seekers on certain topics.       Figure 7: Blockmodel for the forum discussion in the MOOC  discussion forum.     Table 4: Properties of the discovered roles.   Role Top inform.  giving   Top inform.  seeking   Mean  in- reach   Mean  out- reach   1 1. Mathematical  finance                               2. Investment                              3. Depreciation                                  4. Taxation   1. Mathematical  finance                               2. Investment                              3. Depreciation          4. Taxation   8.38 8.45   2 None 1. Mathematical  finance              2. Investment          3. Depreciation  4. Rate of return  5. Question   3.58 0.43   3 1. Mathematical  finance                2. Investment         3. Rate of return  4. Net present   value     1. Ambiguity  2. Decision   theory   0.28 3.08        6. CONCLUSION  We have analysed the social and semantic structure of a  community of learners participating in a MOOC discussion forum  with respect to user roles in social and semantic context. In the  social dimension users were assigned to roles based on their  regular similarity in the information exchange network of forum  users. In the semantic dimension, roles were modelled based on  the thematic areas in which users were active in by providing or  seeking information. Those semantic roles can be also interpreted  as expertise and information seeking for specific themes  respectively.      We applied our approach on the dataset of a discussion forum that  supported the online Coursera course Introduction to Corporate  Finance. Our research objective was three-fold: a) to define to  what extent the forum communities of users are socially and  semantically structured, b) to study to what extent the social and  semantic structures interdependent and c) to explore whether  socio-semantic blockmodelling can reveal meaningful information  about the forum communication structures.   The results of our study showed that both social and semantic role  structures are present in the discussion forum of the course  (Section 5.1). The semantic coherence of user roles with respect to  the semantic similarity of the users scores far better than a random  assignment of users to semantic roles. The same can be stated  about the error of a blockmodel based on regular similarity of the  users in terms their connection patterns in the information  exchange network. Consequently, the community in the  discussion forum did not evolve completely random as it might be  suggested by the known differences of behaviour and engagement  of participants in MOOC discussion forums.   It was also shown that the social roles and the semantic roles of  the user are not completely independent. We discovered a  moderate correlation between the regular similarity of the users in  the network and their semantic similarity. In our hybrid  blockmodels which combine both types of similarity for role  assignment the resulting models had a better fit with respect to  semantic coherence of roles and the blockmodel error with respect  to the regular role relations than random models, even in the  extreme cases (only regular similarity or only semantic  similarity). However, semantic roles and social roles are also not  completely interchangeable which means that forum  communication has only limited influence on the interests of users  and vice versa. External factors such as individual experience as  well as personal communication preferences might also impact the  evolution of the forum communication.  For our dataset, three different roles were discovered based on the  hybrid social-semantic blockmodelling approach. There was a  majority of users who discuss the main course content. While for  the other roles there was only occasional information exchange  between users within the role, users of this majority role also had  heavy communication with each other. Apart from that there were  also users in the two smaller roles who could either be considered  as users who contributed less to the forum communication where  one of the roles contained more information providing users on  specific course topics and the other comprise users who only seek  for information on very concrete issues.   All these findings suggest that there is a need for better support of  information exchange between peers in MOOCs. Advances in the  design of asynchronous communication in online courses should  consider better adaptivity to different needs of different user roles.  As shown, expertise and information-needs in thematic areas are  not well reflected in the social communication structure of the  discussion forum. Results from socio-semantic role modelling can  be used to provide social support, for example, recommendations  that help students to find proper communication partners for  certain thematic areas. This might enhance the engagement of  learners in sustainable knowledge building dialogues and  information exchange in the discussion forum.  In our future work we aim to investigate more MOOC discussion  forums in order to find out whether the structures we have found  for the course described in this paper can be considered as general  patterns of forum communication in such online courses. Open  issues are still to find out which external factors drive the   evolution of the community and the emergence of different user  roles and how users of different roles are engaged in other  activities of the online course. Therefore, on the methodological  level, it can be interesting to incorporate also user similarity based  on resource access or engagement patterns into the role modelling.      7. REFERENCES  [1] Abnar, A., Takaffoli, M., Rabbany, R. and Zaane, O. SSRM:   structural social role mining for dynamic social networks.  Social Network Analysis and Mining, 5, 1 (2015).    [2] Anderson, A., Huttenlocher, D., Kleinberg, J. and Leskovec,  J. Engaging with Massive Online Courses. In Proceedings of  the 23rd International Conference on World Wide Web.  (Seoul, Korea), 2014, 687-698.   [3] Arguello, J. and Shaffer, K. Predicting Speech Acts in  MOOC Forum Posts. In Proceedings of the Ninth  International AAAI Conference on Web and Social Media.  (Oxford, UK), 2015.   [4] Borgatti, S. P. and Everett, M. G. Two algorithms for  computing regular equivalence. Social Networks, 15, 4  (1993), 361-376.   [5] Breiman, L. Random Forests. Machine Learning, 45, 1  (2001), 5-32.   [6] Brusco, M., Doreian, P., Steinley, D. and Satornino, C.  Multiobjective Blockmodeling for Social Network Analysis.  Psychometrika, 78, 3 (2013), 498-525.   [7] Cui, Y. and Wise, A. F. Identifying Content-Related Threads  in MOOC Discussion Forums. In Proceedings of the Second  ACM Conference on Learning @ Scale. (Vancouver, BC,  Canada). ACM, New York, NY, USA, 2015, 299-303.   [8] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T.  K., and Harshman, R. Indexing by latent semantic analysis.  Journal of the American society for information science,  41(6), (1990) 391-407.   [9] Doreian, P., Batagelj, V., Ferligoj, A. and Granovetter, M.  Generalized Blockmodeling (Structural Analysis in the Social  Sciences). Cambridge University Press, New York, NY,  USA, 2004.   [10] Engle, D., Mankoff, C. and Carbrey, J. Courseras  introductory human physiology course: Factors that  characterize successful completion of a MOOC. The  International Review of Research in Open and Distributed  Learning, 16, 2 (2015), 46-68.   [11] Fang, Y. and Wang, J. Selection of the number of clusters via  the bootstrap method. Computational Statistics & Data  Analysis, 56, 3 (2012), 468-477.    [12] Ferschke, O., Howley, I., Tomar, G., Yang, D. and Ros'e  CP. Fostering Discussion across Communication Media in  Massive Open Online Courses. In Proceedings of the 11th  International Conference on Computer Supported  Collaborative Learning. (Gothenburgh, Sweden), 2015, 459- 466.   [13] Fortunato, S. Community detection in graphs. Physics  Reports, 486, 3 (2010), 75-174.   [14] Gillani, N. and Eynon, R. Communication patterns in  massively open online courses. The Internet and Higher  Education, 23, 10 (2014), 18-26.      [15] Gillani, N., Yasseri, T., Eynon, R. and Hjorth, I. Structural  limitations of learning in a crowd: communication  vulnerability and information diffusion in MOOCs. Scientific  Reports, 4 (Sep. 2014), 6447.   [16] Glenda S. Stump, Jennifer DeBoer, Jonathan Whittinghill,  Lori Breslow. Development of a Framework to Classify  MOOC Discussion Forum Posts: Methodology and  Challenges. Available online:  https://tll.mit.edu/sites/default/files/library/Coding_a_MOOC_Discu ssion_Forum.pdf. 02/04/2015.   [17] Han, L., Kashyap, A. L., Finin, T., Mayfield, J. and Weese, J.  UMBC_EBIQUITY-CORE: Semantic Textual Similarity  Systems. In Proceedings of the Second Joint Conference on  Lexical and Computational Semantics, Association for  Computational Linguistics, 2013.   [18] Harrer, A. and Schmidt, A. An Approach for the  Blockmodeling in Multi-Relational Networks. In  Proceedings of the International Conference on Advances in  Social Networks Analysis and Mining, (Istanbul, Turkey)  IEEE, 2012, 591-598.   [19] Harrer, A., Zeini, S. and Sabrina Ziebarth. Visualisation of  the Dynamics for Longitudinal Analysis of Computer- mediated Social Networks - Concept and Exemplary Cases.  In From Sociology to Computing in Social Networks. Theory,  Foundations and Applications. Springer, Vienna, 2010.   [20] Hecking, T,, Harrer, A., Hoppe, H.U. Uncovering the  Structure of Knowledge Exchange in a MOOC Discussion  Forum. In Anonymous Proceedings of the International  Conference of Advances in Social Network Analysis and  Mining. (Paris, France). IEEE, 2015, in press.   [21] Huang, J., Dasgupta, A., Ghosh, A., Manning, J. and  Sanders, M. Superposter Behavior in MOOC Forums. In  Proceedings of the First ACM Conference on Learning @  Scale Conference. (Atlanta, Georgia, USA). ACM, New  York, NY, USA 2014, 117-126.   [22] Kim, S. N., Wang, L. and Baldwin, T. Tagging and Linking  Web Forum Posts. In Proceedings of the Fourteenth  Conference on Computational Natural Language Learning.  (Uppsala, Sweden). Association for Computational  Linguistics, Stroudsburg, PA, USA, 2010, 192-202.   [23] Kizilcec, R. F., Schneider, E., Cohen, G. L. and McFarland,  D. A. Encouraging Forum Participation in Online Courses  with Collectivist, Individualist and Neutral Motivational  Framings. Proceedings of the European MOOCs Stakeholder  Summit, (Lausanne, Swizerland), 2014.   [24] Liu, W., Kidzinski, L. and Dillenbourg, P. Semi-automatic  annotation of MOOC forum posts. In Proceedings of the 2nd  International Conference on Smart Learning Environments.  (Sinaia, Romania), 2015.   [25] Lorrain, F. and White, H. C. Structural equivalence of  individuals in social networks. The Journal of mathematical  sociology, 1, 1 (1971), 49-80.   [26] Malzahn, N., Harrer, A. and Zeini, S. The Fourth Man -  Supporting self-organizing group formation in learning  communities. In Proceedings of the Computer Supported  Collaborative Learning Conference 2007. (New Brunswick,  NJ, USA). ICLS, 2007, 547-550.   [27]  Duinn, P. and Bridge, D. Collective Classification of Posts  to Internet Forums. In Case-Based Reasoning Research and  Development LNCS 8765 (2014), 330-344.    [28] Onah, D. F., Sinclair, J., Boyatt, R. and Foss, J. G. Massive  open online courses: learner participation. In Proceeding of  the 7th International Conference of Education, Research and  Innovation. (Seville, Spain). IATED Academy, 2014, 2348- 2356.   [29] Rabbany, R., Takaffoli, M. and Zaiane, O. R. Analyzing  participation of students in online courses using social  network analysis techniques. In Proceedings of educational  data mining. (Eindhoven, The Netherlands), 2011, 21-30.   [30] Ros Carolyn P, Goldman, P., Zoltners Sherer, J. and  Resnick, L. Supportive technologies for group discussion in  MOOCs. Current Issues in Emerging eLearning, 2, 1 (2015),  5.   [31] Rossi, L. A. and Gnawali, O. Language independent analysis  and classification of discussion threads in Coursera MOOC  forums. In Proceedings of the 15th International Conference  on Information Reuse and Integration, (Redwood City, CA,  USA), 2014, 654-661.   [32] Rossi, R. A. and Ahmed, N. K. Role Discovery in Networks.  CoRR, abs/1405.7134 (2014).   [33] Sharif, A. and Magrill, B. Discussion Forums in MOOCs.  International Journal of Learning, Teaching and  Educational Research, 12, 1 (2015).   [34] White, D. R. and Reitz, K. P. Graph and semigroup  homomorphisms on networks of relations. Social Networks,  5, 2 (1983), 193-234.   [35] Wong, J., Pursel, B., Divinsky, A. and Jansen, B. An  Analysis of MOOC Discussion Forum Interactions from the  Most Active Users. In Social Computing, Behavioral- Cultural Modeling, and Prediction  LNCS 9021, (2015),  452-457.   [36] Yang, D., Wen, M., Kumar, A., Xing, E. P. and Rose, C. P.  Towards an integration of text and graph clustering methods  as a lens for studying social interaction in MOOCs. The  International Review of Research in Open and Distributed  Learning, 15, 5 (2014).   [37] Ziberna, A. Generalized blockmodeling of sparse networks.  Metodolozkizvezki, 10, (2013), 99-119.           "}
{"index":{"_id":"26"}}
{"datatype":"inproceedings","key":"Oleksandra:2016:UML:2883851.2883919","author":"Oleksandra, Poquet and Shane, Dawson","title":"Untangling MOOC Learner Networks","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"208--212","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883919","doi":"10.1145/2883851.2883919","acmid":"2883919","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, forums, interpersonal interactions, networked learning","abstract":"Research in formal education has repeatedly offered evidence of the importance of social interactions for student learning. However, it remains unclear whether the development of such interpersonal relationships has the same influence on learning in the context of large-scale open online learning. For instance, in MOOCs group members frequently change and the volume of interactions can quickly amass to chaos, therefore impeding an individual's propensity to foster meaningful relationships. This paper examined a MOOC for its potential to develop social processes. As it is exceedingly difficult to establish a relationship with somebody who seldom accesses a MOOC discussion, we singled out a cohort defined by its participants' regularity of forum presence. The study, analysed this 'cohort' and its development, in comparison to the entire MOOC learner network. Mixed methods of social network analysis (SNA), content analysis and statistical network modelling, revealed the potential for unfolding social processes among a more persistent group of learners in the MOOC setting.","pdf":"Untangling MOOC Learner Networks  Poquet, Oleksandra   School of Education   University of South Australia   Adelaide, Australia  skrypnyko@gmail.com   Dawson, Shane  Teaching Innovation Unit   University of South Australia  Adelaide, Australia   shane.dawson@unisa.edu.au        ABSTRACT  Research in formal education has repeatedly offered evidence of  the importance of social interactions for student learning.  However, it remains unclear whether the development of such  interpersonal relationships has the same influence on learning in  the context of large-scale open online learning. For instance, in  MOOCs group members frequently change and the volume of  interactions can quickly amass to chaos, therefore impeding an  individuals propensity to foster meaningful relationships. This  paper examined a MOOC for its potential to develop social  processes. As it is exceedingly difficult to establish a relationship  with somebody who seldom accesses a MOOC discussion, we  singled out a cohort defined by its participants regularity of  forum presence. The study, analysed this cohort and its  development, in comparison to the entire MOOC learner network.  Mixed methods of social network analysis (SNA), content  analysis and statistical network modelling, revealed the potential  for unfolding social processes among a more persistent group of  learners in the MOOC setting.   CCS Concepts  Applied computing  Education  E-learning   Keywords  MOOCs, forums, interpersonal interactions, networked learning   1. INTRODUCTION  It is well noted that student relationships formed via in-class  interactions can lead to improved academic performance,  resilience, satisfaction, and sense of belonging in their course of  study [13]. However, the ability to design a learning environment  conducive to such exchanges requires high-level skills in  moderating group processes and an understanding of the socio- emotional aspects of course-discussions. As online learning is  now the new normal for higher education, the design tasks and  discussion moderation processes have become far more complex  for teachers to undertake. The complexities associated with  facilitating effective course discussions are all the more salient  with the advent of Massive Open Online Courses (MOOCs).  Shortcomings of forum technologies and teachers time  constraints limit the capacity for actively promoting interpersonal  interactions amongst the course participants. Whats more,  learners perceive the volume of interactions as overwhelming and  chaotic [9], casting doubt as to whether meaningful student  relationships can actually be formed. In this context, research  construes MOOC forums as learning in crowds, where learners  require well-developed skills to navigate networks of loosely  connected individuals dropping in and out of discussion in a  highly asynchronous manner [11].  Course size and the lack of cohort boundaries call into question  the degree to which trust, safety and similar socio-psychological  constructs associated with closely-knit small groups, occur in  open online learning environments. Research however, is yet to  offer a narrative as to how social processes unfold in MOOCs.  This paper summarizes a work-in-progress regarding what could  constitute a MOOC community of learners  a bounded group  motivated by learning and creating the terms for such learning  through socially shared interactions. Such a group could  potentially cultivate interpersonal relationships and gradually  deepen shared cognitive engagement.   2. PROBLEM FORMULATION  While research into learning at scale is relatively novel, much of  the work to date has been devoid of analytical frameworks distinct  for MOOC settings. Existing practices, especially for LMS- confined courses, largely stem from what worked for bounded  groups coming together to take a class for credit with common  commencement and completion dates. Theories of online  education are also situated in research traditions analysing micro- level interactions in small groups with a local focus. Yet,  academic work continues to draw on the theories and models of  the old paradigm without nuancing these to a rapidly changing  education landscape. Theories are being replanted from a  conventional online course into the open online environment. For  instance, Kilgore and Lowenthal [8] transferred the community of  inquiry model directly to the MOOC context, and reported the  social presence scores. Similarly, Kellogg et al. [7] applied  Gunawardenas knowledge construction framework to an entire  MOOC cohort. In both examples the theoretical lenses assume  progressive interactions of the same learners over an extended  period of time, and it is unclear, for example, how to appropriately  interpret the low levels of knowledge construction or social  presence that is collected from a one-time drop-in forum  participant. In essence, these authors have applied research  frameworks from more formal education settings to learning in an  open environment without testing if the theoretical assumptions  still hold.  While it is uncertain if social processes in MOOCs evolve  similarly to those in formal online courses, researchers analysed  MOOC participation and identified varying student engagement  patterns [1,3]. Classifications of course engagement patterns   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM mustbe  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom.   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883919     differ, and include forum activity as a component, along with  lecture views, course logins, etc. Regardless of the differences, the  presence of more committed learners that sustain course  participation can be inferred through various classifications. The  social dynamics among those with sustained forum activity may  compare to the social processes in formal courses.  This study is geared towards understanding the social forms  emerging on MOOC forums through the analysis of a more  persistent group of forum contributors. For this purpose, we  delineated a sub-group of forum participants contributing to the  forum in a repeated manner, and experiencing timely replies to  their posts. Although the volume of the contributions varied, a  common element among the identified participants was their  repeated presence on the forum. Networks of the entire MOOC  cohort and this delimited group were constructed. Through SNA  the frequency and intensity of co-occurrences of dyads in forum  conversations were identified, along with the structure capturing  these co-occurrences. Since SNA indicated growth of cohorts  activity, we hypothesized that the socio-emotional processes  pertinent to communities may have taken place. Hence,  conversations by the regular participants were qualitatively  analysed in relation to their triggers. Exponential random graph  modelling was, then, applied to test whether the types of  conversations and the structural influence of their contributors  drove the formation of ties.   3. FRAMING THE RESEARCH  Networked learning (NL) was adopted as a theoretical framework  to describe the overlapping relationships that co-exist in the  complex social organizations that manifest in educational settings  [6]. In contrast with most theoretical approaches to learning,  analysing a social entity from the NL perspective does not bias or  privilege the strong relationships that imply closeness and unity of  purpose within a group of actors. In line with sociocultural roots  of NL, the content of ties underpinning the network of regular  contributors was examined to contextualize the relations between  the network actors. Such contextualization was done in relation to  whether the ties appeared to be triggered by cognitive or socio- emotional needs of the learners [10], and whether in relation to  course assignments or other issues.   The study addressed the following research questions:  1. How does a network of regular forum participants compare to  the overall MOOC cohort  2. What is the content shared by the regular participants  3. Do structural learner characteristics and conversation types  define the formation of the regular participants network   4. METHODS  3.1 Data  The data were collected from the Solar Energy MOOC offered by  the Delft University of Technology via the edX platform. The  course lasted eight weeks, comprising 57091 students with 2730  receiving a certificate of completion. The MOOC was designed as  a bachelor level foundation course, and required basic knowledge  of physics and math. Forum facilitation strategy consisted of staff  and student community assistants actively attending to student  concerns.   3.2 User Groups  A total of 5949 students posted on the forum. Some 3221  individuals had received, at least once, a reply from another peer.  This group of individuals is referred to as all learners. The sub- population excluded 2728 individuals whose posts were not   replied to by another forum participant. For the MOOC under  study learner activity stabilized by the first 3 weeks of the course  offering [14]. Thus, participation in a mutual and timely  interaction experienced by a learner in at least three weeks of the  course was chosen as the criterion for the inclusion in the group of  regular participants (regular forum contributors) resulting in a  group of 254 individuals.   3.3 Social Network Analysis  A series of undirected weighted networks for both the all learners  and regular participants groups were constructed. These networks  constituted participants co-occurrences in forum conversations,  i.e. discussions defined by taking turns and contributing answers  to one specific question or problem. That is, if A posted a  question, and B and C replied to it, then A, B and C would all be  linked by undirected edges in a graph.   Instead of week-by-week analyses we approached the MOOC as  lasting one introductory week and three thematic modules. Thus,  the networks were compared across four time periods,  corresponding to the assessed course modules: i) Stage 1 for the  first week of the course; ii) Stage 2 for thematic module 1 in  weeks two and three; iii) Stage 3 for the thematic module 2 in  weeks four, five and six; iv) stage 4 for thematic module 3 in  weeks seven and eight. Using the igraph package in R [2], classic  network measures were calculated for the networks of all learners  and regular forum contributors. Influential participants were  identified by the application of k-means clustering to learners  betweenness and clustering coefficients in the all contributors  network.   3.4 Content Analysis  A bipartite network was constructed to identify shared content  produced by the regular participants. It included 986  conversations where at least two regular participants co-occurred.  These conversations were sorted into four categories. Cognitive  Task included conversations explicitly triggered by course tasks,  such as quizzes and graded assignments. Social Task referred to  participants sense-making of their ability to complete course  tasks or understand course content. These conversations may have  related to the course lectures, quizzes or graded assignments but  focused on participants emotions, such as frustration or  excitement about tasks. Cognitive Non-Task reflected the  participants attempt to engage with the subject matter through  other sources but quizzes, exams or assignments. These explicitly  related to the concepts or interests related to the courses topic, i.e.  conversations about learner-made home pv-systems or politics of  solar energy. Social Non-Task focused on social aspects, i.e.  introductions, norm setting or the purpose of MOOCs. Through  these conversations course participants shared their understanding  of what being in the MOOC should be, and negotiated the  expected and the acceptable values (e.g. whether an infinite  number of attempts on graded quizzes was more appropriate than  one and only pass or fail). During the coding process additional  categories emerged. The MOOC forum contained conversations  asking about the course structure (deadlines and certification) and  technical aspects of the MOOC (technical inquiries). These  conversations were subsequently grouped as Administrative and  Technical Inquiries. The remaining minor categories included a)  undefined conversations overlapping between several categories;  b) discussions related to meta-cognitive skills, such as advice on  how to learn best in a MOOC setting. The inter-rater reliability for  two coders was found to be Kappa= 0.72.   3.5 Statistical Network Modelling     ERGM [12] was applied to understand whether the formation of  network ties in the regular participants network was dependent on  any of the qualitative attributes derived during the research. One  bipartite network captured regular participants contributions and  types of conversations they co-participated in during the eight  official weeks of the course. In this instance, if two people  contributed to the same conversation, instead of being linked to each  other, they would have an undirected tie to the discussion node. For  ERGM, participants were assigned attributes in accordance with  their k-cluster (Section 4.3), while discussions nodes were labeled  under a correspondent category from qualitative analysis (Section  4.4). Data were analyzed with the ergm package in R [4]. Goodness  of fit was estimated as suggested in [5].   4. RESULTS  4.1 Structural Analysis  Research question 1 compared the structure of interactions  underpinning regular forum contributors with that of the broader  cohort. The results suggest that the regular participants network  had a higher potential for establishing relationships within its  dyads. The structure of co-occurrences for regular forum  contributors was not dense, and yet the more present participants  co-occurred more often, their exchanges had more turn taking, and  their participation grew over time, particularly from Stage 1 to  Stage 3. The regular participants network was structurally  dependent on a non-clique of individuals who met more learners,  and exchanged interactions with each other more frequently. Due  to their brokering power (high betweenness and low clustering  coefficient) these individuals brokered information between less  active contributors.  The regular participants network represented about 10% of the  entire cohorts contributors. The activity from regular forum  contributors grew over time up until Stage 4 of the course. In  contrast the activity of all learners on the MOOC forum declined  (Table 1) over the course duration. The maximum edge weight maximum interactions between two individualsis observed  within regular participants network. A forum contributor in the all  learner network co-occurred with the same person about four  times (median degree). The median degree of co-participation  during the entire course for a regular participant was 14 times.   Table 1: Networks of the entire cohort and regular forum  contributors   Entire Cohort Regular Participants   Stages             1 2 3 4 1 2 3 4   Nodes          2042 846 641 493 121 174 213 149   Edges         20628 2253 1993 1425 415 559 832 483   Density         0.01 0.01 0.01 0.01 0.06 0.04 0.04 0.04   Degree          0.11 0.3 0.25 0.4 0.3 0.4 0.4 0.4   Betweenness 0.16 0.3 0.26 0.43 0.2 0.3 0.3 0.4   Closeness    0.001 0.002 0.003 0.01 0.04 0.03 0.06 0.06     Repeated participation and higher frequency of interactions  suggested there was the potential for participants to become aware  of each other. Furthermore, the interactions exchanged by learners  also reflected foreshadowing the establishment of group norms,  characteristic of communities. The growth of the regular  participants network occurred up to Stage 3, but declined at Stage   4. The entire cohorts networks activity grew at that time. Such a  change may have been due to the late staff announcement noting  that students with a perfect exam score and active forum  contributions would be eligible for a draw into a free educational  trip for two MOOC participants. The motivation to fulfil the  eligibility criteria led to a sudden increase in forum activity during  the last two weeks of the course among those previously not  engaged in discussions. This increase in activity was not well  received by the regular participants with learners posting  messages discrediting such participation as superficial.  The two networks differed in relation to their centralization  measures. As anticipated, the regular participants network was  more centralized than the all learners network with a degree  value of 0.6 and 0.2, respectively. The closer the centralization  measure is to 1, the more highly centralized the network is, and  marked by the clear boundary between its core and periphery.  With the consideration of how the network was constructed, here  individuals participating in most conversations were likely to have  highest brokering power.  Table 2: Clusters of regular forum contributors based on their   node-level betweenness and clustering coefficient      Cluster      Participants, N   Cluster Centers   Clustering  Coefficient   Betweenness  Centrality   1 82 -0.88 0.024   2 8 -1.71 4.64   3 47 1.62 -0.25   4 118 0.08 -0.23     To capture the varying influence occupied by regular participants  in the network, we applied k-means clustering to individual  betweenness centrality and their clustering coefficient measured in  the entire course network. Betweenness here represented  participation in various discussions, thus brokering power of  carrying history of the group from one conversation to another.  Clustering coefficient indicated the level of individual  embeddedness in separate conversations. The observed results  (Table 3) explained 93% of the variability in the two selected  network measures for this group of participants. Cluster 2 with  eight individuals of very high betweenness and a low clustering  coefficient describes typical hyperactive participants (mostly,  community assistants) answering and participating in many  conversations. Cluster 1 with some 82 individuals is characteristic  of moderately active students whose structural positions in the  network are represented by their low clustering coefficient and  moderate betweenness.   4.2 Shared Conversations  The second research question investigated the shared content  negotiated through the interactions between the regular  participants. We were interested in the types of conversations  where most regular contributors have participated, as well as the  types of conversations they shared over the course (Table 3 and  4), to gain insight whether group formation processes could have  been present. We observed that in the early course stage, regular  forum contributors had opportunities to converse on issues  building towards group development, such as defining the context,  expectations and setting common ground. Later stages indicate the  increase of cognitive engagement, and last stage offered space for  socio-emotional processes of celebrating and sharing.  Conversations with the highest overlap of participants were of     socio-emotional non-task nature, or of longer depth around  difficult exam questions.   Cognitive conversations about the subject matter or course topic  provided the main content for discussions all throughout the  course. Regular contributors extensively and progressively  conversed about Solar Energy assignments and exams (cognitive  task and non-task conversations). The only exception was the first  week of the course where the regular participants have mostly co- occurred in conversations of socio-emotional non-task nature.    Table 3: Types of conversations by regular forum  contributors at the four different stages of the course   Categories  1 2 3 4   Cognitive   task 14% 39% 65% 49%   non-task 16% 15% 12% 23%   Socio-emotional   task 4% >3% 2% 2%   non-task 43% 10% 4% 4%   Other  info-queries 17% 17% 11% 16%   mixed 3% 6% 4% 3%   Meta-cognitive  1% 2% 1% 0.2%     In the MOOC the cognitive conversations differed, and the  opening and closing of the course were also characterized by  higher volume of content-relevant but not-course specific  discussions of broader Solar Energy issues (cognitive non-task).  Conversations about politics and history of Solar Energy taken  place early in the course were not as value-neutral, as the task  discussions, and may have provided additional opportunities for  establishing a baseline of activity. Relational contingency tables  revealed that both more and less influential learners participated in  cognitive conversations.   Socio-emotional non-task conversations throughout the entire  course served for negotiation of rules and norms typical to  communities, with highest volume in the opening week. Socio- emotional conversations comprised 43% of all interactions in  Stage 1. Some of the bulk of socio-emotional conversations at  Stage 1 can be explained through introductions. While these  overwhelm participants due to their volume, they also provided a  snapshot of those taking the course, and helped participants decide  how much they self-identify with the crowd. For example,  whether an individual shared a common need to take the course  and whether it was of a professional interest to form relationships  beyond simple task-related Q&A. However, socio-emotional  conversations were far more diverse than just introductions. The  regular participants conversed about accepted forum behaviors  such as hacking the system or posting task answers before the  deadline; the need and appropriateness of having staff in the  forums. Students discussed the quality of teaching, provided  suggestions for forum improvement, and reflected on the purpose  of learning in MOOCs, i.e. learning to know or learning to receive  a grade.    Informational and administrative queries comprised up to 17% of  all the conversations shared between regular participants. Many  occasional participants queries were related to simple  administrative issues (deadlines) or simple technical issues (e.g.  typing a formula to score in quizzes). Similar issues had been  raised early on in the course, but then appeared to come up again  and again raised by course latecomers. In relation to the   conversations about the learning strategies, structurally influential  students throughout the course largely contributed to these.  Regular contributors had opportunities to engage in the  development of social processes to define them as a community,  but not all of them were active in these conversations. Regular  participants conversations (Table 4) that received the most input  were either socio-emotional non-task in nature from early in the  course, or cognitive task-related taken place during the exam time  at the end of Stage 2 - Stage 3. Shared socio-emotional non-task  conversations related to the MOOC and what the course  represented for its participants. More central content task-related  conversations were of longer depth and evolved around difficult  exam questions. Finally, informational and technical inquiries  about how to make the system work for participants early on also  received a lot of overlap in regular participants where they  offered constructive solutions and set the tone for what their  community would become.   Table 4: Top 5 conversations with highest regular  participants overlap   Type                                               Interactions, N         Contributors, N                  Occasional Regular   Social Non-task 52 23 15   Cognitive Task/Social Non-Task 46 5 15   Cognitive Task 26 4 15   Informational Query 103 62 13   Social Non-Task 39 12 12   4.3 Statistical Modeling  From the first and the second research questions we concluded  that there was the potential for learners to form peer-relationships  within the regular participants network. It also appeared that  types of conversations were representative of the formation of a  community of learners. Through statistical network modeling, we  inquired if the structural learner characteristics (influential and  others) and conversation types (cognitive, socio-emotional,  informational) defined the formation of the regular contributors  network. We report one large ERGM (AICModel=26109;  AICNullModel=29925) that included main effects controlling for the  effect of cluster activity and discussions types, as well as  homophily and interaction effects (Table 5).  Throughout all attempted models Clusters 1 and 2 had a  significant effect over the network development. It is also  noteworthy that Cluster 2, with extreme propensity to form edges  did not reveal a tendency for homophily, while Cluster 1 did. That  is, the edge to a conversation was likely to have been formed  between two learners that were characterized by somewhat high  betweenness and somewhat low clustering coefficient.  The effects of the discussion types were not significant for  network modeling. Such results could be anticipated given the  limitations of current modeling. First of all, the network did not  model learner interactions at 4 time stages, but all through the  course, which could level the importance of conversations that  were only of relevance in specific course stages. Furthermore,  modeled learner influence was determined only via forum  contributions (posts), and excluded learner-to-learner interactions  resulting from forum views, i.e. how often each learner post was  read and by whom. Since many posts require one answer, a lag in  viewing a post may result in a non-contribution (absence of the  edge). Our future work will include all interactions executed by     the delineated cohort (forum views and forum posts), as well as  account for the lag between a published post and the length of  time for a participant to view the post.   Table 5: ERGM Results             FINAL MODEL    Estimate SE   EDGES -5.43*** 0.1  MAIN EFFECTS    Influential Participants      Cluster 1 1.11*** 0.1   Cluster 2 4.16*** 0.09   Discussion Types     Content Related 0.08 0.1   Socio-emotional 0.01 0.1   Informational -0.16 0.07   HOMOPHILY    Cluster 1 and Cluster 1 0.39*** 0.04  Other Clusters (treated as one) -0.16*** 0.04  INTERACTION    Content-related conversation    Two influential participants     -0.36***     0.04   Two non-influential participants 0.27*** 0.06   One more influential and one less   influential participant  0.07* 0.03   Non-content related conversations     Two influential participants -0.28*** 0.05   Two non-influential participants 0.35*** 0.07   One more influential and one less   influential participant  0.06 0.04   5. CONCLUSIONS  Interpersonal relationships students form can provide cognitive  and emotional support that ultimately benefit the learning process.  However, in the context of MOOCs the sheer size and fluid group  boundaries may impede a learners capacity to establish such  effective relationships. To begin understanding how social  processes unfold at scale, this paper analysed a MOOC forum  crowds potential for developing social processes. Analyses  focused on the social formation within the forum defined by how  often its participants contributed to the forum. Similar to its  bounded counterparts in formal online education, such a group  could potentially cultivate interpersonal relationships and  gradually deepen a shared cognitive engagement.  The imposed cohort was analysed in relation to the frequency  and intensity of interpersonal interactions within it, and to the  prompts behind the groups conversations. In the analysed course,  the MOOC crowd included a group of learners with varying  volume of participation but relatively persistent regularity of  contributions. SNA indicated that through their repeated  participation such a group had more opportunities for establishing  relationships. As the activity of the entire group grew over time,  the more present learners co-occurred more often, and their  exchanges had more turn taking indicating sustained conversation.  Qualitative analyses highlight the types of conversations where  shared group processes occurred. Early course stages offered  opportunities for defining the context, expectations and setting  common ground, later stages indicated the increase of cognitive  engagement, and final stages offered space for socio-emotional  processes of celebrating and sharing. Statistical modelling  highlighted the role of non-clique of brokers and moderately  active learners in developing the network and maintaining group  history.     6. ACKNOWLEDGMENTS  The authors thank TU Delft Online Learning, Thieme Hennis and  Pieter de Vries, Sreko Joksimovi and Brian Keegan.   7. REFERENCES  [1] Coffrin, C., Corrin, L., de Barba, P., & Kennedy, G. (2014).   Visualizing patterns of student engagement and performance in  MOOCs. Proceedings of the Fourth International Conference  on Learning Analytics and Knowledge, 8392.  http://dx.doi.org/10.1145/2567574.2567586   [2] Csardi, G. & Nepusz, T. The igraph software package for  complex network research. InterJournal Complex Systems,  (2006), 1695. http://igraph.org/   [3] Ferguson, R. & Clow, D. (2015). Examining engagement:  analysing learner subpopulations in Massive Open Online  Courses (MOOCs). Proceedings of the Fifth International  Conference on Learning Analytics and Knowledge, 5158.  http://dx.doi.org/10.1145/2723576.2723606   [4] Handcock, M., Hunter, D., Butts, C., Goodreau, S., Krivistky,  P., & Morris, M. (2015). ergm: Fit, Simulate and Diagnose  Exponential-Family Models for Networks. The Statnet Project.  http://www.statnet.org/   [5] Harris, J.K. (2013). An introduction to exponential random  graph modeling. Sage Publications.   [6] Jones, C., Ferreday, D., & Hodgson, V. (2008). Networked  learning a relational approach: weak and strong ties. Journal of  computer assisted learning 24(2), 90102.  http://dx.doi.org/10.1111/j.1365-2729.2007.00271.x   [7] Kellogg, S., Booth, S., & Oliver, K. (2014). A social network  perspective on peer supported learning in MOOCs for  educators. The International Review of Research in Open and  Distributed Learning, 15(5)   [8] Kilgore, W. & Lowenthal, P. (2015). The Human Element  MOOC: An experiment in social presence. Student-teacher  interaction in online learning environments, 373391   [9] Knox, J. (2014). Digital culture clash: massive education in  the E-learning and Digital Cultures MOOC. Distance  Education 35(2), 164177.  http://dx.oi.org/10.1080/01587919.2014.917704   [10] Kreijns, K., Kirschner, P., & Vermeulen, M. (2013). Social  aspects of CSCL environments: A research framework.  Educational Psychologist, 48(4), 229-242.  http://dx.doi.org/10.1080/00461520.2012.750225   [11] Milligan, S. (2015). Crowd-sourced learning in MOOCs:  Learning analytics meets measurement theory. Proceedings of  the Fifth International Conference on Learning Analytics snd  Knowledge, 151155.  http://dx.doi.org/10.1145/2723576.2723596   [12] Robins, G., Pattison, P., Kalish, Y., & Lusher, D. (2007). An  introduction to exponential random graph (p*) models for  social networks. Social networks, 29(2), 173191.  http://dx.doi.org/10.1016/j.socnet.2006.08.002   [13] Thomas, S.L. (2000). Ties that bind: A social network  approach to understanding student integration and persistence.  Journal of Higher Education, 71(5), 591615.  http://dx.doi.org/10.2307/2649261   [14] Vries, P. De, Hennis, T. A., & Skrypnyk, O. (2015). DelftX  MOOCs, the first year (2013-2014) (Report No. 6).  Delft:TU Delf              "}
{"index":{"_id":"27"}}
{"datatype":"inproceedings","key":"Shum:2016:RRW:2883851.2883955","author":"Shum, Simon Buckingham and S'andor, 'Agnes and Goldsmith, Rosalie and Wang, Xiaolong and Bass, Randall and McWilliams, Mindy","title":"Reflecting on Reflective Writing Analytics: Assessment Challenges and Iterative Evaluation of a Prototype Tool","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"213--222","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883955","doi":"10.1145/2883851.2883955","acmid":"2883955","publisher":"ACM","address":"New York, NY, USA","keywords":"education, learning analytics, metadiscourse, natural language processing, reflection, rhetoric, writing analytics","abstract":"When used effectively, reflective writing tasks can deepen learners' understanding of key concepts, help them critically appraise their developing professional identity, and build qualities for lifelong learning. As such, reflecting writing is attracting substantial interest from universities concerned with experiential learning, reflective practice, and developing a holistic conception of the learner. However, reflective writing is for many students a novel genre to compose in, and tutors may be inexperienced in its assessment. While these conditions set a challenging context for automated solutions, natural language processing may also help address the challenge of providing real time, formative feedback on draft writing. This paper reports progress in designing a writing analytics application, detailing the methodology by which informally expressed rubrics are modelled as formal rhetorical patterns, a capability delivered by a novel web application. This has been through iterative evaluation on an independently human-annotated corpus, showing improvements from the first to second version. We conclude by discussing the reasons why classifying reflective writing has proven complex, and reflect on the design processes enabling work across disciplinary boundaries to develop the prototype to its current state.","pdf":"Reflecting on Reflective Writing Analytics: Assessment  Challenges and Iterative Evaluation of a Prototype Tool   Simon Buckingham Shum1, gnes Sndor2, Rosalie Goldsmith3,   Xiaolong Wang1, Randall Bass, Mindy McWilliams4   1 Connected Intelligence Centre  3 Inst. for Interactive Media in Learning   University of Technology Sydney  Broadway, Ultimo, NSW 2007, AUS   {Simon.BuckinghamShum,  Rosalie.Goldsmith,   Xiaolong.Wang} @uts.edu.au   2 Xerox Research Centre Europe  6 chemin Maupertuis   F-38240 Meylan  FRANCE   agnes.sandor@xrce.xerox.com   4 Georgetown University  37th and O Streets N.W.   Washington D.C. 20057, USA  bassr@georgetown.edu   mcwillie@georgetown.edu   ABSTRACT  When used effectively, reflective writing tasks can deepen  learners understanding of key concepts, help them critically  appraise their developing professional identity, and build qualities  for lifelong learning. As such, reflecting writing is attracting  substantial interest from universities concerned with experiential  learning, reflective practice, and developing a holistic conception  of the learner. However, reflective writing is for many students a  novel genre to compose in, and tutors may be inexperienced in its  assessment. While these conditions set a challenging context for  automated solutions, natural language processing may also help  address the challenge of providing real time, formative feedback  on draft writing. This paper reports progress in designing a  writing analytics application, detailing the methodology by which  informally expressed rubrics are modelled as formal rhetorical  patterns, a capability delivered by a novel web application. This  has been through iterative evaluation on an independently human- annotated corpus, showing improvements from the first to second  version. We conclude by discussing the reasons why classifying  reflective writing has proven complex, and reflect on the design  processes enabling work across disciplinary boundaries to develop  the prototype to its current state.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education  General Terms  Algorithms, Design, Experimentation, Human Factors,  Measurement   Keywords  Learning Analytics, Education, Writing Analytics, Reflection,  Natural Language Processing, Metadiscourse, Rhetoric   1.  ACADEMIC REFLECTIVE WRITING   Reflection has long been regarded as a key element in student  learning and professional practice in higher education [2, 10, 16,  18]. It can allow students a window into their developing  professional identity [15], deepen understanding of key concepts  [24], and provide opportunities for lifelong learning [18].  However, it has been so broadly interpreted and implemented in  the university curriculum that the concept of reflection has  become attenuated [28]. Because of such broad interpretations,  defining what is meant by reflection is no easy task [16]. The  definition by Boud, Keogh and Walker [2] provides a useful  perspective:   Reflection is an important human activity in which people  recapture their experience, think about it, mull over &  evaluate it. It is this working with experience that is  important in learning ([2], p.43)    Reflection is thus regarded as an intrinsic element of learning,  especially of experiential learning in professional degree  programs such as teacher education, nursing, engineering and  architecture. As reflection is a social cognitive process, one of the  challenges when using it as a tool for learning is to find ways in  which students can demonstrate their reflective activities [2, 10].   Reflective writing tasks are the most common form of  implementing reflective activities in the university curriculum, as  writing is still the main form of assessment in higher education,  notwithstanding a number of debates surrounding the practice of  reflective writing. These debates include issues such as: how such  tasks should be incorporated into the curriculum, how such  writing should be taught or developed, and how  or indeed  whether  reflective writing should be assessed [2, 26].   However, reflective writing is for many students, and educators, a  novel genre to compose in, and to assess. We introduce the  complexities next (Sec.2), and describe the particular contexts in  which we are using reflective writing (Sec.3). We then introduce  the technical platform we are developing (Sec.4), before moving  to describe the methodology by which we move from rubrics, to  rhetorical patterns (Sec.5). Two iterations of the parser are then  detailed (Sec.6), before the discussion reflects on the complexities  of classifying reflective statements, and the importance of a  participatory process for establishing trust among the diverse  stakeholders in an analytics ecosystem (Sec.7).   2. ASSESSMENT CHALLENGES  The assessment of reflective writing is less straightforward than  for more familiar forms of analytical academic writing. This is in   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org. LAK '16,  April 25-29, 2016, Edinburgh, United Kingdom  2016 ACM. ISBN  978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883955     part because reflective writing is different in nature and purpose;  its intention is to communicate a subjective, personal and  individual interpretation of experiences and what has been learned  from them. Students are encouraged to consider their mistakes and  demonstrate changes in points of view rather than present the  correct answer. Another potentially problematic aspect of  assessing reflective writing is the different perspectives (of  academics and students) on what reflective writing could or  should be. A shared understanding of what constitutes a deep or  superficial reflection is critical to valid and reliable assessment,  but the literature indicates that this has been an ongoing challenge.  Inter-coder reliability has been particularly difficult to establish  [10, 26].  Related to this is the need for a shared language to teach and  assess reflective writing, as identified by Ryan [18] in a project  specifically intended to develop the teaching of reflective writing  across a number of disciplines in an Australian university [20]  Many academics lack the meta-language to identify or explain  what they regard as key elements of deep reflective writing. They  are therefore unable either to give clear directions to students  about how to approach a reflective writing task, or to justify the  marks that they give to students assignments.   Boud and Walker put forward the argument that as reflective  writing is very different in nature and purpose from analytical  academic writing, it should be assessed using criteria that are  sensitive to that particular genre ([3], p.194). In their seminal  paper on how and whether to assess reflective writing tasks,  Sumsion and Fleet make the important point that some students  may reflect deeply but not have mastery of the genre of reflective  writing, whereas other students with stronger writing skills or  abilities to write reflectively may appear to be reflecting without  actually doing so ([26], p.124). This is an aspect of reflective  writing that is difficult to resolve, but is one that is worth trying to  parse in analysis. Additionally, reflective writing often asks  students to reflect on experiences in a personal way. Therefore,  they must decide to what degree they wish to disclose their  uncertainties and vulnerabilities, and understand that expressed  appropriately in academic reflective writing, this will be assessed  as a strength rather than a weakness.   Thus it can be seen that although reflective writing can be a  powerful tool in student learning in the higher education context,  its practice and assessment are by no means straightforward. On  the one hand, there is a risk that students have not been properly  introduced to it as a new form of writing that is relevant to their  studies, and will approach reflective writing tasks in a strategic or  perfunctory manner as simply another assignment to complete as  efficiently as possible. The evidence in the literature cited above  is that they typically respond with superficial descriptions of their  experiences, or with broad statements such as I learned a lot. On  the other hand, as detailed below when we consider assessment, it  is not straightforward to establish a shared understanding amongst  academics (not to mention students) of what appropriate reflection  is when expressed in academic writing, and how it can be  developed and assessed.   Lastly, a critical challenge to address is that of capacity to provide  rigorous assessment and personalised feedback at scale (cf. the  contexts at the University of Technology Sydney and Georgetown  University in Washington, DC, introduced next). When teaching a  large course, the assessment of any written assignment or paper  becomes a daunting task, now made more complex by the  unfamiliar genre. If instructors do not know how to provide  appropriate feedback and grading, this risks confirming in   students minds that this novel kind of reflection is peripheral to,  or an interesting diversion from, the real learning that they  signed up for.   In the light of the evidence of the benefits of reflective writing  reviewed initially, these complexities do not dissuade many  educators from using reflective writing as a way to help students  engage in deeper internalization and meaning-making of their  experiences, as interpreted and analysed through the lens of theory  or discipline. However, our goal is to see how we may lower these  entry barriers to shifting assessment towards deeper reflection  on authentic learning.  This sets the challenging context into which we now introduce  learning analytics. Our working hypothesis is that writing  analytics could in principle be an enabler if a tool can help  educators adopt new practices with reflective writing, with  enhanced formative feedback available to students to help build  their ability. Is reflective writing, in all its complexity, amenable  to natural language processing (NLP), to deliver meaningful  feedback    3. REFLECTIVE WRITING CONTEXTS  3.1 Reflective writing for Engineers (UTS)  At the University of Technology Sydney, all engineering students  in the 4 year degree program undertake two 6-month internships  which are part of the practice program. At the completion of each  internship students are required to submit a reflective report that  details changes in their professional, personal and technical  awareness. The cohort size is approximately 200 per semester,  with reports expected to be 40-50 pages, and hence very time- consuming to mark. It is difficult for tutors to provide formative  feedback on drafts during the semester, both because of the size of  the cohort and because the subject is delivered in block mode. An  initiative is now under way to develop finer-grained assessment  and grading of reflective writing, which contributes to the context  for the writing analytics work reported here.   3.2 Reflective writing for Formation (GU)  For about two years, the Formation by Design project1 at  Georgetown University (GU) has been working (in collaboration  with others, including UTS), to consider how the concept of  formation should shape the university experience   specifically, how do we define, intentionally design for, and  assess this quality As the project defines it: The concept of  formation is at the heart of an education dedicated to shaping  students to be fully human, to cultivating their authentic selves,  and to inhabiting a sense of personal responsibility for improving  the world. The importance of redefining metrics and analytics  sits at the heart of the work: Learning  and especially learner- centered analytics hold much promise as a mechanism for  integrating qualitative and quantitative measures of formation, as  well as visualizing and feeding meaningful data back to  stakeholder groups at every level of the educational ecosystem.  A key approach in this work is the process of internal reflection  that integrates new knowledge and experiences, and creates  meaning from these. Reflective writing, used in academic settings  such as course work following experiential learning, is a  commonly used technique to both provoke the action of reflection,  and capture the product of reflection for interpretation by another  person, most often the course instructor, who uses this product to                                                                     1 https://futures.georgetown.edu/formation      interpret the learning and change in the student that has taken  place.  The Engelhard Project for Connecting Life and Learning at GU,  which aims to increase student well-being and deepen  engagement, has been using reflective writing for ten years in  over 325 courses, creating a corpus of thousands of student  reflections in over 28 disciplines. A sample of this corpus, taken  from courses in Biology, Health Studies, Philosophy, Psychology,  and Sociology, was used in the collaborative effort described in  this paper to explore an analytics-supported approach to  systematically assess the depth and extent to which reflection, and  by extension learning and change triggered by the well-being  module and discussion, was occurring for these students.      4. MODELLING REFLECTIVE WRITING   4.1 NLP platform: XIP  We use the Xerox Incremental Parser (XIP) [1] for automated  reflective writing analysis. XIP is a linguistic analysis engine and  rule writing formalism, which has provided the platform for the  development of high-performance English dependency parsing of  general texts. The input to the analysis is free text, which is  incrementally processed by consecutive NLP steps: from  segmentation into sentences and lexical units, through part-of- speech disambiguation, to extracting syntactic dependency  relationships among the lexical elements. Besides syntactic  analysis, core XIP processing performs general semantic analysis  functions like named entity recognition [5] and semantic  normalization [4]. The maturity of the syntactic and semantic  parsing capability is evidenced by its applications for a wide  variety of NLP tasks including information extraction (e.g. [11]),  sentiment analysis (e.g. [6]) and discourse analysis (e.g. [22]).    XIP includes a salient sentences module that models and detects  relevant rhetorical moves in analytical writing genres like  scientific and scholarly research articles, and research reports [8,  12, 23]. This provides reliable dependency parsing, and an  integrated set of NLP capabilities that provide the necessary   resources to build patterns for capturing features of analytical  writing. See [25] for a more detailed rationale for the use of the  analytical writing parser in education, and a prototype dashboard,  while [25] reports preliminary evaluation in the context of an  analytical writing assignment. The reflective writing parser  documented in this paper is an extension of this XIP module.   4.2 AWA: an end-user application onto XIP  This work is part of a broader development effort at UTS to  rapidly prototype writing analytics of different sorts with staff and  students. Six months prior development effort, in close  partnership with academic staff, had created a web application  called Academic Writing Analytics (AWA) providing an  educational user interface onto XIP.2 This enables a piece of  writing to be submitted for analysis, and the raw output from the  parser is rendered in AWA as interactive highlighted text  (illustrated in Figure 2).    4.3 Related approaches  Although reflective writing has been studied widely, little work  has been devoted to its automated analysis. Besides the  complexity of describing or formalizing the features of reflective  writing, the constitution of annotated corpora and establishing  evaluation measures are major challenges for the task. We are at  present aware of only two other learning analytics projects related  to reflective writing, proposing different methods for reflection  detection, corpus constitution and evaluation.  Ullmann, et al. [27] developed a rule-based categoriser that  decides if a text is reflective or not. Based on theoretical research  in reflective writing they proposed a list of five elements of  reflection: Description of experience, Personal experience,  Critical analysis, Taking perspectives into account and Outcome  of reflective writing. These elements are associated with a set of                                                                     2  AWA information: http://utscic.edu.au/tools/awa    Figure 1: AWAs user interface highlights sentences in the students text which match XIPs rhetorical patterns. Function  Keys such as SH signal the function that the sentence appears to be playing. Mousing over the highlight displays a prompt   reminding the user of the meaning of the F key.     indicators, which are used in 16 rules to detect reflective  sentences. E.g. a rule for detecting Description of an experience is  Past tense sentence with self-related pronoun as subject. Eight  different resources and tools serve as dedicated annotators that  provide input for the rules, such as the Stanford Parser to perform  syntactic parsing for identifying subjects of sentences, and a self- reference annotator that contributes with a list of lexical elements  conveying self-reference.   The whole system is integrated within the UIMA framework.  The  input texts are categorised as either reflective or not reflective  according to the presence and the quantity of the detected  elements of reflection. The system parameters were developed  based on 10 prototypical reflective texts, and the test was carried  out by crowdsourcing paid annotators (via Amazon Mechanical  Turk) to evaluate the presence of the reflective elements in texts.  The texts are a collection of blog posts, and their topic is not  specified in the paper. The results showed a positive correlation  between the reflective features identified by the annotators, and  the texts categorized as reflective by the parser.  Ullmann et al.s rule-based methodology is similar to ours, and  the elements of reflection that they identify as well as the  indicators overlap with the rubrics and patterns described in  Section 5.2. The major difference between the two systems is the  implementation framework and the evaluation method. Whereas  Ullmann et al use an array of different tools for detecting the  different indicators of the reflective elements, and an independent  rule formalism, XIP is a single, modular system implementing  syntactic analysis, lexical resources and the dependency rules that  detect the reflective patterns. We cannot directly compare the  performance results of the two parsers since the results reported in  Ullmann et al refer to a whole-document categorisation task,  while the task XIP performs is to detect and label reflective  sentences without evaluating the whole document as reflective or  not.   In contrast to Ullmann et al., and this paper, Gibson et al. [9]  focus not on the fully automated detection of the linguistic  indicators of academic reflective writing; instead, they aim to  develop a way to model how NLP could support (not automate)  the human identification of anomalies in a text, a potential  ingredient in reflective writing: Essentially, our objective was to  outline the necessary steps that, given an anomaly in one context,  allow a new context to be created in which that anomaly is  resolved, without modifying the original context. Anomalies  include student irony, sarcasm and humour (e.g. Im spending  my weekend marking assignments. I love it - cant imagine doing  anything else), plus moves which may map to the contrast  sentence type described in this paper (further work is needed to  clarify this). Their Anomaly Recontextualization approach thus  seeks to formalise the distinctive human ability to recognise and  make sense of information which is apparently anomalous, until  one reframes the context. They report preliminary results showing  that when supervised, the model is capable of identifying different  kinds of anomalies in student feedback, in relation to a student- supplied rating of progress satisfaction, and an analyst supplied  coding of self-others balance.   5. ITERATIVE DESIGN METHODOLOGY  To summarise, at this point we had now implemented an alpha  prototype application. The availability of an independently   annotated corpus at Georgetown University offered the chance to  conduct a systematic evaluation. We now describe a rapid  prototyping methodology for formalising rubrics into executable  patterns in XIP. In the discussion we reflect on whether this model  could generalize to other contexts.    5.1 Start with informal rubrics   Rubrics are common in education, as an instructional and grading  guide for students and graders as to what good looks like,  sometimes mapped to different grades. The first step in our  process was for the UTS Academic Literacies Researcher (ALR)  who was affiliated with the engineering faculty (Goldsmith), to  provide a set of examples of the kinds of constructions that are  typical signifiers of a reflective move. In order to develop a  greater shared understanding amongst the engineering tutors in the  practice program of what reflective writing is, and how it could be  developed and assessed, the ALR had consulted with one of the  subject coordinators. Through a combination of prior scholarship  in the field to contextualize research for practitioners [14, 19],  direct analysis of engineering students reflective reports, and  discussion with the subject coordinator, the ALR designed the  rubric to identify linguistic features and textual moves commonly  associated with deep or significant reflections (Table 1).      Table 1: Rubrics for reflective writing  1. Describing the context of the event that triggers the   reflection (why, when, where, who, how much, what): the  more detail the better, as long as the event is non-trivial    2. Expressions about learning something specific, e.g. I  learned that (i.e. not merely I learned a lot)   3. Expressions of reflecting specifically, e.g. On reflection I  could see that.    4. Expressions of increased confidence or ability, e.g. I am  more confident, am now able, feel/am comfortable, can  plan, can utilise, can develop a strategy   5. Expressions of experimentation and ability, e.g. I tried, I  tested, I experimented, I developed my capability to, I  was/am able to, I was/am unable to, I practised, I asked, I  sought advice, I overcame, I couldnt overcome   6. Verbs that show awareness or shifts in perception, e.g. I  began to understand, I could see, I could visualise, I could  perceive, I became aware, I became, I grew, I realised, I  recognised   7. Reference to the past: time markers and use of past tense  (e.g. when I started; before my internship); shift between  habitual past tense (e.g. I used to) and the present or the  recent past (e.g. since then I have)   8. Reference to the present and future in the context of  reflecting on changed behaviour, expectations or beliefs,  e.g. since; now; when; as it turned/turns out; it became  clear   9. Expressions of the unexpected and of prior assumptions,  e.g. I thought, I believed, I expected, I assumed; I was  surprised, I didnt think, I didnt expect; I didnt know at first,  I didnt understand; I didnt have adequate; I lacked   10. Expressions of challenge, e.g. I felt challenged, I was  under-prepared, I didnt know how, I wasnt sure, I wasnt  comfortable, I felt inadequate, I felt uncertain, I was  scared/frightened, I was overwhelmed, it was difficult/hard   11. Verbs that show pausing, e.g. I stopped, I paused, this  made me stop, I thought, I reflected   12. Expressions about applying theory to practice, e.g. I could  see how this worked; I learned how to apply; I realised that  there are different ways of doing something; what we were  taught is not how they do things here     5.2 Define formal rhetorical patterns   The next step involved modelling the rubrics as patterns, and  encoding them into XIP. The patterns consist of meta-expressions,  the most basic of which is AUTHORs REFLECTION. It is  instantiated in sentences by any syntactically related pair of words  that refer to the concept of AUTHOR and to the concept of  REFLECTION, e.g. I think, my idea, the suggestion that I  put forward. We have added lexicons to the parser, which are  lists of words and expressions that can instantiate the various  concepts that constitute the meta-expressions. These lexicons are  taken partly from the rhetorical parser previously developed,  partly from the rubrics, and partly from the corpora and various  synonym lists. The lexicons are evolving through the use of the  AWA: as new words come up, they can be added to enlarge the  coverage of the analysis. Since the parser performs dependency  analysis, we could develop rules that identify the instantiations of  the meta-expressions in the sentences.   Figure 2 illustrates how meta-expressions model two of the AWA  reflective sentence categories using the rubrics: The category  Capability includes the AUTHORs REFLECTION on HER  CAPABILITY, and the category Shift in Perception contains the  AUTHORs REFLECTION involving CONTRAST IN her  REFLECTION.    As can be seen, the XIP categories use the examples in the rubrics  as a basis for developing the meta-expressions. Altogether we  have set up the following categories based on the rubrics: Setting  Context (Table 1: 1st and 6th rubrics), Specific Reflection (2nd and  3rd rubrics), Capability (4th and 5th rubrics), and Shift in  Perception (6th rubric). Any words listed in a given rubric that are  not mentioned in the categories all contribute to the lexicons.   Our estimation is that it took the XIP analyst five person days  effort to define and conduct preliminary testing of these new  sentence types, with a day then needed to update AWA to handle  the new XIP output markup, and render them in the user interface.    5.3 Independent reflective writing corpus  A corpus of 30 pieces of student reflective writing (containing 382  sentences) was collected and anonymised, selected from  university courses that were part of the well-being project at GU  described above. Academic staff and linguistics graduate students  coded each writing submission as shallow or deep reflection, as  well as whether the reflection extended beyond the personal self  to the realm of domain or world (typically expressed as the  academic discipline and the students future role as a  professional). Sentence-level highlighting was used to identify  evidence in support of the overall code assigned. Coding consisted  of trial and revision of rating rubrics, independent coding,  subsequent discussion, and finally shared agreement upon coding.  An example of a shallow reflection sentence is: I learned so  much in this class that I will apply in my life. Even though this  student implies a lot of learning occurred, s/he does not go into  detail and describe the learning or the application to life. In  contrast, a student who goes into more depth writes So, this  course really opened my eyes to some new issues that I had not  been aware of before and even to some of the problematic ways I  have been taught about my own identity.    The GU team coded the corpus holistically at the student writing  product level, independent of any knowledge of the underlying  formal rhetorical patterns modelled in the parser just described. In  this sense, they were coding freely as educators, rather than to  test the parser.    6. PARSER EVALUATION   We now describe the methodology by which we evaluated the  parser. As part of the iterative development design, we have tested  two versions to date.    6.1 Results (first iteration)  The quality of classification performed by this first version of the  parser was tested on the independently annotated GU corpus.     TP (true positive) = a sentence labeled as reflective both  by the parser, and the human analyst    TN (true negative) = a sentence not labeled as reflective  either by the parser or the human analyst    FP (false positive) = a sentence labeled as reflective by  the parser, but not by the human analyst    FN (false negative) = a sentence labeled as reflective by  the human analyst, but not by the parser      The confusion matrix from this evaluation is shown in Table 2,  together with the well-established metrics in classification  methodology for Precision, Recall, Accuracy and an overall  indicator F1.     Reflection Type: CAPABILITY  Academics rubric: Expressions of increased confidence or  ability (am more confident, am now able, feel/am comfortable,  can plan, can utilise, can develop a strategy)  Example: [course name] made me think about the ways I can  contribute to the health care system as a person instead of a  simple source of knowledge. I realized that I could make a  difference in people's lives not only by my fieldwork but by  becoming a support system of encouragement and assistance.  XIP Concept Dependencies: AUTHOR REFLECTION +  AUTHOR CAPABILITY  i.e. The sentence contains a REFLECTION by the AUTHOR  and in addition a CAPABILITY word that is syntactically related  to the AUTHOR   XIP output:   [course name] made me  think about the ways I can contribute  to the health care system as a person instead of a simple  source of knowledge     Reflection Type: SHIFT IN PERCEPTION  Academics rubric: Verbs that show awareness or shifts in  perception (I began to understand, I could see, I could  visualise, I could perceive, I became aware, I became, I grew, I  realised, I recognised  Example: However, contrary to my preconceptions, the class  was an eye opening experience in which I was able to connect  with other first year freshman who are going through the same  things I am. Not only did it bring reassurance, but a new  perspective on the transition that accompanies freshman year  of college.  XIP Concept Dependencies: AUTHOR REFLECTION +  CONTRAST IN REFLECTION  i.e. The sentence contains a REFLECTION by the AUTHOR  and CONTRAST IN REFLECTION   XIP output:  However , contrary to my preconceptions , the class was an  eye opening experience in which I was able to connect with  other first year freshman who are going through the same  things I am     Figure 2: From informal rubrics for good reflective  writing, to formal patterns in XIP.       ANALYSTS    Reflective Unreflective   XIP  Reflective TP: 35 FP: 45   Unreflective FN: 55 TN: 247  Precision 0.438 P=TP/TP+FP   Recall 0.389 R=TP/TP+FN   Accuracy 0.738 A=(TP+TN)/(TP+FP+FN+TN)   F1 0.412 F=2PR/(P+R)   Table 2. Results of the first evaluation  Considering the fact that XIPs development and the GU  evaluation were entirely independent, these results were  promising. We had a closer look at the false negatives and the  false positives. Regarding false negatives, we identified three  types of sentences. The first type contained elements that  corresponded to the established patterns, but the words were  missing from the reflective lexicon that the parser was using. In  this case adding the words to the lexicon solved the problem. For  example the following sentence was not recognized as conveying  a SHIFT due to the lack of the word realize in the lexicon:   Over the past year I have come to realize that many of my  close friends seek support and counseling through campus  support and outside healthcare providers.    Once the word is added, the pattern AUTHOR SHIFT is  recognized in the XIP dependency SUBJ-N(realize,I), meaning  that I is the normalized subject of realize.  The second type of false negative contained sentences where no  reflective pattern was found. This is the case in the following  sentence highlighted by the human annotator:   When I walk into a lecture hall, I look for a familiar face,  perhaps one that I met during [course name].   The human analyst identified this sentence as the last of four  sentences that together were representing a reflection on the  students experience in the course, which had resulted in a change  that s/he carried into other settings:    The environment was welcoming and comfortable, so it was  much easier to discuss matters such as those in a classroom  with other students and a professor when normally  conversations of that nature would take place among friends.  [Course name] cultivated an environment where we were able  to learn from each other and build off of other ideas. Looking  back on the semester, I dont think I could have felt as  comfortable and at ease as I do now without this class. When I  walk into a lecture hall, I look for a familiar face, perhaps one  that I met during [course name].    The semantics of shift in the parser, however, includes a shift in  learning or reflection, which is not the case in the last sentence.  This is why it is not selected. In this case the XIP category did not  cover the analysts category, which also included a shift in  behaviour. This may also be a case, discussed in more depth  below under false positives, where the human annotator was  coding the meaningful details that followed the reflective set-up  identified by the parser. The parser had selected as reflective the  third sentence in this example, whereas the human focused on the  result of the reflection, which in this case appears in a new  sentence. If these sentences had been connected by a semi-colon  or woven together, the whole sentence would have been chosen   by the parser to include this content. This is a limitation of the  sentence-level analysis.   The third type of false negative led us to add two new patterns  that were not conveyed by the reflective rubrics: sentences that  describe other peoples point of view and reflections about the  class. The following sentence conveys other peoples point of  view:   For some, it was described as less pressuring and time  constrained than high school, while others felt like college  made them give up some free time they may have had in the  past.   Concerning the false positives, the annotators considered that  several of them could indeed be annotated as deeper reflections,  but they were not highlighted because the same idea had been  expressed earlier in the essay (see description of annotators  feedback below). Some other false positives were the result of too  loose an implementation of the patterns. For example, the  Capability pattern whose rubric is Expressions of increased  confidence or ability (am more confident, am now able, feel/am  comfortable, can plan, can utilise, can develop a strategy)  erroneously classified the following sentence:   Through different people's reactions to this situation I was  able to learn about the different ways people would solve her  situation and whether or not they really felt all that bad for  her deviance.   The solution to this kind of false positive is adding restrictive  rules that exclude them, even though there was agreement that this  is an important category. In this case, we decided to temporarily  exclude the Capability type, because of time constraints for new  rule development.  A major result of this first iteration was that it gave rise to  introducing more subtle rules for filtering out shallow reflections  from the deeper ones. Since the human annotation focused on  high quality reflections, some of the false positives revealed cases  when the sentence did contain a reflective pattern, but the  reflection itself had a shallow content. The following sentence is  an example:   I really enjoyed the freedom of being able to pick whatever  science-related topic interested me.      Taken together the first iteration allowed us to make significant  improvements in the system, as evidenced in the second iteration.  New XIP sentence categories for Superficial (shallow) reflections  were added. Not discussed in this paper were additional categories  where the students reflect on how their experiences relate to what  is being learned in formal Class, and deeper reflections which go  beyond expressing personal views about a context and take into  account the Viewpoints of other stakeholders (see Figure 2 user  interface).   6.2 Results (second iteration)  In developing the second version we took into account the errors  and missed sentences in the first iteration: we expanded the  lexicon, disambiguated some words, and introduced new sentence  labels. Table 3 shows some improvement of the results on the  corpus of 30 annotated texts. As the table shows, adding new  words and filtering out surface reflection, as expected,  significantly improves recall, and somewhat improves precision.  After this preliminary testing, we obtained an expanded corpus of  annotated extracts from the Georgetown University team  containing 312 extracts and 2366 sentences. Table 4 shows the  results of this evaluation compared to Table 3: accuracy did not     decrease significantly, which is promising since the new  evaluation corpus had almost ten times as many sentences as the  first, which increases the number of potential new words that  might not have been recognised by XIP. As for the degradations  in other indices, we discuss this in Sec. 7.2.      ANALYSTS    Reflective Unreflective   XIP  Selected TP: 53 FP: 51   Unselected FN: 32 TN: 278  Precision 0.509 (+0.071) P=TP/TP+FP   Recall 0.623 (+0.234) R=TP/TP+FN   Accuracy 0.799 (+0.061) A=(TP+TN)/(TP+FP+FN+TN)   F1 0.560 (+0.148) F=2PR/(P+R)   Table 3. Results of second test. Brackets show the  improvement with respect to first iteration results in Table 2.     ANALYSTS    Reflective Unreflective   XIP  Selected TP: 129 FP: 494   Unselected FN: 219 TN: 1524  Precision 0.207 (-0.302) P=TP/TP+FP   Recall 0.37 (-0.253) R=TP/TP+FN   Accuracy 0.698 (-0.101) A=(TP+TN)/(TP+FP+FN+TN)   F1 0.266 (-0.294) F=2PR/(P+R)   Table 4. Test results on a larger corpus. Brackets show the  degradation with respect to Table 3.   6.3 Classifying shallow reflections  We have two indirect indications that the parser could detect  shallow reflections. Firstly, we compared the ratio of sentences  labelled as shallow reflection in a good and a poor UTS  engineering report (recall these are sizeable, 40-50 pages), and  found that in the good report 26% of the reflective sentences were  annotated as shallow reflection against 48% in the poor report,  almost twice as many. Although just one case, this corresponds to  the direction one would hope for.   Secondly and more robustly, we compared sentences labelled as  shallow reflection by the parser with the human annotations in the  entire annotated GU corpus of reflections. Of the 209 reflections  marked as shallow by the parser, 49 were annotated by the human  annotators as deep reflection, with the remaining 160 uncoded  (i.e. by implication, shallow). Although more rigorous evaluation  is necessary, these two tests are indicative that the shallow  reflection classifier may add value to the analysis.    7. DISCUSSION AND FUTURE WORK  7.1 Incremental rollout strategy  The first step in validation has been to build the confidence of  reflective writing experts that the XIP parser has a classification  scheme based in sound pedagogy and scholarship. The second  step has been to quantify the performance quality, and we are  encouraged that the parser, developed from scholarship in  engineering reflection, is able to produce coherent results on a test  corpus from other disciplines.  Once the academics are satisfied that AWA adds more value than  distraction, and that the user experience is good enough, the next   step is to introduce students to it. Based on other testbeds  currently under way, the approach will most likely be a  combination of private experimentation by students, survey and  interview data, and detailed user experience evaluations using  video recording and think-aloud protocols.   7.2 A closer look at False Positives  We have shown that from the first to second design iterations, we  were able to demonstrate immediate, albeit minor, improvements  by making small changes of several different sorts to XIP in the  light of feedback from the Georgetown University academics who  had performed the hand coding. As the academics explained when  they were commenting on AWAs output, they approached the  coding of the writing (which was almost all reflective to some  degree) in a more holistic manner than the exhaustive sentence- by-sentence procedure used by XIP (emphasis added). Moreover,  they set the bar high in their criteria:   First, although we coded sentences, we were fairly focused on  assigning a code to the overall essay, so we were focused  more at the student level rather than the sentence level. Our  approach meant that in practice we were highlighting  sentences that were the most reflective, or had the most  evidence. We did not always comprehensively highlight.  Many of the sentences [XIP] found are contained in essays we  had coded as reflective overall, but we had left out that  particular sentence.   Second, because our initial coding and the nature of the  assignment indicated that we had a corpus that was largely  reflective (and we have evidence that 99% of the cases of  these student essays had self-reflection) we left uncoded  reflection that was merely what we would have called  surface-level self-reflection. We only coded sentences that  either pushed the envelope on the depth scale or pushed from  the self to be reflecting on domain or world. In essence, we  agree that there are many surface-level self-reflective  sentences in here that we didnt code. But your parser found a  lot of those!   In looking at the false positives, the human annotators had these  additional observations about the types of patterns that seemed to  generate false positives. One FP pattern seemed to be where the  parser was correctly recognizing sentence-level reflection, but the  annotator had disregarded that sentence as deeply reflective  because it was set in the context of an extremely short piece of  writing, typically containing only two sentences. If an instructor is  looking for meaningful student reflection, it typically does not  occur with the amount of desired detail in two sentences. For  example, the parser identified the second sentence of this two- sentence essay as reflective, but the human coder had ignored it  because of its lack of detail or explication.   Before I came to this class I had never really thought much  about gender and what it means or that it is something that is  fluid. Taking this course was completely eye opening and  really made me think about things I have never had the  chance to think about.     Along similar lines, the human annotators had originally  approached their coding in taking a whole-essay or whole-text  approach. In this approach, essay entries such as the one above  would not have qualified for deep reflection because of lack of  detail. For the purposes of comparing with the sentences  highlighted by the parser, the annotators highlighted sentence- level evidence for the more holistic approach, and probably did so  less systematically than the parser.     Another FP pattern was where the annotator interpreted a sentence  as descriptive whereas the parser highlighted it as reflective.  This  may have been because the annotators were looking specifically  for a personal self-reflection where the student was integrating  content with their own personal experience and thoughts. The  parser, on the other hand, selected sentences where the student  was reflecting generally on the course environment for everyone  or the mode of teaching as being effective.   A third pattern was where the human annotator highlighted a  sentence following one that the parser selected. During analysis, it  became clear that the parser was identifying the reflective set- up, and the human was focusing on the meaningful content that  then followed.  The annotators were not trained in recognizing  particular reflective moves, nor were they coding for these moves.  When reviewing AWAs output, it was clear to the GU analysts  that they were often picking up on the meaningful description,  which came after the linguistic reflective construction. When  these were separated into two different sentences, the coding  between human and parser did not overlap, even though taken as a  whole, they were both finding the same passage.  All efforts to develop and validate writing analytics must navigate  this kind of difference in the way that people and machines make  sense of a text. These specific comments were encouraging in the  sense that the academics had set a high threshold for their  highlighting of deeper reflection. Perhaps in order to be truly  useful to instructors for assessing and to students for feedback and  improvement, an automated parser would ideally need to  incorporate a two-stage process.  The first would involve  identifying sentence-level reflective moves, and the second would  re-evaluate the analysis of the selected sentences within the  context of the whole piece, or the moves that are being made in  that piece.    7.3 On the risks of gaming the system  A justified concern around machine analysis of writing is that  students seek to reverse engineer the features of interest to the  parser, and then reproduce them in a meaningless way. However,  we do not consider this a realistic danger since AWA is not being  used for summative grading purposes, but to provide rapid  formative feedback by highlighting and tagging potentially  relevant reflective elements. The student is thus only fooling  themselves, and in other contexts when we give AWA briefings to  students, we emphasise that the machine will make mistakes, and  that final grade is a function of more factors than the mere  presence of the right rhetorical features. The relevance of the  reflection should take into account the entire content of the  sentence, and as noted, the meaning at the paragraph or even  whole document level, which remains the province of human  interpretation. The opening line of the feedback page reminds  users: AWA does not of course know if it is beautifully crafted  nonsense  you must decide that.   Used formatively, therefore, there should be no secret about  sharing with students the linguistic features driving AWA  quite  the opposite. The rubrics that are the foundation of the automated  analysis should be taught to students as guidelines, providing the  language and exemplars for reflection that are so often missing  from their experience. Moreover, AWAs output will use  terminology consistent with the rubrics. According to this  approach, students should be encouraged to argue with the  machine, and each other, when they disagree with the feedback.  Assuming there is an acceptable signal-to-noise ratio, this is  exactly the higher level of discourse that we want to provoke.   7.4 Participatory design to build trust  We have described a methodology for rapid prototyping as a way  to build trust among key stakeholders. While newcomers to  writing analytics can understand in principle what the potential of  NLP is, it is only when the UTS Academic Literacies Researcher  (ALR), and the academics in UTS and Georgetown University,  could see for themselves how AWA behaved, that this potential  became tangible. Central to this dynamic is good communication,  mediated by the learning analytics R&D team as brokers and  designers. Central to the mainstreaming of writing analytics tools  is trust among the key stakeholders.  This dialogue was conducted through a mix of synchronous and  asynchronous exchanges across three countries. The important  quality is reciprocity, such that all parties are learning from each  other. A key relationship in question is whether the ALR with  expertise in reflective writing trusts that her work is being  translated with transparency (she understands the process) and  integrity into the XIP rhetorical patterns (the results match her  judgements). The Georgetown University academics were not  involved in the design of the initial patterns, but gave feedback on  integrity, which led to conversations about how XIP worked, and  changes being made.   The user interface went through rapid prototyping with the ALR  (and many other UTS academics testing it for their texts), using  think-aloud walkthroughs. The resulting design served as a  sufficiently intuitive rendering that the Georgetown University  team had no difficulty in understanding how to make sense of it  when reviewing and critiquing output.  Trust is built through reciprocity, which in learning analytics  design means ultimately, that you feel you can influence the code.  While the core team can of course directly change AWA, we  envisage offering ways for users (i) to give direct feedback to  AWA on the usefulness of the sentences it is highlighting, and (ii)  to edit the lexicon so that generic and discipline-specific  terminology causing false positives and negatives can be reduced.  We can expand the circle of users able to exert control over their  tools by learning from the end-user development community  who have studied the ecosystems that evolve around software  tools that permit different kinds of end-users to modify the  applications behaviour to differing degrees, and the different user  interfaces and exchange mechanisms that enable this [7, 13].   In principle this approach should generalize to other contexts, and  to other kinds of analytics, depending on the quality of the  common ground and reciprocity that can be established.   8. CONCLUSIONS  We have introduced the distinctive features and purposes of  reflective writing as practiced in educational contexts for decades,  as well as the complexities this creates for teaching, learning and  assessment. This has been the subject of active research  independent of, and preceding, the emergence of learning  analytics. Recognising and understanding this evidence base sets  the context for any learning analytics design efforts.   Given the challenges of teaching, learning and assessing academic  reflective writing, we have identified the potential that NLP  combined with a good user experience can play. A writing  analytics tool such as AWA goes beyond rubrics that make  explicit the important features of this genre of writing in general,  by highlighting the linguistic forms it finds in the students own  text, instantaneously. The educators engaged with the AWA team  do not feel threatened by this kind of machine intelligence;  delivered in this form they see its potential to address weaknesses     in the current system. AWA shows potential as a vehicle for  codifying informal rubrics for academic reflective writing in a  form that is accessible to academics, tutors and students. If AWA  fulfills its promise, we are moving to a scenario of being able to  offer 24/7 formative feedback to learners, on their own drafts or  any other text they choose to reflect on. This feedback could also  form the basis for discussion with peers and/or tutors, a  provocation for sharing their understandings of what deep  reflective writing looks like  especially now that it can be  made visible in new ways.   9. REFERENCES  [1] At-Mokhtar, S., Chanod, J-P., and Roux, C. 2002.   Robustness beyond shallowness:  incremental dependency  parsing. Natural Language Engineering, 8, 2/3 (Aug. 2002),  121-144.   DOI= http://dx.doi.org/10.1017/S1351324902002887    [2] Boud, D., Keogh, R., and Walker, D. 2013. Reflection:  Turning Experience into Learning. Routledge, Abingdon,  Oxon.    [3] Boud, D. and Walker, D. 1998. Promoting reflection in  professional courses: the challenge of context. Studies in  Higher Education, 23, 2 (Aug. 2006), 191-206. DOI=  http://dx.doi.org/10.1080/03075079812331380384    [4] Brun, C., and Hagge, C. 2003. Normalization and  paraphrasing using symbolic methods. Proceedings of the  Second International Workshop on Paraphrasing, Volume  16. Association for Computational Linguistics, 2003. DOI=  http://dx.doi.org/10.3115/1118984.1118990    [5] Brun, C., and Hagge, C. 2004. Intertwining deep syntactic  processing and named entity detection. Advances in Natural  Language Processing. Springer: Berlin, 2004. 195-206.  DOI= http://dx.doi.org/10.1007/978-3-540-30228-5_18   [6] Brun, C., Popa, D.N., and Roux, C. 2014. XRCE: Hybrid  classification for aspect-based sentiment analysis. Proc. 8th  International Workshop on Semantic Evaluation (SemEval  2014), 838842. (Dublin, Ireland, August 23-24, 2014).    [7] Burnett, M.M., and Scaffidi, C. 2011. End-User  Development. The Encyclopedia of Human-Computer  Interaction, 2nd Edition. (Eds. Mads Soegaard and Rikke  Friis Dam). Interaction Design Foundation:  https://www.interaction-design.org/literature/book/the- encyclopedia-of-human-computer-interaction-2nd-ed    [8] De Liddo, A.,  Sndor. ., and Buckingham Shum, S. 2012.  Contested collective intelligence: Rationale, technologies,  and a human-machine annotation study. Computer Supported  Cooperative Work (CSCW) 21.4-5 (2012): 417-448. DOI=  http://dx.doi.org/10.1007/s10606-011-9155-x   [9] Gibson, A. and Kitto, K. 2015. Analysing reflective text for  learning analytics: an approach using anomaly  recontextualisation. Proceedings of the Fifth International  Conference on Learning Analytics & Knowledge. ACM: NY.  DOI= http://dx.doi.org/10.1145/2723576.2723635   [10] Hatton N. and Smith, D. 1995. Reflection in teacher  education: Towards definition and implementation. Teaching  & Teacher Education. 11, 1 (Jan. 1995), 33-49. DOI=  http://dx.doi.org/10.1016/0742-051X(94)00012-U   [11] Huang, Z., ten Teije, A., van Harmelen, F., and At-Mokhtar,  S. 2014. Semantic representation of evidence-based clinical  guidelines. Proc. 6th International Workshop on Knowledge  Representation for Health Care. 2014: Vienna (21 July  2014). p. 78-94. DOI= http://doi.org/10.1007/978-3-319- 13281-5_6   [12] Lisacek, F., Chichester, C., Kaplan, A., and Sandor, . 2005.  Discovering paradigm shift patterns in biomedical abstracts:  application to neurodegenerative diseases. In Proceedings of  the First International Symposium on Semantic Mining in  Biomedicine (SMBM) (41-50).    [13] MacLean, M., Kathleen Carter, Lvstrand, L., and Moran, T.  1990. User-tailorable systems: pressing the issues with  buttons. Proc. Conference on Human Factors in Computing  Systems. ACM, New York, 175-182. DOI=  http://dx.doi.org/10.1145/97243.97271   [14] Moon, J. 2010.  Reflective Learning Workshop (Handout  10/07), University of Worcester, UK. http://worc.ac.uk  /edu/documents/Jenny_Moon_RefLearnlong07.doc   [15] Reidsema, C., Goldsmith, R., & Mort, P. 2010. Writing  to learn: reflective practice in engineering design,  Proceedings of ASEE Symposium Singapore, October 18-21    [16] Rodgers, C. 2002. Defining reflection: Another look at John  Dewey and reflective thinking. Teachers College Record.   104.4 (2002): 842-866.   http://www.tcrecord.org/content.aspcontentid=10890    [17] Russell, R. 2001. Reflection in higher education: A concept  analysis. Innovative Higher Education. 21.6 (2001). DOI=  http://dx.doi.org/10.1023/A:1010986404527   [18] Ryan, M. 2011. Improving reflective writing in higher  education: a social semiotic perspective, Teaching in Higher  Education, 16, 1 (Jan. 2011), 99-111. DOI=  http://doi.org/10.1080/13562517.2010.507311    [19] Ryan, M. 2010. The 4 Rs Model of Reflective Thinking,  Version 1.5. Developing Reflective Approaches to Writing  (DRAW) Project, Queensland University of Technology.  http://www.citewrite.qut.edu.au/write/4Rs-for-students- page1-v1.5.pdf   [20] Ryan, M. & Ryan, M. 2012. Developing a systematic, cross- faculty approach to teaching and assessing reflection in  higher education. Office of Learning and Teaching.  http://www.olt.gov.au/system/files/resources/PP9_1327_Rya n_report_2012.pdf    [21] Sndor, ., Kaplan, A., and Rondeau, R. 2006. Discourse  and citation analysis with concept-matching. International  Symposium: Discourse and Document. (Caen, 15-17 June  2006).    [22] Sndor, . 2007. Modeling metadiscourse conveying the  author's rhetorical strategy in biomedical research abstracts.  Revue Franaise de Linguistique Applique, 12, 2, 97-108.    [23] Sndor, ., and Vorndran, A. 2009. Detecting key sentences  for automatic assistance in peer reviewing research articles in  educational sciences. Proceedings of the Workshop on Text  and Citation Analysis for Scholarly Digital Libraries, 47th  Annual Meeting of the Association for Computational  Linguistics (Singapore, 2-7 Aug, 2009).   [24] Scouller, K. 1998. The influence of assessment methods on  students learning approaches: multiple choice question  examination versus assignment essay. Higher Education, 35,  4, 453-472.   DOI= http://dx.doi.org/10.1023/A:1003196224280   [25] Simsek, D., Buckingham Shum, S., Sndor, ., Liddo, A. D.,  and Ferguson, R. 2013. XIP Dashboard: Visual analytics  from automated rhetorical parsing of scientific  metadiscourse. 1st International Workshop on Discourse- Centric Learning Analytics, 3rd  International Conference on  Learning Analytics & Knowledge (Leuven, BE, Apr. 8-12,  2013).    [26] Sumsion, J. and Fleet, A. 1996. Reflection: can we assess it  Should we assess it. Assessment & Evaluation in Higher     Education, 21, 2 (July. 2006), 121-130. DOI=  http://dx.doi.org/10.1080/0260293960210202   [27] Ullmann, T. D., Wild, F. and Scott, P. 2012. Comparing  automatically detected reflective texts with human  judgements. In 2nd Workshop on Awareness and Reflection  in Technology-Enhanced Learning, Saarbruecken, Germany,  September 2012, 101-116.    [28] Webster-Wright, A. 2013. The eye of the storm: a mindful  inquiry into reflective practices in higher education.  Reflective Practice, 14, 4 (May. 2013) 556-567. DOI=  http://dx.doi.org/10.1080/14623943.2013.810618   [29] Xerox Incremental Parser. Open Xerox Documentation:  https://open.xerox.com/Services/XIPParser        "}
{"index":{"_id":"28"}}
{"datatype":"inproceedings","key":"Zhu:2016:LEP:2883851.2883934","author":"Zhu, Mengxiao and Bergner, Yoav and Zhang, Yan and Baker, Ryan and Wang, Yuan and Paquette, Luc","title":"Longitudinal Engagement, Performance, and Social Connectivity: A MOOC Case Study Using Exponential Random Graph Models","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"223--230","numpages":"8","url":"http://doi.acm.org/10.1145/2883851.2883934","doi":"10.1145/2883851.2883934","acmid":"2883934","publisher":"ACM","address":"New York, NY, USA","keywords":"ERGM, MOOC, exponential random graph model, forum participation, learning, network analysis","abstract":"This paper explores a longitudinal approach to combining engagement, performance and social connectivity data from a MOOC using the framework of exponential random graph models (ERGMs). The idea is to model the social network in the discussion forum in a given week not only using performance (assignment scores) and overall engagement (lecture and discussion views) covariates within that week, but also on the same person-level covariates from adjacent previous and subsequent weeks. We find that over all eight weekly sessions, the social networks constructed from the forum interactions are relatively sparse and lack the tendency for preferential attachment. By analyzing data from the second week, we also find that individuals with higher performance scores from current, previous, and future weeks tend to be more connected in the social network. Engagement with lectures had significant but sometimes puzzling effects on social connectivity. However, the relationships between social connectivity, performance, and engagement weakened over time, and results were not stable across weeks.","pdf":"Longitudinal Engagement, Performance, and Social  Connectivity: a MOOC Case Study Using Exponential Random   Graph Models   Mengxiao Zhu    Yoav Bergner   Yan Zhang  Educational Testing Service   Princeton, NJ 08541   mzhu, ybergner, yzhang002   @ets.org  Ryan Baker   Yuan Wang  Teachers College   Columbia University   New York, NY 10027    ryanshaunbaker @gmail.com    elle.wang@columbia.edu   Luc Paquette     University of Illinois, U-C   Champaign, IL 61820   lpaq@illinois.edu     ABSTRACT  This paper explores a longitudinal approach to combining  engagement, performance and social connectivity data from a  MOOC using the framework of exponential random graph  models (ERGMs). The idea is to model the social network in the  discussion forum in a given week not only using performance  (assignment scores) and overall engagement (lecture and  discussion views) covariates within that week, but also on the  same person-level covariates from adjacent previous and  subsequent weeks. We find that over all eight weekly sessions,  the social networks constructed from the forum interactions are  relatively sparse and lack the tendency for preferential  attachment. By analyzing data from the second week, we also  find that individuals with higher performance scores from  current, previous, and future weeks tend to be more connected in  the social network. Engagement with lectures had significant but  sometimes puzzling effects on social connectivity. However, the  relationships between social connectivity, performance, and  engagement weakened over time, and results were not stable  across weeks.   Categories and Subject Descriptors  K.3.1 Computer Uses in Education   General Terms  Measurement, Performance, Theory.   Keywords  MOOC, network analysis, forum participation, exponential  random graph model, ERGM, learning.      1. INTRODUCTION  By its design, the field of learning analytics takes a fine-grained  approach to using data in service of understanding learning  processes and supporting outcomes. Learning processes are of   course contextual, dynamic, social, and highly variable in myriad  ways. Thus, from a data-driven standpoint, the integration of  multiple streams of data evolving over time (contextual,  cognitive, affective, social, etc.) is the ultimate objective. Such  integration is immensely challenging, requiring interdisciplinary  efforts and high quality data.  This paper explores a longitudinal approach to combining  engagement, performance and social connectivity data from a  MOOC using the framework of exponential random graph  models (ERGMs) [32]. The idea is to model the social network  in the discussion forum in a given week not only using  performance (quiz scores) and overall engagement (lecture and  discussion views) covariates within that week, but also using  person-level covariates from adjacent previous and subsequent  weeks.   Our models, described in more detail below, address the  following questions. First, do engagement with learning content  and performance on assessments provide additional  informationbeyond the endogenous mechanisms of a random  network structureabout a learners connectedness to others in  the social network Second, do time-lagged effects, such as  joining the discussion in response to struggling with the previous  assignment, relate to performance and engagement counts from  the previous week Addition, rather than substitution, means that  present-week effects are controlled for, in the sense of multiple  linear regression predictors, in the estimation of past-week  effects. Finally we add future-week effects as well. None of these  are causal models, and there is clearly no causal mechanism for  future week scores to influence social connectedness. A  significant positive interaction within this framework, again  controlling for present and past scores, would suggest that future  outcomes may be related to social connectedness in the present.  We note that the approach here contains many important  simplifications. Links or edges between networked participants  are dichotomous, that is, they are not weighted by frequency of  communications or characterized by other measures of type or  strength. An edge either exists or does not exist. Related to this  point, but distinct, the content of the forum posts [10] is  completely ignored in this analysis. Finally, we acknowledge  that our engagement and performance variables are relatively  simple distillations of these aspects of the learning experience.  The caveats above mean that results presented here should be  taken as preliminary and provisional. Our intention, however, is  to convey a framework for integrated analysis which can be   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883934      improved through refinement. Augmenting the model with  linguistic content is part of ongoing work.  The organization of the paper is as follows. Section 2 reviews  related work. Section 3 describes our dataset. Section 4  introduces the methods for building social networks and for  analyzing the network data. Section 5 provides the results and  Section 6 the related discussions.     2. RELATED WORK  2.1 Forums as Content Resources  Discussion forums in a MOOC may be alternately viewed as a  (dynamic) content resource and as a social resource. These  characterizations are not mutually exclusive, but they emphasize  different aspects and suggest different analytic approaches.  Insofar as students predominantly view posts authored by others,  for example as a means of finding help with homework or  information about course policies, the forums are a content  resource. Of course, there would be no content if no one posted  to the forum, but it is possible to count posts in addition to views  as simply two dimensions of forum usage.   A number of studies have examined the relationship between  performance (grades), attitudes, and discussion forum usage in  terms of view and post counts [24, 26, 31]. The methods involved  in such analyses are typically correlational using aggregated  values over the duration course. Nevertheless operationalization  of both predictor and outcome variables poses significant  challenges, especially when applying similar methods to massive  open online courses (MOOCs), where the population of users is  so diverse [6].    2.1.1 Longitudinal effects  Week-by-week analysis of forum content, coded according to the  Collaborative Learning Conversation Skill Taxonomy [38], was   presented in [39], in order to study the evolution of content over  time. Models incorporating student change from early to late  course stages in their (volumetric) use of the forums for  homework help were examined in [6].   2.2 Forums as Social Resources  The allure of discussion forums as a focus of study on online  learning lies, at least partly, in connection to theories of learning  such as social cognitive theory [4], social constructivism [42], and  connectivism [35]. Insofar as forums are viewed as a social  resource, the quantity of views or posts or content tags may take  a back seat to the relational aspect of replying to or receiving a  reply from another learner. Analyses that emphasize the social  aspect of discussion forums thus commonly apply some type of  social network analysis (SNA) to the communication network of  learners as defined through their forum interactions.  Information sharing and the flow of content in social networks of  learners were studied in both distance and on-campus learning  over a decade ago [3, 9, 18], before the proliferation of learning  management systems and MOOC platforms [11]. As discussed in  a recent review of SNA applications in discussion forums [30],  the most common methods since then have been visualization of  networks; characterization of users, for example, by centrality;  and community detection. Recent MOOC applications have  examined super-poster behavior [21], structure of networks in a  teacher-oriented MOOC [25], affinity characteristics of  communities [7], impact of community affiliation on performance  [8], as well as the distinction between communities and crowds  [16].   2.2.1 Generative network models  While SNA has often been used to extract descriptive statistics  about networks and individuals, it is also possible to model the  network itself as an outcome of a random generating process.  Such approaches allow one to test hypotheses of reciprocity or  homophily in a network, as was done using exponential random        Figure 1: Weekly forum interaction social networks.     graph models (ERGMs) in [2] and [25]. Alternate methods,  similar in spirit, include latent space models [19] and stochastic  blockmodels [36]. Hierarchical versions of these were applied to  teacher networks in [40].    2.3 Nonsocial networks  It should be noted that as a general method for learning analytics,  SNA has also been applied to networks where the entities are not  persons but rather concepts or states in a state space navigated by  learners. The use of the word social in this context, although  habitual because of SNA, has the potential to create confusion.  Community detection applied to a state-space graph in a logic  tutoring system, for example, was used to identify student pitfalls  or misconceptions [13]. Network analysis was also applied to eye- tracking movements in mathematics problem solving in [48].      3. DATA SET  The dataset used in this study was collected from a Coursera  course Big Data in Education (BDE) offered by Teachers  College, Columbia University in Fall 2013 [44]. In this course,  students learned and applied educational data mining and learning  analytics methods. The course spanned eight weekly sessions,  each consisting of five to seven lecture videos, readings, and a  quiz, which requires the students to do data analysis and submit  the results. All weekly assignments were automatically graded  and accounted for 100% of the final course grade (70% was the  threshold for earning a certificate). Students were encouraged to  use the online forums throughout the duration of the course, and  this was the principal way in which instructors and TAs interacted  with learners.  BDE had over 48,000 enrollees during the duration of the course,  with a small portion actively participating. A total of 1,380  students completed at least one assignment, and 638 students  received a certificate. This study concerns the relationship  between forum participation and other learning activities. Thus,  we restrict our attention to the users who posted or commented at  least once in the forum. This yielded a sample of 770 individuals,  including students, the instructor, and teaching assistants. Among  this sample, 440 students (57%) completed at least one  assignment, and 155 (20%) earned a certificate.     Following the organization of the course, and in order to enable  comparisons between weeks, we constructed separate datasets for  each week unit. The cutoff times for assigning events to weeks  were determined by the release of new units by the instructor,  rather than by strict calendar intervals. Variables extracted from  the log data included the number of posts  initiated/commented/replied, the number of posts viewed, the  number of video lectures downloaded or viewed, and scores on  the weekly quizzes. Although the quizzes allowed multiple  attempts, we computed scores based on whether answers were  correct on the first attempt. As found by [5], this type of scoring  resulted in score distributions with more variance and reduced  ceiling effects than scoring based on eventual correctness.     4. METHODS  4.1 Constructing social networks from the  forum posts  As is typical, a forum thread in BDE was initiated by an original  poster (OP). Participants could then reply to the OP or  comment on previous replies. Although the MOOC data logs   distinguish these types of actions, students did not consistently  use them differently. In other words, sometimes a reply was really  a comment to a specific prior reply, and sometimes a comment  was really an open reply to the OP. Thus we chose not to  distinguish these two actions after all.   The choice arises as to whether a reply should connect its poster  to everyone who posted previously or only to the OP. This has  been handled differently in different studies, and depending on  whether the links are directed or undirected [7, 27]. All  individuals involved in a post are frequently connected to each  other in collaboration networks or co-authorship networks [1].  The resulting structure will be a fully-connected clique [47].  However, we did not adopt this method with the consideration  that these relations, especially those among the repliers or  commenters the forum posts, might be much weaker than the  relations among researchers who co-authored a paper together.   To construct our social networks, we connected only the OP and  the subsequent repliers. As a result, each thread is represented by  a star in the network, with the OP as the node in the middle and  all others in the periphery. However, there were multiple threads  in the network for each week. Eight undirected social networks  were thus constructed. The nodes in the networks are individuals  and the links indicating the replying or commenting relations.  The visualizations of the weekly forum interaction social  networks are shown in Figure 1 (generated using the gplot  function in the R package Statnet [17]). All networks show a  relatively big connected component and several smaller  components in the periphery. The visualizations show the  network shrinking over time, which is confirmed by basic  summary statistics. We plot the size, density, size of the largest  connected component, and the maximum degree of the nodes of  the networks in Figure 2. These basic network statistics [45] are  defined as follows. The size of a network is defined as the total  number of nodes in the network. The density of the undirected  network is the number of links divided by the maximum possible  number, which for an undirected graph with n nodes is n(n-1)/2.  The connected component is defined as the subset of a network,  in which any two nodes are connected directly or through other  nodes, and the largest connected component is the one with the   largest number of nodes. The degree of a node is the count of  number of links connected to that node, and the maximum degree  of the nodes in a network is the degree of the most connected  node.     Figure 2: Basic network statistics for forum interaction   social networks in BDE.     The size of the networks decreased from the maximum of 450 in  the first week to 80 in the eighth week. For each network, the  largest component consisted of 85% or more of the nodes and thus  had the same longitudinal trend as the full network. The  maximum node degree for each network was achieved by either  the instructor or the head community TA, again following the  same decreasing trend. On the other hand, the densities of the  networks increased over time, showing that forum participants  tend to have closer relations as time went on. Week 5 seemed to  depart a little in each of these trends, possibly due to the timing  of the Thanksgiving holiday in the U.S.     4.2 Exponential Random Graph Models  (ERGMs)   To relate network measures to individual attributes, one might  consider using a standard regression approach. However, when  individuals are embedded in the same network, their network  attributes (edge/link statistics) are interdependent. These  dependencies violate the independent observation assumptions of  regression models. Rather than limit ourselves to correlations  between descriptive network measures and person (node)  attributes, we study the relation between network structures and  node attributes in a framework that explicitly accounts for the  dependencies among nodes and links. Exponential random graph  models (ERGMs) [32], also known as p* models, were developed  specifically for analyzing social network data and testing  hypotheses on both network structures and on the interactions of  node attributes with network structures. ERGMs have been  widely used in applications ranging from the structure of  adolescent peer influence [41] to social connections in virtual  worlds [22].  The basic idea of ERGMs is to build a stochastic model that  captures the generative features of an observed social network.  By analogy with regression models, dependent variables are the  nodes and links in the observed network, and the independent  variables are summary statistics for various network structural  features. Node attribute effects are analogous to interaction terms,  that is, of node attributes and structural features. Some examples  of the structural features include links, triangles, stars, or links  between nodes sharing a particular attribute. The summary  statistic for links is just the density of a network.   Mathematically, the general ERGMs are a class of stochastic  models that share the following general form [46].   P Y = y& = 1k & exp - .g y&0   where Y is a random variable representing the network and 2 is  the specific observed network. The state space of Y is the  collection of all possible networks with the same number of nodes  as the observed network, y. g y& is a vector of network statistics,   is the vector of corresponding coefficients, and 3 & is a  normalizing constant. It is calculated by summing up  exp -.g y&0 over the space of all possible networks.  General ERGMs do not impose any dependence assumptions.  Rather, one constructs different models by specifying these  dependence assumptions. For instance, in the simplest case, it can  be assumed that all links in the network are equivalent and thus  that the probability of an observed network depends only on the  number (equivalently, density) of links in the network. This is  known as the Bernoulli Model or the ErdsRnyi Model [14]. It  is not particularly realistic for most social networks.   More complex dependency assumptions include dyadic  independence models (p1) [20], for which reciprocated edges are  included as a structural feature or, further still, p2 models, which  add conditional dependence on node-level attributes [12, 28]. In  Markov random graphs [15], it is assumed that an edge between  two nodes i and j depends on any other possible edges involving  i or j. Curved exponential models have introduced nonlinear  functions of the  parameters to better capture the structural  features for social networks [23, 34].  Simulation and estimation  methods were also developed for ERGMs with the Markov  dependence assumption [29, 33, 46].  The output results of ERGMs ( coefficient estimates) may also  be interpreted by analogy with regression models. A significant  positive value for a coefficient corresponding to a structural  feature, for example, triangles, indicates that this feature occurs  more than would be expected by chance. For node attributes  (covariates), the interpretation of the coefficient is a bit different.  In that case, the conditional log-odds of an edge connecting two  nodes i and j is understood to be increased by the product of the  coefficient and the sum of covariate values for the two nodes.      4.3 Variables from the MOOC data  Social Networks  For this study, we are interested in the relationship between  MOOC forum social links and course participants behavior and  performance in current and adjacent weeks. As described in  Section 4.1, there are eight social networks, one for each week.  Nodal attributes  Each individual has four attributes extracted from each weeks  data, including assignment score (score, the only performance  attribute), the number of posts initiated/commented/replied  (posts), the number of posts viewed (post views), and the number  of video lectures downloaded or played (lectures). Counts of  posts, post views, and lectures constitute our simplified  engagement measures. We are also interested in how these  attribute measures from adjacent weeks are related to the current  weeks social networks. Thus, the dataset for week 4 includes the  social network from week 4s forum data and engagement and  performance attributes from weeks 3, 4, and 5.   5. RESULTS AND DISCUSSION  5.1 Hierarchical analysis method for ERGM  on Week 2  With the social networks and variables as described in Section  4.3, we build a hierarchical sequence of ERGMs. We use the R  package Statnet [17] for the analysis. The variables, blocks, and  coefficient estimates using the Week 2 data are shown in Table  1.  Model 1 serves as the baseline model, which contains two  structural features of the forum network, edge and alternating k- stars [23]. The first feature accounts for the density of the  network, and the second captures the tendency for hubs. As  shown in Table 1, the negative value for edge (-4.09) indicates  that the density of the network is lower than would occur by  chance. A negative value for alternating k-stars (-0.99) indicates  that hub frequencies are also lower than would occur by chance.  The phenomenon in which higher degree nodes attract more  links is known as preferential attachment [1]. In our dataset, even  though we do see some nodes with high degree, the overall  generating mechanism does not lean towards adding more links     to the high degree nodes (after controlling the density of the  network).  For Model 2, we add four nodal attributes, two related to  performance and two related to engagement. Score captures the  tendency for individuals with high assignment scores to be more  active in the social network. Score difference captures the  tendency for two linked individuals to have similar assignment  scores. With this term, we hope to test the hypothesis that social  interactions may be more likely between similarly performing  students. Our engagement attributes account for increasing  connectedness by individuals who view many posts or lectures.   The results from Model 2 are a significant positive effect for  assignment scores on connectedness in the forum network and a  significant negative effect for lecture views/downloads. The  score difference and post view effects were not significant. The  negative effect of lecture engagement is certainly a bit puzzling,  especially if one thinks of this in terms of high lecture viewing  being associated with low connectedness. However, the  mathematically equivalent converse relationhigh  connectedness associated with low lecture viewingis actually  plausible if expert individuals who already know the content  do a lot of replying.   Table 1: ERGMs for Week 2     * indicates p < 0.05;  indicates p < 0.1. Model convergence  information available upon request.    In Model 3, we added attributes from the previous week (week 1  for the week 2 model). It turned out that individuals with higher  assignment scores from the previous week tended to have more  links in the social network in the current week. Individuals who  viewed more posts from the previous week also tended to have  more links in the current week, while individuals who posted  more in the previous week tended to have fewer links in the  current weeks social network (controlling for other effects). The   lecture views/downloads from the previous week did not have a  significant effect on the current weeks relations.  Finally, in Model 4 (the full model with all variables), we explore  whether forum social relations correlate with future learning  behavior. The effects for both assignment scores and the lecture  views/downloads were positive and significant, indicating that  individuals with more social links in the current week tend to have  higher assignment scores and more lecture views/downloads in  the next week.   5.2 Beyond Week 2  Results from Week 2 offer many interesting findings, especially  the between-week interactions of social connectedness and  learner performance and engagement attributes. We ran the full  set of models on data from week 2 through week 7 (weeks 1 and  8 were included where appropriate given the inclusion of  previous or following week counts). Results are reproduced in  the Appendix for continuity.  The structural feature effects from the baseline Model 1 are  relatively stable and consistent over all weeks. However, in more  complex models, the effects were not consistent with the findings  in the Week 2 network. For example, in Model 2, the effects of  score difference were generally not significant, except in Week  8 where connected individuals did tend to have similar scores.  The negative effects of lecture engagement (views/downloads)  held true for Weeks 2, 3, and 4, but not beyond that and pointed  in the opposite direction for Week 1. Overall, Week 1 exhibited  exceptions on several effects, which may not be too surprising  given first week effects in any course.   Similar observations concerning fading or inconsistent effects  held true in Models 3 and 4 for network statistics with individual  attributes from the previous week and subsequent week. The  effect of previous week score, significantly positive in Week 2,  was not significant in most other weeks except for Weeks 6 and  7, and in the latter case, the sign had changed. Subsequent week  score was consistently a positive effect for Weeks 2 and 3,  though nonsignificant afterwards. And future lecture  engagement was significantly positively twice and negative once  (following Week 5).     6. CONCLUSIONS AND FUTURE WORK  Summarizing the results across all four models for Week 2, our  major findings are as follows. First, the forum social network is  relatively sparse and does not tend to have the preferential  attachment feature observed in a lot of social networks. Second,  individuals assignment scores, from current, previous, and  future weeks, are all positively related to being more active in  social network. Third, social connectedness was negatively  correlated with lecture engagement from this week, but  positively correlated with lecture engagement in the following  week. As with regression models, results from the ERGMs do  not enable one to draw causal conclusions, but they do provide  evidence about associations. Another finding of this study is that  it seems that the significant effects in earlier weeks of the course  do not persist later in the course. Sometimes, the effect directions  even reversed.  This study has several limitations, which suggest directions for  future work. As discussed in the introduction, the meaning of  social connectedness here remains fairly simple. Links are  unweighted and reflect only the process of replying (or  commenting) to an original poster. Content analysis, preferably    Model 1 Model 2 Model 3 Model 4  Effect Estimate   (S.E.)  Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   density (edge) -4.09*  (0.09)   -4.06*  (0.15)   -4.12*  (0.30)   -3.95*  (0.30)   tendency for  hubs  (alt. k-stars)   -0.99*  (0.19)   -0.93*  (0.19)   -0.76*  (0.20)   -0.63*  (0.21)   score  (current week)    0.19*  (0.10)   0.05   (0.11)   -0.43*  (0.18)   score difference  (current week)    0.18   (0.13)   0.18   (0.13)   0.17   (0.13)   post reads  (current week)    -0.14   (0.34)   -1.36*  (0.47)   -1.61*  (0.49)   lectures  (current week)    -0.49*  (0.20)   -0.62*  (0.28)   -1.26*  (0.37)   Score  (previous week)     0.28*  (0.13)   0.32*  (0.13)   posts reads  (previous week)     2.01*  (0.50)   2.39*  (0.53)   post  (previous week)     -1.20*  (0.42)   -1.21*  (0.42)   lectures  (previous week)     -0.38   (0.64)   -1.05   (0.67)   score  (next week)      0.38*  (0.15)   lectures   (next week)      0.62*  (0.24)     automated for scalability, could significantly improve the  definition of the network itself. Steps toward including natural  language processing of posts in defining the network are part of  ongoing work. However, even with a simple criterion for  network links, a bipartite network [43] could be a useful  alternative way to model the post-reply structure of a discussion  forum. In such a network, forum threads and individuals would  be included as two different types of nodes and directed links  would represent the contributions of posters to the threads.  In analyzing our dataset over the weekly sessions, we did find  different and even inconsistent results between weeks. The  approach of meta-analysis [37] or hierarchical models [40] might  be useful in aggregating results and finding the stable trend. On  the other hand, it would be interesting to analyze and compare  results from other datasets to see which, if any, of the effects in  this study generalize to other cases. BDE was a traditional  MOOC as opposed to a connectivist or social constructivist  MOOC. It is possible that the interplay between social  connectedness, engagement, and performance is more  pronounced and/or consistent in courses that emphasize social  construction of understanding.     7. ACKNOWLEDGMENTS  This research was supported by the National Science Foundation  (DRL-1418219).      8. REFERENCES  [1] Albert, R. and Barabsi, A.-L. 2002. Statistical   mechanics of complex networks.  Reviews of Modern  Physics. 74, (2002), 4797.   [2] Aviv, R. et al. 2004. Mechanisms and Architectures of  Online Learning Communities. Proceedings of IEEE  ICALT 2004. (2004).   [3] Aviv, R. et al. 2003. Network analysis of knowledge  construction in asynchronous learning networks.  Journal of Asynchronous Learning Networks. 7, 3  (2003), 123.   [4] Bandura, A. 1986. Social foundations of thought and  action: A social cognitive theory. Prentice-Hall series  in social learning theory. Prentice Hall.   [5] Bergner, Y. et al. 2015. Estimation of Ability from  Homework Items When There Are Missing and / or  Multiple Attempts. Proceedings of the Fifth  International Conference on Learning Analytics And  Knowledge - LAK 15. (2015).   [6] Bergner, Y.B. et al. 2015. Methodological Challenges  in the Analysis of MOOC Data for Exploring the  Relationship between Discussion Forum Views and  Learning Outcomes. Proceedings of 8th International  Conference on Educational Data Mining. (2015).   [7] Brown, R. et al. 2015. Communities of Performance &  Communities of Preference. Proceedings of the 2nd  International Workshop on Graph-Based Educational  Data Mining. (2015).   [8] Brown, R. et al. 2015. Good Communities and Bad  Communities: Does membership affect performance  Proceedings of the 8th International Conference on  Educational Data Mining. (2015), 612614.   [9] Cho, H. et al. 2002. Social Network Analysis of  Information Sharing Networks in a CSCL Community.  Proceedings of the Conference on Computer Support   for Collaborative Learning. (2002), 4350.  [10] Crossley, S. et al. 2015. Language to Completion:   Success in an Educational Data Mining Massive Open  Online Class. Proceedings of the 8th International  Conference on Educational Data Mining. (2015), 388 391.   [11] Dahlstrom, E. et al. 2014. The Current Ecosystem of  Learning Management Systems in Higher Education:  Student, Faculty, and IT Perspectives. EDUCAUSE  Research Report. (2014), 27.   [12] Duijn, M.A.J. et al. 2004. p2: a random effects model  with covariates for directed graphs. Statistica  Neerlandica. 58, 2 (May 2004), 234254.   [13] Eagle, M. et al. 2012. Interaction Networks: Generating  High Level Hints Based on Network Community  Clustering. Proceedings of the 5th International  Conference on Educational Data Mining. (2012), 164 167.   [14] Erds, P. and Rnyi, A. 1959. On Random Graphs. I.  Publicationes Mathematicae.  6, (1959), 290297.   [15] Frank, O. and Strauss, D. 1986. Markov Graphs.  Journal of the American Statistical Association. 81, 395  (1986), 832842.   [16] Gillani, N. and Eynon, R. 2014. Communication  patterns in massively open online courses. The Internet  and Higher Education. 23, (Oct. 2014), 1826.   [17] Handcock, M.S. et al. 2008. statnet: Software Tools for  the Representation, Visualization, Analysis and  Simulation of Network Data. Journal of Statistical  Software. 24, 1 (2008), 111.   [18] Haythornthwaite, C. 2001. Exploring Multiplexity:  Social Network Structures in a Computer Supported  Distance Learning Class. The Information Society. 17,  3 (2001), 211226.   [19] Hoff, P. et al. 2002. Latent space approaches to social  network analysis. Journal of the American Statistical  Association. 97, 460 (2002), 10901098.   [20] Holland, P.W. and Leinhardt, S. 1981. An exponential  family of probability distributions for directed graphs  (with discussion). Journal of the American Statistical  Association. 76, (1981), 3365.   [21] Huang, J. et al. 2014. Superposter behavior in MOOC  forums. Proceedings of the first ACM conference on  Learning @ scale conference - L@S 14. (2014), 117 126.   [22] Huang, Y. et al. 2009. The Formation of Combat  Groups in Online Games. the Fourth Annual INGRoup  Conference.   [23] Hunter, D.R. 2007. Curved exponential family models  for social networks. Social Networks. 29, 2 (2007), 216 230.   [24] Kay, R.H. 2006. Developing a comprehensive metric  for assessing discussion board effectiveness. British  Journal of Educational Technology. 37, 5 (Sep. 2006),  761783.   [25] Kellogg, S. et al. 2014. A social network perspective on  peer supported learning in MOOCs for educators. The  International Review of Research in Open and  Distributed Learning.   [26] Kortemeyer, G. 2007. Correlations between student  discussion behavior, attitudes, and learning. Physical  Review Special Topics - Physics Education Research. 3,  1 (Jan. 2007), 010101.   [27] Laghos, A. and Zaphiris, P. 2006. Sociology of Student-    Centred E-Learning Communities: a Network  Analysis. Learning. (2006), 98106.   [28] Lazega, E. and van Duijn, M. 1997. Position in formal  structure, personal characteristics and choices of  advisors in a law firm: A logistic regression model for  dyadic network data. Social Networks. 19, (1997), 375 397.   [29] Pattison, P.E. and Wasserman, S. 1999. Logit models  and logistic regressions for social networks: II.  Multivariate relations. British Journal of Mathematical  and Statistical Psychology. 52, (1999), 169193.   [30] Rabbany, R. et al. 2013. Collaborative Learning of  Students in Online Discussion Forums: A Social  Network Analysis Perspective. Educational Data  Mining: Applications and Trends. (2013), 130.   [31] Ramos, C. and Yudko, E. 2008. Hits (not Discussion  Posts) predict student success in online courses: A  double cross-validation study. Computers & Education.  50, 4 (May 2008), 11741182.   [32] Robins, G.L. et al. 2007. An introduction to exponential  random graph (p*) models for social networks. Social  Networks. 29, 2 (2007), 173191.   [33] Robins, G.L. et al. 1999. Logit models and logistic  regressions for social networks, III. Valued relations.  Psychometrika. 64, (1999), 371394.   [34] Robins, G.L. et al. 2007. Recent developments in  exponential random graph (p*) models for social  networks. Social Networks. 29, 2 (2007), 192215.   [35] Siemens, G. 2005. Connectivism: A Learning Theory  for the Digital Age. International Journal of  Instructional Technology and Distance Learning. 2, 1  (2005).   [36] Snijders, T. a. B. and Nowicki, K. 1997. Estimation and  Prediction for Stochastic Blockmodels for Graphs with  Latent Block Structure. Journal of Classification. 14, 1  (1997), 75100.   [37] Snijders, T.A.B. and Baerveldt, C. 2003. A Multilevel  Network Study of the Effects of Delinquent Behavior  on Friendship Evolution. The Journal of Mathematical  Sociology. 27, 2 (2003), 123151.   [38] Soller, A. 2001. Supporting social interaction in an  intelligent collaborative learning system. International  Journal of Artificial Intelligence in . 12, 1 (2001).   [39] Song, L. and McNary, S. 2011. Understanding students  online interaction: Analysis of discussion board  postings. Journal of Interactive Online Learning. 10, 1  (2011), 114.   [40] Sweet, T.M. et al. 2012. Hierarchical Network Models  for Education Research: Hierarchical Latent Space  Models. Journal of Educational and Behavioral  Statistics. 38, 3 (Sep. 2012), 295318.   [41] Valente, T.W. et al. 2013. Variations in network  boundary and type: A study of adolescent peer  influences. Social Networks. 35, 3 (Jul. 2013), 309316.   [42] Vygotsky, L. 1978. Interaction between learning and  development. Mind and Society. Harvard University  Press. 7991.   [43] Wang, P. et al. 2013. Exponential random graph model  specifications for bipartite networksA dependence  hierarchy. Social Networks. 35, 2 (May 2013), 211222.   [44] Wang, Y. et al. 2014. A Longitudinal Study on Learner  Career Advancement in MOOCs. Journal of Learning  Analytics. 1, 3 (2014), 203206.   [45] Wasserman, S. and Faust, K. 1994. Social network  analysis: Methods and applications. Cambridge  University Press, Cambridge.   [46] Wasserman, S. and Pattison, P.E. 1996. Logit models  and logistic regressions for social networks: I. An  introduction to Markov graphs and p*. Psychometrika.  61, 3 (1996), 401425.   [47] Xiao, W.-K. et al. 2007. Empirical study on clique- degree distribution of networks. Physical Review E. 76,  3 (2007), 37102.   [48] Zhu, M. and Feng, G. 2015. An Exploratory Study  Using Social Network Analysis to Model Eye  Movements in Mathematics Problem Solving.  Proceeding of the 5th International Learning Analytics  and Knowledge Conference (LAK 15) (Poughkeepsie,  NY, 2015).   9. APPENDIX    Tables for ERGM results for weekly networks.    Model 1    Week 1  Week 2 Week 3 Week 4 Week 5 Week 6 Week 7 Week 8  Effect Estimate   (S.E.)  Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Density (edge) -4.36* (0.05) -4.09* (0.09) -3.58* (0.09) -3.63* (0.12) -3.01* (0.14) -3.05* (0.10) -3.29* (0.15) -2.59* (0.15)   Tendency for  Hubs   (alt. k-stars)   -1.71* (0.12) -0.99* (0.19) -1.21* (0.22) -0.79* (0.26) -0.94* (0.33) -1.46* (0.26) -0.72* (0.32) -1.64* (0.37)   * indicates p<0.05;  indicates p < 0.1  Model 2   Effect Week 1 Week 2 Week 3 Week 4 Week 5 Week 6 Week 7 Week 8   Estimate   (S.E.)  Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Estimate  (S.E.)   Density (edge) -4.49* (0.15) -4.06*(0.15) -3.51* (0.15) -3.43* (0.15)      -2.74* (0.17) -2.98* (0.14) -3.48* (0.23) -2.68* (0.16)     Tendency for  Hubs   (alt. k-stars)   -1.63* (0.13) -0.93* 0.19) -1.15* (0.23) -0.65* (0.27) -0.53 (0.38) -1.41*(0.27) -0.54 (0.34) -1.58* (0.31)   score  (current week) -0.14* (0.05) 0.19* (0.10) 0.20 (0.11) 0.16 (0.14) -0.60* (0.22) 0.31 (0.17) 0.06 (0.21) -0.59 (0.31)   score  difference  (current week)   0.02 (0.08) 0.18 (0.13) 0.07 (0.14) 0.09 (0.15) -0.08 (0.21) -0.20 (0.17) 0.03 (0.20) 0.64* (0.32)   post views   (current week) -0.71* (0.29) -0.14 (0.34) -0.42 (0.34) -0.24 (0.33) 0.33 (0.83) -0.59 (0.77) -2.99* (1.20) 0.34 (0.43)   lectures   (current week) 0.83* (0.25) -0.49* (0.20) -0.30* (0.15) -0.55* (0.20) 0.71 (2.00)      -1.21 (0.81) 0.37 (0.27) 0.11 (0.19)     * indicates p<0.05;  indicates p < 0.1     Model 3   Week 2 Week 3 Week 4 Week 5 Week 6 Week 7 Week 8  Effect Estimate (S.E.) Estimate (S.E.) Estimate (S.E.) Estimate (S.E.) Estimate (S.E.) Estimate (S.E.) Estimate (S.E.)  Density (edge) -4.12* (0.30) -3.51* (0.16)      -3.35* (0.16)     -2.76* (0.18)      -2.97* (0.15)     -3.48* (0.23)      -2.85* (0.18)       Tendency for Hubs   (alt. k-stars) -0.76* (0.20) -1.12* (0.23)      -0.51 (0.28)     -0.47 (0.39)      -1.26* (0.29)    -0.11 (0.37)      -1.23* (0.33)        score  (current week) 0.05 (0.11) 0.38* (0.17)      0.43 (0.26)     -0.94* (0.40)      -0.27 (0.30)      1.03 (0.58)      -0.46 (0.33)        score difference  (current week) 0.18 (0.13) 0.07 (0.14)      0.09 (0.15)     -0.08 (0.22)      -0.20 (0.17)      0.03 (0.19)      0.61 (0.31)        post views   (current week) -1.36* (0.47) -0.67 (0.43)      0.08 (0.43)     0.58 (1.31)      -0.73 (0.89)      -4.07* (1.43)      0.45 (0.56)        lectures   (current week) -0.62* (0.28) -0.32 (0.22)      0.34 (0.39)      -3.40 (4.76)      1.38 (1.41) 2.38* (0.60)      0.05 (0.66)        Score  (previous week) 0.28* (0.13) -0.25 (0.17)      -0.26 (0.27)      0.39 (0.38)      0.73* (0.29) -1.13 (0.63)      -0.30 (0.26)        posts reads   (previous week) 2.01* (0.50) 0.81 (0.53)      -0.84 (0.61)      -0.16 (1.07)      0.24 (0.61) 2.72 (1.55)      -0.75 (1.07)        post  (previous week) -1.20* (0.42) -0.40 (0.42)      0.71 (1.36)      -0.65 (0.69)      -0.46 (0.64)     -2.20 (1.47)      2.22 (1.38)        lectures  (previous week) -0.38 (0.64) 0.06 (0.31)      -0.91* (0.39)      0.54 (0.56)      -7.38* (2.87)   -9.34* (2.54)      0.30 (0.70)        * indicates p<0.05;  indicates p < 0.1     Model 4   Week 2 Week 3 Week 4 Week 5 Week 6 Week 7   Effect Estimate (S.E.) Estimate (S.E.) Estimate (S.E.) Estimate (S.E.) Estimate (S.E.) Estimate (S.E.)  density (edge) -3.95* (0.30) -3.60* (0.17) -3.35* (0.16) -2.95* (0.19) -2.97* (0.15) -3.56* (0.24)  tendency for hubs  (alt. k-stars) -0.63* (0.21) -0.89* (0.25) -0.49 (0.29) -0.03 (0.42) -1.26* (0.29) 0.0005 (0.39)   score  (current week) -0.43* (0.18) -0.29 (0.25) 0.26 (0.31) -0.39 (0.59) -0.46 (0.45) 0.65 (0.55)   score difference  (current week) 0.17 (0.13) 0.07 (0.14) 0.09 (0.16) -0.09 (0.21) -0.19 (0.17) 0.02 (0.20)   post views   (current week) -1.61* (0.49) -0.96* (0.47) 0.02 (0.44) -0.18 (1.48) -0.87 (0.95) -4.90* (1.48)   lectures   (current week) -1.26* (0.37) -0.49 (0.36) 0.54 (0.48) 10.88 (5.99) 0.67 (1.80) 0.40 (0.96)   Score  (previous week) 0.32* (0.13) -0.56* (0.20) -0.40 (0.28) -0.30 (0.44) 0.73* (0.31) -0.64 (0.66)   posts reads   (previous week) 2.39* (0.53) 1.12* (0.57) -0.84 (0.61) -0.12 (1.20) 0.33 (0.65) 3.85* (1.70)   post  (previous week) -1.21* (0.42) -0.55 (0.45) 0.71 (1.37) -0.19 (0.73) -0.58 (0.67) -2.70 (1.46)   lectures  (previous week) -1.05 (0.67) 0.15 (0.32) -0.82* (0.40) 0.30 (0.61) -7.64* (2.94) -9.31* (2.58)   score  (next week) 0.38* (0.15) 1.02* (0.25) 0.38 (0.30) 0.56 (0.49) 0.17 (0.42) -0.13 (0.33)   lectures  (next week) 0.62* (0.24) 0.13 (0.33) -3.15 (3.73) -9.14* (2.07) 0.26 (0.42) 1.98* (0.79)   * indicates p<0.05;  indicates p < 0.1     "}
{"index":{"_id":"29"}}
{"datatype":"inproceedings","key":"Epp:2016:ELL:2883851.2883896","author":"Epp, Carrie Demmans","title":"English Language Learner Experiences of Formal and Informal Learning Environments","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"231--235","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883896","doi":"10.1145/2883851.2883896","acmid":"2883896","publisher":"ACM","address":"New York, NY, USA","keywords":"affect, analytics, communication, experience sampling methodology (ESM), formal learning, informal learning, language learning","abstract":"Many people who do not know English have moved to English-speaking countries to learn English. Once there, they learn English through formal and informal methods. While considerable work has studied the experiences of English language learners in different learning environments, we have yet to see analytics that detail the experiences of this population within formal and informal learning environments. This study used the experience sampling methodology to capture the information that is needed to detail the communication and affective experiences of advanced English language learners. The collected data reveals differences in how English language learners perceived their communication success based on their learning context, with higher levels of communicative success experienced in formal learning settings. No such differences were found for learners', highly negative, affect. The data suggest a need for additional emotional support within formal and informal learning environments as well as a need for oral communication support within informal contexts.","pdf":"English Language Learner Experiences of Formal and  Informal Learning Environments     Carrie Demmans Epp   Learning Research and Development Center (LRDC), University of Pittsburgh    3939 O'Hara Street, Pittsburgh, PA, United States of America   cdemmans@pitt.edu      ABSTRACT  Many people who do not know English have moved to English- speaking countries to learn English. Once there, they learn  English through formal and informal methods. While considerable  work has studied the experiences of English language learners in  different learning environments, we have yet to see analytics that  detail the experiences of this population within formal and  informal learning environments. This study used the experience  sampling methodology to capture the information that is needed to  detail the communication and affective experiences of advanced  English language learners. The collected data reveals differences  in how English language learners perceived their communication  success based on their learning context, with higher levels of  communicative success experienced in formal learning settings.  No such differences were found for learners, highly negative,  affect. The data suggest a need for additional emotional support  within formal and informal learning environments as well as a  need for oral communication support within informal contexts.        Categories and Subject Descriptors  Human-centered computing  Human computer interaction  (HCI); HCI design and evaluation methods; Field studies;  General Reference  Cross-computing tools and techniques   Measurement;   Keywords  Experience sampling methodology (ESM), communication, affect,  informal learning, formal learning, language learning, analytics   1. INTRODUCTION  English is recognized as a world language and that of  international business. This elevated status has contributed to the  development of an English-language learning marketplace and  increased educational tourism for the purpose of developing ones  proficiency. These English language learners (ELL) and those  who have immigrated to English speaking countries are trying to  improve their knowledge but are falling behind [17].    The experiences of ELLs within classroom settings [7] and their  performance on high stakes or other assessments [17] have been  explored. Historically, these assessments have evaluated reading,  writing, and listening. More recently, these assessments have  included ELLs speaking ability [11]. However, these assessments  do not typically measure the learners ability to converse. As a  result, the literature cannot inform our understanding of ELLs  ability to communicate in the variety of situations that they  encounter [7]. Moreover, there has been little work exploring  ELLs experiences of their everyday lives or that details their  experiences across learning contexts (i.e., formal and informal).  This work, therefore, details ELLs ability to communicate and  their affective experience based on their learning context, which  has yet to be done in spite of recognition for the role that affect  and communication play in language learning [25,27]. This work  was performed with an understudied population [19]: advanced  ELLs who are living in an English language environment because  they participate in educational tourism that allows them to partake  in a variety of informal language-learning activities.   To detail how advanced ELLs communication and affective  experiences differ based on their learning context, a form of self- reporting called the experience sampling methodology (ESM)  [15,23] was used. ESM enabled the collection of detailed data  about learner experiences from a variety of formal and informal  learning contexts that included their courses and shopping centres.   This data allows ELLs communication and affective experiences  to be compared across formal and informal learning contexts. This  comparison shows that ELLs experienced similarly high negative  affect in both contexts, indicating that these ELLs need additional  emotional support in all learning environments. In contrast to  their consistently high negative affect, learner communicative  success differed significantly by context: ELLs achieved their  communication goals more when they were in formal learning  environments than when speaking with those they encountered in  their everyday lives. This difference may be due to a variety of  reasons, including higher levels of speaker cooperativeness or  familiarity with the type of language that is used in class. The  lower communication success rates that were reported in informal  learning environments indicate that ELLs need additional  communication support in everyday settings.    2. ANALYTICS IN LANGUAGE  LEARNING   Many of the approaches that aim to improve education or assess  learning are not typically evaluated across educational contexts  [1]. When interventions have been evaluated more broadly, their  evaluation has relied on simple metrics of learner actions that can  be easily logged through a learning system [8], or it has been  highly qualitative with little use of analytics. When more complex  analytics are used within tutoring environments, they have      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom   Copyright is held by the owner/author(s). Publication rights licensed  to ACM.  ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883896     focused on learner mastery of language knowledge and skills [10].  This focus may be the result of the ease with which these types of  analyses can be performed and the difficulty of capturing data in  more complex real world settings. Regardless of the reason behind  the lack of analytics work that studies learners experiences in  more complex settings [8], there is a need to begin applying an  analytics lens to these less instrumented learning environments so  that we can inform praxis in these complex, less controlled spaces.  The need for analytics in language learning is even more  pronounced, with explicit calls for additional work in this area  [4,21] since analytics from the areas of high-stakes and formal  testing or intelligent tutoring systems have yet to describe how  ELL experiences and outcomes, which includes their affective  states and communication with others, are related.   2.1 Communication  Current language-learning theories argue that the production (e.g.,  writing or speaking) portion of communication serves three  purposes: it enables reflection about our language use and  knowledge, allows us to notice shortcomings in our knowledge,  and allows us to test our hypotheses about how the language  should be used [25]. This production of language can take place  during the learners everyday activities, during their language  courses, or when they are using technologies such as discussion  forums or online games. However, communication is rarely  assessed over the variety of contexts in which it can occur [7] and  few of the technology-enhanced environments that aim to scaffold  language learning support open-ended speech [3] unless the  technology has been repurposed to support interactions between  the learner and a peer or instructor (e.g., email or Skype). This  lack of instrumentation in authentic communication scenarios  means that the communication experiences of ELLs have been  inadequately explored. It also means that analytics have yet to be  performed on these rich learning opportunities, which limits our  ability to understand these environments, improve language  assessment, and improve the supports that we provide to learners.   2.2 Affect  A substantial body of work with respect to how learner affect  influences their actions and outcomes within the constrained  environment of intelligent tutoring systems has emerged [2,20].  This work continues to grow and uses a variety of methods to  capture information about learner affect. These methods include  self-reporting [5,15], observer coding [2], sentiment analysis [30],  and the use of physiological sensors [16]. Almost none of this  work has been performed with language learners. It is more  common to see the development of models that predict learner  affect or its role on varied outcome measures within other learning  domains [2,30]. This lack of analytics around affects role within  language learning is inconsistent with the theoretical  argumentation for the importance of affect to language learning.   The common view of affect within language learning states that a  learner must be open to learning for learning to occur [18].  Research into the role of various affective states within language  learning shows that learner anxiety hinders learning whereas self- confidence and motivation enable learning [12,27]. However,  these affective states have been operationalized in different ways.  This lack of agreement and systematic investigation into ELLs  affective states across contexts limits our understanding of the  role that affect plays within language learning. A far more  nuanced understanding of ELLs affective experiences is needed.    This study, therefore, aims to detail the communicative and  affective experiences of advanced ELLs across learning contexts.   3. METHODOLOGY  3.1 ESM: Communication and Affect  Experience sampling (ESM) is a self-report method [15]. It aims  to discover and represent peoples experiences through their eyes.  It has been used to evaluate ubiquitous technologies [6] by having  users complete regular reports that detail varied perceptual and  contextual information [15]. ESM is commonly used to collect  information about the participants affective state and social or  geographical context [15,23]. In keeping with this tradition,  information about participants social context (i.e., who they were  with and where they were), affective state, and perceived  communicative success were collected. This information was  gathered through an android application (see [13] for details).    Two established scales were used to measure participants  affective state. The first is the international short form of the  positive and negative affect schedule (I-PANAS-SF) [28] and the  second is the 5-point version of the self-assessment manikin  (SAM) [24]. These two measures were used to complement one  another because the SAM, which is a pictographic semantic- differential scale, had not been validated cross-culturally and the  I-PANAS-SF requires knowledge of the adjectives that describe  ones affective state. The SAM asks users to select the picture that  best depicts how they feel. These pictures are mapped to a 5-point  rating scale, where 1 is highly negative and 5 is highly positive.  The I-PANAS-SF asks respondents to rate the extent to which  they feel a particular descriptor on a 5-point scale, where 1 is  Strongly Disagree and 5 is Strongly Agree. It was possible that  participating ELLs might not have sufficient knowledge of the  vocabulary used within the I-PANAS-SF, which is why the SAM  was also used. Following the I-PANAS-SF and SAM, participants  were asked to explain why they felt the way that they did.   Participants were asked about the number of communication  attempts that they had made in a single day and how successfully  they thought that they had communicated across all of those  events. Participants perceptions of their communicative success  were collected using a slider (0 = not at all successful, 100 =  completely successful). Participants were also asked to report on  specific communication events. The specific communication  events that participants reported on were their most recent  attempts at communicating using English when signaled.  Participants could not control when they were asked for this  information and they were asked to report on their most recent  attempt to communicate in English twice per day. In addition to  asking participants for their perceived communicative success  from their most recent attempt at communication, they were  asked, What were you trying to communicate.    To ensure that participants were reporting on their experiences  from varied contexts, signal-contingent sampling was used.  Participants were asked to complete a self-report questionnaire  when signaled. To enable this sampling, participants waking  hours were subdivided into 6 segments. Participants were then  signaled at a random time within each segment. Once signaled, a  participant could complete the provided self-report form or post- pone reporting. If a participant postponed reporting he or she was  re-signaled 30 minutes later unless the sampling window had  expired. For example, the first signal for a participant who wakes  up at 7 am and goes to sleep at 11 pm would occur at some point     between 7 am and 9:40 am. Were that signal to be postponed at  9:30 am, the participant would not be allowed to provide that  information. If the participant were to postpone a 7:10 am signal,  s/he would have up to 5 additional opportunities to complete the  form. Information about participants affect was collected at 3  time points and information about their communicative success  was collected at 2 time points throughout the day. The sixth  sampling period collected participants reflection of their  experiences over the course of the day. This reflective data is  reported. However, it cannot inform our understanding of how  learner communication and affective experiences differ between  formal and informal learning contexts. Only the momentary  reports can help answer this question. The daily reflective reports  are, therefore, used to inform the interpretation of the differences  that were observed using participants momentary reports.    3.2 Other Data Collection  The perceived competence scale (PCS) [9] was modified to  measure ELLs sense of self-efficacy with respect to their ability  to learn English and their ability to communicate using English.   Semi-structured interviews were conducted to elicit information  about participants learning activities, the technologies they had  used, and their communication experiences. The interviews also  provided a venue to clarify the content of the reports that were  submitted as part of the experience sampling process.    3.3 Participants  Recently arrived international students were recruited through an  academic English training program in a major North American  city. Twelve ELLs consented to participate. However, P12 (a 23  year old speaker of Kazakh and Uygur) submitted too few self- reports so her data was excluded. The remaining 11 participants (5  - female, 6 - male) had an average age of 22.82 years (SD = 4.98).  P10 spoke Arabic, P9 spoke Kazakh, and the other 9 spoke  Portuguese as their mother tongue. Participants had been in an  English environment between 3 and 10 months, and 5 of them  (P1, P5, P6, P9, P10) lived in homes where more than one  language was spoken.    The participants whose data was used completed 73.18% of the  reports on average (SD = 11.24) and received 71.67 dollars on  average (SD = 14.26): this amount consisted of 20 dollars for  returning the data-collection device and 0.50 dollars per submitted  sample up to a maximum of 100 dollars.     3.4 Study Context  Data was collected from two contexts. The first was the  participants academic English courses (i.e., their formal learning  environment). These courses used a communicative approach to  language learning where students read papers and presented a  topic from their discipline to the class who then questioned the  presenter. The second context was the city in which participants  lived (i.e., their informal learning environment). This city  provided many opportunities for communication in English and  ensured continued exposure to English-language media.   Participants were signaled regardless of where they were or what  they were doing. They were instructed not to provide pictures  from private locations, such as the bedroom or bathroom, if they  were in these types of locations when they were signaled.  Participants reported data when they were at home, shopping,  interacting with friends, or using public transportation.   4. RESULTS  The results detailing participants affective and communication  experiences are based on the 218 daily reflective self-reports and  966 momentary self-reports that were collected through the ESM  application. The interview data indicate that signals were ignored  when participants were in the bathroom (P11), had dirty hands  (P9), were fighting with a significant other (P11), sleeping (P9),  or did not hear the signal (P1, P2, P3, P12). They also ignored the  signal when they felt that responding would break the flow of  their current activity (P1, P4, P5, P6, P7, P8).   4.1 Self-Efficacy   Participant responses to the communication ( = .922) and  learning ( = .916) versions of the PCS were highly reliable. They  were also highly correlated (r(22) = .90, p < .001), which  confirms the strong link between ELLs belief in their ability to  learn and their ability to communicate. Moreover, participant  responses indicated their confidence in their ability to learn  English (M = 6.17, SD = 0.91) and their ability to communicate  using English (M = 5.84, SD = 1.02).   4.2 Communication  The daily self-reports show that participants experienced days  when they did not communicate in English and days when they  communicated extensively using English. These high  communication days, while infrequent, were characterized by  ELLs participation in activities that depended on communication  with others. For example, giving presentations in class, going  shopping, or volunteering at local schools. However, it was far  more common for participants to only speak with  others in  English when absolutely necessary, with participants reporting a  median of 10 communication attempts per day (IQR = 12).   Similar to the amount of communication reported, participants  experienced complete communication success and failure. Their  communicative success was typically quite high (Mdn = 90.50%,  IQR = 19.00) and there was a weak relationship between the  number of times that ELLs tried to communicate and their ability  to communicate successfully (r(216) = .26, p < .001). There was  also a weak relationship between the number of times that they  tried to communicate and their daily I-PANAS-SF (r(216) = .22, p  = .001) scores (see section 4.3 Affect).  Of the 401 samples of momentary communicative success, 108  were taken from formal learning contexts and 293 from informal  contexts. Participants reported an average success rate of 86.98%  (SD = 15.27) when in class and 81.78% (SD = 20.30) when  outside of class. These reports indicate participants were  significantly more successful when conversing in formal learning  environments (t(252.51) = - 2.79, p = .006; F = 17.19, p < .001).   The interviews revealed that participants relied on the patience of  their conversational partners and that they employed a variety of  strategies (e.g., planning and reframing) to mitigate or prevent  miscommunication before using a technology to overcome  communication barriers. This decision was influenced by their  beliefs about the effectiveness of certain tools and by social  factors: participants did not want to appear rude by consulting  their mobile phone when in the middle of a conversation. As a  result, they would only use technological supports when they were  preparing for an interaction or their other strategies had failed.   P4, P5, P8, and P10 were frustrated by many peoples speaking  rate and rapid conversational pace. These speaker attributes made  it difficult for them to participate in conversation. P8 and P10 also     Table 1: The number of students who used each technology to  support their language-learning and communication activities.   Web Search Dictionary Translator Sub-titles Games Media  3 11 9 7 7 11      reported that this problem was more pronounced in service  industry settings where accents introduced additional barriers.   4.3 Affect  A total of 565 samples of participants momentary affective state  were collected: 95 came from formal learning environments and  470 came from informal learning environments. Participant  responses to the momentary version of the I-PANAS-SF ( =  .756) were sufficiently reliable and consistent with their responses  to the SAM (r(563) = .48, p < .001). The daily measures of the I- PANAS-SF showed a similar relationship with the SAM (r(216)   = .51, p < .001) and similar reliability ( = .759) to the momentary  version of this measure. Given these relationships and space  constraints, only the details of the I-PANAS-SF are reported.    Participant momentary affect, did not differ significantly (p =  .544) between informal (M = 35.86, SD = 5.72) and formal (M =  35.47, SD = 5.44) learning environments.    Participant responses to the negative subscale of the daily measure  indicate consistently high levels of negative affect (M = 22.02, SD  = 3.35): participants negative affect score was 1.75 times larger  than their positive affect score (M = 13.33, SD = 4.83). Ceiling  effects were also observed for the negative affect subscale of P5,  P6, P7, P9, and P11. These data indicate that participants were  continually challenged independent of their learning environment.  Participant negative affect did not seem to influence their sense of  self-efficacy or their willingness to engage in learning activities  that required oral English: both those with ceiling and those with  lower (P1, P4, P8) negative affect sought opportunities to  communicate using English.    4.4 ELL Support Tool Use  Participants aligned themselves with communicative approaches  to language learning where they used a variety of mobile  applications and services that included dictionaries, translators,  dedicated vocabulary support tools, or Google web and image  searches (see Table 1) to overcome gaps that they had noticed in  their knowledge. P1, P7, and P11 provided examples of their  noticing gaps in their knowledge as the result of trying to  communicate orally with others using English. Participants also  reported using these technologies to increase their exposure to  English by consuming different types of media. The employed  media included novels, textbooks, online news articles, music,  television, games, and movies. When engaging in online multi- player games, participants would listen to others speaking in  English and communicate in English through the text-based chat  system, but they reported that the fast-paced nature of the game  prevented them from participating in oral conversation.   5. DISCUSSION  The variability observed in participants daily I-PANAS-SF  positive and negative subscale scores was inconsistent with prior  work [29]. When the ceiling effects that were observed on the  negative subscale are considered in conjunction with the stability  of this measure, it is likely that the expected variability was not  observed because of the challenges that participating ELLs faced   on a daily basis. This suggests that their negative affect was so  high that potential changes were not measurable. These  experiences imply that ELLs need additional emotional support  outside of the classroom and that instructor encouragement will  not fully meet their affective needs.   The relationship between participant affect and the number of  communication attempts that they made empirically confirms  arguments for the inseparability of affect and language learning  [27]. Similar support for the relationship between learning and  communication comes from the strong relationship between the  communication and learning versions of the PCS: one cannot  communicate through a language without having learned that  language and communication supports language learning [26].  Since only half of the participants sought informal learning  opportunities, the relationship between participants affective  states and the number of communication attempts they made  emphasizes the importance of finding ways to encourage ELLs to  pursue the learning opportunities that are present in their everyday  lives. These relationships further suggest that interventions which  encourage effort, bolster affect, or highlight learner successes may  help increase the likelihood that ELLs will pursue the learning  opportunities that are part of speaking with others, especially in  informal contexts.  The higher success levels that participants experienced when  communicating in formal learning contexts confirms that ELLs  struggle with the various forms of English that are used in  everyday life [14]. This difference also highlights the potential  relationship between ELLs communicative success and the extent  to which communication partners from each of these learning  contexts appear to cooperate, with instructors and peers being  more patient and more willing to take the time and energy to  negotiate a common understanding. Differences in the nature of  communication between these learning environments indicates a  need to support ELL communication, which is further illustrated  through some ELLs reliance on mobile translators or friends and  family when communicating with others [14]. Differences in  environmental factors, such as noise and distractions, may also  play a role in ensuring higher levels of communicative success  when students are in formal learning environments.    6. CONCLUSION  The communication and affective experiences of English language  learners who are in English-dominant environments could be  better understood. The experience sampling methodology was  combined with other measures and interviews to collect self- report data about ELL experiences. This effort is the first to  systematically collect data that details the experiences of these  learners within both formal and informal learning environments.  Future work could consider the general applicability of the study  methods to other domains.  The lack of difference in learners affective states between the two  learning contexts along with their high levels of negative affect  indicates that they need additional support. This support needs to  be provided both within the classroom and their everyday lives.  The differences in learners ability to communicate between these  two contexts indicates a need for the creation of strategies and  tools that can scaffold language learner communication within  informal learning environments that are characterized by noise,  unfamiliar language, and uncooperative communication partners.  Additional work exploring the nature of these environments and     that of ELLs communication failure will need to be performed in  order to develop these strategies and tools.   7. ACKNOWLEDGMENTS  The author held a Walter C. Sumner Fellowship.    8. REFERENCES  [1] Emmanuel G. Blanchard. 2014. Socio-Cultural Imbalances in   AIED Research: Investigations, Implications and  Opportunities. International Journal of Artificial Intelligence  in Education 25, 2: 204228.    [2] Nigel Bosch, Sidney DMello, Ryan Baker, et al. 2015.  Automatic Detection of Learning-Centered Affective States in  the Wild. IUI 2015, ACM Press, 379388.   [3] Jack Burston. 2014. The Reality of MALL: Still on the  Fringes. CALICO Journal 31, 1: 103125.    [4] Jack Burston. 2014. MALL: the pedagogical challenges.  Computer Assisted Language Learning 27, 4: 344357.    [5] Lee Anna Clark and David Watson. 1999. The PANAS-X:  Manual for the Positive and Negative Affect Schedule -  Expanded Form. Psychology Publications. Retrieved from  http://ir.uiowa.edu/psychology_pubs/11   [6] S. Consolvo and M. Walker. 2003. Using the Experience  Sampling Method to Evaluate Ubicomp Applications. IEEE  Pervasive Computing 2, 2: 2431.    [7] Alister Cumming. 2004. Broadening, Deepening, and  Consolidating. Language Assessment Quarterly 1, 1: 518.    [8] Shane Dawson, Dragan Gaevi, George Siemens, and Srecko  Joksimovic. 2014. Current State and Future Trends: A  Citation Network Analysis of the Learning Analytics Field.  Proceedings of the Fourth International Conference on  Learning Analytics And Knowledge, ACM, 231240.    [9] Edward L Deci and Richard M Ryan. 2002. Handbook of  self-determination research. University of Rochester Press,  Rochester, NY.   [10] Carrie Demmans Epp and Gordon I. McCalla. 2011.  ProTutor: Historic Open Learner Models for Pronunciation  Tutoring. Artificial Intelligence in Education (AIED),  Springer, 441443.    [11] Carrie Demmans Epp and Daniel Riccardi. 2013. The  Speaking Section of the Test of English for International  Communication (TOEIC). TESL Ontario Contact 39, 6167.   [12] L. Quentin Dixon, Jing Zhao, Jee-Young Shin, et al. 2012.  What We Know About Second Language Acquisition: A  Synthesis From Four Perspectives. Review of Educational  Research 82, 1: 560.    [13] Clayton Epp and Carrie Demmans Epp. 2014. MyLog.  DevCache Consulting, Toronto, Canada. Retrieved from  http://www.cdemmansepp.com/research/mylog   [14] Daryl Gordon. 2004. Im Tired. You Clean and Cook.  Shifting Gender Identities and Second Language  Socialization. TESOL Quarterly 38, 3: 437457.    [15] Joel M. Hektner, Jennifer A. Schmidt, and Mihaly  Csikszentmihalyi. 2007. Experience sampling method:  measuring the quality of everyday life. Sage Publications,  Thousand Oaks, Calif.   [16] Alicia Heraz and Claude Frasson. 2011. Towards a Brain- Sensitive Intelligent Tutoring System: Detecting Emotions   from Brainwaves. Advances in Artificial Intelligence 2011: 1 13.    [17] M. J. Kieffer, N. K. Lesaux, M. Rivera, and D. J. Francis.  2009. Accommodations for English Language Learners  Taking Large-Scale Assessments: A Meta-Analysis on  Effectiveness and Validity. Review of Educational Research  79, 3: 11681201.    [18] Stephen D Krashen. 1982. Principles and practice in second  language acquisition. Pergamon, Oxford; New York.   [19] Agnes Kukulska-Hulme. 2013. Re-skilling Language  Learners for a Mobile World. The International Research  Foundation for English Language Education (TIRF),  Monterey, CA, USA. Retrieved from http://www.tirfonline.  org/english-in-the-workforce/mobile-assisted-language- learning/re-skilling-language-learners-for-a-mobile-world/   [20] Blair Lehman, Sidney DMello, and Art Graesser. 2012.  Confusion and complex learning during interactions with  computer learning environments. The Internet and Higher  Education 15, 3: 184194.  http://doi.org/10.1016/j.iheduc.2012.01.002   [21] Qing Ma. 2013. Computer Assisted Vocabulary Learning:  Framework and Tracking Users Data. CALICO Journal 0, 0:  230243.    [22] Gloria Ramirez, Xi Chen, Esther Geva, and Heidi Kiefer.  2009. Morphological awareness in Spanish-speaking English  language learners: within and cross-language effects on word  reading. Reading and Writing 23, 3-4: 337358.    [23] Ulrich Schimmack. 2003. Affect Measurement in Experience  Sampling Research. Journal of Happiness Studies 4, 1: 79 106.    [24] Hyeon-Jeong Suk. 2006. Color and Emotion - a study on the  affective judgment across media and in relation to visual  stimuli. Retrieved September 30, 2012 from http://irtel.uni- mannheim.de/pxlab/demos/index_SAM.html   [25] Merrill Swain. 1995. Three functions of output in second  language learning. In Principles and Practice in Applied  Linguistics, G. Cook and B. Seidlhofer (eds.). Oxford  University Press, Oxford, UK.   [26] Merrill Swain. 2006. Languaging, agency, and collaboration  in advanced second language proficiency Reading Reflection.  In Advanced Language Learning: The Contribution of  Halliday and Vygotsky, Heidi Byrnes (ed.). Continuum, New  York, NY, USA, 95108.   [27] Merrill Swain. 2013. The inseparability of cognition and  emotion in second language learning. Language Teaching 46,  02: 195207.    [28] E. R. Thompson. 2007. Development and Validation of an  Internationally Reliable Short-Form of the Positive and  Negative Affect Schedule (PANAS). Journal of Cross- Cultural Psychology 38, 2: 227242.    [29] D. Watson, L. A. Clark, and A. Tellegen. 1988. Development  and validation of brief measures of positive and negative  affect: the PANAS scales. Journal of Personality and Social  Psychology 54, 6: 10631070.   [30] M Wen, D. Yang, and Penstein-Ros, Carolyn. 2014.  Sentiment analysis in MOOC discussion forums: What does it  tell us 130137.   eth     "}
{"index":{"_id":"30"}}
{"datatype":"inproceedings","key":"Wells:2016:AEO:2883851.2883894","author":"Wells, Marc and Wollenschlaeger, Alex and Lefevre, David and Magoulas, George D. and Poulovassilis, Alexandra","title":"Analysing Engagement in an Online Management Programme and Implications for Course Design","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"236--240","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883894","doi":"10.1145/2883851.2883894","acmid":"2883894","publisher":"ACM","address":"New York, NY, USA","keywords":"analysing interaction data, engagement and performance, predicting student performance","abstract":"We analyse engagement and performance data arising from participants' interactions with an in-house LMS at Imperial College London while a cohort of students follow two courses on a new online postgraduate degree in Management. We identify and investigate two main questions relating to the relationships between engagement and performance, drawing recommendations for improved guidelines to inform the design of such courses.","pdf":"Analysing Engagement in an Online Management Programme and Implications for Course Design  Marc Wells Educational Technology Unit  Imperial College London m.wells@imperial.ac.uk  Alex Wollenschlaeger London Knowledge Lab  Birkbeck, Univ. of London awolle01@dcs.bbk.ac.uk  David Lefevre Educational Technology Unit  Imperial College London david.lefevre@imperial.ac.uk  George D. Magoulas London Knowledge Lab  Birkbeck, Univ. of London gmagoulas@dcs.bbk.ac.uk  Alexandra Poulovassilis London Knowledge Lab  Birkbeck, Univ. of London ap@dcs.bbk.ac.uk  ABSTRACT  We analyse engagement and performance data arising from participants interactions with an in-house LMS at Imperial College London while a cohort of students follow two courses on a new online postgraduate degree in Management. We identify and investigate two main questions relating to the relationships between engagement and performance, draw- ing recommendations for improved guidelines to inform the design of such courses.  Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Distance Learning  Keywords  Analysing interaction data, engagement and performance, predicting student performance  1. INTRODUCTION Two key affordances of online provision in the context  of management education are the ability to offer students a more flexible programme of study and the opportunity to deliver education to an additional pool of students who are unable or unwilling to attend classes on campus. To this end, Imperial College London has developed a digital campus to deliver several postgraduate degree programmes online. This consists of an in-house Learning Management System (LMS), online courseware, and provision of tablet computers to all students. All course materials and learn- ing interactions are held within the learning platform. The system has proven very popular with both students and tu- tors, and it has been co-developed and evaluated with these stakeholder groups over a number of years [6].  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00  DOI: http://dx.doi.org/10.1145/2883851.2883894  The platform tracks interactions and activity completions by students, presenting an opportunity to undertake a pilot project analysing potential relationships between different forms of participant engagement in the platform in order to better understand how students are engaging with their online courses and how the design of the courses may be im- proved for the benefit of subsequent cohorts. The research presented here has used interaction data from the first term (semester) of a new 2-year online postgraduate degree in Management. This term lasted 10 weeks during which stu- dents studied two compulsory courses, in Accounting and in Marketing. We report on the design and application of a range of analyses of the interaction data, with the aim of investigating two key questions: Q1: Effect of staff engagement on student engagement. Q2 : Effect of student engagement on student performance.  Section 2 discusses background and related work. Sec- tion 3 presents the design of our research. In Sections 4 and 5 we present and discuss our results. Section 6 draws overall conclusions and identifies directions of future work.  2. RELATED WORK Siemens [5] and Dyckhoff et al. [3] provide broad exposes  of the field of learning analytics and conclude by emphasising the need to develop tools that are useful to learners, teach- ers, institutions and researchers. A necessary step towards the development of such tools is the establishment of rela- tionships between the behaviour of different agents within a learning system and subsequent outputs such as learners levels of performance and satisfaction. The focus of our re- search here is learner engagement. Much work has been done recently in identifying patterns of learner engagement within Massive Open Online Course (MOOC) environments, e.g. [2, 4], in order to identify more successful students, identify students at risk of dropping out, and support students self- awareness. Patterns of learner engagement have also been analysed on more traditional distance learning courses, again with an emphasis on predicting students at risk [7].  A previous study [1] has focussed on learner engagement in the context of online management programmes. This study of 48 online MBA courses, from one institution, investigated the impact of course technologies, instructor behaviours and learner behaviours on perceived learning, learner performance and learner satisfaction. Instructor behaviour was found to    Table 1: Student demographics Number  Total 67 Gender Women 17  Men 50 Region Africa/Middle East 19  Americas 13 Asia/Pacific 14 Europe 3 UK 18  Highest qualification Bachelors 42 Masters 20 Doctoral 4 Professional 1  Employment sector Energy/Mining 14 Banking/Finance 10 Telecomm 6 Consulting 5 Cons. Goods/Retail 5 IT/Technology 5 Education 4 Other 18  predict learner satisfaction, however students social pres- ence was found to be significant in predicting all three out- puts.  The present research contributes to this body of litera- ture by analysing learner engagement within one particular category of online learning. The Online Management pro- gramme at Imperial can be distinguished from MOOCs, and from many other distance learning courses, due to the rela- tively small cohort size of students on the programme, the highly selective nature of admissions, the presence of a small amount of face-to-face tuition (there are three on-campus weeks, one at the start of year 1 and two at the start of year 2) and the high levels of support provided by course staff. As a consequence, student retention is not a primary issue as very few students fail to complete their degrees. Moreover, because Imperials in-house LMS has been co-developed and evaluated with students, tutors and administrators over a number of years, acceptance of this technology by these co- horts of students is also not a primary issue in our context (c.f. [1]). However, there is still variability in students per- formance and their levels of satisfaction with their courses. Both these factors are therefore worthy of attention from a learning perspective, and in this paper we focus on the former.  3. DESIGN The Online Management programme at Imperial is highly  selective and draws a diverse audience of students from around the world. For the 2014-15 session of the new postgraduate degree in Management that is our focus here, 67 students from 29 countries on five continents were admitted (see Ta- ble 1). The median student age was 35 (interquartile range [33,38]). All students had significant work experience, with a median of 10 years (interquartile range [7,12.5]).  3.1 Course Design Our study here focusses on the first two courses taken  in the 2014-15 session of the new postgraduate degree: Ac- counting and Marketing. Both courses followed the same delivery structure: 2 weeks pre-study, 10 weeks term time study, and a 4 week exam period. As per equivalent face- to-face MBA programmes, the course design places an em- phasis on encouraging an active approach to learning and community-oriented activities. Students are asked to com-  Table 2: Accounting course design Module structure Activity Breakdown  Pre-Study [DE] [100%] Week 1 [DE, PA, CC, F] [60%, 30% 8%, 2%] Week 2 [DE, PA, CC, F] [70%, 11%, 8%,9%,2%] Week 3 [DE, PA, CC, PS, F] [48%, 20%, 1%, 20%, 11%] Week 4 [DE, PA, CC, PS] [30%, 43%, 16%,11%] Week 5 [DE, CC, PS] [59%, 37%, 4%] Week 6 [DE, PA, PS, F] [44%, 44%, 10% 2%] Week 7 [DE, PA, CC, PS, F] [50%, 18%, 11%, 14%, 8%] Week 8 [DE, PA] [62%, 38%] Week 9 [DE, PA, CC] [50%, 31%, 18%] Week 10 [DE, CC, PS, F] [13%, 49%, 23%, 15%] Exam Period [DE] [100%] CC: Construction and connection; DE: Demonstration and exposition; F: Feedback PA: Practice and application; PS: Production and submission  Table 3: Marketing course design Module structure Activity Breakdown  Pre-Study [DE] [100%] Week 1 [DE, PA, CC, PS, F]] [66%, 8%, 8%, 13%, 5%] Week 2 [DE, CC, PS, F] [39%, 38%, 20%, 3%] Week 3 [DE, CC, F] [43%, 52%, 5%] Week 4 [DE, CC, PS, F] [31%, 46%, 10%, 13%] Week 5 [DE, PA, CC, PS] [58%, 4%, 25%, 13%] Week 6 [DE, CC, PS] [51%, 19%, 30%] Week 7 [DE, CC, PS] [23%, 41%, 36%] Week 8 [DE, CC, PS, F] [33%, 36%, 29%, 2%] Week 9 [DE, PA, CC, PS, F] [40%, 3%, 30%, 10%, 17%] Week 10 [DE, PA, PS] [32%, 15%, 53%] Exam Period [DE, CC, F] [48%, 20%, 32%]  plete a sequence of activities each week which are predom- inantly asynchronous in nature. The activities can be cat- egorised as: Demonstration and exposition, Practice and application, Construction and connection, Production and submission, and Feedback. Tables 2 and 3 outline the deliv- ery structure of each course along with the design breakdown of activities.  3.2 Data collected Engagement data was collected for students and for course  staff, where course staff refers to the course leaders (one for Accounting and two for Marketing), the teaching assis- tants (one for each course) and the administrative staff (two staff supporting the whole degree). For students, engage- ment events were categorized according to their initiation mechanism and included material-initiated, tutor-initiated and student-initiated events:  Material-initiated events were events that were the re- sult of prompts within the course materials. During the instructional design phase of the course development these material-initiated events were embedded into the course de- livery to ensure students had the opportunity to apply learn- ing concepts and share their knowledge with other students. The types of prompting events reflected the course design, covering practice and application activities, such as quizzes, questions and polls, and construction and connection activ- ities, such as word cloud and geotagging exercises and open discussions.  Tutor-initiated events were replies by students to forum and newsfeed topics initiated by course staff. The rationale for adopting this measure was that students who are more engaged would be more likely to participate in discussions initiated by course staff. For this study, no attempt was made to discriminate between forum replies by analysing the contents of the replies, the only requirement being that the reply was made by students not by course staff. A more detailed content-based analysis will be undertaken in future    work. A third measure considered was student-initiated engage-  ment events, defined as the number of forum topics initiated by individual students. The reasoning behind this choice of metric was that more engaged students would be more likely to initiate discussions with the course tutor and their fellow students on the discussion forums. Here, too, no attempt was made to analyse the content of the topics that were initiated, and again we leave this as future work.  For staff engagement, the principal metrics were the num- ber of forum discussion topics initiated by staff members and, more implicitly, the overall course design, principally the distribution of activities. Both metrics serve to the de- fine the number of potential engagement venues for students.  To complement the engagement data, performance and demographic data was collected for all students. Perfor- mance data included the marks obtained for the coursework and exam components of the two courses as well as the over- all mark. Demographic data is shown in Table 1.  3.3 Analysis The analyses planned to investigate Questions 1 and 2  were dictated by the available data. Question 1 examined whether the level of engagement shown by course staff had any effect on the levels of engagement of students. As de- scribed above, the primary measure for this analysis was the number of engagement events for both staff and stu- dents. This question was addressed largely through descrip- tive means. Student engagement events were aggregated across all students and grouped by initiation route: mate- rial, tutor or student initiation, as defined above. Overall student engagement, calculated as a sum across all initia- tion routes, was also considered. These aggregated measures were plotted against time so that trends associated with spe- cific characteristics of each course and key events such as exam dates or the end of term periods could be visualized.  Question 2 was centred on students themselves and ex- amined whether engaged students performed better. Cor- relation and simple linear regression analyses were used to investigate interactions between overall student engagement, as defined for Question 1, and overall course performance.  4. RESULTS  4.1 Question 1 Figures 1  3 show, respectively, the numbers of student-  initiated, tutor-initiated and material-initiated engagement events occurring during the pre-study, term-time study and exam period of the Accounting course and the Marketing course. Figure 4 shows the aggregated overall engagement. Important timed events, indicated by dashed vertical lines, are shown in the graphs and include: the start of the pre- term preparation time (prep), the start of the term (start), the end of the term (end), the due date for coursework (coursework), and the date of the final exam (exam).  For both courses, we see that student-initiated engage- ment was relatively stable over time. For the Accounting course, there was a spike in engagement in the weeks fol- lowing the end of the term and immediately preceding the examination period, suggesting that students were engaging more frequently in discussions about the course materials ahead of the assessment and exam.  Tutor-initiated engagement was low for both courses, in  absolute terms, although this may be in part due to the chosen metric, as course staff do not tend to initiate fo- rum discussions. In contrast, material-initiated engagement was high for both courses, representing the majority of the overall engagement. There was a downward trend in this as the courses progressed, more pronounced for the Accounting course. Since material-initiated events are the result of en- gagement activities that are explicitly planned by the course staff, this downward trend is likely to be reflective of a lower number of opportunities for student engagement rather than a lack of engagement by students.  Overall student engagement across the three categories was similar for the two courses, at least initially (Figure 4). The lower overall engagement observed for the Accounting course towards the end of the term is the result of a signif- icant drop in material-initiated engagement opportunities. To provide an overview of engagement for each course, the student-, tutor- and material-initiated engagement events are plotted together for Accounting in Figure 5 and Mar- keting in Figure 6. These graphs highlight again the higher proportion of engagement events that were initiated by the course material.  Week  En ga  ge m  en t e  ve nt  s  (n  )  prep 1 2 3 4 5 6 7 8 9 10 exam  0 10  20 30  40 50  60 70  80 accounting marketing start end coursework exam  Figure 1: Student initiated engagement  Week  En ga  ge m  en t e  ve nt  s  (n  )  prep 1 2 3 4 5 6 7 8 9 10 exam  0 2  4 6  8 10 accounting  marketing start end coursework exam  Figure 2: Tutor initiated engagement  4.2 Question 2 For this question, we wished to determine whether the  level of engagement exhibited by students was reflected in the marks obtained in assessment activities. This required a measure of individual student engagement, and a natu- ral choice was to use the overall engagement as defined for Question 1, aggregated at the student level. Student per- formance was represented by the overall mark obtained by a student on a course (Accounting or Marketing).  The Pearson correlation coefficient () between student engagement and course mark was 0.389 for Accounting and    Week  En ga  ge m  en t e  ve nt  s  (n  )  prep 1 2 3 4 5 6 7 8 9 10 exam  0 50  15 0  25 0  35 0  accounting marketing start end coursework exam  Figure 3: Material initiated engagement  Week  En ga  ge m  en t e  ve nt  s  (n  )  prep 1 2 3 4 5 6 7 8 9 10 exam  0 50  15 0  25 0  35 0  accounting marketing start end coursework exam  Figure 4: Total student engagement  0.228 for Marketing, suggesting that there is a weak rela- tionship. Simple regression analysis was used to further study this relationship by regressing overall course marks on overall engagement. For Accounting, the simple linear regression coefficient was positive and significantly differ- ent from zero, although the magnitude was small (coef = 0.5041, p = 0.00116, R2 = 0.151; Figure 7). For Marketing, the coefficient was not significantly different from zero (coef = 0.07680, p = 0.0635; Figure 8). These correlation and regression results suggest that there may have been a small, positive relationship between engagement and performance at the level of individual students.  5. DISCUSSION  5.1 Influencing student engagement Distance learning students by necessity have less interac-  tion with course staff. One of the primary questions ad- dressed here was whether there was a relationship between the level of engagement of students and that of staff. Fig- ures 4, 5 and 6 summarize the overall engagement of students across the two courses analyzed. The most striking observa- tion, seen for both the Accounting and Marketing courses, is the drop off in engagement in the second half of the term as compared with the first half. This is most apparent in the material-initiated student engagement (Figure 3).  For the Accounting course, the drop in material-initiated engagement was followed by an increase in student-initiated engagement (Figure 1). This spike could have two possible explanations: this is the period just before the final exam, therefore students often have more questions; alternatively, another explanation could be the lack of a revision session for the Accounting course, and hence students engagement in more online activity to compensate for that. The Marketing  Week  En ga  ge m  en t e  ve nt  s  (n  )  prep 1 2 3 4 5 6 7 8 9 10 exam  0 50  15 0  25 0  35 0  student tutor material start end coursework exam  Figure 5: Accounting student engagement  Week  En ga  ge m  en t e  ve nt  s  (n  )  prep 1 2 3 4 5 6 7 8 9 10 exam  0 50  15 0  25 0  35 0  student tutor material start end coursework exam  Figure 6: Marketing student engagement  course, conversely, did have an online synchronous session with the course leader in the weeks leading up to the final exam, which could explain the lack of a similar spike in student-initiated engagement for that course.  Following the analysis presented here, subsequent scrutiny of the designs of the two courses revealed a gradual reduc- tion in the engagement activities in the second half of both courses (more pronounced in Accounting) as more revision material tended to be introduced. This presents an opportu- nity for the Programme team to review these course designs ahead of their next delivery. The differentiation between the Accounting and Marketing courses in this respect presents an opportunity for peer interaction between the respective course leaders (even going so far as to show them the graphs in Figures 5 and 6) to encourage reflection, peer discussion and ultimately improvement of the course designs.  5.2 Student engagement and performance As Imperials Online Management programme is deliv-  ered predominantly online, the inherent motivation and self- discipline of each student is important in ensuring success on the programme. In our study here, we examined whether there is a relationship between engagement and performance at the student level. From correlation and simple linear re- gression analyses, it appears that there may be a weak posi- tive relationship between individual student engagement and performance. This effect was small and inconsistent, being more pronounced for the Accounting course (Figures 7 and 8).  A pertinent point here is the uniformity of the profile of students who are admitted to Imperials Business School (the student cohort is highly selective, to include people with suitable educational and employment experiences). More- over, since by design the majority of engagement events are driven by the course material, the overall student engage-    0 10 20 30 40 50 60  30 40  50 60  70 80  90  Engagement events (n)  C ou  rs e   m ar  k  (%  )  Figure 7: Regression of overall mark vs engagement events on Accounting course  0 10 20 30 40 50 60  50 55  60 65  70 75  80  Engagement events (n)  C ou  rs e   m ar  k  (%  )  Figure 8: Regression of overall mark vs engagement events on Marketing course  ment metric may not capture individual student character- istics well. A more suitable metric that more effectively captures individual differences may be able to identify more clearly an effect between engagement and performance. This is an area of ongoing study.  6. CONCLUSIONS This paper has considered a new online postgraduate de-  gree in Management at Imperial College London, delivered online through an in-house LMS, and has analysed inter- action data relating to a cohort of 67 students taking the first two courses of the degree. Two main questions were considered relating to the relationships between engagement and performance in this highly selected group of students. Our analyses showed that student-initiated engagement was relatively stable over time, although increasing in the run- up to the examination period if there was a lack of an ex- plicitly planned revision session in the course. Material- initiated events are the result of engagement activities ex- plicitly planned by the course staff, and our analyses showed a drop-off in these in the second half of both courses. Sub- sequent scrutiny of the course designs revealed a reduction in opportunities for engagement as more revision material was introduced. Since our correlation and regression results suggest that there may be a small, positive relationship be- tween engagement and performance at the level of individual students, our recommendations to course leaders going for- wards are to: (i) include an explicit revision session within their courses, and (ii) continue to design opportunities for material-initiated engagement even within the sessions in the latter parts of their courses that are introducing revi-  sion material. One of the guiding principles of course design on Impe-  rials Online Management Programme is to encourage sus- tained and synchronised engagement by a student cohort with rich online course material, rather than ad hoc recourse to contacting the course staff. The rationale for this is to maximise students opportunities to share their experiences through the planned discussion activities. A secondary con- sideration is scalability: by keeping the student cohort in- step it should not be necessary to deploy increasing numbers of course tutors as cohort numbers grow. This secondary consideration remains be tested: for example, it is antici- pated that the second intake on the new postgraduate degree in Management will comprise 150+ students, presenting the opportunity to investigate further this hypothesis.  We are now planning larger-scale investigations of Ques- tions 1 and 2, over more courses and more students as the current and new student cohorts progress through the post- graduate degree in Management, and more broadly through other degrees delivered at Imperials digital campus. These larger-scale studies will address additional questions, such as learners levels of satisfaction and perceived learning, and will include finer-grained analyses of students interactions, including analysing the contents of students posts. Our overarching aim is to employ the analyses described here, as well as design new ones, in order to investigate the re- lationships between different forms of participant engage- ment in the learning platform and their effects on students levels of performance and satisfaction, with the goal of de- riving improved guidelines for course designs and engaging course leaders in reflection, peer discussion and ongoing pro- fessional development.  7. REFERENCES [1] J. Arbaugh. System, scholar or students Which most  influences online MBA course effectiveness Journal of Computer Assisted Learning, 30(4):349362, 2014.  [2] C. Coffrin, L. Corrin, P. de Barba, and G. Kennedy. Visualizing patterns of student engagement and performance in moocs. In Proc. LAK14, pages 8392, 2014.  [3] A. L. Dyckhoff, V. Lukarov, A. Muslim, M. A. Chatti, and U. Schroeder. Supporting action research with learning analytics. In Proc. LAK 2013, pages 220229.  [4] R. Ferguson and D. Clow. Examining engagement: analysing learner subpopulations in massive open online courses (MOOCs). In Proc. LAK 2015, pages 5158.  [5] G. Siemens. Learning analytics: envisioning a research discipline and a domain of practice. In Proc. LAK 2012, pages 48.  [6] M. Wells, D. Lefevre, and F. Begklis. Innovation via a Thin LMS: A middleware alternative to the traditional learning management system. In Proc. 30th Ascilite Conference. Macquarie University, Australia, 2013.  [7] A. Wolff, Z. Zdrahal, A. Nikolov, and M. Pantucek. Improving retention: predicting at-risk students by analysing clicking behaviour in a virtual learning environment. In Proc. LAK 2013, pages 145149.    "}
{"index":{"_id":"31"}}
{"datatype":"inproceedings","key":"Harrison:2016:MFI:2883851.2883923","author":"Harrison, Scott and Villano, Renato and Lynch, Grace and Chen, George","title":"Measuring Financial Implications of an Early Alert System","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"241--248","numpages":"8","url":"http://doi.acm.org/10.1145/2883851.2883923","doi":"10.1145/2883851.2883923","acmid":"2883923","publisher":"ACM","address":"New York, NY, USA","keywords":"early alert systems, evaluation, financial, student retention","abstract":"The prevalence of early alert systems (EAS) at tertiary institutions is increasing. These systems are designed to assist with targeted student support in order to improve student retention. They also require considerable human and capital resources to implement, with significant costs involved. It is therefore an imperative that the systems can demonstrate quantifiable financial benefits to the institution. The purpose of this paper is to report on the financial implications of implementing an EAS at an Australian university as a case study. The case study institution implemented an EAS in 2011 using data generated from a data warehouse. The data set is comprised of 16,124 students enrolled between 2011 and 2013. Using a treatment effects approach, the study found that the cost of a student discontinuing was on average $4,687. Students identified by the EAS remained enrolled for longer, with the institution benefiting with approximately an additional $4,004 in revenue per student over the length of enrolment. All schools had a significant positive effect associated with the EAS and the EAS showed significant value to the institution regardless of the timing when the student was identified. The results indicate that EAS had significant financial benefits to this institution and that the benefits extended to the entire institution beyond the first year of enrolment.","pdf":"Measuring financial implications of an early alert  system   Scott Harrison  University of New England   Armidale, NSW 2351  Australia    sharris3@myune.edu.au   Renato Villano  University of New England   Armidale, NSW 2351  Australia    rvillan2@une.edu.au     George Chen  University of New England   Armidale, NSW 2351  Australia    gchen2@une.edu.au   Grace Lynch  University of New England   Armidale, NSW 2351  Australia    glynch2@une.edu.au     ABSTRACT  The prevalence of early alert systems (EAS) at tertiary institutions  is increasing. These systems are designed to assist with targeted  student support in order to improve student retention. They also  require considerable human and capital resources to implement,  with significant costs involved. It is therefore an imperative that  the systems can demonstrate quantifiable financial benefits to the  institution. The purpose of this paper is to report on the financial  implications of implementing an EAS at an Australian university  as a case study. The case study institution implemented an EAS  in 2011 using data generated from a data warehouse. The data set  is comprised of 16,124 students enrolled between 2011 and 2013.  Using a treatment effects approach, the study found that the cost  of a student discontinuing was on average $4,687. Students  identified by the EAS remained enrolled for longer, with the  institution benefiting with approximately an additional $4,004 in  revenue per student over the length of enrolment. All schools had  a significant positive effect associated with the EAS and the EAS  showed significant value to the institution regardless of the timing  when the student was identified. The results indicate that EAS had  significant financial benefits to this institution and that the  benefits extended to the entire institution beyond the first year of  enrolment.    Categories and Subject Descriptors  G.1.6 [Numerical Analysis]: Optimization  Linear  programming; G.3 [Probability and Statistics]: Multivariate  statistics, Statistical computing;    General Terms  Measurement, Reliability, Theory   Keywords  Financial, evaluation, student retention, early alert systems   1. INTRODUCTION  An increased number of universities are implementing EAS, and  correspondingly an increased number of vendors are offering  systems. Proactively identifying students in need of support  comes at a cost in terms of implementing and maintaining such  systems. For the administrators, hard questions about financial  resources need to be asked. How much should an institution spend  to attain an improvement in student outcomes Do early alert  systems have a positive return on investment and, if so, by how  much If changes are made to the system, can those changes be  quantified    This study analyses the effect of a specific EAS using a financial  metric; namely, student tuition fees. From the intuition's  perspective, these tuition fees can be termed as revenue. The  interchangeability of these terms depends on perspective;  however throughout this study the term revenue is used.    The data set captured 16,124 students enrolled over a three-year  period between 2011 and 2013 when the EAS was initially  deployed at the case study institution. Several revenue models  were tested using a treatment effects approach. This study (1)  analyzed the financial implications of student retention rates,  estimating the overall cost of students discontinuing, (2) the  overall effect EAS had on revenue from students, (3) compared  the difference in revenue for continuing and completing students  versus discontinuing students, under the conditions of EAS  identification, (4) analyzed the variation in revenue between  schools within the university, and (5) examined the effects on   revenue associated with the timing of identification. All estimated  financial effects are calculated in Australian dollars.   2. PREVIOUS STUDIES   Research into the financial implications and effectiveness of EAS  is limited due to the field still developing and maturing. Simons  [10] conducted a survey of 529 four-year higher education  providers in the US. A key research question of the study focused  on the effectiveness of early alert programs, how did institutions  measure effectiveness and the overall impact of the program on  students [10]. Two key measures, overall retention and between   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org. LAK '16,  April 25-29, 2016, Edinburgh, United Kingdom  2016 ACM. ISBN 978- 1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883923      teaching periods persistence, were the most frequent responses.  In evaluating the program effectiveness,    of the almost 40% that noted retention as the ultimate goal, most  did not clarify a specific program outcome that precipitated  retention. It is nearly impossible to link student retention in  general terms directly back to a service area. [10]   While some time has passed since this survey was conducted, the  facts still hold true that there has been little published on program  effectiveness.    Arnold and Pistilli [1] estimated the benefits of Course Signals  program at Purdue. It was concluded that students taking signals  courses had the effect of improving graduation rates by 21%. This  study however was criticized by Caufield [7] for not making it  clear if the number of courses were controlled for. As such, it was  not possible to disaggregate the effects of students taking more  Course Signals courses because they persist,  [compared to]  persisting because they are taking more Signals courses [6].   Marrington et al. [9] analyzed the benefits of an in-house program  for the first-year students at Queensland University of  Technology. The student success program monitored first-year  student data identifying students at risk of attrition, allowing  targeted interventions to take place. A critical part of this study  was to take the estimated effects of the program and to turn these  into tangible financial benefits of the program. Using EFTSL  (equivalent full-time student load) estimates of student tuition  fees paid, around AUD$1.7 million in student tuition fees were  retained, taking into account program costs. It was argued that this  was positive evidence for the economic case of EAS.    The main issue with EAS evaluation is establishing a causal link  between the system and improved retention outcomes. It is  fundamental to all institutions implementing EAS that they can  prove that the system is causing an improvement in outcomes.  The objective of this section is to quantify financially the  additional benefit derived from improvements in retention.   3. METHODOLOGY  3.1 Case study institution  The case study institution is one of 39 public institutions in  Australia. It is a regional university with approximately 22,400  students (undergraduates and postgraduates), of which 79% are  distance (off-campus online) students. The university comprises  of 10 traditional schools of study, including arts, science, business  and health. It has a strong focus on supporting students from  diverse backgrounds, with multiple pathways of admission. The  retention rate is typically below the national average and  improving retention is an important focus of the university.   The EAS implemented at the case study institution was developed  in 2009, revised in 2011, and deployed across the institution. 34  triggers which capture various aspects of the students enrolment  are used to create a daily score. The scores are ranked, with the  top-200 students each day being identified as in need of targeted  support. The identification process is the demarcation for  separating students into identified and not-identified categories.   3.2 Calculating revenue  Revenue is generally calculated by the price per unit of study  multiplied by the quantity of units undertaken. In an ideal  situation, tuition fees for each student are provided as part of the  data set. However, in lieu of this data not being presented, it is  possible to create a close approximation of the student fees paid  by each student, using the fee schedule of the institution and   matching this with the students academic record. The fee  schedule comes in three distinct brackets which were used in  previous statistical models. These are domestic Higher Education  Loan Program (HELP) students, domestic fee paying students and  international students. Intersecting with domestic student fee  categories, university courses are assigned bands, an Australian  government regulated fee structure for study undertaken in a  given area. The domestic student fee schedule is provided in  Table 3.1.   Table 3.1: Fees per unit of study      Domestic  HELP   Domestic  Fee Paying    Per Unit Per Unit   Band 1 $755 $679   Band 2 $1,076 $968   Band 3 $1,260 $1,134   Band 4 $755 $679   Band 5 $1,260 $1,134   Band 6 $1,076 $968   Band 7 $1,076 $968     International student fees are set independent from the regulated  pricing scheme and charged on an annual basis. A full year of  study typically consists of 8 units of study. The annual  international fee is divided by 8, proving the estimated cost per  unit of study for an international student. The cost per unit of  study was imputed based on the international student fee schedule  for 2013 [11] and matched with the corresponding bands of study  for domestic students. The fee schedule used in this study links  schools to fee category, presented in Table 3.2.   Table 3.2: Fee schedule by school   School Band Domestic Help  Domestic  Upfront   International  Fee   1 Band 1 $755 $680 $2,228   2 Band 1 $755 $680 $2,228   3 Band 2 $1,076 $968 $2,621   4 Band 6 $1,076 $968 $2,621   5 Band 3 $1,260 $1,134 $2,231   6 Band 4 $755 $680 $2,228   7 Band 3 $1,260 $1,134 $2,621   8 Band 2 $1,076 $968 $2,296   9 Band 1 $755 $680 $2,228   10 Band 5 $1,260 $1,134 $1,966     Using the fee schedule in Table 3.2, the revenue generated from  each student is estimated by multiplying the students fee  category for school of enrolment, with the number units  undertaken where tuition fees would be paid. A limitation on this  method is that it fails to capture the effect on fees when students     undertake double degrees in different schools with different  bands. The model categorizes students in the school where the  majority of study was undertaken. This is a source of variation  between the estimated tuition fees and the actual tuition fees.  However, it can be argued that on aggregate across the large data  set, the tuition fees associated with double degrees will balance  out.    3.3 Treatment effects modelling  3.3.1 Background  Treatment effects models refer to a family of statistical models  which allow causal inferences to be made using observational  data. The models were originally designed for use in medicine,  where ethical reasons and study design limitations prevented the  use of control groups. The model is applicable for the evaluation  of EASs, where it is difficult to have a true control group of  students with limited, controlled or no access to student support.    There exist many different treatment effects models to suit  varying situations. Propensity score matching (PSM) is one of the  models, with frequent use in economic applications. Propensity  score refers to the likelihood of being in the treated/untreated  groups based on observed explanatory variables. Bryson [3] used  PSM to test if being a union member caused employees to have  higher wages, finding that this only occurred in specific cases.  Brand and Halaby [2] used a combination of regression and PSM  approaches to analyze the effects of elite college attendance and  career outcomes, with mixed results when analyzing wage  premiums. Caliendo et al. [4] used PSM to measure the effects of  job creation schemes in Germany, with sub-group analysis  revealing that only long-term unemployed women from East  Germany having a significant benefit from the schemes.    PSM functions on matching observations from the treatment  group to the non-treatment group to develop the counterfactual  analysis required in causal analysis. A contextualized diagram  depicting the matching process is presented in Figure 3.1.      Figure 3.1: Diagram of treatment effects nearest neighbor   matching  In the diagram, a student identified by the EAS is matched with a  similar student with exactly the same gender (Male = Blue,  Female = Red) and school (S). Comparing the two students, the  treatment effects model imputes how much the identified student  would pay in tuition fees based on not being identified, estimated   from the non-identified students outcome. The vice versa applies;  the non-identified students tuition fees are imputed based on  being identified by the EAS. The imputed results are then  compared, allowing an estimation of the treatment effect. The  method further allows matching on multiple nearest-neighbors,  which increases the precision of effect estimation [5].   Two characteristics of the matching process are, firstly, some  students will be excluded due to no suitable match being found.  For example, in Figure 3.1, the male student from school 9 has no  comparable match in the treatment group. This results in varying  sample sizes for models. Secondly, excluding some students from  the sample means some explanatory variables may be excluded  due to collinearity. While these characteristics of the process are  not ideal, the estimated effects remain robust due to the model  focusing on the overall average effect of the treatment, and not  the individual effect of any one explanatory variable.   Caliendo and Kopeinig [5] outlined the practical considerations  of implementing PSM methods, including selection of the  matching algorithm. Four matching algorithms are commonly  used to pair observations for analysis. Firstly, the nearest neighbor  (NN) algorithm identifies the nearest neighbor based on a distance  function measure of propensity score. The second algorithm,  caliper and radius, limits the distance to the nearest neighbor to  ensure that matches are representative of the alternative outcomes  of treatment. Thirdly, stratification and interval matching allows  analysis within sub-groups, comparing mean differences in  outcomes between treated and control observations. Finally,  kernel and local linear matching are non-parametric estimators  that use weighted averages on (nearly) all, depending on the  choice of kernel function, individuals in the control group to  construct the counterfactual outcome [5].    3.3.2 Approach taken  For the purposes of this analysis, the nearest neighbor method was  selected. This algorithm allows observations to be exactly  matched based on theoretical underpinnings of the model  estimated. The models use demographic, institutional and  workload variables. The demographic variables used are gender,  age, age squared and Aboriginal or Torres Strait Islander (ATSI)  status. Previous research has shown that demographic variables  have a significant effect on retention and therefore need to be  included in the model. The institutional variables used include fee  type, prior studies, course type and school of study. The fee type  of the students is important as the fee schedule varies between  these groups, while course type and school of study also reflect  varying fee structures. The workload variable is the average  number of units studied per teaching period. Workload is included  to ensure that students undertaking similar number of units for  each teaching period are compared. For determining financial  effects in this study, it is important to exactly match students in  the treatment group to the control group, on categorical variables;  these include gender, ATSI status, domestic fee, international fee,  course type, and school of study. The data set used for the analysis  was a pooled data set containing data across multiple teaching  periods with students commencing at various times.    A potential source of bias with the model is students who started  in 2011 and identified by the EAS are matched and compared to  students who started in 2013 and not identified by the EAS. To  address this issue, an additional variable was included in the  model, teaching period of commencement. This codes the  students commencement date as a categorical variable, allowing  exact matching of students based on when they first started  classes. This prevents temporal mismatching to occur and ensures   Not EAS  Identified   EAS  Identified   S3   S2   S9   S5   S3   S3   S2   S5     robust estimation of the effect. Additionally, students are matched  with replacement with a minimum of 4 matches per student.  The result is the average quality of matching will increase and  the bias will decrease [5].   This study used five models to estimate varying effects on  revenue. The objective of each model in order was (1) estimate  the revenue effect of discontinuing study; (2) estimate the revenue  effect of being identified by the EAS; (3) estimate the revenue  effects of discontinuing under the condition of EAS  identification; (4) estimate the effects of being identified by the  EAS within schools of the case study institution; and (5) estimate  the effects of being identified by the EAS at times of  identification.   The first model used the students enrolment status as the  treatment variable, with revenue as the dependent variable,  estimating the financial effects associated with discontinuing. By  comparing discontinued students to all other students, it is  possible to estimate the cost to the institution when a student  discontinues. Comparing discontinued students to completed  students estimates the cost to the institution of a discontinuing  student not completing their qualification. These two measures  indicate the magnitude of financial loss associated when students  decide to discontinue. Diagrammatically, the relationship  between revenue and the discontinuation is depicted in Figure 3.2.        The relationship between revenue and the EAS is depicted in  Figure 3.3. When estimating the effect of a given treatment, it is  important to not include variables which can be affected by the  treatment itself [5]. It is plausible and expected that student  performance will change as a result of the EAS identification and  resulting support. As such, its exclusion from the model is  justified.   The treatment effects approach has two parameters of interest in  identifying the causal relationship. The first parameter is the  average treatment effects (ATE), which compares the effect to the  entire population. This provides a broad estimate of overall causal  effect of the EAS. The second parameter is the average treatment  effects on the treated (ATET), which only captures the effect on  those people affected by a particular treatment. This provides a  more specific estimate of what the causal effect would be under  the condition of being identified by the EAS. These two  parameters of interest are estimated for all models.         Figure 3.3: EAS Treatment Effects Model   4. RESULTS  To make valid inferences from the estimated models, analysis of  the matching process verified that the estimated models were  valid. The results of the post estimation analysis for the first  model are presented in Table 4.1. This is representative of the  matching results for all models. Two measures indicate the  validity of the matching process, standardized differences in  means between the control and treatment effects groups, and the  variance ratios. It is expected that under perfect matching of  observations, the standardized differences after matching should  approach zero, while the variance ratios after matching should  approach one.    Table 4.1: Post estimation check for valid matching      Standardized  Differences Variance Ratios   Raw Matched Raw Matched   Age -0.0085 0.0405 1.0063 1.1000   Age2 -0.0060 0.0477 0.9910 1.1469  Units   Enrolled 0.1695 0.0577 0.6756 0.8593      The results show that age and age squared variables were similar  before and after the matching process. This is supported by the  matching density plot in Figure 4.1.    The density graphs for age show that the matching process  improved the distribution of observations on age in the peak at  around age 20. However, the matching process didnt match quite  as well at the age of 30. This difference is only minor and overall  this indicates that the matching process on age is valid.    The matching process on the basis of the workload was also  successful. Workload was measure by the average number of  units a student undertakes during their study. The standardized  difference in means improved from 0.1695 to 0.0577, with the  variance ratios increasing after matching to be closer to one. The  density plot in Figure 4.2 shows the matching process aligns the  distribution of the control and treatment groups.       EAS Identification EAS Results   Course  Institution   Student   Enroll  Revenue   Early alert system and  targeted student support   Course  Institution   Student   Enroll   Revenue paid by  enrolled or   completing students   Revenue/fees paid by  discontinuing   Figure 3.1: Discontinuation treatment effects model       Figure 4.1: Balance plot for matching on age     Figure 4.2: Balance density plot for matching on workload   This indicates valid matching on workload, indicating that the  model overall is statistically valid. Given that the remaining  variables in the model are categorical in nature and exactly  matched, only the continuous variables needed to be verified. It  can be concluded that the matching process is valid for all models.   4.1 Financial implications of discontinuing  The first model estimated analyzed the effect on revenue resulting  from the decision to discontinue. One approach to quantifying this  effect compared discontinued students to the entire student  cohort, consisting of enrolled and completed students. This  captured the loss of revenue at the moment when a student  discontinues. Another approach compared discontinued students  only to those who have completed their qualifications. This  provided an estimate of the overall loss of revenue when a student  doesnt complete their course of study. The results for the two  approaches are presented in Table 4.2.   The results for both models are significant at the 1% level. The  cost of discontinuing indicates that for each student discontinued,  the university on average loses $4687 overall, with the decision  for an individual student costing $4231 on average. Comparing  discontinued and completed students, the institution loses $7170  overall when a student discontinues instead of completing. In any  particular case, the cost is higher at around $7307.   Table 4.2: Estimated costs of discontinuing      Cost of discontinuing Cost of not completing   ATE ATET ATE ATET   Coefficient  ($) 4,687  a 4,231a 7,170a 7,307a   Robust  Standard  Error ($)   65.60 70.06 255.66 339.99   Sample  Size 13,690 13,690 2,460 2,460   a,b,c significant at 1%, 5% and 10% levels of significance,  respectively.   Table 4.3 indicates the retention rates at the case study university  over the period captured by the data set [8]. It includes the number  of students who would be retained from a 1% increase in the  retention rate. Combining the ATE estimated from Table 4.2 with  the retention rates in Table 4.3, the financial implications of  improving retention by 1% are presented in Table 4.4.   Table 4.3: Retention rates   Year  Dept.   Education  Retention Rate   Number of  students retained  from 1% increase   in retention  2011 73.47% 121.20   2012 73.26% 128.57   2013 71.61% 135.04     The benefit of a 1% increase in retention is calculated by the  number of students affected by a 1% increase multiplied by the  ATE.   Table 4.4: Financial implications of improving retention   Year   Discontinued  v Enrolled   Discontinued  v Completed   Benefit of 1%  Retention   Increase ($)   Benefit of  1% Retention  Increase ($)   2011 568,100 868,947   2012 602,646 921,786   2013 632,972 968,173     Comparing discontinued students to both enrolled and completed  students, the benefits of increasing student retention by 1% range  from $568,100 in 2011 to $632,972 in 2013. The benefits of  increasing student retention and seeing these students through to  graduation results in a financial benefit of between $868,947 in  2011 to $968,173 in 2013. Given that student numbers have been  increasing over time, this means that the financial benefits of  increasing retention will also increase over time.   In summary, the results show there is a significant financial effect  associated with students discontinuing. This is not a surprising  outcome, however what is important is the magnitude of the  problem. Being able to accurately estimate the size of the  financial implications associated with retention yields important     information on potential benefits that can be gained from  increasing student retention. Furthermore, this provides an  important benchmark to measure from should new programs be  introduced to affect student retention.   4.2 The effect of an EAS on revenue  Measuring the effects of an EAS is an important process all  institutions need to undertake to validate the efficacy of the  system. Using treatment effects models, students were divided  into two groups, identified by the EAS and not identified by the  EAS. Ideally, having more information on the student support  process post identification would allow more detailed analysis of  the individual aspects of the program and their contribution to the  revenue function.    In lieu of this detailed data no being present, it is still applicable  to treat the EAS as a black box process and estimate meaningful  effects resulting from the EAS. The estimated ATE and ATET are  presented in Table 4.5.    The effect of the EAS on revenue is significant at the 1% level for  both the ATE and ATET estimates. The EAS is estimated to  increase overall student tuition fee spending by around $4004 per  student.   Table 4.5: Overall financial effect of the EAS on revenue     ATE ATET   Coefficient  ($) 4,004  a 5,058a   Robust  Standard  Error ($)   80.87 102.56   Sample  Size 14,012 14,012   a,b,c significant at 1%, 5% and 10% levels of significance,  respectively.   Individual students identified by the program on average  continued to spend an extra $5058 in fees than students not  identified by the program. This is a significant finding which  highlights the financial benefits of the EAS. This positive result  corroborates the results found from earlier models, that there is a  significant positive effect associated from the program.   The estimate however does not come without limitations and  issue. No similar benchmark estimate of student support system  is available prior to the introduction of the EAS. As such, it is not  possible to estimate the benefits of changing from the prior  support system to the EAS. This is an important measure in  determining the benefit/cost ratio of installing the EAS and would  help validate the EAS further. On the positive side, however, the  estimate provided can be used to benchmark future changes to the  system. If the system is enhanced or changed in any manner, then  the same process can be repeated to allow comparisons of the  program valuations and estimation of benefit/cost ratios can be  calculated for the program revision.   4.3 Discontinuing and EAS identification  The previous two sections estimated the cost of students  discontinuing and the benefit of the EAS overall. Another way to  analyze the financial effect of the EAS is to estimate the  additional tuition fees a continuing or completing student spends  versus a discontinuing student, under the condition they were  identified or not identified by the EAS. Only the ATE is shown   for simplicity, however the ATET estimates only varied slightly  from the ATE estimates.    The results for each groups is significant at the 1% level. The  results show if a student is not identified by the EAS and  continues, they will spend approximately $2263 more than a  student who discontinues. If however the student is identified by  the EAS, the student will spend $5138 additional tuition fees  compared to a student who discontinues. A t-test for the  difference of two means results in a t-test statistic of 25.36. This  means the difference between the two groups is significantly  different at the 1% level. The additional $2875 in student tuition  fees associated with EAS identification further supports the  benefits of the EAS system.   Table 4.6: Cost of discontinuing given EAS identification    Discontinue vs Continue  Discontinue  vs Complete   Not  Identified   ATE  Coefficient   ($)  2,263a 4,960a   Robust  Standard  Error ($)   69.77 112.37   Sample  Size 3,142 183   Identified   ATE  Coefficient   ($)  5,138a 7,528a   Robust  Standard  Error ($)   89.33 321.97   Sample  Size 9,099 1,623   a,b,c significant at 1%, 5% and 10% levels of significance,  respectively.   In the second model, if a student was not identified by the EAS  and completed, on average the students would spend an additional  $4960 compared to a discontinuing student. If a student was  identified and completed, then the additional amount of revenue  spent compared to a discontinuing student was $7528. A t-test for  differences in means attains a test statistic of 7.53, indicating that  the difference is significant at the 1% level. Again, the statistical  difference between these two groups suggests the financial  benefit of the EAS.    4.4 EAS effects within schools  Variation between schools yields important information about  level of identification within each school and can assist with areas  of targeted resource allocation. For this section, the data set of  students is divided into individual schools. The treatment effects  model estimates the effect of the EAS on revenue within each  school. The results are presented in Table 4.7.   The results show significant effects in all schools at the 1% level.  The estimated treatment effects on identified students range from  school 2, the smallest identification effect of $2,263, through to  school 3 with an identification effect of $6,507. The range of  variation between schools indicates that the program has a larger  benefit in some school versus others. This supports the case that  the differences between schools need to be factored into EAS  design.     Table 4.7: EAS financial effects within schools   School  (Sample   Size)   ATE  Coefficient   ($)   ATE  Standard  Error ($)   ATET  Coefficient   ($)   ATET  Standard  Error ($)   Base (1001) 5112a 307 6098a 377   1 (1409) 2881a 206 3771a 265   2 (2086) 2263a 134 2992a 180   3 (1320) 6507a 304 7685a 359   4 (1677) 4446a 251 5143a 305   5 (1753) 4931a 296 6017a 366   6 (2940) 3394a 145 4363a 186   7  - - - -   8 (807) 5627a 460 6816a 543   9 (919) 2269a 196 3189a 288  a,b,c significant at 1%, 5% and 10% levels of significance,  respectively.   The most important conclusion that can be drawn from the results  is that all schools have a significant positive effect associated with  the EAS. This supports the case of EAS being implemented at an  institutional level where a unified approach is taken to offering  support.   4.5 Timing of EAS identification  The final method of analyzing the EAS effect was to introduce  the temporal effect. It is expected that students identified earlier  in their studies will have a greater revenue effect than students  identified later in their studies. Students can be divided into four  categories, those never identified by the EAS, and those identified  by the EAS in either the first, second or third year of study. Table  4.8 shows that the majority of students are first identified by the  EAS in their first year.    Table 4.8: Timing of student identification   Year Identified Number of Students  Not Identified 4,830   1 10,619   2 596   3 79   Total 16,124     Using the treatment effects model, students identified in years 1,  2 and 3 are compared to the base group of students not identified.  The results are presented in Table 4.9.    The results for the ATET show that students identified by the EAS  in their first year contributed $4000 more in revenue than students  not identified by the EAS. By the second year, this had reduced  to $2675 but in the third year it increased to $3479. All results are  significant at the 1% level which indicates that the EAS maintains  significant value for all students, regardless of when they were  identified. This is an important finding as it shows that there is  value of an EAS beyond the first year of enrolment.      Table 4.9: EAS effects by year of identification   Year 1 2 3   Estimate ATE ATET ATE ATET ATE ATET  Coefficient   ($) 4,000 a 5,151a 2,675a 3,911a 3,479a 3,256a   Robust  Standard  Error ($)   83 106 214 260 546 675   Sample  Size 13343 13343 1755 1755 187 187   a,b,c significant at 1%, 5% and 10% levels of significance,  respectively.   5. DISCUSSION  Revenue from student tuition fees is a fundamental source of  income for institutions. The size of the retention problem was  estimated using revenue from tuition fees as a baseline metric.  The treatment effects method of estimating the benefits of the  EAS yielded statistically significant results in all models  estimated. Using demographic, institutional and workload  variables to match observations, valid comparisons were made  between students when a control group is not possible. This  supports the use of treatment effects modelling as an appropriate  method of analysis and evaluation for EAS.   Comparing discontinued students to the general student  population, the first model indicated that the cost of students not  being retained was approximately $4,687 per student. If  discontinued students are compared to graduating students, the  estimate is significantly higher, costing the institution  approximately $7,170 per student. These estimates show the  magnitude of financial loss associated with current student  discontinuation. A modest 1% increase in the undergraduate  student retention rate can yield significant financial benefits. With  a 1% increase in student retention, the university could have  gained an additional $568,000 in 2011, up to $633,000 in 2013.  If the 1% of undergraduate students retained continues onto  graduation, this increases further to a benefit of $868,000 in 2011  to $968,000 in 2013. The estimated cost of discontinuation is an  important measure for any institution. For the case study  institution, since approximately $4,700 is lost per student who  discontinues, then theoretically this also acts as an upper estimate  of the additional amount of funds that can be spent per student to  improve retention. This forms an important baseline measurement  for funding student support initiatives for the case study  institution.   Estimating the effect of the EAS overall showed that students  identified by the EAS end up paying more tuition fees on average  than students not identified by the system. This correlates to being  enrolled longer and undertaking more units. The revenue linked  to the EAS was quantified, with students identified by the EAS.  This is an important significant finding. Given that the majority  of student support services at the institutional level function  through the EAS, this provides an overall estimate of the value of  student support at the case study institution. For administrators,  having this information can greatly enhance understanding and  the importance of adequately funded student support services.  Furthermore, this estimated value allows a benchmark to be taken.  Future changes and enhancements to the EAS can be quantified  allowing benefit/cost estimates to be calculated, allowing  administrators to make evidence based decisions.     Analyzing students under the condition of EAS identification, the  results supported the previous estimates. Within the identified  group of students, the revenue from continuing students was on  average $5,138 more than discontinuing students, whereas in the  not identified group, the revenue from continuing students was  only on average $2,263 more than discontinuing students. The  difference between these two amounts was statistically  significant, indicating that there is a significant increase in  revenue associated with EAS identification. When comparing  discontinuing students to graduating students not identified by the  EAS, the revenue from graduating students was on average  $4,960 more than discontinuing students. When students were  identified by the EAS, the revenue from graduating students was  $7,528 more than those discontinuing students. The difference  between the two estimated effects was statistically significant at  the 1% level, further supporting the conclusion that the EAS has  significant financial benefits in terms of revenue, which translates  to increased student enrolment and improved retention.   Analyzing within schools, the results showed an amount of  variation of EAS effects on revenue. School 9 had the smallest  ATE estimate of $2207 with school 3 having the largest ATE of  $6617. Overall, all schools (where possible to estimate) had a  benefit from the EAS, supporting an institution wide approach to  EAS design. This is an important significant finding, as it shows  that a demonstrated institution wide approach to EAS design can  have benefits for all schools within the institution. This will vary  depending on institutions; however the results here indicate that  for the case study institution, the EAS is a benefit to all schools.  The results indicate that some schools may also benefit from  additional support programs within schools, and that funding  allocated to these programs should reflect both the benefits and  the need for programs based on retention rates.   The final model analyzed the value of the EAS based on when  students were first identified. It shows that students identified in  their first year of study corresponded to an additional $4,000 in  revenue. If the student was identified in their second or third year  of study, the additional amount of revenue was $2,675 and  $3,479, respectively.    All of these estimates were significant at the 1% level, indicating  that the EAS has significant value for all students, independent of  course progression. This is a significant finding in the context of  student retention, given that the major focus in the past has been  on first year student retention. The results show that EAS can  have significant value to students at later stages of study. This  further supports that EASs should not only be implemented at the  school level, but capture all students within the institution.  Overall, the results show that there is significant financial benefit  to implementing an institution wide EAS which captures all  students irrespective of course progression. This lays the  foundation for future studies where a full economic analysis,  including the costs of implementation and maintenance of EAS,  can be conducted.   6. ACKNOWLEDGMENTS  The authors acknowledge the support of the staff at the case study  institution in making an expansive data set available for analysis.               7. REFERENCES  [1]  Arnold, K.E. and Pistilli, M.D., 2012. Course Signals at   Purdue: Using learning analytics to increase student success.  In Proceedings of the 2nd International Conference on  Learning Analytics and Knowledge ACM, 267-270. DOI=  http://dx.doi.org/10.1145/2330601.2330666   [2]  Brand, J.E. and Halaby, C.N., 2006. Regression and  matching estimates of the effects of elite college attendance  on educational and career achievement. Social Science  Research 35, 3, 749-770. DOI=  http://dx.doi.org/10.1016/j.ssresearch.2005.06.006   [3] Bryson, A., 2002. The union membership wage premium: an  analysis using propensity score matching. Accessed from  https://ideas.repec.org/s/cep/cepdps.html   [4]  Caliendo, M., Hujer, R., and Thomsen, S., 2008. The  Employment Effects of Job Creation Schemes in Germany-A  Microeconometric Evaluation. Advances in econometrics 21,  383-430. DOI=  http://www.emeraldinsight.com/doi/abs/10.1016/S0731- 9053(07)00013-8   [5]  Caliendo, M. and Kopeinig, S., 2008. Some practical  guidance for the implementation of propensity score  matching. Journal of economic surveys 22, 1, 31-72. DOI=  http://onlinelibrary.wiley.com/doi/10.1111/j.1467- 6419.2007.00527.x/full   [6]  Caufield, M., 2013. A simple, less mathematical wat to  understand the course signals issue. Accessed from  http://hapgood.us/2013/09/26/a-simple-less-mathematical- way-to-understand-the-course-signals-issue/   [7]  Caufield, M., 2013. Why the Course Signals Math Does Not  Add Up. In Hapgood. Accessed from  http://hapgood.us/2013/09/26/why-the-course-signals-math- does-not-add-up/   [8]  Department Of Education, 2014. 2013 Student Summary,  Department Of Education Ed., Canberra. Accessed from  https://education.gov.au/selected-higher-education-statistics- 2013-student-data   [9]  Marrington, A.D., Nelson, K.J., and Clarke, J.A., 2010. An  economic case for systematic student monitoring and  intervention in the first year in higher education. In  Proceedings of 13th Pacific Rim First Year in Higher  Education Conference QUT, Adelaide. Accessed from  http://www.fyhe.com.au/past_papers/papers10/content/pdf/6 D.pdf   [10]  Simons, J.M., 2011. A National Study of Student Early  Alert Models at Four-Year Institutions of Higher Education.  In ProQuest LLC. Accessed from  http://eric.ed.gov/id=ED535792   [11]  University Of New England, 2013. International  Prospectus, University of New England.           "}
{"index":{"_id":"32"}}
{"datatype":"inproceedings","key":"Khan:2016:DSR:2883851.2883911","author":"Khan, Imran and Pardo, Abelardo","title":"Data2U: Scalable Real Time Student Feedback in Active Learning Environments","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"249--253","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883911","doi":"10.1145/2883851.2883911","acmid":"2883911","publisher":"ACM","address":"New York, NY, USA","keywords":"dashboard, feedback, learning analytics, visualizations","abstract":"The majority of applications and products that use learning analytics to understand and improve learning experiences assume the creation of actionable items that will affect students through an intermediary. Much less focus is devoted to exploring how to provide insight directly to students. Furthermore, student engagement has always been a relevant aspect to increase the quality of a learning experience. Learning analytics techniques can be used to provide real-time insight tightly integrated with the learning outcomes directly to the students. This paper describes a case study deployed in a first year engineering course using a flipped learning strategy to explore the behavior of students interacting with a dashboard updated in real time providing indicators of their engagement with the course activities. The results show different patterns of use and their evolution throughout the experience and shed some light on how students perceived this resource.","pdf":"Data2U: Scalable Real time Student Feedback in Active  Learning Environments  Imran Khan  imran_ak@outlook.com  Abelardo Pardo  abelardo.pardo@sydney.edu.au  Faculty of Engineering and IT  The University of Sydney, NSW 2006 Australia  ABSTRACT The majority of applications and products that use learning ana- lytics to understand and improve learning experiences assume the creation of actionable items that will affect students through an in- termediary. Much less focus is devoted to exploring how to provide insight directly to students. Furthermore, student engagement has always been a relevant aspect to increase the quality of a learning experience. Learning analytics techniques can be used to provide real-time insight tightly integrated with the learning outcomes di- rectly to the students. This paper describes a case study deployed in a first year engineering course using a flipped learning strategy to explore the behavior of students interacting with a dashboard updated in real time providing indicators of their engagement with the course activities. The results show different patterns of use and their evolution throughout the experience and shed some light on how students perceived this resource.  CCS Concepts Applied computing ! Computer-assisted instruction; E-learning;  Keywords Feedback; Learning Analytics; Dashboard; Visualizations  1. INTRODUCTION Learning analytics applications are increasing their presence in  higher education institutions after being portrayed as the most dra- matic factor shaping the future of this sector [26]. There is a grow- ing portfolio of approaches and products that rely on ubiquitous data collection applications to increase the insight on how students learn. But the target group for the majority of these initiatives are either governments, institutional management teams, curricu- lum designers, or instructors. The body of research considering students as the direct target group to receive data-driven insight is not as comprehensive. Using data to support decision making pro- cesses is complex and may be more effective when mediated by an expert. But students in higher education institutions are partici- pating in learning experiences hosted in increasingly diverse tech-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.  ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883911  nological platforms and with an equally diverse type of strategies. Student engagement is one of the aspects that has been investigated in detail specially in active or online learning scenarios.  There are numerous barriers preventing a more direct connection between learning analytics and the provision of frequent, meaning- ful feedback as it requires a tight integration between the collection, analysis and reporting stages of the process. Additionally, feed- back given to students should provide insight beyond simple click counts when visiting resources in the Learning Management Sys- tem (LMS). It should contain information about their higher level engagement in a course to facilitate reflection and the identifica- tion of remediation actions. This paper presents a naturalistic case study to explore how students use a dashboard displaying infor- mation about their engagement with preparatory activities in a first year engineering course used a flipped learning strategy.  2. RELATED WORK The increasing presence of Learning Management Systems (LMS)  as mediating platforms in learning experiences has position them as one of the most commonly used data sources for analytics [3, 1, 11]. Numerous studies have been presented in the area of learn- ing analytics in which the data recorded by LMSs is used to cre- ate reports, visualizations, recommendations, etc. [7, 20, 24]. One of the most common problems tackled with LMS data is the de- tection of students at risk of abandoning a course, the institution, or under-performing. The first systems took basic indicators from LMSs and explored the correlation between them and the course outcomes [21]. Macfadyen and Dawson presented a comprehen- sive analysis of which factors derived from LMS interactions had a strongest correlation with academic performance [18]. Goggins et al. expanded the data sources to include social interactions among students in the contexts requiring group work and formal collabo- ration [10]. The value of these systems is in identifying students at risk so they can receive support to reduce such risk. Although most of the studies were in the context of online learning, simi- lar analyses were also proposed in face-to-face or blended learning environments with similar results [22].  Other systems focus on visualizations to increase either student or instructor awareness [5, 28]. These visualizations can be graph- ical representations of discussion forum activities, social interac- tion, or the relation between scores and estimated cognitive activ- ities [25, 19, 4]. The underlying assumption is that visualization help users increase their insight about the learning process. The immediacy of these visualizations is also a relevant aspect influ- encing their effectiveness [17]. The systems in this space can be divided into two categories: those providing information directly to students and those in which the information is made available to an intermediary [16]. Several systems have been proposed to    support this mediated provision of feedback. For example, the Sig- nals project allows instructors to run a set of pre-defined procedures to process student scores throughout the semester and generate a traffic light conveying the estimated risk of failing the course to students [27]. Other systems offer instructors sophisticated mech- anisms to collect data from the students and dispatch highly per- sonalized actions [17, 30, 16]. Kay and Bull [2] proposed the use of open student models to manage student information captured in a learning experience. Fritz proposed a tool offering students a dashboard derived from LMS data [9]. However, the metrics were course-agnostic factors such as the number of visits to the platform or the number of clicks in the course pages. In a contribution closes to the one described in this paper, Gunn and Bushway [12] explored the effect of providing students a map of their competency-based assessment. They observed a slight increase in academic perfor- mance and a significant increase in registration levels in adult edu- cation. Reimers and Neovesky [23] published a review of the state of the art in student dashboards and a study about the type of infor- mation that students would find more valuable.  From a pedagogical point of view, feedback has been shown to have a positive effect on students [13, 14]. However, serious bar- riers appear when trying to scale techniques for large student co- horts. At the same time, pedagogical strategies based on active learning are gaining momentum as the body of evidence showing their effectiveness is growing [8, 29]. But active learning methods rely heavily on frequent feedback to maintain students engaged and productive [6].  This paper is positioned at the intersection of these three spaces: the increasing presence of active learning methodologies, the in- creasing requirements for feedback in these contexts, an the use of learning analytics to report pedagogically meaningful indicators to students in real time.  3. CASE STUDY  3.1 Design The case study collected data about student interactions with the  online resources available in a first-year engineering course de- ployed in a flipped and blended learning context at an Australian higher education institution. The events were captured and used to produce a set of low-level indicators, their values were aggregated and made available through the dashboard shown in Figure 1 pro- vided as an additional resource in the online platform (a link in the template used for every course page). The system was deployed in a naturalistic context during the course and the anonymized data was analyzed after the course was finished.  3.2 Data Sources The following data sources were used in the study:   Interaction with video clips embedded in the course notes.   Answers to formative assessment questions next to the video clips and embedded in the course notes.   Answers to a summative assessment sequence of exercises.   Access to the any course resource.   Access to the dashboard.  The information provided by the dashboard is produced by com- bining data from other sources. It includes four indicators about the level of engagement with videos, formative questions next to the videos, formative question in the course notes, and a sequence  Figure 1: Example of Dashboard with Real Time Feedback  of summative exercises. Each indicator is represented as a percent- age and two values are shown: the individual score and the average with respect to the cohort.  3.3 Sample The course had an enrollment of 290 students. A total of 224  students used the dashboard at some point during the course (N = 224). The course lasted for 13 weeks and was designed following a flipped learning strategy. Every week students were required to attend a 2 hour lecture, a 2 hour tutorial, and a 3 hour laboratory sessions. The total workload was estimated at 11 hours per week. Students had to prepare the lecture and tutorial sessions with online activities. The information provided in the dashboard was aggre- gated from the engagement with these preparation activities that included summative assessments accounting for 20% of the course final mark.  The dashboard was available to the students through a link in the course pages, updated every 15 minutes, and the values reset at the start of every week. Students could reviewing the data reported in the preceding weeks.  3.4 Variables The interactions with any online resource, videos, multiple choice  questions and exercise sequences were recorded as events. Every event was labeled with its type, time, date, user, and contextual in- formation describing the action recorded. For the purpose of the study the events were divided into two groups: Dashboard Events and Regular Events. Dashboard events are recorded when a student accesses the dashboard. Regular events include the rest of events occurring in the environment.  The date and time for each event was used to extract informa- tion about sessions. We assumed that a study session was defined as a continuous sequence of events with separation below 30 min- utes. Although there are several heuristics to calculate these ses- sions [15], we selected this method based on the type of activities that were part of the course. If the separation between two events from the same user was larger than 30 minutes, the first event was marked as the end of the previous session, and the second event marked as the start of the next session.  The length of a session was calculated as the difference in min- utes between the first and last event. Additionally, for each session and student, a variable was calculated with the number of dash- board views per session and the number of events before and after the first dashboard view.    3.5 Data Analysis Four data analysis were carried out. The first one used a K-means  clustering algorithm to explore if the students could be clustered based on the number dashboard accesses during the course. The elbow method was used to detect the optimum number of clusters. The second calculation also used K-means but based on when stu- dents accessed the dashboard during a study session. For this anal- ysis a new variable was introduced with the number of events in a session before and after the first access to the dashboard. The third analysis explored how the pattern of engagement with the dash- board evolved throughout the semester. The number of events be- fore and after the interaction with the dashboard for each session and week were calculated. The clustering results obtained in the previous analysis were then used to label student interactions and obtain the changes in the dashboard use. The last study explored the relationship between dashboard use and the scores obtained in the midterm examination.  4. RESULTS AND DISCUSSION The descriptive statistics for the variables in the study are shown  in Table 1.  Table 1: Descriptive Statistics of Variables  Variable name Mean (SD) Avg Session Length (min) 18.03 (33.82) Total Sessions 82.71 (37.72) Avg Events Per Session 29.45 (13.47) Total Events 2364 (1228) Avg Dashboard View Per Session 0.31 (0.44) Total Dashboard Views 26.10 (38.23) dbrd.events.bfr 23.23 (16.31) dbrd.events.aftr 25.29 (21.85)  Several variables such as the average session length (in minutes) or the average events per session have high standard deviations.  4.1 Access to the Dashboard The first study aims at characterizing students based on the num-  ber of times they access the dashboard. We used the number of dashboard events per user and the K-means algorithm to calculate the clusters. The elbow method showed an optimum of 4 clusters with a quotient between the Sum of Square in Between (SSB) and the Total Sum of Squares (SST) of SSB/SST = 92.5% denoting a clear differentiation among the clusters.  Table 2 shows the cluster centers and the percentage of students in each cluster. The clusters have been labeled Low Engagement (LE), Medium Engagement (ME), High Engagement (HE) and Ex- treme Engagement (EE). The LE cluster contains most of the co- hort and they viewed the dashboard once every 10 study sessions. The ME cluster contains students who viewed the dashboard ev- ery 2 sessions. Almost 10% of the students access the dashboard once per study session and there is a small percentage of students checking the dashboard more than once per session  Given the structure of the course, only a subset of activities affect the values shown in the dashboard. For this reason, the LE cluster seems to contain the expected behavior whereas the access rate for the HE and EE clusters is unusually high. This result provides a characterization based on the number of accesses. A more nuanced analysis is needed to distinguish the type of usage.  4.2 Dashboard Interaction Styles  Table 2: Cluster Centers and Student Proportions  Cluster Proportion Dashboard Views Per Session LE 64.57% 0.143 ME 24.21% 0.595 HE 9.41% 1.213 EE 1.79% 2.615 SSB / SST = 92.5% N = 224  The second study aimed at gaining a deeper understanding of how students interacted with the dashboard. For each student we selected the sessions that contained at least one dashboard view and then calculated the number of regular events before and after the first dashboard access.  The K-Means algorithm was also used for this analysis and, as in the previous case, an optimum number of clusters of 4 was found using the elbow method. In this case we obtained the quality metric SSB/SST = 84.7%, lower than in the previous case, but still denoting a robust differentiation among clusters.  Table 3 shows the cluster centers. Based on the number of events before and after a dashboard view, the clusters were labeled as:  1. Dashboard View in the middle of a study session (DBM)  2. Dashboard View in the beginning of a study session (DBB)  3. Dashboard View in the middle of a long study session (DBML)  4. Dashboard View near the end of a study session (DBE)  Table 3: Dashboard Styles and Proportions  Cluster Proportion events.bfr events.aftr Middle (DBM) 38.91% 12.33 10.52 End (DBE) 21.72% 44.62 14.72 Beginning (DBB) 28.99% 19.39 35.86 Mid. Long (DBML) 10.41% 30.06 73.13 SSB / SST = 84.6%, N = 221  The distribution of students in these clusters is more balanced that in the previous analysis. The largest cluster, DBM, contains those students that viewed the dashboard right in the middle of their study session. A possible interpretation is that the dashboard provided students in this cluster information consulted after cer- tain steps in a study session. The second largest cluster, DBB, may contain students that the need to know in advance where they are positioned with respect to the activities requested in a week. The DBML cluster is very similar to DBB except that the students tended to have longer study sessions. The two clusters (DBB and DBML) account for almost 40% of students pointing to the most common use of the dashboard. The smaller cluster, DBE, contains students that access the dashboard towards the end of the study ses- sion. These students may use the dashboard only to verify that it is at the expected levels. The interpretation of these results provides a preliminary description of how students interact with the dash- board. A more comprehensive data collection with qualitative data derived from interviews would be required to verify these assump- tions.  4.3 Interaction Styles over the Semester The dashboard provides feedback about the engagement for one  week in the course. Every seven days all indicators are reset to zero. This feature was decided to provide students with a more    Figure 2: Evolution of Dashboard Interaction Styles over 12 Weeks  accurate notion of their sustained effort. In this context we wanted to explore if the use of the dashboard changed over the duration of the semester. This part of the study was conducted based on the clusters obtained in the previous section. The events were divided into 12 regions, one for each week in the semester (weeks 1 and 6 were discarded as there were no preparation activities scheduled).  For every week and student, we extracted all sessions contain- ing dashboard views. We then calculated the events before and after each dashboard views as in the second analysis and averaged all sessions for each student. The K-Means model created in the previous section was used to predict the category of the dashboard interaction for every week of the semester. Figure 2 shows the evo- lution of the number of students in each cluster over the 12 week period.  The DBM cluster is the predominant type of interaction in almost every week of the course. This means that most of the students sys- tematically used the dashboard as a reference in the middle of their work sessions. The evolution of the DBB cluster shows an interest- ing pattern. After a large number of students being in this category in the first and second weeks, the size of the cluster has a very sig- nificant reduction. The evolution of the number of students in the DBE cluster is almost the opposite. The first two weeks the cluster has a reduced number of students, but then it grows until it becomes the second most populated cluster. A possible explanation for this trajectory is because the course was a first-year course students needed to monitor their engagement to adapt to the new context. As the weeks went by students only needed the dashboard towards the end of their study session to verify that they were achieving the expected levels of engagement. A more detailed study would be required to establish the causality of this relation.  The DBML cluster starts with a modest number of students and remains with a minority of members throughout the semester. A user is included in this cluster when the dashboard is in the middle of a session, but the session has a large duration. If we consider both the DBM and DBML clusters together we can conclude that as the semester advances more students use the dashboard in the middle of their study sessions.  Aside from the evolution of the size of the clusters over time, the figure also shows a decrease on the number of accesses to the dashboard as the semester advances. This seems to reinforce the hypothesis that as the semester advances students do not need to  consult the resource as often. An increase in familiarity with the required tasks may cause a less frequent access to the feedback about the level of engagement.  4.4 Dashboard Use and Midterm Scores The final study explored the relation between the use of the dash-  board and the scores obtained in the midterm examination held in Week 6 of the semester and worth 20% of the total course marks. A model was derived using linear regression between the scores and the average dashboard views (ADVS). The linear coefficients had no statistical significance and R2 = 0.002 suggesting that the number of dashboard views are not associated with the midterm score.  5. CONCLUSIONS The wide variety of data being collected through the use of tech-  nology in learning scenarios provides an unprecedented opportu- nity to improve the quality of the student experience. Numerous initiatives focus on discovering knowledge to be used by the in- structor to provide student support. Using analytics to provide real-time feedback to students is a much less explored area. Real- time feedback becomes more relevant for learning experiences that require frequent student engagement. Active learning or flipped learning are strategies that require students to interact frequently with the learning environment. This paper has presented a study case to explore student behavior in the context of a flipped learn- ing course in which students are given indicators of their weekly engagement through a dashboard integrated in the online platform. The results identified four clearly delimited cluster of users that used the dashboard differently to observe their level of engagement with respect to a reference initially defined by the instructor. Addi- tionally, students change the use of the dashboard as they progress through the learning experience suggesting a process of adaptation to the course requirements in terms of participation. No statistically significant relation has been found between the use of the dash- board and academic performance.  These initial results point to various avenues for further explo- ration. The most important is the nee to complement the quanti- tative data with qualitative information derived from student inter- views. A more detailed description of how students perceive the    data could deepen the understanding for the changes in the use of the dashboard and its connection with learning approaches. Addi- tionally, using different versions of the dashboard can help to iden- tify the most valuable format to present the information to students. Further improvements could be obtained by analyzing the data in relation with the type of activities in the course. Detecting varia- tions in the access to the dashboard depending on the type of ac- tivity could provide a more precise characterization of its use and even inform the learning design process.  6. REFERENCES [1] P. Baepler and C. J. Murdoch. Academic Analytics and Data  Mining in Higher Education. International Journal for the Scholarship of Teaching and Learning, 4(2), 2010.  [2] S. Bull and J. Kay. Student Models that Invite the Learner In : The SMILI:() Open Learner Modelling Framework. International Journal of Artificial Intelligence in Education, 17(2):89120, 2007.  [3] J. Campbell, P. DeBlois, and D. Oblinger. Academic Analytics. Technical report, EDUCAUSE White Paper, 2007.  [4] S. Dawson, A. Bakharia, and E. Heathcote. SNAPP: Realising the affordances of real-time SNA within networked learning environments. In L. Dirckinck-Holmfeld, V. Hodgson, C. Jones, M. D. Laat, D. McConnell, and T. Ryberg, editors, International Conference on Networked Learning, pages 125133, 2010.  [5] E. Duval. Attention please!: learning analytics for visualization and recommendation. In International Conference on Learning Analytics, pages 917. ACM Press, 2011.  [6] R. M. Felder and R. Brent. Active Learning: An Introduction. ASQ Higher Education, 2(August), 2009.  [7] R. Ferguson. The State of Learning Analytics in 2012: A Review and Future Challenges a review and future challenges. Technical report, The Open University UK, 2012.  [8] S. Freeman, S. L. Eddy, M. McDonough, M. K. Smith, N. Okoroafor, H. Jordt, and M. P. Wenderoth. Active learning increases student performance in science, engineering, and mathematics. Proceedings of the National Academy of Sciences of the United States of America, pages 16, May 2014.  [9] J. Fritz. Classroom walls that talk: Using online course activity data of successful students to raise self-awareness of underperforming peers. The Internet and Higher Education, 14(2):8997, Mar. 2011.  [10] S. P. Goggins, K. Galyen, and J. Laffey. Network Analysis of Trace Data for the Support of Group Work: Activity Patterns in a Completely Online Course. ACM Press, 2010.  [11] P. Goldstein and R. Katz. Academic analytics: The uses of management information and technology in higher education, volume 8. Educause Center for Applied Research, 2005.  [12] J. Grann and D. Bushway. Competency Map: Visualizing Student Learning to Promote Student Success. In Proceedings of the Fourth International Conference on Learning Analytics And Knowledge, LAK 14, pages 168172, New York, NY, USA, 2014. ACM.  [13] J. Hattie and H. Timperley. The Power of Feedback. Review of Educational Research, 77(1):81112, Mar. 2007.  [14] J. A. C. Hattie. Visible learning: A synthesis of over 800 meta-analyses related to achievement. Routledge, New York,  2008. [15] V. Kovanovi, D. Gaevic, S. Dawson, S. Joksimovic, R. S.  Baker, and M. Hatala. Penetrating the Black Box of Time-on-task Estimation Categories and Subject Descriptors. In Proceedings of the International Conference on Learning Analytics and Knowledge. ACM Press, 2015.  [16] A. E. Krumm, R. J. Waddington, S. D. Teasley, and S. Lonn. A Learning Management Sytem-Based Early Warning Sytem for Academic Advising in Undergraduate Engineering. In Learning Analytics. From Research to Practice, pages 103119. Springer New York, 2014.  [17] S. Lonn, A. E. Krumm, R. J. Waddington, and S. D. Teasley. Bridging the Gap from Knowledge to Action : Putting Analytics in the Hands of Academic Advisors. In International Conference on Learning Analytics and Knowledge, pages 184187. ACM Press, 2012.  [18] L. P. Macfadyen and S. Dawson. Mining LMS data to develop an early warning system for educators: A proof of concept. Computers & Education, 54(2):588599, Feb. 2010.  [19] R. Mazza and V. Dimitrova. CourseVis: A graphical student monitoring tool for supporting instructors in web-based distance courses. International Journal of Human-Computer Studies, 65(2):125139, Feb. 2007.  [20] R. Mazza and C. Milani. Gismo: a graphical interactive student monitoring tool for course management systems. TEL04 Technology Enhanced Learning04 . . . , 2004.  [21] L. V. Morris, C. Finnegan, and S.-S. Wu. Tracking student behavior, persistence, and achievenet in online courses. The Internet and Higher Education, 8:221231, 2005.  [22] A. Pardo and C. Delgado Kloos. Stepping out of the box. Towards analytics outside the Learning Management System. In International Conference on Learning Analytics, pages 163167. ACM New York, USA, 2011.  [23] G. Reimers and A. Neovesky. Student Focused Dashboards. An Analysis of Current Student Dashboards and What Students Really Want. In International Conference on Computer Supported Education, 2015.  [24] C. Romero and S. Ventura. Educational data mining: a review of the state of the art. IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews, 40(6):601618, 2010.  [25] J. Saltz, R. Hiltz, and M. Turoff. Student social graphs: Visualizing a students online social network. Nov. 2004.  [26] G. Siemens and P. Long. Penetrating the Fog: Analytics in Learning and Education. Educause Review, 48(5):3140, 2011.  [27] Z. Tanes, K. E. Arnold, A. S. King, and M. A. Remnet. Using Signals for appropriate feedback: Perceptions and practices. Computers & Education, 57(4):24142422, Dec. 2011.  [28] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. L. Santos. Learning Analytics Dashboard Applications. American Behavioral Scientist, (February), Feb. 2013.  [29] C. E. Wieman. Large-scale comparison of science teaching methods sends clear message. Proceedings of the National Academy of Sciences of the United States of America, 111(23):831920, June 2014.  [30] M. Wright, T. McKay, C. Hershock, K. Miller, and J. Tritz. Better Than Expected: Using Learnign Analytics to Promote Student Success in Gateway Science. Change: The Magazine  of Higher Learning, 46(1):2834, 2014.    "}
{"index":{"_id":"33"}}
{"datatype":"inproceedings","key":"Ruiz:2016:SLC:2883851.2883888","author":"Ruiz, Samara and Charleer, Sven and Urretavizcaya, Maite and Klerkx, Joris and Fern'andez-Castro, Isabel and Duval, Erik","title":"Supporting Learning by Considering Emotions: Tracking and Visualization a Case Study","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"254--263","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883888","doi":"10.1145/2883851.2883888","acmid":"2883888","publisher":"ACM","address":"New York, NY, USA","keywords":"face to face interactions, quantified-self, self-reflection, students' emotions, visual dashboards, visualization","abstract":"The adequate emotional state of students has proved to be essential for favoring learning. This paper explores the possibility of obtaining students' feedback about the emotions they feel in class in order to discover potential emotion patterns that might indicate learning fails. This paper presents a visual dashboard that allows students to track their emotions and follow up on their evolution during the course. We have compiled the principal classroom related emotions and developed a two-phase inquiry process to: verify the possibility to measure students' emotions in classroom; discover how emotions can be displayed to promote self-reflection; and confirm the impact of emotions on learning performance. Our results suggest that students' emotions in class are related to evaluation marks. This shows that early information about students' emotions can be useful for teachers and students to improve classroom results and learning outcomes.","pdf":"Supporting learning by considering emotions:  Tracking and Visualization. A case study      Samara Ruiz  Department of Languages and   Computer Systems  University of the Basque Country   San Sebastin, Spain  samara.ruiz@ehu.eus     Joris Klerkx   Department of Computer Science  KU Leuven   Leuven, Belgium  joris.klerkx@kuleuven.be   Sven Charleer  Department of Computer Science   KU Leuven  Leuven, Belgium   sven.charleer@kuleuven.be     Isabel Fernndez-Castro  Department of Languages and   Computer Systems  University of the Basque Country   San Sebastin, Spain  isabel.fernandez@ehu.eus      Maite Urretavizcaya  Department of Languages and   Computer Systems  University of the Basque Country   San Sebastin, Spain  maite.urretavizcaya@ehu.eus      Erik Duval  Department of Computer Science   KU Leuven  Leuven, Belgium   erik.duval@kuleuven.be    ABSTRACT  The adequate emotional state of students has proved to be  essential for favoring learning. This paper explores the  possibility of obtaining students feedback about the emotions  they feel in class in order to discover potential emotion patterns  that might indicate learning fails. This paper presents a visual  dashboard that allows students to track their emotions and  follow up on their evolution during the course. We have  compiled the principal classroom related emotions and  developed a two-phase inquiry process to: verify the possibility  to measure students emotions in classroom; discover how  emotions can be displayed to promote self-reflection; and  confirm the impact of emotions on learning performance. Our  results suggest that students emotions in class are related to  evaluation marks. This shows that early information about  students emotions can be useful for teachers and students to  improve classroom results and learning outcomes.     Categories and Subject Descriptors  Human-centered computing  Visualization   Visualization systems and tolos; Applied computing   Education  Interactive learning environments;    General Terms  Measurement, Experimentation, Human Factors.   Keywords  Self-reflection, quantified-self, students emotions, face to face   interactions, visual dashboards, visualization.   1. INTRODUCTION  Current studies about the teaching learning flow in traditional  classes are immersed in student-centered theories that emphasize  the student role as the principal actor in her own learning  [Hannafin and Land 1997]. The students interactions with their  teacher in classroom, or online in distance-learning scenarios,  became the key for measuring learning progress. The  PresenceClick system [19] [20] records and processes the  interactions arising in traditional learning sessions between  teachers and students in order to provide timely feedback.  Teachers become aware of the knowledge status and other  characteristics of their students, which allows them to adapt their  teaching (e.g increase or decrease learning pace). Similarly,  students can monitor their own progress and that of the group,  which can trigger a reflection process that leads to learning  improvements. PresenceClick is composed of various modules  to capture the interactions that happen in class, including  attendance to class, students doubts and answers to teachers  questions, and many more.   Even though emotions are not interactions by themselves, they  can be one of the most influential factor in the way students  interact. A good atmosphere and a positive learning environment   in classrooms motivates students and leads them to more  effective learning, whereas negative emotions, such as fear and  stress, can potentially disrupt learning [7]. Besides influencing  learning, the ability to regulate emotions is a proven predictor of  academic outcomes. Students who can effectively regulate their  emotions are more resilient in overcoming failure [7].   According to literature, learning analytics applications support a  process model that drives teachers and students through four  stages of tracking and visualizing learner activities: (self- )awareness, (self-)reflection, sense making and impact [21].  Assuming that emotions and their regulations influence different  aspects of learning, we investigate if a system that is able to  capture and show the principal emotions that students feel in  class will help them during the various stages of this process  model. This paper presents our design process and evaluation of   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883888     our visual dashboard that allows emotion tracking and creates an  actionable feedback loop for students and teachers in order to  improve the face to face learning environment. Through our  learning analytics dashboard, we aim to answer the following  research issues:   A) Is it possible to adequately measure emotions that  students feel in the classroom  B) How can students emotions be visualized to promote  self-reflection  C) What is the real impact of tracking and visualizing  emotions on teaching-learning contexts   With these questions in mind, the goal of our dashboard is two- fold. First of all, involving students in a self-regulation process  will allow them to gain knowledge about the emotion patterns of  themselves and their peers. This could help them to regulate  their emotions and therefore improve their progress in class [7].  Secondly, making the teacher aware of the emotional climate in  class will allow her to detect problems early on and potentially  adapt the class pace.   Our dashboard has been evaluated in two stages. First, we made  an exploratory analysis with 15 students during a six-week  course by self-report, that is the most common and potentially  the best way to measure a persons emotional experiences [18].  The results of the analysis lead us to extend the PresenceClick  environment by integrating the verified model into its new  EmotionModule. The second evaluation stage was carried out on  the extended system in two phases during two semesters in two  subjects (with 97 and 81 students respectively).  The remainder of this paper is structured as follows. Section 2  presents so-called quantified-self apps that aim to track emotions  for self-reflection, and our own evaluated emotion model (TEA)  that was used to conceptualize students emotions. Section 3  presents the design process and evaluation of our solution.  Finally, Section 4 discusses our findings and conclusions.   2. QUANTIFYING EMOTIONS  This section presents the work related to quantify emotions and  presents the model that we propound for tracking students  emotions.   2.1 Background  The so-called quantified-self apps are steadily gaining an  important space in our daily routine [15] [16]. These apps cover  different life aspects, such as health, sport, travelling or learning,  and help people collect personal data about their own behaviors,  habits and thoughts. Many of them are focused on tracking  emotions with the aim of involving the user in self-monitoring  and self-reflection processes to regulate different aspects of their  own life.  Several web-based systems track user emotions and mood  through different techniques, such as self-report [13], selection  of colors to express mood [10] or by analyzing the raw vocal  intonations in real-time [2]. Many other applications that  quantify emotions are designed to be used on the go and run on  mobile phones, such as, MoodPanda [12] or In Flow [8]. In the  same way, the LIM app measures the interest of the audience in  a lecture [17] and MoodMap enables users to note and review  his or her own mood over time, and to obtain an insight about  team mood [4]. The majority of these apps represent the users  mood through a numeric scale, with just one or two dimensions  of emotions (happiness, interest, positive-negative mood,   activity level, etc.). In addition they mainly focus on general  emotions that are not necessarily related to learning. In this field,  learners emotions have been widely studied, since the well- known Ekman model of facial expressions [3] used in multiple  tutoring systems, to the model provided by Pekrun and  colleagues through the Achievement Emotions Questionnaire  (AEQ) [14], broadly used in educational emotion research.   2.2 The TEA Model  We propose a Model of Emotions in an academic context that  combines and refines the models provided by Pekrun and  colleagues [14] and Arroyo and colleague [1], which is based in  turn in Ekmans model [3]. The formers findings meaningfully  relate detected emotions to students learning and performance.  They identify eight different emotions related to the classroom  environment (enjoyment, hope, pride, anger, anxiety, shame,  hopelessness and boredom). The authors also set out a new  research question: what can we do to foster positive academic  emotions and to help students avoid negative emotions, or to  cope with negative emotions in a flexible way once they emerge  We will try to answer this question by tracking emotions  through self reflection. The work by Arroyo and colleagues  considers that emotions interest, frustration, excitement and  confidence have an educational component related to a real  classroom.   In this paper we present a possible answer to research question  A by using a questionnaire to promote individual introspection  based on the combination of both models. This blend gives rise  to the Model that we have defined as the Twelve Emotions in  Academia, the TEA Model, with six positive emotions  (enjoyment, hope, pride, confidence, excitement and interest)  and six negative ones (anxiety, anger, shame, hopelessness,  boredom and frustration). All together they define the  positive/negative emotion spectrum (see Figure 1).         3. EXPERIMENTS, METHODS &  DISCUSSION  This section presents the experiment carried out on the TEA  Model (TEAM) to answer the research questions presented in  section 1 (A, B, C). The experiment was divided in two phases:  (1) exploratory analysis to validate both the proposed TEAM  and the visualization provided to students; and (2) the extension  of the PresenceClick system with an Emotion Module to let  students visualize their emotions easily. The PresenceClick  system streamlines the capture of students attendance to class  and allows teachers and students to register their classroom   Figure 1: The Twelve Emotions in Academia Model     interactions expeditiously in order to improve their knowledge  about the teaching-learning process [19] [20].   3.1 Phase 1: Exploratory analysis  The objective of this phase was to respond to research questions  A and B. Therefore, we studied two issues: whether the TEAM  emotions are understandable and quantifiable by students, and  whether the visualizations utilized are clear and useful to help  students to reflect on their emotions. Finally, we partially  tackled question C by using the students point of view to check  whether tracking emotions has an impact on his or her learning.  The evaluation of this phase produced some ideas for  improvement that were applied in Phase 2.   3.1.1 Context and participants  This analysis was held in the context of an optional subject  about Multimedia at the Computer Science Faculty of the  University of Leuven (Belgium). One teacher, one observer and  15 students were involved, and the process lasted two months.   3.1.2 Instruments  We adapted the AEQ schema to track students emotions for the  proposed TEA Model (see section 2.2). The questionnaire was  going to be used frequently during the course and, to avoid  students tedium, we extracted the most representative item from  the 5-6 AEQ items for each emotion (Emotion Items  EI); we  also adapted the selected items to the contexts before class/after  class. Equivalent items were included for the TEAM emotions  that were not included in AEQ. For example, the items for  enjoyment were: I enjoy being in class or I enjoy working in  the subject activities during the week. Two more groups of  items were also included to answer research question A and to  learn about students opinions about quantifying emotions.  Firstly, students were asked to what extent each emotion  influenced their learning (Influence Items  II; i.e.: The more  interest I have in class, the better is my learning). Secondly,  students were asked about their certainty when assessing their  own emotions in class (Confidence Items  CI; i.e.: I can  measure my anxiety grade in class with certainty). The result is  an adaptable TEAM Questionnaire (TEAMQuest) using  different group of items (EI, II or CI) depending on the purpose  of the experiment in each moment. The TEAMQuest was  designed with GoogleDocs technology in a 6-Likert scale, to  allow students to evaluate each sentence in a simple way from  completely disagree to completely agree.   Some prototype visualizations were created to show students  their emotions in order to favor reflection and to respond to  research question B (Figure 2 and Figure 3). The intention of  these visualizations was to involve students into this process and  not to show real numbers (as we can see the figures). Each  emotion was represented consistently by one color during all the  experiment; the selected colors are represented in Figure 1. Two  visualization schemas were used: typical plain graphs  visualization (Vg)-bubbles, stacked bars and boxplots- and a  more innovative visualization based on squares (Vs). The bubble  chart in Figure 2 (a) shows the students timeline evolution for  each emotion: rows represent the answers to the TEAMQuest in  one session (before or after class) and columns represent  emotions. The bubble size corresponds to the students valuation  for the emotion in one session, so the bigger the bubble the more  intense the emotion. The stacked bars Figure 2 (b) represent the  average value of every students emotion for all sessions in  contrast to the average values of the group. This visualization  schema allows comparing the general positive/negative balance   of the student to the one of the group, and contrasting the  emotions of the student with each other. Finally, the boxplot  chart Figure 2 (c) shows the timeline evolution of the group and  the comparison to the students emotions. Boxplots display the  emotion/session group distribution and black lines represent the  evolution of the students emotion through the different sessions.  By means of this graph the student can easily identify if there is  any difference between his or her feelings and those of the  group. Regarding own emotions, for example, it is easy to see  that the anxiety varies a lot through the weeks while interest  remains the same. In addition, as [10] states, time-based  visualizations allow the instructor to analyze the changes of each  emotion during the term of the class.       Figure 2: Visualization of typical graphs for one specific  student (Vg)   Figure 3: Visualization of the squares schema for a specific  student (Vs)          The squares schema (Figure 3) shows the positive/negative  emotion balance individually and by group. Each row displays  the results for a questionnaire, and the elements inside represent  the set of results for all participant students, ordered from the  lowest positive balance to the highest. Each large square is  composed of smaller light/dark squares to indicate  positive/negative emotions. A completely light large square  means the student is very positive, and the more dark it is the  more negative he or she is. The first column of the schema  represents the group average balance for each session, and the  most intense square in each row represents the student who is  using the system. So, the student can recognize himself or  herself in the ordered set of emotions balance and easily analyze  his or her evolution during the course in relation to the group.  For example, for the student in Figure 3, we can see that as the  course advanced, his/her emotions dropped considerably for  some weeks in comparison to the group.   The evaluation of the experiment was carried out following the  guidelines of [9], which depend on the specific evaluation  context of a visualization system. In order to evaluate their  usability & utility, and to discover the students opinion and  willingness to track their emotions, we developed a three-step  evaluation process that included: Satisfaction Questionnaire  (SQ), review of the students accesses in the system (A) and  Interviews to participants (I). The Satisfaction Questionnaire  was also created in GoogleDocs with a 6-Likert scale questions  (e.g.: Being aware of my emotions influences my behaviour in  class; Visualization 1 is easy to understand). To obtain a more  exact measure of students preferences, a question was included  to ask them to distribute 20 points among all the proposed  visualizations. Three open questions allowed students to give  their opinion about visualizations, lack of information and  whatever issue related to emotion tracking. The study of the  accesses in the system was done by checking the logs stored in  GoogleAnalytics. Finally, interviews included 8 questions for  students to confirm the data provided in the Satisfaction  Questionnaire (e.g.: Did you have any problem understanding  the graphs Do you think the chosen colors were suitable).  Table 1 presents a summary of the instruments.   Table 1: Instruments summary   Instruments   Adaptable TEAMQuest  Emotion Items (EI)  Influence Items (II)  Confidence Items (CI)   Visualizations  Bubbles, stacked bars and  boxplots (Vg)  Squares schema (Vs)   Evaluation  Satisfaction Questionnaire (SQ)  Accesses in the system (A)  Interviews (I)     3.1.3 Procedure   During five weeks (see Table 2) in which students attended just  one classroom per week, we asked students to fill out the  TEAMQuest twice: before the class to reflect the emotions they  felt when they were working on the activities of the subject  during the previous week to the class (TEAMb1 or TEAMb2)  and after the class to reflect on the emotions in that session  (TEAMa). Although the questionnaire was anonymous, we  asked students to use a unique fictitious ID during the evaluation  process in order to discover their data evolution during the  course.   All the instances of the questionnaire were composed by the  Emotions Items (EI) according to the context before/after class.  In addition, the first time we passed out the questionnaire  (TEAMb1), we also included the questions about the influence  of emotions in their learning (II) and their confidence in their  answers (CI). One week later (in Class2), we repeated the  experiment in order to verify the certainty of the answers.   In the 3rd and 4th class of the experiment, students could  visualize some partial results (Vg) to increase their motivation  and avoid withdrawals. From the 5th class on, each student  could use a simple web prototype to visualize the evolution of  her emotions (Vg and Vs). The visualizations were always  personal and each student could only accessed to his/her results  by means of the id indicated on the questionnaires. At the end of  the experiment (6th class), and after accessing the prototype,  they were asked to answer a satisfaction questionnaire (SQ)  about the process of tracking emotions and the usefulness and  usability of the visualizations. After the last class and during the  next week some interviews (I) took place, as well as the system  access study (A).   Table 2: Procedure summary    TEMQuest Visualizations Evaluation   C la  ss 1 Before TEAMb1     After TEAMa    C  la ss  2 Before TEAMb1     After TEAMa     C la  ss 3 Before TEAMb2 Vg    After TEAMa     C la  ss 4 Before TEAMb2 Vg    After TEAMa     C la  ss 5 Before TEAMb2 Vg Vs    After TEAMa     C la  ss 6 Before  Vg Vs SQ   After   A,I   TEAMb  TEAMQuest with Emotion Items before class; TEAMa   TEAMQuest with Emotion Items after class; Vg: typical  graphs Visualization; Vs: Squares schema Visualization; SQ:  Satisfaction Questionnaire; A: Access study; I: Interviews;     3.1.4 Results and discussion  During the first two weeks, two before-class questionnaires  recorded the students opinion about tracking emotions by  means of the items of Influence in Learning (II) and Confidence  (CI). 15 out of 15 students answered the first questionnaire and  13 the second. Regarding Influence in Learning, Figure 4   presents the answers to II-Class1 and II-Class2. Blue boxplots in  the left part represent the positive emotions while the brown  colors represent the negative ones. Light colors correspond to  the first class and dark ones are related to the second. Except for  shame in Class1 (SHA in Figure 4), students agreed (4 or larger)  that these emotions influenced their learning. It should be noted,  that the second time they answered this question (dark boxplots),     they thought more about almost all emotions influencing  learning. Therefore, we concluded that the more aware of these  emotions they were, the more they thought they influenced their  learning.      Regarding confidence they opined they could assess their  emotions in class for each emotion (e.g.: I think I can reliably  measure my anxiety in class). In the second class, the average  value for all emotions was 4 or bigger (Figure 5). In addition, we  found correlations between the responses in how reliable do  you think you can quantify your emotions and the influence  they thought each emotion had in their learning. So, the more  importance they gave to an emotion related to their learning, the  more they believed they could evaluate/quantify it.        The significantly good results obtained for the items II and CI in  Class1 and Class2 confirmed the influence of emotions on  learning and the students confidence on his or her ability to  assess emotions, we therefore consider the proposed TEA Model  as an adequate proposal to register students emotions. We also  concluded that students could measure their emotions by means  of the TEAM Questionnaire, which responds positively to the  research question A, i.e. is it possible to measure the emotions  that students feel in classroom   The visualizations provided (research question B) allowed the  teacher to conclude that the emotional climate of the classroom  was positive because the distribution for positive emotions  obtained high values and those for negative ones were low for  all the questionnaires (see group emotions Figure 2 and Figure  3). Thus, he or she could deduce his or her students were  comfortable and engaged in the subject. The visualizations  allowed students to think about their emotions and compare  them to those of the rest of the group, so changes on students  behaviour could be expected from the conclusions individually  obtained. For example, the emotion results after the first class   (EIa in Class1) for the student represented in Figure 2 were  mainly negatives (see rectangle selections): level of frustration  high, highest level of shame and, in general, the positive  emotions had the lowest values during the course. After that  session her emotions improved, a possible cause might have  been an awkward assignment, such as a public presentation  carried out in that session. By watching her emotion  visualizations and comparing them to the group, she could also  deduce the public presentation positively impacted her learning  emotions. Being aware of this fact could help her to increase  control over her emotions and to improve her learning processes;  nonetheless, the impact and the scope of the introspection  processes will depend on the students individual characteristics.   Figure 6 shows the students opinion about the two provided  visualizations, as it was reflected on the Satisfaction  Questionnaire. Most of the students thought both visualizations  were easy to understand (first two boxplots), and the average  value was positive for both of them. They also considered both  visualizations helped them to be aware of their own emotions  and those of the group; just 1 student disagreed for Vg and 2 for  Vs in the individual perspective; 2 students disagreed for Vg and  4 for Vs in the group perspective. Regarding the students  reflection, several opinions appeared but the majority thought  the evolution of the own emotions and the comparison to the  group make her reflect; a maximum of 4 students rated these two  questions negatively for each visualization. Students were also  asked about understandability problems in the Interviews, and  almost all of them agreed with no major problems and were  satisfied with the applied color code. Therefore, as the proposed  visualizations were understood by the majority, the usability was  considered high. However, some interesting feedback was  received that shall be taken into account in order to improve the  next phase of the experiment, e.g. the stacked bars were not a  good visualization schema because data were difficult to  compare.       The usefulness was also considered high because students rated  positively the fact that visualizations involved them in learning  awareness and reflection. However, some different opinions  were also recorded, such as I dont think it is useful for me. I  cannot easily express my feelings on a piece of paper, or on a  numeric scale, or It is useful if you look at it a couple of  weeks later. It is visual and maybe you can do something about  it. Based on these results, we conclude that showing this type of  emotion visualizations to students can be a good way to promote  their self-reflection process. In addition, the proposed  visualizations have proved to be good options to answer research   Figure 6: Students opinion about visualizations     Figure 5: Students opinion about confidence on the  own answers (answers to CI in Class1 and Class2)      Figure 4: Students opinion about the influence of  emotions in learning (answers to II in Class1 and Class2)   .         question B, i.e. How can students emotions be displayed to  favor self-reflection   To answer research question C, Does tracking emotions really  impact on teaching-learning contexts, the opinion of students  was explored in the Satisfaction Questionnaire in this phase, and  other evaluation methods were planned for future phases, such  as teachers opinion or the relation between the students grades  and the emotions during the course. The results showed that 10  out of 15 students agreed that being aware of their own emotions  could influence their learning and that being aware of group  emotions could make them reflect on their own emotions. The  majority of the students (13 out of 15) thought that a students  emotion tracking app could be really useful for teachers.  However, only a few of them would continue tracking (6 out of  15), and several students pointed out the wasted time due to the  lack of agility of the tool as the main cause. In addition, the  system Accesses revealed students only visualized their  emotions when they were in the classroom and were asked to do  it, even though they could check them whenever they wanted  during the last two weeks of the course.   In summary, although the visualization usability and usefulness  were considered high and a considerable number of students  believed tracking emotions could impact their learning, only  some of them would continue tracking their emotions. As a  result, we concluded that students motivation was not enough to  put an effort into tracking emotions, and pointed out as a  possible cause a stronger sensation of wasted time than the  possibility of improving learning. We must also remark that not  all the students felt comfortable evaluating their emotions, which  can be produced by an unfamiliar terminology, especially at the  beginning of the experiment, or even by the difficulty of  reflecting and expressing feelings about the teaching-learning  context. Although some answers to question C derive from  various students opinions, a much deeper analysis is needed.  However, the data tends towards more favorable evaluations of  the impact of visualizations in learning when students show  more positive emotions during the course.  In conclusion, the results obtained in the experiment to answer  the A, B, and C research issues by means of the Influence Items,  the Confidence Items and the Satisfaction Questionnaire indicate  that emotions can be measured by means of suitable questions  (TEAMQuest) and that students do not find big problems in  quantifying their emotions. The experiment proved that emotion  visualizations were clear and useful for students, and also  allowed us to record the students opinion to be taken into  account to improve the next phase. Finally, as the impact of  tracking emotions on learning was differently valued by  students, we planned a new experiment with a bigger set of  participants that would allow us to reach more solid conclusions.  Furthermore, we improved the tracking process in order to be  more dynamic and motivational for students.   3.2 Phase 2: Integration in PresenceClick  In this stage we adapted the presentation of the Emotions Items  (EI) of the TEAMQuest and the visualizations to be integrated in  the PresenceClick system. As a result of the integration, the new  component EmotionModule is aimed to motivate students to  track their emotions by means of more comprehensible  visualizations, and more interactive and direct ways of  answering the questionnaire.    In this phase, the students participating already knew  PresenceClick and had used it from their personal accounts. In   addition, the integration was not a waste of effort avoiding the  use of external links to GoogleDocs and it also solved the  problem of the students access with different codes. The system  also maintained the students anonymity in the teachers  visualizations.   Considering that research issue A was already answered in  Phase 1, the objectives of this stage were to check whether the  improved visualizations were clear enough and useful enough in  helping students to reflect on their emotions (research issue B)  and to discover whether students think that emotion tracking  may impact on their learning and whether they would keep  tracking (research issue C). We also studied whether tracking  emotions is an indicator of the students performance in the  subject, which would allow teachers to maintain awareness of  the possible evolution of their students, which in turn could  derive in new decisions and impacts on the course.   3.2.1 Context and Participants  The EmotionModule was tested during two semesters in the  compulsory subjects Object Oriented and Modular  Programming (MOOP) and Basic Programming (BP), both  belonging to the first year of the Computer Science Degree at  the University of the Basque Country (UPV/EHU), Spain. 97  students enrolled in MOOP during the second semester and 81  students enrolled BP in the first semester of the next academic  year. Both subjects had three sessions per week. Since  completing the questionnaires before and after each class (as in  Phase 1) could be too tiresome for students, the tool included  mechanisms to allow teachers quantifying students emotions  just when they thought it could be significant.   3.2.1.1 Instruments  The EmotionModule lets teachers create four types of emotional  events to capture the emotional state of the classroom  (Emotional Capture Event - ECE). Teachers can create ECEs  according to their own criteria or considering chronological  aspects. Chronological ECEs allow to define specific slot times:  class, week, and any teacher-determined time period; teachers  criteria allow to freely relate an ECE to whatever classroom  activity, e.g. exercise, report or group work. By means of a pre- established parameter, teachers delimit the moments in which  students can complete the questionnaire, and also decide  whether attendance is compulsory or not. PresenceClick allows  students to respond to the ECE according to the established  parameters.   The questionnaire integrated on PresenceClick was obtained by  refining the one used in Phase 1 to clearly separate positive and  negative emotions and also to improve the allure of the interface.  The agreement/disagreement scale in the interface was then  represented by emoticons (Figure 7), and several  information/help messages (derived from the AEQ) were linked  to the sensitive names of the emotions; e.g.  I have felt  comfortable in class and enjoyed the developed activities. The  more I participate in class, the more I enjoy the work done for  the Enjoyment emotion.   The visualization schemas were also transformed to make them  more understandable and useful. They were divided in two parts:  for one specific event (Figure 8) and for all events (Figure 9).  For one specific event, the bubble visualization (see Figure 2.a)  was adapted to include emoticons in order to gain the students  attention and increase motivation (see oval in Figure 8). Having  been poorly rated, stacked bars (see Figure 2.b) were substituted     by bar charts that show the individual global positive/negative  balance of emotions in contrast with the group, instead of  showing comparisons emotion by emotion (see rectangle in  Figure 8) where the number in each chart indicates the medium  of the positive/negative emotions from 1 to 6). As we can see,  this student was very negative in this event.                 Boxplots and bar graphs were used for all events in general in  comparison to the group. Boxplots show relevant information  about the course evolution itself in comparison to the group and  were rated very positively (see oval and curved lines in Figure  9). Bar charts were also used to indicate the average value of  positive/negative emotions (see rectangle in Figure 9). Although  square visualization (Figure 3) had a good acceptation in Phase   1, it was dismissed because it was not intuitive enough and  produced difficulties for non-expert students (as they were in  Phase 1). In this phase, we also created visualizations for  teachers, who could anonymously watch the emotional state of  the class. As emotions are sensitive information, the teachers  view is practically the same as the students one, but he can only  access the general emotions of the group and not those of a  specific student. In this example, we can see that the emotions  for this student dropped considerably during the week that one  specific lesson was given.      The evaluation of the EmotionModule was carried out by means  of a Satisfaction Questionnaire (SQ) and by studying the  students accesses in the system (A) with Google Analytics. The  questionnaire was similar to the one used in Phase 1, and  included three groups of 6-likert scale questions. The first group  concerned the grade of agreement/disagreement about  registering emotions in class, e.g. Tracking emotions helps me  be more positive in the subject and improving my learning. The  second and the third groups were related to the utility and  usability of the two pages of visualizations, respectively  emoticons and boxplots, e.g.: I think data in the boxplot page is  simple and easy to understand. In addition, three open  questions allowed students to freely express their opinion.   3.2.2 Procedure  The experiment involved two stages during two academic years.  In the first stage, events from different nature were used to test  the system and obtain the first impressions of students about the  process of quantifying emotions. In contrast to the previous year,  in the second stage, the objective was to study the students  emotion trends, letting both, teachers and students, get an idea of  the evolution of their emotions regarding the outlined milestones  in the subject to be resolved in laboratory classes. In order not to   Figure 8: Students mood visualization for one specific  event   Figure 7: Emotions Items in PresenceClick     Figure 9: Students mood visualization for all three events  in MOOP. A- a class, B- a lesson, and C- final course work   (A-session ECE)     (B-week ECE)    (C-free ECE)                  (A-session ECE)  (B-week ECE)    (C-free ECE)       influence their answers, in both stages students could only  visualize the global results of each event once it was closed and  responses were no longer admitted, according to the parameters  chosen by the teacher.   In MOOP three Emotional Capture Events (ECE) were carried  out during a month: an ECE session, an ECE week and a free  ECE were created. Students were asked to fill in the first one  just after finalizing a laboratory session with compulsory  attendance, and 41 out of 48 attendants filled it in. The event  dealt with the tasks just developed, which involved several  programming skills. The second event was related to a specific  lesson that was imparted during six sessions (two weeks), three  of which with compulsory attendance. Students were asked by  Moodle to respond the event and 20 students took part. Finally,  the third event was related to the practical work they had to  complete during the whole course. The attendance was optional  and, as in the previous event, students were asked to fill in the  questionnaire by Moodle, and 41 out of 97 enrolled students  carried it out.   The second year, BP students were asked six times to respond  the questionnaire. The first event was created just after the first  days of class in order to let the teacher know the emotional state  of the group at the course beginning and 56 students answered.  The remaining events were created for every laboratory class  across the course, and 36, 57, 48, 29 and 13 students participated  (last event participation was low due to a server problem).    Once the experiment was concluded in each stage, we carried  out the Logs study and the Satisfaction Questionnaire in  GoogleDocs and spread by Moodle. The participation in the  questionnaire was considerably lower in percentage terms than  in Phase 1 with 36% of students in MOOP and 22% in BP. This  was attributed to the fact that groups were large and that  questionnaires were carried out in both cases when they had  already finished the classes. Even though it is not an enough  sample of students to obtain significant results, the conclusions  derived from students answers can give us insights about their  perceptions of visualizations and the entire process of tracking  emotions.    3.2.2.1 Results and discussions  Research issue B displaying emotions for self-reflection was  addressed in the same way as Phase 1, and visualizations were  supposed to help students to be more aware of their own  emotions just by reflecting on them and comparing their own  mood to that of their peers. Knowing the general mood of the  classroom could help students to act more cohesively and tightly  interrelated. For example, Figure 9 shows the mood  visualization of a specific student in MOOP who is in general  slightly less positive than the group and slightly more negative  as the column charts indicate. In addition, the black lines in the  boxplots indicate possible difficulties in understanding the  concepts given during the week. The results for the last ECE  improved considerably and, in general, the values were between  the majority. It can be interpreted that visualizing her results and  comparing them to the group helped her to increase attention or  effort while studying, which improved her general mood. In this  subject, the average values for all the positive emotions were  always high while the negative ones were generally low (Figure  9) so the teacher could derive that the classroom was in a good  mood although not too much. In addition, frustration had an  increasing average value for each event, ending in 4 points. It  could be interpreted that students frustration was steadily rising  due to the increases in the difficulty level of the assignments.   Taking this into account, the teacher could re-consider the to be  less exigent, or maybe he confirmed the effects of a challenging  work-plan. In any case, having information about the mood class  makes the teacher aware and sensitive enough as to take steps to  improve the teaching-learning work-plan, if necessary.   Figure 10 shows the students opinion about visualizations as  reflected in the satisfaction questionnaire for each course.  Boxplots indicate students agreed that the emoticons and bar  charts visualizations were easy to understand for one event, but  they found some difficulties when interpreting boxplots. In both  courses one third of students thought they were not easy to  understand, although in BP we gave the students the possibility  to obtain some help in the system in order to get a clear  explanation about boxplots to avoid loss of motivation. Students  also agreed that visualizations gave them interesting  information, specially the emoticons page; however, 28%  students for MOOP and 22% for BP thought the boxplot page  was not interesting at all. Finally, in MOOP 61% of students  thought that both visualizations helped them reflect about their  emotions, while in BP the satisfaction in this point increased  notably and 75% agreed with it. Probably, this is due to the fact  that the experiment was carried out during the entire semester,  letting them compare their evolution to the groups one for a  large period. In conclusion, we consider that EmotionModule  usability is high because students did not have major problems  in understanding the given information, although it can be  improved due to the boxplots visualization. Utility of  visualizations is also good because the majority of students  agreed that the provided information is interesting and it makes  them reflect on their emotions. It was detected that contextual  information could enrich the visualizations in order to help  teachers and students in their reflection process (for example, it  would be useful to know that negative emotions are related to  lessons, while positive emotions could be linked to discussion  sessions).      Regarding the impact of tracking emotions on students learning  (research issue C), in MOOP only a third of the participants in  the satisfaction questionnaire thought it helps them to be more   Figure 10: Students opinion about visualizations in Phase 2       positive in the subject or that it could have an impact in their  behavior. However, two thirds in BP agrees that the  EmotionsModule helps them to me more positive and the 73%  said that it could have an impact in their behavior. This means  that satisfaction increased the second course notably in these  aspects, probably due also to the continuity in tracking emotions.  In MOOP two thirds thought it would really help the teacher to  be more aware of the class situation, while in BP only one  student disagreed with it. While in MOOP 50% of the students  would continue registering, in PB 80% would, which implies a  significant increase in the beliefs of the impact of emotion  tracking. However, less than the 25% of students that normally  attend the course responded the satisfaction questionnaire, which  means that perhaps the remaining students could decrease the  satisfaction of the group.   As in Phase 1, the system accessed revealed accessing peaks for  the created events just when the teacher warned students to get  in, which could indicate low motivation amongst students since  few accesses were registered at other moments. This could be  due to the lack of habit to tracking emotions. In addition,  although the majority understood the emotions and what they  were evaluating, 61% of students did not feel comfortable with  tracking in MOOP and the 27% in BP. Therefore, the  disconformity descended notably in the second subject, probably  due to the fact that they tracked emotions during a larger period,  getting use to the process. We should also take into account that  the experiment was carried out in Computer Science, and maybe  students with other profiles, such as Psychology or Pedagogy,  would be even more open to this kind of practice.   In both subject the final exam was used to check the students  performance. In MOOP 45% of students enrolled in the subject  took it, while 73% in BP did. The emotions measures were took  by calculating the media to all the filled events for each student.  In MOOP we found a significant correlation between the grades  and the students emotions: the more positive the student is, the  higher the grade is ( = 0.46, p = 0.0057), while the more  negative, the lower ( = 0.46, p = 0.0039) [5]. However, in BP  we did not find correlation between the mark and emotions.  Taking both dataset altogether correlations between the mark  and negative emotions appeared ( = 0.33, p = 0.0011). Table 3  summaries the correlation information between the grades in the  exam and the emotions.   Table 3: Correlations between the mark and emotions    Positive emotions Negative emotions   MOOP 0.45 (p = 0.0057) -0.46 (p = 0.0039)   BP 0.12 (p = 0.351) -0.28 (p=0.034)   MOOP+BP 0.24 (p = 0.02) -0.33 (p=0.0011)     Therefore, emotions registered during the course seem to be a  possible indicator of the students mark in the final exam, which  could provide teachers with early information useful for  adapting the course dynamically and so improving students  learning outcomes. Probably the correlations in MOOP are  higher that in BP because the emotions events in this subject  were done little time before the exam, so they had more or less  clear their possible performance on it and their feelings could be  influenced by this fact. However, in BP events were done during   the whole course, where their emotions can vary a lot according  to the different activities during classes. As positive and  negative emotions parameters were took as a whole measure  from all the events is possible that big information was lost in  the way, so studying correlations between mark and emotions  across time will be took into account in future research lines in  order to predict students performance.   In summary, this Phase has positively valued a set of  visualization schemas proposed to display students emotions,  and has confirmed these visualizations are a good mechanism to  involve students in self-reflection processes, which answers  research issue B How can students emotions be displayed to  favor self-reflection. The research on issue C Do tracking  emotions really impact on teaching-learning contexts has not  produced concluding results. On one hand, tracking emotions  seems to make an impact on the learning of some students, but a  deeper study on a broader and richer sample is needed to achieve  general conclusions. On the other hand, results indicate that the  continuity in the use of the emotion dashboard drives to a major  satisfaction of students, which implies a willingness towards the  possibility of behavioral changes. Finally, the information  provided by tracking emotions seems to be a good indicator of  the success/failure of students in the subject, which could benefit  the teachers management of the teaching-learning strategies.    4. CONCLUSIONS  Several works have proved that the emotional state of students is  an important conditional factor to a successful learning  experience, but (A) How can students learning emotions be  quantified If we are able to identify them, (B) How can they  successfully be shown to students and teachers And, (C) How  can we students and teachers benefit from them These  questions establish the context and goals of the work here  presented. We have proposed a method to track students  emotions during the course, and have provided students and  teachers with information about the resulting emotional state of  the class. Our aim is to use visualization techniques to drive the  students through the different phases of the learning analytics  process model (awareness, reflection, sense making and impact):  increasing students awareness of emotions within themselves as  well as the group, involving themselves in self-reflection  processes that positively impacts their learning results and  allows teachers to improve their teaching-learning strategies.  This paper presents some answers to the posed questions  through an incremental two-phase experiment, which involved  different subjects, number of students, and improvements in  visualization techniques and inquiry methods.   This paper has introduced the TEA Model (TEAM) that involves  the main emotions detected in educational contexts. It has  explored the possibility of measuring students emotions through  inquiry methods (questionnaire). In Phase 1, we registered the  students beliefs about the impact of their own emotions on  their learning and their trust in their own answer. The  significantly good results to these questions validated the TEA  Model and also allowed us to conclude that students could  measure their emotions (question A) adequately. However, we  discovered that a considerable number of students felt  uncomfortable doing it. We suppose that a habit of registering  emotions, and even selecting a more familiar terminology could  decrease this discomfort. As a result, the inquiry mechanisms  and visualizing techniques were improved in evaluation Phase 2.  The EmotionModule was developed to integrate the TEAM in  the PresenceClick environment by means of emoticons to make     the process of capturing emotions more dynamic and  motivational. The procedure of capturing and visualizing  emotions became more agile by means of this system due to its  simple interfaces and students prior knowledge of  PresenceClick.   According to the evaluation results of both phases, students  emotions can be displayed through several visualization  techniques bubble charts, stacked bars, boxplot charts,  emoticons that involve them in self-reflection processes  (question B). It should be noted that not all the students agree on  this point, nor were the visualization schemas equally  successful. Boxplots generated larger problems amongst  students, but due to the amount of information they can  communicate with a small learning curve, we consider them as a  good visualization that can go with suitable explanations to help  understanding and increase interest.   Finally, we evaluated the impact of tracking emotions on  students behaviour (question C) by taking both experiments as a  whole. The results indicate that approximately half of the  students (55%) considered that tracking emotions could have a  positive impact in their behavior. Only regarding results of the  second experiment where students used the EmotionModule  during a whole semester, increases this result considerably  (73%) although the satisfaction survey was only completed by  22% of the students who participated in the experiment. We  have also confirmed that emotions visualization could be a good  proxy for the students performance. This could help teachers to  make appropriate strategically decisions that are based on the  classroom mood during the course. Almost all students were  convinced that it would be really helpful for teachers to know  the emotional state of the classroom.   5. ACKNOWLEDGMENTS  This work has been supported by the Government of the Basque  Country (IT722-13), the University of the Basque Country  (PPV12/09, UFI11/45), Gipuzkoako Foru Aldundia (FA- 208/2014-B),  Erasmus+ programme, Key Action 2 Strategic  Partnerships, of the European Union under grant agreement  2015-1-UK01-KA203-013767   ABLE project.  and  European  Communitys Seventh Framework Programme (FP7/2007-  2013) under grant agreement No 318499 - weSPOT project.   6. REFERENCES  [1] I. Arroyo, D. G. Cooper, W. Burleson, B. P. Woolf, K.   Muldner, and R. Christopherson. Emotion Sensors Go To  School. In Proceedings of the 2009 conference on Artificial  Intelligence in Education, 2009, pp. 1724.   [2] Beyondverbal. [Online]. Available:  http://www.beyondverbal.com.   [3] P. Ekman,  W.V. Friesen and J.C. Hager, The facial action  coding system, 2en edn. London: Wei- denfeld & Nicolson,  2002.   [4] A. Fessl, V. Rivera-Pelayo, V. Pammer, and S. Braun.  Mood Tracking in Virtual Meetings. In 21st Century  Learning for 21st Century Skills, vol. 7563, A. Ravenscroft,  S. Lindstaedt, C. Kloos, and D. Hernndez-Leo, Eds.  Springer Berlin Heidelberg, 2012, pp. 377382.   [5] G. Gray, C. McGuinness, and P. Owende, An Investigation  of Psychometric Measures for Modelling Academic   Performance in Tertiary Education. In Sixth International  Conference on Educational Data Mining, 2013   [6] M. Hannafin and S. Land. The foundations and  assumptions of technology-enhanced student-centered  learning environments. Instr. Sci., vol. 25, no. 3, pp. 167 202, 1997.   [7] C. Hinton, K. Miyamoto, and B. Della-Chiesa Brain  Research, Learning and Emotions: implications for  education research, policy and practice1. Eur. J. Educ., vol.  43, no. 1, pp. 87103, 2008.   [8] Inflow. [Online]. Available: http://www.inflow.mobi.  [9] H. Lam, E. Bertini, P. Isenberg, C. Plaisant, and S.   Carpendale. Empirical Studies in Information  Visualization: Seven Scenarios. Vis. Comput. Graph. IEEE  Trans. On, vol. 18, no. 9, pp. 15201536, Sep. 2012.   [10] D. Leony, P.J. Muoz-Merino, A. Pardo, C.D. Kloos.  Provision of awareness of learners emotions through  visualizations in a computer interaction-based environment.  Expert Systems with Applications. 40, 5093  5100 (2013).   [11] Moodjam. [Online]. Available: http://moodjam.com.  [12] Moodpanda. [Online]. Available:   http://www.moodpanda.com.   [13] Moodscope. [Online]. Available:  https://www.moodscope.com.   [14] R. Pekrun, T. Goetz, A. C. Frenzel, P. Barchfeld, and R. P.  Perry. Measuring emotions in students learning and  performance: The Achievement Emotions Questionnaire  (AEQ). Stud. Emot. Acad. Engagem., vol. 36, no. 1, pp. 36 48, Jan. 2011.   [15] Personal informatics. [Online]. Available:  http://www.personalinformatics.org.   [16] Quantified self. [Online]. Available:  http://quantifiedself.com.   [17] V. Rivera-Pelayo, J. Munk, V. Zacharias, and S. Braun.  Live interest meter: learning from quantified feedback in  mass lectures. In Proceedings of the Third International  Conference on Learning Analytics and Knowledge, New  York, NY, USA, 2013, pp. 2327.   [18] Robinson, M.D., Barrett, L.F.: Belief and Feeling in Self- reports of Emotion: Evidence for Semantic Infusion Based  on Self-esteem. Self and Identity. 9, 87111 (2010).   [19] S. Ruiz, M. Urretavizcaya, and I. Fernandez-Castro,  Monitoring F2F interactions through attendance control. In  Frontiers in Education Conference, 2013 IEEE, 2013, pp.  226232.   [20] S. Ruiz, M. Urretavizcaya, I. Fernndez-Castro, and J.-M.  Lpez-Gil. Visualizing Students Performance in the  Classroom: Towards Effective F2F Interaction Modelling.  In Design for Teaching and Learning in a Networked  World, vol. 9307, G. Conole, T. Klobuar, C. Rensing, J.  Konert, and . Lavou, Eds. Springer International  Publishing, 2015, pp. 630633.   [21] K. Verbert, E. Duval, J. Klerkx, S. Govaerts, and J. L.  Santos. Learning Analytics Dashboard Applications. Am.  Behav. Sci., Feb. 2013.        "}
{"index":{"_id":"34"}}
{"datatype":"inproceedings","key":"Muslim:2016:RID:2883851.2883921","author":"Muslim, Arham and Chatti, Mohamed Amine and Mahapatra, Tanmaya and Schroeder, Ulrik","title":"A Rule-based Indicator Definition Tool for Personalized Learning Analytics","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"264--273","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883921","doi":"10.1145/2883851.2883921","acmid":"2883921","publisher":"ACM","address":"New York, NY, USA","keywords":"indicator, learning analytics, open learning analytics, personalized learning analytics","abstract":"In the last few years, there has been a growing interest in learning analytics (LA) in technology-enhanced learning (TEL). Generally, LA deals with the development of methods that harness educational data sets to support the learning process. Recently, the concept of open learning analytics (OLA) has received a great deal of attention from LA community, due to the growing demand for self-organized, networked, and lifelong learning opportunities. A key challenge in OLA is to follow a personalized and goal-oriented LA model that tailors the LA task to the needs and goals of multiple stakeholders. Current implementations of LA rely on a predefined set of questions and indicators. There is, however, a need to adopt a personalized LA approach that engages end users in the indicator definition process by supporting them in setting goals, posing questions, and self-defining the indicators that help them achieve their goals. In this paper, we address the challenge of personalized LA and present the conceptual, design, and implementation details of a rule-based indicator definition tool to support flexible definition and dynamic generation of indicators to meet the needs of different stakeholders with diverse goals and questions in the LA exercise.","pdf":"A Rule-Based Indicator Definition Tool for Personalized  Learning Analytics  Arham Muslim Learning Technologies  (Informatik 9) RWTH Aachen, Germany  muslim@cil.rwth- aachen.de  Mohamed Amine Chatti Learning Technologies  (Informatik 9) RWTH Aachen, Germany  chatti@informatik.rwth- aachen.de  Tanmaya Mahapatra Learning Technologies  (Informatik 9) RWTH Aachen, Germany  tanmaya.mahapatra@rwth- aachen.de  Ulrik Schroeder Learning Technologies  (Informatik 9) RWTH Aachen, Germany schroeder@cil.rwth-  aachen.de  ABSTRACT In the last few years, there has been a growing interest in learning analytics (LA) in technology-enhanced learning (TEL). Generally, LA deals with the development of meth- ods that harness educational data sets to support the learn- ing process. Recently, the concept of open learning analyt- ics (OLA) has received a great deal of attention from LA community, due to the growing demand for self-organized, networked, and lifelong learning opportunities. A key chal- lenge in OLA is to follow a personalized and goal-oriented LA model that tailors the LA task to the needs and goals of multiple stakeholders. Current implementations of LA rely on a predefined set of questions and indicators. There is, however, a need to adopt a personalized LA approach that engages end users in the indicator definition process by supporting them in setting goals, posing questions, and self- defining the indicators that help them achieve their goals. In this paper, we address the challenge of personalized LA and present the conceptual, design, and implementation details of a rule-based indicator definition tool to support flexible definition and dynamic generation of indicators to meet the needs of dierent stakeholders with diverse goals and ques- tions in the LA exercise.  CCS Concepts Information systems ! Information systems appli- cations; Data analytics; Applied computing ! E-lear- ning; Human-centered computing ! Visual analytics;  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883921  Keywords Learning analytics, open learning analytics, personalized lea- rning analytics, indicator  1. INTRODUCTION Learning analytics (LA) represent the application of big  data and analytics in education [13]. LA has been defined at the first international conference on learning analytics and knowledge (LAK11) as the measurement, collection, analy- sis and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs [13]. Chatti et al. pre- sented a reference model for LA and provided a systematic overview based on four dimensions: What kind of data does the system gather, manage, and use for the analysis, Who is targeted by the analysis, Why does the system analyze the collected data and How does the system perform the analysis of the collected data [6, 5].  A particularly rich area for future research is open learning analytics (OLA) [13]. In general, OLA encompasses dierent stakeholders associated through a common interest in LA but with diverse needs and objectives, a wide range of data coming from various learning environments and contexts, as well as multiple infrastructures and methods that enable to draw value from data in order to gain insight into learning processes. The various objectives in OLA (e.g. monitoring, analysis, prediction, intervention, tutoring, mentoring, as- sessment, feedback, adaptation, personalization, recommen- dation, awareness, reflection) need a tailored set of questions and indicators to serve dierent stakeholders with diverse goals [5]. An indicator can be described as specific calcu- lators with corresponding visualizations, tied to a specific question [9]. Current implementations of LA rely on a pre- defined set of indicators. This is, however, not helpful in the case of OLA where the set of required indicators is un- predictable. Thus, it is crucial to follow a personalized and goal-oriented LA model that supports end users in setting goals, posing questions, and self-defining the indicators that help them achieve their goals. In this paper, we address the challenge of personalized LA and present the conceptual,    design, and implementation details of a rule-based indicator definition tool to support flexible definition of indicators to meet the needs of dierent stakeholders with diverse goals.  The remainder of the paper is structured as follows: In Section 2, we introduce the concept of personalized LA and its associated challenges. Followed by the review of related work in Section 3. In Section 4, the design and implementa- tion of a Rule-Based Indicator Definition Tool is presented. The results of evaluation are discussed in section 5. Finally, Section 6 gives the concluding remarks and the future work of this paper.  2. PERSONALIZED LEARNING ANALYT- ICS  The concept of Open Learning Analytics (OLA) was in- troduced in 2011 by a group of leading thinkers on LA in an initial vision paper published by the Society for Learn- ing Analytics Research (SoLAR) [13]. A first summit was then held in Indianapolis, Indiana in March 2014 to pro- mote networking and collaborative research [1]. Building on the first summit, the Learning Analytics Community Ex- change (LACE) project organized in December 2014 the Open Learning Analytics Network Summit Europe to de- velop a shared European perspective on the concept of an open learning analytics framework [7].  So far, from the initial vision paper through the last sum- mit, the development of the concept of open learning an- alytics was restricted to a discussion on the need for open source software, open standards, and open APIs to address the interoperability challenge in this field as well as how im- portant tackling the ethical and privacy issues is becoming for a wide deployment of LA. Recently, Chatti et al. point out that OLA refers to an ongoing analytics process that encompasses diversity at all four dimensions of their LA ref- erence model [5]:   What It accommodates the considerable variety in learning data and contexts. This includes data coming from traditional education settings (e.g. LMS) and from more open-ended and less formal learning settings (e.g. PLEs, MOOCs).   Who It serves dierent stakeholders with very diverse interests and needs.   Why It meets dierent objectives according to the particular point of view of the dierent stakeholders.   How It leverages a plethora of statistical, visual, and computational tools, methods, and methodologies to manage large datasets and process them into metrics which can be used to understand and optimize learning and the environments in which it occurs.  A key challenge in OLA is to serve dierent stakeholders with diverse goals and questions. It is essential to see end users as the central part of the LA practice. In fact, user involvement is the key to a wider user acceptance, which is required if LA tools are to serve the intended objective of improving learning and teaching. Thus, it is important to follow a personalized and goal-oriented LA model that tailors the LA task to the needs and goals of dierent stake- holders. There is a need to adopt a user-in-the-loop LA ap- proach that engages end users in a continuous inquiry-based  LA process to define the right Goal / Question / Indicator (GQI) triple. Giving users the opportunity to set their goal, pose questions, and specify the indicator to be applied is a crucial step to achieve personalized LA results. This would also make the LA process more transparent, enabling users to see what kind of data is being used and for which purpose.  From a technical perspective, most LA tools currently available are designed in such a way that a user can select from a predefined catalogue of static questions and indica- tors with predetermined data source, analytics method, and visualization type to be displayed in a dashboard. If a user needs a new Indicator which is not available in the catalogue, she has to request it from the tools developer. To achieve personalized LA, there is a need for an indicator definition tool that enables end users to dynamically define new ques- tions and indicators that meet their goals in an ecient and eective way. This tool should provide a user-friendly inter- face that supports an interactive, exploratory, and real-time user experience to enable a flexible data exploration and vi- sualization manipulation.  3. RELATED WORK In the LA literature, there is a wide range of tools that use  dashboard to graphically show dierent indicators. These tools, however, use predefined set of indicators and do not allow end users to define new indicators. To the best of our knowledge, there is no research which has been conducted to achieve personalized learning analytics that enables end users to dynamically define new indicators based on their goals. However, related to our research, there are tools available which support users in representing and executing analysis workflows. For instance, Gohnert et al. propose a web-based analytics workbench to support researchers of so- cial networks in their analytic process. The workbench oers a visual representation of the data flow, using the pipes-and- filters design pattern. Following a multi-agent approach, the workbench also allows an integration of additional analysis components [10]. Similarly, the Konstanz Information Miner (KNIME) oers a modular environment, which provides a graphical workbench for visual assembly and interactive ex- ecution of data pipelines. Further, KNIME enables integra- tion of new algorithms as well as data manipulation or visu- alization methods in the form of new modules or nodes [3]. The two data pipelining environments presented above can be used to model workflows which visualize every single step of the analysis process as one node and make the flow of data explicit by connecting those nodes. These environments of- fer users powerful mechanisms for visual representation of multi-step analysis workflows as well as data manipulation. However, such workflow and pipelining tools have two major limitations when used in the context of personalized learn- ing analytics. First, these tools are data-driven rather than goal-oriented and thus cannot be used to support LA users in setting goals, posing questions, defining indicators, and relating them to questions. Second, these tools are designed to support specialists in the respective domains. In KNIME, for instance, a user needs to interact with a large variety of nodes, from which she can select data preprocessing steps, data mining and machine learning algorithms along with their parameters, as well as visualization tools, and connect them in the right way to generate a valid data workflow. The complexity of these tools might hinder non-technical user to adopt them. It is widely recognized in the LA lit-    erature that LA tools should be designed in a way that can help learners, teachers, and institutions to achieve their an- alytics objectives without the need for having an extensive knowledge of the techniques underlying these tools. In par- ticular, educational data mining tools should be designed for non-expert users in data mining.  In the next section, we present the Rule-based Indicator Definition Tool (RIDT) which oers an easy to use and eec- tive environment to define the Goal / Question / Indicator (GQI) triple and leverages rule engines to reduce the com- plexity of the indicator definition process.  4. RULE-BASED INDICATOR DEFINITION TOOL  The aim of RIDT is to achieve personalized and goal- oriented LA by enabling end user to define their own set of goals, question and indicators. In the following sections, we present the design and implementation of RIDT through detailed discussion of possible user scenario, systems require- ments, conceptual architecture, implementation details, and evaluation.  4.1 User Scenario Ayesha is a lecturer at RWTH University where she uses  the university learning management system L2P to admin- ister her courses. She uses RIDT to monitor her eLearning class using various predefined indicators such as presence of students in class, most viewed/downloaded documents, and the progress of students in assignments. Recently, Ayesha came up with the requirement to see which students are ac- tive in her new Web Technologies class. She looked in the list of available indicators but did not find all the indicators she was looking for which can define the activeness of stu- dents in a class. She uses the RIDT interface to define a new analytics question how active are my students in the Web Technologies class. She reuses the existing indicator presence of students in class and updates the filter to get data related to the Web Technologies class.  After that she starts creating a new indicator namedpost rate in discussion forums. She chooses the indicator type (statistics), selects L2P as a data source, selects discussion forum as a category, applies a filter to get data related to the Web Technologies class, and chooses to visualize the results as a bar chart. Similarly, she creates another indica- tor named post rate in wikis. Finally, she creates a new composite indicator collaboration rate in class by combin- ing the two indicators post rate in discussion forums and post rate in wikis and choosing stacked bar chart to visu- alize the combined results. The newly created question and associated indicators are then added to the list of available questions and indicators for future use by other users.  4.2 Requirements Developing a tool to allow dynamic definition of new indi-  cators and managing them is a challenging task. The main requirements for RIDT are described below.  4.2.1 Personalization  It is necessary to provide users full control in the whole indicator definition process which is key to a wider user ac- ceptance of the tool.  4.2.2 Transparency  The whole indicators definition process should be trans- parent for users to see what kind of data is being used and for which purpose. This is vital to drive forward the accep- tance of the tool.  4.2.3 Usability  RIDT should provide simple, intuitive, and consistent User Interface (UI) to easily guide users through the indicator definition process.  4.2.4 Reusability  It is necessary to design an architecture that supports the reuse of existing questions and indicators by other users as well as the reuse of existing indicators in the generation of new ones.  4.2.5 Modularity, Flexibility and Extensibility  RIDT should have a modular and flexible architecture to support faster, cheaper, and less disruptive adaptability of new indicators types due to ever changing requirements of the users.  4.2.6 Performance and Scalability  Scalability and performance should be taken into consid- eration when designing the architecture of RIDT in order to support growing number of end users and indicators of dierent types.  4.3 Conceptual Approach In the following sections, we discuss the design of RIDT  with a focus on the underlying data models, the abstract architecture, and a system workflow.  4.3.1 Data Models  Dierent data model specifications have been introduced in the LA literature [8]. RIDT uses the data model called Learning Context Data Model (LCDM) suggested by Thus et al. [14] in the frame of the Learning Context Project1. To note, that the concepts discussed in this paper are valid if other data models are used.  As compared to other candidate data models for LA, such as Experience API (xAPI), IMS Caliper, Activity Streams Contextualized Attention Metadata (CAM), Learning Reg- istry Paradata, and NSDL Paradata, LCDM is a compro- mise between completeness, flexibility, and simplicity. It is a user-centric, modular, and easy to understand data model that holds additional semantic information about the con- text in which an event has been generated [14, 12]. As illus- trated in Figure 1, LCDM associates users with an arbitrary amount of events. The elements of each event are described as follows:   Source: This model gathers data from dierent ap- plications. To preserve this context information, the name of the application that generated this event is stored as source, such as, L2P, Moodle, etc.   Platform: Since the centralized data model hold data from dierent kinds of devices, this attribute specifies the platform from which the event originated, such as mobile device, web-based, or stationary device.  1http://www.learning-context.de/    Figure 1: Learning Context Data Model (LCDM) Data Model (adapted from [14])   Timestamp: This attribute indicates when the event has been created to preserve the sequence of events.   Session: This attribute is used to group dierent events together.   Action: This attribute defines what kind of action hap- pened in the event, such as, update, delete, start, end, etc.   Category: This attribute comprises of three parame- ters, namely type, major and minor. Type is an op- tional parameter which defines if the event is of pri- vate, professional, or academic nature. Major and mi- nor describe the broader semantic information of the event. Major defines the group (environmental, bio or activity) and minor gives the kind of the event (Wiki, Discussion forum, Calendar, etc.). This is a predefined list but it can be extended based on new requirements.   Entity: An event can be described by an arbitrary amount of key-value pairs. This could e.g. be the lon- gitude and latitude of a position with their respective values or this could be the title and the resource of a learning material.  Besides using LCDM to represent learning events, RIDT uses an additional data model to manage and store questions and indicators, as shown in entity relationship diagram in Figure 2. Every question can have multiple indicators as- sociated with it and an indicator can be associated with multiple questions. Dierent properties are associated with the question and indicator entities. For example, the Query property stores the data query for the Indicator to fetch data from the database every time the indicator is executed. All the properties of the indicator defined by the user are stored  Has  User  Name  Number of  Indicator  Last  Execution   Time  ID  Name  Total  Executions  Name  Last  Execution   Time  User  Name  Total  Executions  Template  Is  Composite  ID  Chart  Engine  Query  Chart Type  Question  Indicator  n  n  Figure 2: Question & Indicator Entity Relationship Diagram  Data Storage  Interaction  Analytics Data  Stakeholders  Student  Teacher Researcher  Educational Institution  . . .   Data and Environments  Personal  Learning   Environment  (PLE)  Virtual  Learning   Environment  (VLE)  Social  Networks  Professional  Networks  Learning  Management   Systems (LMS)  . . .   Rule EngineIndicator Editor  Rule-based Indicator Definition Tool  Learning Activities  Data  Indicator Generator  Figure 3: RIDT Conceptual Architecture  in the Template property in a JSON format to be reused for the generation of a new indicator based on an existing one.  4.3.2 Abstract Architecture  Figure 3 shows the abstract architecture of RIDT. It is composed of three main components: Indicator Editor, Rule Engine, and Indicator Generator. The Indicator Editor pro- vides a user-friendly interactive interface to set the LA goal, formulate questions, and define indicators associated with those questions. The Rule Engine is responsible for man- aging indicator types and their associated queries. The In- dicator Generator is responsible for the generation of new indicators by executing the queries and rendering the indi- cator.  4.3.3 System Workflow  RIDT follows the GQI approach to allow users to define new indicators. The user starts the indicator definition pro-    cess by interacting with the Indicator Editor and defining the LA goal such as monitoring, analysis, prediction, inter- vention, feedback, adaptation, recommendation, awareness, reflection, etc. Based on the defined goal, the user asks the LA question and associates indicators to answer this question. Some questions can be answered using a single indicator whereas complex questions would require multiple indicators to fully answer them. For instance, the question how active are students in class can have presence of stu- dents in class, post rate in discussion forums, and post rate in wikis as associated indicators to answer it properly.  To define a new indicator, the Indicator Editor commu- nicates through Indicator Generator with the Rule Engine to obtain the list of possible indicator types and with the data storage to get possible data objects based on the data model used in RIDT, as discussed in Section 4.3.1. Taking the example of the indicator post rate in discussion forums in L2P courseroom Web Technologies, the user first selects the indicator type number of X in Y. Then she specifies the data Source as L2P, the Action as post, the Category as discussion forum. Additionally, she applies a filter on the coursename Entity as Web Technologies. The Indica- tor Generator communicates with the Rule Engine to get the query template related to the selected indicator type and generates the query based on the specified data objects and filters. In our example, in SQL terms, the generated query SELECT COUNT(*) FROM table event WHERE Source=L2P AND Action=post AND Category=discuss- ion forum AND Entity.coursename=Web Technologies; will be associated with the indicator post rate in discus- sion forums in L2P courseroom Web Technologies. After defining the indicator properties and filters, the user selects the appropriate visualization technique to visualize the in- dicator such as bar chart using D3.js.  The user can create new indicators by following the same flow, load an existing indicator as a template and modify its properties, or define a new composite indicator by combining two or more existing ones. Finally, the user can approve the question and its associated indicators which are then stored in the question / indicator database along with the related query and visualization for each indicator.  4.4 Implementation RIDT is a Web application based on the Java Spring  Framework2 that follows the Model View Controller (MVC) design pattern [11]. It is developed using the Test-Driven Development (TDD) approach [2]. The application makes extensive use of Web technologies, such as Servlets, JavaServer Pages (JSP), JavaBeans at the server side and jQuery3, jQuery UI4, AJAX5, and noty6 at the client side. In the following sections, we discuss the technical and implementa- tion details of the dierent components of RIDT, as depicted in Figure 4.  4.4.1 Indicator Editor  The Indicator Editor is the collection of MVC Views pro- viding an interactive user interface (UI) for the RIDT. As  2http://projects.spring.io/spring-framework/ 3https://jquery.com/ 4https://jqueryui.com/ 5http://www.w3schools.com/ajax/ 6http://ned.im/noty/  Rule-based Indicator Definition Tool  Rule Engine (Drools)Indicator Editor  Indicator Generator  Indicator  Dispatcher   Servlet  Indicator  Controllers  Indicator  Models  View Resolver  URL Handler Mapping  Figure 4: RIDT Technical Architecture  shown in Figure 5, the main UI consists of four main sec- tions:  Question Information  In this section the user starts the indicator definition process by entering the LA question. After that, the user can asso- ciate a set of possible indicators with the question by loading an existing indicator and modifying its properties or defin- ing a new indicator. For every indicator, the user has to enter a unique name and select a type. Furthermore, in this section the user can visualize the question which opens a new UI Question Visualization to render all the associated indicators in a dashboard form, as shown in Figure 6. From this UI, the user has the possibility to combine two or more indicators and generate a new composite indicator which is associated with the question as an independent indicator. Composite indicators have their own name and visualization and they continue to function even if the source indicators are deleted. In Figure 6, the indicator collaboration rate in class is a composite indicator created by combining the two indicators post rate in discussion forums and post rate in wikis. After defining all the associated indicators, the ques- tion along with its indicators can be saved in the question / indicator database.  Indicator Information  In this section the user can specify basic filtering parameters of the indicator, such as from which Source data should be used (L2P, Moodle, etc.), data related to which Action type should be used (add, update, delete, post, begin, end, etc.) and which Category should be used (Wiki, Discussion forum, Assignments, etc.). These parameters represent the elements of the LCDM data model, as discussed in Section 4.3.1.  Indicator Properties & and Summary  In this section the user can refine the data fetched from LCDM event store based on the basic filtering parameters specified in the Indicator Information section by specifying additional filtering parameters of the indicator. The user    Figure 5: RIDT Main User Interface  can add filters to specify Entity attributes, get data for a particular User, in a particular Timestamp range, for a spe- cific Session. The summary of all applied filters can then be viewed and edited under the Filter Summary tab. Un- der the Graph Options tab the user can define properties related to the visualization of the indicator, such as graph type (e.g. bar chart, pie chart) and the visualization library to use for rendering the graph (e.g. D3.js, Google charts, JGraph). The Indicator Summary tab shows a compre- hensive summary of the selected properties for the current indicator. After all the properties are defined the indicator can be previewed under the Graph Preview tab.  Question Summary  This section displays a detailed summary for the current question and its associated indicators that have already been defined. Furthermore, the user has the option of loading the indicators to edit their properties or deleting them before finalizing the question.  4.4.2 Rule Engine  This component is responsible for managing the rules for the dierent indicator types and their associated queries. In RIDT, Drools7 is used as a rule engine which stores rules using MVFLEX Expression Language (MVEL)8 in a file.  7http://www.drools.org/ 8https://github.com/mvel  Each rule in the file reflects a combination of selected ad- ditional filtering parameters and maps it to an appropriate query. Every Drools rule have three main sections, a rule part which contains a phrase to serve as the human identifi- able name of the rule, a when part where the condition of the rule is specified and a then part to describe the actions which are to be taken if the specified conditions are satisfied. Listing 1 shows a Drools rule for the scenario when there are no additional filters specified by the user while defining the indicator.  dialect  mvel   rule  When no additional filters are present   when  EntitySpecification (  userSpecifications.isEmpty() == true,  timeSpecifications.isEmpty() == true,  sessionSpecifications.isEmpty() == true,  entityValues.size()== 1,  entityValues.get(0).geteValues() ==  ALL )  $pFilter : ProcessUserFilters()  $userParameter : EntitySpecification(  $source : selectedSource,  $platform : selectedPlatform,  $action : selectedAction,  $major : selectedMajor,    Figure 6: RIDT Question Visualization User Interface  $minor : selectedMinor,  $type : selectedType,  $key : entityValues,  $pobject: persistenceObject,  $filter : filteringType,  $robject : retrievableObjects)  then  $userParameter.setHql( SELECT   + $robject  +   FROM  + $pobject +   WHERE Action=   + $action +   AND Source IN    + $pFilter.processSource($source, $filter)  +   AND Platform =  + $platform  +   AND Category.Major =   + $major  +   AND Category.Minor =  + $minor  +   AND Category.Type =   + $type  +   AND Key =   + $key.get(0).getKey() +   );  end  Listing 1: Default Drools rule when no additional filters has been added.  After the user has defined all the parameters of the in- dicator using Indicator Editor, the Indicator Generator ini- tiates a unique Drools instance in which it inserts the rule file related to indicator type, the indicator filtering param- eters entered by the user, and other necessary java objects to parse user data and generate the hibernate query. Every  time the indicator is executed, the Indicator Generator uses this hibernate query to fetch the relevant data for visualizing the indicator.  The benefits of following a rule-based approach in RIDT are twofold. First, it is possible to replace LCDM with other data models (e.g. xAPI, IMS Caliper) by altering the when and then parts in the rule to accommodate the attributes of the new data model. Second, it is easy to use a dierent database technology (e.g. NoSQL) by defining appropriate actions in the then part.  4.4.3 Indicator Generator  The Indicator Generator is the backbone of RIDT which acts as a mediator between dierent components in the tool. It is responsible for generating the data queries by commu- nicating with the Rule Engine, executing the queries to get data for visualization, and finally rendering the indicator to be displayed on the Indicator Editor. The dierent compo- nents of the Indicator Generator are described below:  Indicator Dispatcher Servlet  The Indicator Dispatcher Servlet (IDS) is responsible for handling all incoming requests and returning the responses. Based on the request, the IDS consults the URL Handler Mapping component to resolve which controller is appropri- ate for this request and dispatch the request to the returned Indicator Controller for further processing.    Indicator Controllers  The Indicator Controllers are designed to handle user re- quests and invoke the suitable service methods based on the HTTP request type which can be either GET or POST. The service methods process the requested URL and return the logical name of the corresponding view to IDS. Con- trollers in RIDT can be broadly classified into two major cat- egories: JSON-based controllers and view-based controllers. JSON-based controllers are basically server end points for the client side scripts to interact with the server. For ex- ample, when the user types a new name for an Indicator in Indicator Editor, the system must adhere to constraints like minimum number of characters in the name, check for duplicate names etc. In order to achieve this functionality, the client side scripts send a HTTP GET/POST request to these controllers with additional parameters. These con- trollers perform the necessary validation on receiving the request and return the appropriate response in JSON for- mat which enables the system to perform in consistent, re- sponsive and error-free manner. View-based controllers are mainly responsible for validating the data on the page and performing any additional processing in response to the in- coming request for the specific page.  Indicator Models  Models are Plain Old Java Objects (POJOs) responsible for holding the core logic of RIDT. Models are divided in to three main categories:   Domain Objects: These objects hold the necessary business logics which are invoked from the indicator controller based on certain criteria.   Data Mappers: These objects are exclusively respon- sible for retrieval and storage of requisite data from data-source using Hibernate.   Services: These are higher level objects responsible for interaction between the Domain Objects and the Data Mappers.  5. EVALUATION AND DISCUSSION We conducted a user evaluation of RIDT to gauge the ef-  fectiveness and usability of the tool. We employed an eval- uation approach based on the System Usability Scale (SUS) as a general usability evaluation as well as a custom ques- tionnaire to measure the eectiveness of indicator definition in RIDT. During the evaluation process, the user was briefly introduced to the concept of personalized LA and the goals of RIDT. The tool was evaluated with two teachers, three research assistants, and seven students. The participants were asked to perform dierent tasks and subsequently fill out the questionnaire.  5.1 Participants Background The first part of the questionnaire captured the partic-  ipants backgrounds. Figure 7 shows the user background survey results. About 75% of the users were familiar with LA. All of them supported the idea to have an LA sys- tem at RWTH Aachen University which can be accessed by both students and teachers online. But only about 33% of the users had actually used an LA system with real time data. Also, 33% reported that they had prior experience with defining indicators in an LA system.  75  33 33  100  25  67 67  0  0  10  20  30  40  50  60  70  80  90  100  Pe rc en  ta ge  Background  Yes No  Figure 7: User Background Survey Result  90  67  97  88 85  93 90  93 90  0  10  20  30  40  50  60  70  80  90  100  Pe rc  en ta  ge  User Expectation  Figure 8: User Expectation Survey Result  5.2 Expectations The second part of the questionnaire captured the expec-  tations on the features that the users generally would like to have in an LA system. Figure 8 shows the user expectation survey results. About 90% of the users wanted to formulate their own questions and indicators as well as preview them. Over 90% of the users wanted to select dierent combina- tion of data, add filters, select visualization types, modify existing indicators to create new ones, and delete existing indicators. About 88% of the users wanted to customize ex- isting indicators available in the system. And, 67% of the users wanted to use pre-existing questions and indicators in the system.  5.3 Usefulness The third part of the questionnaire captured the useful-  ness of the tool. Figure 9 shows the average percentage of user rating on system usefulness. There were ten questions    81.5 80  90.75 90.75  86.25 85.5  89.25  86.25  89.25  81.5  70  75  80  85  90  95  100  Pe rc en  ta ge  Usefulness  Figure 9: Average User Ratings on System Useful- ness  in total which were answered after performing various tasks in RIDT. Overall, in terms of functionality, the response was positive as all the questions rated 80% or above. Top ques- tions, Define visualization type, Preview indicator, Ed- it/delete indictor and Visualize existing questions and in- dicatorwhich rated near 90% indicates that the most useful feature to the users was to trying out dierent visualization techniques and find best visualizations for their indicators. However, some participants suggested to introduce catalog for easy and fast addition of default filters as well as improve the notification mechanism.  5.4 Usability The fourth part of the questionnaire dealt with the usabil-  ity of the tool based on the System Usability Scale (SUS) which is a simple, ten-item attitude Likert scale giving a global view of subjective assessments of usability [4]. The questions are designed to capture the intuitiveness, simplic- ity, feedback, responsiveness, eciency of the UI, users com- fort level in using the UI, and the steepness of the learning curve which a user must go through to successfully use the UI. Based on the results of the questionnaire, the usability scale (SUS score) of RIDT is calculated to be approximately 53, which is below the standard average SUS score of 68. In general, the respondents found the system intuitive, easy to use, and easy to learn. However, they expressed their con- cerns with inconsistencies in the UI and the lack of feedback and responsiveness of the tool. The majority of the eval- uators wanted a more eective way of on-screen guidance during the indicator definition process.  5.5 Discussion of Results In general, the evaluation results demonstrate that the  majority of the users liked and appreciated the concept on which RIDT has been developed, namely to define own in- dicators in an eective and ecient manner. They liked the features oered by the tool like formulation of ques- tions and indicators, flexibility of visualization type selec- tion, previewing capabilities, and aggregation of simple in- dicators to build composite ones. However, the majority of the users suggested that the UI should be more consistent and responsive. It is obvious from the evaluation results  that future work involving improvements in UI in terms of feedback is necessary through a more eective user notifi- cation mechanism which would guide the user through the indicator definition process.  6. CONCLUSION AND FUTURE WORK In the last few years, there has been an increasing inter-  est in learning analytics (LA) which calls for a movement from data to analysis to action to learning. In this paper, we introduced the concept of personalized learning analyt- ics that engages end users in a continuous inquiry-based LA process, by supporting them in interacting with the LA tool to set goals, pose questions, and self-define the indicators that help them achieve their goals. We presented the theo- retical, design, implementation, and evaluation details of a rule-based indicator definition tool (RIDT) to support flex- ible definition and dynamic generation of indicators. RIDT leverages rule engines to reduce the complexity of the indica- tor definition process. Users can use RIDT to easily define new indicators by exploring learning event data, selecting the appropriate dataset for the indicators, applying dierent levels of filters, and specifying which visualization technique should be used to render the indicators. The evaluation re- sults are promising and show that RIDT has the potential to support a personalized learning analytics experience. Future work includes the improvement of the RIDT user interface mainly in terms of consistency and responsiveness, the im- plementation of new indicator types, and the integration of more visualization techniques.  7. REFERENCES [1] S. Alexander et al. OLA press release (2014).  Retrieved from http://solaresearch.org/initiatives/ola/.  [2] D. Astels. Test driven development: A practical guide. Prentice Hall Professional Technical Reference, 2003.  [3] M. R. Berthold, N. Cebron, F. Dill, T. R. Gabriel, T. Kotter, T. Meinl, P. Ohl, K. Thiel, and B. Wiswedel. Knime-the konstanz information miner: version 2.0 and beyond. AcM SIGKDD explorations Newsletter, 11(1):2631, 2009.  [4] J. Brooke. Sus-a quick and dirty usability scale. Usability evaluation in industry, 189(194):47, 1996.  [5] A. M. Chatti, V. Lukarov, H. Thus, A. Muslim, F. A. M. Yousef, U. Wahid, C. Greven, A. Chakrabarti, and U. Schroeder. Learning analytics: Challenges and future research directions. eleed, 10(1), 2014.  [6] M. A. Chatti, A. L. Dyckho, U. Schroeder, and H. Thus. A reference model for learning analytics. International Journal of Technology Enhanced Learning, 4(5-6):318331, 2012.  [7] A. Cooper. Open Learning Analytics Network - Summit Europe (2014). Retrieved from http://www.laceproject.eu/ open-learning-analytics-network-summit-europe-2014/.  [8] A. Cooper. Standards and Specifications - Quick Reference Guide (2014). Retrieved from http://www.laceproject.eu/dpc/ standards-specifications-quick-reference-guide/.  [9] A. L. Dyckho, D. Zielke, M. Bultmann, M. A. Chatti, and U. Schroeder. Design and implementation    of a learning analytics toolkit for teachers. Journal of Educational Technology & Society, 15(3):5876, 2012.  [10] T. Gohnert, A. Harrer, T. Hecking, and H. U. Hoppe. A workbench to construct and re-use network analysis workflows: concept, implementation, and example case. In Proceedings of the 2013 IEEE/ACM international conference on advances in social networks analysis and mining, pages 14641466. ACM, 2013.  [11] A. Le and J. T. Rayfield. Web-application development using the model/view/controller design pattern. In Enterprise Distributed Object Computing Conference, 2001. EDOC01. Proceedings. Fifth IEEE International, pages 118127. IEEE, 2001.  [12] V. Lukarov, M. A. Chatti, H. Thus, F. S. Kia, A. Muslim, C. Greven, and U. Schroeder. Data models in learning analytics. In DeLFI Workshops, pages 8895. Citeseer, 2014.  [13] G. Siemens, D. Gasevic, C. Haythornthwaite, S. Dawson, S. Buckingham Shum, R. Ferguson, E. Duval, K. Verbert, and R. S. d Baker. Open learning analytics: an integrated & modularized platform, 2011.  [14] H. Thus, M. A. Chatti, C. Greven, and U. Schroeder. Kontexterfassung,-modellierung und-auswertung in lernumgebungen. In DeLFI 2014-Die 12. e-Learning Fachtagung Informatik, pages 157162. Gesellschaft  fur Informatik, 2014.    "}
{"index":{"_id":"35"}}
{"datatype":"inproceedings","key":"Kevan:2016:DMS:2883851.2883941","author":"Kevan, Jonathan M. and Menchaca, Michael P. and Hoffman, Ellen S.","title":"Designing MOOCs for Success: A Student Motivation-oriented Framework","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"274--278","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883941","doi":"10.1145/2883851.2883941","acmid":"2883941","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOC, confirmatory factor analysis, framework, learning analytics, motivation, structural equation modeling","abstract":"Considerable literature exists regarding MOOCs. Evaluations of MOOCs range from ringing endorsements to its vilification as a delivery model. Much evaluation focuses on completion rates and/or participant satisfaction. Overall, MOOCs are ill-defined and researchers struggle with appropriate evaluation criteria beyond attrition rates. In this paper, we provide a brief history of MOOCs, a summary of some evaluation research, and we propose a new model for evaluation with an example from a previously-delivered MOOC. Measurement of the MOOC success framework through four student satisfaction types is proposed in this paper with a model for informal learning satisfaction, one of the proposed types, theorized and tested. Results indicated theoretical underpinnings, while intended to improve instruction, might not have influenced the same satisfaction construct. Therefore, future research into alternative satisfaction factor models is needed.","pdf":"Designing MOOCs for Success:   A Student Motivation-Oriented Framework   Jonathan M. Kevan  Department of Learning Design &   Technology,   University of Hawaii at Manoa   Honolulu, Hawaii 96822  jkevan@hawaii.edu   Michael P. Menchaca  Department of Learning Design &   Technology,   University of Hawaii at Manoa   Honolulu, Hawaii 96822  mikepm@hawaii.edu   Ellen S. Hoffman  Department of Learning Design &   Technology,   University of Hawaii at Manoa   Honolulu, Hawaii 96822  ehoffman@hawaii.edu        ABSTRACT  Considerable literature exists regarding MOOCs. Evaluations of  MOOCs range from ringing endorsements to its vilification as a  delivery model. Much evaluation focuses on completion rates  and/or participant satisfaction. Overall, MOOCs are ill-defined  and researchers struggle with appropriate evaluation criteria  beyond attrition rates. In this paper, we provide a brief history of  MOOCs, a summary of some evaluation research, and we propose  a new model for evaluation with an example from a previously- delivered MOOC. Measurement of the MOOC success framework  through four student satisfaction types is proposed in this paper  with a model for informal learning satisfaction, one of the  proposed types, theorized and tested. Results indicated theoretical  underpinnings, while intended to improve instruction, might not  have influenced the same satisfaction construct. Therefore, future  research into alternative satisfaction factor models is needed.   CCS Concepts   Applied computing ~ E-learning    Applied computing ~  Computer-assisted instruction    Applied computing ~ Learning  management systems    Applied computing ~ Distance learning     Applied computing ~ Computer-managed instruction   Keywords  Structural equation modeling; confirmatory factor analysis;  learning analytics; motivation; framework; MOOC.   1. INTRODUCTION: MOOC EVOLUTION  Although in existence fewer than ten years [32], MOOCs have  garnered considerable recognition both in mainstream media as  well as scholarly publications. A current search of the term  MOOC in Google Scholar (October 30, 2015) returns over  48,000 citations. A similar search in Google (October 31, 2015)   returns nearly 9 million results. Despite such cognizance, MOOCs  remain ill-defined. Two of the co-authors previously published a  paper with an extensive review of literature providing a  description of the evolution of MOOCs [13]. A brief synopsis is  provided here.    Originally conceived as courses to be provided online, for free,  and on a massive scale, contemporary MOOCs vary widely,  having differing purposes, designs, and delivery models. Some  focus on content (xMOOC) and others are student-centered  (cMOOC) [3]. MOOCs have been characterized as instructionist,  constructivist, and connectivist [9, 18, 30]. Some rely heavily on  digital media and others require intense discussion. Some follow  very formal schedules and others are loose, with the ability to start  and stop almost any time. Some are linear and others not.  Delivery may be completely asynchronous, mostly asynchronous  with some synchronous requirements, while some MOOCs even  include face-to-face meetings. Curiously, with the explosion of  MOOCs offered [7], many MOOCs lack even the Massive  quantity that supposedly defines them [3]. Massive has become  more possibility than reality.    Current MOOC research ranges from categorical support [25] to  considerable concern [34], with popular media simultaneously  lauding MOOCs and sounding their death knell [35]. Supportive  research generally falls into three categories: (a) these courses are  open and free and increase access to education for all [2], (b)  these courses can serve large numbers of participants at the same  time [2], and (c) these courses are often designed by the highest  quality experts in particular fields [24]. Those with less positive  views of MOOCs frequently mention several concerns: (a) low  levels of course completion [15, 21, 36], (b) technical problems  [11], (c) inconsistent pedagogical design [31, 35], and (d) concern  over course quality [5, 20, 32]. Regardless of the debate about the  appropriateness or sustainability of MOOCs, the fact is that they  remain and continue to grow [1, 23, 26].    A particularly important area of MOOC research covers  evaluation. Much evaluation research faults MOOCs for  considerable non-completion rates [15, 16, 21, 36]. Some studies  go beyond measuring completion, with many looking specifically  at student and/or instructor satisfaction [12, 15, 22, 28, 33, 36].  These studies tend to focus on outcomes with success often  predicated on course completion. Reich (2015) laments that the  next generation of MOOC research needs to adopt a wider range  of research designs (p. 34).       Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom Copyright is  held by the owner/author(s). Publication rights licensed to ACM.   ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883941     2. A NEW LEARNING FRAMEWORK FOR  EVALUATION  In order to address the need for more complex evaluation,  Hoffman and Menchaca (2015) developed a new learning  framework for examining MOOC outcomes. Since MOOC  participants can be said to participate simultaneously in both  formal and informal learning contexts, new measures for success  are needed. In our framework, attention is directed away from  supply-side measurements to more client-based ones such as  expectations, intentions, goals, values, attitudes, enrichment, and  perceived learning. Hoffman and Menchaca stated:     Beyond simple measurements for success, a  newer framework will allow us to better  understand the experience of learners rather  than simply counting if they completed or not.  At the same time, these revised measures have  the potential to shift educational research in  new directions that will enhance our  understanding of learning in general, ways to  measure outcomes in more nuanced terms,  and assist in the design of learning  environments that will meet the requirements  of this new college-going population as well  as for those involved in formal university  course and degree programs. In sum, a newer  framework for MOOCs, one drawing on  contemporary methods for mining data, a  newer understanding of psychology and the  brain, innovations in educational technology,  and deeper theories of learning design would  provide a better vantage point for making  critical decisions when the stakes are so large.  (2015, p. 7).      3. A STRUCTURAL EQUATION MODEL  In this paper, in order to address the theoretic framework seeking  new measurements for MOOC evaluation, a structural equation  model to measure MOOC participant satisfaction is proposed.  Structural Equation Modeling (SEM), considered a superset of  other statistical approaches, is a methodology used to  quantitatively measure and assess a theoretically based model  [27]. Using this approach multiple theoretical constructs of  student satisfaction can be measured and correlated to understand  their overall influence on MOOC satisfaction. This concept  respects the complexity of student motivations and frames MOOC  success and design as a result of this diversity. In order words,  success is measured as a function of a MOOCs ability to meet  student expectations and not the other way around.    For this new learner framework, we propose student motivation be  considered a more appropriate metric to measure MOOC success.  Theoretical grounding for diversity in student satisfaction was  extracted from several recent studies categorizing motivations for  MOOC participation [4, 35, 36]. Review and consolidation of  research results yielded four initial factors proposed to weigh on  overall MOOC participant satisfaction:   1. Content Satisfaction,  2. Social Satisfaction,  3. Informal Learning Satisfaction, and  4. Formal Learning Satisfaction.      3.1 Content Satisfaction  Students seeking content satisfaction are registering for MOOCs  to access resources for immediate interest or on-demand needs.  Often, these students have no intent of participating in the course  beyond registration and are primarily interested in the type of  content available or seeking answers to an immediate intrinsic or  extrinsic need.     3.2 Social Satisfaction  MOOC participants attempting to expand their social network or  influence are trying to achieve social satisfaction. These students  could be in multiple states of prior knowledge ranging from: (a)  brand new to a topic and looking to meet fellow self-motivated  students to (b) domain experts interested in sharing their  knowledge. Regardless of existing domain knowledge, the  primary goal of this type of student is to make new social  connections in a discipline of interest.     3.3 Informal Learning Satisfaction  Informal learning students can be motivated by a variety of  intrinsic and extrinsic motivation factors ranging from personal  interest to the potential for future job opportunities. What makes  these students unique is disinterest or inability to participate in the  scheduling or workload rigidity often present in formal education.  Preferring to self-regulate instruction, these students often have  external commitments and consider high workloads or content  designed for formal delivery as barriers to satisfaction.     3.4 Formal Learning Satisfaction  This student type, often participating for extrinsic motivational  reasons, is seeking a course aligned with their experience in  formal educational environments. In the United States, this  experience is traditionally expository in nature with groups of  students progressing through content collectively. Looking for  pressure, these students prefer formal supports and often access  to instructional faculty.    Since student motivation is often multi-faceted, each learner will  show a unique combination and weighting of desires for MOOC  satisfaction. If each motivational type also corresponds to targeted  educational interventions, developing a MOOC that satisfies all  students is inherently complicated and perhaps practically difficult  to achieve. In order to explore maximization of MOOC success, if  such success is defined as student satisfaction, then instructional  design strategies that cater to individual motivational needs,  culture and instructional context must be identified and verified as  effective.    To achieve this, MOOC design elements can be separated and  measured for their unique influence on each satisfaction factor  (see Figure 1). For example, required synchronous class sessions  with group discussion may be effective as part of a strategy for  formal learning or social satisfaction yet may negatively correlate  with content or informal learning desires. Iteratively including and  measuring the impact of educational strategies on key satisfaction  factors can help guide educators in designing effective MOOCs,  analyzing summative results, and ultimately lead towards a  formative dashboard for on-demand notification of required  instructional adjustments.       Figure 1: MOOC Satisfaction Model      4. INFORMAL LEARNING  SATISFACTION  To begin defining instructional strategies and technologies that  influence MOOC success, each satisfaction construct must be  individually modeled to theory, tested and then correlated to other  identified constructs. Informal learning satisfaction was selected  as an initial MOOC satisfaction sub factor to analyze with future  factors to be defined in subsequent studies. The following initial  explanatory model for informal learning satisfaction is proposed  to explore how MOOC design and delivery correlates with  satisfaction. Each satisfaction factor is intended as a living  research design to be continuously modified and tested as new  interventions are identified, developed and investigated. Therefore  the proposed informal learning model is not intended to be a  complete model, but a reference point for the reproduction of  research and situation of future development cycles.   This informal learning satisfaction factor looks at the relationship  between three elements used to explore this student type: self- determination, content interaction and self-regulated supports.  Self-determination, a theory that explores an individual's interest  and value of educational experiences [8], is used to explore the  mindset and subsequent MOOC interactions of this student type.  Content interaction in an informal learning environment is  optional and measurement of these events can help identify poorly  scaffolded and low engagement material that could turn this  student type away. Finally, self-regulated supports like checklists  may help informal learners to manage their learning process [9,  19]. Trace data of such interactions may help identify the efficacy  of tools and their relationship with informal learning satisfaction.   5. OPERATIONAL EXAMPLE  Full SEM model analysis requires multiple satisfaction constructs  to be analyzed together; therefore, testing of additional  satisfaction factors must predicate full analysis. Until additional  factors can be correlated, Confirmatory Factor Analysis (CFA), a  type of SEM analysis, is an appropriate initial analytical step for  testing the model of a single satisfaction construct. The informal  learning satisfaction model was tested using CFA, with data  collected in a cMOOC offered at the University of Hawaii at  Manoa (N=28). Two students opted out of the study bringing the  final sub-population to 26 students. Students were informed of the  study upon registration in the MOOC and capable of opting out  and having accumulated research data deleted at anytime. Data  were collected by the Learning Management System using the  Experience API, a social constructivist informed data  specification aligned with the pedagogy of cMOOCs generally,  and this MOOC specifically [17]. All data collected for informal  learning satisfaction are outlined in Table 1.   Table 1: Sub Factors and Measure Data   Elements Data Point   Content  Interaction   Total content views: Total number of times  viewing a page with instructional content    Total design views: Total number of times  viewing student final project submissions   Self Regulated  Supports   Check offs: Number of times student used an  embedded and optional content completion  checklist    Self  Determination   Extra discussions: Total number of discussion  posts beyond amount targeted by instructor    Extra peer review: Total number of peer  projects reviewed by MOOC students beyond  the amount targeted by the instructor      5.1 Results  Goodness-of-Fit indices are used to assess how well the proposed  informal learning satisfaction model fits the data set [26]. The first  analysis, the Comparative Fix Index (CFI), resulted in a value of  .925, smaller than the recommended cut-off of .95 [14].  Additionally, the Room Mean Square Error of Approximation  (RMSEA) index value of 0.178 was found, this value exceeds the  accepted 0.10 cutoff [6] indicating the model is a poor fit to the  available data and further refinement is needed. See Figure 2 for  standardized factor loadings, * indicates statistically significant  relationships at a p < .001 level. Analysis was done using R and  the open source statistical package lavaan.     Figure 2: Informal Learning Satisfaction MOOC Model      6. CONCLUSIONS  While a portion of the poor fit can be explained by a small sample  size, the variety of factor loadings ranging from .969 to -0.286 as  well as the poor fit indices clearly indicate a model that does not  accurately reflect a single construct. Content interaction, self- regulated supports and self-determination may have impact on  student satisfaction based on past research, but they do not appear  to contribute to the same satisfaction construct in our initial  analysis.     Further work is needed to define an informal learning satisfaction  construct that can be eventually analyzed in relation to other types  of MOOC participant satisfaction. Although the model tested in     this study did not result in strong evidence towards an informal  learning satisfaction model, it should be noted that these data  elements should now be considered as potential traces for other  satisfaction constructs. It is through this iterative development and  testing that a full MOOC Satisfaction Model can be ultimately  achieved.     7. RECOMMENDATIONS  Using learning analytics trace data, this study explored the  identification of informal learning satisfaction of MOOC  participants as part of a larger SEM model identifying multiple  satisfaction factors. Initial results indicate that content interaction,  self-regulation supports in the form of checklists, and self- determination may not help identify informal learning satisfaction  as originally predicted. While this result was unexpected, it was  the intent of this study to propose the analysis of MOOC design  from the perspective of student satisfaction. In that light, we  found that various MOOC design elements that may have been  designed to positively influence learning are not necessarily  working towards the same student motivations. Therefore, we  suggest future researchers continue to propose alternative  satisfaction factor models that can be ultimately incorporated and  analyzed into the larger MOOC Satisfaction Model.    8. REFERENCES    [1] Allen, I. E., and Seaman, J. 2014. Grade change: Tracking   online education in the United States. Babson Survey  Research Group and Quahog Research Group, LLC,  Oakland, CA. Available at  http://www.onlinelearningsurvey.com/reports/gradechange.p df   [2] Anderson, T., and McGreal, R. 2012. Disruptive pedagogies  and technologies in universities. Educ Technol Soc. 15, 4,  380-389. Retrieved from http://www.ifets.info/   [3] Baggaley, J. 2013. MOOC rampant. Distance Education. 34,  3, 368-378. DOI=  http://dx.doi.org/10.1080/01587919.2013.835768   [4] Belanger, Y., and Thornton, J. 2013. Bioelectricity: A  quantitative approach Duke Universitys first MOOC.  Available at  http://dukespace.lib.duke.edu/dspace/bitstream/handle/10161 /6216/duke_bioelectricity_mooc_fall2012.pdfsequence=1.   [5] Bragg, A. B. 2014. MOOCs: Where to from here Train Dev  J. 41, 1, 20-21.    [6] Browne, M. W., and Cudeck, R. 1993. Alternative ways of  assessing model fit. Sage focus editions. 154, 136-136.   [7] CotoNet. 2015. MOOC List. Available at https://www.mooc- list.com   [8] Deci, E. L., Vallerand, R. J., Pelletier, L. G., and Ryan, R.  M. 1991. Motivation and education: The self-determination  perspective. Educational psychologist. 26, 3-4, 325-346.   [9] Delclos, V.R. and Harrington, C. 1991. Effects of strategy  monitoring and proactive instruction on childrens problem- solving performance. J Educ Psychol. 83, 3542.   [10] Downes, S. 2012. The rise of MOOCs. Recuperado el, 1.    [11] Fini, A. 2009. The technological dimension of a Massive  Open Online Course: The case of the CCK08 course tools.  International Review of Research in Open and Distance  Learning. 10, 5. Available at  http://www.irrodl.org/index.php/irrodl/article/view/643/1402   [12] Heutte, J., Kaplan, J., Fenouillet, F., Caron, P.A., and  Rosselle, M. 2014. MOOC user persistence: Lessons from  French educational policy adoption and deployment of a  pilot course. In Learning Technology for Education in  Cloud. MOOC and Big Data, L. Uden, J. Sinclair, Y.H. Tao  and D. Liberona Ed. vol. 446. Springer International  Publishing, New York, NY, 13-24.   [13] Hoffman, E.S. and Menchaca, M.P. 2015. Personal learning  goals versus attrition in MOOCs: A learner framework for  MOOC 2.0. In World Conference on E-Learning (Kona,  Hawaii, 2015) Association for the Advancement of  Computing in Education.   [14] Hu, L. T., and Bentler, P. M. 1999. Cutoff criteria for fit  indexes in covariance structure analysis: Conventional  criteria versus new alternatives. Structural equation  modeling: a multidisciplinary journal. 6, 1, 1-55.    [15] Jordan, K. 2014. Initial trends in enrolment and completion  of massive open online courses. The International Review of  Research in Open and Distributed Learning. 15, 1, 133-160.  Available at  http://www.irrodl.org/index.php/irrodl/article/view/1651   [16] Kassabian, D. W. 2014. The value of MOOCs to early  adopter universities. EDUCAUSE Review. Available at  http://www.educause.edu/ero/article/value-moocs-early- adopter-universities   [17] Kevan, J., and Ryan, P. 2015. Experience API: Flexible,  decentralized and activity-centric data collection.  Technology, Knowledge and Learning. 1-7. DOI=  http://dx.doi.org/ 10.1007/s10758-015-9260-x   [18] King, A. 1991. Effects of training in strategic questioning on  childrens problem-solving performance. J Educ Psychol. 83,  307317.   [19] Knox, J., Bayne, S., Macleod, H., Ross, J., and Sinclair, C.  2012. MOOCs pedagogy: The challenges of developing for  Coursera. Association for Learning Technology Online  Newsletter. Available at  https://newsletter.alt.ac.uk/2012/08/mooc-pedagogy-the- challenges-of-developing-for-coursera/   [20] Koller, D., Ng, A., Do, C., and Chen, Z. 2013. Retention and  intention in massive open online courses: In depth. Educause  Review. 48, 3, 62-63. Available at  http://www.educause.edu/ero/article/retention-and-intention- massive-open-online-courses-depth-0   [21] Liyanagunawardena, T. R. 2014. MOOC experience: A  participant's reflection. ACM SIGCAS Computers and  Society. 44, 1, 9-14. DOI=  http://dx.doi.org/10.1145/2602147.2602149   [22] Liyanagunawardena, T. R., Adams, A. A., and Williams, S.  A. 2013. MOOCs: A systematic study of the published  literature 2008-2012. International Review of Research in  Open and Distance Learning. 14, 3, 202-227. Available at  http://www.irrodl.org/     [23] Martin, F. G. 2012. Education: Will massive open online  courses change how we teach (Viewpoints).  Communications of the ACM. 55, 8, 26. DOI=  http://dx.doi.org/10.1145/2240236.2240246   [24] Milligan, C., Littlejohn, A., and Margaryan, A. 2013.  Patterns of engagement in connectivist MOOCs. MERLOT  Journal of Online Learning and Teaching. 9, 2, 149-159.  Available at  http://jolt.merlot.org/vol9no2/milligan_0613.htm   [25] Pirani, J. 2013. A compendium of MOOC perspectives,  research, and resources. EDUCAUSE Review. Available at  http://www.educause.edu/ero/article/compendium-mooc- perspectives-research-and-resources   [26] Raykov, T., and Marcoulides, G. A. 2012. A first course in  structural equation modeling. Routledge. DOI=  http://dx.doi.org/10.4324/9780203930687   [27] Reich, J. 2014. MOOC completion and retention in the  context of student intent. EDUCAUSE Review. Available at  http://www.educause.edu/ero/article/mooc-completion-and- retention-context-student-intent   [28] Reich, J. 2015. Rebooting MOOC research. Science. 347,  6217, 34-35. DOI=  http://dx.doi.org/10.1126/science.1261627   [29] Siemens, G. 2005. Connectivism: A learning theory for the  digital age. International Journal of Instructional  Technology and Distance Learning. 2, 1, 3-10. Available at  http://www.itdl.org/Journal/Jan_05/article01.htm   [30] Siemens, G. 2012. MOOCs are really a platform [Blog].  Elearnspace. Available at   http://www.elearnspace.org/blog/2012/07/25/moocs-are- really-a-platform/   [31] Siemens, G. 2013. Massive Open Online Courses:  Innovation in education, In Open Educational Resources:  Innovation, Research and Practice, R. McGreal, W.  Kinuthia and S. Marshall Ed. Vancouver, BC Canada:  Commonwealth of Learning and Athabasca University. 5-16.   [32] Stein, R. M., and Allione, G. 2014. Mass attrition: An  analysis of drop out from a Principles of Microeconomics  MOOC. PIER Working Paper 14-031: Penn Institute for  Economic Research. DOI=  http://dx.doi.org/10.2139/ssrn.2505028   [33] Vardi, M. Y. 2012. Will MOOCs destroy academia (Editor's  Letter). Communications of the ACM. 55, 11, 5. DOI=  http://dx.doi.org/10.1145/2366316.2366317   [34] Wiley, D. 2012. The MOOC misnomer [Blog]. Iterating  toward Openness. Available at  http://opencontent.org/blog/archives/2436   [35] Yang, D., Sinha, T., Adamson, D., and Rose, C. P. 2013.  Turn on, tune in, drop out: Anticipating student dropouts in  massive open online courses, in Proceedings of the 2013  Annual Conference on Neural Information Processing  Systems, Data-Driven Education Workshop (Lake Tahoe,  Nevada, 2013) 10, 13.   [36] Yuan, L., Powell, S., and Cetis, J. 2013. MOOCs and open  education: Implications for higher education.   [37] Zheng, S., Rosson, M. B., Shih, P.C., and Carroll, J.M. 2015.  Understanding student motivation, behaviors and perceptions  in MOOCs. ACM Press. 1882-1895. DOI=  http://dx.doi.org/10.1145/2675133.2675217        "}
{"index":{"_id":"36"}}
{"datatype":"inproceedings","key":"Ostrow:2016:ALI:2883851.2883872","author":"Ostrow, Korinn S. and Selent, Doug and Wang, Yan and Van Inwegen, Eric G. and Heffernan, Neil T. and Williams, Joseph Jay","title":"The Assessment of Learning Infrastructure (ALI): The Theory, Practice, and Scalability of Automated Assessment","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"279--288","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883872","doi":"10.1145/2883851.2883872","acmid":"2883872","publisher":"ACM","address":"New York, NY, USA","keywords":"assessment of learning infrastructure, automated analysis, randomized controlled experiments at scale, the assistments testbed, tools for learning analytics, universal data reporting","abstract":"Researchers invested in K-12 education struggle not just to enhance pedagogy, curriculum, and student engagement, but also to harness the power of technology in ways that will optimize learning. Online learning platforms offer a powerful environment for educational research at scale. The present work details the creation of an automated system designed to provide researchers with insights regarding data logged from randomized controlled experiments conducted within the ASSISTments TestBed. The Assessment of Learning Infrastructure (ALI) builds upon existing technologies to foster a symbiotic relationship beneficial to students, researchers, the platform and its content, and the learning analytics community. ALI is a sophisticated automated reporting system that provides an overview of sample distributions and basic analyses for researchers to consider when assessing their data. ALI's benefits can also be felt at scale through analyses that crosscut multiple studies to drive iterative platform improvements while promoting personalized learning.","pdf":"The Assessment of Learning Infrastructure (ALI):  The Theory, Practice, and Scalability of Automated Assessment     Korinn S. Ostrow, Doug Selent, Yan Wang,   Eric G. Van Inwegen, Neil T. Heffernan  Worcester Polytechnic Institute   Worcester, MA 01609  {ksostrow, dseslent, ywang14,   egvaninwegen, nth} @wpi.edu    Joseph Jay Williams  VPAL Research    Harvard University  Cambridge, MA 02138   joseph_jay_williams@harvard.edu    ABSTRACT  Researchers invested in K-12 education struggle not just to  enhance pedagogy, curriculum, and student engagement, but also  to harness the power of technology in ways that will optimize  learning. Online learning platforms offer a powerful environment  for educational research at scale. The present work details the  creation of an automated system designed to provide researchers  with insights regarding data logged from randomized controlled  experiments conducted within the ASSISTments TestBed. The  Assessment of Learning Infrastructure (ALI) builds upon existing  technologies to foster a symbiotic relationship beneficial to  students, researchers, the platform and its content, and the  learning analytics community. ALI is a sophisticated automated  reporting system that provides an overview of sample  distributions and basic analyses for researchers to consider when  assessing their data. ALIs benefits can also be felt at scale  through analyses that crosscut multiple studies to drive iterative  platform improvements while promoting personalized learning.   Categories and Subject Descriptors  K: Applications to Education. K.3: Computers and Education.   I.2.2: Automatic Programming. G.3: Probability and Statistics.   General Terms  Measurement, Documentation, Experimentation, Standardization.   Keywords  Assessment of Learning Infrastructure, Automated Analysis,  Randomized Controlled Experiments at Scale, The ASSISTments  TestBed, Universal Data Reporting, Tools for Learning Analytics.   1. INTRODUCTION  An immense community of researchers, educators, and  administrators seeks to enhance the effectiveness of educational  practices. Those invested in K-12 education struggle not just to  enhance pedagogy, curriculum, and student engagement, but also  to harness the power of technology in ways that will optimize  learning. Researchers often fall back on observational studies or  turn to data mining large longitudinal datasets due to the  difficulties inherent to conducting student-level randomized   controlled experiments (RCEs) in authentic learning  environments. Software for sharing educational data has driven  tremendous progress in educational research and best practices.  For instance, the Pittsburgh Science of Learning Centers  DataShop [8], funded by the National Science Foundation,  provides an extensive database of educational datasets for post  hoc data mining and analysis. However, the pace and power of  educational research would increase drastically if researchers had  easier access to environments in which they could design,  implement, and analyze hypothesis driven experiments. The RCE  remains the gold standard in determining causal relationships  and was referred to when the U.S. Department of Education  advocated for K-12 schools to apply basic findings from cognitive  science to improve educational practices [16]. Without the  assistance of scalable technologies, it has been difficult for  researchers to answer the call to conduct RCEs within authentic  academic settings [6] due to the high cost of establishing and  maintaining sample populations, the complications inherent to  randomization at the teacher-level (i.e., vast samples are required),  and the often invasive curriculum restrictions necessary to  establish sound controls.     When designed with flexibility and collaboration in mind, online  learning platforms offer a unique and scalable approach to  educational research and data analysis. Users of online learning  platforms (i.e., students and teachers) create hundreds of  thousands of data points each day, with databases of rich learner  information growing exponentially as platforms gain popularity  and validity as powerful learning aids. Beyond achievement  measures, these systems provide opportunities to collect  information including (but not limited to) behavior and affect [2,  17], learning interventions within content or feedback [14, 15],  and interactions between skill domains that help guide curriculum  development [1]. Through flexibility in content design,  manipulation, and delivery, researchers are able to tap into the  elements that drive effective learning within authentic K-12  classroom environments. When content can be manipulated to  include parallel assignments, fashioned as conditions within  RCEs, researchers are able to determine best practices and work  toward personalized learning. Further, designing these  environments with the open, collaborative, and perhaps even  competitive design of RCEs in mind can strengthen internal  validity and promote open source data reporting for review and  replication of findings upon publication [11]. By allowing data  scientists, educational researchers, and K-12 educators to work  collaboratively within online learning platforms, all are  empowered to dynamically evaluate and improve the  effectiveness of the platform and its content while fostering  growth in learner analytics.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components of  this work owned by others than ACM must be honored. Abstracting with  credit is permitted. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom     2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00    DOI: http://dx.doi.org/10.1145/2883851.2883872   mailto:Permissions@acm.org http://dx.doi.org/10.1145/2883851.2883872      1.1 Research in the ASSISTments TestBed  ASSISTments is a unique online learning platform that was  designed with educational research as one of its primary goals [5].  The platform is used for both classwork and homework by over  50,000 users around the world, and provides students with  immediate feedback and rich tutorial strategies and teachers with  powerful assessment through a variety of reports that pinpoint  where students are struggling and empower data driven teaching  [5]. Recent funding from the NSF has allowed ASSISTments to  promote educational research at scale through the development of  the ASSISTments TestBed (www.ASSISTmentsTestBed.org).  External researchers can use the TestBed to embed studies within  ASSISTments content and non-invasively tap into our user  population at virtually no cost and in a fraction of the time  previously required to run experiments within K-12 environments.   The process of conducting an RCE within the TestBed typically  involves researchers modifying preexisting certified content to  include treatment interventions and student-level random  assignment. The latter feature makes the TestBed a unique and  robust tool for conducting research; rather than delivering the  same treatment condition to all students within a particular class,  students in the same class will be randomly assigned to different  conditions while participating in the same assignment (i.e.,  content, feedback, or delivery may vary from student to student).  The library of certified ASSISTments content consists primarily  of middle and high school mathematics skills, with content  organized and tagged by Common Core State Standard [10].  However, this library has grown to include content in physics,  chemistry, and electronics, and researchers are able to develop  their own content for experimentation in other domains.    Figure 1 depicts a simple study design implemented within the  ASSISTments TestBed.  Inclusion of a student in this type of  study is dependent on her ability to access video content (note that  many schools block video servers like YouTube). When the  student begins her assignment, she must first pass a Video  Check, or a standard problem that serves as password protection  to study participation.  If the  student can  access video,  she enters        Figure 1: A simple research design that can be built using the   ASSISTments TestBed to compare learning interventions.   the password provided in the short clip as her answer, and her  correct response serves as the Then in an If-Then routing   structure. If the student enters anything other than the password as  a response, she is provided a default assignment without video  content and is not considered a study participant. While this  process attempts to control for technical issues, it does not  demand the fidelity of study participants (i.e., we cannot currently  track viewing statistics for embedded videos). Upon being routed  into the study depicted in Figure 1, students are randomly  assigned into one of two conditions using a Choose Condition  routing structure. Note that although two conditions are presented  here for simplicity, the system is able to compare any number of  conditions. The platforms approach to random assignment will be  discussed further in Section 3.1.2.   In the present example, there are three possible paths that a  student may follow as she progresses through her assignment (the  specific trace of these paths will become important in the  automated reporting and analysis of student performance  presented in Section 3). For each student, regardless of path,  ASSISTments logs substantial data detailing performance as the  student progresses through the assignment. This data includes  binary measures of problem accuracy (i.e., a correct or incorrect  first response), the students first action (i.e., an attempt vs.  requesting tutoring), the number of attempts per problem, the  number of feedback interactions per problem (i.e., hints requested  or scaffolds seen), whether or not the student saw the bottom out  hint (i.e., the correct answer, provided to keep the student from  getting stuck within the assignment), and start and end times for  each problem. For researchers with a fine-toothed comb,  ASSISTments can also provide logged information at the action  level, detailing each step taken within a problem. ASSISTments is  also able to track user information that is ultimately helpful to  researchers, including data on the students performance in the  system prior to their inclusion in a study, student characteristics  (i.e., gender, age), and additional variables at the class and school  levels. Through use of the TestBed, this information is  consolidated, anonymized, and provided to researchers through  unified reports (depicted in Section 3.1.1) to enhance the ease  with which RCEs are conducted at scale.      1.2 Utility of Automated Data-Preprocessing  With students accessing experiments naturally in authentic  learning environments, sample populations increase as a function  of time. For instance, within three months of deploying a study  within ASSISTments, a researcher may accrue 740 participants.  This process does not require direct interaction between  researcher and teachers, although some researchers choose to  work directly with local classrooms to establish stronger controls.  As external researchers are unfamiliar with the ASSISTments  database and the inner workings of the platform, universal data  reporting and preprocessing techniques were designed to ease the  hurdle of interpreting system output. Without preprocessing, a  researcher analyzing data from the study depicted in Figure 1  would need to use raw data to decipher whether students should  be included in analyses, what condition each student experienced,  details pertaining to each students experience within that  condition (i.e., how many problems were completed, their content,  and all associated performance data), and how each student  performed at posttest. While such rich information is helpful in  analyzing a study, providing researchers with a surplus of data  necessitates larger and more complex datasets that must still meet  ease of use requirements. Although different researchers focus on  different information (as it applies to their particular hypotheses),  an infrastructure for data preprocessing, restructuring, and        reporting was necessary to bring ASSISTments to the next level  as a shared scientific instrument for educational research.   In the following sections we discuss the creation of an automated  reporting and analysis system built to provide researchers with  data logged from RCEs conducted within the ASSISTments  TestBed. The Assessment of Learning Infrastructure (ALI) builds  upon existing technology to foster a symbiotic relationship  beneficial to students, researchers, the platform and its content,  and the science of learning. Evolving from a universal data  logging and retrieval tool, ALI is quickly becoming a  sophisticated system for automated analysis, offering researchers  an overview of their sample population and conducting a selection  of analyses for consideration when assessing data. The benefits of  ALI can also be felt at scale, with analyses spanning content to  drive platform improvements with the long-term goal of  personalizing learning.   2. ALI IN THEORY  The Assessment of Learning Infrastructure is an automated  research assistant that, while not meant to replace the researcher,  is meant to lighten the load of working with large data files output  from RCEs conducted within the ASSISTments TestBed. ALI  alerts the researcher to new data, presents that data in a  meaningful way, tentatively examines effects observed between  conditions, and flags potential threats to validity. On a weekly  basis, as well as on demand, ALI consults all logged information  pertaining to a study and conducts preliminary analyses on student  participation and performance (described further in Section 3).  The potential benefits of automated reporting and analysis are  broad; in the next four sections we briefly discuss how ALIs  success will affect ASSISTments and its users, researchers and the  Testbed, and the greater learning analytics community.   2.1 Benefits to ASSISTments Users  ALIs work at scale will help to guide the development of  stronger learning interventions and, eventually, drive personalized  learning within ASSISTments. Research conducted within the  TestBed is unique in that while researchers are able to alter  content and deliver versatile interventions as previously  exemplified in Figure 1, such manipulations are not invasive.  Study participation and student performance within an assignment  is passively logged. A student may notice that some of her  assignments include video feedback or have extra survey  questions while others do not, but she is not informed that she is  participating in an RCE. A primary goal driving the TestBeds  ability to implement RCEs within ASSISTments is the provision  of normal instructional practice and interventions that do not  compromise learning.    ALI is also beneficial to teachers, as the infrastructure is able to  separate rich study information from daily assessment data.  Teachers are responsible for assigning content within  ASSISTments to their students. Although it seems as though  research designs created in the TestBed would complicate daily  assessment, class and student reports have been designed such that  teachers are provided pertinent information in a clean and concise  manner. This low profile approach to conducting research  maintains a highly participatory subject pool. Teachers wishing to  conduct action research within their classes may do so by working  with the TestBed as well, although most prefer to use day-to-day  reports to guide their teaching practices rather than large  automated data files.   2.2 Benefits to the Researcher  For those conducting RCEs within the ASSISTments TestBed,  ALI plays the role of research assistant. The infrastructure  intelligently communicates with researchers when new data is  available for analysis and provides an overview of the sample  distribution across conditions to signify the power of current  analyses. Although researchers will undoubtedly run their own in  depth analyses, standard high-level analyses can be automated to  save time and reduce monotony. For example, ALIs ability to  trace a students path through an assignment allows the  infrastructure to infer what condition the student experienced.  This allows ALI to test for differential attrition rates across  conditions and notify the researcher of apparent selection biases.   This simple analysis can serve as a beneficial warning against  analyzing posttest results due to potential threats to internal  validity. Combined with the data preprocessing and sophisticated  reporting that ALIs analytics are built upon, these notifications  are often enough to save researchers from hours of wasted labor.    2.3 Benefits to the Platform  When considered at scale, ALIs capabilities for data reporting  and analysis contribute to the enhancement of the ASSISTments  platform by supporting practical improvements to content and  feedback without interrupting student learning. As researchers  collaborate and compete to design interventions within the  ASSISTments TestBed, it will grow increasingly possible to  evaluate interventions at scale, both across skills and  longitudinally within students. Ideally, the best version of content  and delivery observed (to date) for a particular skill would be  delivered to students as the control condition in new RCEs.  Through this approach, each study offers the potential for iterative  improvement as experiments are launched and re-launched,  capturing key features of design-based educational research  methodology [3]. Such improvements additionally benefit users  through the predicted outcome of enhanced learning gains and  researchers through the rapid succession and enhanced validity of  positive findings.    ALIs ability to analyze at scale will also help the ASSISTments  team to quickly isolate and remove ineffective interventions. It is  our goal that in the near future, ALI will conduct robust analyses  across multiple studies while considering student, class, and  school level characteristics. Roughly speaking, ALI will allow  ASSISTments to personalize learning by better understanding  why certain educational practices and interventions work for  certain students but not for others.    2.4 Benefits for Learning Analytics  How can ALI and the promotion of infrastructures like ALI within  other learning platforms benefit the learning analytics  community At its very core, ALI answers the general call of  learning analytics, in that the infrastructure emphasizes  measurement and data collection as activities that institutions need  to undertake and understand, and focuses on the analysis and  reporting of the data [20]. A strong focus on providing universal  measures of learning garnered from authentic learning  environments will strengthen the validity of findings from a broad  range of interventions that seek to isolate best practices in  education.   Further, much attention in the broader scientific and psychological  research communities has recently befallen the general inability to  replicate research findings [7, 11]. The same is likely true for  educational research, with little emphasis placed on data  accountability. Perhaps the best outlet for promoting open data,        the Pittsburgh Science of Learning Centers Data Shop [8] takes a  number of steps in the right direction with regard to shared  datasets that promote open, replicable, and sound science. ALI  builds upon the PSLCs model of open data reporting by  establishing stable, timestamped links to every data analysis  report ever provided to a researcher throughout the duration of  their work within ASSISTments. Researchers are asked to cite the  report from which they draw data for final analyses and  publication (explained further in Section 3.1.5).  References to  these reports will also drastically increase the availability of  preprocessed and anonymized educational datasets for researchers  wishing to mine big data without designing specific interventions.   In some ways, ALI is also an extension of industry track research  focused on learning analytics; companies like Google and  Microsoft increasingly implement large-scale experimentation in  online learning environments to consider reporting metrics and  analytic methods that meet practical goals rooted in scientifically  sound evidence [9]. If infrastructures like ALI were incorporated  into other learning platforms, similar large-scale experimentation  could easily be promoted for its importance to learning analytics.   3. ALI IN PRACTICE  The Assessment of Learning Infrastructure has grown  considerably over the past year. ALI began as a robust SQL query  to the ASSISTments database to retrieve unified information  across multiple studies and to present it to researchers in a single  format. Ease of use requirements, communication considerations,  and feedback from external researchers has helped ALI to grow  beyond data preprocessing and reporting into a tool for learning  analytics at scale. The following sections discuss how ALI has  evolved and provides examples of the infrastructures current  capabilities in reporting, analyzing, and communicating data from  RCEs conducted within the ASSISTments TestBed.    3.1 ALIs Current Capabilities  3.1.1 Data Reporting at Scale  When a researcher submits a study to the ASSISTments TestBed,  details about the study and the researchers contact information  are entered into ALIs study repository. Although researchers can  request immediate data analysis reports on demand, ALI defaults  to a weekly inspection of each study in the database and makes a  decision regarding whether or not to process a data analysis report  for the researcher. This decision is based on measured increases in  sample size. Due to common curricula structures, certain skills are  only used at specific times of year and thus, an assignment with  an embedded study may be highly popular during the Fall term  but not the Spring term. When ALI inspects the studys logged  data, at least three new participants since the last ALI  communication are required to trigger a new data report.   As teachers using ASSISTments are able to make copies of  assignments and alter their content, ALI is also able to detect  when teachers have assigned a copy of a study. ALI is  sophisticated enough to recognize when a copy is identical to the  original study and include data associated with the copy in each   report. If a copy of the study has been altered (i.e., problems were  removed or sections were changed), ALI does not report data  associated with the copy. This ensures that researchers receive all  data associated with their experiment without corrupt data.      Once ALI has determined that new data is available, several  robust SQL queries are run on the ASSISTments database. Three  major queries are used to a) retrieve student data detailing student,  class, and school level characteristics for each student recorded  prior to random assignment (see Table 1; field definitions are  beyond the scope of this paper but are available in our glossary at  [13] for additional reference), b) retrieve problem level data (see  Table 3), and c) detect the problem set structure (i.e., the paths  depicted in Figure 1) for each student with logged data. These  three queries provide ALI with the information necessary to  establish reports and conduct automated analysis. By working  closely with researchers throughout the development of ALI, we  have designed four different universal data representations in an  attempt to meet dynamic research needs. Subsets of data  exemplifying each type of report are provided below. Table 2  shows fields typical to the Action Level file. This file offers the  finest granularity of data logged by ASSISTments as a student  works through an assignment. Each row provides information  pertaining to a single step within a problem (i.e., when the  problem is initiated, or when the student asks for a hint).  A subset  of the Problem Level file is depicted in Table 3. This file provides  the same data as that found in the Action Level file, but the  granularity has increased. Each row provides information  pertaining to a single problem, with actions collapsed across  columns. Student Level files, as depicted in Table 4, offer the  coarsest granularity of data reporting. In this type of file, each row  provides information pertaining to the entire assignment for a  single student. For each feature or action, problem information is  presented across columns in the order in which the student  experienced the assignment, with the number of columns for each  feature extrapolated to the maximum number of problems  experienced by any student in the file. An alternative version of  Student Level data is also provided in which each student  assignment is represented by a series of rows, each representing a  feature for problems displayed across columns (akin to a pivot  table of the file described in Table 4). Full examples of each data  file are available at [13] for further consideration. Links to each  data file are gathered and presented to the researcher in a single,  organized communication, depicted in Figure 2 and discussed  further in Section 3.1.5.    When preprocessing is complete and all data files have been  compiled, ALI sends analytic commands to Rserve, an extension  to the R programming language that allows for other applications  to call R functions via a TCP/IP connection [19]. The  ASSISTments team created a client side API to interact with  Rserve, allowing ALI to send requests to R.  Because Rserve is  not multithreaded, several instances of Rserve run on separate  ports on the ALI server. The server is designed to recycle existing  connections, with a connection pool equal to the maximum  number of threads used by ALI. This allows several data     Table 1: A theorized subset of student historical data. Each row contains student, teacher, and school characteristics linked to a  particular student, using information sourced prior to random assignment   Student Class ID Grade School ID Guessed Gender  Birth  Year   Prior HW  Completion %   Prior Class HW  Completion %   Normalized HW  Mastery Speed   A 1007475 8 5597 Male 2001 0.83 0.88 0.33  B 1180278 8 5597 Male 2001 0.76 0.88 0.03  C 1180278 8 5597 Male 2001 0.76 0.88 0.03  D 1322778 7 2342 Female 2002 0.95 0.97 -0.39        Table 2: A theorized subset of an action level data file. Each row represents a single action within a single problem as experienced  by a student. This is the finest granularity of data reported by ALI   Student Problem ID Sub-Problem ID Order Action Type Timestamp Answer Correctness  A PRAUVJS 806533 1 Start 08/26/15 15:25:26 -- --  A PRAUVJS 806533 2 Hint 08/26/15 15:25:52 -- --  A PRAUVJS 806533 3 Answer 08/26/15 15:26:40 18.2 TRUE  A PRAUVJS 806533 4 End 08/26/15 15:26:42 -- --  A PRAVKJX 833840 1 Start 08/26/15 15:26:43 -- --      Table 3: A theorized subset of a problem level data file. Each row contains all the information linked to a single problem as   experienced by a student. This is a popular form of data for student modeling and analytics  Student Assignment ID Problem ID Correct Answer  Hints Attempts Start Time End Time   A 1007475 PRAUVJS 1 18.2 0 1 08/26/15 15:25:26 08/26/15 15:26:42  A 1007475 PRAVKJX 1 14.3 0 1 08/26/15 15:26:43 08/26/15 15:27:45  A 1007475 PRAVKHT 1 6.4 0 1 08/26/15 15:27:50 08/26/15 15:28:47  B 1180278 PRAUVJX 0 22.8 2 3 08/26/15 17:14:22 08/26/15 17:15:42  B 1180278 PAVKGZ 0 7.2 0 2 08/26/15 17:15:43 08/26/15 17:17:31      Table 4: A theorized subset of a student level data file. Each row contains all information linked to a single students experience of   the problem set. Assignment information is presented across columns in the order in which the student experienced problems  Student Assignment ID Late Mastered Correct Q1 Correct Q2 Correct Q3 Answer Q1 Answer Q2 Answer Q3   A 1007475 1 1 1 1 1 18.2 14.3 6.4  B 1180278 0 0 0 0 1 17 14.1 6.4  C 1180278 1 0 0 1 -- 24.6 14.3 --  D 1322778 0 1 1 1 1 18.2 14.3 6.4     analysis reports to occur simultaneously, all using different  Rserve connections. This approach lowers the turnaround time  when a researcher actively requests data. It also keeps weekly  reporting as efficient as possible, as all datasets in ALIs study  repository are assessed weekly for potential reporting.   3.1.2 Smart Structures  In order to determine what to analyze, ALI must first process the  structure of a study and trace each students path through the  assignment (as previously discussed in relation to Figure 1). As  ALI parses the assignments structure, the infrastructure is able to  make intelligent decisions upon meeting certain section types  within the design. This is accomplished by recursively generating  the assignments reported structure into tree form. Within the  Problem Level data file presented in Table 3, each problem is  labeled with a path, similar to that used when traversing a set of  folders within an operating system. ALI steps through each  problem path for each student to establish an intuitive structure of  the study and to cluster students by condition.   RCEs within the ASSISTments TestBed are designed by taking  advantage of a variety of section types offered by the platform.  The If-Then routing discussed in Section 1.2 was an example of  a section type. When ALI observes an If-Then structure that  issues a routing standard like a Video Check, the infrastructure  intelligently conducts its analyses on students assigned to the  study and disregards students routed to alternative content.   Similarly, studies often employ parallel experimental and control  conditions delivered using a section type referred to as a Choose  Condition. This section type is used to drive random assignment.  The Choose Condition depicted in Figure 1 included two  parallel conditions: an assignment with video content and a  control assignment with traditional text content. Currently, in  order for ALI to recognize an assignment as a research study, a  Choose Condition must be present when mapping the  assignments structure. ALI then assesses logged data within each  condition and considers any section immediately following these  conditions as a subsequent posttest (see Figure 1). Using this   information, ALI is able to aggregate statistics and perform a  selection of simple analyses across problems and students.    It is important to note that research designs within the  ASSISTments TestBed can grow far more complex than the  simple structure presented herein. When assignments include  nested section types and multiple If-Then routing standards,  ALI currently has difficulty interpreting condition and isolating  posttest content. In its current form, ALI is only meant to assist  researchers with the analysis of common design patterns. Future  work, discussed in Section 5, will expand ALIs ability to  intelligently parse studies using tagging rules set forth by the  researcher.   3.1.3 Selection Bias  After establishing a studys structure and sample distribution, ALI  is able to assess assignment completion rates across conditions  and alert researchers to potential threats to internal validity due to  selection bias. ALI records the observed number of students in  each condition that began the assignment, and considers logged  assignment end times to consider the proportion of students that  ultimately completed the assignment. The observed distribution is  then compared to the expected distribution of proportional  attrition in a normal sample. A Chi-squared analysis is used to  determine if the observed distribution of attrition significantly  differs from the expected distribution. ALI then flags conditions  that have a reliably different attrition rate and alerts the researcher  of a potential threat to internal validity. Without considering  differential attrition across conditions, an analysis of posttest  performance may inaccurately suggest the significant effect of a  particular condition that was actually driven by the  disproportionate loss of weaker students. This simple analysis,  presented to researchers as shown in Figure 3, may help even the  most seasoned experts to accurately assess their sample. It is  important to note that while ALI provides this warning, the  infrastructure still releases all data to the researcher and never  prohibits the researcher from further analysis. The goal of ALIs  selection bias assessment is not to impede or prevent analysis, but  rather to advocate sound analytic practices.           Raw Data Files    Raw data files contain the logged information for each student that has participated in your study. We provide this data in a variety of formats, as explained  below, to assist in your analytic efforts. We use Google Docs to share these files with you. If you would like to process these files manually, we recommend  downloading the CSV file of your choice and saving the file as an Excel spreadsheet or workbook to retain formatting and formulas. If you will be passing  the file directly to a statistical package, downloading the CSV to a convenient location should suffice.    For a field glossary and tutorials on how to read each type of file, visit our Data Glossary.    Historical Data  Covariate File - A collection of useful covariates for the students participating in your study. This file includes student level variables (i.e., gender), class  level variables, (i.e., homework completion rates), and school level variables (i.e., urbanicity). Click here for a tutorial on how to link this file to your  experimental data.    Experimental Data  1. Action Level - One row per action per student; the finest granularity. Students participating in your study have performed 13,655 actions (e.g.,   beginning problems, attempting to answer problems, asking for tutoring, and eventually completing problems).  2. Problem Level - One row per problem per student. Students participating in your study have completed 2,280 problems. The flow through a single   problem incorporates many actions, resulting in a coarser data file (fewer rows).  3. Student Level - One row per student; the coarsest granularity. Columns are laid out in opportunity order to depict the students progression through the   problem set. Problem level information is expanded to one column per problem per field (column heavy).  4. Student Level + Problem Level - One row per field per student. Columns are laid out in opportunity order to depict the students progression through   the problem set. An alternative view of student level information (row heavy).     Figure 2: A thoroughly developed universal reporting of logged data from students participating in RCEs. Each file presented here  is discussed further, including depictions of file subsets, in Section 3.1.1.        The Assessment of Learning Infrastructure (ALI)    Completion Rates  Students that have started your study: 329  Students that have completed your study: 251     Bias Assessment  Before analyzing learning outcomes, we suggest first assessing potential bias introduced by your experimental conditions (i.e., examine differential attrition).  The table below reports the number of students that have completed your study, split out by experimental condition.     Condition Started (n) Completed (n) Completed (%)  Group A  Experiment 1 109 80 73.39  Group B  Experiment 2  87 60 68.97  Group C  Control  99 89 89.90  Total 295 229 77.63     NOTE: A significant difference was found between observed and expected completion rates across conditions, 2 (2, N = 295) = 13.467, p < .01. This means  that a selection effect may have occurred. Hypothesis testing with regard to posttest scores has not been conducted out of an abundance of caution.    Mean and Standard Deviation of Posttest Score by Condition  To examine learning outcomes at posttest, an analysis of means was conducted across conditions. The table below reports mean posttest score and standard  deviation for each condition. This information was sourced from our automated posttest sub-report.     Completed (n) Posttest Score*  Group A  Experiment 1 80 34.40 (4.34)  Group B  Experiment 2 60 32.95 (3.89)  Group C  Control  89 44.11 (3.72)  Total 229 37.15 (3.98)  * Presented as Mean (SD).     Figure 3: Current ALI analytic reporting.  Available analyses include a Chi-squared test comparing the observed and expected  sample distributions, simple hypothesis testing, and an analysis of means on posttest performance between conditions. Note that  these analyses are currently driven by the structure of the assignment as parsed by ALI from Problem Level data.  Future work   includes allowing researchers to tag their study with items of interest to automate analysis with greater sophistication.         3.1.4 Simple Hypothesis Testing  After conducting a selection bias assessment, ALI progresses to a  set of simple hypothesis tests with regard to posttest performance.  If ALI detects a posttest section when parsing an assignments  structure, the infrastructure compares performance across  conditions by referring to the previously aggregated group  distributions. ALI approaches posttest analysis much like a  researcher would: if only two conditions are detected within the   study, ALI conducts a t-test, while if more than two conditions are  detected, ALI conducts an Analysis of Variance (ANOVA). ALI  currently has the API to support simple univariate and  multivariate analyses including ANOVA, ANCOVA, MANOVA,  and MANCOVA. ALI stores all input parameters for a given  statistical test in a single object. The parameters are extracted  from this object and transformed into the appropriate R function  calls through the Rserve API communication. Results are  accumulated and presented to the researcher alongside an analysis        of means, as shown in Figure 3, allowing the researcher to  observe the direction of the reported effect. Note that in the  present example, ANOVA results are not presented to the  researcher out of an abundance of caution due to ALIs detection  of a potential selection bias. Our goal in restricting this  information is strictly in the promotion of sound scientific inquiry.  It should also be noted that covariates are not presently considered  in ALIs hypothesis testing. Future work will control for student,  class, and school level characteristics sourced from the historical  student data file (see Table 1) by using ANCOVA or MANCOVA  approaches in an attempt to explain additional variance in learning  outcomes.   3.1.5 Data Storage and Researcher Output  When ALIs automated analysis is complete, ALI stores all data  files and analytic output on Google Drive in archival quality. This  data cannot be altered but can be downloaded by anyone. For  active studies, copyright protection will be placed on new data  analysis reports for one year from the studys initial run date.   This means that researchers will have a full calendar year to  publish on their findings before their data becomes freely  available to the public.    ALI communicates to researchers via email, providing a link to a  stable URL for a Google Doc housing that weeks data analysis  report. The Doc contains links to all raw data files, as shown in  Figure 2, and provides automated analysis as depicted in Figure 3.  The creation of this Google Doc is automated, based on an HTML  template file that uses custom tagging conventions to insert  variables with dynamic text or data. Using this method, the same  report can be generated multiple times or across multiple  assignments with changes to only the pertinent information. This  allows for customized reporting based on the results of ALIs  analysis. The Google Doc report also provides researchers with  links to additional resources including a glossary explaining  features of the data and video tutorials on how to understand each  file type (available at [13]).    When researchers are ready to publish findings, a condition of  working with the ASSISTments TestBed requires that they  include a reference in their work to the stable record from which  they sourced the data files used for final analyses. This approach  allows reviewers and secondary researchers to gain access to raw  study data, thereby encouraging replication and open science  [11]. In addition to the raw data, secondary researchers will also  be able to use these references to access ALIs analytic report,  including all automated analyses.   4. ANALYSIS AT SCALE  Although ALIs analytic structure is still somewhat rudimentary,  considered at scale, comparisons of findings from multiple studies  can offer substantial insights for the ASSISTments platform and  in more general terms, for the learning analytics community. By  simultaneously examining attrition outcomes across studies it  becomes possible to make claims about the quality of  interventions that crosscut multiple skills. As ALIs analytical  capabilities increase, analysis at scale will grow even more  powerful.    As a proof of concept of the potential benefits of automated  analysis at scale, ALI was run across a special dataset including  25 studies that are currently running within ASSISTments. This  file was created for another sophisticated approach to modeling  student performance across multiple studies [18], but serves as a  perfect example of ALIs capabilities at scale. In the spirit of open   data, this file is available for reference at [12]. The studies in this  file were selected from a group of 126 studies currently running  within the ASSISTments platform based on the following criteria:   Studies selected contained at least 50 students within each   condition that completed the assignment.   Studies selected were designed within Skill Builders, a   mastery learning based assignment that considers predefined  thresholds for student completion (i.e. by default, to  complete the assignment the student must solve three  consecutive problems accurately).     As most of the studies in this file were built prior to the  implementation of automated path-logging (which drives ALIs  ability to read in the structure of the study and infer a condition  for each student), condition was manually traced and logged for  each student based on his or her observed problem sequence. A  number of these studies were also built before the availability of  If-Then routing and subsequent checks for internal validity (i.e.,  the Video Check explained in connection to Figure 1). As such,  it is difficult to tell if students experienced technical difficulties  during the course of a condition. To analyze this dataset using all  of the capabilities that ALI has with recently designed studies, we  manually notated flags regarding the observed fidelity of  conditions. This flagging also included whether students tested  out of the condition experience (i.e., if a student was assigned to  a condition in which the treatment was presented through  feedback but answered the first 3 consecutive problems  accurately, they did not ultimately experience the treatment). As  only three of the studies in this file contained valid posttest  information, we only present ALIs selection bias assessment for  consideration at scale (see Table 5).    The 25 studies presented in Table 5 span a variety of  investigations including: assessing the effect of various types of  video tutoring (i.e., pencasts, teacher recorded instruction, online  resources) compared to traditional text-based tutoring across  multiple designs (i.e., using scaffolding, using hints, as an  intervention to wheel-spinning [2], or provided based on student  choice), investigating the manipulation of content (i.e.,  interspersing learning with humor through comics in content or  feedback, asking students to gauge their confidence in solving  problem content, and altering student mindset (as inspired by [4]),  and challenging cognitive principles (i.e., mental representations,  and alterations in the consistency of math equations). Assignment  names, as presented in Table 5, are tagged with the grade level  and domain of the skill content as defined by Common Core State  Standards [10]. Despite differences in domain and  experimentation, ALI is able to provide a sense of condition  quality across studies at scale.   The results of the simple Chi-squared analyses in Table 5 may not  seem significant at first, but are actually quite insightful at scale.  In studies with two conditions, experiment vs. control (20  comparable sets of the 25 shown in Table 5), the control groups  showed less attrition in 15, while the experimental groups showed  less attrition in only five. On its own, this comparison suggests  that experimental conditions correlate with higher attrition rates.  However, this attrition is only significantly different than that of a  normally distributed sample in five studies  (p < .05), with  experimental conditions showing significantly more attrition than  expected in four studies, and control conditions showing  significantly more attrition than expected in only a single study.   At scale, these analyses can help researchers and developers  determine which interventions are effectively retaining students,          Table 5: ALIs Bias Assessment at Scale - Observed Distributions and Chi-Squared Analyses Across 25 Problem Sets   Problem Set by Condition Started (n) Completed (n) Completed (%) df 2    p   Multiplying Mixed Numbers 5.NF.B.4a 775 466 60.13 1 5.30 0.021*        Control 403 258 64.02           Experiment 372 208 55.91     Understanding Vocabulary About Circles G-C.A.2 695 674 96.98 1 4.87 0.027*        Control 330 325 98.48           Experiment 365 349 95.62     Equivalent Expression 6.EE.B.4 273 240 87.91 1 0.39 0.532        Control 138 123 89.13           Experiment 135 117 86.67     Writing Inequalities from Situations 6.EE.B8 627 539 85.96 1 2.21 0.138        Control 338 297 87.87           Experiment 289 242 83.74     Dividing Mixed Numbers 6.NS.A.1 1864 1285 68.94 1 0.99 0.321        Control 943 660 69.99           Experiment 921 625 67.86     Finding Expected Value SS.MD.B.5 457 337 73.74 1 0.06 0.802        Control 224 164 73.21           Experiment 233 173 74.25     Conditional Probability SS-CP.A.3 515 366 71.07 1 0.70 0.401        Control 281 204 72.60           Experiment 234 162 69.23     Permutations and Combinations SS-CP.B.2 540 456 84.44 1 0.00 0.958        Control 265 224 84.53           Experiment 275 232 84.36     Basic Logarithm Manipulation F-BF.B.5 136 121 88.97 1 0.21 0.645        Control 62 56 90.32           Experiment 74 65 87.84     Properties of Exponents 8.EE.A.1 545 435 79.82 1 0.24 0.626        Control 264 213 80.68           Experiment 281 222 79.00     Intermediate Logarithm Manipulation F-BF.B.5 205 169 82.44 1 8.44 0.004**        Control 102 92 90.20           Experiment 103 77 74.76     Solving abct = d LE.A.4a 147 122 82.99 1 0.01 0.914        Control 72 60 83.33           Experiment 75 62 82.67     Finding Inverse Functions F-BF.B.4 301 143 47.51 1 3.32 0.068        Control 145 61 42.07           Experiment 156 82 52.56     Composition of Functions F-BF.A.1c 219 173 79.00 1 0.86 0.354        Control 118 96 81.36           Experiment 101 77 76.24     Sequences F-BF.A.2 382 241 63.09 1 0.20 0.658        Control 198 127 64.14           Experiment 184 114 61.96     Comparing Values - Multiplying by Fractions 5.NF.B.5a 129 121 93.80 1 1.59 0.208        Control 69 63 91.30           Experiment 60 58 96.67     Converting Radians to Degrees F-TF.A.1 245 226 92.24 1 0.23 0.631        Control 129 120 93.02           Experiment 116 106 91.38     Trigonometric Ratios G-SRT.C.8 307 266 86.64 1 0.91 0.341        Control 141 125 88.65           Experiment 166 141 84.94     Pythagorean Theorem  Finding the Hypotenuse 8.G.B.7 447 349 78.08 1 6.40 0.011*       Control 237 174 73.42          Experiment 210 175 83.33     Solving 1-Step Equations 7.EE.B.4a 928 818 88.15 1 0.01 0.934       Control 459 405 88.24          Experiment 469 413 88.06     Prime Factorization 6.NS.B.4 1238 1058 85.46 2 0.97 0.616       Control  430 369 85.81          Experiment 1 399 345 86.47          Experiment 2 409 344 84.11     Order of Operations (No Exponents) 7.NS.A.3 1231 1172 95.21 2 4.50 0.105       Group A - Consistent/Neutral 597 574 96.15          Group B - Inconsistent 300 287 95.67          Group C - Mixed 334 311 93.11      Note. p < .10, *p < .05, **p < .01. df = Degrees of Freedom.           Table 5: ALIs Bias Assessment at Scale - Continued   Problem Set by Condition Started (n) Completed (n) Completed (%) df 2    p   Multiplying Simple Fractions 5.NF.B.4a 598 559 93.48 3 1.54 0.673       Group A  No Choice + Text 142 131 92.25          Group B  Choice + Text 222 211 95.05          Group C  Choice + Video 76 71 93.42          Group D  No Choice + Video 158 146 92.41     Rotations 8.G.A.3 306 186 60.78 1 0.82 0.365       Experiment 1 145 92 63.45          Experiment 2 161 94 58.39     Reflections 8.G.A.3 239 171 71.55 1 0.17 0.680       Experiment 1 125 88 70.40          Experiment 2 114 83 72.81     Note. p < .10, *p < .05, **p < .01. df = Degrees of Freedom.    and more importantly, critical design issues that drive students  away. As many of these 25 studies were designed prior to the  implementation of internal validity checks (i.e., assessing a  students technical abilities with video content), we believe that  the analyses in Table 5 suggest higher attrition in experimental  conditions because certain students were assigned to content that  they had difficulty accessing. This finding would not likely hold  true when considering studies run more recently, suggesting the  importance of the recent implementation of If-Then routing.  Future work with ALI at scale will help to confirm this  hypothesis. Usability is a concern within any online learning  system, and providing students with access to default assignments  when they cannot access enriched content is a safe practice.    It is also important to consider the percentage of students  excluded from analysis prior to the assessments presented in  Table 5. Within all sets, an average of 22.85% of students did not  actually experience condition and were removed from the sample  prior to analysis. Students that fail to experience interventions  implemented within feedback (due to mastery or performance at  ceiling) provide valuable information to researchers regarding the  raw (inflated) sample size required to achieve statistical power.  Certain elements of a studys design, including the content  domain (i.e., some topics are easier than others and students  require less feedback on average), and the type of feedback  provided (i.e., on demand feedback requires a larger raw  population than feedback provided automatically upon the  students incorrect response), can have a significant impact on the  raw sample size required to attain enough treated students to  reliably detect effects. RCEs that consider interventions  implemented strictly within problem content have fewer issues  with regard to raw sample sizes as all students experience the  intervention regardless of performance, easing potential issues  surrounding intent-to-treat analyses.   Finally, analyzing the selection effects inherent to multiple  assignments simultaneously allows ASSISTments to evolve more  rapidly, providing benefits to users, researchers, and the learning  analytics community. As the experimental conditions in Table 5  exhibited only 1.5% greater attrition on average than control  conditions, it is possible that the benefits of these experimental  interventions may still outweigh the increase in attrition.  Additional data mining would be necessary to determine a  standard at which the potential for emphasized learning gains  within an experimental condition no longer outweighed the  potential for increased attrition. However, regularly conducting  this type of broad scale analysis across assignments could quickly  isolate studies with conditions considered extremely detrimental,  and the condition could be discontinued in order to limit the  interventions negative impact on students. ALIs automated   analysis makes the process of intervention validation dramatically  more efficient and robust. From these findings, and from future,  more powerful iterations of ALIs at-scale capabilities,  ASSISTments will be able to deliver rapid iterations of  interventions with the goal of optimizing students interactions  with the system through enhanced usability and strengthened  content and delivery methods.    5. LIMITATIONS & FUTURE WORK  As ALI is constantly evolving and gaining new capabilities, the  version of the infrastructure presented here carries a number of  limitations. As made apparent by the complex methods applied to  consider ALIs effects at scale, the infrastructure is currently only  able to recognize studies with logged path information. The  implementation of path logging occurred in March 2015, and ALI  is only able to reliably analyze studies that were created after this  implementation. This limitation is compounded by ALIs  inferences of the study design and posttest items. As studies  within the ASSISTments TestBed can be designed using a number  of complex, nested structures, ALIs current decisions about study  designs are not exceptionally intelligent. A serious limitation of  the work presented herein is that the infrastructure is currently  only able to reliably recognize and analyze study designs with  simple structures (i.e., If-Then routing, a single Choose  Condition, and a clear cut posttest section that directly follows an  intervention).    While these limitations influence ALIs significance for the  learning analytics community, they can easily be resolved through  future work. One of our current focuses is the implementation of a  tagging system that will allow researchers to identify pertinent  sections of a study prior to its distribution. Using unified naming  structures for the design of assignment sections within the  building process (e.g., [experiment], [control], [posttest]),  researchers will essentially be able to tell ALI exactly how to  approach analysis. This will allow ALI to provide customized  analysis and, potentially, refined data files that are preprocessed  according to the researchers distinct needs. Tagging will also  allow for analyses that collapse similar treatment groups (i.e.,  experimental group 1 and experimental group 2 could both be  tagged with [experiment] to denote that ALI should collapse these  conditions), that isolate unconventional posttest problems (i.e.,  problems falling within a section that does not immediately follow  a Choose Condition), and that assess growth models of student  performance (i.e., by measuring pre- to posttest gains, or through  more complex hierarchical models).   Future work for the ALI team also includes defining a powerful  list of student, class, and school level variables for use as  covariates in statistical analyses. Variables that have already been        established include measures of each students prior performance  within ASSISTments, measures of their completion rate on  classwork and homework assignments, and normalized values that  compare the students performance and attrition against that of  their class. As such, future iterations of ALIs at-scale capabilities  will also be able to control for particular student characteristics in  order to assess the true variance established by experimental  interventions. Additional content is also being built into  ASSISTments and made available in the TestBed to collect self- report measures from students for use as possible covariates. Rich  covariates will provide ALI with the ability to examine the effects  of experimental interventions across groups while controlling for  substantial variance, making automated analyses far more robust.   6. CONTRIBUTION  The learning analytics community will benefit greatly from the  Assessment of Learning Infrastructure (ALI) and the promotion of  similar infrastructures for other online learning platforms.  Currently, very few learning technologies serve as scientific tools  for researchers to conduct and communicate the findings of sound  educational research at scale. By allowing researchers to conduct  research within authentic learning environments through  classwork and homework completed within online learning  platforms, it is possible to collect rich log files that can be  reported in universal formats and analyzed using automated  processes. As a community, a strong focus on providing universal  measures and analyses from these platforms will strengthen the  validity of findings from a broad range of interventions that seek  to isolate best practices in education. The broad dissemination of  vast anonymized educational datasets will also propel the field  toward more transparent, replicable, and reputable scientific  practice, improving learning analytics for all.    7. ACKNOWLEDGMENTS  We acknowledge funding from multiple NSF grants (1440753,  1252297, 1109483, 1316736, 1535428, 1031398), the U.S. Dept.  of Ed. (R305A120125, R305C100024, P200A120238), and ONR.  We also thank our colleagues Yang Lu and Anthony Botelho for  their roles in designing ALI. Thanks to S.O. & L.P.B.O.   8. REFERENCES  [1] Adjei, S.A. & Heffernan, N.T. (2015). Improving Learning   Maps Using an Adaptive Testing System: PLACEments. In  Conati, Heffernan, Mitrovic, & Verdejo (eds.) Proc of the  17th Int Conf on AIED. Springer, 517-520.   [2] Beck, J.E. & Gong, Y. (2013). Wheel-spinning: Students  who fail to master a skill. In Lane, Yacef, Mostow & Pavlik  (eds.) Proc of the 16th Int Conf on AIED.  Springer-Verlag,  431-440.    [3] Brown, A.L. (1992). Design experiments: Theoretical and  methodological challenges in creating complex interventions  in classroom settings. J of Learning Sciences, 2(2), 141-178.    [4] Dweck, C.S., Chiu, C., & Hong, Y. (1995). Implicit theories  and their role in judgments and reactions: A world from two  perspectives. Psychological Inquiry, 6(4), 267-285.   [5] Heffernan, N. & Heffernan, C. (2014). The ASSISTments  Ecosystem: Building a Platform that Brings Scientists and  Teachers Together for Minimally Invasive Research on  Human Learning and Teaching. Int J of AIED, 24(4), 470- 497.   [6] Institute of Education Sciences. (2003). Identifying and  Implementing Educational Practices Supported by Rigorous  Evidence: A User Friendly Guide. U.S. Dept of Ed.  Washington, D.C.   [7] Ioannidis J.P.A. (2005). Why Most Published Research  Findings Are False. PLoS Med 2(8): e124.    [8] Koedinger, K.R., Baker, R.S., Cunningham, K., Skogsholm,  A., Leber, B., & Stamper, J. (2010). A data repository for the  EDM community: The PSLC DataShop. Handbook of  EDM, 43.    [9] Kohavi, R., Longbotham, R., Sommerfield, D., & Henne, R.  M. (2009). Controlled experiments on the web: survey and  practical guide. Data mining and knowledge discovery,  18(1), 140-181.   [10] National Governors Association Center for Best Practices &  Council of Chief State School Officers. (2010). Common  Core State Standards. Washington, DC: Authors.   [11] Open Sci Collab. (2015). Estimating the reproducibility of  psychological science. Science, 349 (6251).     [12] Ostrow, K. (2015). Data for The Assessment of Learning  Infrastructure (ALI): The Theory, Practice, and Scalability of  Automated Assessment. Accessed from  http://tiny.cc/LAK2016-ALI    [13] Ostrow, K. & Heffernan, C. (2014). How to Create  Controlled Experiments in ASSISTments. Retrieved from  https://sites.google.com/site/assistmentstestbed/   [14] Ostrow, K.S. & Heffernan, N.T. (2014). Testing the  Multimedia Principle in the Real World: A Comparison of  Video vs. Text Feedback in Authentic Middle School Math  Assignments.  In Stamper, et al. (eds.) Proc of the 7th Int  Conf on EDM, 296-299.   [15] Ostrow, K., Heffernan, N., Heffernan, C., & Peterson, Z.  (2015). Blocking vs. Interleaving: Examining Single-Session  Effects within Middle School Math Homework. In Conati,  Heffernan, Mitrovic, & Verdejo (eds.) Proc of the 17th Int  Conf on AIED. Springer, 388-347.   [16] Pashler, H., Rohrer, D., Cepeda, N. & Carpenter, S.K.  (2007). Enhancing learning and retarding forgetting: Choices  and consequences. Psychonomic Bulletin & Review. 14 (2),  187-193.   [17] San Pedro, M., Baker, R., Gowda, S., & Heffernan, N.  (2013). Towards an Understanding of Affect and Knowledge  from Student Interaction with an Intelligent Tutoring System.  In Lane, Yacef, Mostow & Pavlik (eds.) Proc of the 16th Int  Conf on AIED.  Springer-Verlag, 41-50.   [18] Selent, D., Patikorn, T., Heffernan, N. (Under Review).  ASSISTments Dataset from Multiple Randomized  Controlled Experiments. Submitted to the 3rd Annual ACM  Conference on L@S.   [19] Urbanek, S. (2003). Rservea fast way to provide R  functionality to Applications. In Hornik, Leisch, & Zeileis,  Proc of the 3rd Int Workshop on DSC, ISSN 1609-395X.  http://rosuda.org/rserve.   [20] U.S. Department of Education, Office of Educational  Technology. (2012). Enhancing teaching and learning  through educational data mining and learning analytics: An  Issue Brief. Washington, DC    1. INTRODUCTION  1.1 Research in the ASSISTments TestBed  1.2 Utility of Automated Data-Preprocessing   2. ALI IN THEORY  2.1 Benefits to ASSISTments Users  2.2 Benefits to the Researcher  2.3 Benefits to the Platform  2.4 Benefits for Learning Analytics   3. ALI IN PRACTICE  3.1 ALIs Current Capabilities  3.1.1 Data Reporting at Scale  analysis reports to occur simultaneously, all using different Rserve connections. This approach lowers the turnaround time when a researcher actively requests data. It also keeps weekly reporting as efficient as possible, as all datasets in ALIs stud...  3.1.2 Smart Structures  3.1.3 Selection Bias  3.1.4 Simple Hypothesis Testing  3.1.5 Data Storage and Researcher Output    4. ANALYSIS AT SCALE  5. LIMITATIONS & FUTURE WORK  6. CONTRIBUTION  7. ACKNOWLEDGMENTS  8. REFERENCES   "}
{"index":{"_id":"37"}}
{"datatype":"inproceedings","key":"Papamitsiou:2016:ACT:2883851.2883926","author":"Papamitsiou, Zacharoula and Karapistoli, Eirini and Economides, Anastasios A.","title":"Applying Classification Techniques on Temporal Trace Data for Shaping Student Behavior Models","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"299--303","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883926","doi":"10.1145/2883851.2883926","acmid":"2883926","publisher":"ACM","address":"New York, NY, USA","keywords":"assessment analytics, computer-based testing, learner behavioral modeling, supervised learning classification","abstract":"Differences in learners' behavior have a deep impact on their educational performance. Consequently, there is a need to detect and identify these differences and build suitable learner models accordingly. In this paper, we report on the results from an alternative approach for dynamic student behavioral modeling based on the analysis of time-based student-generated trace data. The goal was to unobtrusively classify students according to their time-spent behavior. We applied 5 different supervised learning classification algorithms on these data, using as target values (class labels) the students' performance score classes during a Computer-Based Assessment (CBA) process, and compared the obtained results. The proposed approach has been explored in a study with 259 undergraduate university participant students. The analysis of the findings revealed that a) the low misclassification rates are indicative of the accuracy of the applied method and b) the ensemble learning (treeBagger) method provides better classification results compared to the others. These preliminary results are encouraging, indicating that a time-spent driven description of the students' behavior could have an added value towards dynamically reshaping the respective models.","pdf":"Applying classification techniques on temporal trace data  for shaping student behavior models   Zacharoula Papamitsiou  IPPS in Information Systems   University of Macedonia  156 Egnatia Avenue    Thessaloniki, 54621, Greece  papamits@uom.edu.gr   Eirini Karapistoli  IPPS in Information Systems   University of Macedonia  156 Egnatia Avenue    Thessaloniki, 54621, Greece  ikarapis@uom.gr  Anastasios A. Economides  IPPS in Information Systems   University of Macedonia  156 Egnatia Avenue    Thessaloniki, 54621, Greece  economid@uom.gr  ABSTRACT  Differences in learners behavior have a deep impact on their  educational performance. Consequently, there is a need to detect  and identify these differences and build suitable learner models  accordingly. In this paper, we report on the results from an  alternative approach for dynamic student behavioral modeling  based on the analysis of time-based student-generated trace data.  The goal was to unobtrusively classify students according to their  time-spent behavior. We applied 5 different supervised learning  classification algorithms on these data, using as target values  (class labels) the students performance score classes during a  Computer-Based Assessment (CBA) process, and compared the  obtained results. The proposed approach has been explored in a  study with 259 undergraduate university participant students. The  analysis of the findings revealed that a) the low misclassification  rates are indicative of the accuracy of the applied method and b)  the ensemble learning (treeBagger) method provides better  classification results compared to the others. These preliminary  results are encouraging, indicating that a time-spent driven  description of the students behavior could have an added value  towards dynamically reshaping the respective models.    Categories and Subject Descriptors  K.3 [Computers and Education]: General   Keywords  Learner behavioral modeling, assessment analytics, computer- based testing, supervised learning classification.    1. INTRODUCTION  The landscape on Learning Analytics (LA) research over the last  five years has rapidly changed. Lately, the educational research  community has shifted towards exploring different, multiple, more  complex and more information rich data sources (e.g. tangible and  wearable computing, immersive learning environments, shared  workspaces, massive open online courses, etc.), in order to   identify new suitable measures of learning and success (e.g. affect,  attention, attitudes, community structure, degrees of competence,  expectations, satisfaction, social dynamics, etc.) and develop  applications that are expected to enable personalized learning on a  large scale (e.g., [1], [2], [3], [4], [5], [6]).   Detecting undesirable learner behaviors, and profiling learners are  among the core objectives of LA research. Moreover, differences  in learners behavior have a deep impact on their performance. As  apparent, there is a need to detect and identify these differences  and build suitable learner models accordingly. These models will  further assist in improving the personalization of educational  services at a larger scale.   In this paper, we present a method for dynamic student behavioral  modeling based on the analysis of temporal, student-generated  trace data. The goal was to unobtrusively classify students  according to their time-spent behavior during assessment  processes. In particular, we explored a large range of supervised  learning classification algorithms (SLA) (namely Artificial Neural  Networks-ANNs, Support Vector Machines-SVMs, Nave Bayes- NB, k-Nearest Neighbors-kNN and treeBagger) on a dataset  consisting of time-based data (including total time to answer  correctly, total time to answer wrongly, total idle time, effort and  goal expectancy) using as target values (class labels) the students  performance during a Computer-Based Assessment (CBA)  process. Next, we present the results from a study with 259  undergraduate participant students from a Greek University. For  the study, we employed the LAERS assessment environment [7].  We discovered that a) the low misclassification rates, as well as  the high sensitivity and performance measures are indicative of  the accuracy of employing temporal trace data for student  behavioral modeling and b) the ensemble learning (treeBagger)  method provides better and more solid classification results  compared to the other SLA algorithms.   The rest of the paper is organized as follows: in section 2, we  briefly review existing work regarding student modeling methods,  approaches and results. In section 3, we explain the motivation  and rationale of our research. In section 4, we present the  experiment methodology, the data collection procedure and the  analysis methods that we applied, while in section 5, we analyze  the results from the case study. Finally, in section 6, we discuss  the major findings and conclude with future implications.   2. RELATED WORK  Student modeling can be defined as the process of information  extraction from different sources into a profile representation of  students knowledge level, cognitive and affective states, and  meta-cognitive skills on a specific domain or topic [8, 9]. The     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  permissions@acm.org.   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883926       goal is to describe and/or predict particular behavioral patterns.  Identification and modeling of students and students learning  behavior is a primary educational research objective. A student  model is used to represent multiple students characteristics   either static (e.g., age, gender, etc.), or dynamic. The most  common among the dynamic characteristics include students  personality traits, performance, goals, achievements, prior and  acquired domain knowledge, [10], learning strategies, preferences  and styles [11], making decisions and analyzing abilities, critical  thinking and communication skills, collaborative skills [12],  errors and misconceptions, motivation, emotions and feelings,  self-regulation, self-explanation and self-assessment [13], [31].  More recently, the time dimension has been explored for  modeling user behavior [e.g., 14, 15, 16]. Barua et al. [14]  included temporal data in order to construct a model for long-term  goal setting representation, while Belk et al. [15] also used  response-times in combination with the given answers in order to  discriminate Verbal users from Imagers. The work in [16] is  yet another example of time-spending exploitation during an  experiment with eye-tracking technology for classification of  users in a user-independent fashion.   In the educational research domain, Shih, Koedinger and Scheines  [17] used worked examples and logged response times to model  the students time-spent in terms of thinking about a hint and  reflecting on a hint. The goal was to capture behaviors that are  related to reasoning and self-explanation during requesting hints.  It was found that specifying the moments that teacher should  intervene requires to better distinguish between students who use  worked examples, how they use them and their response times.   Additional studies have shown that the temporal interpretation of  students engagement in task solving during testing, could be used  for predicting their progress [7, 18]. In particular, it was found  that Total Time to Answer Correctly (TTAC), Total Time to  Answer Wrongly (TTAW) and goal expectancy (GE) are strong  determinants of Actual Performance (AP), and Total Idle Time  (TIT) is an indicator of students effort (EFF) [30] during testing.   3. MOTIVATION AND RARIONALE OF  THE RESEARCH  As stated in the introduction, differences in learners behavior  have a deep impact on their performance. As apparent, there is a  need to detect and identify these differences and build suitable  learner models accordingly. These models will further assist in  improving personalization of educational services. Consequently,  it is important for systems developers to identify the parameters  that could be used for fully adapting the assessment items to the  examinees level of ability/expertise or for providing personalized  feedback during CBA (e.g., the recommendation of the most  appropriate next testing item).    Further, since the temporal interpretation of the students behavior  has been found to explain satisfactorily their actions [14, 16] and  to provide statistically significant explanation of students  performance [7, 18], additional research should be conducted  regarding whether temporal, user-generated trace data could be  used for student behavioral modeling purposes.  In this paper, we suggest a method for dynamic student behavioral  modeling. In particular, we propose the use of temporal, student- generated trace data that are unobtrusively and seamlessly tracked  during a CBA procedure. Such mechanisms for tracking temporal  data are cost-effective, consume low computational resources, and   can be easily implemented in any CBA system. Moreover, our  methodology applies multiple supervised learning algorithms  (SLAs), including ANNs, SVMs, kNN, NB and treeBagger for  classifying and analyzing this type of data.   4. METHODOLOGY  4.1 Research participants and data collection  In this study, data were collected from a total of 259  undergraduate students (108 males [41.7%] and 151 females  [58.3%], aged 20-27 years old (M=22.6, SD=1.933, N=259) from  the Department of Economics at University of Macedonia,  Thessaloniki, Greece. 12 groups of 20 to 25 students attended the  midterm exams of the Computers II course (related to databases,  information systems and introduction to e-commerce). For the  purposes of the examination, we used 34 multiple choice  questions. Each question had two to four possible answers, but  only one was the correct. The participants could skip or re-view  the questions and/or alter the submitted answer. Finally, the  participation to the midterm exams procedure was optional. As an  external motivation to increase the students effort, we set that  their score would participate up to 30% to their final grade.   4.2 The LAERS Assessment Environment  In our study, we used the LAERS assessment environment [7],  which is a CBA system that we are developing in order to  automate the provision of personalized recommendations of most  appropriate next task as adaptive feedback service.   At the first phase of the implementation, we configured a testing  mechanism and a tracker that logs the students temporal data.  The testing unit displays the multiple choice quiz tasks delivered  to students. Each task is displayed separately and one-by-one. The  students can temporarily save their answers on the tasks, before  submitting the quiz, and they can change their initial choice by  selecting the task to re-view from the list underneath. They submit  the quiz answers only once, whenever they estimate that they are  ready to do so, within the duration of the test.    Table 1: Features from the raw log files   Feature   1. student ID 2. the task the student works on  3. the answer the student submits 4. the correctness of the submitted   answer   5. the timestamp the student  starts viewing a task   6. the timestamp the student  chooses to leave the task (saves   an answer)   7. the total time the student  spends on viewing the tasks   and submitting the correct   answers   8. the total time the student spends  on viewing the tasks and   submitting the wrong answers   9. the idle time the student  spends viewing each task   10. how many times the student  views each task   11. how many times the students  change the answer they submit   for each task   12. the students total time on task   13. the students GE 14. the effort required (EFF)  15. the students AP        The second component of the system records the students activity  data during testing. We also embedded into the system a pre-test  questionnaire (consisting of 3 questions) in order to measure each  students goal expectancy (GE) (a measure of student self- confidence and goal orientation regarding the use of a CBA,  proposed in Computer Based Assessment Acceptance Model  (CBAAM) [19]). GE has two dimensions: the students  preparation to take the CBA and the desirable level of success for     each student. GE actually measures if the learners are fulfilled  with their preparation. The students, before taking the CBA, set a  goal regarding a percentage of correct answers that provides them  a satisfying performance. The three items from the questionnaire  that measure GE were: a) GE1: Courses preparation was  sufficient for the CBA, b) GE2: My personal preparation for the  CBA, and c) GE3: My performance expectations for the CBA. GE  was found to be a direct strong determinant of the temporal  variables and concurrently an indirect strong determinant of AP  [7]. The overall features/attributes of students activity either  tracked, computed and/or measured are listed in Table 1.   4.3 Feature Subset Selection  The initial raw log file contained a sample of the 15 attributes  (features) to be used in this study. Our pre-experimental thoughts  were that some of the attributes were noisy; i.e. contain signals  not related to the target of classification. Therefore, we first  attempted to remove spurious attributes using feature subset  selection. Feature selection reduces the dimensionality of data by  selecting only a subset of features (i.e., predictor variables) to  create a model. Selection criteria usually involve the minimization  of a specific measure of predictive error for models fit to different  subsets. Algorithms search for a subset of predictors that  optimally model measured responses, subject to constraints such  as required or excluded features and the size of the subset. Note  that the number of attributes to select is crucial in the analysis of  the data. In this experiment, we ranked the 15 attributes from most  to least informative. The attributes were ranked using the  sequential feature selection method of MATLAB. This method  has two components: a) an objective function, called the criterion,  which the method seeks to minimize over all feasible feature  subsets, and b) a sequential search algorithm, which adds or  removes features from a candidate subset while evaluating the  criterion.    4.4 Analysis Methods  The machine learning techniques are divided into those performed  without supervision (unsupervised learning), and those that take  place under supervision (supervised learning). The algorithms that  belong to the first category build a model without knowing the  desired outputs for the training set. Typical example of  unsupervised learning is association rules mining between the  values of characteristics within the learning vectors. However,  most of the research activity in the field of machine learning  concerns learning with supervision (supervised learning), typical  example of which is the categorization or classification problems.  Suppose there is a data set containing observations with  measurements on different variables (called predictors) and their  known class labels. If predictor values are obtained for new  observations, could the classes those observations belong to be  determined This is the problem of supervised classification: the  task of assigning objects to one of several predefined classes. In  other words, it is the task of learning a target function f to map  each input attribute set x to one of the predefined class labels y.   Each classification technique employs a learning algorithm to  identify a model that best fits the relationship between the  attribute set and the class label of the input data. These techniques  operate in two phases: the training phase and the testing phase.  [20, 21]. In the present study, we explored 5 different advanced  supervised learning techniques for classifying students based on  their time-based characteristics (predictors) and according to their  actual performance (class label). In particular, we tried Artificial  Neural Networks (ANNs), Support Vector Machines (SVM),  Nave Bayes (NB), k-Nearest Neighbors (kNN) and the   treeBagger method. These are some of the well-known classifiers  used in the machine learning field, and the most common  approaches explored with a plurality of different attributes in the  learning analytics and educational data mining research domain.   Artificial Neural Networks (ANNs) are computational systems  based on the structure, processing method and learning ability of  the brain [29]. When performing classification analysis with an  existing dataset, a commonly adapted approach, named holdout  validation, is used to split the data into a larger set for training the  ANN and a smaller set for testing the model [23]. In this work, a  Feed Forward neural network has been created and trained.  Support Vector Machines (SVM) is a supervised learning method  for linear modeling. For classification purposes, nonlinear kernel  functions are often used to transform the data into a feature space  of a higher dimension than that of the input before attempting to  separate them using a linear discriminator [24, 25]. In this work, a  third degree polynomial kernel function was employed.  Nave Bayes (NB) are a family of simple probabilistic classifiers  based on applying Bayes' theorem with strong independence  assumptions between the predictors within each class. During the  training step, the method estimates the parameters of a probability  distribution. Next, during the prediction step, the method  computes the posterior probability of that sample belonging to  each class, and classifies the test data accordingly [20].  k-Nearest Neighbors (kNN) is a non-parametric method used for  classification. Given an unknown sample, a kNN classifier  searches the pattern space for the k training samples that are  closest to the unknown sample. kNN is based on the principle that  the samples within a dataset will generally exist in close proximity  to other samples that have similar properties [26, 27].   Bagging Bagging (Bootstrap Aggregating) is an ensemble  method that creates separate samples of the training dataset and  creates a classifier for each sample. In fact, bagging predictors is a  method for generating multiple versions of a predictor and using  these to get an aggregated predictor. The aggregation does a  plurality vote when predicting a class. The multiple versions are  formed by making bootstrap replicates of the learning set and  using these replicates as new learning sets [20, 28].   4.5 Measures and Performance Criteria   Evaluation of the performance of a classification model is based  on the counts of test records correctly and incorrectly predicted by  the model, tabulated in a confusion matrix. Generally speaking,  the (i,j) element in the confusion matrix is the number of samples  whose known class label is class i and whose predicted class is j.  The diagonal elements represent correctly classified observations.   However, the confusion matrix is not convenient to compare the  performance of different models. Accuracy is a single-value  performance metric defined as the proportion of correct  predictions to the total predictions. Further, the performance of a  model can be expressed in terms of its error rate, which is given  as the proportion of wrong prediction to the total predictions [20,  21]. The errors committed by a classification model are generally  divided into two types: resubstitution errors (training errors) and  test errors (generalization errors). The resubstitution error is the  proportion of misclassified observations on the training set,  whereas the test error is the expected prediction error on an  independent set. A good model must have low resubstitution error  as well as low test error [20, 22].  A method commonly used to evaluate the performance of a  classifier is cross validation. The k-fold cross validation method  segments the data into k equal-sized partitions. This procedure is  repeated n times so that each partition is used the same number of  times for training and exactly once for testing. We used a     stratified k=10-fold cross validation with n=100 iterations for  estimating the misclassification (test) error [20, 21, 22].  Moreover, sensitivity analysis is a method for identifying the  cause-and-effect relationship between the inputs and outputs of  a prediction model. This method is often followed in machine  learning techniques to rank the variables in terms of their  importance according to the sensitivity measure [22]. Finally, F- score (or F-measure) is a measure of a test's accuracy. It considers  the precision and the recall of the test to compute the score. In  simple terms, high precision means that an algorithm returned  substantially more relevant results than irrelevant, while  high recall means that an algorithm returned most of the relevant  results [21, 22]. The F-score can be interpreted as a weighted  average of the precision and recall, where an F-score reaches its  best value at 1 and worst score at 0 [20].   5. RESULTS  Table 2 outlines the SLA methods we applied on the input data,  the number of classes being predicted (i.e., the different categories  of students performance results), the overall accuracy of the  prediction (for training and testing respectively) together with the  respective sample sizes (90% for training and 10% for testing for  all SLA methods), and the tool used during the analysis.   Table 2: A summary of our experiment   SLA used # of classes  predicted   Sample size Accuracy of  prediction   Simulation  tool used   ANNs, SVMs,  NB, kNN,  treeBagger    7-class  259 samples in  total  233 for training  26 for testing    100% for  training   76% for testing   MATLAB   5.1 Exploratory data analysis   Table 3 illustrates the variables (features) used to train and test the  machine learning networks, during the experiment, as well as the  range of their values:   Table 3: Features used for training and testing    Variable Description Type Value Range T em  poral    TTAC Total time to  answer correctly   Simple 0 (msec)   TTAW Total time to  answer wrongly   Simple 0 (msec)   TIT Total idle time Simple 0 (msec)  EFF Effort Composite 0-1   Time-varying AP* Actual  Performance   Simple 0-1.5   Self-reported GE Goal  expectancy   Latent 0-5  *  AP: target (output)-dependent variable   Further to that, Table 4 illustrate the covariance matrix for all five  input variables. As it can be seen, there are no strong correlations  between the variables.    Table 4: Covariance matrix for all predictor variables      TIT TTAC TTAW EFF GE   TIT 1.000          TTAC -0.082 1.000        TTAW -0.313 0.357 1.000      EFF -0.353 0.056 0.259 1.000    GE 0.128 0.098 0.055 0.564 1.000   5.2 Classification results   In this study, we initially explored the previously described  methods with an input dataset consisting of three variables  (predictors): TTAC, TTAW and GE. We chose to examine these  variables based on the formerly reported results from [7]. Table 5  presents the performance results (resubstitution error, true test  error, sensitivity, and F-score) for all the methods used to develop  a classification model in this study with these three features and  with testing sample size 10% of the initial dataset.    Table 5: Performance metrics for cross-validation 10% with three  features   Test Set Size |cvpartition| = 10% (k-fold=10)   Classifier ANN SVM kNN NB ENS**  Resub Error 0.34 NaN 0.30 0.38 0.00  True Test Error* 0.24 0.27 0.28 0.24 0.24  Sensitivity 0.96 0.95 0.95 0.96 0.96  F-score 0.87 0.85 0.86 0.85 0.88   *True test error=cross-validation error, **ENS:ensembles of decision trees   These results demonstrate that all methods achieve high  classification performance, since the true test error varies from  0.24 (ENS method) to 0.28 (kNN method). Further to that, the  sensitivity measure is close to 1 in most cases (0.95-0.96) and the  F-score is also high (0.85-0.88). Moreover, from this table it  becomes apparent that the ENS method provides better  classification results compared to the other methods, while the  kNN and NB methods also achieve satisfactory results.  Based on this finding, we examined how the highly performing  methods (ENS, kNN and NB) change their output when applied to  more input variables (predictors). We explored this question with  two additional features: TIT and EFF. Table 6 illustrates the  performance metrics for the ENS, kNN and NB methods with 4  (initially we added TIT) and 5 (finally we added EFF) features  and testing sample set to 10% of the initial dataset.   Table 6: Performance metrics for test set size 10% with 4 and 5  features   Forward Feature  Selection   'TTAW', 'TTAC', 'GE',  EFF'   'TIT', 'TTAW',  'TTAC', 'EFF', 'GE'   Classifier kNN NB ENS kNN NB ENS  Resub Error 0.30 0.37 0.00 0.30 0.36 0.00  True Test Error 0.28 0.28 0.28 0.24 0.32 0.24  Sensitivity 0.95 0.95 0.95 0.96 0.94 0.96  F-score 0.85 0.86 0.85 0.88 0.82 0.84  As seen from this table, ENS does not seem to be affected by the  additional features, providing results similar to the previous ones  (conducted using with three features only). On the contrary, the  performance of the other two methods is slightly reduced when  the number of predictors increases.   6. DISCUSSION AND CONCLUSIONS   In this paper, we explored student-generated temporal trace data  for modeling students behavior during a CBA procedure  according to the students performance score. Our goal was to  unobtrusively and seamlessly identify the students time-spent  behavioral patterns in order to dynamically shape the respective  models. The motivation for our experimentation was based on  previous research studies that analyzed temporal parameters for  user modeling and reported significant results. During our  experimentation, we applied 5 advanced SLA techniques.   Our findings verify formerly reported results [15], [17] regarding  the capability of temporal data to represent, describe and model  the students behavior. In particular, our findings indicate that the  total time to answer correctly and the total time to answer wrongly  in combination with the goal expectancy could satisfactorily be  used for classification of students during computer-based testing.  The low misclassification rates are indicative of the accuracy of  the proposed method. Further to that, from tables 5 and 6 it  becomes apparent that the ensemble learning (treeBagger) method  provided the most accurate classification results compared to the  other methods. However, an interesting finding that requires more  investigation is that most algorithms perform worse when two  additional features are included in the analysis. We still have to  explore why this is happening and weather these additional  features are appropriate for classification purposes.   Based on the findings, we suggest that one can identify a set of  functional temporal (or behavioral) factors/parameters that could  constitute the core components of a systems architecture. For     example, TTAC, TTAW, TIT, EFF and GE are only indicative  variables that could be embedded into a testing system in order to  model the test-takers and to guide adaptation and personalization  of services. Systems like that would aim at personalizing the  deliverable service according to their users model. For example,  such a service could be the recommendation of the next most  appropriate task according to the students model and detected  level of expertise (based on the corresponding timely predicted  performance). In this case, the system should be trained in order  to recognize and model its current users based on their temporal  and behavioral data. Then, it should choose the appropriate task  (among the collection of tasks from an item bank) that best  corresponds to the needs and meets the abilities of the user, in  order to improve the expected outcome. Finally, the system  should inform the users about their progress and either suggest the  selected task (as a CAT system) or allow the users to make their  own choice of the next task (as a CBT system).  The approach suggested in this paper was applied on a dataset  collected during an assessment procedure in the context of mid- term exams. However, the nature of the data collected (time-based  parameters) and the general-purpose methodology followed for  the analysis of these data (SLA), render this approach replicable  and/or transferable to other contexts, and eliminate the restriction  of using it only during testing. The temporal factors are not  contextualized to the LAERS assessment environment, but a  similar tracker could be embedded in any adaptive learning  system. For example, time-related parameters (time-spent) could  be tracked to measure the duration of solving/implementing sub- activities or sub-tasks in the context of project-based learning, or  to measure the duration of studying and exercising with learning  modules during inquiry-based learning, etc., along with the  number of repeating the intermediate, facilitating steps (e.g.  watching educational videos, opening and using educational  resources, participating in discussions, etc.).   As a next step, we are planning to deeper explore the patterns of  these classes in terms of time-spent, i.e., which are the specific  characteristics of the time-spent behavior of the examinee that  belong to each one of the classes. Moreover, we plan to  investigate other patterns within the students time-spent behavior  aiming at identifying unwanted behaviors that affect the  assessment results, by employing other suitable mining  techniques, like process mining. Finally, we envisage creating the  learner model simultaneously, while the student takes the test, in a  stream mining fashion, which would enrich the profile modeling  with a notion of dynamics, allowing for adaptive question  sequencing.   7. REFERENCES  [1] Aramo-Immonen, H., Jussila, J., Huhtamki, J. 2015. Exploring co-  learning behavior of conference participants with visual network  analysis of Twitter data, Comput Human Behav, 51, 1154-1162.   [2] Agudo-Peregrina, . F., Iglesias-Pradas, S., Conde-Gonzlez, M. .,  Hernndez-Garca, . 2015. Can we predict success from log data in  VLEs Classification of interactions for learning analytics and their  relation with performance in VLE-supported F2F and online  learning, Comput Human Behav, 31, 542-550.   [3] Kovanovi, V., Gaevi, D., Joksimovi, S., Hatala, M., Adesope, O.  2015. Analytics of communities of inquiry: Effects of learning  technology use on cognitive presence in asynchronous online  discussions, Internet High Educ, 27, 74-89.   [4] Tempelaar, D. T., Rienties, B., & Giesbers, B. 2014. In search for  the most informative data for feedback generation: Learning  analytics in a data-rich context, Comput Human Behav, 47,157-167.   [5] van Leeuwen, A., Janssen, J., Erkens, G., Brekelmans, M. 2015.  Teacher regulation of cognitive activities during student  collaboration:Effects of learning analytics, Comput Educ, 90, 80-94.   [6] Veletsianos, G., Collier, A., & Schneider, E. 2015. Digging Deeper  into Learners' Experiences in MOOCs: Participation in social   networks outside of MOOCs, Notetaking, and contexts surrounding  content consumption. Brit. J. Educ. Technol. 46(3), 570-587.   [7] Papamitsiou, Z., Terzis, V. Economides, A. A. 2014. Temporal  Learning Analytics during computer based testing, In Proceedings  of the 4th International Conference on Learning Analytics and   Knowledge (LAK14), Indianapolis, USA, 31-35.  [8] McCalla, G. 1992. The central importance of student modeling to   intelligent tutoring. In E. Costa [Ed.], New Directions for Intelligent  Tutoring Systems. Berlin: Springer Verlag.   [9] Thomson, D., Mitrovic, A. 2009. Towards a negotiable student  model for constraint-based ITSs. 17th International Conference on  Computers in Education, Hong Kong, 8390.    [10] Self, J. A. 1990. Bypassing the intractable problem of student  modeling. In C. Frasson & G. Gauthier (Eds.), Intelligent-tutoring  systems: At the crossroads of AI and education, 107123, Norwood,  NJ: Ablex    [11] Pea-Ayala, A. 2014. Educational data mining: A survey and a data  mining-based analysis of recent works. Expert Syst Appl, 41(4),  14321462    [12] Mitrovic, A., Martin, B. 2006. Evaluating the effects of open student  models on learning. 2nd international conference on adaptive  hypermedia and adaptive web-based systems, 296305    [13] Pea, A., Kayashima, M. 2011. Improving students meta-cognitive  skills within intelligent educational systems: A Review. 6th  International Conference on Foundations of Augmented Cognition,  Orlando, Florida, USA, 442451    [14] Barua, D., Kay, J., Kummerfeld, B., Paris, C. 2014. Modeling long  term goals, 22nd International Conference on User Modeling,  Adaptation and Personalization, Aalborg, 1-12    [15] Belk, M., Germanakos, P., Fidas, C., Samaras, G. 2014. A  personalization method based on human factors for improving  usability of user authentication tasks, 22nd Int. Conf. on User  Modeling, Adaptation and Personalization, Aalborg, 13-24,    [16] Bixler, R. DMello, S. 2014. Toward fully automated person- independent detection of mind wandering, 22nd  Int. Conf. on User  Modeling, Adaptation and Personalization, Aalborg, 37-48    [17] Shih, B., Koedinger, K.R., Scheines, R. 2008. A response time  model for bottom-out hints as worked examples. In R. de Baker, T.  Barnes, J. Beck (Eds), Proc. 1st International Conference on  Educational Data Mining, Montreal, 117126    [18] Papamitsiou, Z., Economides, A. A. 2014. Students perception of  performance vs. actual performance during computer-based testing:  a temporal approach, 8th Int. Technology, Education and  Development Conference, Valencia, 401-411    [19] Terzis, V., Economides, A. A. 2011. The acceptance and use of  computer based assessment, Comput Educ, 56(4), 10321044    [20] Tan, P-N., Steinbach, M., Kumar, V. 2005. Introduction to Data  Mining, (1st Edition). Addison-Wesley Longman Publishing Co.,  Inc., Boston, MA, USA    [21] Alpaydin, E. 2010. Introduction to Machine Learning. MIT Press   [22] Mitchell, T. 1997. Machine Learning, Mcgraw-Hill, New York   [23] Arlot S., Celisse A. 2010. A survey of cross-validation procedures   for model selection. Statistics Surveys, 4, 4079   [24] Cortes, C., Vapnik, V. 1995. Support-vector networks. Machine   Learning, 20 (3), 273   [25] Cristianini, N., Shawe-Taylor, J. 2000. An Introduction to Support   Vector Machines and Other Kernel-based Learning Methods,  Cambridge University Press, London, UK.   [26] Altman, N. S. 1992. An introduction to kernel and nearest-neighbor  nonparametric regression. Am. Stat., 46 (3), 175-185    [27] Cover, T. Hart, P. 1967. Nearest neighbor pattern  classification. IEEE Trans on Information Theory, 13 (1), 2127    [28] Breiman, L.1996.Bagging predictors. Machine Learning,24,123-140   [29] Haykin, S. 1998. Neural networks: A comprehensive foundation   (2nd ed.). Prentice Hall.  [30] Papamitsiou, Z., Economides, A.A. 2015. A temporal estimation of   students on-task mental effort and its effect on students  performance during computer based testing, IEEE 18th Int. Conf. on  Interactive Collaborative Learning (ICL2015)   [31] Economides, A. A. 2009. Adaptive context-aware pervasive and  ubiquitous learning. International Journal of Technology Enhanced  Learning, 1(3), 169-192.        "}
{"index":{"_id":"38"}}
{"datatype":"inproceedings","key":"Renz:2016:UAT:2883851.2883876","author":"Renz, Jan and Hoffmann, Daniel and Staubitz, Thomas and Meinel, Christoph","title":"Using A/B Testing in MOOC Environments","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"304--313","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883876","doi":"10.1145/2883851.2883876","acmid":"2883876","publisher":"ACM","address":"New York, NY, USA","keywords":"A/B testing, MOOC, controlled online tests, e-learning, microservice","abstract":"In recent years, Massive Open Online Courses (MOOCs) have become a phenomenon offering the possibility to teach thousands of participants simultaneously. In the same time the platforms used to deliver these courses are still in their fledgling stages. While course content and didactics of those massive courses are the primary key factors for the success of courses, still a smart platform may increase or decrease the learners experience and his learning outcome. The paper at hand proposes the usage of an A/B testing framework that is able to be used within an micro-service architecture to validate hypotheses about how learners use the platform and to enable data-driven decisions about new features and settings. To evaluate this framework three new features (Onboarding Tour, Reminder Mails and a Pinboard Digest) have been identified based on a user survey. They have been implemented and introduced on two large MOOC platforms and their influence on the learners behavior have been measured. Finally this paper proposes a data driven decision workflow for the introduction of new features and settings on e-learning platforms.","pdf":"Using A/B Testing in MOOC Environments  Jan Renz Hasso Plattner Institute  Prof.-Dr.-Helmert-Str. 2-3 Potsdam, Germany jan.renz@hpi.de  Daniel.Hoffmann Hasso Plattner Institute  Prof.-Dr.-Helmert-Str. 2-3 Potsdam, Germany  daniel.hoffmann@hpi.de  Thomas Staubitz Hasso Plattner Institute  Prof.-Dr.-Helmert-Str. 2-3 Potsdam, Germany  thomas.staubitz@hpi.de Christoph Meinel  Hasso Plattner Institute Prof.-Dr.-Helmert-Str. 2-3  Potsdam, Germany christoph.meinel@hpi.de  ABSTRACT In recent years, Massive Open Online Courses (MOOCs) have become a phenomenon oering the possibility to teach thousands of participants simultaneously. In the same time the platforms used to deliver these courses are still in their fledgling stages. While course content and didactics of those massive courses are the primary key factors for the success of courses, still a smart platform may increase or decrease the learners experience and his learning outcome. The paper at hand proposes the usage of an A/B testing framework that is able to be used within an micro-service architecture to validate hypotheses about how learners use the platform and to enable data-driven decisions about new features and settings. To evaluate this framework three new features (Onboarding Tour, Reminder Mails and a Pinboard Digest) have been identified based on a user survey. They have been implemented and introduced on two large MOOC platforms and their influence on the learners behavior have been mea- sured. Finally this paper proposes a data driven decision workflow for the introduction of new features and settings on e-learning platforms.  Categories and Subject Descriptors H.4 [Information Systems Applications]; H.5 [Information interfaces and presentation]; K.3.1 [Computer Uses in Education]; J.4 [Social and Behavioral Sciences]  Keywords MOOC, A/B Testing, microservice, E-Learning, Controlled Online Tests  Permission to make digital or hard copies of all or part of this work for per- sonal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstract- ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-4190-5/16/04...$15.00 DOI: http://dx.doi.org/10.1145/2883851.2883876  1. INTRODUCTION 1.1 Controlled Online Tests In the 18th century, a British naval captain wondered why sailors serving on the ships of the Mediterranean countries did not suer from scurvy. On those ships, citrus fruits were part of the rations. So he ordered one half of his crew to eat limes (the treatment group), while the other half consumed the same rations they received before (the control group). Despite the displeasure of the crew, the experiment was successful. Without knowing the cause of the eect (that lack of vitamin C caused scurvy), he found out that limes prevented it [18]. This lead to citrus fruits being a part of the sailors rations and a healthier crew on all ships.  In the late 1990s, Greg Linden, a software engineer at Ama- zon, developed a prototype showing product recommenda- tions based on the current shopping cart content at check- out [13]. He was convinced that transferring the impulse buys, like candy at the checkout lane, from grocery stores to online shopping and improving them by personalization would increase the conversion rate and so lead to more in- come for the shop. While he received positive feedback from his co-workers, one of his bosses, a marketing senior vice- president strongly opposed his idea because he believed it would distract customers from checking out and therefore lead to a loss of revenue. So Linden was forbidden to work on it any further. Being convinced of the possible impact he did not follow this management decision, but instead launched a controlled online test. One group of customers saw the rec- ommendations, the other did not. The senior vice-president was furious when he found out that the feature was launched. But it won by such a wide margin that not having it live was costing Amazon a noticeable chunk of change, so he could not keep up his concerns. The feature was rolled out for all users short time later. Today testing is an essential part of amazons philosophy.  These two examples show how experimentation helps to vali- date hypotheses with data and how they may also contradict intuition and preconceptions. As pointed out by Thomke, experimentation matters because it fuels the discovery and creation of knowledge and thereby leads to the development and improvement of products, processes, systems, and orga- nizations. ([25])    Experimentation has long been costly and time-consuming as it would require special lab setups or paying agencies, but the web makes it possible to quickly and cost-eciently evaluate new ideas using controlled experiments, also called A/B tests, split tests, or randomized experiments [9]. As stated in [8] it can be expected that small changes can have a big impact to key metrics. They also integrate well with agile methodologies, such as the ones described in Lean Startup by Eric Ries, which is an approach for launching businesses and products, that relies on validated learning, scientific experimentation, and iterative product releases to shorten product development cycles, measure progress, and gain valuable customer feedback. ([11]). He states that no one, in despite of his expertise can fully anticipate the users behaviour, so only by testing the best solutions for both the user and the provider can be determined.  MOOCs (used here as a synonym for scalable e-learning platforms) provide their service to thousands of learners, so they have a critical mass of users that enables the platform providers to run those controlled online experiments. Instead of using these tests to increase conversion rates or sales, the aim here is to identify instruments to optimize the learning experience and the learning outcome of those users.  1.2 openHPI This work focuses on the MOOC platforms openHPI and openSAP. openHPI is a non-profit project provided by the Hasso Plattner Institute (HPI) in Potsdam, Germany for opening courses derived from the curriculum of the Institute for the general public.  The web university team of the chair of Internet and Web Technologies had previous experience with online learning research, having established the tele-TASK platform for recorded HPI lectures. They also provide a tele-recording system. But they have never been fully satisfied with the usage of the provided content.  In November 2012, the first MOOC in German language was held on openHPI, rendering HPI one of the first European MOOC providers. In 2014 an average of 7,000 participants have been enrolled at course end [17]. SAP, a well-known German software company, published their first MOOC on openSAP in May 2013. It targets professionals working with SAP products and is also used to educate SAP employees [22].  Both providers use the same underlying system, internally called Xikolo (Tsonga, a Bantu language, for school). Thus, the implementation of the A/B Testing framework and the changes to the user interface are equally applicable for openHPI and openSAP and have been applied in the aca- demic context as well as in the enterprise learning context.  This paper describes the introduction of an A/B-Testing Framework to a micro-service based MOOC platform and the results obtained evaluating this service with dierent A/B tests. The remainder of the paper at hand is structured as follows:   Section 2 gives an overview on the architecture of the  A/B Testing Framework and the underlying Learning Analytics Engine.   Section 4 describes how the possible test candidates have been identified.   In Section 5 the three A/B tests conducted and their results are introduced and discussed.   A conclusion and a discussion of future work can be found in Section 6.  2. A/B TESTING IN MICROSERVICE BASED LEARNING PLATFORMS  With the advent of MOOCs a large amount of educational data became available. There are two communities dealing with its analysis: Learning Analytics and Educational Data Mining. While they have many things in common, both are concerned about how to collect and analyze large-scale educational data for a better understanding of learning and learners, they have slightly dierent goals [23]. Learning Analytics aims at providing insights to teachers and learners, whereas Educational Data Mining rather focuses on auto- matic adaptation of the learning process with not necessarily any human interference.  For a better evaluation of learning data across dierent MOOCs, a general database schema was proposed by Veera- machaneni et al. called MOOCdb [26, 7]. The authors suggest developing a shared standard set of features that could be extracted across courses and across platforms ([26]). The schema includes three dierent modes named observing, sub- mitting, collaborating and feedback.  Another approach is the Experience API (also known as xAPI or TinCan API) suggested by the Advanced Distributed Learning (ADL) Initiative [1]. It defines a way to store statements of experience, typically but not necessarily in a learning environment. A statement has at least three parts actor, verb and object representing subject, verb and object in a sentence. Additional properties can include references to resources like an UUID as id, a result denoting the outcome, contextual information in context or the time of the statement in timestamp.  In order to gather learning data on openHPI, a versatile and scalable solution called Lanalytics which allows to track user actions in a service-oriented environment [19] (for details on openHPIs distributed architecture see subsection 2.1) was implemented. The recorded actions can be stored in a variety of dierent formats (such as MOOCdb and Experience API) and data stores (such as PostgreSQL1, a relational database, elasticsearch2, a document database, and Neo4j3, a graph database). This LAnalytics framework sets the foundation for further research in this work.  2.1 openHPI Architecture openHPI is based on a micro-service architecture [15], which means there is no monolithic application, but multiple ser- vices, each with a defined responsibility [12]. The decision 1PostgreSQL: http://www.postgresql.org 2elasticsearch: https://www.elastic.co 3Neo4j: http://neo4j.com  http://www.postgresql.org https://www.elastic.co http://neo4j.com   to go for a Service Oriented Architecture was based on the learnings that resulted from employing and extending a mono- lithic application to run MOOCs in a previous version of the platform. Each service runs in its own process and handles only a small amount of data storage and business logic. This approach has a number of advantages. As the services are designed around capabilities, each service can use the technology that serves best the use case including dierent programming languages or DBMS that fit best [12]. Currently all but one service is implemented as a RubyOn- Rails application due to the existing developer qualification. Scaling in a micro-service architecture can be realized by distributing the services across servers, replicating only those needed. With a monolithic application, the complete appli- cation has to be replicated. Each service can be deployed independently, which makes it easier to continuously deploy new versions of the services [20]. In contrast to monolithic applications a fault in one service does not necessarily aect the whole application. Lastly, micro-services are relatively small and therefore easier to understand for a developer. Most of openHPIs developers are students and spend only a few hours per week actively developing. Therefore, this architecture not only minimizes the risk of breaking other parts of the software (by isolation), it also enables developers to become experts in a certain part of the app (exposed by one or more services).  While having many advantages, the presented architecture prohibits using one of the many available A/B-Testing solu- tions like the Ruby gems split4 and vanity5. These libraries are designed to work within monolithic applications. Other existing solutions, such as Optimizely use JavaScript to al- ter the interface and to measure events. These solutions mostly target marketing driven A/B Tests with a simple set of metrics and changes (for example display a dierent pricetag or alternative landing page). But in our case many functionalities that might me relevant for A/B testing are not only part of the User Interface (UI). Instead they might include actions that happen in one of the underlying services or even asynchronous actions that are not UI related at all. This is where UI focused approaches will fail.  Additionally, the measured metric is not simply tracking conversions, but queries possibly complex data gathered by the Learning Analytics framework [24]. Furthermore the used metrics may consist of learning data. Keeping this data within the system and not sending it to a 3rd party tool avoids problems with data privacy. So a dedicated custom prototype was built to enable A/B testing in the Xikolo-framework.  2.2 Workflow Each time a user accesses a page within the learning platform, the system detects if there are any tests currently running in the scope of the visited page by querying the Grouping Service. If there are tests running, the system needs to check if the user has one of this test features enabled. This check is handled by the Account Service. It will return an already given test group assignment or create a new one by applying 4split, the Rack Based A/B testing framework: https:// github.com/splitrb/split  5Vanity, Experiment Driven Development for Ruby: https: //github.com/assaf/vanity  a set of rules and deciding if the user should be in the test or the control group for each requested test. In Figure 1 the communication between the dierent parts of the system is shown in more detail. While this workflow generates additional requests, there was no measurable performance decrease of the front-end, as this calls could be run in parallel with other calls to the server backend. All code that is related to a function that is currently in A/B testing must be encapsulated in a code block. This code will only be executed if this user is part of the test group. This way of implemented features could later be used to make this feature being active or deactivated on a per platform or per user base using so called feature flippers, so this can be considered no extra work.  Figure 1: Abstract sequence diagram showing the communication between the browser and the ser- vices.  2.3 Administrators Dashboard The creation of new AB tests with a certain complexity involves writing additional code and taking care that this code is well tested and rolled out, so this part can only be  https://github.com/splitrb/split https://github.com/splitrb/split https://github.com/assaf/vanity https://github.com/assaf/vanity   provided by the development team. The management of running A/B tests can be achieved using a newly introduced section within the backend of the learning software. There, administrators (or all users equipped with the needed set of permission) can enable, edit and view user tests. This includes not only the meta data of the user tests, but also the live test results. All those users can see the gathered data on a dashboard shown in item 2. For each metric the number of participants, the number of participants that did not yet finish the user test and the number of participants for whom the metric wait interval did not end yet is displayed. If a metric has been evaluated for some users in both groups the eect size is displayed, calculated as Cohens d [3].  Figure 2: Screenshot of the administrators dash- board of a user test showing 1) general properties of the test, 2) and for each metric the indices 3) and names of the test groups, 4) the number of partic- ipants, 5) the number of participants that did not finish the test, 6) the trials waiting for the metric result, 7) the mean of the group, 8) the p-value of statistical significance, 9) the eect size, 10) the re- quired number of participants for a power of 0.8, 11) box plots of the group results.  3. METRICS Witte and Witte define quantitative data as a set of ob- servations where any single observation is a number that represents an amount or a count, whereas qualitative data is defined as a set of observations where any single observation is a word, or a sentence, or a description, or a code that represents a category ([28]).  Thus, quantitative data describes the intensity of a feature and is measured on a numerical scale. Qualitative data has a finite number of values and can sometimes be ordinally scaled. Qualitative usability studies observe directly how the user interacts with the technology, noting their behavior and attitudes, while quantitative studies indirectly gather numerical values about the interaction, mostly for a later mathematical analysis [21].  Each user test can have multiple metrics based on quantita- tive data, for example if the user enrolled in the course in question or the number of specific actions performed by the user in a given time frame. Most metrics require some time  to pass in between the beginning of the user test (the user being assigned to one of the groups and presented with a certain functionality) and the measurement of the metrics. If the user test is course-specific, only actions concerning this course are queried. The amount of time relates on the metrics. Metrics that are based on the learning outcome might need a certain amount of self tests done by the users or the course to be ended. Other metrics that focus on user activity may need at least some days.  Most of the metrics query data is gathered by the LAnalytics service. This service processes messages sent in the services on certain events, for example if a user asks a new question, answers one or watches a video. This data is then sent and received using the Msgr gem6, which builds on RabbitMQ7. The received events are then transformed and processed by several pipelines. While this is an asynchronous processing, usually all events are processed near real time. The LAna- lytics service allows the usage of dierent storage engines, however all relevant events for the metrics for this tests are stored in an elasticsearch instance using the Experience API [1] standard. An Experience API statement consists of four parts: subject, verb and object, in this case user, verb and resource. The resource needs a UUID (Universally Unique Identifier) and can contain additional information for faster processing for example the question title. Additionally, the statement has a timestamp and a context, for example the course ID.  The following metrics are currently implemented and can be used within A/B tests:  3.1 Pinboard Posting Activity The pinboard posting activity counts how often a user asks, answers and comments questions and discussions in the pin- board of a course.  Verbs: ASKED QUESTION, ANSWERED QUESTION, COMMENTED  3.2 Pinboard Watch Count The pinboard watch count denotes the number of viewed questions and discussions of a user.  Verb: WATCHED QUESTION  3.3 Pinboard Activity This pinboard activity combines pinboard posting activity and pinboard watch count. Considering the dierent amounts of eort, a weighting is applied. The posting activity con- tributes with a ratio of 90%, while the watch count is weighted with 10%.  3.4 Question Response Time The question response time denotes how long after a question was asked, the question is answered by a user. To compute this metric all Experience API statements with the verb ANSWERED QUESTION are retrieved for a user, then the matching ASKED QUESTION statement is queried and  6Msgr: https://github.com/jgraichen/msgr 7RabbitMQ: https://www.rabbitmq.com/  https://github.com/jgraichen/msgr https://www.rabbitmq.com/   the average dierence between this timestamps is computed. Since not all users answer questions in the specified time frame, empty values need to be allowed, but these values are removed before significance testing.  3.5 Visit Count The visit count denotes how many items a user visited, in- cluding videos, selftests and text parts. This metric can be filtered by time and course. Verbs: VISITED  3.6 Video Visit Count The video visit count denotes the number of visited videos per user. This metric can be filtered by time, video and course.  Verb: VISITED Filter: content type == video  3.7 Course Activity The course activity summarizes the aforementioned metrics to measure the overall activity of a user in a course. The pinboard activity is weighted with 50%, while the visit count is included without weight.  3.8 Course Points After the end of a course the number of points are persisted and the quantiles of the users points are calculated. For each enrollment a completed event is emitted, which is received and processed by the LAnalytics Service. The course points metric returns the number of points a user received in a specified course.  Verbs: COURSE COMPLETED  3.9 Micro Survey Not all interface changes can be evaluated with an objective metric, for example design changes. For these cases a quali- tative feedback metric is used. It allows for fast evaluation by prompting users to rate whether they like the displayed version. In contrast to the other metrics, this one is just a concept and is not yet implemented. For this metric every users would be asked to rate a functionality or a design. Then the ratings provided by test and control group can be compared.  4. IDENTIFYING TEST CANDIDATES To utilize the power of an A/B Testing framework, possible test candidates must be identified and selected.  4.1 Dropout and Absence in MOOCs Since MOOCs can be joined freely and impose no commit- ment on the user, there is a high number of students who do not visit the course after enrollment, stop visiting it after a while, or leave it completely. The reported dropout rate on Coursera is 91% to 93% [10] and on openHPI 8 it is between 77 and 82% [17, 16]. So the number of registrations should be seen as an indicator of interest rather than the ambition to finish the course. Halawa et al. [6] claim that not only complete dropout is a problem, but also periods of absence  8openHPI: https://open.hpi.de  which have an impact on the users performance. While 66% of all students of the analyzed course with an absence of less than two weeks entered the final exam and scored 71% on average, only 13% of the students that were absent longer than one month took the final exam with a mean score of 46%.  Several recent works addressed this issue. One counter- measure is to make the course content available for every interested person. Only if wanting to take an assignment or to contribute to the forums a registration is necessary. This way people that just want to take a look at the content but are not interested in taking the course are filtered out from the participants.  Yang et al. [29] point out that higher social engagement corresponds with lower dropout, because it promotes com- mitment and therefore lower attrition. This was also shown by Grunewald et al. [5] in an analysis of the first two openHPI courses. However, one half of the participants did not actively participate in forum discussions. openHPI programming courses have higher completion rates than other courses. An average of 31% received a certificate in the two program- ming courses, while the average completion rate in 2014 was 19.2% [17]. The courses provide an interactive programming environment. Exercises have predefined test cases, against which students can try their code against. This higher engage- ment of learners might be a reason for the larger completion rate.  4.2 User Experience Survey For a prior investigation of how users perceive their experi- ence on openHPI, we conducted a survey. It was announced via an email to all users and on openHPIs social media channels. From March 25, 2015 to May 25, 2015, all users have been asked for their opinion about their user experience on and the usability of the platform. The survey contained questions about existing functionalities, but also about un- finished or unpublished functionalities and functionalities not available on the platforms, but maybe available on other MOOC platforms. The survey yielded 512 responses of which 161 were incomplete. For the following evaluation only the complete responses are considered. 61% of the participants are older than 40 years and 63% are male. 71.6% of all par- ticipants are satisfied with the overall usability of openHPI (a rating of 4 or 5 on a scale from 1 to 5). 73% were satisfied with the learnability, 73.1% with the video player and 71.9% with the tests. Only the discussions deviate from these re- sults. They have a satisfaction rate of 61.5%. Additionally, when asked whether the discussions support their learning process, only 36.1% agreed. Regarding gamification, 29.7% rated the importance of gamified elements for them with 4 or 5. 34.9% agreed that gamification elements would influence their course participation in a positive way.  In conclusion, the overall perception of the usability of the openHPI platform is at a high level, but the discussions are not as helpful as intended. The didactical concept and expectation and the user perception diverge. This gap can be closed using the experimentation framework and should be addressed when optimizing the learning outcome.  5. CONCLUDED TESTS  https://open.hpi.de   Based on the survey results three tests have been selected to evaluate the introduced A/B testing framework based on the expected impact. The selection was based on the predicted user acceptance and the expected impact in combination with the amount of work needed to implement these new features. As the learners in MOOCs are connected via the forum, it is also important to respect this fact while choosing possible test candidates, as this could lead to confusion or jealousy. While all of these tests required to implement prototypes of these features none of these functionalities were so essential or prominent that not having it may lead to disappointed users. Some of the tests featured additional explanatory text, explaining that the user is part of a test group. One possible test candidate featuring gamification elements which are really prominent on the platform was not chosen for this reason. As we run several platforms, a possible test strategy is to roll it out on one instance of the platform only and then normalize the metrics. All tests could be easily stopped or deactivated by the user, disabling the tested feature for that user.  5.1 Onboarding  Figure 3: Third step of the tour explaining the course navigation  It may be useful for inexperienced users to get an overview about the systems functionality which could lead to a more intense usage of the MOOC platform. A tour was created that lets them visit the most important pages and explains the features in eleven steps: It starts automatically after a user enrolls for their first course and highlights a single part of the currently displayed page while providing some additional explanatory test as shown in Fig. 3. The first steps explain the course area containing the week and item navigation. Then the user is forwarded to the pinboard page and dierence between questions and discussions (questions are posted with a specific problem in mind and have answers, discussions want to debate certain course contents and have only comments) are explained. Afterwards, the progress page is opened and the progress layout and how to retrieve certificates after course end is explained. The tour was implemented using intro.js9 in a version modified to support multiple pages.  To validate the hypothesis, we conducted a user test using the aforementioned testing framework.  9intro.js: https://github.com/usablica/intro.js  The course activity metric (subsection 3.7) and the pinboard activity metrics (subsection 3.3) were used to validate the impact of the alternative group.  5.1.1 Alternatives  After enrollment the groups saw:  Group 0: a confirmation that they are enrolled.  Group 1: a welcome message and a tour guiding them through the course area.  5.1.2 Setup  The test ran for a week starting on May 20, 2015 17:20 targeting users who enrolled for their first course on openHPI. It started after enrollment and ended immediately for the control group and after skipping or finishing the tour for the treatment group.  The control group comprised 172 participants, the alternative 119 (plus 16 that did not finish the tour).  All metrics were evaluated after one week.  5.1.3 Results  The results (Table 1, Table 2) show that an onboarding tour increases the number of visits of learning items (34.5 % for videos, 27.9 % for all items). However, the dierence is not significant, p<0.05.  Table 1: Onboarding: Results for visit count  Name Participants Mean Change p  Control 172 11.49 Tour 119 14.70 +27.93% 0.15 Total 291 12.80  Table 2: Onboarding: Results for video visit count  Name Participants Mean Change p  Control 172 4.01 Tour 119 5.39 +34.48% 0.11 Total 291 4.58  The change in pinboard activity is negative (-7%, Table 3).  Table 3: Onboarding: Results for pinboard activity  Name Participants Mean Change p  Control 172 0.27 Tour 119 0.25 -6.99% 0.55 Total 291 0.26  5.2 Reminder Mails A possible measure to prevent that learners drop out of the course over time is to send reminder emails after a certain period of inactivity. Since on most platforms of the openHPI ecosystem new contents are published on Mondays, we chose a period of four days to reach participants before the weekend, when they have more time to work on the course.  https://github.com/usablica/intro.js   The emails are sent by the Notification Service, which is responsible of receiving updates sent in other services and forwarding them as a notification on the web or as an email. The regular email for this test is initiated using a daily routine at 2:00 UTC. It queries all enrollments of users that have not visited their course for the last four days and have visited less than 90% of the course content. The latter restriction prevents users that nearly finished a course, but are not interested in some parts of it to repeatedly receive reminders. Depending on the group, the popular questions of the courses discussions of the four last days and unwatched videos are queried for each enrollment.  5.2.1 Alternatives  The test is designed in a multivariate manner and comprises three alternative groups. The groups are sent:  Group 0: no email  Group 1: an email reminding them to visit the course again  Group 2: an email reminding them to visit the course again including an extract of the latest activity in discussions  Group 3: an email reminding them to visit the course again including videos they did not see yet  Group 4: an email reminding them to visit the course again including an extract of the latest activity in discussions and videos they did not see yet  An exemplary email as a user part of group 4 would receive is show in Figure 4. As with all notifications, the users can opt-out of these emails.  5.2.2 Setup  The test ran for two weeks starting on July 6, 2015, 22:00 UTC, targeting all users enrolled in Web Technologies on openHPI. A trial started when the first reminder email was sent and ended upon successful delivery.  The control group comprised 1830 participants, the alter- natives 1831, 1833, 1834, and 1868 summing up to 9196 participants in total.  All metrics were evaluated after one week. Only the course point metric was evaluated after the certificates were pub- lished.  5.2.3 Results  The results show that sending a reminder email increases the overall course activity (Table 4). However, only emails containing videos show a statistically significant change of 37.4% with a p-value of 0.02 < 0.05 for videos and 43.3% (p-value 0.009 < 0.05) for videos and questions.  The same eect can be seen for the visit count (Table 5) with an increase of 38.3% and a p-value of 0.018 < 0.05 for videos, and 43.5% (p-value 0.009 < 0.05) for videos and questions and even stronger for the video visit count (Table 6) with a gain of 60.4% and a p-value of 0.004 < 0.05 for videos, and 73.1% (p-value 0.002 < 0.05) for videos and questions.  Figure 4: Sample reminder email for group 4  Table 4: Reminder Emails: Results for course activ- ity  Name Participants Mean Change p  Control 1830 1.12 Text 1831 1.34 19.92% 0.109 Questions 1833 1.2 6.96% 0.337 Videos 1834 1.53 37.36% 0.02 Q. and V. 1868 1.6 43.32% 0.009 Total 9196 1.36  Table 5: Reminder Emails: Results for visit count Name Participants Mean Change p  Control 1830 1.1 Text 1831 1.33 20.63% 0.102 Questions 1833 1.19 7.45% 0.327 Videos 1834 1.53 38.26% 0.018 Q. and V. 1868 1.58 43.46% 0.009 Total 9196 1.35  The results for pinboard activity (Table 7) are surprising, as they reveal a decrease of discussions visits for all alternatives. The decrease is less for the alternatives showing questions, but the results still indicate that reminder emails have no impact, if not a negative impact (that was not tested), on the number of visits in the discussions. A possible explanation could be that users that saw the recommended content of    Table 6: Reminder Emails: Results for video visit count  Name Participants Mean Change p  Control 1830 0.47 Text 1831 0.6 29.0% 0.07 Questions 1833 0.59 26.87% 0.11 Videos 1834 0.75 60.38% 0.004 Q. and V. 1868 0.81 73.08% 0.002 Total 9196 0.64  the pinboard could realise that there is no content that would motivate them to visit the pinboard, while otherwise they may just have browser there and then explored some interesting threads.  Table 7: Reminder Emails: Results for pinboard watch count  Name Participants Mean Change p  Control 1830 0.12 Text 1831 0.07 -46.22% 0.915 Questions 1833 0.08 -38.22% 0.846 Videos 1834 0.06 -51.23% 0.932 Q. and V. 1868 0.1 -14.33% 0.635 Total 9196 0.09  Another outcome we did not expect was that the emails have no positive eect on the total points achieved in the course. As Table 8 shows, the means of all alternatives are inferior to that of the control group.  Table 8: Reminder Emails: Results for course points  Name Participants Mean Change p  Control 1830 9.57 Text 1831 8.43 -11.88% 0.899 Questions 1833 7.87 -17.71% 0.977 Videos 1834 8.71 -8.97% 0.83 Q. and V. 1868 9.51 -0.63% 0.526 Total 9196 8.82  5.3 Pinboard Digest Mails In the pinboard students can discuss course contents and ask questions about advanced topics to help them to understand the taught content better. As outlined in subsection 4.1 social interaction also prevents dropouts. However, in the user experience survey on openHPI (see subsection 4.2), the statement Did the discussions support your learning process received only a rating of 2.93 on a scale from 1 to 5. This test evaluated if a daily pinboard overview email fuels the participation in the discussions of a course. Such an email contains questions the user has not yet seen, but received much attention from others. It also includes unanswered questions to reduce the time until a question is answered.  These emails are also sent each day at 4:00 UTC. For each enrollment in an active course with an open forum it is determined if the user visited the course in the last two weeks. This avoids disturbing users that are dropped out  to receive these emails. Reminder emails are designed for those cases. A background job queries the pinboard service for unseen questions for that user and unanswered questions in the course.  5.3.1 Alternatives  The groups are sent:  Group 0: no email  Group 1: an email including five unseen questions with the most activity and five unanswered questions with the demand to answer them  An exemplary email is show in Figure 5. Similar to the reminder emails, users can disable these emails.  Figure 5: Sample pinboard digest email for group 1  5.3.2 Setup  The test ran for one week starting on July 15, 2015, 00:00 UTC, targeting all users enrolled in Driving Business Results    with Big Data on openSAP . A trial started when the first pinboard digest email was sent and ended upon successful delivery. The control group comprised 2019 participants, the alternative 2035 summing up to 4054 participants. All metrics were evaluated after one week.  5.3.3 Results  The results show that sending daily discussions overviews increases the pinboard activity of the aected users (Table 9) by 64% which is a statistically significant improvement (p- value 0.021 < 0.05). They also raise the posting activity in the discussions (Table 10) by 140%, which is statistically significant as well (p-value 0.03 < 0.05).  Table 9: Pinboard Digest Mail: Results for pinboard activity  Name Participants Mean Change p  Control 2019 0.06 Digest 2035 0.1 63.98% 0.021 Total 4054 0.08  Table 10: Pinboard Digest Mail: Results for pin- board posting activity  Name Participants Mean Change p  Control 2019 0.02 Digest 2035 0.04 139.77% 0.03 Total 4054 0.03  6. CONCLUSION AND FUTURE WORK The introduction of an A/B Testing Framework gives plat- form developers, architects and providers the needed toolset to be aware of the impact of new instruments and features before introducing them to all users. This lays the foundation for constant improvement of the user experience and learning outcome on openHPI and MOOC platforms in general. The possibility to have a live view on the current data situation was very helpful and empowered agile decisions like extend- ing tests or run them on other instances of the portal as well. While the evaluation of the new framework was successful, the majority of the test runs have been successful.  It is not uncommon for A/B-tests to fail. In fact, reported success rates hover around 10 and 12.5% [4, 2], the others show little change between the groups. But in the case of the onboarding test the margin was rather large but not significant. This test should be repeated over a longer time span to examine if the results are similar and whether a larger sample size causes statistical significance.  The reminder email test confirmed our hypothesis that re- minding students after a certain period of absence increases their course activity. Showing videos in the email was the decisive factor for statistical significance, which indicates that users are more content driven, not social driven. We also received positive feedback from users who appreciated the feature. However, the test also yielded surprising results. The email decreased the pinboard visits regardless of the fact whether it included questions or not. It also did not aect the course results of the learners in a positive way. A  possible explanation could be that it was performed in the last two weeks of the course running for six weeks. Users that dropped out before might either be not determined to complete the course in the first place or were deterred by the workload needed to catch up. A test running over the full length of a course could show if the results can be reproduced.  The test concerning pinboard digest emails verified our as- sumption that it increases the participation in the discussions. The alternative succeeded by the large margin of 140% more questions, answers and comments.  Some of the results confirm our hypotheses and some con- tradict our intuition. Hence, the tool justifies its place in the openHPI ecosystem. It allows to decide which features should be included backed with data. Disadvantageous ones are not published permanently and only those with a positive impact on the learning behavior or experience are included.  Based on this results next steps could be taken to improve this framework. This includes introducing pre- and post tests, as well as other actions to allow better interpretation as suggested in [14].  6.1 New feature release cycle Thanks to the presented framework feature decisions for openHPI and openSAP can be based on empirical data. The workflow of releasing new features now looks as follows. After the implementation of the feature it is reviewed internally in the development team. Depending on the importance of the feature, it is also reviewed by the customer / product owner. If the reviews are positive the feature is deployed, but deactivated using feature flippers. Metrics are defined to assess the feature. Then one or more user tests are performed. After a predefined time span the test results are reviewed. If there is an improvement in the metrics and there is no negative impact (for example comunicative pollution) the feature is rolled out for all users, since it evidentially improves the learning behavior of the participants. If no change is detected, it is rolled out but deactivated by default. Users that want to use the feature can activate it. If the test reveals that the new feature performs worse than the control, it is not rolled out.  This new workflow allows for fast evaluation of features. Rather than deciding by intuition which features should be added, they are tested beforehand if they are beneficial for the users. Only those that perform better are activated for all users.  6.2 Evaluation of used metrics and introduc- tion of negative metrics  The used metrics could be evaluated based on the mea- suremnt of this metrics in courses. For the tests where the activity was increased but the course results stayed the same, additional metrics should be introduced to assure that there is no negative impact of the introduced test. Therefore sev- eral metrics that are based on negative events should be introduced. As during the time of evaluation of the new framework such events were not recorded by the learning analytics service, these metrics are not yet implemented. One    possible metric is the amount of course or platform unen- rollments. Another possible metric is the amount of users unsubscribing from notification mails. This metric could help to indicate users being annoyed by too many mails received from the platform.  6.2.1 Marketing or content driven style A/B tests  All tests run in the context of the evaluation of this new framework are based on new functionalities. Still, it could be also used to run A/B test evaluating smaller, more content- driven or UI driven changes within the platform. This work could be started on the course detail pages. These pages can be compared to classical landing pages, therefore it can be predicted that they have a huge unused potential. An enrollment rate per page visitor rate could be used as a metric. However, given the simple requirements of tests like this could also be run by using Optimizely or other externally provided testing tools. As Willems states in [27] other MOOC platforms like edX allows optional content modules for a given sub set of learners. Also Khan Academy allows A/B testing on certain exercise types. All of these AB tests may be supported by the platform, but do not include functional additions. These types of tests involve additional creation of content, therefore they are hard to realize given the bottleneck of content creation, however the presented A/B testing framework could be used for tests like this.  6.2.2 Community contributed test cased  Another idea of gathering test cases is to collect them from users and the research community. Therefore the authors are open for suggestions of tests. It is planned to at least run three community contributed tests within the next month.  7. REFERENCES [1] Advanced Distributed Learning (ADL). Experience  API v1.0.1, 2014. [2] P. Chopra. Appsumo reveals its A/B testing secret:  only 1 out of 8 tests produce results - VWO Blog, 2011. [3] J. Cohen. Statistical Power Analysis for the Behaviorial  Sciences. Lawrence Erlbaum Associates, Inc, 1977. [4] M. A. Farakh. Most of your AB-tests will fail, 2013. [5] F. Grunewald, E. Mazandarani, C. Meinel, R. Teusner,  M. Totschnig, and C. Willems. openHPI - A case-study on the emergence of two learning communities. In 2013 IEEE Global Engineering Education Conference (EDUCON), pages 13231331. IEEE, Mar. 2013.  [6] S. Halawa, D. Greene, and J. Mitchell. Dropout Prediction in MOOCs using Learner Activity Features. eLearning Papers, 37(March):110, 2014.  [7] S. Halawa and U.-m. O. Reilly. MOOCdb : Developing Standards and Systems to support MOOC Data Science Massachusetts Institute of Technology. Technical report, 2014.  [8] R. Kohavi, A. Deng, R. Longbotham, and Y. Xu. Seven rules of thumb for web site experimenters. In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 14, pages 18571866, New York, NY, USA, 2014. ACM.  [9] R. Kohavi, R. M. Henne, and D. Sommerfield. Practical guide to controlled experiments on the web. In Proceedings of the 13th ACM SIGKDD international  conference on Knowledge discovery and data mining - KDD 07, volume 2007, page 959, New York, New York, USA, 2007. ACM Press.  [10] D. Koller. MOOCs on the Move: How Coursera Is Disrupting the Traditional Classroom, 2012.  [11] J. H. Kolodziej. The lean team., volume 37. 2011. [12] J. Lewis and M. Fowler. Microservices, 2014. [13] G. Linden. Geeking with Greg: Early Amazon:  Shopping cart recommendations, 2006. [14] M. W. Lipsey, K. Puzio, C. Yun, M. A. Hebert,  K. Steinka-Fry, M. W. Cole, M. Roberts, K. S. Anthony, and M. D. Busick. Translating the statistical representation of the eects of education interventions into more readily interpretable forms. National Center for Special Education Research, 2012.  [15] C. Meinel, M. Totschnig, and C. Willems. openHPI: Evolution of a MOOC Platform from LMS to SOA. Proceedings of the 5th International Conference on Computer Supported Education, pages 593598, 2013.  [16] C. Meinel, C. Willems, J. Renz, and T. Staubitz. Reflections on Enrollment Numbers and Success Rates at the openHPI MOOC Platform. In Proceedings of the European MOOC Stakeholder Summit 2014, pages 101106, Lausanne, Switzerland, 2014.  [17] OpenHPI. Blick nach vorn  und zuruck!, 2015. [18] M. Q. Patton, P. H. Rossi, H. E. Freeman, and S. R.  Wright. Evaluation: A Systematic Approach., 1981. [19] J. Renz, T. S. Suarez, Gerardo Navarro, and C. Meinel.  Enabling schema agnostic learning analytics in a service-oriented mooc platform. L@S 16.  [20] C. Richardson. Microservices Architecture pattern, 2014.  [21] C. Rohrer. When to Use Which User Experience Research Methods, 2008.  [22] Sap Se. About openSAP, 2014. [23] G. Siemens and R. S. J. D. Baker. Learning analytics  and educational data mining. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge - LAK 12, page 252, New York, New York, USA, 2012. ACM Press.  [24] G. N. Suarez. Enabling Learning Analytics in a Service-Oriented MOOC Platform. Masters thesis, Hasso Plattner Institute, 2015.  [25] S. Thomke. Experimentation Matters. 2003. [26] K. Veeramachaneni and U.-m. OReilly. Developing  data standards and technology enablers for MOOC data science. MOOC Research Initiative Conference, (October 2013):18, 2013.  [27] J. J. Williams, K. Ostrow, X. Xiong, E. Glassman, J. Kim, S. G. Maldonado, N. Li, J. Reich, and N. Heernan. Using and designing platforms for in vivo educational experiments. In Proceedings of the Second (2015) ACM Conference on Learning @ Scale, L@S 15, pages 409412, New York, NY, USA, 2015. ACM.  [28] R. S. Witte and J. Witte, S. Statistics. John Wiley & Sons, 9 edition, 2009.  [29] D. Yang, T. Sinha, D. Adamson, and C. Rose. Turn on, Tune in, Drop out: Anticipating student dropouts in Massive Open Online Courses. Proceedings of the NIPS Workshop on Data Driven Education, pages 18, 2013.    Introduction  Controlled Online Tests  openHPI   A/B Testing in Microservice based learning platforms  openHPI Architecture  Workflow  Administrators Dashboard   Metrics  Pinboard Posting Activity  Pinboard Watch Count  Pinboard Activity  Question Response Time  Visit Count  Video Visit Count  Course Activity  Course Points  Micro Survey   Identifying test candidates  Dropout and Absence in MOOCs  User Experience Survey   Concluded Tests  Onboarding  Alternatives  Setup  Results   Reminder Mails  Alternatives  Setup  Results   Pinboard Digest Mails  Alternatives  Setup  Results    Conclusion and Future Work  New feature release cycle  Evaluation of used metrics and introduction of negative metrics  Marketing or content driven style A/B tests  Community contributed test cased    References   "}
{"index":{"_id":"39"}}
{"datatype":"inproceedings","key":"Joksimovic:2016:TNP:2883851.2883928","author":"Joksimovi'c, Sre'cko and Manataki, Areti and Gavsevi'c, Dragan and Dawson, Shane and Kovanovi'c, Vitomir and de Kereki, In'es Friss","title":"Translating Network Position into Performance: Importance of Centrality in Different Network Configurations","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"314--323","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883928","doi":"10.1145/2883851.2883928","acmid":"2883928","publisher":"ACM","address":"New York, NY, USA","keywords":"ERGM, MOOC, academic achievement, learning, social network analysis, social processes","abstract":"As the field of learning analytics continues to mature, there is a corresponding evolution and sophistication of the associated analytical methods and techniques. In this regard social network analysis (SNA) has emerged as one of the cornerstones of learning analytics methodologies. However, despite the noted importance of social networks for facilitating the learning process, it remains unclear how and to what extent such network measures are associated with specific learning outcomes. Motivated by Simmel's theory of social interactions and building on the argument that social centrality does not always imply benefits, this study aimed to further contribute to the understanding of the association between students' social centrality and their academic performance. The study reveals that learning analytics research drawing on SNA should incorporate both - descriptive and statistical methods to provide a more comprehensive and holistic understanding of a students' network position. In so doing researchers can undertake more nuanced and contextually salient inferences about learning in network settings. Specifically, we show how differences in the factors framing students' interactions within two instances of a MOOC affect the association between the three social network centrality measures (i.e., degree, closeness, and betweenness) and the final course outcome.","pdf":"Translating network position into performance:  Importance of Centrality in Different Network   Configurations   Sreko Joksimovi   Moray House School of Education    University of Edinburgh   Edinburgh, UK   s.joksimovic@ed.ac.uk      Shane Dawson   Learning and Teaching Unit   University of South Australia   Adelaide, Australia   shane.dawson@unisa.edu.au   Areti Manataki   School of Informatics   University of Edinburgh   Edinburgh, UK   a.manataki@ed.ac.uk      Vitomir Kovanovi   School of Informatics   University of Edinburgh   Edinburgh, UK   v.kovanovic@ed.ac.uk   Dragan Gaevi   Schools of Education and Informatics   University of Edinburgh   Edinburgh, UK   dragan.gasevic@ed.ac.uk      Ins Friss de Kereki   Engineering School   Universidad ORT Uruguay   Montevideo, Uruguay   kereki_i@ort.edu.uy       ABSTRACT  As the field of learning analytics continues to mature, there is a  corresponding evolution and sophistication of the associated  analytical methods and techniques. In this regard social network  analysis (SNA) has emerged as one of the cornerstones of learning  analytics methodologies. However, despite the noted importance  of social networks for facilitating the learning process, it remains  unclear how and to what extent such network measures are  associated with specific learning outcomes. Motivated by  Simmels theory of social interactions and building on the  argument that social centrality does not always imply benefits, this  study aimed to further contribute to the understanding of the  association between students social centrality and their academic  performance. The study reveals that learning analytics research  drawing on SNA should incorporate both  descriptive and  statistical methods to provide a more comprehensive and holistic  understanding of a students network position. In so doing  researchers can undertake more nuanced and contextually salient  inferences about learning in network settings. Specifically, we  show how differences in the factors framing students interactions  within two instances of a MOOC affect the association between  the three social network centrality measures (i.e., degree,  closeness, and betweenness) and the final course outcome.   Categories and Subject Descriptors  Education; K.3.1 [Computer Uses in Education] Distance learning   General Terms  Social Processes, Learning   Keywords  Social network analysis, ERGM, MOOC, Academic achievement   1. INTRODUCTION  Social network analysis (SNA) has been one of the most   commonly applied methods in learning analytics research [1, 2].  Network approaches can extend analyses beyond the individual  level to focus on group dynamics. As such, SNA can provide  insight into the quantity and types of interactions or relationships  that occur between participants, groups and communities in  conventional as well as online settings [1, 3, 4]. Recently, with the  development of social networking sites that allow for a relatively  straightforward extraction of social networks, the application of  SNA in education has significantly increased [1, 5, 6]. However,  despite the volume of SNA applied within education research, few  studies have fully realized the potential of network analyses to  provide new insights into our understanding of learning [3].   Although SNA provides a rich set of tools and methods that help  improve the understanding of learning in social networks [3, 7],  the majority of the studies utilizing SNA in education are  primarily based on examining structural regularities underlying  student interactions [4, 8]. Researchers mainly rely on network  structural properties (e.g., centrality and density) [9, 10] or  generative processes (e.g., triad closure), usually observed in  isolation [8], to describe emerging patterns of students  engagement. For example, by examining measures of centrality,  embeddedness or triadic closure in social networks, researchers  can reveal who is interacting with whom and what is the strength  of interactions, the actors occupying more central or peripheral  positions in the network, and how such network engagement  patterns can affect learning [3, 4, 10, 11]. Although with limited  generalizability, such analyses are of great importance in  uncovering weak and strong ties that bridge communities/groups  of students, revealing the most influential actors or individuals  that may have a more advantageous position [12, 13].    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883928     The major characteristic of the descriptive models used in the  traditional application of SNA in (online) education has focused  on describing relationships between observed variables, rather  than explaining why such structure exists [8]. Although models  for descriptive analysis help explain the association between  network variables and identify potentially relevant processes in  the network structure, they do not allow for the generalization of  findings across the networks. The lack of inferential power that  characterizes these mathematical, descriptive models (e.g.,  measuring centrality or density) is indirectly depicted through the  interpretation of the association between learning outcome and  measures of students social centrality. Despite the prevailing, and  largely unchallenged, understanding that occupying a higher  social centrality leads to a higher academic performance [3, 9,  10], research findings are inconclusive about which centrality  measure (or combination of measures) is the most significant  predictor of academic achievement. Additionally, several recent  studies have revealed somewhat contradictory results, indicating  that the predictive power of social centrality measures highly  depends on the context that frames students interactions [11, 14].    A potential rationale for explaining the inconsistencies in the  educational research may lie in the lack of accountability for the  network context that frames social interactions [15, 16]. Research  and practice in learning analytics commonly relies on general  models (i.e., context independent) in order to inform learning and  teaching processes, predict learning outcomes or provide  appropriate scaffolds [15]. However, without considering specific  learning settings, those models could lead to incomplete  conclusions. Likewise, applying SNA without accounting for the  processes that guide network formation and consideration of the  quantity and quality of interactions could also result in a model  that does not reliably capture the underlying social processes [8].  Thus, in order to provide for more valid inferences and identify  the determinants that explain regularities of network formation, a  sound theoretical approach driving the choice of the analytics  methods is required. In so doing, the theory driven approach can  help explain the underlying network structure and provide the  context for the interpretation of revealed social processes.   1.1 SNA and MOOC research  The emergence of Massive Open Online Courses (MOOCs) has  provided new opportunities for the application of SNA among  researchers and practitioners interested in studying networked  learning [17, 18]. Given the high numbers of students enrolling  into MOOCs [19] and the immense amount of data related to  students participation and interaction collected by MOOC  platforms, it has become even more challenging to understand  patterns that drive learning in such networked settings. Therefore,  studies investigating MOOCs have relied on SNA methods in  order to visualize and examine regularities in interactions  emerging from social learning activities that students and teachers  engage with [20, 21], as well as to investigate the association  between centrality in social networks and student performance  [11, 14], to name a few. However, this research while valuable,  still fails to adequately account for both context and the structural  properties of the established networks.   To address this deficit the present study incorporates both theory  related to the importance of super-strong ties [16, 22] in  network development as well as the statistical methods for  generalizing network inference, i.e., Exponential Random Graph  Models (ERGMs) [23]. The study analyses two separate instances  of the same MOOC offered in different languages during the same   period of time. In so doing, the study aims to provide further  evidence for the importance of accounting for the contextually  salient determinants that define network formation when studying  social networks. In the following, we compared two social  networks, emerging from student discussions, with respect to the  statistical properties that define underlying network structures  [23]. We utilized statistical network analysis (i.e., ERGMs  specifically), rather than mathematical (descriptive) methods, as it  is a more comprehensive approach to explaining uncertainty  inherent in the observed data and determining which of the  network processes present significant factors that frame the  network evolution [4, 8, 23]. Finally, following the differences in  the regularities framing the social relations within the two  networks analyzed, we examined the association between social  centrality measures (i.e., degree, closeness, and betweenness) and  the academic performance (i.e., obtained certificate  none,  normal, distinct), within the different contexts.    2. BACKGROUND  2.1 Social Network Analysis in Educational  Research  The initial application of SNA dates back to the 1930s involving a  Harvard study that analyzed interpersonal relations and the  formation of cliques [24]. The concept of social centrality was  first introduced in the 1940s, with a significant uptake noted in  the 1950s and the 1960s [9, 24]. Nevertheless, from these early  studies it appeared that while the researchers at the time agreed  that centrality is an important structural property of social  networks, there was a lack of consensus regarding what centrality  means and how it should be measured [9]. In his seminal work,  Freeman (1979) revisited the concept of centrality and identified  three network structural properties that should be considered as a  measure of centrality  degree, closeness, and betweenness. In  formal online courses, SNA studies have aimed at revealing  whether and how those structural properties, as defined by  Freeman (1979) and others, are associated with learning.  However, different studies have often produced contradicting  results. For example, Russo and Koesten [25] showed that  network prestige (in-degree) and centrality (out-degree)  significantly predict cognitive learning outcomes. Cho and  colleagues [26] also concluded that network centrality measures  were significantly and positively associated with a students final  grade. However, results from Cho and colleagues [26] also  revealed that only closeness centrality was a significant predictor  of the course grade. The association between grades and the other  two centrality measures  i.e., degree and betweenness centrality -  was not statistically significant. Gaevi and colleagues [27] also  observed a significant association between grade point average  (GPA) and two measures of network centrality (eccentricity and  closeness centrality) in a fully online master of science in  information systems program. However, similar to the Cho et als  [26] study, Gaevi and colleagues [27] also failed to find a  significant association between GPA and degree and betweenness  centrality. Thus, without detailed contextual information it  becomes challenging to conclude which of the centrality measures  are considered important predictors of a students overall  academic achievement. More simply put, the absence of context  limits our understanding of how network position influences  student learning.  Research in MOOCs further argues for the necessity to account  for various contextual factors when interpreting SNA in     networked learning settings. Specifically, contemporary research  shows that the association between student centrality in MOOC  discussion forums and academic performance, depends on the  context of the course [11, 14]. For example, Jiang and colleagues  [14], analyzed the association between degree, betweenness and  closeness centrality and student grades within two MOOCs in  Algebra and Financial Planning. While the results indicated a  significant and positive association between the final course grade  and two centrality measures (degree and betweenness) for the  Algebra MOOC, none of the measures were significantly  correlated with the student grades for the Financial Planning  MOOC. Further, the approach applied in the study by Dowell and  colleagues [11] differs from the traditional application of SNA in  MOOCs. More precisely, Dowell et al. [11] aimed at predicting  two different achievement measures final course grade and social  centrality  using linguistic properties of student generated  content. Results showed that the linguistic characteristics  positively associated with social centrality were negatively  associated with the final course grade, and vice versa. Although  Dowell and colleagues [11] did not directly compare social  centrality and course grades, their findings indicate that these two  measures of learning tend to capture different achievement  metrics, suggesting further that the skills associated with these  two learning-related outcomes differ (p.7, ibid.).  This review of the existing literature, suggests that future research  should provide additional insight into the contextual factors that  may impact on the association between students position in the  network and their learning outcomes. Instead of focusing solely  on the network structural properties to describe patterns of  students engagement within MOOC discussion forums, we aim to  utilize statistical network analysis to provide contextual  information about the processes that stimulate the underlying  network formation. Particularly, we aim to reveal important  regularities in interaction structure among the course participants  that could provide a valid context for the interpretation of network  structural properties. It should be noted that contextual factors are  not necessarily related to the course design and instructional  conditions. Here, we observe context in terms of the factors that  frame individuals social behavior. According to Simmel [28] the  nature of interaction between the two individuals in a social  network is derived from the collective behavior, which accounts  for the general social situation that goes beyond the two focal  parties.    2.2 Simmelian Ties Theory  In addition to the direct measures of the network structural  properties, SNA research should also consider the contextual  factors that influence the development of the network. The most  influential research in SNA argues that those individuals who  occupy more central roles (primarily focusing on betweenness  centrality) will have higher potential to benefit from such  positions and attain their goals [9, 13, 29]. Thus, in his seminal  work, Granovetter [13] argued that weak ties are those that enable  more straightforward access to information disseminated through  a social network. Burt [12] goes even further arguing that the  strength of ties is not as relevant as the fact that a given tie bridges  otherwise distinct groups or cliques in the social network. As Burt  noted [p]eople whose networks bridge the structural holes  between groups have an advantage in detecting and developing  rewarding opportunities [30, p. 354]. Both theories are in line  with Freemans [9] definition of centrality and assume that the  more central persons in a social network occupy a more  advantageous position. Nevertheless, Krackhardt [16] posits that   centrality does not necessarily imply less constraints and more  benefit. If a node is linked in what Krackhardt [16] calls a  Simmelian tie, such a position could impose additional  limitations. In the context of the present study, this could suggest  that while a student centrally positioned in the network has a high  potential for control over the information flow, the actual realized  gains for their learning may be diminished. Therefore, as  Krackhardt [16] posits, traditional SNA analysis (in his case  traditional role analysis) should be supported with Simmelian Ties  analysis. In the present study, we argue that Simmelian Ties  Theory [28] presents a sound theoretical framework in providing  valid context for interpreting the importance of social centrality  for the academic achievement.   Simmels theory of social behavior focuses on studying  relationships that occur between people in order to explain their  actions [16, 28]. Simmel argued that context is the primary factor  influencing what people do and why they behave in a particular  manner. Context is determined by the set of third others who also  engage in various relationships with the two focal parties [31, p.  16]. Thus, as Simmel argued, the establishment of such triadic  nodes should be the fundamental unit of analysis in order to  understand social behavior [16, 28]. Triads are considered to be  qualitatively different from the dyadic relationships that Burt [12]  and Granovetter [13], among others, focus on [16, 22]. This  difference originates in the nature of the formed relationships. The  two nodes forming a dyad are more independent and retain more  individuality in their relationship [16, 22]. For instance, should  disagreement occur in a dyad, both parties can choose to cease  any further interaction. However, a triadic tie requires a higher  level of negotiation. If a member of a group disagrees and ceases  further interaction the group remains to exist and a connection  remains. Thus, Krackhardt [22] described Simmelian ties as  super-strong (p.24), ties that qualitatively add durability and  power (p.24, ibid.), beyond the strong ties as previously defined  by Granovetter [13] and Krackhardt [32].   Simmelian ties theory differs from psychological theories, such as  Heiders [33] balance theory, in explaining structural properties  for the existence of symmetric and transitive triples, that are  considered main processes in social networks [16]. According to  Heiders [33] theory, people are motivated to establish and  maintain relationships that would allow them to keep comfortable  communicating with others. The Simmelian theory, on the other  hand, assumes that once cliques are formed, they resist changing,  becoming strong and stable, thus decreasing propensity to  dissolve over time [28]. However, there is no inherent  motivation to form a clique [31, p. 21], it is rather the social  structure, or the context, that causes formation of certain network  structures [28].   Building further on one of Krackhardts [22] conclusions (i.e.,  that traditional SNA should be supported with Simmelian ties  analysis), and given the theorized relationship between the social  centrality and the expected benefits, it seems reasonable to  analyze whether networks under study exhibit properties of  Simmelian ties. In the educational context, such strong ties could  indicate the existence of tightly connected groups, focused around  common interests.   2.3 Exponential random graph models in  Online Learning  A majority of studies applying SNA in online and distance  education relies on mathematical models to describe relationships  between observed variables [34]. Such studies are particularly     useful in revealing important network characteristics or what  processes should be observed within the social network [8]. For  example, using descriptive models we would be able to determine  whether Simmelian ties exist in a given network. However, in  order to reveal whether these processes (i.e., propensity to form  super-strong ties) occur more often than expected if ties were  generated randomly, as well as what other micro-level processes  (e.g., popularity, propensity for triad closure) determine social  dynamics in a given network, we need to rely on statistical models  [8]. The quadratic assignment procedure for analyzing dyadic data  sets [35], Exponential Random Graph Models (ERGM) and  stochastic blockmodels for the cross-sectional social network  analysis and community detection [23, 36], as well as longitudinal  models for studying evolution of networks and behavior [37] are  some of the commonly proposed methods. ERGM specification  allows us to model Simmelian statistics (i.e., a process of  formation of super-strong ties). Hence, this approach is directly  applicable for exploring hypothetical network processes that could  explain the evolution of the observed cross-sectional network [8,  23].   As a generalization of p1 models and Markov graphs [38],  exponential random graph models for social networks, also known  as p* models, were introduced by Frank and Strauss [39] and  Wasserman and Pattison [40]. ERGMs belong to the family of  probability models for network analysis that allow for more  generalizable inferences over the structural foundations of social  behavioral patterns [23, 38]. Observing network ties as random  variables, ERGMs allow for modeling overall network structure  through a set of local network processes [38]. ERGMs assume  that each tie within these local network processes (e.g., mutuality,  transitivity or triad closure) is conditionally dependent, indicating  further that empirical network ties do not form at random, but  that they self-organize into various patterns arising from  underlying social processes [41, p. 3]. Although ERGMs, and  similar statistical methods (e.g., longitudinal probabilistic social  network analysis  [4]), have been successfully applied in social  sciences [42], medical research [43] and studying traditional  education [8], their application in the context of online learning  and MOOCs is rather sparse.  From the perspective of the analytical methods applied and the  educational context analyzed, Kellogg et al.s [5] study is perhaps  the most relevant for our research. In their mixed methods study,  Kellogg and colleagues [5] aimed at providing more  comprehensive understanding of the dynamic processes that  underlie peer support learning in MOOCs tailored towards  educators in K-12 settings. The quantitative part of the study  included application of SNA tools and techniques  descriptive  network measures and ERGMs  in the analysis of the two  interaction networks obtained from discussion forums. In order to  examine mechanisms of peer support in the two MOOCs, Kellogg  and colleagues [5] analyzed various patterns of selective mixing  and network statistics: reciprocity, homophily by professional role  (e.g., principal), gender, educational background, grade levels,  differences in experience (i.e., heterophily), and three proximity  mechanisms based on the state or country, geographical region,  and group assignment. The results indicate a strong and  significant reciprocity effect, suggesting that students are more  likely to reply to a peer when there has been prior evidence of  reciprocity. Nevertheless, homophily and heterophily effects, as  well as proximity mechanisms differed across the networks  analyzed.   2.4 Research questions  The education literature suggests that researchers predominantly  rely on descriptive methods when applying SNA in online  learning settings. There is far less evidence of the research  accounting for network specific variables that could provide  contextual background for the interpretation of the underlying  processes. Given the inconsistencies in findings on the association  between social centrality and learning outcome, we aimed at  determining whether network social dynamics have an impact on  the predictive power of network structural position. We were  particularly interested to find out whether a network formed  around an online course is characterized by the propensity to form  Simmelian ties. We hypothesized that these super-strong  relationships could influence the potential benefits students derive  from occupying more central positions in the network. Thus, we  defined the following two research questions:       RQ1. Are there differences in the underlying processes that  determine network formation within social networks formed in  various online learning settings   RQ2. Is the propensity for forming Simmelian ties significantly  different than expected if ties were formed randomly  Eventual differences in the social dynamics that frame social  interactions within the two networks analyzed would provide a  valid context for the interpretation of the possible variances in the  predictive power of the social centrality measures. Therefore, we  defined our third research question as follows:  RQ3. If there are differences in regularities that frame network  structure among the course participants, how do these  discrepancies affect the association between social centrality and  academic performance   3. METHOD  3.1 Data  This study analyzed forum discussions within two instances of a  single course that were delivered on the Coursera platform in  Spring 2015. The two instances, Code Yourself!1 (CDY) and A  Programar!2 (APR), were designed to be identical with respect to  the content and teaching methods, with the only difference being  the delivery language, i.e., English in CDY and Spanish in APR.  The MOOC aimed to introduce young teenagers to computer  programming, while covering the basic topics in computational  thinking and software engineering. The content of this 5-week  course consisted of lecture videos, quizzes and peer-assessed  programming projects, which were translated and tailored for  English and Spanish-speaking audiences. A common marking  scheme was established, whereby students were deemed to have  successfully completed the course (and obtained a certificate)  when they had a score of at least 50% for the coursework. A  distinction was awarded for students receiving a score of 75% or  more. CDY and APR were designed to be identical not only in  content, but also with respect to their simultaneous delivery with  the MOOCs running from March-April 2015. This implies that all  aspects of the MOOCs were equivalent including weekly course  announcements and matching instructor-initiated prompts in the  discussion forums, and adopting a common strategy for minimal  instructor intervention in the forums.                                                                     1 https://www.coursera.org/learn/codeyourself  2 https://www.coursera.org/learn/a-programar     Despite the common approach for the two course instances,  student engagement and performance was considerably different  in CDY and APR. As shown in Table 1, almost 60,000 students  enrolled in CDY and more than 25,000 in APR. However, almost  the same number of students completed the two courses  1,597 in  CDY and 1,595 in APR. Moreover, regardless the smaller student  cohort (in overall), higher number of students engaged with the  forum discussions in the APR course, resulting in a more  intensive forum activity produced (Table 1).   Table 1: Descriptive statistics for the number of enrolled  students, students engaged with the course content and   discussion forum, as well as the obtained certificates    CDY APR  Enrolled 59,531 25,255  Engaged 26,568 13,808  Engaged with forum 1,430 1,818  Posted messages   Threads 776 (1.69; 1.75) 1,081 (3.53; 5.12)  Posts 4,204 (3.13; 7.75) 5,940 (3.53; 5.12)   Comments 1,981 (3.42; 9.06) 2,686 (3.21; 6.75)  Total 5,177 7,409   Obtained certificate  Normal 586 644  Distinct 1,011 951   Total 1,597 1,595  Note: Thread, Posts and Comments rows display counts in the following   format  total (average; SD)     Figure 1: Proportion of students that watched a lecture each   week     Figure 2: Proportion of students browsing forums each week   Large differences were also observed with respect to student  engagement with the course materials. The proportion of students  that visited the course, watched a lecture, submitted an exercise or  browsed the forums each week in CDY was always smaller than  the corresponding proportion for APR that week. As depicted in   Figure 1 and Figure 2, in some cases this difference reached levels  of about 8%. It is also worth mentioning that the weekly  engagement steadily dropped in CDY during the 5-week duration.  In contrast for APR there was a steady drop during the first 4  weeks, followed by an increase in engagement for the final week.   3.2 Analysis  3.2.1 Social Network Analysis  To address the first two research questions, we extracted two  directed weighted graphs to represent interactions occurring  within discussion forums for the two course instances (CDY and  APR). Although several approaches have been proposed for  extracting social networks from discussion forums, we relied on  the most commonly applied approach that considers each message  as being directed to the previous one [11, 44]. For example, if  author A2 replied to a message posted by author A1, we would  add a directed edge A2->A1. Further, if A3 posted a comment on  A2s post, we would include A3->A2 edge as well. Finally, social  graph included all the students who posted to the discussion  forum.  Social network analysis was conducted through two  complementary phases; statistical network analysis and structural  (i.e., traditional) network analysis. The statistical network  analysis was performed using ERGMs in order to reveal various  networks statistics and examine processes that guided network  formation for both of the courses instances. Relying on commonly  used network statistics [4, 5, 8] we examined network formation  mechanisms at the two levels; dyadic and triadic. At the dyadic  level, we aimed to investigate the effects of selective mixing,  reciprocity, popularity, and expansiveness. Selective mixing  reflects a students propensity to interact with their peers based on  the combination of their individual characteristics [8, 23]. Thus,  we considered a homophily effect with respect to the following  students attributes:  - Achievement: none, normal, and distinct;  - Domestic: a student was from either the United Kingdom or   Uruguay (as the course was offered by two universities from  these two countries) or was from an alternate country;,   - Gender: male, female;   - Access group: student, instructor, or teaching staff.  Reciprocity, on the other hand, is a network statistic that models  students tendency to form mutual ties and cluster together [23].  In the case of our study, this property would allow for revealing  whether students tend to continue interaction with their peers who  replied to their posts. Finally, popularity and expansiveness tend  to model processes that would indicate the existence of students  who receive a significant number of replies to their posts or  students who tend to reply more often to their peers posts,  respectively.  At the triadic level, we examined effects of triadic closure and  Simmelian ties formation. Existing research argues that cyclic  and transitive triples are the common characteristics of networks  emerging from social media [45]. However, with directed  networks, these two statistics are captured within the triangle term  [8, 23]. Nevertheless, models with triangle term are almost always  degenerate [23], therefore, geometrically weighted edgewise  shared partner distribution (gwesp) is used instead. We also  modeled Simmelian ties [32] in order to examine whether the  network(s) analyzed conform to the Simmelian ties theory. That  is, whether the networks exhibit a formation of cliques of students  that tend to interact with each other significantly more often than     with the rest of their peers. Such a statistic could indicate that  those students are primarily being focused on their field of interest  and rarely interacting with other students.  The analysis of network structural properties relied on most  commonly used SNA measures that capture various aspects of  graph structural centrality  degree, closeness, and betweenness  centrality [9, 10, 34]. Degree centrality is considered the most  straightforward centrality measure, focusing on the local structure  surrounding the node and indicating the number of connections  (ties) a node has in the network [9]. It is commonly interpreted as  a measure of popularity [34] or the extent to which observed node  has a potential for activity in communication [9, p. 219]. Given  that our focus was on the analysis of weighted networks, we relied  on the weighted degree centrality, that accounts for the weight of  edges a node has in the network [46]. Closeness centrality  measures a distance of a given node to all other nodes in the  network [9]. Closeness centrality measures nodes potential to  connect easily with other nodes. Finally, betweenness centrality is  perhaps the most significant for the context of our study, given  Krackhardts [16] view on the association between the strength of  the ties and expected benefits for the nodes that bridge two  distinct parts of the network.   We consider three models, for each of the networks, based on the  described set of statistics  a demographic attribute model (DM)  that includes only processes based on students characteristics;  triadic closure and Simmelian ties model (TSM), including only  gwesp and simmelian statistics; and a full model that combines the  two (FM). Comparing likelihood-based measure of AICc, we  further continued selecting the most parsimonious model, which  would provide the best fit to our data. The social networks were  analyzed using the ergm 3.1.2 [47], an R package for statistical  network analysis, and using igraph 0.7.1 [7], a comprehensive R  software package for complex social network analysis research.   3.2.2 Regression Analysis  To examine the association between the dependent variable (i.e.,  obtained certificate), and the independent variables (i.e., three  centrality measures), we adopted multinomial logistic regression  (MLR) analysis [48], in order to answer our third research  question. MLR is predictive analysis that is used to explain the  association between a nominal dependent variable that has more  than two levels (none, normal, and distinct), and one or more  continuous independent variables [48]. It does not make any  assumptions of normality, linearity and homogeneity of variance  for the independent variables [48].  Aiming to observe the association between the three centrality  measures  degree, closeness, and betweenness centrality  and  the course outcome, we build three MLR models. Each model  included one dependent (obtained certificate) and one  independent variable (degree, closeness, or betweenness  centrality). The analyses were performed using the mlogit 0.2-4  package for R that enables estimation of multinomial logit models  [49].   4. RESULTS AND DISCUSSION  4.1 Network Characteristics  Descriptive statistics (Table 2) indicate rather diverse processes  within the two networks analyzed. Given the difference in the  number of nodes (Table 2) it is expected that the APR network  would have a considerably higher number of edges, and perhaps  moderately higher weighted degree. However, higher modularity,  average clustering coefficient and higher number of connected   components, could indicate a less cohesive group of students  within the CDY instance of the course [1]. Moreover, descriptive  statistics also indicate a comparable number of reciprocal ties,  whereas the number of super-strong ties is considerably higher  in case of the English version of the course.   Table 2: Descriptive statistics for social networks extracted  from CDY and APR discussion forums   Descriptives  CDY APR  Edges 3,620.00 4,736.00  Avg. W. Degree 4.00 4.69  Density 0.002 0.001  Modularity 0.45 0.33  Conn. comp. 16.00 9.00  Avg. clust. coef. 0.12 0.09  Reciprocity 231.00 176.00  Simmelian 41.00 7.00  Simmelian ties 144.00 32.00  Popularity 758.55 839.00  Expansiveness 1373.42 1612.53   In case of both networks under the study, the full model provided  the best fit, indicated by the lowest value for AICc (CDY: DM   2,830,818.00, STM  49,863.82, FM  48,371.14, and APR: DM   4,577,956.00, STM- 67,786.65, FM  66,921.94). Estimated  coefficients are presented in Table 3, whereas goodness-of-fit  statistics indicate that models provide a satisfactory fit for the  data. It is also important to note that we aimed at assessing  homophily at the level of access groups (i.e., students, teachers,  teaching staff) and triad closure (gwesp) (Section 3.2.1).  However, those two statistics indicated an overall worse fit to our  data than the selected (i.e., best fit) model; therefore, both  statistics were excluded from the final models analyzed.   Table 3: Analysis of the estimates for the two ERG models   CDY FM and APR FM    CDY APR   Estimate SE Estimate SE  Baseline (Edges) -5.45*** 0.04 -5.81*** 0.09  Selective mixing   Distinct 0.98*** 0.03 0.47*** 0.12  None 0.15*** 0.03 -0.20** 0.08   Normal 0.60*** 0.17 0.68** 0.25  Domestic -0.95*** 0.03 -0.09 0.07   Gender 0.02 0.03 - -  Structural mechanisms   Reciprocity 3.81*** 0.09 4.20*** 0.55  Simmelian 4.89*** 0.61 - -  Popularity -3.68*** 0.10 -4.75*** 0.29   Expansiveness - - -0.25 0.21  Note: * p < .05. ** p < .01. *** p < .001.   It is revealing that differential homophily for the final course  outcome (i.e., obtained certificate) shows that both networks  exhibited a higher likelihood of assortative mixing between the  students who obtained the certificate. Similar to Kellogg and  colleagues study [5], our results suggest that the more successful  students tend to interact more often. However, the likelihood of  interaction between the most successful students is higher in the  CDY course. Whereas, the same effect holds between the students  who did not obtain the certificate in case of the English instance  of the course (although with less likelihood), the effect is negative  in the Spanish version of the course. Students who did not obtain     a certificate in the APR instance of the course were less likely to  interact with each other.  Homophily for the students country of residence, revealed a  significant effect for the English instance of the course, whereas  the effect was not significant in the Spanish version. Kellogg and  colleagues [5] observed a similar effect - i.e., homophily by state  or country) and found a significant positive increase in the  likelihood that two students from the same state or country will  create a tie. In our study, however, we examined selective mixing  between domestic students. Given that two courses were  particularly designed for two diverse groups of students, we aimed  at investigating how that aspect would influence students  tendencies to connect with their peers. Our results revealed that  students, who are considered domestic in the CDY course  instance, were less likely to connect with their domestic peers.  Observing students demographic data, we could perhaps expect  the same effect within both models, given that similar numbers of  students (7% in CDY and 10% in APR) were considered domestic  in both networks. However, the observed effect was not  statistically significant for the Spanish version of the course.  The effect of reciprocity was significant for the models of both  networks, indicating that students tended to continue interacting  with peers who replied to their posts. Although the estimates seem  rather high, those values are in line with results of Lusher,  Koskinen, and Robins [50] and Kellogg et al. [5] studies, who  also revealed a very strong effect of direct interaction between  students. It appears that a strong effect of reciprocity could be  seen as one of the defining characteristics of interaction in online  social networks in general [50]. Moreover, Lusher and colleagues  [50] further identified such networks as self-disclosing (p.249)  and bonding (p.249), characterized by strong ties relations  between the nodes. In such networks, students tend to self- disclose themselves, bonding with their peers, creating  comfortable environment for knowledge sharing and learning  [50]. However, given rather the low cohesion at the network level  for both networks (i.e., low density  Table 2), it seems  reasonable to conclude that students commonly interact within  smaller groups of peer students [24].   The effect of Simmelian ties was not consistent across both the  networks. While it was strong and significant for the CDY  network, in the case of the APR course we were not able to fit the  model with Simmelian statistics. Thus, although the strong effect  for reciprocity could indicate existence of strong ties, it seems that  the ties within the English version of the course evolved to super- strong ties, as defined by [16, 22]. The existence of Simmelian  ties beyond the chance level is a significant defining characteristic  of the social network emerging from the CDY discussion forum.  These ties are structurally embedded within relatively small,  highly connected and cohesive groups, commonly referred to as  communities [45]. Interactions within those communities are  more often and qualitatively different from interactions with other  peer students. This finding could be further explained by a rich- club phenomenon (p.1), an analogy used by Vaquero and  Cebrian [7] to explain frequent and intense (p.1, ibid.)  interactions occurring within relatively small groups of students,  where students benefit greatly from these structural arrangements.  The effect of expansiveness was not significant in the APR social  networks. However, we were not able to fit the model to a  satisfactory quality using this network statistics in case of the  CDY network. On the other hand, the strong negative effect of  popularity in the CDY network is also in line with Kelloggs [5]  study. Kellogg et al. [5] and Lusher and colleagues [50] argue that   such an effect could indicate that all the students have a similar  level of popularity and that most likely networks were not  centralized on in-degree [5, p. 275]. Considering the previous  results (i.e., the strong effect of reciprocity) this result seems quite  intuitive. Moreover, given the fact that we observed interactions  within a discussion forum, this effect further contributes to the  understanding that students in both networks tended to engage  into further interaction with their peers, rather than simply posting  a message without the intent to contribute the further discussion.  In addressing the first and second research question, we were  able to conclude that the observed networks differ with respect to  the determinants of network formation. The most notable  difference is related to the structure of super-strong ties, where  CDY network exhibit a formation of cliques formed around  students who tend to interact within the strong and stable groups  of peers, which resist change [31, p. 21]. Although the APR  network showed the same regularities with respect to reciprocity  of interaction and popularity, the effect of Simmelian ties was not  present. Finally, the APR network also revealed higher tendency  that students would interact more often with higher performing  peers.   4.2 Social centrality and academic  achievement  Analyzing the association between the students centrality and the  final learning outcome further revealed differences between the  two networks. Specifically, in the case of the CDY course  instance, only weighted degree centrality was significantly  associated with the course outcome  2(1) = 9.048, p=.011.  However, multinomial regression analysis showed that an increase  in weighted degree significantly increased the likelihood of  obtaining certificate with distinction, compared to not completing  the course successfully, whereas there was no significant  difference between normal certificate and failing the course  (Table 4). On the other hand, closeness and betweenness  centrality were not significantly associated with the course  outcomes.  Table 4: Results of the multinomial regression analysis of the   association between social centrality and the final learning  outcome (i.e., obtained certificate)     Estimate SE t  Weighted Degree   CDY distinct  0.008* 0.004 2.720   normal 0.007 0.004 1.618   APR distinct  0.046*** 0.006 7.318   normal 0.046*** 0.006 7.413  Closeness   CDY distinct  0.002 0.038 0.046   normal 0.062 0.066 0.934   APR distinct  -0.064* 0.030 -2.113   normal -0.105** 0.037 -2.816  Betweenness   CDY distinct  0.000009 0.000005 1.621   normal -0.000003 0.00001 -0.185   APR  distinct 0.0001*** 0.00002 5.584  normal 0.0001*** 0.00002 5.562   Note: * p < .05. ** p < .01. *** p < .001; Reference levels for each of the  analysis was none  i.e., student did not obtain a certificate.   The APR social network revealed different patterns. All of the  observed centrality measures were significantly related to the  likelihood to obtain a certificate  weighted degree, 2(1) =     90.217, p<.001; closeness, 2(1) = 9.679, p=.008, and  betweenness, 2(1) = 59.832, p<.001. Even more so, an increase  in each of the centrality measures significantly increased the  likelihood of both  obtaining a certificate with distinction, and a  normal certificate (Table 4), compared to not completing the  course. It should be noted that direction of closeness centrality is  opposite to the betweenness and degree centrality  lower values  indicate lower distance (i.e., higher closeness) of a given node to  all other nodes in the network [10].  There are two important aspects of the findings presented in the  previous section. First, we would argue that our results support  [16, 22] understanding of the importance of social centrality in  providing greater opportunity for wellpositioned individuals.  Although Krackhardt [16, 22] discusses the potential to bridge  between two social groups (i.e., betweenness centrality), we  would posit that the importance of the most commonly addressed  centrality measures in educational research  degree (to a certain  extent), closeness, and betweenness  should be interpreted with  respect to the propensity to form Simmelian ties. Following  Krackhardts [16] argument that occupying a bridging role can  be more constraining (p. 184, ibid.), our results show that  depending on the given context, a higher social centrality does not  necessarily imply a better academic performance. In that sense, we  could conclude that those students who are occupying positions  between strongly connected groups of students might not be able  to benefit significantly from their position. Observed from the  perspective of roles, as defined by Krackhardt [16], this finding  could further indicate that students within the CDY course  instance tended to primarily interact with peers who share the  same interests, and perhaps have the same or similar level of  knowledge. Nevertheless, further research is needed to address  this assumption.  The second important finding of our results relates to the  development of an interactive rich-club [7]. In their analysis of  the relationship between the social structure and performance,  Vaquero and Cebrian [7] concluded that students tend to interact  within the groups of strongly connected peers. Vaquero and  Cebrian [7] labeled those groups as a rich-club, where students  engage in interaction with their peers at the very beginning of the  course, and tend to remain within the same cliques throughout the  course. Vaquero and Cebrian [7] further showed that those  persistent interactions are maintained between high performing  students, whereas low performing students would usually attempt  to join those groups later in the course. However, such attempts  would usually fail to produce reciprocity in the interaction with  high performing students. Thus, those rich-clubs or the groups  of strongly connected students could be easily connected with  Krackhardts [16] cliques (i.e., groups of students connected with  super-strong, Simmelian ties).  From the analysis of the two social networks it would appear that  interaction within the CDY discussion forum tended to follow the  social structure as noted in Vaquero and Cebrians [7] study. This  could imply that students within the APR course instance were  more socially inclusive, and supportive of their peers who may  have joined late in the discussions. On the other hand, it could  also mean that the majority of students in the APR course instance  were simply engaged in the discussions from the very beginning  of the course. Both of these possible interpretations require  further research to more comprehensively explain the reasons for  the observed differences in social interactions within two different  networks of students (i.e., student in CDY and APR course).  Nevertheless, it should be noted that we do not assume that those   students who attained a more central position in a social graph are  necessarily low performing students.   With respect to the third research question, our results support  the assumption that social centrality in networks that are formed  around strongly connected components (i.e., rich-club or  Simmelian groups, as with the CDY network) is not associated  with the final course outcome. Whereas, on the other hand, with  more relaxed interactions (i.e., the APR network), however still  assuming a high level of reciprocity in social ties, social centrality  is significantly and positively associated with the course outcome  (i.e., obtained certificate). Finally, it should be noted that  weighted degree centrality diverges from this pattern to a certain  extent (Table 4).     5. CONCLUSIONS & IMPLICATIONS  This study investigated the importance of the context that defines  students social interactions for the association between structural  centrality and learning outcome. Primarily, we grounded the  theoretical framework in Simmels theory of social interactions  and Krackhardts [16] argument that the quality of tie itself  interacts with the bridging role to produce more constraint on the  unsuspecting actor (p.184), to define network specific properties  that would allow us to make more valid inferences. Finally,  supplementing descriptive SNA with statistical network analysis  and multinomial logistic regression, we were able to conclude that  social centrality within the network characterized with super- strong ties, does not necessarily imply benefits. On the other  hand, structural centrality in the network with reciprocal ties,  where all participants have similar level of popularity, yet without  a significant effect of super-strong ties, is positively associated  with the likelihood of obtaining a certificate at the end of the  course.    Analyzing roles in an organization, Krackhardt [16] concluded  that traditional role analysis on raw network relations (p. 208),  should be supplemented with the Simmelian ties analysis, arguing  further that such an analysis provides more insight into  organizational phenomena (p.208). Our study extends  Krackhardts [16] argument in two directions. Primarily, we argue  that any traditional SNA (not just role analysis), should be  supported with the Simmelian ties analysis, as those ties are  qualitatively different from weak and strong ties as defined by  Granovetter [13], and therefore provide a more comprehensive  understanding of social interactions and the dynamics influencing  the overall network. Moreover, as a consequence of this  theoretical recommendation, it is reasonable to argue that  traditional (primarily descriptive) approaches to the analysis of  social interactions should be supported by statistical network  analysis. Relying solely on mathematical approaches we are able  to identify the most significant patterns in the established social  interactions. However, in order to understand which of the  identified patterns significantly determine network structure and  occur beyond the chance, more profound (statistical) models are  required [8, 23, 47].   Through the statistical network analysis methods, we were able to  provide context to interpret an association between social  centrality and academic achievement. Again we refer to the  previous work by Krackhardt [16, 22, 31] to explain how  Simmelian ties could affect ones position within an organization.  Krackhardt [16] identified those super-strong ties as more  enduring, more visible, and more critical than sole-symmetric  ties (p.208), that is, ties that constrain and influence (ibid.).      One of the imposed connotations of our findings, for both  research and practice domains, is the necessity to account for  contextual information when interpreting the potential gains  implied by the network structural properties. For example,  revealing and visualizing network structure using deeply  embedded relations (i.e., Simmelian backbones) [45] could  significantly improve the quality of information presented in  social learning analytics dashboards, such as the one presented in  the work by Schreurs and colleagues [20]. Moreover, providing  additional information about the social dynamics should  supplement any feedback based on the measures of structural  centrality. Likewise, research on predicting association between  descriptive network measures and products of learning, in  educational settings, should be constructed on valid theoretical  assumptions that could support conclusions about inferred social  dynamics.   Further research should also integrate temporal dynamics to  investigate how certain network processes evolve over time. A  promising approach in that direction would be application of  Temporal ERGMs [51], or similar models, for studying time- evolving social networks. Moreover, as indicated by Edwards [42]  and Kellogg and colleagues [5], as well as in our previous work  [11], [52], SNA should be integrated with content analysis to  account for the quality of students contribution. Finally, it should  be noted that 39% of CDY students who submitted the survey,  stated that English was their first language. On the other hand,  97% of student who participated in APR course and submitted the  survey chose Spanish as their first language. However, we were  not able to include this information in the model, since majority of  students who participated in the course did not submit the survey.  This also reflected to the students who participated in the  discussion forum. Nevertheless, investigating whether language,  as a predominate medium for communication between students in  a computer-mediated learning environment [52], influences  development of the underlying social processes, presents a  promising venue for future research.   Several limitations of our study need to be acknowledged. We  analyzed students interactions within discussion forum in two  instances of a same MOOC. Although we relied on a most  commonly accepted method for network construction, this  approach tends to underestimate the intensity of all the  interactions within the given settings. Moreover, analysis of  interactions in a more informal settings, such as connectivist  MOOC [53], would also contribute to the greater generalizability  of our findings. Finally, data from different subject domains (e.g.,  social science) should be analyzed in order to account for diverse  learning settings.   6. ACKNOWLEDGMENTS  This work is partially supported under SOCIAM: The Theory and  Practice of Social Machines.  The SOCIAM Project is funded by  the UK Engineering and Physical Sciences Research Council  (EPSRC) under grant number EP/J017728/2 and comprises the  Universities of Oxford, Southampton, and Edinburgh.   7. REFERENCES  [1] K. Cela, M. Sicilia, and S. Snchez, Social Network   Analysis in E-Learning Environments: A Preliminary  Systematic Review, Educ. Psychol. Rev., vol. 27, no. 1,  pp. 219246, 2015.   [2] S. Dawson, D. Gaevi, G. Siemens, and S. Joksimovic,  Current state and future trends: A citation network  analysis of the learning analytics field, in Proceedings of  the Fourth International Conference on Learning  Analytics And Knowledge, 2014, pp. 231240.   [3] B. V. Carolan, Social Network Analysis Education:  Theory, Methods & Applications. Social Network Analysis  Education: Theory, Methods & Applications. SAGE  Publications, Inc. SAGE Publications, Inc., 2014.   [4] K. Stepanyan, K. Borau, and C. Ullrich, A Social  Network Analysis Perspective on Student Interaction  within the Twitter Microblogging Environment, in  Advanced Learning Technologies (ICALT), 2010 IEEE  10th International Conference on, 2010, pp. 7072.   [5] S. Kellogg, S. Booth, and K. Oliver, A social network  perspective on peer supported learning in MOOCs for  educators, Int. Rev. Res. Open Distrib. Learn., vol. 15, no.  5, 2014.   [6] D. McFarland, D. Diehl, and C. Rawlings,  Methodological Transactionalism and the Sociology of  Education, in Frontiers in Sociology of Education, vol. 1,  M. T. Hallinan, Ed. Springer Netherlands, 2011, pp. 87 109.   [7] L. M. Vaquero and M. Cebrian, The rich club  phenomenon in the classroom, Sci Rep, vol. 3, Jan. 2013.   [8] S. Goodreau, J. Kitts, and M. Morris, Birds of a Feather,  or Friend of a Friend Using Exponential Random Graph  Models to Investigate Adolescent Social Networks*,  Demography, vol. 46, no. 1, pp. 103125, 2009.   [9] L. C. Freeman, Centrality in social networks conceptual  clarification, Soc. Netw., vol. 1, no. 3, pp. 215239, 1979.   [10] S. Wasserman, Social network analysis: Methods and  applications, vol. 8. Cambridge university press, 1994.   [11] N. Dowell, O. Skrypnyk, S. Joksimovi, A. C. Graesser, S.  Dawson, D. Gaevi, P. de Vries, T. Hennis, and V.  Kovanovi, Modeling Learners Social Centrality and  Performance through Language and Discourse, presented  at the In Proceedings of the 8th International Conference  on Educational Data Mining, Madrid, Spain, 2015.   [12] R. S. Burt, STRUCTURAL HOLES. Harvard University  Press, 1995.   [13] M. S. Granovetter, The strength of weak ties, Am. J.  Sociol., pp. 13601380, 1973.   [14] S. Jiang, S. M. Fitzhugh, and M. Warschauer, Social  Positioning and Performance in MOOCs, in Proceedings  of the Workshops held at Educational Data Mining 2014,  co-located with 7th International Conference on  Educational Data Mining (EDM 2014), London, United  Kingdom, 2014, vol. 1183, p. 14.   [15] D. Gaevi, S. Dawson, T. Rogers, and D. Gaevi,  Learning analytics should not promote one size fits all:  The effects of instructional conditions in predicting  academic success, Internet High. Educ., vol. 28, pp. 68   84, 2016.   [16] D. Krackhardt, The Ties that Torture: Simmelian Tie  Analysis in Organizations, Res. Sociol. Organ., vol. 16,  pp. 183210, 1999.   [17] D. Gaevi, V. Kovanovi, S. Joksimovi, and G. Siemens,  Where is research on massive open online courses  headed A data analysis of the MOOC Research  Initiative, Int. Rev. Res. Open Distrib. Learn., vol. 15, no.  5, 2014.     [18] M. De Laat and F. Prinsen, Social learning analytics:  Navigating the changing settings of higher education., J.  Res. Pract. Assess., vol. 9, no. 4, pp. 5160, 2014.   [19] K. Jordan, Synthesising MOOC completion rates |  MoocMoocher, 2015. [Online]. Available:  https://moocmoocher.wordpress.com/2013/02/13/synthesis ing-mooc-completion-rates/. [Accessed: 23-Aug-2015].   [20] B. Schreurs, C. Teplovs, R. Ferguson, M. de Laat, and S.  Buckingham Shum, Visualizing Social Learning Ties by  Type and Topic: Rationale and Concept Demonstrator, in  Proceedings of the Third International Conference on  Learning Analytics and Knowledge, New York, NY, USA,  2013, pp. 3337.   [21] O. Skrypnyk, S. Joksimovi, V. Kovanovi, D. Gaevi,  and S. Dawson, Roles of course facilitators, learners, and  technology in the flow of information of a cMOOC, Int.  Rev. Res. Online Distance Learn., vol. (in press), 2015.   [22] D. Krackhardt, Super Strong and Sticky, Power Influ.  Organ., p. 21, 1998.   [23] M. Morris, M. S. Handcock, and D. R. Hunter,  Specification of Exponential-Family Random Graph  Models: Terms and Computational Aspects, J. Stat.  Softw., vol. 24, no. 4, 2008.   [24] J. Scott, Social Network Analysis. SAGE Publications,  2012.   [25] T. C. Russo and J. Koesten, Prestige, centrality, and  learning: A social network analysis of an online class,  Commun. Educ., vol. 54, no. 3, pp. 254261, 2005.   [26] H. Cho, G. Gay, B. Davidson, and A. Ingraffea, Social  networks, communication styles, and learning performance  in a CSCL community, Comput. Educ., vol. 49, no. 2, pp.  309329, Sep. 2007.   [27] D. Gaevi, A. Zouaq, and R. Janzen, Choose Your  Classmates, Your GPA Is at Stake!: The Association of  Cross-Class Social Ties and Academic Performance, Am.  Behav. Sci., 2013.   [28] G. Simmel, The Sociology of Georg Simmel. Simon and  Schuster, 1950, 1950.   [29] D. J. Brass, Being in the right place: A structural analysis  of individual influence in an organization, Adm. Sci. Q.,  pp. 518539, 1984.   [30] R. S. Burt, Structural Holes and Good Ideas, Am. J.  Sociol., vol. 110, no. 2, pp. 349399, 2004.   [31] D. Krackhardt and M. Handcock, Heider vs Simmel:  Emergent Features in Dynamic Structures, in Statistical  Network Analysis: Models, Issues, and New Directions,  vol. 4503, E. Airoldi, D. Blei, S. Fienberg, A. Goldenberg,  E. Xing, and A. Zheng, Eds. Springer Berlin Heidelberg,  2007, pp. 1427.   [32] D. Krackhardt, The strength of strong ties: The  importance of philos in organizations, Netw. Organ.  Struct. Form Action, vol. 216, p. 239, 1992.   [33] F. Heider, The Psychology of Interpersonal Relations.  Taylor & Francis, 1958.   [34] P. J. Carrington, J. Scott, and S. Wasserman, Models and  methods in social network analysis, vol. 28. Cambridge  university press, 2005.   [35] W. Simpson and others, The quadratic assignment  procedure (QAP), in North American Stata Users Group  Meetings 2001, 2001.   [36] C. DuBois, C. Butts, and P. Smyth, Stochastic  blockmodeling of relational event dynamics, in   Proceedings of the Sixteenth International Conference on  Artificial Intelligence and Statistics, 2013, pp. 238246.   [37] T. A. Snijders, Models for longitudinal network data,  Models Methods Soc. Netw. Anal., vol. 1, pp. 215247,  2005.   [38] G. Robins, P. Pattison, Y. Kalish, and D. Lusher, An  introduction to exponential random graph (p*) models for  social networks, Soc. Netw., vol. 29, no. 2, pp. 173  191,  2007.   [39] O. Frank and D. Strauss, Markov graphs, J. Am. Stat.  Assoc., vol. 81, no. 395, pp. 832842, 1986.   [40] S. Wasserman and P. Pattison, Logit models and logistic  regressions for social networks: I. An introduction to  Markov graphs andp, Psychometrika, vol. 61, no. 3, pp.  401425, 1996.   [41] P. Wang, G. Robins, P. Pattison, and E. Lazega,  Exponential random graph models for multilevel  networks, Soc. Netw., vol. 35, no. 1, pp. 96115, 2013.   [42] G. Edwards, Mixed-method approaches to social network  analysis, Natl. Cent. Res. Methods, p. 30, 2010.   [43] S. L. Simpson, S. Hayasaka, and P. J. Laurienti,  Exponential Random Graph Modeling for Complex Brain  Networks, PLoS ONE, vol. 6, no. 5, p. e20039, 2011.   [44] M. de Laat, V. Lally, L. Lipponen, and R.-J. Simons,  Investigating patterns of interaction in networked learning  and computer-supported collaborative learning: A role for  Social Network Analysis, Int. J. Comput.-Support.  Collab. Learn., vol. 2, no. 1, pp. 87103, 2007.   [45] B. Nick, C.-K. Lee, P. Cunningham, and U. Brandes,  Simmelian backbones: amplifying hidden homophily in  facebook networks, in Advances in Social Networks  Analysis and Mining (ASONAM), 2013 IEEE/ACM  International Conference on, 2013, pp. 525532.   [46] M. E. Newman, Scientific collaboration networks. II.  Shortest paths, weighted networks, and centrality, Phys.  Rev. E, vol. 64, no. 1, p. 016132, 2001.   [47] D. R. Hunter, M. S. Handcock, C. T. Butts, S. M.  Goodreau, Morris, and Martina, ergm: A Package to Fit,  Simulate and Diagnose Exponential-Family Models for  Networks, J. Stat. Softw., vol. 24, no. 3, pp. 129, 2008.   [48] J. S. Cramer, The standard multinomial logit model, in  Logit Models from Economics and Other Fields,  Cambridge University Press, 2003, pp. 104125.   [49] Y. Croissant, mlogit: multinomial logit model. 2013.  [50] D. Lusher, J. Koskinen, and G. Robins, Exponential   Random Graph Models for Social Networks: Theory,  Methods, and Applications. Cambridge University Press,  2012.   [51] S. Hanneke, W. Fu, and E. P. Xing, Discrete temporal  models of social networks, Electron. J. Stat., vol. 4, pp.  585605, 2010.   [52] S. Joksimovi, N. Dowell, O. Skrypnyk, V. Kovanovi, D.  Gaevi, S. Dawson, and A. C. Graesser, How do you  connect Analysis of Social Capital Accumulation in  connectivist MOOCs, presented at the The 5th  International Learning Analytics and Knowledge (LAK)  Conference (Accepted), 2015.   [53] G. Siemens, Connectivism: A learning theory for the  digital age, Int. J. Instr. Technol. Distance Learn., vol. 2,  no. 1, pp. 310, 2005.     "}
{"index":{"_id":"40"}}
{"datatype":"inproceedings","key":"Bakharia:2016:CFL:2883851.2883944","author":"Bakharia, Aneesha and Corrin, Linda and de Barba, Paula and Kennedy, Gregor and Gavsevi'c, Dragan and Mulder, Raoul and Williams, David and Dawson, Shane and Lockyer, Lori","title":"A Conceptual Framework Linking Learning Design with Learning Analytics","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"329--338","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883944","doi":"10.1145/2883851.2883944","acmid":"2883944","publisher":"ACM","address":"New York, NY, USA","keywords":"intervention design, learning analytics, learning design","abstract":"In this paper we present a learning analytics conceptual framework that supports enquiry-based evaluation of learning designs. The dimensions of the proposed framework emerged from a review of existing analytics tools, the analysis of interviews with teachers, and user scenarios to understand what types of analytics would be useful in evaluating a learning activity in relation to pedagogical intent. The proposed framework incorporates various types of analytics, with the teacher playing a key role in bringing context to the analysis and making decisions on the feedback provided to students as well as the scaffolding and adaptation of the learning design. The framework consists of five dimensions: temporal analytics, tool-specific analytics, cohort dynamics, comparative analytics and contingency. Specific metrics and visualisations are defined for each dimension of the conceptual framework. Finally the development of a tool that partially implements the conceptual framework is discussed.","pdf":"A Conceptual Framework linking Learning Design with  Learning Analytics  Aneesha Bakharia Information Systems School  Queensland University of Technology  Brisbane, Australia aneesha.bakharia@qut.edu.au  Linda Corrin Melbourne Centre for the Study of Higher Education  The University of Melbourne Melbourne, Australia  lcorrin@unimelb.edu.au  Paula de Barba Melbourne Centre for the Study of Higher Education  The University of Melbourne Melbourne, Australia  paula.de@unimelb.edu.au  Gregor Kennedy Melbourne Centre for the Study of Higher Education  The University of Melbourne Melbourne, Australia gek@unimelb.edu.au  Dragan Gaevic Moray House School of Education and School of  Informatics University of Edinburgh  Edinburgh, Scotland dgasevic@acm.org  Raoul Mulder School of BioSciences  The University of Melbourne Melbourne, Australia  r.mulder@unimelb.edu.au  David Williams Department of Physiology  The University of Melbourne Melbourne, Australia  d.williams@unimelb.edu.au  Shane Dawson Teaching Innovation Unit  University of South Australia Adelaide, Australia  shane.dawson@unisa.edu.au  Lori Lockyer School of Education Macquarie University  Sydney, Australia lori.lockyer@mq.edu.au  ABSTRACT In this paper we present a learning analytics conceptual framework that supports enquiry-based evaluation of learn- ing designs. The dimensions of the proposed framework emerged from a review of existing analytics tools, the analy- sis of interviews with teachers, and user scenarios to under- stand what types of analytics would be useful in evaluating a learning activity in relation to pedagogical intent. The proposed framework incorporates various types of analyt- ics, with the teacher playing a key role in bringing context to the analysis and making decisions on the feedback pro- vided to students as well as the scaolding and adaptation of the learning design. The framework consists of five di- mensions: temporal analytics, tool-specific analytics, cohort dynamics, comparative analytics and contingency. Specific metrics and visualisations are defined for each dimension of the conceptual framework. Finally the development of a tool that partially implements the conceptual framework is discussed.  Categories and Subject Descriptors K3.2 [Computers & Education]: Computer and Informa- tion Science Education - computer science education, infor- mation systems education  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883944  Keywords Learning analytics, Intervention design, Learning design  1. INTRODUCTION Over recent years, learning analytics has emerged as a  powerful tool for addressing a range of educational chal- lenges and issues, including concerns over institutional reten- tion (particularly for underrepresented groups), and continu- ous improvement of the student learning experience through personalised learning. However, the vast potential of learn- ing analytics to influence and mitigate many of these con- cerns remains essentially untapped in terms of day-to-day teaching practice. Most analytics studies have drawn on historical data to identify patterns in students learning be- haviour which are then related to academic performance and/or retention. Much of this work, however, is lacking in an understanding of the pedagogical context that influences student activities, and how identifying patterns in students learning behaviours can be used to influence and contribute to more positive teaching and learning experiences [16, 5]. Essentially there is a knowledge gap for teachers in attempt- ing to bridge the divide between the information provided by learning analytics and the types of pedagogical actions designed by teachers to support student learning. The field of learning design oers a way to address this gap by help- ing teachers to articulate the design and intent of learning activities which can be used as a guide for the interpretation of learning analytics data.  In this paper we outline a framework that brings together both learning design and analytics to create more mean- ingful representations of data for teachers. In so doing we argue that an understanding of course context is essential to providing more adaptive and appropriate analytics visualisa-  http://dx.doi.org/10.1145/2883851.2883944   tions to aid interpretation and translation into direct actions to support student learning. The framework development is situated within the context of a cross-institutional learning analytics study in Australia which investigated the peda- gogical concerns and needs faced by teachers in their local contexts, and how learning analytics may usefully provide actionable evidence that allows them to respond to those concerns or needs. The interview data collected from teach- ers that informed the design of the framework will be pre- sented as well as a description of an online analytics tool was developed which operationalised parts of the framework.  2. BACKGROUND As the field of learning analytics evolves, the need to  align both analytic approaches and outputs with a concep- tual frame of reference and educational context has been ac- knowledged [6, 22]. The field of learning design oers the po- tential to do this. Lockyer, Agostinho and Bennett (in press) [14] define learning design as both a process of creating and adapting pedagogical ideas as well as the product of a for- malised description of a sequence of learning tasks, resources and support that a teacher constructs for students for an entire, or part of, an academic semester. The field of learn- ing design allows educators and educational researchers to articulate how educational contexts, learning tasks, assess- ment tasks and educational resources are designed to pro- mote eective interactions between teachers and students, and students and students, to support learning (see [7, 15]). Given this, well-articulated learning designs provide clear insight into teachers pedagogical intent behind the learning activities and assessment tasks they provide to students [16]. These articulated learning designs can, therefore, provide a critical frame of reference for the interpretation of patterns of student interactions that are generated by learning an- alytics techniques. In other words, learning design can be said to provide a semantic structure for analytics [18, pp. 312].  While it makes conceptual sense to argue that articulated learning designs can provide aframe of reference for the in- terpretation of learning analytics outputs, questions remain about how this can be achieved in practice. This is compli- cated by the fact there are several ways such learning designs can be represented, including the Learning Design Visual Se- quence [3] and the IMS Learning Design specification [10]. Thus a core challenge for the learning analytics community is to determine conceptual and practical frameworks that can link teachers enacted practice (i.e., their learning designs) with data and evidence that emerge from learning analytics through the use of accessible learning analytics tools. This needs to be achieved in ways that ultimately are useful in informing ongoing educational practice.  3. METHODOLOGY The development of the framework was informed by the  outcomes of a study conducted across three Australian uni- versities in 2014/2015. The study aimed to develop an online tool to provide meaningful analytics to teachers to support teaching and learning. Three main sources of information were used to help conceptualise and categorise the analytic needs and wants of teachers. These included a review of the literature on existing learning analytics tools, interviews with teaching sta across the three institutions, and spe-  cific user scenarios for each of the courses that were to be used to pilot the tool. The review of the literature iden- tified several learning analytics tools that had been devel- oped to provide learning analytics data to teachers. Each of these tools were then examined to determine if any theo- retical/learning design foundation was present, the metrics used, and the methods of data visualisation employed.  The semi-structured interviews were conducted with teach- ers across the three participating Australian universities. The purpose of the interviews was to determine the ways in which learning analytics could be used to assist teach- ers to address the fundamental education problems or situ- ations they face in the delivery of online and blended learn- ing. The interviews explored the curriculum structures and learning designs teachers employed within their classes, the pedagogical problems they faced in their teaching, the ways in which they used technology-based tools in their teach- ing, and the role learning analytics could play in addressing some of their known concerns. The sample consisted of 12 participants, four from each institution, who were involved in the delivery of courses that used a range of tools within the Learning Management System (LMS). To ensure cross- disciplinary representation, the participants were course co- ordinators from across the arts, professions and science disci- plines. Table 1 provides an overview of the discipline, num- ber of enrolled students and tools used in the delivery for each participants course. Once all interviews had been con- ducted a thematic analysis was performed on the interview transcripts to identify and group the needs and wants of teachers interviewed.  In addition to the review of existing analytics tools and the interviews, user scenarios were developed for each of the courses that were to be used to pilot the analytics tool. The case profiles contained more detailed information about the four pilot courses including: learning outcomes, structure of lectures/tutorials, assessment details, LMS tools used, the purpose of these tools within the curriculum, current cur- riculum evaluation methods, and suggestions for analytics that could be useful for the course. Additional suggestions identified through the user scenarios were then added to the outputs of the literature review and interview analysis. An important function of the user scenarios was to inform the prioritisation of development of the web-based analytics tool components.  The existing analytics tool review, interview analysis, and user scenarios combined to form the basis for the develop- ment of the conceptual framework which identifies dierent types of learning analytics that could respond directly to the kinds of issues and concerns that teachers expressed with the learning contexts, learning designs and learning activi- ties they used in their online and blended learning environ- ments. Each dimension of the conceptual framework was then used to identify the functional requirements of a learn- ing analytics tool that would support enquiry-based learning design. The user scenarios were then used to prioritise the development of the web-based learning analytics tool.  4. RESULTS AND DISCUSSION In this section, a conceptual framework that links learn-  ing analytics to learning design is presented. The conceptual framework consists of five dimensions: temporal analytics, comparative analytics, cohort dynamics, tool specific analyt- ics, and contingency and intervention support tools. In the    Table 1: Course details for recruited participants.  proposed conceptual framework the teacher plays a crucial role in bringing context to the analysis and making deci- sions on the feedback provided to students as well as the scaolding and adaptation of the learning design.  4.1 Temporal Analytics All the interview participants indicated that they valued  the ability to see course, content and tool access statistics over the duration of the course within the LMS. The abil- ity to see students access to elements within a course (i.e., curriculum content and tools) and the duration of students sessions was identified as important for teachers as they tried to reconcile how they have structured the overall course and scheduled key activities within it, with how the students have chosen to access the content, tasks and assessment tools within the LMS. In particular, they wanted to iden- tify course material that was valuable to students and was being reviewed multiple times. They also wanted to check whether students viewed or posted in a discussion forum. Ten of the twelve interviewees indicated a need to review access statistics at an individual student level to be able to provide individualised support and/or to deal with student appeals.  Whilst teachers sequence and schedule content and ac- tivity according to their learning design, within the LMS there is no explicit way to represent this pedagogical intent when implementing content and activities. The LMS allows content and activities to be added in a hierarchical struc- ture. Through the interviews and user scenarios it was clear that most teachers created weekly folders to contain the con- tent and activities specific to the topic being covered in that week. Therefore the week was defined as the most rele- vant period for temporal analysis and the lense with which to review student activity allowing the teacher to link this  back to the course structure and schedule. An additional request, also within the temporal dimen-  sion, made by six of the twelve interview participants was the ability to see course and content access before and after key instructional events, such as the weekly lecture, tutori- als, assessment due dates, or the start and finish of a quiz. As an example, one participant was interested in knowing whether students were accessing prescribed pre-reading ma- terial prior to the lecture and tutorial. Five other interview participants were also interested in seeing resource access (i.e., slides, notes, video recordings) prior to a quiz, and ob- serving the impact this had on student grades. Temporal events specified by the participants fell into the following three categories:  Recurring Events  Events that occurred at the same time each week such as a tutorial or a lecture.  Submission Events  Events which included the due dates for the submission of assessment items and the dates on which an online quiz was made available to students.  Single Events  Events that only occurred once in the semester, such as a guest lecture or field trip.  4.2 Comparative Analytics Comparative analytics allow the teacher to observe pat-  terns or relations between two or more aspects of a course. Each of the interview and user scenario participants requested to be able to see how scheduled learning activities impacted on student participation (and thereby gain some insight into whether activities were achieving the desired learning design    objective). In addition, nine of the interview participants wanted to be able to compare these activities and levels of participation with each other over time. This type of comparative analysis is only possible with a clear knowledge of the course structure and activity scheduling within the LMS. By being able to comparatively review activities and students participation in them, activities can be identified that may be candidates for redesign, for a greater level of student scaolding, or for other forms of intervention.  Comparative analytics in this context is defined as the provision of analytics in a form that enables the teacher to compare dierent types of activities that may occur within the same time period as well as the same types of activities occurring over dierent time periods. Examples include:   the ability to compare access to content, communica- tion and assessment tools over the duration of a week or the whole semester.   the ability to compare access to each content, commu- nication and assessment item by week.   the ability to compare each students course access by week.  Comparative analytics is not restricted to the temporal domain, and is equally applicable to social network, content or discourse analyses. For example, a social network dia- gram that describes the online discussion flows of a single small group could be the subject of comparative analytics, showing how the involvement of the group members shifts and changes over time and in relation to other components of the course. In this way, comparative analytics provides a lense through which the structure and sequence of designed activities within the curriculum, implemented through the use of LMS tools, can be evaluated.  4.3 Cohort Dynamics Seven of the participants in the interviews expressed the  desire to view which students had accessed or not accessed a specific item of content or tool. Similar to the request for course access statistics (as described in the Temporal Analytics section), this requirement on the surface seems relatively simple. However, while LMS reports provide sim- ple access logs, typically they require extensive processing to be easily used and interpreted. Moreover, teachers would be required to drill-down and extensively filter data to gain insight into individual students patterns of access and usage for content items. All of the interview and user scenario par- ticipants requested the ability to view tool specific metrics displayed by student. For example, they wanted the abil- ity to view the number of quiz attempts, access to lecture recordings and forum posts made by a particular student.  The ability to view students access to course items also relates to the ability to identify student pathways and the impact their patterns of activity may have had on learning outcomes. While the request for access to view students who had both accessed and not accessed course content and tools could be regarded as relatively simple, several partic- ipants requested the ability to analyse the access pathways that relate access patterns to student performance on assess- ment. As an example, one participant requested the ability to compare the performance of students who attended lec- tures and accessed online lecture recordings with students  who only viewed the lecture recording. There was an ex- pectation that there would be common access patterns for particular groups of students and that identifying these stu- dent groups would allow teachers to better understand the cohort dynamics. Another example of this was a request to compare the access patterns of successful and unsuccessful students, based on grades, to identify dierences and advise the underperforming students on more eective approaches to study.  An understanding of the cohort dynamics can be helpful in determining how dierent groups of students interact and engage with overarching curriculum structures and the spe- cific learning designs of elements of the course (e.g content, assessments, discussion). If dierent types of interaction patterns manifested for dierent groups of students, these may results in dierent learning outcomes. The implica- tion is that the learning design may be more accessible by particular groups of students, resulting in dierent levels of success. This could help to highlight where learning activ- ities may need to be scaolded or interventions may need to be considered for particular cohorts. Cohort dynamics may also dier between course oerings with each semester bringing a dierent student dynamic that must be under- stood in realtime in order for learning designs to be adapted to better meet student requirements [20].  4.4 Tool Specific Analytics While temporal analytics relate to all content and tools  implemented in the LMS course sites, interview and user scenario participants also identified the need for analytics that were specifically tailored to the particular LMS tools they were using. Simple requests included the analysis of quiz scores, quiz attempts, and counts for discussion forum posts. More advanced analytics were requested for quizzes, such as, a need for quiz item analysis that can be more easily interpreted than what the LMS currently provides. The ability to map questions to concepts and provide ag- gregate reporting across these concepts was also requested. One participant suggested analytics based on content analy- sis would be useful to identify the topics that students were either exploring in a discussion forum. Six of the interview participants were interested in visualisations able to show the networks that were forming within collaborative activi- ties involving forums. Sociograms, as illustrated by Lockyer, Heathcote and Dawson [16] provide a way to identify devi- ations between observed and anticipated interactions based on the learning design. There was also a need identified by four of the interview participants for the analysis of learning activities occurring outside the LMS with participants using a variety of social media platforms (i.e., Facebook, Twitter, Blogs). Interview participants who utilize lecture recordings or media resources in their learning design were interested in knowing whether the media was streamed or downloaded, and what portions were being played.  Tool specific analytics can clearly be used in conjunction with comparative analytics and cohort dynamics. In partic- ular, there is potential to compare an emerging social net- work by week and include any specific tool related metrics as features to a clustering algorithm. The addition of tool spe- cific metrics would allow additional types of student groups based upon similarity to be discovered.    4.5 Contingency and Intervention Support Tools  Eleven of the twelve interview participants highlighted the value of identifying and intervening when students were de- termined to be potentially at risk (because they did not access crucial content or achieve a pass score on a quiz or assessment). In most cases, the intervention proposed was sending an email to identified students alerting them to the fact they were falling behind and providing them with advice on the areas on which they should concentrate. Participants indicated that currently the identification and selection of students who were at risk was often a labour intensive and manual task. Groups of students were often determined by filtering a spreadsheet of data based on quiz scores, and then emailing specific feedback and guidance to students whose score fell within a certain range.  The contingency dimension seeks to address this issue by providing tools to help teachers identify and select an indi- vidual, a group, or multiple groups of students - based on some determined parameters - for the purpose of providing appropriate intervention and guidance. The contingency di- mension is associated with the cohort dynamics dimension, in that patterns established on the basis of student similarity discovery algorithms will allow teachers to eectively select and identify student groups for some kind of intervention. Contingency is, however, seen as an outcome of cohort dy- namics - only after the teacher has understood why students in a particular group or cohort were similar (e.g., they may not have participated in key activities or grasped a core con- cept) can he or she provide appropriate advice to both sup- port students understanding and scaold their approaches to learning activities.  Contingency also requires a productivity element, in that, tools to facilitate the communication with students are needed. Particularly, the ability to select and email multiple students or send template messages with custom fields to person- alise email with student details and scores (i.e., mail merge features). The provision of tools that simplify workload in terms of performing interventions is a necessity, potentially increasing the chance for teachers to adopt the use of tech- nology in their teaching practice [8].  5. THE LEARNING ANALYTICS FOR LEARNING DESIGN CONCEPTUAL FRAMEWORK  Figure 1 illustrates the proposed Learning Analytics for Learning Design Conceptual Framework. The proposed con- ceptual framework aims to transform learning design into a teacher-led enquiry-based practice. In the framework, the teacher plays a central role in bringing contextual knowl- edge to the review and analysis of the learning analytics and then in making decisions in relation to contingency.  The framework incorporates the dierent analytical di- mensions that emerged from the review of existing learning analytics tools, thematic analysis of the semi-structured in- terviews conducted with the study participants and the user scenarioss, namely temporal analytics, tool specific analyt- ics and cohort dynamics. Each analytical dimension can be reviewed and analysed through a comparative approach by the teacher. Comparative analytics and related support visualisations are relevant across all other analytical dimen- sions and contribute to the teachers ability to make sense  of the data. Comparative analysis therefore provides the lense through which the teacher evaluates the learning de- sign implementation within the LMS in direct relation to the activity scheduling and sequencing dictated by the course design.  The teacher plays a key role within the proposed concep- tual framework in bringing teaching and learning context to the analysis. Firstly, the teacher must use their tacit domain knowledge and understanding of the macro (i.e., the course structure and higher level curriculum design) and micro (i.e., implementation of activities within the LMS) level learning designs while analysing and reviewing the learning analyt- ics and visualisations. This view is supported by Lockyer, Heathcote and Dawson [16], who state that The interpre- tation of visualizations also depends heavily on an under- standing the context in which the data were collected and the goals of the teacher regarding in-class interaction. (p. 1446). Secondly, the teacher uses the insight gained from the analytics and contextual knowledge to make decisions on improving the delivery of the learning objectives, thereby adapting the learning design (i.e., contingency). Persico and Pozzi [20] see the teacher drawing analogies with similar ac- tivities and carrying out comparative evaluation. Pardo, El- lis, and Calvo [19] argue that analysis of the digital footprint cannot alone lead to informed learning activity redesign and that qualitative data on why the students have engaged in dierent ways leads to improved interpretation. The teacher is therefore crucial in being able to collate the required qual- itative data and incorporate these findings in the learning design adaptation decision-making process.  In the proposed conceptual framework, contingency oc- curs as the output and is the result of learning design deci- sions made by the teacher. Contingency can take the form of restructured or scaolded learning activities or recommen- dations and feedback provided to distinct student groups. Contingency requires the teacher to understand the dier- ent types of analytics and interpret the patterns emerging from the cohort dynamics dimension. Contingency also re- lies on the availability of tools to identify, select, filter and communicate feedback to students.  The cohort dynamics dimension needs to be supported by algorithms that are able to discover and provide inter- pretable student usage and similarity patterns to the teacher. Yi, et al. [24], define pattern detection as a means to find specific distributions, trends, frequencies, outliers, or struc- ture in the dataset and go on to say that by using pattern detection, people may not only find what they have been looking for but also discover new knowledge that they did not expect to find. Manual pattern discovery may however be a dicult and time consuming task, given the number of features to be evaluated. There are two types of machine learning algorithms that are suitable for inclusion in the co- hort dynamics dimension:  Sequential Pattern Mining  Sequential pattern mining algorithms are able to find not only what content and/or tools students are ac- cessing but also whether there was an order in the way that groups of students accessed the content and/or tools [21]. The page flow visualisation [1] represents a useful way to visualise the output of a sequential mining algorithm.    Figure 1: The Learning Analytics for Learning Design Conceptual Framework  Unsupervised Clustering  Unsupervised clustering algorithms are capable of find- ing similar students based on what the student ac- cessed, when they accessed the items, and any other metrics such as quiz scores, forum contributions, and forum post vocabulary. Example algorithms include k-means clustering, dimension reduction, non-negative matrix factorisation and nearest-neighbour classifiers. These algorithms are commonly used in the educa- tional data mining and learning analytics literature for student profiling [13, 17].  In Table 2, the specific types of analytics, metrics and visualisations that relate to each of the dimensions in the framework are included. Table 2 can be viewed as a blueprint for the types of analytics that need to be included in order to use learning analytics to inform enquiry-based learning design improvements.  6. THE LOOP TOOL - A REFERENCE IM- PLEMENTATION OF THE PROPOSED CONCEPTUAL FRAMEWORK  The open source Loop tool is the reference implementation for the Learning Analytics for Learning Design Conceptual Framework (for more detail on the component of the tool see [4]). The Loop tool is programmed in Python and uses the Django web application framework. The Loop tool is made up of two components: (1) a data warehouse and (2) a web interface to display metrics and visualisations. The Loop tool supports the Blackboard and Moodle Learning Management Systems and is a partial implementation of the proposed conceptual framework. The Loop tool currently implements the temporal and comparative dimensions. The  cohort dynamics, tool specific and contingency dimensions are currently being implemented.  6.1 Course Data Preprocessing In order to perform the access-related analytics by person  (i.e., teacher, tutor or student), by week, and by content item or tool, both a course access log and the course struc- ture are required. In older versions of Moodle, the course export format contained both the logs and the hierarchi- cal course structure manifest. In more recent versions of Moodle, the course export zip file and a csv export from the log tracking database table need to be processed. For Blackboard courses the IMS-CP archive format [9] (which includes forum posts) and an export of the Accumulator database table need to be processed. The IMS-CP archive format contains the manifest file in XML format that pro- vides the course structure hierarchy.  Within the data warehouse, multidimensional cubes are created using a star schema architecture [2] commonly found in Business Intelligence applications. The star schema in- cludes a fact table (i.e., a table with a timestamped entry for each course item accessed) with related dimension tables that store the dates (i.e., a table containing the the day, month, year and day of week), the course items by type and the users (students and teaching sta). The star schema allows analytics for aggregate calculations by week, tool and student to be stored. An additional categorisation for course items is performed to group items for analysis based upon whether the item is a content item, a communication tool or an assessment tool.  6.2 Dashboards The Loop tool provides a dashboard for each week in a  semester. The week was determined, through the inter-    Table 2: Metrics and Visualisations for the Conceptual Framework  Temporal Analysis Tools Specific Analysis   Access by day of week and total access by week   Access per course structure item   Session duration and average session duration   Unique page views   Ability to visualise the impact of assessment and other events on activity   Types of analytics/visualisation:   Timelines   Event Markers for recurring, submission and once-o events   The inclusion of metrics, analytics and visualisation specific to the type of LMS and social media tools being used.   Quizzes   Quiz scores   Number of quiz attempts   Quiz item analysis   Forums   No of forum posts   Topics being discussed (i.e., Topic Modeling)   Social network analysis   Discourse analysis  Cohort Dynamics/Patterns Comparative Analysis   Inclusion of weekly metrics, overall semester metrics and tool specific metrics as features for pattern discov- ery algorithms   Analysis of student sequential access for patterns   Session duration and average session duration   Types of analytics/visualisation:   Finding students who accessed content/tools and those that did not   Cohort dynamics can be found by clustering stu- dent features by week and also by course duration whole of course.   Allowing teachers to search for similar students   Allowing teachers to search for students with spe- cific attributes (e.g., Quiz score lower than, etc)   Need to compare the impact of dierent learning ac- tivities   Week by week comparison of content and tool access   Week by week comparison of student course content item access   Comparison of access and usage of content, communi- cation and assessment within the LMS course   Comparative analysis with previous cohorts (i.e., group dynamics might change and learning design needs to adapt to it)   Types of analytics/visualisation:   Timelines and event markers   Correlations between activities and measures of engagement eg correlation between communica- tion tool access and session time or quiz scores   Display of expandable hierarchical course struc- ture tree with column counts for each week  Contingency and Decision Support Tools   Inclusion of weekly metrics, overall semester metrics and tool specific metrics as features for pattern discov- ery algorithms   Ability for teachers to easily recommend strategies and provide feedback to students   Types of Contingency Tools:   Finding students who accessed content/tools and those that did not   Allowing teachers to search for similar students   Allowing teachers to search for students with spe- cific attributes (e.g., Quiz score lower than, etc)   Allowing teachers to email groups of students us- ing templates (e.g., mail merge)    views and user scenarios, to be a good period of activity measurement in terms of the way topics and activities are designed within higher education. Each weekly dashboard includes content, communication tool and assessment tool access graphs for each day of the week. Recurring, sub- mission and single events can also be defined. These are displayed on the timeline graphs to allow teachers to see the pre and post event course access changes.  Figure 2 shows the weekly dashboard (temporal dimen- sion). Summary statistics for session duration and average session length are included as sparklines. Top users and items assessed during the week are also displayed on the dashboard.  The Loop tool uses bubble charts to illustrate the percent- age of activity that has occurred before and after an event (Figure 3). The teacher is able to define multiple events which they can then be selected to produce the pre and post event visualisation.  6.3 Comparative Visualisations A tree table is used to display hierarchical course content  access by week (see Figure 4). The inclusion of the tree table visualisation was inspired by Loco-Analyst, a tool identified in the literature review which integrated analytics with the course structure [11]. Weekly forum post counts, assessment item attempts and average scores are also shown by week.  6.4 Contingency For each course item a teacher is able to see the students  who accessed the item and those who have not accessed the item. Filtering by date is also provided. This is the only basic form of contingency that the Loop tool currently in- cludes. In future versions of the Loop tool, the functionality for teachers to easily identify similar groups of students as well as recommend strategies and provide feedback to stu- dents will be provided.  6.5 Individual Course Item and Student Views The Loop tools allows teachers to drill-down to view indi-  vidual student access statistics and specific tool metrics (i.e., temporal and comparative dimensions). Figure 5 shows a timeline of total student access for a specific course item. Assessment attempts, scores and number of forum posts are also included (i.e., tool specific dimension).  6.6 Future Directions This paper has detailed the progress made in Phases 1  and 2 of an Australian Government Oce of Learning and Teaching funded project called Completing the Loop [12]. The first two phases involved collection of data to inform the development of the framework and design of the tool (as outlined above in the methodology section). Phase 3 of the project has commenced and a trial of the Loop tool is underway. Results from the trial using courses from Black- board and Moodle will be used to validate and extend the proposed conceptual framework.  Additional functionality that supports the cohort dynam- ics and contingency dimensions are currently being inves- tigated for inclusion in the Loop tool. Figure 6 shows the result of using the t-sne dimension reduction algorithm [23] to cluster students using using weekly course metrics (i.e., content access, communication tool access, assessment at- tempts and scores, forum posts and session duration in each  week within the semester) as features. The aim is to pro- vide visualisations that allow teachers to identify clusters of students and then provide insight on why students in each cluster are similar to help teachers interpret student groups and provide appropriate feedback and intervention.  7. CONCLUSION In this paper, we have proposed a learning analytics con-  ceptual framework for learning design. The framework was informed by an understanding of the types of learning an- alytics that would be useful to support the evaluation of learning designs. While clear dimensions for the types of analytics required emerged from interviews with teachers, it was evident that the teacher played crucial roles in bringing the learning and teaching context into the interpretation of the analytics and also in making decisions based upon the analytics. Five main types of analytics namely temporal, comparative, tool specific, cohort dynamics and contingency were identified.  The proposed framework makes a useful theoretical contri- bution, bridging the gap between learning design and learn- ing analytics while establishing a platform to support enquiry- based evaluation and scaolding of learning activities. The utility of the proposed framework, however, lies in its abil- ity to direct the types of analytics and contingency support tools that are essential to support the learning design pro- cess. As illustrated in this paper, each dimension in the con- ceptual framework leads to clear analytics and visualisation requirements; which in turn were able to be implemented within the Loop tool. The Loop tool is currently being tri- aled and enhanced to incorporate analytics and tools for the cohort dynamics and contingency dimensions. The evalua- tion results of the Loop tool will be used to further develop, refine and validate the proposed framework.  8. ACKNOWLEDGEMENTS Support for this project has been provided by the Aus-  tralian Government Oce for Learning and Teaching. The views in this project do not necessarily reflect the views of the Australian Government Oce for Learning and Teach- ing. The study discussed in this paper was approved by the University of Melbournes Melbourne Graduate School of Education Human Ethics Advisory Group (MGSE HEAG), approval number 1339454.    Figure 2: Weekly Course dashboard included in the Loop Tool (Temporal dimension): A = Daily page views;  B = Critical learning events; C = Week metrics; D = Top accessed content.  Figure 3: Individual course item access (Temporal  dimension)  Figure 4: A tree table visualisation (Comparative  dimension)  Figure 5: Individual course item access (Temporal  dimension)  Figure 6: Student groups discovered by t-sne dimen-  sion reduction    9. REFERENCES [1] Introducing flow visualization: visualizing visitor flow.  http://analytics.blogspot.com.au/2011/10/ introducing-flow-visualization.html. Accessed: 2015-10-12.  [2] Star schema. https://en.wikipedia.org/wiki/Star schema. Accessed: 2015-10-12.  [3] S. Agostinho, B. M. Harper, R. Oliver, J. Hedberg, and S. Wills. A visual learning design representation to facilitate dissemination and reuse of innovative pedagogical strategies in university teaching. 2008.  [4] L. Corrin, G. Kennedy, P. de Barba, A. Bakharia, L. Lockyer, D. Gasevic, D. Williams, S. Dawson, and S. Copeland. Loop: A learning analytics tool to provide teachers with useful data visualisations. In Globally connected, digitally enabled. Proceedings ascilite Perth 2015, pages 409413, 2015.  [5] D. Gasevic, S. Dawson, T. Rogers, and D. Gasevic. Learning analytics should not promote one size fits all: The eects of instructional conditions in predicating academic success. The Internet and Higher Education, 2016.  [6] D. Gasevic, S. Dawson, and G. Siemens. LetaAZs not forget: Learning analytics are about learning. TechTrends, 59(1):6471, 2015.  [7] P. Goodyear. Teaching, technology and educational design: The architecture of productive learning environments. The Australian Learning and Teaching Council, 2009.  [8] M. S.-J. Gregory and J. M. Lodge. Academic workload: the silent barrier to the implementation of technology-enhanced learning strategies in higher education. Distance Education, pages 121, 2015.  [9] IMS Global Learning Consortium. Content packaging specification. https://www.imsglobal.org/content/packaging. Accessed: 2015-10-12.  [10] IMS Global Learning Consortium. Learning design specification. http://www.imsglobal.org/learningdesign, note = Accessed: 2015-10-12.  [11] J. Jovanovic, D. Gasevic, C. Brooks, V. Devedzic, M. Hatala, T. Eap, and G. Richards. Loco-analyst: semantic web technologies in learning content usage analysis. International journal of continuing engineering education and life long learning, 18(1):5476, 2008.  [12] G. Kennedy, L. Corrin, L. Lockyer, S. Dawson, D. Williams, R. Mulder, S. Khamis, and S. Copeland. Completing the loop: returning learning analytics to teachers. In Rhetoric and Reality: Critical perspectives on educational technology. Proceedings ascilite Dunedin 2014, pages 436440, 2014.  [13] V. Kovanovic, D. Gasevic, S. Joksimovic, M. Hatala, and O. Adesope. Analytics of communities of inquiry: Eects of learning technology use on cognitive presence in asynchronous online discussions. The Internet and Higher Education, 27:7489, 2015.  [14] L. Lockyer, S. Agostinho, and S. Bennett. Design for e-learning. In J. F. . E. M. C. Haythornthwaite, R. Andrews, editor, The SAGE Handbook of  E-learning Research. Sage, Thousand Oaks, CA, in press.  [15] L. Lockyer, S. Bennett, S. Agostinho, and B. Harper. Handbook of research on learning design and learning objects: issues, applications, and technologies (2 volumes). IGI Global, Hershey, PA, 2009.  [16] L. Lockyer, E. Heathcote, and S. Dawson. Informing pedagogical action: Aligning learning analytics with learning design. American Behavioral Scientist, page 0002764213479367, 2013.  [17] G. Lust, J. Elen, and G. Clarebout. Regulation of tool-use within a blended course: Student dierences and performance eects. Computers & Education, 60(1):385395, 2013.  [18] Y. Mor, R. Ferguson, and B. Wasson. Editorial: Learning design, teacher inquiry into student learning and learning analytics: A call for action. British Journal of Educational Technology, 46(2):221229, 2015.  [19] A. Pardo, R. A. Ellis, and R. A. Calvo. Combining observational and experiential data to inform the redesign of learning activities. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 305309. ACM, 2015.  [20] D. Persico and F. Pozzi. Informing learning design with learning analytics to improve teacher inquiry. British Journal of Educational Technology, 46(2):230248, 2015.  [21] P. Reimann, L. Markauskaite, and M. Bannert. e-research and learning theory: What do sequence and process mining methods contribute British Journal of Educational Technology, 45(3):528540, 2014.  [22] B. Rienties, L. Toetenel, and A. Bryan. Scaling up learning design: impact of learning design activities on lms behavior and performance. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 315319. ACM, 2015.  [23] L. Van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(2579-2605):85, 2008.  [24] J. S. Yi, Y. Kang, J. T. Stasko, and J. A. Jacko. Understanding and characterizing insights: how do people gain insights using information visualization In Proceedings of the 2008 Workshop on BEyond time and errors: novel evaLuation methods for Information Visualization, page 4. ACM, 2008.  http://analytics.blogspot.com.au/2011/10/introducing-flow-visualization.html http://analytics.blogspot.com.au/2011/10/introducing-flow-visualization.html https://en.wikipedia.org/wiki/Star_schema https://www.imsglobal.org/content/packaging http://www.imsglobal.org/learningdesign   Introduction  Background  Methodology  Results and Discussion  Temporal Analytics  Comparative Analytics  Cohort Dynamics  Tool Specific Analytics  Contingency and Intervention Support Tools   The Learning Analytics for Learning Design Conceptual Framework  The Loop Tool - A Reference Implementation of the Proposed Conceptual Framework  Course Data Preprocessing  Dashboards  Comparative Visualisations  Contingency  Individual Course Item and Student Views  Future Directions   Conclusion  Acknowledgements  References   "}
{"index":{"_id":"41"}}
{"datatype":"inproceedings","key":"Rienties:2016:ILD:2883851.2883875","author":"Rienties, Bart and Toetenel, Lisette","title":"The Impact of 151 Learning Designs on Student Satisfaction and Performance: Social Learning (Analytics) Matters","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"339--343","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883875","doi":"10.1145/2883851.2883875","acmid":"2883875","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative learning, data analytics, distance learning","abstract":"An increasing number of researchers are taking learning design into consideration when predicting learning behavior and outcomes across different modules. This study builds on preliminary learning design work that was presented at LAK2015 by the Open University UK. In this study we linked 151 modules and 111.256 students with students' satisfaction and performance using multiple regression models. Our findings strongly indicate the importance of learning design in predicting and understanding performance of students in blended and online environments. In line with proponents of social learning analytics, our primary predictor for academic retention was the amount of communication activities, controlling for various institutional and disciplinary factors. Where possible, appropriate communication tasks that align with the learning objectives of the course may be a way forward to enhance academic retention.","pdf":"The impact of 151 learning designs on student satisfaction  and performance: social learning (analytics) matters   Bart Rienties & Lisette Toetenel  Institute of Educational Teachnology   Open University UK  [First.Lastname]@open.ac.uk   ABSTRACT  An increasing number of researchers are taking learning design  into consideration when predicting learning behavior and  outcomes across different modules. This study builds on  preliminary learning design work that was presented at LAK2015  by the Open University UK. In this study we linked 151 modules  and 111.256 students with students satisfaction and performance  using multiple regression models. Our findings strongly indicate  the importance of learning design in predicting and understanding  performance of students in blended and online environments. In  line with proponents of social learning analytics, our primary  predictor for academic retention was the amount of  communication activities, controlling for various institutional and  disciplinary factors. Where possible, appropriate communication  tasks that align with the learning objectives of the course may be a  way forward to enhance academic retention.   CCS Concepts   Applied computing~Distance learning      Applied computing~E-learning    Keywords       Data analytics, Collaborative Learning, Distance Learning   1. INTRODUCTION  There is an increased interest in predictive modeling in education.  Beyond identifying students that require additional support, in the  Learning Analytics Knowledge (LAK) community many scholars  are interested in identify trends in learning and teaching from rich  data sources. In order to identify the meaning of some of these  trends, pedagogical information is required and this has often  been ignored to date [1]. Pedagogical knowledge or information  granted through Learning Design provides the context to the  quantitative analysis that Learning Analytics is able to provide.  Although several studies [2-4] and a specific LAK workshop [5]  on learning design have used principles of learning design to  unpack the complexities of learning analytics in the last four to  five years, few studies have empirically compared the impact of  different learning designs on learning processes and outcomes.   For example, Conole [6, p121] describes learning design as a  methodology for enabling teachers/designers to make more  informed decisions in how they go about designing learning  activities and interventions, which is pedagogically informed and  makes effective use of appropriate resources and technologies.  Learning design is widely studied in the Higher Education sector,  but the definition of this concept has various meanings in different  settings and similar work has been carried out under such names  as pedagogical patterns, learning patterns and pattern language  [3, p1441].  Learning design is implemented as a way to improve course  design [4, 7, 8], but few studies have empirically connected  learning designs of a substantial number of courses with learning  behavior in Learning Management Systems (LMSs) and learning  performance. This study builds on preliminary learning design  work that was presented at LAK2015 by the Open University UK  (OU). In this explorative study we indeed found that learning  design decisions made by teachers were related to learning  behavior of students in blended and online environments [9]. An  important finding of this study amongst 40 modules and 27K  students was that assimilative learning design activities (such as  reading, watching) were positively correlated to learner  satisfaction, but negatively to academic performance. Our current  study builds on this initial explorative study by focusing on an  extensive elaboration of the scope and reach of our data analysis,  whereby we linked 151 modules and 111K students with students  satisfaction and performance using multiple regression models,  whereby we were able to control for various institutional and  disciplinary factors to determine what the key drivers for learning  are, and whether our initial findings still hold with this richer  dataset.    1.1 Learning design meets learning analytics  While substantial progress has been made in the last 10 years in  conceptualising learning design [7, 8] by for instance using a  data-informed approach, relatively few studies have investigated  how educators in practice are actually planning, designing,  implementing and evaluating their learning design decisions.  Evaluating the success of a learning activity for instance by  analysing activity logs of student behavior is more informative  when compared to the overall pedagogy and design of the course.   By linking large datasets across a range of 40+ modules in online  and blended learning settings, both studies [9] point to the  important notion often ignored in learning analytics: by analysing  the impact of learning design on learner satisfaction and academic  performance across a range of modules, a cross-sectional study  may provide crucial (generalizable) insights beyond the specific  research findings within a single module or discipline.   In a recent study by Li, Marsh and Rienties [10], using logistical  regression modelling learner satisfaction data were analysed and  the findings indicated that these proxies of learning design had a   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883875       strong and significant impact on overall satisfaction. Similarly,  using logistic regression with a primary purpose of improving  aggregate student number forecasts Calvert [11] found 30  variables in five broad categorizations which broadly predicted  progression of students..  Although these studies provide (indirect) evidence that proxies of  learning design and individual student characteristics influence  learner satisfaction and academic retention, none of these studies  have identified across a large range of modules whether  objectively mapped learning designs of modules have an impact  of actual student behavior, learner satisfaction and academic  retention. In this follow-up study of our LAK 2015 paper, we aim  to address this gap by comparing the learning designs of 151  modules that were followed by over 110k online students at  different disciplines, levels, and programmes.    2. METHOD  2.1 OULDI Learning Design   The learning design taxonomy used for this article was developed  as a result of the Jisc-sponsored Open University Learning Design  Initiative (OULDI) [12], and was developed over five years in  consultation with eight Higher Education institutions. In contrast  to instructional design, Learning Design is process based [6];  practitioners make informed design decisions with a pedagogical  focus and communicate these to their colleagues and learners.  This is especially relevant for institutions which deliver distance  learning. The OU follows a collaborative design approach, based  upon almost a decade of academic and institutional research ([13].   Table 1: Learning design activities   Type of activity Example   Assimilative Attending to  information   Read, Watch, Listen,  Think about, Access.   Finding and  handling  information   Searching for and  processing  information   List, Analyse, Collate,  Plot, Find, Discover,  Access, Use, Gather.    Communication Discussing module  related content with at  least one other person  (student or tutor)   Communicate, Debate,  Discuss, Argue, Share,  Report, Collaborate,  Present, Describe.   Productive Actively constructing  an artefact   Create, Build, Make,  Design, Construct,  Contribute, Complete,.    Experiential Applying learning in a  real-world setting    Practice, Apply, Mimic,  Experience, Explore,  Investigate,.   Interactive  /adaptive   Applying learning in a  simulated setting    Explore, Experiment,  Trial, Improve, Model,  Simulate.    Assessment All forms of  assessment  (summarive, formative  and self assessment)    Write, Present, Report,  Demonstrate, Critique.   For a detailed description of the seven learning descriptions and  theoretical foundations, we refer to previous published work [9,  14]. Assimilative activities relate to tasks in which learners attend  to discipline specific information. These include reading text  (online or offline), watching videos, or listening to an audio file.  In terms of social learning analytics conceptualisations, the next   five categories describe different options available to teachers to  create an interactive, social learning environment [1, 15]. By  finding and handling information, for example on the internet or  in a spreadsheet, learners take responsibility for extending their  learning, and are therefore engaged in active learning.  Communicative activities refer to any activities in which students  communicate with another person about module content.  Productive activities refer to activities whereby learners build and  co-construct new artefacts. Experimental activities provide  learners with the opportunity to apply their learning in a real life  setting. Interactive activities endeavor to do the same, such as  simulations. Finally, assessment activities encompass all learning  materials focused on assessment to monitor (formative) progress  and/or traditional assessment for measurement purposes. Table 1  identifies the seven types of learning activity in the OULDI  model.   2.2 Setting  This study took place at the OU, the largest higher education  provider of online distance education in Europe. A process of  module mapping (i.e. analyzing and providing visualizations of  the learning activities and resources involved in a module) was  introduced as part of a university-wide learning initiative [9, 14]  which aims to use learning design data for quality enhancement.  The mapping process is comprehensive, but labour intensive;  typically taking between one and three days for a single module,  depending on the modules number of credits, structure, and  quantity of learning resources. A team of learning design  specialists reviewed all the available learning materials, classifies  the types of activity, and quantifies the time that students are  expected to spend on each activity.   Classifying learner activity can be subjective, and consistency is  important when using the data to compare module designs across  disciplines in the institution. Therefore, the learning design team  held regular meetings to improve consistency across team  members in the mapping process. Once the mapping process was  complete, the learning design team manager reviewed the module  before the findings were sent to the faculty. Academics had the  opportunity to comment on the data before the status of the design  was finalised. In other words, each mapping was at least reviewed  by three people, which enhanced the reliability and robustness of  the data relating to each learning design.   2.3 Instruments  2.3.1 Learning Design mapping  The learning design tool at the OU is a combination of graphical,  text-based tools that are used in conjunction with learning design  workshop activities, which were mandated at particular times in  the design process. In total 189 modules were mapped by the  learning design team in the period January 2014-October 2015.  Given that the OU offers multiple presentations of modules per  year, in total 276 module implementations were recorded, of  which we could link 151 modules with learning performance data  (see next section). In total 113.725 students were enrolled in these  151 modules, with an average module size of 753.15 (SD=  828.89). For each module, the learning outcomes were captured in  the Learning Design tools. Each activity within the modules  weeks, topics, or blocks was categorized according to the learning  design taxonomy (see Table 1). These categorizations were  captured in a visual representation in the form of an activity  planner (or blueprint).      2.3.2 Learner satisfaction  Our second core dependent variable is learner satisfaction. In the  past thirty years, the OU has consistently collected learner  feedback to further improve the learning experience and learning  designs. In line with other learner satisfaction instruments [16- 18], at the OU the Student Experience on a Module (SEaM)  questionnaire was implemented. The SEaM institutional survey  was introduced in 2012/13 combining two previous surveys using  a census approach; so inviting all learners on all modules to  participate. It consists of three themed sets of in total 40  questions: 1) The module overall (10 items), 2) Teaching, learning  and assessment (14 items) and 3) Feedback on the tutor (16  items). Following our analysis of key drivers amongst 65K  students learning experience [10], for this analysis we used the  aggregate scores of five core items that drive learner satisfaction.   2.3.3 Academic retention  Our first dependent variable is academic retention, which was  calculated by the number of learners who completed and passed  the module relative to the number of learners who registered for  each module. Academic retention is a key concern of many  institutions, and in particular at the OU. The academic retention  ranged between 34.46% and 100%, with an average of 69.35  (SD= 12.75). These figures do need to be read in the context of  the OUs mission to provide education for all, regardless of  entrance requirements [19].    2.3.4 Institutional analytics data  In line with previous studies [20-22], we included several  institutional analytics data that are known to influence the  students learning experience, such as the level of the course  (ranging from level 1 to level 4, which is roughly translated into  year 1 to post-graduate) [11], the specific discipline [23], the year  of implementation, size of the class or module [20-22]. In terms of  VLE engagement, the average number of minutes spent in the  VLE per week were used as proxy for engagement [24].     2.4 Data analysis  All data were collected on an aggregate, module level. As a first  step, we merged the learning design data with the LMS and  learner retention data based upon module ID and year of  implementation. In total 151 module implementations could be  linked with LMS learning behavior and learning performance  data. In order to correct for any selection-bias in terms of selecting  modules for these mapping activities, we compared these 151  module implementations vs. 1016 module implementations which  were not mapped in the Learning Design tool in 2014/2015.  Indeed significantly more level 0-1 and fewer post-graduate  modules were mapped, but no significant differences were found  in terms of academic performance or student experience (so  limiting selection bias). As the learning design team primarily  focused on large scale undergraduate modules, this result was  expected.   All data was anonymized by the first author, whereby names and  codes of modules and respective disciplines were replaced by  random codes to safeguard the identities of teachers and their  respective faculty. Follow-up regression analyses were conducted  using SPSS 21.    3. RESULTS  3.1 Relating learning design with learner  satisfaction  As a next step, we linked the learning design metrics with learner  satisfaction. On average, 80.85% (SD= 11.06) of the 26483  (28.99%) students who responded to the SEAM survey were  satisfied with their learning experience, with a range of 39-97%.  A significant positive correlation was found between assimilative  activities and Average SEAM (r = .333, p < .01), while negative  correlations were found in terms of finding information (r = -.258,  p < .01) and communication (r = -.224, p < .01).   Three separate regression analyses were conducted, whereby  learner satisfaction was significantly predicted by students who  followed the Level 0 access models, whom were significantly  more positive than other modules. Other institutional variables  such as disciplinary differences were mostly not significantly  predicting learner satisfaction, in line with previous findings [10]  that students at the OU have similar learning experiences  irrespective of disciplinary differences.   Table 2: Regression model of learner satisfaction and learning   performance predicted by institutional and learning design  analytics      Learning  Satisfaction   Learning  performance   Level0 .351** .005  Level1 .265 .017  Level2 -.212 -.004  Level3 -.018 .215  Year of implementation -.059 -.151*  Faculty 1 .213* .360**  Faculty 2 .045 -.189*  Faculty 3 .236* .069  Faculty other .051 .034  Size of module -.071 -.239**  Finding information -.294** -.154  Communication .050 .500**  Productive -.274** .133  Experiential -.105 .008  Interactive .221* -.049  Assessment -.221* .063   LMS engagement .117 -.190*  Learning Satisfaction  -.058  R-sq adj 31% 36%   n = 150 (Model 1-2), 140 (Model 3), * p < .05, ** p < .01  When we added the learning design activities, learner satisfaction  was significantly negatively predicted by finding information,  experiential and assessment learning activities, and positively  predicted by interactive activities (again with assimilative  activities as the reference point). Separate analysis with  assessment as reference point (not illustrated) indicated that  assimilative activities significantly and positively predicted  learner satisfaction, while the betas for the other three predicting     learning activities remained similar. Finally, when we added LMS  engagement the primary predictors remained the same, but  engagement in LMS did not predict learner satisfaction. The seven  learning activities explained 18% of variance, and when the  institutional analytics were included 12% of unique variance was  explained. The final, complete model is presented in Table 2. In  other words, learning design activities had a significant and  substantial impact on learner experience, whereby modules with  more assimilative and fewer inquiry and discovery-based learning  activities were perceived to lead to better learner experiences (for  at least those who complete the surveys).    3.2 Relating learning design with learning  performance  Three regression models were used to predicted academic  retention, whereby the final model is presented in Table 2.  Academic retention was significantly positively predicted by  students following Faculty 1 (relative to reference point of Faculty  4). Furthermore, academic retention was negatively predicted by  the overall size of the module and year of implementation. In  other words, modules that were relatively large in size, more  focused on natural sciences, and those that were taught in more  recent academic years 2014-2015 had relatively lower retention  rates than smaller modules and modules taught in academic years  2012-2013. When adding the average learning experience and  VLE engagement, no significant relations were found between  learner satisfaction, VLE engagement and academic retention.  Finally, when adding the seven learning design activities,  communication significantly and positively predicted academic  retention. LMS engagement negatively predicted academic  retention when the seven learning design activities were included,  which may counterbalance some of the effects of communication.  The seven learning activities explained 11% of variance, and  when the institutional analytics were included 6% of unique  variance was explained. Separate analyses (not illustrated) with  assessment rather than assimilative learning design activities as a  reference point indicated that assimilative had a negative but non- significant impact on retention when taking the other variables  into account. In other words, as illustrated in Figure 1 in simple  laymen terms, communication (as reported in percentages on Y- axis) seemed to be a key lever for retention (as reported from 0-1  on X-axis) in blended and online distance education at the OU.     Figure 1: Communication and academic retention (per   discipline).   4. DISCUSSION  Pedagogy and learning design have played a key role in computer- assisted learning in the last two decades [6], but research has not  extensively linked learning design to learner performance [23,  25]. Progress has recently been made in how (combinations of)  individual learning design elements (e.g., task design, feedback,  scaffolding, structure) influence learning processes and success in  experimental and natural settings within single modules. Building  on our first study [9], this study has provided strong empirical  evidence that learning design had a significant influence on  learner satisfaction and academic retention amongst 151 modules  followed by 113.725 students.   Our first and perhaps most important finding is that learning  design and learning design activities in particular strongly  influenced academic retention. A major innovation is that we were  able to move beyond simple correlation analyses to multiple  regression analyses, whereby we were able to control for common  institutional analytics factors and disciplinary differences. This  approach was useful, as our initial analysis with correlation  analysis presented at LAK2015 seemed to indicate that modules  with a heavy reliance on content and cognition (assimilative  activities) seemed to lead to lower completion and pass rates.  However, when controlling for the institutional data sources and  modelling the seven learning design activities simultaneously, the  negative link between assimilative learning design and academic  retention was no longer significant. The primary predictor of  academic retention was the relative amount of communication  activities. This is an important finding as most teachers are the  OU and across the globe have a tendency to focus on cognition  rather than social learning activities [21, 23, 26], while recently  several authors in the LAK field have encouraged teachers and  researchers to focus on the social elements of learning [1, 21].  Our second important finding was that learner satisfaction was  strongly influenced by learning design. Modules with assimilative  activities and fewer student-centred approaches like finding  information activities received significantly higher evaluation  scores. However, a crucial word of caution is in place here.  Although we agree with others [17, 18, 21] that learner  satisfaction and happiness of students is important, it is  remarkable that learner satisfaction and academic retention were  not even mildly related to each other in Table 2. More  importantly, the (student-centred) learning design activities that  had a negative effect on learner experience had a neutral to even  positive effect on academic retention.   Two possible explanations are available for the widely different  effects of learning design on learner satisfaction and academic  retention. First, although more than 80% of learners were satisfied  with their learning experience, as evidenced by several leading  scholars [25, 26] learning does not always needs to be a nice,  pleasant experience. Learning can be hard and difficult at times,  and making mistakes, persistence, receiving good feedback and  support are important factors for continued learning. Our findings  indicate that students may not always be the best judge of their  own learning experience and what help them in achieving the best  outcome.   Second, on average 72% of students who participated in these 151  modules did not complete the learner satisfaction survey. In  certain modules actual dropout was well above 50%, indicating  that students were voting with their feet when the learning  design and/or delivery did not meet their learning needs. An  exclusive focus on learner satisfaction might distract institutions     from understanding the true learning experiences and academic  retention. If our findings are replicated in other contexts, a crucial  debate with academics, students and managers needs to develop  whether universities should focus on happy students and  customers, or whether universities should design learning  activities that stretch learners to their maximum abilities and  ensuring that they eventually pass the module. Where possible,  appropriate communication tasks that align with the learning  objectives of the course may seem to be a way forward to enhance  academic retention.   5. CONCLUSION AND FUTURE WORK  A major innovation in comparison to our initial study is that we  were able to execute multiple regression analyses, whereby we  were able to control for common institutional analytics factors and  disciplinary differences, but it highly likely that additional factors  contribute to the satisfaction and retention to the factors included  in the model.  In the near future, we would be able to extend this  sample further when more data becomes available in order to  better understand the complex (inter)relations of learning design  on learning processes and outcomes as we will be able to combine  this with further data sets such as student and tutor comments.   In addition, combining this analysis with the learning outcomes  data allows sharing of good practice based upon robust analysis.  Furthermore, a particularly useful feature would be to integrate  this with demographic, individual and socio-cultural data about  students, so that subgroups can be analysed. This may influence  whether a learning design is suitable for a range of learners. In  terms of practical implications for LAK, researchers, teachers and  policy makers need to be aware of how learning design choices  made by teachers influence subsequent learning processes and  learning performance over time.   REFERENCES  [1] Ferguson, R. and Buckingham Shum, S. Social learning analytics:   five approaches. ACM, City, 2012.  [2] Thompson, K., Ashe, D., Carvalho, L., Goodyear, P., Kelly, N. and   Parisio, M. Processing and Visualizing Data in Complex Learning  Environments. American Behavioral Scientist, 57, 10 (October 1,  2013 2013), 1401-1420.   [3] Lockyer, L., Heathcote, E. and Dawson, S. Informing Pedagogical  Action: Aligning Learning Analytics With Learning Design.  American Behavioral Scientist, 57, 10 (October 1, 2013 2013), 1439- 1459.   [4] Lockyer, L. and Dawson, S. Learning designs and learning analytics.  In Proceedings of the Proceedings of the 1st International  Conference on Learning Analytics and Knowledge (Banff, Alberta,  Canada, 2011). ACM, [insert City of Publication],[insert 2011 of  Publication].    [5] Lockyer, L. and Dawson, S. Where learning analytics meets learning  design. In Proceedings of the Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (Vancouver,  British Columbia, Canada, 2012). ACM, [insert City of  Publication],[insert 2012 of Publication].    [6] Conole, G. Designing for Learning in an Open World. Springer,  Dordrecht, 2012.   [7] MacLean, P. and Scott, B. Competencies for learning design: A review  of the literature and a proposed framework. British Journal of  Educational Technology, 42, 4 2011), 557-572.   [8] Armellini, A. and Aiyegbayo, O. Learning design and assessment with  e-tivities. British Journal of Educational Technology, 41, 6 2010),  922-935.   [9] Rienties, B., Toetenel, L. and Bryan, A. Scaling up learning  design: impact of learning design activities on LMS behavior and   performance. ACM, City, 2015.   [10] Li, N., Marsh, V. and Rienties, B. Modeling and managing learner  satisfaction: use of learner feedback to enhance blended and online  learning experience. Decision Sciences Journal of Innovative  Education2016).   [11] Calvert, C. E. Developing a model and applications for probabilities  of student success: a case study of predictive analytics. Open  Learning: The Journal of Open, Distance and e-Learning, 29, 2  2014), 160-173.   [12] Cross, S., Galley, R., Brasher, A. and Weller, M. Final Project  Report of the OULDI-JISC Project: Challenge and Change in   Curriculum Design Process, Communities, Visualisation and   Practice. JISC, York, 2012.   [13] Cross, S., Galley, R., Brasher, A. and Weller, M. Final Project   Report of the OULDI-JISC Project: Practice, Challenge and Change   in Curriculum Design Process, Communities, Visualisation and   Practice. City, 2012.  [14] Toetenel, L. and Rienties, B. Analysing 157 Learning Designs using   Learning Analytic approaches as a means to evaluate the impact of  pedagogical decision-making. British Journal of Educational  Technology2016).   [15] Buckingham Shum, S. and Ferguson, R. Social Learning Analytics.  Journal of educational technology & society, 15, 3 2012).   [16] Marsh, H. W. SEEQ: a reliable, valid, and useful instrument for  collecting students' evaluations of university teaching. British Journal  of Educational Psychology, 521982), 77-95.   [17] Onwuegbuzie, A. J., Witcher, A. E., Collins, K. M. T., Filer, J. D.,  Wiedmaier, C. D. and Moore, C. W. Students Perceptions of  Characteristics of Effective College Teachers: A Validity Study of a  Teaching Evaluation Form Using a Mixed-Methods Analysis.  American Educational Research Journal, 44, 1 (March 1, 2007  2007), 113-160.   [18] Zerihun, Z., Beishuizen, J. and Os, W. Student learning experience as  indicator of teaching quality. Educational Assessment, Evaluation  and Accountability, 24, 2 (2012/05/01 2012), 99-111.   [19] Richardson, J. T. E. Approaches to studying across the adult life  span: Evidence from distance education. Learning and Individual  Differences, 26, 0 (8// 2013), 74-80.   [20] Arbaugh, J. B. and Duray, R. Technological and Structural  Characteristics, Student Learning and Satisfaction with Web-Based  Courses: An Exploratory Study of Two On-Line MBA Programs.  Management Learning, 33, 3 (September 1, 2002 2002), 331-347.   [21] Arbaugh, J. B. System, scholar, or students Which most influences  online MBA course effectiveness Journal of Computer Assisted  Learning, 30, 4 2014), 349-362.   [22] Marks, R. B., Sibley, S. D. and Arbaugh, J. B. A Structural Equation  Model of Predictors for Effective Online Learning. Journal of  Management Education, 29, 4 (August 1, 2005 2005), 531-563.   [23] Rienties, B., Kaper, W., Struyven, K., Tempelaar, D. T., Van Gastel,  L., Vrancken, S., Jasinska, M. and Virgailaite-Meckauskaite, E. A  review of the role of Information Communication Technology and  course design in transitional education practices. Interactive Learning  Environments, 20, 6 2012), 563-581.   [24] Tempelaar, D. T., Rienties, B. and Giesbers, B. In search for the most  informative data for feedback generation: Learning Analytics in a  data-rich context. Computers in Human Behavior, 472015), 157-167.   [25] Kirschner, P. A., Sweller, J. and Clark, R. E. Why Minimal Guidance  During Instruction Does Not Work: An Analysis of the Failure of  Constructivist, Discovery, Problem-Based, Experiential, and Inquiry- Based Teaching. Educational Psychologist, 41, 2 (2006/06/01 2006),  75-86.   [26] Koedinger, K., Booth, J. L. and Klahr, D. Instructional Complexity  and the Science to Constrain It. Science, 342, 6161 (November 22,  2013 2013), 935-937.          "}
{"index":{"_id":"42"}}
{"datatype":"inproceedings","key":"Bos:2016:SDR:2883851.2883890","author":"Bos, Nynke and Brand-Gruwel, Saskia","title":"Student Differences in Regulation Strategies and Their Use of Learning Resources: Implications for Educational Design","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"344--353","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883890","doi":"10.1145/2883851.2883890","acmid":"2883890","publisher":"ACM","address":"New York, NY, USA","keywords":"blended learning, cluster analysis, individual differences, learning dispositions, regulation strategies","abstract":"The majority of the learning analytics research focuses on the prediction of course performance and modeling student behaviors with a focus on identifying students who are at risk of failing the course. Learning analytics should have a stronger focus on improving the quality of learning for all students, not only identifying at risk students. In order to do so, we need to understand what successful patterns look like when reflected in data and subsequently adjust the course design to avoid unsuccessful patterns and facilitate successful patterns. However, when establishing these successful patterns, it is important to account for individual differences among students since previous research has shown that not all students engage with learning resources to the same extent. Regulation strategies seem to play an important role in explaining the different usage patterns students' display when using digital learning recourses. When learning analytics research incorporates contextualized data about student regulation strategies we are able to differentiate between students at a more granular level. The current study examined if regulation strategies could account for differences in the use of various learning resources. It examines how students regulated their learning process and subsequently used the different learning resources throughout the course and established how this use contributes to course performance. The results show that students with different regulation strategies use the learning resources to the same extent. However, the use of learning resources influences course performance differently for different groups of students. This paper recognizes the importance of contextualization of learning data resources with a broader set of indicators to understand the learning process. With our focus on differences between students, we strive for a shift within learning analytics from identifying at risk students towards a contribution of learning analytics in the educational design process and enhance the quality of learning; for all students.","pdf":" Student differences in regulation strategies and their use  of learning resources: implications for educational design   Nynke Bos  University of Amsterdam   Faculty of Social and Behavioural Sciences  Amsterdam, The Netherlands   N.R.Bos@uva.nl   Saskia Brand-Gruwel  Open University of the Netherlands   Faculty Psychology and Educational Sciences  Heerlen, The Netherlands   Saskia.Brand-Gruwel@ou.nl     ABSTRACT  The majority of the learning analytics research focuses on the  prediction of course performance and modeling student behaviors  with a focus on identifying students who are at risk of failing the  course. Learning analytics should have a stronger focus on  improving the quality of learning for all students, not only  identifying at risk students. In order to do so, we need to  understand what successful patterns look like when reflected in  data and subsequently adjust the course design to avoid  unsuccessful patterns and facilitate successful patterns.  However, when establishing these successful patterns, it is  important to account for individual differences among students  since previous research has shown that not all students engage  with learning resources to the same extent. Regulation strategies  seem to play an important role in explaining the different usage  patterns students display when using digital learning recourses.  When learning analytics research incorporates contextualized data  about student regulation strategies we are able to differentiate  between students at a more granular level.   The current study examined if regulation strategies could account  for differences in the use of various learning resources. It  examines how students regulated their learning process and  subsequently used the different learning resources throughout the  course and established how this use contributes to course  performance.  The results show that students with different regulation strategies  use the learning resources to the same extent. However, the use of  learning resources influences course performance differently for  different groups of students. This paper recognizes the importance  of contextualization of learning data resources with a broader set  of indicators to understand the learning process. With our focus  on differences between students, we strive for a shift within  learning analytics from identifying at risk students towards a  contribution of learning analytics in the educational design  process and enhance the quality of learning; for all students.   Categories and Subject Descriptors   Applied computing~Computer-assisted instruction   Mathematics of computing~Exploratory data analysis    Theory of computation~Unsupervised learning and clustering   Keywords  Individual differences, regulation strategies, blended learning,  cluster analysis, learning dispositions.   1. INTRODUCTION  All animals are equal, but some animals are more equal than  others [32]. Although it initially seems that Orwell's criticism on  the totalitarian political system of the Soviet Union has nothing to  do with learning analytics, a closer look at this commandment  suggests otherwise. Learning analytics, in general, treats all  students as equal, while in fact some students are more equal than  others.   The objectives relating to the use of learning analytics can  globally be described to serve six goals: predicting course  performance and discovering learner models; suggesting relevant  learning resources to student; increasing reflection and awareness  about the learning process; enhancing social learning  environments by visualization of social interactions; detecting  undesirable learning behaviors; and detecting affects of learning  like boredom or confusion [37]. Although these issues are highly  interrelated, the majority of the learning analytics research focuses  on the prediction of course performance and modeling student  behaviors [12] targeted on identifying students who are at risk of  failing the course. This focus has a longer tradition within the  educational data mining community, which could account for this  overrepresentation. However, when modeling student behavior or  predicting course performance to identify at risk students, learning  analytics research focuses often on trace data from just one data  source, for example the use of formative assessments or the  number of comments in a forum or the hits in the Learning  Management System (LMS). In doing so, learning analytics  research ignores other course elements or other available trace  data and draws conclusions based on just a fraction of the course.  The risk of those isolated predictions is that they are detached  from pedagogical experiences, practices [16, 26] and interventions  [45], which reduce learning analytics to a series of clicks, and  page visits. In order to avoid these isolated predictions all trace  data of all available course elements and learning resources  should be taken into account.    Second, learning analytics should have a stronger focus on  improving the quality of learning for all students, not only  identifying at risk students [26]. The predictors for failure or poor  course performance, which are currently found, use predictive  modeling techniques for at risk students and cannot be reasonably  translated into recommendations to improve quality of learning   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883890      for all students [16]. If learning analytics wants to enhance the  quality of teaching and learning, a shift from predictive modeling  to identify students at risk towards pedagogical learning analytics  interventions is needed [45].    With a shift in a focus from identifying at risk students or  predicting failure towards improving quality of learning, learning  analytics needs to become an element of the learning design  process [45]. However, current learning analytics research  provides us with a limited amount of information on how to  improve the quality of education and have an impact on the design  process. Learning data analysis could help us to identify  successful students and their use of the specific learning  resources. If we can subsequently identify ahead of time what  successful and unsuccessful patterns look like [25] and adjust the  course design according to those patterns we can redirect, and  maybe even avoid, unsuccessful use of learning resources and  facilitate successful patterns.    However, when establishing these successful patterns, it is  important to account for individual differences among students.  Current learning analytics research often takes course averages,  for example the average amount of clicks within the LMS or the  average time spent on online learning activities, as a target for  predictive measures. However, this could lead to a false reference  point for some groups of students since other groups of students  can be overly active or inactive and hence influence the average  activity [45]. When examining successful and or unsuccessful  patterns it is important to provide aggregate measures for similar  kind of students. However, it is not clear on which criteria we  should aggregate these students. Learning analytics could  determine how students interact with digital learning resources  and establish successful or unsuccessful patterns of this  interaction. Research shows that students do not interact with  digital learning resources in the same way [14, 26, 27, 28, 29] and  use different learning approaches when using digital learning  recourses [16, 22]. Current learning data analysis uses measures,  as hits in the LMS, which do not reflect individual user  differences and several researchers propose that learning analytics  data should be contextualized with a broader set of indicators [9,  16, 28, 44]. In doing so, trace data will reflect more that solely  hits and clicks, but it opens up the opportunity to differentiate  between differences in students their learning approaches and  learning strategies [16]. So, adding more contextualized data to  trace data from the different learning resources could mean a shift  in the direction towards pedagogical learning analytics.    In summary, within learning analytics research a greater focus is  needed on understanding student behavior so learning analytics  can truly improve the quality of learning which goes further than  targeting at risk students. To understand student behavior, current  data sources need to be supplemented with contextualized data  approaches to learning so individual differences can be accounted  for. Or, as Orwell would put it, to get insight into why some  students are more equal than others.    The current research aims to provide insight into student behavior  by focusing on individual differences in approaches to learning  and subsequently how these differences affect the use of (digital)  learning resources. First we will explore individual differences in  the use of learning resources in a blended learning setting and its  connection with regulation of the learning process. The next  section describes the methodology of the current research and  subsequently the results are presentenced. The paper concludes  with a discussion and lines for future research.   2. BACKGROUND  2.1 Individual differences   One common source of trace data, which is often used to model  student behavior or predict course performance, is data from the  LMS. Results on the strengths of these predictions are  inconclusive although the majority of the results indicate that  duration of use has no direct impact on course performance [30,  35, 47]. Although these studies do differentiate between different  LMS variables, for example messages read, quizzes taken or time  spent online, they do not account for individual differences in the  use of these available digital learning resources. However,  research suggests that students show some distinct usage patterns  when offered different digital learning resources. For example,  within a blended learning course students either rely heavily on  one of the digital resources while ignoring other digital resources  [24], do not use the resources at all [27, 31], or use it as a  substitute for the face-to-face activities [8, 41].   Several studies conducted a cluster analysis based on trace data to  identify these different usage patterns. For example [28] found  four different clusters that reflect differences in the use of the  digital learning resources: the no-users, the intensive-active users,  selective users and intensive superficial users. Similarly [22]  found, also based on cluster analysis, several different user  profiles based on the use of digital learning resources and suggest  that these differences might be related to differences in students  metacognition and motivation. More specific research shows [14]  by performing a cluster analysis based on students conceptions  and approaches to learning, two different profiles: a cluster  focused on understanding and a cluster focused on reproduction  with subsequently differences in course performance.  Although the aforementioned studies show that students do differ  in the use of the learning resources within a blended course.  However there is little insight into why students do or do not use  certain digital learning recourses and what the consequences of  these (un)conscious choices are in relation to course performance,  although research suggests that goal- orientation [28], approaches  to learning [14] and the differences in instructional models [17]  may be an important predictor of frequency and engagement of  use.   2.2 Importance of self-regulation  Agency refers to the capacity to coordinate learning skills,  motivation and emotions to reach the goals. Self-regulated  learners exercise agency as they engage in a cycle of four main  stages: analyzing the task; setting goals and designing plans;  engaging in learning; and adjusting their approach to learning [44,  46]. One important aspect of student self-regulation of learning is  the decision on if and if so, how to use the learning resources  offered during a course: the learner agency [2]. The ability to self  regulate the learning process is reflected by effective approaches  and choices towards learning which are reflected in the capability  a student has to handle a difficult task, practice and evaluate their  learning and subsequently develop a deep understanding of  subject matter [33]. The ability to self-regulate the learning  process in an effective way is linked to academic success [34, 42].  Students personal approaches to learning are intertwined with  various other aspects of learning such as motivational aspects and  regulation of the learning process [39] and goal orientation [28].    2.2.1 Regulation of the learning process  Not all students are able to regulate their learning in an effective  way and some students rely on an external source to regulate their  learning process. This concept is known as external regulation.     This external source could be, for example, the instructor who  guides students through course material or the external source  could be the learning objectives of the course. There are also  students who suffer from a lack of regulation. These students have  difficulties in regulating the learning process as a whole and do  not find any support from internal or external sources. Research  shows that a student their goal orientation plays an important role  with regard to if and how a specific learning resource is being  used [26]. Students with a performance goal orientation show a  selective use of the learning resources, while students with a  mastery goal orientation show an active choice in their learning  resources. However, these differences in goal orientation and  consequences for the use of learning resources are not confirmed  by a similar study [4]. Moreover, studies that report these  differences among students in terms of their use of learning  resources acknowledge, all in retrospect, the importance of  students goal orientation, self-regulation and approaches to  learning when shaping these profiles [22]. The question remains  however if differences in regulation strategies actually causes  differences in the use of learning resources.   The majority of the research about regulation strategies takes  place in traditional settings of education. Fewer studies have been  conducted on the role of regulation strategies and their  implications for online or blended learning, although regulation  strategies do have an impact on the use of digital learning  resources. Students who are able to self-regulate their own  learning are likely to use digital learning recourses differently  than students who use an external regulation strategy. For  example, in their research [11], show that students with a  tendency to external regulate their learning have a higher amount  of logons to the LMS compared to the group of students who were  better at self regulating their learning. Similar results were found  in [36] wherein students, within a blended learning course on  statistics, show distinct differences in their use of the digital  learning resources based on their regulation strategy. Also [29]  investigated how students regulate the use of different learning  resources throughout the course by temporal analysis. A cluster  analysis showed that only a minority of the students (3%)  regulated the use of the learning resources in line with the course  phases and hence with the changing requirements of the course.  To sum up, self-regulation seems to play an important role and  seem to have an impact on the use of learning resources.  However, it remains unclear if these differences are actually  caused by differences in regulation of learning. Further research  into cause and effect of regulation strategies and the use of  learning resources must determine if differences in regulation  strategies causes these differences so successful patterns can be  identified and adjust the course design accordingly.     2.3 Course design  The design of a course determines to a large extent if predictive  modeling techniques will find significant predictors on the use of  digital learning resources [17]. If a course is designed which  requires a fair amount of LMS usage, a greater predictive value of  LMS components will be found compared to a different designed  course in which the LMS usage has a less prominent role [1, 16].  Within blended learning the dominant role still lays within face- to-face educational activities and the digital learning resources,  among the LMS, has a less prominent, and often, supporting role.   Blended learning is often associated with student-oriented  learning, in which students have varying degrees of control over  their own learning process [24]. The current notion of blended  learning is often an instructor-oriented approach in which the   instructor determines the digital learning recourses that will be  used during the course [18]. When blended learning design  focuses on students and their choices to use the digital learning  recourses, there is a large variety in the use of these recourses by  students. Students use digital recourses in different ways, which  were often not intended for in the educational design. For  example, [20] find that when offering student optional learning  resources in a blended course, students rely heavily on one  supporting medium. They conclude that students do not create  blended learninga mix of different digital learning resources and they suggest that students need explicit guidance in how to  effectively combine learning resources. Also [27] finds three  distinct usage patterns in their research on the usage of digital  learning resources in a blended learning course: no-users,  intensive users and incoherent users. They find a significant lower  course performance of the no-users group. The authors provide no  explanation for the causes of these differences in the use of the  learning resource but suggest these differences might reflect  students dispositions as motivation, regulation strategies or  metacognitive ability.   In their research on the use of lecture recordings [8], in which  face-to-face lectures are recorded and made available afterwards,  they find similar usage patterns: no-users, supplemental users and  substitute users. What this study illustrates, is that within a  blended learning setting the offline educational activities, for  example face-to-face lectures, have a direct impact on the use of  digital learning resources. When analyzing learning data within a  blended learning setting one should take into account the course  design; in this case the direct relation between the use of digital  learning recourses and attendance to face-to-face activities, since  the use of digital learning resources is directly influenced by  supplemental or substitutional use. So, besides contextualizing  learning analytics data with data about regulation strategies,  learning analytics data should also consider attendance data for  face-to-face activities within a blended learning setting to account  for influences of the course design.   In summation, current use of learning analytics often uses trace  data from one learning resource for predictive modeling to  identify students who are at risk of failing a specific course.  Learning data analysis should, ultimately, contribute to the quality  of teaching and learning and should be an integrated part of the  educational design process. With this direction towards  pedagogical learning analytics we need to define what success  looks like and how individual differences in the use of digital  learning resources influences these pathways and subsequently  influence the learning design process.   In line with recommendations made by [16, 45] to move beyond  predictive analytics, we focus in this study on the differences in  regulation strategies, and analyze how differences in regulation  strategies reflect in the use of different learning recourses. When  examining the use of different learning resources, we combine the  use of offline learning resources (face-to-face activities) with the  use of online, digital learning resources, since these two nodes of  delivery are inextricably linked together in a blended learning  setting.  This research aims to answer the following questions:  1. Can we identify different clusters of students based on   differences in their regulation strategies  2. Do these differences in regulation strategies reflect in   differences in the use of (digital) learning resources  3. What combinations of (digital) learning resources contribute   most to course performance for each cluster     4. Do differences in regulation strategies reflect in differences in  course performance    3. METHODS  3.1 Participants  The participants were 333 first year university Psychology  students (243 female, 90 male, Mage = 20.17, SDage = 1.66)  attending an obligatory course on Biological Psychology. Students  who took the course as an elective or had taken the course before  were removed from the dataset.    3.2 The Blended Learning Course  The course consisted of 17 face to face lectures, with a 120- minute duration and a 15-minute break in half time over a period  of 8 weeks. These lectures were university style lectures, with the  instructor lecturing in front of the class. The face-to-face lectures  were recorded and made available directly after the lecture had  taken place and were accessible until the exam had finished. In the  course design the recorded lectures were offered to students with  the aim of supplementing the face-to-face lecture. If parts of the  lectures were unclear, students could use the recorded lectures to  revise these parts or revise the entire lecture if needed.  During the week several small workgroups were organized with  mandatory attendance. Before these workgroups, students had to  complete several assignments in the digital exercise book, which  contains additional study materials, supplemented with formative  assessments. Completing the formative assessments was  mandatory, passing or failing these formative assessments was  not. In total there were nine formative assessments available for  students.   Within the LMS, students had access to extra study materials, like  short introduction videos about certain concepts or additional  reading materials available for download.    During the eight-week course there were two separate summative  assessments. The first assessment covered the first four weeks of  the course and had a focus on assessing the knowledge domain.  The second assessment covered the last four weeks of the course  and had a focus on assessing higher order thinking skills. The  final grade for the course was calculated by taking the mean of  both assessments.   Upon completion students received 6 European Credit Transfer  and Accumulation Systems (ECTS).   3.3 Measurement instruments   Before the start of the course students were informed about the  research and were asked to consent. Three students did not  consent and were removed from the results.   In line with [28] we used multiple log indicators to capture ways  students used the learning resources. In most cases the frequency  of use and the duration of use were logged.    3.3.1 Attendance to face-to-face lectures  During the entire time frame of the lectures, student attendance  was registered on an individual level by scanning student cards  upon entry of the lecture hall. The scanning continued until 15  minutes after the lecture had started. The presence of the students  was registered for all 17 lectures of the course.   3.3.2 Viewing of the recorded lectures  The viewing of the lecture recordings was monitored on an  individual level and could be traced back to date, time, amount  and part of the lecture viewed. For each lecture a separate  recording was made, which made it possible to track the amount   of minutes a student watched a specific lecture. Following the  recommendations made by [23] the time on task measure was  calculated based on data cleaning methods used by [19] wherein  sessions shorter than two minutes were not considered to reflect  actual use. Moreover, besides removing the outliers, the time-out  chosen was four hours.    3.3.3 Formative assessments   For each formative assessment a log file within the LMS was  created to determine if a student completed the formative  assessment. Although passing or failing the formative assessments  was not part of the design of the course, these grades were stored  in the LMS. During the course students were obligated to  complete 7 of the 9 formative assessments in order to pass the  course. So besides the number of formative assessments  completed, also the average score of the completed assessments  was calculated.    3.3.4 LMS data  Two different types of LMS data were gathered. Except the  previously mentioned digital resources, the recorded lectures and  the formative assessments, the LMS also offered Powerpoint  slides and additional reading materials (PDF) for download as  well as some illustrative videos about certain topics. First the total  amount of hits within the LMS was registered. These are hits as  clicking on links to the recordings or formative assessments,  clicking on announcements, checking grades, clicking on links to  PDF files or links to certain video files. Second the total time  spent in the LMS during the course was registered. This is the  total time in minutes a student was logged on to the course in the  LMS during the entire timeframe of the course. This measure was  calculated by accumulating the time differences between logging  on the course and subsequently logging of or logging on to  another course.    3.3.5 Summative assessments   During the eight-week course, there were two separate summative  assessments, which were scored on a scale from 1 to 10, with 10  the highest, and 5.5 as a pass mark. The first assessment covered  the first four weeks of the course and the second assessment  covered the last four weeks of the course. Both assessments  contained 20 multiple-choice questions and 2 short essays  questions. The final score for this course was calculated by taking  the mean of these two assessments.    3.3.6 Inventory Learning Style (ILS)   The Inventory Learning Style (ILS) [39] is a self-report diagnostic  instrument intended to measure aspects of study method, study  motives and mental models about studying in higher education.  The ILS consists of 120 items and contains four domains:  processing strategies, regulation strategies, learning orientation  and mental models of learning. For the purpose of the current  study only the sub-scales of the domain regulation strategies were  scored. These sub-scales are: self-regulation (11 items), external  regulation (11 items) and lack of regulation (6 items). For a  complete description of the ILS and each of its subscales we refer  to [39].  The ILS was offered to students during the first week of the  course. Completing the ILS was mandatory.    3.4 Data analysis   To establish differences in regulation strategies of students at the  beginning of the course, we performed a two-step cluster analysis  on ILS regulation strategy data. A two-step cluster analysis  determines the natural and meaningful differences, formed in     clusters, which appear within the current population. The two-step  method is preferred over other forms of cluster analysis when both  continuous and categorical variables are used and when the  amount of clusters is not pre-determined [10]. Cluster analysis  was chosen over scoring the subscale regulation as one factor  model since students tend to show variations in the way they  regulate learning throughout the course, depending for example  on the task at hand [43].  Next a MANOVA between the different clusters was conducted to  determine significant differences in the use of (digital) learning  recourses. The MANOVA was used to determine if certain  clusters, based on regulation of the learning process, made a  significant amount more use of certain learning resources than  others.   Third a stepwise multivariate analysis was conducted for each  cluster to determine the relative contribution of each of the  different learning resources on course performance. This was  done to determine which (combination of) learning resources  contribute to the final grade for each separate cluster and  differences between clusters.   Finally an ANOVA determined if there were any significant  differences between the different clusters and course performance.    4. RESULTS  First, before determining how students differ in their regulation  strategies, the reliability of the subscales of the ILS domain  regulation strategies were calculated. The results can be found in  table 1.    Table 1: Reliability of the ILS subscale Regulation Strategies   Subscale Reliability (Cronbachs Alpha)   Self Regulation .76   External Regulation .71   Lack of Regulation .73     4.1.1 Cluster analysis  Since the subscales show sufficient reliability the next step was to  cluster students based on their reported regulation strategies.  Using the two-step auto-clustering algorithm, 333 students were  assigned to different clusters. The auto-clustering algorithm  indicated that three clusters was the best model, because it  minimized the Schwarz's Bayesian Criterion (BIC) value and the  change in them between adjacent numbers of clusters (Table 2).  The clustering criterion (in this case the BIC) is computed for  each potential number of clusters. Smaller values of the BIC  indicate better models. The improvement in the cluster solution,  as measured by the BIC Change, is not worth the increased  complexity of the cluster model, as measured by the number of  clusters. The ratios of BIC change for the four cluster model is  small, while a three cluster model shows clear distinct patterns.  Table 3 provides insight into the distribution of the three cluster  solution based on the regulation strategies of the students. For  each cluster the means are reflected as well as the means for the  entire population.   Table 3 shows some distinct patterns in the ways students regulate  their learning. Students in cluster 1 show no dominant regulation  pattern, indicating that these students have no clear pattern to  regulate their learning.    Table 2: BIC changes in de auto-clustering procedure.  Number of   Clusters  Schwarz's  Bayesian   Criterion (BIC)   BIC  Changea   Ratio of BIC  Changesb   1 725.80    2 643.95 -81.85 1.00  3 576.39 -76.57 .825  4 565.05 -11.35 .139  5 555.89 -9.46 .116  6 554.90 -.69 .008  7 557.10 2.21 -.03   a. The changes are from the previous number of clusters in the table.  b. The ratios of changes are relative to the change for the two-cluster  solution.   Table 3: Distribution of regulation strategies for three clusters  Cluster number 1 2 3 All   N 128 95 110 333  Self-regulation  21.72 25.36 33.13 26.53   External Regulation 30.99 37.48 35.86 34.45  Lack of Regulation 12.96 19.09 12.71 14.63   Students in cluster 2 use a combination of two regulation  strategies: lack of regulation and external regulation. They seek  guidance in the learning process from external sources but when  this external regulation fails, for example by absence of the  instructor or unclear learning objects, they tend to show a lack of  regulation.   Also cluster 3 shows a combination of two regulation strategies.  They try to self-regulate their learning but when they fail they use  an external source to in order to compensate for this.   Cluster 3 students are mainly able to self regulate the learning  process, but when they fail to do, they use an external regulation  strategy to compensate for this deficiency.   Cluster analysis indeed revealed some distinct patterns in the  ways student regulate their learning with a group showing a  tendency to use an external source to regulate their learning, a  group who is mainly able to self regulate the learning process and  a group showing no distinct preference in how they regulate their  learning.    Different usage patterns  Next a MANOVA determined if there were any significant  differences between the three different clusters and the use of  different (digital) learning resources: lecture attendance, recorded  lectures, hits in Blackboard, Blackboard duration and average  score on formative assessments.   Table 4 shows the means of the use of learning recourses for each  cluster. Cluster 2 students, mainly characterized by external  regulation, show a greater use of the different learning resources.  However, the differences in the use of the learning resources  between the three clusters are not significant with F(2,330) =  1.971, p = .141 for lecture attendance, F(2,330) = .046, p = .995  for recorded lectures, F(2,330) = 1.247, p = .289 for hits in  Blackboard, F(2,330) = 3.206, p = .042 for Blackboard duration  and F(2,330) = .279, p = .757 for the average score on the  formative assessments.     Table 4: Means of the use of learning recourses for the three clusters    Lecture   attendance  Recorded  Lectures   Hits in  Blackboard   Blackboard use  (Minutes)   Formative  Assessments   (average score)   N M SD M SD M SD M SD M SD  Cluster 1 128 4.75 4.02 451.21 468.03 433.16 116.90 638.92 408.17 4.79 1.62  Cluster 2 95 5.54 3.97 467.12 416.71 452.54 142.89 642.14 426.46 5.30 1.27  Cluster 3 110 4.45 4.00 449.93 448.99 422.78 149.48 517.55 416.30 3.42 1.10   Table 5: Model summary for stepwise regression   R R2 Std. Error of the Estimate  Cluster 1 2 3 1 2 3 1 2 3  Model 1a. .157 .283 .365 .025 .080 .133 1.5135 1.5147 1.7196  Model 2b. .426 .395 .497 .181 .156 .247 1.3920 1.4588 1.6107  Model 3c. .672 .482 .710 .452 .232 .504 1.1438 1.3991 1.3139  Model 4d. .677 .490 .710 .458 .240 .504 1.1417 1.3996 1.3201  Model 5e. .680 .495 .721 .463 .245 .520 1.1412 1.4026 1.3039   a. Predictors: (Constant), total face-to-face lectures  b. Predictors: (Constant), total face-to-face lectures, minutes recorded lectures  c. Predictors: (Constant), total face-to-face lectures, minutes recorded lectures, average score formative assessment  d. Predictors: (Constant), total face-to-face lectures, minutes recorded lectures, average score formative assessment, activity BB in minutes  e. Predictors: (Constant), total face-to-face lectures, minutes recorded lectures, average score formative assessment, activity BB in minutes, hits in BB  Although the cluster analysis revealed that there are distinct  differences in the students their regulation strategies, these  differences in regulation have no significant impact on the use of  the different learning resources throughout the course.    4.1.2 Stepwise multi regression analysis   The next step in the analysis was to determine the relative  contribution of each of the different learning resources on course  performance. Since there were no clear indicators that one  learning resource was likely to be of more value than another  learning resource, a stepwise regression was used to find the set  of different learning resources for each cluster. The summary of  the stepwise regression with all the different learning resources  for each cluster are shown in Table 5. SPSS output in Table 5  shows that lecture attendance was entered first in the regression  analysis, explaining only 2,5% of the variance for cluster 1  students, 8% for cluster 2 students and up to 13,3% for cluster 3  students. For all clusters the activity in Blackboard in minutes  and hits in Blackboard were not significant. All other variables  were significant at the 0.05 level. Hence we use model 3 in our  discussion of the results.  The overall models differ in their explained variance: 45,2% for  cluster 1, 23,2% for cluster 2 and 50,4% for students in cluster 3  students. Remember that cluster 2 students mainly used an  external regulation strategy and shows the lowest amount of  variance explained caused by the use of the different learning  resources. Cluster 3 students are students who try to self- regulate their learning process and this model explains the most  of the variance in the use of the different learning resources. The  difference in explained variance between the two groups of  students is around 27%.   Students in cluster 1 and cluster 3 benefit the most from  formative assessments. Students in cluster 2 benefits mostly  from face-to-face lectures. Cluster 1 students also benefit from  recorded lectures, while cluster 3 students benefit more from  attending lectures. So besides differences in the explained  variance for each subgroup, there are also differences in the   types of learning recourses that has an added value for each  cluster.    4.1.3 Course performance  The last step in the data analysis was to perform an ANOVA  with cluster membership as a factor and with the final  assessment as the dependent variable, to determine if differences  in regulation strategies reflect in significant differences in course  performance. A GT2 Hochberg performed the post-hoc analysis  since the clusters differ in size.  There were no significant differences between groups for course  performance as determined by the ANOVA (F(2,330) = 1.018, p  = .363).    5. DISCUSSION  The current study aims to provide insight into differences in how  students regulate their learning process and how differences in  regulation of the learning process have an impact on the use of  (digital) learning resources and subsequently contribute to  course performance.   A cluster analysis showed three distinct patterns in the way  students regulate their learning. One third of the students are  mainly able to self-regulate their own learning process; one third  of the students use an external source to regulate learning; one  third of the students have no clear pattern when regulating their  learning process; they switch between self-regulation, external  regulation and lack of regulation during the learning process.   However, these differences in regulation strategies are not  reflected in differences in the use of (digital) learning resources.  Cluster 2 has a greater use of the different learning resources,  but these differences are not significant. That students with  different regulation strategies do not use the learning resources  differently is confirmed by [36] where they found that both  groups of students used the online learning resources to the same  extent. Nonetheless, they found that differences in regulation  strategies are reflected in course performance, when a high score  on self-regulation correlates negatively with course     performance. This finding indicates that the structure of the  course is beneficiary for students who report low self-regulated  learning, but is a disadvantage for students who report high self-  regulated learning. Although expected that students with better  self-regulation strategies, would perform better in the current  course, literature shows that student often ineffective self  regulation to do so [3]. Moreover, students believe that an  ineffective strategy is a good strategy, which itself may lead to  poor self-regulation although reported otherwise [7].   Current results are confirmed by [27] who found two usage  patterns in the use of learning resources: incoherent and  intensive users. These two groups of users, however, did not  show significant differences in their use of the learning  resources. The current research finds the same pattern; although  frequency and duration of use does not differ between self- regulated and externally regulated students, there are differences  in how this use impacts course performance. For students with  an external regulation strategy, 23% of the variability in course  performance is due to the use of the different learning resources,  while for self-regulated students this variability is 50%. These  differences in explained variance could be caused by the  expertise reversal effect [21]. The expertise reversal effect is a  cognitive load framework that states that instructional  techniques that are effective with inexperienced learners can  lose their effectiveness when used by more experienced learners.  A similar effect also will be obtained if novices must attempt to  process very complex material, which will benefit the  experienced learners. The students who report high self-  regulated learning benefit more from the offered learning  resources. This finding implies that not only duration of use has  an impact on course performance but also the reported  regulation strategy has an impact on the effectiveness of the  learning resources.   This finding has two implications for learning analytics. First the  contextualization of learning data with a broader set of  indicators [9, 16, 26, 44] is crucial in establishing the impact of  the learning data analysis since these conditions affect the  learning process. The effect of internal conditions have also  been stressed by [16] and current research shows that the use of  the same learning resources to the same extent have different  impacts on different groups of students. Second, although all  clicks are equal, some clicks are more equal than others. Current  learning analytics visualization trends use dashboards to mirror a  student their activity with the class average. However, this class  average is not as straightforward as previously assumed.   Besides differences in variability in course performance between  clusters, we also established differences in the use of learning  resources within each cluster. Cluster 1 and 3 show the most  explained variance of the use of the different learning resources,  however their composition of the variance is different. First, the  similarity lies in the explained variance of the formative  assessments, which is the highest for both clusters: 27% for  cluster 1 students and 25,7% for cluster 3 students. These results  are in line with [30] who found three predictive variables for  course performance: number of forum postings, mail messages  sent and assessments completed. This relative high variance is in  line with the constructive alignment [6] of the formative  assessments since they directly address the learning outcomes of  the course and thereby reflect the level of the summative  assessment. Next, the difference in the composition of the  explained variance becomes clear when cluster 3 students  benefit more from attending face-to-face lectures, while cluster 1  students benefit more from watching recordings of these lectures   online (15,5%) although students from both clusters use the  learning resources to the same extent.   Regulation strategies thereby do not account for the previously  reported differences in the use of digital learning resources by  students [8, 20, 27, 28, 29] but does account for differences in  effect of that use.   This research confirms, once again, the low predictive value  LMS use has on course performance [17, 30, 35, 47]. For the  three clusters, frequency and duration of LMS use were not  significant in contributing to course performance. Since most  mirroring techniques often use duration of LMS or frequency of  logons to mirror student behavior, this is one more argument that  this choice of mirroring should be examined critically. We found  no significant relation between the amount of logons to the LMS  and external regulated students. This finding is in contrast with  previous research [11] wherein higher logons to the LMS were  associated with external regulation of the learning process. The  type of content the LMS offers during a course could explain  this contradictory finding. In the current research, LMS content  mainly consists of learning resources associated with learning  activities and did not contain any resource that could be  beneficiary for externally regulated learners, such as teacher- student interactions or a course catalog. When the LMS provides  more information that supports external regulated students, like  course content, it would elicit students to log to the LMS more,  which could account for these differences. Once again, this  shows that course design has an impact on predictive modeling  techniques [17].  Surprisingly no significant differences were found between the  three different clusters and course performance. This result is  similar to [36], who found that self-regulation score correlates  negatively with course performance, indicating that students  who are able to self regulate their learning in general perform  less well than students with less ability to self regulate their  learning.    5.1 Implications for educational design  With modeling student behavior or predicting course  performance to identify students who are at risk of failing the  course, the focus is often on the choices of the instructor for  course design and the choice for certain learning resources [16].  Current research shows that these choices of the instructor have  a different impact on course performance for different groups of  students. If instructors become more aware of these differences  in learning approaches, they can effectuate a shift from blended  teaching towards blended learning that is student orientated [24].  The current notion for blended learning is mostly aimed at  putting technology into the learning environment without taking  into account how that technology contributes to the learning  outcomes [36] and supports individual differences [21].   The current research shows that not all students are able to self  regulate their learning process. This lack of ability reflects in  their ineffective use of the different learning resources, which  eventually does not lead to a high quality of learning. As this  and previous research shows [7, 8, 20, 27] students are not  capable of making suitable choices with regard to their learning  process and understand the value that certain learning resources  have with respect to certain learning outcomes of the course.  The use of pedagogical learning analytics interventions for  students [45] in which learning analytics elicits students to  become self-regulated learners and are made aware of the  pedagogical intentions of the learning resources would benefit  the quality of the learning process. Accurate monitoring of     learning is a crucial component of effective self-regulation of  learning. In this regard teachers need to pay particular attention  to the moments when students are analyzing the task and  designing plans before engaging in learning. Learning analytics  could foster these moments and enable self-regulated learning.   5.2 Limitations of the current research  The current course has a straightforward structure, in which  students are offered guidance by means of formative  assessments, the digital exercise book and online recordings of  face-to-face lectures. The current course design is supporting  students with an external regulation strategy. Previous research  reports that students with a self-regulation strategy will not  benefit from such a course design. Moreover the current course  has a short duration, the time pressure is significant and students  often find the subject matter hard which seldom gives students  the opportunity to read additional literature or to do more that  they are expected to do in a course. These attributes are  characteristic for students who are able to self regulate their  learning. The current course design forces students to use an  external regulation strategy although some students are able to  self regulate their learning. This causes a relative homogeneous  set of learning strategies. Nevertheless, even with this caveat,  the impact of the use of various learning resources clearly  depends on how students regulate their learning.  The current research uses clicks on links and duration of use as a  reflection of student effort, student engagement and participation  [47]. With research into online learning it is always debatable  whether these clicks and hits actually reflect use of digital  resources or that a students clicks on a link and walks away and  hence influencing the time on task measure. However, the time  on task measure is debatable for all study activities, not only for  the use of digital learning resources, since study sessions, and  even attending class, can include e-mail, online shopping and so  on [7].   Another limitation of the current research is the use of the ILS,  since better instruments are available to measure regulation  strategies. For example, in a follow up study the authors used the  MSLQ, which is more in line with current approaches of  assessing self-regulated learning [4, 5, 7, 40]. The reason for the  current use of the ILS was rather straightforward; there is a long  tradition within the faculty of psychology to administer the ILS  to all freshmen (see for example (Busato, Prins, Elshout, &  Hamaker, 1998; 2000) and is even embedded within the  curriculum resulting in an 100% response rate.   The last limitation of the current study is the known calibration  and inaccuracy problems with self-reports about study tactics  [43]. As previously mentioned, students often consider  themselves as self-regulated learners while the tactics they use to  regulate their learning are ineffective. Moreover, even within a  single course these self-reports about regulation of learning  differ as a function of the task before them (multiple choice  exam versus writing a paper) [44].   5.3 Recommendations for future research  This research showed that regulation strategies do not a have a  direct impact on the use of (digital) learning resources.  Nevertheless, it showed that differences in regulation strategies  do have an effect on the explained variance for the learning  resources in relation to course performance. The differences in  explained variance could be caused by the expertise reversal  effect. However, differences in explained variance could also be  caused by the sequence in which students use the different   learning resources. Current educational research, and especially  learning analytics research, hardly ever considers sequences of  the used learning resources as an important factor for course  performance. Temporal analysis, in which methods as sequential  pattern mining are being used, could establish if sequence of the  used learning resources does account for differences between the  actual use of the learning resources and differences in explained  variance currently reported.   6. CONCLUSION  In this research we examined if regulation strategies could  account for previously reported differences in the use of learning  resources. We examined how 333 psychology students regulated  their learning process and subsequently used the different  learning resources throughout the course and established how  this use of the learning resources contributed to course  performance.   The results indicate that differences in regulation strategies do  not account for differences in the use of (digital) learning  resources. However, different regulation strategies do have an  impact on the explained variance the different learning resources  have on course performance meaning that some learning  resources are more effective for some groups of students than  others. Students with an external regulation strategy have the  lowest explained variance on the use of learning resources in  relation to course performance.    This study has several consequences for future practices of  learning analytics and especially mirroring techniques. First, it  gives recognition to the importance of contextualization of the  learning data resources with a broader set of indicators to  understand the learning process. Moreover, this research shows  that mirroring students learning progress based on class average  does not account for differences in impact that use has on course  performance. Lastly, with our focus on differences between  students, we strive for a shift of identifying at risk students  towards a contribution of learning analytics in the educational  design process and enhance the quality of learning; for all  students.    7. ACKNOWLEDGMENTS  The authors would like to thank Bren Meijer for his assistance in  processing the data. Moreover the authors would like to thank  Alan Berg for his general assistance at the Learning Analytics  program at the University of Amsterdam.    REFERENCES  [1] Agudo-Peregrina, . F., Iglesias-Pradas, S., Conde-  Gonzlez, M. ., & Hernndez-Garca, . (2014). Can we  predict success from log data in VLEs Classification of  interactions for learning analytics and their relation with  performance in VLE-supported F2F and online learning.  Computers in human behavior, 31, 542-550.  doi:10.1016/j.chb.2013.05.031   [2] Azevedo, R. (2005). Using hypermedia as a metacognitive  tool for enhancing student learning The role of self- regulated learning. Educational Psychologist, 40(4), 199- 209. doi:10.1207/s15326985ep4004_2   [3] Azevedo, R., Moos, D. C., Greene, J. A., Winters, F. I., &  Cromley, J. G. (2008). Why is externally-facilitated  regulated learning more effective than self-regulated  learning with hypermedia Educational Technology     Research and Development, 56(1), 45-72.  doi:10.1007/s11423-007-9067-0   [4] Beheshitha, S. S., Gaevi, D., & Hatala, M. (2015). A  process mining approach to linking the study of aptitude  and event facets of self-regulated learning. Proceedings of  the Fifth International Conference on Learning Analytics  And Knowledge (pp. 265-269). ACM.  doi:10.1145/2723576.2723628   [5] Biggs, J., Kember, D., & Leung, D. Y. (2001). The revised  two-factor study process questionnaire: R-SPQ-2F. British  Journal of Educational Psychology, 71(1), 133-149.  doi:10.1348/000709901158433   [6] Biggs, J. & Tang, C. (2011). Teaching for quality learning  at university. Maidenhead, UK: McGraw-Hill Education.   [7] Bjork, R. A., Dunlosky, J., & Kornell, N. (2013). Self- regulated learning: Beliefs, techniques, and illusions.  Annual Review of Psychology, 64, 417-444.  doi:10.1146/annurev-psych-113011-143823   [8] Bos, N., Groeneveld, C., Van Bruggen, J., & Brand- Gruwel, S. (2015). The use of recorded lectures in  education and the impact on lecture attendance and exam  performance. British Journal of Educational Technology.  doi:10.1111/bjet.12300   [9] Buckingham Shum, S., & Deakin Crick, R. (2012).  Learning dispositions and transferable competencies:  pedagogy, modelling and learning analytics. Proceedings of  the 2nd International Conference on Learning Analytics  and Knowledge (pp. 92-101). ACM.  doi:10.1145/2330601.2330629   [10] Chiu, T., Fang, D., Chen, J., Wang, Y., & Jeris, C. (2001).  A robust and scalable clustering algorithm for mixed type  attributes in large database environment. Proceedings of the  seventh ACM SIGKDD international conference on  knowledge discovery and data mining (pp. 263-268). ACM.  doi:10.1145/502512.502549   [11] Cho, M. H., & Shen, D. (2013). Self-regulation in online  learning. Distance Education, 34(3), 290-301.  doi:10.1080/01587919.2013.835770   [12] Dawson, S., Gaevi, D., Siemens, G., & Joksimovic, S.  (2014). Current state and future trends: A citation network  analysis of the learning analytics field. Proceedings of the  Fourth International Conference on Learning Analytics  And Knowledge (pp. 231-240). ACM.  doi:10.1145/2567574.2567585   [13] Drachsler, H., Stoyanov, S., & Specht, M. (2014). The  impact of learning analytics on the dutch education system.  Proceedings of the Fourth International Conference on  Learning Analytics And Knowledge (pp. 158-162). ACM.  doi:10.1145/2567574.2567617   [14] Ellis, R., Goodyear, P., Calvo, R., Prosser, M. (2008).  Engineering students' conceptions of and approaches to  learning through discussions in face-to-face and online  contexts. Learning and Instruction, 18(3), 267-282.  doi:10.1016/j.learninstruc.2007.06.001   [15] Entwistle, N., & McCune, V. (2013). The disposition to  understand for oneself at university: Integrating learning  processes with motivation and metacognition. British  Journal of Educational Psychology, 83(2), 267-279.  doi:10.1111/bjep.12010   [16] Gaevi, D., Dawson, S., & Siemens, G. (2015). Lets not  forget: Learning analytics are about learning. TechTrends,  59(1), 64-71. doi:10.1007/s11528-014-0822-x   [17] Gaevi, D., Dawson, S., Rogers, T., & Gasevic, D. (2015).  Learning analytics should not promote one size fits all: The  effects of instructional conditions in predicating academic  success. The Internet and Higher Education.  doi:10.1016/j.iheduc.2015.10.002   [18] George-Walker, L.D., & Keeffe, M. (2010). Self- determined blended learning: a case study of blended  learning design. Higher Education Research &  Development, 29(1), 1-13.  doi:10.1080/07294360903277380   [19] Gorissen, P., Van Bruggen, J., & Jochems, W. (2012).  Usage reporting on recorded lectures using educational data  mining. International Journal of Learning Technology,  7(1), 23-40. doi:10.1504/ijlt.2012.046864   [20] Inglis, M., Palipana, A., Trenholm, S., & Ward, J. (2011).  Individual differences in students use of optional learning  resources. Journal of Computer Assisted Learning, 27(6),  490-502. doi: 10.1111/j.1365-2729.2011.00417.x   [21] Kalyuga, S., Ayres, P., Chandler, P., & Sweller, J. (2003).  The expertise reversal effect. Educational psychologist,  38(1), 23-31. doi:10.1207/s15326985ep3801_4   [22] Kovanovi, V., Gaevi, D., Joksimovi, S., Hatala, M., &  Adesope, O. (2015). Analytics of communities of inquiry:  Effects of learning technology use on cognitive presence in  asynchronous online discussions. The Internet and Higher  Education, 27, 74-89. doi:10.1016/j.iheduc.2015.06.002   [23] Kovanovi, V., Gaevi, D., Dawson, S., Joksimovi, S.,  Baker, R. S., & Hatala, M. (2015). Does Time-on-task  Estimation Matter Implications on Validity of Learning  Analytics Findings. Journal of Learning Analytics, 2(3)(in  press), http://vitomir.kovanovic.info/download=474   [24] Kupetz, R., & Ziegenmeyer, B. (2006). Flexible learning  activities fostering autonomy in teaching training. ReCALL,  18(01), 63-82. doi:10.1017/s0958344006000516   [25] Lockyer, L., Heathcote, E., & Dawson, S. (2013).  Informing pedagogical action: Aligning learning analytics  with learning design. American Behavioral Scientist,  57(10), 1439-1459. doi:10.1177/0002764213479367   [26] Lodge, J., & Lewis, M. (2012). Pigeon pecks and mouse  clicks: Putting the learning back into learning analytics.  Future challenges, sustainable futures. Proceedings ascilite  Wellington, 560-564.   [27] Lust, G., Vandewaetere, M., Ceulemans, E., Elen, J., &  Clarebout, G. (2011). Tool-use in a blended undergraduate  course: In Search of user profiles. Computers & Education,  57(3), 2135-2144. doi:10.1016/j.compedu.2011.05.010   [28] Lust, G., Elen, J., & Clarebout, G. (2013). Students tool- use within a web enhanced course: Explanatory  mechanisms of students tool-use pattern. Computers in  Human Behavior, 29(5). doi:10.1016/j.chb.2013.03.014   [29] Lust, G., Elen, J., & Clarebout, G. (2013). Regulation of  tool-use within a blended course: Student differences and  performance effects. Computers & Education, 60(1), 385- 395. doi:10.1016/j.compedu.2012.09.001   http://vitomir.kovanovic.info/download=474   [30] Macfadyen, L. P., & Dawson, S. (2010). Mining LMS data  to develop an early warning system for educators: A proof  of concept. Computers & Education, 54(2), 588-599.  doi:10.1016/j.compedu.2009.09.008   [31] Orton-Johnson, K (2009). Ive stuck to the path Im afraid:  exploring student non-use of blended Learning. British  Journal of Educational Technology, 40(5), 837-847.  doi:10.1111/j.1467-8535.2008.00860.x   [32] Orwell, G. (1954). Animal Farm: A Fairy Story. London:  Secker and Warburg.    [33] Perry, N. E., Phillips, L., & Hutchinson, L. (2006).  Mentoring Student Teachers to Support SelfRegulated  Learning. The elementary school journal, 106(3), 237-254.  doi:10.1086/501485   [34] Pintrich, P. R. (2000). Multiple goals, multiple pathways:  The role of goal orientation in learning and achievement.  Journal of educational psychology,92(3), 544.  doi:10.1037/0022-0663.92.3.544   [35] Tempelaar, D. T., Rienties, B., & Giesbers, B. (2015). In  search for the most informative data for feedback  generation: Learning Analytics in a data-rich context.  Computers in Human Behavior, 47, 157-167.  doi:10.1016/j.chb.2014.05.038   [36] Tempelaar, D., & Rienties, B. (2008). Explaining student  learning preferences in a blended learning environment for  learning statistics on the basis of student characteristics.  Student Mobility and ICT: Can ICT overcome the barriers  to Life-long learning, 51-60.   [37] Verbert, K., Drachsler, H., Manouselis, N., Wolpers, M.,  Vuorikari, R., & Duval, E. (2011). Dataset-driven research  for improving recommender systems for learning.  Proceedings of the 1st International Conference on  Learning Analytics and Knowledge (pp. 44-53). ACM.  doi:10.1145/2090116.2090122   [38] Verkroost, M. J., Meijerink, L., Lintsen, H., & Veen, W.  (2008). Finding a balance in dimensions of blended  learning. International Journal on E-learning,7(3), 499- 522.                                  [39] Vermunt, J. D. H. M. (1992) Leerstijlen en sturen van  leerprocessen in het hoger onderwijs: naar procesgerichte  instructie in zelfstanding denken [Learning styles and  regulation of learning in higher education: towards  process-oriented instruction in autonomous thinking]  (Amsterdam, Lisse: Swets & Zeitlinger).    [40] Watkins, D. (2001). Correlates of approaches to learning: a  cross-cultural meta-analysis. In: R. J. Sternberg & L. F.  Zhang (Eds.), Perspective on thinking, learning, and  cognitive styles (p. 165-195). Mahwah, NJ, Lawrence  Erlbaum Associates.   [41] Wiese, C., & Newton, G. (2013). Use of Lecture Capture in  Undergraduate Biological Science Education. Canadian  Journal for the Scholarship of Teaching and Learning,  4(2), 4. doi:10.5206/cjsotl-rcacea.2013.2.4   [42] Winne, P.H. & Perry, N.E. (2000). Measuring self- regulated learning. In P. Pintrich, M. Boekaerts, & M.  Seidner (Eds.), Handbook of self-regulation (p. 531-566).  Orlando, FL: Academic Press.   [43] Winne, P. H., & Jamieson-Noel, D. (2002). Exploring  students calibration of self reports about study tactics and  achievement. Contemporary Educational Psychology,  27(4), 551-572. doi:10.1016/s0361-476x(02)00006-1   [44] Winne, P. H. (2006). How software technologies can  improve research on learning and bolster school reform.  Educational Psychologist, 41(1), 5-17.  doi:10.1207/s15326985ep4101_3   [45] Wise, A. F. (2014). Designing pedagogical interventions to  support student use of learning analytics. Proceedings of  the Fourth International Conference on Learning Analytics  And Knowledge (pp. 203-211). ACM.  doi:10.1145/2567574.2567588   [46] Woolfolk, A., Walkup, V., & Hughes, M. (2008).  Psychology in education. Harlow, UK: Pearson-Longman.   [47] Zacharis, N. Z. (2015). A multivariate approach to  predicting student outcomes in web-enabled blended  learning courses. The Internet and Higher Education,27,  44-53. doi:10.1016/j.iheduc.2015.05.002     "}
{"index":{"_id":"43"}}
{"datatype":"inproceedings","key":"David:2016:SEC:2883851.2883885","author":"David, Yossi Ben and Segal, Avi and Gal, Ya'akov (Kobi)","title":"Sequencing Educational Content in Classrooms Using Bayesian Knowledge Tracing","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"354--363","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883885","doi":"10.1145/2883851.2883885","acmid":"2883885","publisher":"ACM","address":"New York, NY, USA","abstract":"Despite the prevalence of e-learning systems in schools, most of today's systems do not personalize educational data to the individual needs of each student. This paper proposes a new algorithm for sequencing questions to students that is empirically shown to lead to better performance and engagement in real schools when compared to a baseline approach. It is based on using knowledge tracing to model students' skill acquisition over time, and to select questions that advance the student's learning within the range of the student's capabilities, as determined by the model. The algorithm is based on a Bayesian Knowledge Tracing (BKT) model that incorporates partial credit scores, reasoning about multiple attempts to solve problems, and integrating item difficulty. This model is shown to outperform other BKT models that do not reason about (or reason about some but not all) of these features. The model was incorporated into a sequencing algorithm and deployed in two classes in different schools where it was compared to a baseline sequencing algorithm that was designed by pedagogical experts. In both classes, students using the BKT sequencing approach solved more difficult questions and attributed higher performance than did students who used the expert-based approach. Students were also more engaged using the BKT approach, as determined by their interaction time and number of log-ins to the system, as well as their reported opinion. We expect our approach to inform the design of better methods for sequencing and personalizing educational content to students that will meet their individual learning needs.","pdf":"Sequencing Educational Content in Classrooms using  Bayesian Knowledge Tracing  Yossi Ben David  Department of Information  Systems Engineering  Ben-Gurion University, Israel  bendavidyossi@gmail.com  Avi Segal  Department of Information  Systems Engineering  Ben-Gurion University, Israel  avise@post.bgu.ac.il  Yaakov (Kobi) Gal  Department of Information  Systems Engineering  Ben-Gurion University, Israel  kobig@bgu.ac.il  ABSTRACT Despite the prevalence of e-learning systems in schools, most of todays systems do not personalize educational data to the individual needs of each student. This paper proposes a new algorithm for sequencing questions to students that is empir- ically shown to lead to better performance and engagement in real schools when compared to a baseline approach. It is based on using knowledge tracing to model students skill acquisition over time, and to select questions that advance the students learning within the range of the students ca- pabilities, as determined by the model. The algorithm is based on a Bayesian Knowledge Tracing (BKT) model that incorporates partial credit scores, reasoning about multiple attempts to solve problems, and integrating item diculty. This model is shown to outperform other BKT models that do not reason about (or reason about some but not all) of these features. The model was incorporated into a sequenc- ing algorithm and deployed in two classes in dierent schools where it was compared to a baseline sequencing algorithm that was designed by pedagogical experts. In both classes, students using the BKT sequencing approach solved more dicult questions and attributed higher performance than did students who used the expert-based approach. Students were also more engaged using the BKT approach, as deter- mined by their interaction time and number of log-ins to the system, as well as their reported opinion. We expect our approach to inform the design of better methods for se- quencing and personalizing educational content to students that will meet their individual learning needs.  1. INTRODUCTION The proliferation of e-learning systems in schools means  that educational software is increasingly used by a wide ar- ray of learners from dierent age groups, socio-economic backgrounds and cultures. This creates new opportunities for using computational methods to support students in their learning process.  We focus on e-learning systems that are deployed in K12  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. LAK 16 April 25-29, 2016, Edinburgh, United Kingdom  ACM ISBN 978-1-4503-2782-4/14/04 ...$15.00. http://dx.doi.org/10.1145/2883851.2883885. DOI: 10.1145/1235  schools, consisting of large repositories of questions in a vari- ety of subjects (e.g., math, English, physics). Students can solve exercises and practice their skills in tests. Teachers can prepare assignments to the entire class or to a group of students, assess their students answers and follow their progress using graphs and reports. In contrast to intelligent tutoring systems, in which hints and content selection are adapted to the students needs, there is no o-the-shelf sequencing mechanism that is available in e-learning sys- tems. For the most part, these systems require teachers to select the questions for students to solve, which demands time and eort, or allow students free choice about which questions to solve, which may result in students picking very easy questions that do not advance their learning. This pa- per describes a general approach for augmenting e-learning systems that are deployed in schools with automatic tools for sequencing educational content to students.  Our approach follows the mastery learning paradigm [3] in which domain knowledge is divided into a hierarchy of component skills, students learn skills at their own pace, and knowledge of simple skills should be demonstrated be- fore moving on to more dicult questions relating to more complex skills. We model the acquisition of students knowl- edge of these skills over time using a computational model, and use this model to personalize educational content to students.  There is significant work on computational models for tracing students knowledge to determine when skills have been learned. A family of methods called Bayesian Knowl- edge Tracing (BKT) use probabilistic methods and machine learning to model students skill acquisition in Intelligent Tutoring Systems. Most uses of the BKT model focus on predicting individual student performance on a given set of problems, or selecting the set of questions that are deemed most suitable for the students inferred skill level. We fo- cus on a dierent problem: which question to select next that will best advance the students knowledge given the students inferred skill level.  The basic BKT model includes inherent assumptions that limit its applicability in e-learning systems: it assumes that question answers are dichotomous (correct or incorrect), it does not reason about question diculty and it does not consider the general case where students perform several at- tempts to solve the same given problem. Extensions to the basic BKT model have been proposed separately, but in- tegrating them together poses additional challenges to the model representation and parameter learning. We present a model-selection approach which receives as input a stu-    dents response and a set of candidate BKT models, and chooses the best performing BKT model from the training set to use for predicting the students performance on the question. Our extension to the BKT model is shown to lead to significant improvement from the state of the art BKT models, when compared on two e-learning datasets in mathematics from the literature, that contain millions of responses to questions.  The model described above was integrated into a sequenc- ing algorithm that selects questions to students in a way that is based on their inferred skill mastery. The algorithm re- ceives as input a set of questions for a student and a BKT model, and ranks the questions based on the predicted score given by the model as well as the students knowledge of the relevant skills. It selects the question that is intended to advance the students knowledge of the relevant skill, while matching the students capabilities, so that the ques- tion is within the zone of possible achievement for the stu- dent [21].  We compared the performance of students using this al- gorithm to that of students using a sequencing approach that was engineered by pedagogical experts and which se- lects questions in increasing order of diculty. We com- pared both approaches empirically in two dierent schools in Israel using the math component of an e-learning sys- tem that includes more than 10,000 questions divided into hundreds of topics. In each school, two classes were cho- sen in which students exhibited similar performance on a set of predetermined math questions. All students in a class subsequently used the BKT or the expert-based sequenc- ing approach for a period of several weeks, in which they were able to use the system at their leisure at home or in school. Our results showed that in both schools, students using the BKT-Sequencing algorithm solved more harder questions (e.g., higher diculty levels), performed signifi- cantly better on harder questions, and spent more time in the system than did the students using the expert-based ap- proach. In a follow-up survey that was administered in one of the schools, the BKT-Sequence algorithm was reported by students to be more helpful than the expert-based approach.  The contributions of this paper are threefold. First, it ex- tends the Bayesian Knowledge Tracing literature to reason about partial credit scores and number of question retries. Second, it allows to choose the best performing BKT model on new data using model selection. Third, it provides a new algorithm for sequencing questions to students in re- altime that was evaluated in two dierent schools, demon- strating that combining knowledge tracing models in exist- ing e-learning systems can improve students performance.  2. BACKGROUND AND RELATED WORK Our work is based on computational models for tracing  students knowledge in e-learning, called Bayesian Knowl- edge Tracing (BKT). BKT was originally proposed by Cor- bett and Anderson [6, 7] to model students evolving knowl- edge of skills required to solve problems in an intelligent tu- toring system for teaching LISP programming. BKT mod- els students knowledge as a set of binary variables, one per skill, where the skill is either mastered by the student or not. Future implementations of BKT used a Dynamic Bayesian Network to maintain a probability distribution over knowl- edge of each skill. Observations in the BKT model consist of the correctness of students responses to questions which  Figure 1: Basic Knowledge Tracing Model  are binary: a student can get a score of 0 or 1 on each problem/step. It updates the distribution over time by ob- serving the correctness of students responses when applying the relevant skill to a question.  The basic knowledge tracing model is shown in Figure 1. This model is defined by the following four parameters: the probability that a skill L is known prior to answering the first question of this skill, denoted P (L0); the probability the skill will be learned at each opportunity to use the skill, denoted P (T ); the probability the student will guess cor- rectly if the skill is not known, denoted P (G); and the prob- ability of answering the question incorrectly despite knowl- edge of the skill (slipping), denoted P (S). The variable Q is the observed response to a question by a student. Each of the variables has binary values, true or false. Based on these parameters, inference can be made about the students knowledge at the nth opportunity to apply a skill, which is denoted P (Ln). Two basic assumptions of this model are that 1) a student is scored dichotomously (true or false) on each time; 2) a student can be in a known or unknown state for each skill, and that once a skill is known it is not forgot- ten.  Using the parameters defined above, predicting whether a students response is correct at time opportunity n is com- puted using the law of total probability as follows:  P (correctn) = P (Ln)  (1P (S))+ (1P (Ln)) P (G) (1)  Similarly, the probability that a response is incorrect at time n is computed below:  P (incorrectn) = P (Ln)P (S)+(1P (Ln))(1P (G)) (2)  There are two distinct stages to using the knowledge trac- ing model. The first stage is to fit the model parameters from data. The second stage is using the model to infer the students knowledge over time given the students re- sponses. Given an observation of the students response at time opportunity n (correct or incorrect) the probabil- ity P (Ln) that a student knows the skill is calculated using Bayes rule. When a correct response is observed, this prob- ability is as follows:  P (Ln | correctn) = P (Ln)  (1 P (S))  P (correctn) (3)  where P (correctn) is defined in Equation 1. When an incor-    rect response is observed, this probability is as follows:  P (Ln | incorrectn) = P (Ln)  P (S) P (incorrectn)  (4)  where P (incorrectn) is defined in Equation 2. Lastly, we show how the students knowledge of the skill is  updated given its interaction with the system. This estimate is the sum of two probabilities: the posterior probability that the student already knew the skill (contingent on the evidence), and the probability that the student did not know the skill, but was able to learn it.  P (Ln) =P (Ln1 | evidencen1)+ (1 P (Ln1 | evidencen1))  P (T )  (5)  Many extensions have been proposed to the BKT model over the years. We mention those most relevant to our work, in that they address challenges that arise in the classroom, namely modeling partial credit scores, adapting the model to handling retries and item diculty, and heuristics for fit- ting BKT parameters to data. We refer to the survey by Desmarais and Baker [10] for a detailed account of these methods. Wang and Heernan [20] extended the model to account for partial credit scores, by assigning continu- ous values to the question node in the model and learning Gaussian distributions over the guess and slip parameters with fixed standard deviations. Ostrow et al. [14] adapt the BKT model to handle partial credit score using a tabling method. They maintain a discrete probability distribution table over item correctness that depends on the partial credit scores attributed to the question and its item diculty.  Pardos and Heernan [17] introduced the KT-IDEMmodel which extends the basic BKT model to account for item di- culty. The model fits separate guess and slip parameters for each item in a skill, and the question node is conditioned on the item node in the network topology. This model has been shown to outperform the standard BKT model on the ASSISTment data set. In subsequent work, Pardos et al. [16] have extended the KT-IDEM model to account for the re- tries of each question. Using only the first or last response of a student to a question loses important information about how the student learns the skill. They included a count node representing the number of tries for each question, and conditioned the guess and slip parameters on the value of the count nodes. They showed this model, called Count- KT-IDEM, was able to improve performance on a homework data sets from a MOOC course in which multiple attempt behavior for questions was prevalent.  We now turn to the first stage of using the BKT model which is to learn the parameter values of the model from data. The most commonly used approach to fit parameters in the BKT literature uses the Expectation-Maximization (EM) [9] algorithm which is a maximum likelihood approach for training the model in the presence of missing data [15, 17, 16]. Using the EM algorithm for large-scale datasets carries a high computational cost and has been shown to lead to model degeneracy, characterized by extreme parameter values and identifiability issues [8].  An alternative for using EM for large-scale educational datasets includes the Empirical Probabilities approach by Hawkins and Heernan [11] which is based on a heuristic for estimating at which point the student learned the skill. The skill is assumed to be completely unknown when solv- ing all questions requiring the skill before that point, and  completely known when solving all questions beyond that point. A knowledge sequence for the student is the result- ing sequence of known and unknown states. This is in ac- cordance with the assumptions of the BKT model, where learned skills are never forgotten. The approach selects the knowledge sequence that best matches the observations con- sisting of an ordered sequence of the students responses to questions, called a performance sequence. Let Ci be the stu- dents performance for the question at time i (using value 0 for incorrect and value 1 for correct). Let K be a knowl- edge sequence and let Ki 2 K denote the state for the ques- tion at time i (using value 0 for unknown and value 1 for known). Let K be the optimal knowledge sequence that satisfies the following:  K = argmin K  X  i  |Ci Ki| (6)  For example, if the observations relating to a students per- formance on a sequence of questions are (correct, incor- rect, incorrect, correct, correct), then the most likely knowl- edge sequence that matches up the observation sequence is (unknown, unknown, unknown, known, known), since this knowledge sequence matches up with four of the five perfor- mances correctly, more than any other possible knowledge sequence does. The BKT parameters are assigned as follows:  P (L0) = X  i  K0 |K0|  (7)  P (T ) = i 6=0(1Ki1) Ki i 6=0(1Ki1)  (8)  P (G) =  P i Ci(1Ki)P i(1Ki)  (9)  P (S) =  P i(1 Ci)KiP  i Ki (10)  Our work also relates to computational models for se- quencing educational content to students. Shen et al. [19] used ontologies of competencies to recommend learning paths to students based on competency gap analysis. Huang et al. [13] used Markov chains to find popular sequences over learn- ing objects considering choices of many students and trying to minimize entropy between dierent learning objects in the sequence. Brusilovsky et al. [4] tracked user navigation and used user direct feedback to recommend paths of ad- ditional content to visitors of educational digital libraries. Clement et al. [5] used a Multi-Armed Bandit approach to suggest policies for intelligent tutoring systems which were evaluated in simulation.  Item Response Theory (IRT) [1] is a theory of education measurement that is used to select the most appropriate items for students based on individual ability, as inferred from their past interactions with the system. Unlike knowl- edge tracing, it does not seek to maintain a model of the students knowledge. IRT has been used extensively in ex- amination settings such as the SAT, and has recently been applied to on line settings such as Khan Academy.  Among the few works that consider diculty of questions or other learning objects, Bielikova et al. [2] proposed an Adaptive Learning Framework that recommends sequence of items based on content similarity and knowledge pre- requisites. Their field trial shows that using an adaptive system, where the learning object diculty is an aspect of    Figure 2: A translation to a question from the K9  system involving identifying triangles.  adaptation, delivers higher learning gain for students com- paring to manual recommendations. In their work however the learning object diculty is not personalized by student basis, as is the case in our work. In another work by Hsieh et al. [12], the authors used fuzzy logic theory to construct an appropriate learning path based on the learners miscon- ceptions found in a preceding quiz. This process however is not adaptive on an ongoing basis and requires a pre-test as an integral part of the mechanism, unlike our proposed solution.  3. ADAPTING BKT TO THE CLASSROOM This section describes how we augmented the existing  BKTmodels to capture students interactions with e-learning systems in the classroom. We begin by describing the K9 e- learning system, which constitutes a formidable part of our empirical analysis. K9 is deployed in over 120 schools in Israel and used by over 10,000 students in grades 1 through 9. It spans questions from diverse subjects, such as English as a foreign language, science education, and mathematics, which was chosen as the focus of our empirical investigation.  Each mathematics question in the K9 database is mapped to one of 256 dierent topics and sub-topics at increasing levels of specifity (e.g., arithmetic operators, addition, two- digit numbers, etc.), and each question is also labeled by a diculty level (1 being easiest; 5 being hardest) that was determined by pedagogical experts. Figure 2 shows one of the mathematics questions in the database that requires stu- dents to identify dierent types of triangles. This question was assigned a diculty level of 3.  We collected about 4 million student responses to over 10,000 unique questions in mathematics spanning four years of use in dierent schools. Each response is a tuple that includes a question, a unique (anonymized) identifier for the student answering the question, number of retries for this question, and the students score on the question, which is a positive number in the range of [0, 1].  There are several challenges to using BKT in existing e- learning applications in the classroom. First, the standard BKT model assumes an all-or-nothing score (whether or not the student answered a question correctly). We need to in- corporate into the representation that answers may be par- tially correct. We also need to account for the fact that the question diculty and the number of retries aect students skill acquisition for the question. The second challenge is that the computational complexity of the EM algorithm is  exponential in the number of variables, and its use in prac- tice is costly in terms of computation time. Both issues has been addressed separately in prior work [11, 20, 16], but as we show, integrating the approaches and using these ap- proaches in the classroom in not straightforward.  In our approach for reasoning about partial credit scores we treat the evidence (the students score for a question) as a weighting factor that determines the extent to which the students response was correct. It diers from Wang and Heernan [20] in that it does not rely on the EM algorithm, and diers from Ostrow et al. [14] in that it does not use predefined methods to determine the posterior distributions. For example, receiving a score of 66% for the question shown in Figure 2 means that 66% of the students response is correct, while 34% of the students response is incorrect. Hence we define the posterior probability P (Ln | evidencen) of learning a skill given the evidence as the sum of posterior probabilities of answering correctly and incorrectly, weighted by the students score for the question.  P (Ln | evidencen) =scoren  P (Ln | correctn)+ (1 scoren)  P (Ln | incorrectn)  (11)  where P (Ln | correctn) and P (Ln | incorrectn) are com- puted in Equations 3 and 4, respectively.  It can be shown that the term on the right-hand-side of this equation yields a number in the range [0, 1]. In partic- ular, if the students score is a perfect score of 0 or 1, then Equation 11 reduces to the one that is used in the classic BKT model. Thus the equation defines a probability distri- bution over Ln given the evidence at time n.  We addressed the parameter fitting challenge by adapting the Empirical Probabilities method [11] to handle contin- uous scores. Specifically, for each question i, we replaced the binary valued Ci term in the optimal knowledge se- quence definition of Equation 6 with scorei for each ques- tion i. (And similarly for computing the guess and slip pa- rameter values of Equations 9 and 10).1 For example, if a students performance sequence in K9 is (0.3, 0.8, 1, 0.6, 1) where 1 is the perfect score for that question, the accu- racy score for the knowledge sequence (unknown, known, known, known, known) and the students performance se- quence will be (0.3 + 0.2 + 0 + 0.4 + 0) = 0.9.  4. CANDIDATE MODELS We incorporated the partial credit score approach in sev-  eral BKT models from the literature which dier in their representation and number of parameters. We assigned a skill for each topic in the set of 256 topics in the K9 database.  The first model we considered was the basic BKT model shown in Figure 1 (denoted SIMPLE). There were two pa- rameters in this model (for the prior over L0 and T ) and a guess and slip parameter for each skill, totaling 4 parameters per skill.  We also considered several extensions by Pardos et al. [17] to the basic model. One extension assigned guess and slip parameters for dierent items (denoted KT-IDEM), in or- der to represent item diculty and other information that is embedded in the question itself. Although the questions  1We ran a pilot study in which we compared several variants of the EM algorithm (Baum-Welch and Gradient Descent) to using the EP heuristic to estimate the parameters for our candidate model.    in the K9 system were labeled by diculty level, individual students vary in their perceived diculty of questions. We assigned each item directly to a question, thus the number of parameters for this model was 2 + 2 #(num. questions) per skill. Another extension assigned separate parameters for dierent number of question retries (denoted COUNT). We discretized the number of retries into two separate cat- egories: one for all retries representing the first attempt to solve the problem, and one for more than the first at- tempt to solve the problem.2 The number of parameters for this model was 2 + 2  2 per skill. We also considered a model that assigned separate parameters for both dif- ferent number of questions and question retires (denoted KT-IDEM-COUNT), which was also discretized to two val- ues. The number parameters for this model was 2 + 2  2  #(num. questions) per skill.  Lastly, students from dierent ages and grades may dier in their performance for the same questions, so it makes sense to condition the model on the students grade level. Therefore we used a new model that assigns separate guess and slip parameters for dierent school grades (e.g., 4th or 5th grade). The number parameters for this model was 2 + 2  2 #(num. grades) #(num. questions) per skill.  For each of these models, we implemented two methods for dealing with partial credit. The first method (called Weight), was the approach using Equation 11 to compute the posterior probability of learning a skill given the evi- dence, and fitted parameters using a modification of Empir- ical Probabilities approach [11] that included Equations 9 and 10.  The second method for dealing with partial credit scores (called Threshold), reduced the students score to a bi- nary variable by assigning a correct response if the score was above a given threshold.  evidence =  ( correct if score  threshold, incorrect otherwise  (12)  This method used the original EP approach for parameter fitting and for computing the posterior probabilities over the skill P (Ln), as described by Hawkins et al. [11].  5. EMPIRICAL METHODOLOGY We evaluated each of the partial credit approaches in the  models described above on two domains. The first domain was the K9 dataset described in Section 3, while the second domain was the ASSISTment data set used by Ostrow et al. [14].  Table 1 shows the distribution of the questions over four years of use of the K9 system, including the number of dif- ferent questions, the number of students and the number of responses. Note that only partial information was collected for 2015, hence the smaller number of response instances compared to the other years.3  We set the prior parameter values for all models in a way that is similar to those set by other works in the BKT liter- ature. Specifically, the prior probability over skill was set to  2The number of categories was determined using a held-out set of data that was not used for testing purposes. 3The records in this dataset were anonymized and the study was approved by the institutional review board of Ben- Gurion university.  Year #(responses) #(students) #(questions) 2009 1,865,737 11,601 8,613 2011 1,438,702 14,349 8,367 2014 1,061,161 5,384 5,714 2015 448,222 5,384 5,714 Total 4,813,822 49,294 32,099  Table 1: K9 Database statistics  Figure 3: RMSE measures (y-axis) for the dierent  BKT models (top) and breakdown by year (bottom)  on K9 dataset  P (L0) = 0.5; the probability of acquiring a skill given feed- back was set to P (T ) = 0.3; the probability of guess and slip was set to P (G) = 0.2 and P (S) = 0.15 respectively. We used the same prior parameter values for all skills (topics).  We used the root mean square error (RMSE) for eval- uating the dierent models. RMSE is calculated for each item by comparing the predicted correctness (P (correcti)) of a response to the actual response, within the range [0, 1] for partial credit scores. RMSE has been shown to be the strongest performance indicator for BKT with significantly higher correlation than LL and AUC [18].  We begin with presenting the results from the K9 dataset. We employed a standard five-fold cross-validation technique which randomly split the data set into five folds, each in- cluding data from the four years of use of the system. Fig- ure 3 (top) shows the average RMSE value (y-axis) over five rounds in which dierent folds of the data served for train- ing and testing. For each of the models, we report results using the weight approach for modeling partial scores, as well as the approach using thresholds which were set to 1 and 0.75. All reported results were statistically significant in the p < 0.05 range using t-test measures.  As shown in the figure, for all BKT models, reasoning about partial credit scores with theWeighted method achieved    Figure 4: RMSE measures (y-axis) for the dierent  BKT models on ASSISTMENT dataset  better performance (lower RMSE scores) than both of the threshold approaches. Additionally, the best performance was obtained by the KT-IDEM-Count model. Figure 3 (down) breaks down the performance of the weighted partial credit approach for the dierent models for each year. As shown in the figure, all methods exhibited consistent performance when testing on dierent years.  The second domain was the ASSISTment dataset used by Ostrow et al. [14], which was compiled from problem logs from the ASSISTments platform during the 2012-2013 school year.4 Here, we used two of the BKT models (BA- SIC and KT-IDEM) using the partial credit score approach described above.5 We compared the performance of these techniques to the ones suggested by Ostrow et al., which addressed the partial credit score issue by using predefined probability tables that depended on problem diculty, par- tial credit scores, or both. All of the BKT models we used employed the Weighted partial credit score approach that was described in the previous section.  Figure 4 compares the RMSE scores (y-axis) for the Sim- ple and KT-IDEM approaches (left side of the figure) to the Partial Credit, problem diculty, and combined approaches of Ostrow et al. (right side of the figure). As shown in the figure, the KT-IDEM model obtained lower RMSE score than did all other approaches.  6. MODEL SELECTION In this section we address the task of choosing which of the  BKT models to use when interacting with new students in the classroom, who were not used to train the model. A nat- ural candidate is to use the model with the best average per- formance, as reported in Section 3. However, a single model cannot be optimal for predicting students performance on all questions (as suggested by Pardos and Heernen [17]). For example, we observed that for specific instances, the simple BKT model was able to outperform more complex models like the KT-IDEM model. Following this insight, we employed a model selection approach, that chose which BKT model to apply for a given response based on the per- formance of a set of candidate models on the training set of past questions.  The model selection process receives the following as in- put: a set of possible BKT models, a new students response  4This dataset is publicly available at http://tiny.cc/ LaS2015Submission. 5We did not use the grade model because the students grade was not available in the data, and we did not use the count model because it was superceded by the KT-IDEM model.  Figure 5: Comparison of MixedModel to other ap-  proaches on subset of K9 dataset from 2015  to a test-question, and number of retries for this student and question. It selects from the training set the set of all ques- tions matching the test question and number of retries, and returns the model with the lowest average RMSE score on the training set, which is used for prediction on the test- question. The performance on the training set is used as a proxy for the performance of the chosen BKT model on the actual test question.  We evaluated this MixedModel approach on both K9 and ASSISTment dataset. Figure 5 compares the performance of the MixedModel on the K9 dataset in terms of RMSE (y-axis). The set of candidate models included the subset of models equals to the top performing RMSE models of Sec- tion 3: the SIMPLE model, the COUNT model, the KT- IDEM model, and the KT-IDEM-Count model. To choose the best model to use in the test-set environment, we com- puted the RMSE values for each of the candidate models on the training set. In K9, we used the subset of the dataset from years 2009,2011 and 2014 as training, and tested on data from 2015, to simulate situations in which new students will be using the system, with little to no prior history of their interactions. As can be seen by the figure, the Mixed- Model approach achieved significantly lower RMSE scores than did the other models. The higher RMSE values of the other models as compared to other years can be explained by the lower number of records compared to other years.  We also compared the model selection approach in the AS- SISTment data set (not shown in the figure) with the SIM- PLE and KT-IDEM BKT models. We used a standard five- cross validation technique to train and test the MixedModel approach. The results were similar, in that the Mixed- Model approach was able to achieve the best performance on new data and significantly outperform the KT-IDEM method. Consequently we decided to adopt the Mixed- Model approach when deploying our sequencing algorithm in schools.  7. THE BKT-SEQUENCE ALGORITHM In this section, we describe an algorithm that utilizes BKT  for the purpose of selecting questions in realtime in the class- room, called BKT-Sequence. The algorithm receives as in- put a BKT model and a set of questions. Each question is assumed to have an associated skill. The output of the algorithm is the next question to present to the student.  The BKT-Sequence algorithm is shown in Figure 6. The algorithm first determines the minimal and maximal pre- dicted scores for each question in Q (lines 2-3). Line 5 com- putes the students intended score for each question. The    1: function ChooseNextQuestion(Q) 2: MinScore = min{P (scoren0) | n0 2 Q} 3: MaxScore = max{P (scoren0) | n0 2 Q} 4: For all questions n 2 Q compute 5: WantedScoren = MinScore+(MaxScoreMinScore)  (1 P (Ln) + n) 6: Din = |P (scoren)WantedScoren|  Penaltyn 7: Q0 = all questions of skill argminm02Q P (Lm0) 8: Return question j such that j = argminj2Q0 Dij  Figure 6: BKT-Sequence Algorithm  interval maxScore minScore determines the range of pos- sible scores for the student for question n. It then computes an intended score that the student should get for each ques- tion in order to advance its learning, with a value that is between these minimal and maximal scores. This score de- pends on the current mastery level of the student in that skill. Low levels of knowledge of the skill (as determined by P (Ln)) should lead to higher intended scores, The intended score is the point within the range between MaxScore and MinScore that best matches the students mastery level of the skill for the question. This point is determined by the lack of knowledge that the student has over the skill, which is (1P (Ln)+ n), where n is a correction term for n that is explained below. As the knowledge of a particular skill grows, the intended score for a question of this skill will be lower (closer to MinScore than to MaxScore). Lastly, the the algorithm returns the question in Q with a  predicted score that is closest to that of the intended score, i.e., closest to the score of the question that is the best match for the student. Specifically, Line 6 computes the absolute dierence between these two scores and in line 8 the algo- rithm returns the question that minimizes this dierence. As the knowledge of a particular skill grows, more dicult questions will be selected because they are associated with a lower predictive score than easier questions. We note the following implementation details. First, our  discussions with pedagogical experts who were concerned that transitioning rapidly to more dicult questions may reduce students motivation. We capped the increase or de- crease in value to the posterior over the skill level at 0.1, which was based on showing examples to the pedagogical experts.  P (Ln) =  8 >>><  >>>:  min {P (Ln | evidencen1), P (Ln) + 0.1} if P (Ln | evidencen1) > P (Ln)  max {P (Ln | evidencen1), P (Ln) 0.1} otherwise  (13) Second, we employed a correction term n for identifying  questions with degenerate parameter values in which an- swering incorrectly does not change the posterior (P (Ln | incorrectn) = P (Ln1)). Third, we introduced a penalty score for each question n  that is equal to the number of retries for the question. In this way, we avoid selecting questions that were attempted many times in the past. To illustrate the use of this algorithm, consider a set of  questions selected by a teacher or student and belonging to the skill of matching triangles. Suppose that the P (Ln) for this student is 0.6952. The minimum predicted grade  Item ID Diculty Predicted Num n P (G) P (S) Di Level Score Attempts  9805 3 0.696 1 0.417 0.462 0.189 0.003 11575 3 0.659 3 0.347 0.287 0.158 0.010 965 1 0.708 4 0.411 0.478 0.178 0.051  11625 3 0.474 1 0.575 0.232 0.406 0.323  Table 2: Example of candidate questions for BKT-  Sequence algorithm  for the student (MinScore) is 0.2317 and the maximum pre- dicted grade (MaxScore) is 0.8592. Table 2 shows a few of the candidate questions in the set of questions that provide the lowest intended scores for the student, showing for each question: the diculty level, the predicted score, the num- ber of retries (attempts) by the student, the corrective term n, the parameter values P (G) and P (S), and the dier- ence between the predicted score and the intended score. In this case, all of the questions belong to the same topic, so the algorithm will choose the question with the lowest dif- ference between the predicted and intended score (itemID 9805), which is associated with a diculty level of 3. Note that there are other questions in the set with the same dif- ficulty level, which were not chosen. For example, the last question (ItemID 11625) in the list was not chosen because of a high value for the P (S) (slip) parameter, which resulted in a lower predicted score, thus increasing the gap between the intended score and the predicted score for this question. The other questions (ItemIDs 11575 and 11625) were not chosen because of their high number of retries, which penal- ized their score.  8. DEPLOYMENT IN CLASSROOMS In cooperation with educational researchers and the devel-  opers of the K9 system, we were able to deploy the sequence algorithm in two dierent schools in Israel. The default use of the system in all schools is that students select a topic and a level of diculty, and the system selects random questions within these categories. The number of consecutive retries allowed for each question was limited to three (the same question could appear in another session). The students could also choose not to answer a question. Students re- ceived feedback from the system about their score for each question. An analysis of the data of students interactions shows that students choose to solve (mostly) easy problems and that they exhibit overall high grades.  We hypothesized that using the BKT-Sequence algorithm to generate educational content would get students to at- tempt to solve more dicult problems, without harming their performance on these problems or their satisfaction from using the e-learning system, when compared to an al- ternative sequencing approach.  Our experiment was conducted in two separate schools that did not use the K9 system prior to the experiment. The system was introduced in both schools at the same time, and the experiment was conducted during the last two months of the school year, between April 28th, 2015 and June 29th, 2015. Students interacted with K9 in sessions of 15 ques- tions. We compared between two approaches to sequence questions to students.  The first approach used the BKT sequence algorithm of Figure 6. The second approach was determined by peda- gogical experts, and included a set of questions randomly    School Group size Num. questions Time in system Num. of logins (BKT, ASC) (BKT, ASC) (BKT, ASC) (BKT, ASC)  A (12, 9) (921, 1054) (60, 31) (8.75, 10) B (26, 26) (4253, 3534) (74, 42) (22, 14)  Table 3: Statistics for both BKT and ASC sequenc-  ing approaches  sampled from dierent level of diculties as follows: 20% questions of level 1 diculty; 30% questions of level 2 di- culty; 40% questions of level 3 diculty; 10% questions of level 4 diculty. (The pedagogical experts did not want to include questions of the most dicult level.) The questions were sequenced to students in ascending order of diculty.  In each school, we chose two classrooms, one of which was randomly assigned to use the BKT-Sequence algorithm (de- noted BKT), while the other classroom was assigned to the ascending (ASC) algorithm. To gauge the level of students in each classrooms we administered a single session of 15 questions in mathematics that were sampled from dierent topics and diculty levels in mathematics. The questions for this preliminary test were chosen by a domain expert. There was no statistically significant dierence between the two groups in each school in the average score on this pre- liminary test. Hence we asserted that the students in each group exhibited similar knowledge baselines of the material.  Each classroom in K9 used its respective condition. We restricted ourselves to sessions from students who completed the preliminary session. We did not track the individual students progress during the experiment nor control any of the conditions in the classroom beyond the use of the sequence algorithm. Students could use the system with no supervision and practice as many questions as they want. Table 3 shows the distribution of the number of students in each group, the total number of questions solved, the average time spent on each question (in seconds), and the number of logins to the system.  Figure 7 shows the performance of students using the BKT-Sequence algorithm and the ASC algorithm in both schools. The x-axis shows the diculty levels of the dif- ferent questions from easy (1) to hard (5), while the y-axis shows the average grade obtained by students. As shown in the figure, for schools A and B, for low levels of diculty (levels 1 and 2) students using the ASC sequencing outper- formed students using the BKT-Sequence algorithm. For medium level of diculty (level 3), there was no dierence between students performance using both algorithms. For higher level of diculty (level 4), students using the BKT- Sequence algorithm achieved higher performance than those using the ASC algorithm.  Table 4 shows the number of questions posed by each al- gorithm for each level of diculty by each school. As shown in the table, there were significantly more dicult questions posed by the BKT sequencing algorithm than the ASC al- gorithm.  A natural question that arises is whether students were less motivated to work with the BKT-Sequence algorithm because it gave them harder questions. As shown by Ta- ble 3, the average time spent on solving questions in the system was significantly higher (for all schools) for students using the BKT-Sequence algorithm than the ASC algorithm. In addition, we were able to conduct a survey in one of the schools (School B) that participated in the experiment. We  Level ASC A BKT A ASC B BKT B 1 358 131 1104 667 2 324 191 1176 1089 3 291 297 936 1216 4 81 302 318 1281 5 0 218 0 525  Table 4: Number of questions by diculty level in  the BKT-Sequence and Ascending Algorithm Con-  ditions in school A and school B.  asked the students in each sequencing condition the follow- ing question: How much did you feel the question helps you to understand the relevant topic Students could choose to answer that (1) the question did not help at all, (2) helped a little, or (3) helped a lot. The average score for the BKT algorithm was 2.2, while the score for the ASC algorithm was 1.53, which was also verified to be significantly lower (p < 0.05). We can thus conclude that reasoning about stu- dents skill knowledge in the BKT-Sequence algorithm re- sulted in students receiving questions that are more suitable for them, as also determined by their subjective opinions.  9. DISCUSSION AND CONCLUSION In this work, we used a computational knowledge tracing  approach to augment existing e-learning systems to sequence educational content to students in the classroom. Our ap- proach followed three main steps. First, we extended the Bayesian Knowledge Tracing approaches to account for the challenges of modeling classroom data. We showed that an augmented BKT model that reasoned about partial credit scores, item diculty and multiple retries was able to out- perform existing models from the literature in predicting students scores.  Second, we provided a model selection approach that se- lects the best model to use out of a given set of candidate BKT models and a students response. We showed the e- cacy of this approach when predicting new students scores.  Third, we designed an algorithm that used the model se- lection approach to choose questions to students by reason- ing about their inferred skill knowledge. The algorithm fol- lows the mastery learning paradigm, by identifying questions that are predicted to advance students knowledge within skills needing improvement. It provides the student with the question that is predicted to advance her knowledge while keeping her in the zone of possible learning [21] by suggesting questions that are within the range of the students inferred capabilities.  Our algorithm was subsequently deployed in an e-learning system for mathematics and evaluated in two dierent schools with new students. It was compared to a baseline sequenc- ing approach which sampled questions of varying diculty levels according to a domain expert. It was shown to lead students to solve more dicult questions, spend more time in the system, and report higher levels of satisfaction from the systems than students who used the alternative method. Our results demonstrate the benefits of a general technique for augmenting existing e-learning systems in a way that improves students performance.  We address several issues arising from our study. First, we chose to evaluate our algorithms in real classrooms over the course of several weeks. Students varied widely in their    0   0.1   0.2   0.3   0.4   0.5   0.6   0.7   0.8   0.9   1  2  3  4   BKT  ASC   Figure 7: Performance comparison between BKT-Sequence and ASC algorithm for each skill level in school  A (left) and school B (right). The x-axis shows the diculty levels, while the y-axis shows the average grade.  use of the system in the dierent classrooms, including time of use (whether at class or at home), and whether they were incentivized to use the system by their teachers. Because we did not use controlled laboratory conditions, we chose to forego the use of pre and post tests to measure the eects of our algorithms on students performance and learning gains. However, the fact that the students in both algorithm con- ditions exhibited similar performance on a standardized set of questions before commencing the study confirms that it was conducted on equal grounds.  Second, we note that in all schools, students using the ascending algorithm performed better on easier questions (levels 1 and 2) while students using the BKT-Sequencing algorithm performed better on harder questions (levels 3 and 4). This can be explained by the fact that there were sig- nificantly more easier questions proposed by the ascending algorithm than the BKT-Sequence algorithm. However, we were able to show that it is possible to get students to solve more dicult questions - a primary goal of e-learning sys- tems in school. Furthermore students expressed more satis- faction from using this system, despite having to solve more dicult questions.  We are currently extending the work in several ways. First, we will compare the algorithm to other ASC manual strate- gies and to a more elaborate baseline approach that consid- ers students performance when deciding on the next ques- tion to ask. Second, we are designing new BKT models that consider gamification elements which are becoming more prevalent in students work.  10. ACKNOWLEDGMENTS The authors wish to thank the Lnet software company  (https://lnet.org.il/) for providing the infrastructure for data collection and sequencing. Special thanks is due to Iris Tabak for her very helpful advice on experiment design and previous drafts. Thanks also to Guy Shani on helpful com- ments on previous drafts of this paper.  11. REFERENCES [1] F. B. Baker and S.-H. Kim. Item response theory:  Parameter estimation techniques. CRC Press, 2004. [2] M. Bielikova, M. Simko, M. Barla, J. Tvarozek,  M. Labaj, R. Moro, I. Srba, and J. Sevcech. Alef: from application to platform for adaptive collaborative learning. In Recommender Systems for Technology Enhanced Learning, pages 195225. Springer, 2014.  [3] J. H. Block, P. W. Airasian, B. S. Bloom, and J. B. Carroll. Mastery learning: Theory and practice. Holt, Rinehart and Winston New York, 1971.  [4] P. Brusilovsky, L. N. Cassel, L. M. Delcambre, E. A. Fox, R. Furuta, D. D. Garcia, F. M. Shipman, and M. Yudelson. Social navigation for educational digital libraries. Procedia Computer Science, 1(2):28892897, 2010.  [5] B. Clement, P.-Y. Oudeyer, D. Roy, and M. Lopes. Online optimization of teaching sequences with multi-armed bandits. In International Conference on Educational Data Mining (EDM), 2014.  [6] A. T. Corbett and J. R. Anderson. Knowledge tracing: Modeling the acquisition of procedural knowledge. User Modelling and User-Adapted Interaction, 4(4):253278, 1995.  [7] A. T. Corbett and A. Bhatnagar. Student Modeling in the ACT Programming Tutor: Adjusting a Procedural Learning Model With Declarative Knowledge. In User Modeling, pages 243254. Springer Vienna, Vienna, 1997.  [8] R. S. d Baker, A. T. Corbett, and V. Aleven. More accurate student modeling through contextual estimation of slip and guess probabilities in bayesian knowledge tracing. In Intelligent Tutoring Systems, pages 406415, 2008.  [9] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the em algorithm. Journal of the royal statistical society. Series B (methodological), pages 138, 1977.  [10] M. C. Desmarais and R. S. J. de Baker. A review of recent advances in learner and skill modeling in intelligent learning environments. User Modeling and User-Adapted Interaction, 22(1-2):938, 2012.  [11] W. J. Hawkins, N. T. Heernan, and R. S. J. de Baker. Learning Bayesian Knowledge Tracing Parameters with a Knowledge Heuristic and Empirical Probabilities. Intelligent Tutoring Systems, 8474(Chapter 18):150155, 2014.  [12] T.-C. Hsieh, M.-C. Lee, C.-Y. Su, et al. Designing and implementing a personalized remedial learning system for enhancing the programming learning. Educational Technology & Society, 16(4):3246, 2013.  [13] Y.-M. Huang, T.-C. Huang, K.-T. Wang, and W.-Y. Hwang. A markov-based recommendation model for exploring the transfer of learning on the web. Educational Technology & Society, 12(2):144, 2009.    [14] K. Ostrow, C. Donnelly, S. Adjei, and N. Heernan. Improving student modeling through partial credit and problem diculty. In Proceedings of the 2nd ACM Conf on L@ S, pages 1120, 2015.  [15] Z. Pardos and N. Heernan. Modeling individualization in a Bayesian networks implementation of knowledge tracing. In User Modeling, Adaptation, and Personalization, volume 6075, pages 255266. 2010.  [16] Z. A. Pardos, Y. Bergner, D. T. Seaton, and D. E. Pritchard. Adapting Bayesian Knowledge Tracing to a Massive Open Online Course in edX. EDM, pages 137144, 2013.  [17] Z. A. Pardos and N. T. Heernan. KT-IDEM: Introducing Item Diculty to the Knowledge Tracing Model. In UMAP, 2011.  [18] R. Pelanek. Metrics for evaluation of student models. Journal of Educational Data Mining, 2015.  [19] L.-p. Shen and R.-m. Shen. Learning content recommendation service based-on simple sequencing specification. In Advances in Web-Based LearningICWL 2004, pages 363370. Springer, 2004.  [20] Y. Wang and N. Heernan. Extending knowledge tracing to allow partial credit: Using continuous versus binary nodes. In Artificial Intelligence in Education, pages 181188. Springer, 2013.  [21] J. V. Wertsch. Vygotsky and the social formation of mind. Harvard University Press, 1988.    "}
{"index":{"_id":"44"}}
{"datatype":"inproceedings","key":"Martori:2016:SRB:2883851.2883901","author":"Martori, Francesc and Cuadros, Jordi and Gonz'alez-Sabat'e, Lucinio","title":"Studying the Relationship Between BKT Fitting Error and the Skill Difficulty Index","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"364--368","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883901","doi":"10.1145/2883851.2883901","acmid":"2883901","publisher":"ACM","address":"New York, NY, USA","keywords":"BKT, BKT-BF, RMSE modeling, difficulty index, educational data mining","abstract":"Bayesian Knowledge Tracing (BKT) is one of the most popular knowledge inference models due to its interpretability and ability to infer student knowledge. A proper student modeling can help guide the behavior of a cognitive tutor system and provide insight to researchers on understanding how students learn. Using four different datasets we study the relationship between the error coming from fitting the parameters and the difficulty index of the skills and the effect of the size of the dataset in this relationship. The relationship between the fitting error and the difficulty index can be very easy modeled and might be indicating some problems with BKTs performance. However, large datasets are required to clearly see this connection as there is an important sample size effect.","pdf":"Studying the relationship between BKT fitting error and  the skill difficulty index     Francesc Martori    ASISTEMBE  IQS Universitat Ramon Llull   Via Augusta 390  Barcelona, Spain   francesc.martori@iqs.edu    Jordi Cuadros  ASISTEMBE   IQS Universitat Ramon Llull  Via Augusta 390  Barcelona, Spain   jordi.cuadros@iqs.edu    Lucinio Gonzlez-Sabat  ASISTEMBE   IQS Universitat Ramon Llull  Via Augusta 390  Barcelona, Spain   lucinio.gonzalez@iqs.edu        ABSTRACT  Bayesian Knowledge Tracing (BKT) is one of the most popular  knowledge inference models due to its interpretability and ability  to infer student knowledge. A proper student modeling can help  guide the behavior of a cognitive tutor system and provide insight  to researchers on understanding how students learn. Using four  different datasets we study the relationship between the error  coming from fitting the parameters and the difficulty index of the  skills and the effect of the size of the dataset in this relationship.  The relationship between the fitting error and the difficulty index  can be very easy modeled and might be indicating some problems  with BKTs performance. However, large datasets are required to  clearly see this connection as there is an important sample size  effect.   CCS Concepts   Mathematics of computing~Probabilistic representations   Keywords  BKT, BKT-BF, RMSE modeling, difficulty index, educational data  mining   1. INTRODUCTION  1.1 Bayesian Knowledge Tracing  Bayesian Knowledge Tracing (BKT) [1] is a student model used to  infer a students knowledge given their history of responses to  problems, which it can use to predict future performance. Using  students responses to questions, which are tagged with the skills  that the instructor wants the students to learn, the model tells the  probability a student has mastered a skill.    BKT is a two state Hidden Markov Model, these states being the  one in which the student knows a given skill, and the one where the  student does not. The knowledge state is absorbent, implying that  the student will not forget the skill once it is learned. To calculate  the probability that a student knows the skill given their  performance history, BKT uses four probabilities:    L0, the probability a student knows the skill before attempting  the first problem,    T, the probability a student, who does not currently know the  skill, will know it after the next practice opportunity, that is  the transition probability at each practice opportunity,    G, the probability a student will answer a question correctly  despite not knowing the skill,    S, the probability a student will answer a question incorrectly  despite knowing the skill.    According to this model, knowledge affects performance (mediated  by the guess and slip rates), and knowledge at one time step affects  knowledge at the next time step, but no further. Then, if a student  is in the no knowledge state at time t, then the probability he will  be in the knowledge state at time t+1 is T.    Usually, a separate BKT model is fit for each skill and only the first  attempt at each question is taken for each student, as it is the attempt  containing the most information about the students knowledge.   1.2 Bayesian Knowledge Tracing  Brute  Force   Bayesian Knowledge Tracing  Brute Force [2] (BKT-BF) is an  algorithm to estimate the values for the BKT parameters. It is a  simple brute force algorithm, where a grid of possible values is set  so that for each combination of parameters, a RSS value is obtained.  At the end, the combination of values resulting in the lowest  Residual Sum of Squares (RSS) value for a skill is the one that will  be used in BKT.    In BKT-BF, the RSS is calculated as follows:   =   (,    ,)  2 =1      eq. 1   Where:   Oi,t is {0,1} depending on the students answer to a given  question,    students is the number of different students who faced any  question of a given skill,   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and  that copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than the  author(s) must be honored. Abstracting with credit is permitted. To  copy otherwise, or republish, to post on servers or to redistribute to  lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.  LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883901         mailto:francesc.martori@iqs.edu mailto:jordi.cuadros@iqs.edu mailto:lucinio.gonzalez@iqs.edu   dim is the number of different questions that are tagged with a  given skill    Ci,j is the likelihood to produce a correct answer to a question.  This calculation is derived from the BKT formulas, and it is  done, for the student i, as follows:    , = 1  (1  ) + (1  1)         eq. 2      BKT-BF is, however, is very expensive in computational cost, as  all brute force algorithms are, and does not help the identifiability  [3] problem from BKT; identifiability results in different  combinations of parameter values, some of which make no  theoretical sense, giving similar RSS values. Other used algorithms  to train BKT are EM [4], which is not as computationally  demanding but suffers from local minima issues.    It is noteworthy that BKT parameters are defined in a sense where  the modelling is centered on the student as an individual, but  parameters tend to be trained treating students as a whole, and as a  result not having personalized parameters. Of course this latter  approach might easily result in overfitting although some efforts to  solve this problems have been done lately. [5] [6]   In Martori et al. [7] a very high relationship between the RMSE  from the BKT parameter tuning and a second degree polynomial of  the skills difficulty index was found. The objective of this paper is  to continue that work by looking for the same relationship in other  datasets, checking if there is any sample size effect involved in the  relationship between RMSE and difficulty index, and at the same  time study the local minima problems when fitting the parameters  for the skill modelling.      2. DATA AND METHODS  Four different datasets have been used in this research. OLI MOOC  Psychology was the dataset used in Martori et al. [7]. We have also  included OLI Psychology. These two datasets are private datasets  from PSLC [8] and they use the same knowledge component model  but students and delivery type are different as OLI Psychology was  delivered in the usual OLI context in 2012-13, while OLI MOOC  Psychology was a MOOC that was done in spring 2013. The other  two datasets are public datasets, also from PSLC [8]. The dataset  WPI Assistments, is a Math dataset part of the ASSISTments  project. This dataset was created in 2004-05, and has generated 11  papers, so far. Lastly, the dataset Physics, was collected by the The  Andes Physics Tutoring System in 2008 and was used with students  from the United States Naval Academy.    All four datasets have substantially different number of skills,  students (both in number and profile) and number of records, as it  is summarized in table 1.                                                                            1 The number of skills between brackets corresponds to the number  of skills tagged with more than 4 different questions, as these are  the skills that will be used in the general regression model.    Table 1: Summary of the datasets used   Datasets Skills1 Number of   students   Records in   dataset2   OLI MOOC  Psychology 226 (96) 5564 1 970 401   OLI  Psychology 265 (103) 1088 1 009 255   WPI  Assistments 75 (44) 907 267 407   Physics 244 (93) 97 81 433     To calculate the fitted parameters for BKT we have used BKT-BF  with the following specifications to create the parameter grid, the  bounded approach has been taken in order to avoid model  degeneracy [9] for parameters S and G:    L0 has ranged from 0.05 to 0.95 with a 0.10 step    T  has ranged from 0.05 to 0.95 with a 0.10 step    S  has ranged from 0.05 to 0.30 with a 0.05 step    G has ranged from 0.05 to 0.30 with a 0.05 step   This has resulted in 3600 models fitted per skill to determine the  optimal BKT parameters.    Even though BKT-BF calculates the RSS, in this paper we will  usually refer to the RMSE. This is an advantage as the RMSE is a  standardized version of the RSS error that ranges from 0 to 1.   The skill difficulty index is the percentage of correct answers in  questions tagged with a single skill. In order to simplify the  discussion we will not consider whether the skill is difficult to learn  or if the difficulty is coming from one or more questions included  in the skill. It must be noted that due to the way the difficulty index  is usually calculated, although it is not intuitive, the higher the  difficulty index, the easier the question is.   In order to study the effect of the dataset size, in terms of number  of students and number of questions per skill, we will take random  subsamples of one datasets and rerun the analysis of the RMSE  versus de difficulty index. The samples will range from 100 000  data points to 1 million. It needs to be noted that to we will include  all skills in the analysis, that is, we will not exclude skills tagged  with less than four questions. This especially sensitive with the  smaller subsamples, as lots of data are being left aside.      3. RESULTS  We have fitted the BKT parameters for the total 810 skills from all  four datasets, however, we will only present results of the dataset  OLI Psychology when we refer to individual dataset results.   In general no local minima problems were found when fitting the  parameters of the skills. In some cases, however, the optimal fitting  areas were moderately large. Figure 1 shows the different contour  plots from one skill, where we can see the minimum RSS area  shaded in the darkest grey, as well as interactions among some of     2 Of the cleaned dataset as it only accounts for first answers to   questions tagged to skills with known knowledge components.     the parameters. Contour plots have been calculated using the mean  RSS value for each pair of parameters.         S vs L0 G vs L0      S vs T T vs L0      S vs G G vs T       Figure 2 shows the histograms for the distributions of the fitted  BKT parameters of the OLI Psychology dataset. It is noticeable that  the parameters are settled close to the edges of the studied domain,  especially for the parameters G and T. In fact the parameter G  seems to behave almost as constant across all the skills of the  dataset.               Figure 2: Histograms of the distributions for all parameters of   the dataset OLI Psychology.       Figure 3: Difficulty index vs BKT-BF error for all skills.        Figure 1: Contour plots of the RSS fitting error of the skill   Recognize physical contributions to psychology for the  pairs of parameters.     Figure 3 presents a scatterplot displaying the RMSE and its  corresponding difficulty index of the 336 skills, of all four datasets,  that have more than four different questions tagged to it.   All four datasets seem to follow a parabolic trend line, in fact, if we  fit a 2nd degree polynomial to the data, an adjusted R2 of 0.83 is  obtained. If we look at the data the parabolic behavior is especially  consistent in the largest datasets, in terms of number of records, as  the Physics skills seem to fall off the line. As a matter of fact the  Physics dataset is the one with the fewest number of students and  records.   As said in methods, we will perform the same analysis taking  random subsamples from the OLI Psychology dataset. We have  used the results to run a regression model using a second degree  polynomial of the difficulty index as predictor for each subsample  taken from the dataset OLI Psychology.         Figure 4: Adjusted R2 vs Sample sizes.      Figure 5 shows the same scatterplots displayed in figure 3 for the  sample of 100 000, 400 000, 700 000 and 1 000 000 data points.  We can see how variability disappears as we increase the number  of data points and in fact the adjusted R2 for the regression goes up  from 0.815, when we are using 100 000 data points, to 0.994 with  1 000 000 data points. The estimated parameters for the regression  line do not have a big variability, as could be expected, along the  different sample sizes.        Figure 5: BKT-BF error vs Difficulty index with different   sample size.      4. DISCUSSION  The parabolic relationship between the difficulty index of the skills  and the corresponding RMSE error of the fitted BKT model that  was observed in Martori et al. [7] has been extended to three more  datasets. We are aware that the step size we are using in BKT-BF  is pretty large but we are fitting the parameters to study the behavior  of the error, so we consider that this has no impact on the results  we obtained and the conclusions we reached. On the other hand,  and in accordance to van De Sandes findings [10] on  identifiability, the contour plots are a clear example that no  problems of local minima were found. It needs to be noted,  however, that the optimal areas are very wide, which could be a  confounder for identifiability. Further research around this area is  needed to clarify this.  The parabolic behavior of the difficulty index is indicates that the  BKT performance highly depends on the difficulty index of the  skill. That being said, BKT performs at its best when it is dealing  with either easy or difficult skills. In the worst scenario, when skills  have difficulty indexes around 0.5, the performance of BKT is not  good, in terms of estimating the students likelihood to provide a  correct answer, given that the corresponding RMSEs are also close  to 0.5. This is a problem because the intermediate difficulty index  area should be the one in which students happen to be working  more often, as we expect them to start the learning process without  knowing the skill and, eventually finish having mastered it.   The second degree polynomial of the difficulty index, di*(1-di),  could be thought as the interaction of mastery and S for the first  piece, and the one between the probability of not knowing and the  G parameter. However, further research to see how the four BKT  parameters are related to these findings is very much needed. It  might be indicating, as well, that one or more parameters should be  moved from the skill level, where they are now, to a finer grained  student-skill level. This would be consistent with the work done in  [5] and [6]. Another approach seeking for individualization would  follow the work done by Galyardt and Goldin [11].  There is an important sample size effect, probably related to the  central limit theorem, in the quality of the fit between the RMSE  and the difficulty index. This effect, however, is better noticed  when dealing with large datasets, which involve an important  number of students and difficulty indexes spanning around wide  domain. This situation might not be the usual situation in which  BKT is being used. It is noteworthy that the fact that we cannot  observe the described effect, due to the reasons we just said, does  not exclude that it is indeed happening.  Future research lines should involve using these same methodology  on different knowledge inference models, especially those looking  for in individualization, in order to see if similar behaviors are  observed.    REFERENCES  [1] Corbett, A. T. and Anderson, J. R. 1995. Knowledge tracing:   Modeling the acquisition of procedural knowledge. User  Modeling and User-Adapted Interaction, 4(4), 253-278.  http://dx.doi.org/10.1007/BF01099821    [2] Baker, Corbett, Gowda, Wagner, MacLaren, Kauffman,  Mitchell, & Giguere, 2010. Bayesian Knowledge Tracing  Brute Force model fitting code.  http://users.wpi.edu/~rsbaker/edmtools.html    [3] Beck, J. E., Chang, K. M. 2007 Identifiability: A  fundamental problem of student modeling. In: Conati, C.,  McCoy, K., Paliouras, G. (Eds.) UM 2007. LNCS, vol.   http://dx.doi.org/10.1007/BF01099821 http://users.wpi.edu/~rsbaker/edmtools.html   4511/2007, pp. 137- 146. http://dx.doi.org/10.1007/978-3- 540-73078-1_17   [4] Moon, T. K. 1996. The expectationmaximization algorithm.  IEEE Signal Process. Mag., 13, 4760   [5] Pardos, Z. A. and Heffernan, N. T. 2010. Modeling  Individualization in a Bayesian Networks Implementation of  Knowledge Tracing. In: Paul De Bra, Alfred Kobsa, David  N. Chin (eds.) Proceedings of the 18th International  Conference on User Modeling, Adaptation, and  Personalization (UMAP 2010), LNCS vol. 6075 pp. 255-  266. Springer (2010) http://dx.doi.org/10.1007/978-3-642- 13470-8_24    [6] Yudelson, M., Koedinger, K. and Gordon, G. 2013  Individualized bayesian knowledge tracing models. In  Artificial Intelligence in Education, pages 171180.  Springer, 2013.   [7] Martori, F., Cuadros, J. and Gonzlez-Sabat, L. 2015 Direct  estimation of the minimum RSS value for training Bayesian  Knowledge Tracing parameters. Proceedings of the 8th  International Conference on Educational Data Mining, pp.  364-367    [8] Koedinger, K.R., Baker, R.S.J.d., Cunningham, K.,  Skogsholm, A., Leber, B., Stamper, J. 2010. A Data  Repository for the EDM community: The PSLC DataShop.  In Romero, C., Ventura, S., Pechenizkiy, M., Baker, R.S.J.d.  (Eds.) Handbook of Educational Data Mining. Boca Raton,  FL: CRC Press. http://dx.doi.org/10.1201/b10274-6    [9] Baker, R.S.J.d., Corbett, A. T., Aleven, V.: More Accurate  Student Modeling through Contextual Estimation of Slip and  Guess Probabilities in Bayesian Knowledge Tracing. In:  Woolf, B., Aimeur, E., Nkambou, R., Lajoie, S. (Eds.) ITS  2008. LNCS, vol. 5091/2008, pp. 406-415. Springer, Berlin  Heidelberg (2008) http://dx.doi.org/10.1007/978-3-540- 69132-7_44    [10] van De Sande, B. (2013). Properties of the Bayesian  Knowledge Tracing model. JEDM-Journal of Educational  Data Mining, 5(2), 1-10.   [11] Galyardt, A. and Goldin, I. 2015 Move your lamp post:  Recent data reflects learner knowledge better than older data.  Journal of Educational Data Mining. Vol 7, No 2, pp 83-111     http://dx.doi.org/10.1007/978-3-540-73078-1_17 http://dx.doi.org/10.1007/978-3-540-73078-1_17 http://dx.doi.org/10.1007/978-3-642-13470-8_24 http://dx.doi.org/10.1007/978-3-642-13470-8_24 http://dx.doi.org/10.1201/b10274-6 http://dx.doi.org/10.1007/978-3-540-69132-7_44 http://dx.doi.org/10.1007/978-3-540-69132-7_44   "}
{"index":{"_id":"45"}}
{"datatype":"inproceedings","key":"Liu:2016:MCM:2883851.2883967","author":"Liu, Ran and Patel, Rony and Koedinger, Kenneth R.","title":"Modeling Common Misconceptions in Learning Process Data","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"369--377","numpages":"9","url":"http://doi.acm.org/10.1145/2883851.2883967","doi":"10.1145/2883851.2883967","acmid":"2883967","publisher":"ACM","address":"New York, NY, USA","keywords":"additive factors model, fraction arithmetic, knowledge component model, misconceptions, q-matrix","abstract":"Student mistakes are often not random but, rather, reflect thoughtful yet incorrect strategies. In order for educational technologies to make full use of students' performance data to estimate the knowledge of a student, it is important to model not only the conceptions but also the misconceptions that a student's particular pattern of successes and errors may indicate. The student models that drive the outer loop of Intelligent Tutoring Systems typically do not represent or track misconceptions. Here, we present a method of representing misconceptions in the Knowledge Component models, or Q-Matrices, that are used by student models to estimate latent knowledge. We show, in a case study on a fraction arithmetic dataset, that incorporating a misconception into the Knowledge Component model dramatically improves the overall model's fit to data. We also derive qualitative insights from comparing predicted learning curves across models that incorporate varying misconception-related parameters. Finally, we show that the inclusion of a misconception in the Knowledge Component model can yield individual student estimates of misconception strength that are significantly correlated with out-of-tutor measures of student errors.","pdf":"Modeling Common Misconceptions in Learning Process Data  Ran Liu   Carnegie Mellon University  5000 Forbes Avenue  Pittsburgh, PA 15201  ranliu@cmu.edu   Rony Patel  Carnegie Mellon University   5000 Forbes Avenue  Pittsburgh, PA 15201   rbpatel@andrew.cmu.edu   Kenneth R. Koedinger  Carnegie Mellon University   5000 Forbes Avenue  Pittsburgh, PA 15201   koedinger@cmu.edu       ABSTRACT  Student mistakes are often not random but, rather, reflect  thoughtful yet incorrect strategies. In order for educational  technologies to make full use of students performance data to  estimate the knowledge of a student, it is important to model not  only the conceptions but also the misconceptions that a students  particular pattern of successes and errors may indicate. The  student models that drive the outer loop of Intelligent Tutoring  Systems typically do not represent or track misconceptions. Here,  we present a method of representing misconceptions in the  Knowledge Component models, or Q-Matrices, that are used by  student models to estimate latent knowledge. We show, in a case  study on a fraction arithmetic dataset, that incorporating a  misconception into the Knowledge Component model  dramatically improves the overall models fit to data. We also  derive qualitative insights from comparing predicted learning  curves across models that incorporate varying misconception- related parameters. Finally, we show that the inclusion of a  misconception in the Knowledge Component model can yield  individual student estimates of misconception strength that are  significantly correlated with out-of-tutor measures of student  errors.   CCS Concepts   Applied computing  Computer-managed instruction   Applied computing  Computer-assisted instruction   Keywords  Misconceptions; Knowledge Component Model; Additive Factors  Model; Q-Matrix; Fraction Arithmetic   1. INTRODUCTION  Many meaningful learning experiences result from making  mistakes and receiving useful feedback. In order for educational  technologies to fully capitalize on mistakes as opportunities for  learning, it is important to understand and model the underlying  factors that give rise to common mistakes. Sometimes, errors  reflect a simple lack of knowledge about how to solve the  problem (i.e., if the student asks for a hint or makes a random  guess). But, often, students make errors that reflect a thoughtful   yet incorrect strategy.   Intelligent Tutoring Systems have typically taken common  mistakes and misconceptions into consideration when  constructing the activities of the inner loop [21]. For example,  on any given problem, performing a common error that is tagged  to result from a buggy rule will trigger an immediate bug  message that specifically addresses the misconception. However,  as Intelligent Tutoring Systems span multiple problems and  topics, the outer loop is what keeps track of students  knowledge of various skills over time, across different problems,  and uses these estimates to adaptively select problems. The  longer-term student models that drive the outer loop typically  do not represent or track misconceptions when estimating latent  knowledge.   This is largely due to the limited state-of-the-art methods for  mapping Knowledge Components (KCs)the underlying facts,  skills, and concepts required to solve problems [10]to actual  problem steps. This mapping is called a Knowledge Component  (KC) model and is an essential part of how the student model  tracks knowledge. Currently, any given KC can only be positively  mapped to performance on a problem step. As a result, student  models can only track long-term estimates of the degree to which  students have knowledge that positively contributes to problem  step success. While misconceptions sometimes lead to the correct  answer on certain problem steps for the wrong reason, they lead to  incorrect responses on others. Thus, the current structure of KC  models cannot accommodate an estimate of misconception  activation.   Existing research in psychometrics has produced some methods  for assessing and modeling student errors and misconceptions [7,  13, 20]. These models, however, only apply to static performance  data and do not model the evolution of misconception strength  over time, as learning unfolds. Thus, they miss out on capturing  the temporal richness that is present in learning process data [6]. It  is also not yet clear how these psychometric modeling techniques  can be integrated with the student models that drive adaptive  problem selection in Intelligent Tutoring Systems.   Here, we present a method of representing misconceptions within  KC models. When combined with a statistical model, these  expanded KC models allow us to (1) estimate the overall strength  of a particular misconception within a dataset and (2)  microgenetically [19] estimate the evolving strength of a  misconception as learning unfolds across the dataset. In a case  study, we apply this misconception modeling method to a fraction  arithmetic dataset. We evaluate how the addition of a  misconception to the KC model improves the overall student  models fit to data. We also interpret the qualitative insights that  emerge from explicitly modeling this misconception and the  unique instructional implications of our results. Finally, we show  that explicitly representing a misconception in a KC model allows   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883967     us to estimate individual student-level misconception strengths,  and we validate these estimates using students out-of-tutor  performance data.   2. BACKGROUND  2.1 Intelligent Tutoring Systems  Intelligent Tutoring Systems work by guiding students through a  sequence of instructional activities (i.e., problems) in an outer  loop, while monitoring progress on students activities and  providing just-in-time feedback and hints within an inner loop  [21]. Many Intelligent Tutoring Systems accommodate common  mistakes and misconceptions in the activities of the inner loop.  That is, errors that result from misconceptions can be identified in  the inner loop and lead to bug messages that immediately and  specifically address the misconception that the error is linked to.   The long-term student models [6] that power the outer loop of  Intelligent Tutoring Systems, however, typically do not track  misconceptions or any type of skill that could negatively impact  performance on certain problem steps. Since Intelligent Tutoring  Systems typically span multiple problems and topics, student  models keep an ongoing estimate of students' knowledge of  various skills, concepts, and facts (referred to here as Knowledge  Components, or KCs [10]) over time. Intelligent tutors use these  estimates to select problems that give students more practice on  the KCs that need it. The space of KCs within a tutor or dataset is  defined and mapped to problem steps by a Knowledge  Component (KC) model. The current state of KC modeling is that  any given KC can only be positively mapped to performance on a  problem step. Student models that utilize these KC models cannot  keep estimates of, or attribute performance to, students  misconceived knowledge. Consequently, they cannot use such  information to influence problem selection in the long-term, outer  loop of Intelligent Tutors.   Estimating the strength of common misconceptions in existing  educational datasets and discovering the instructional conditions  or experiences that may strengthen or weaken them is an  important endeavor in its own right. Yet, current approaches to  educational data mining have not explicitly represented  misconceptions (or any type of KC that can negatively affect a  problem step outcome) in the development and refinement of KC  models.   Bayesian Knowledge Tracing, a special case of using Hidden  Markov Models to model student knowledge as a latent variable,  is commonly used to drive the outer loop of Intelligent Tutors. A  different, logistic regression-based model known as the Additive  Factors Model (AFM) [4] is more commonly used towards  evaluating, discovering, and refining KC models [11, 12].  Because the primary goal of the present paper was to expand the  flexibility of KC modeling to incorporate a misconception KC, we  focus on using the Additive Factors Model to conduct our  investigation and model evaluations.   2.2 The Additive Factors Model  AFM is a logistic regression model that extends item response  theory by incorporating a growth or learning term. This statistical  model (Equation 1) gives the probability ! # that a student i will  get a problem step j correct based on the students baseline ability  ($ ), the baseline difficulty (%&) of the required KCs on that  problem step ('#&), and the improvement ((&) in each of the  required KCs with each additional practice opportunity multiplied  by the number of practice opportunities () &) the student has had  with that KC prior to the current problem step [4].   ln , -./01-./2 = $  +  '#&(%&&89: + (&) &)  (1)  The KC model, otherwise known as the Q-Matrix [3, 20], is  represented by Qjk. The parameter Qjk takes on a value of 1 if the  KC k is required for problem step j and a value of 0 if it is not.   2.3 Knowledge Component Models  Log data resulting from student use of Intelligent Tutors contain  information on student attempts on a series of problem steps. Each  of these problem steps can be tied to one or more Knowledge  Components (KCs). This mapping of problem steps to KCs is  accomplished by a KC model.   KC models are an important basis for the instructional design of  automated tutors and for accurate assessments of learning.  Improvements to KC models, when combined with an appropriate  theoretical interpretation, lead to better predictions of what a  student knows, thus resulting in better assessment and more  efficient learning overall [12].   The current state of KC modeling is that any given KC can only  be positively mapped to performance on a problem step. That is,  the KC model representation contains a 1 for any KC that  contributes to success on a problem step and a 0 for any KC that  does not. Thus, student models cannot track long-term estimates  of students misconceived concepts or procedures. Furthermore,  there is no way to include analyses of misconceptions as part of  manual [12] or automated [11] KC model discovery and  refinement. Here, we proposed a way of expanding the flexibility  of KC modeling to include misconceptions and other  misconception-like KCs.   3. MODELING MISCONCEPTION KCs  Our proposed method for modeling misconceptions as KCs is to  map them to problem steps to which they could apply, the way  that regular KCs are, but to utilize -1 in addition to 1 and 0  coding. A misconception is coded as 1 for problem steps on which  applying it yields the correct answer and as -1 for problem steps  on which applying yields an incorrect answer. Problem steps for  which the misconception does not apply are coded as 0. For  example, suppose a student was completing problems asking for  the Least Common Multiple of two numbers and he/she had a  misconception that one always finds the Least Common Multiple  by taking the product of the two numbers. There are some cases in  which this strategy will yield the right answer (e.g., find the Least  Common Multiple of 5 and 9) albeit for the wrong reason, and  for these problem steps the misconception KC is coded as a 1.  There are other cases in which this strategy will yield the wrong  answer (e.g., find the Least Common Multiple of 4 and 12), and  for these steps it is coded as a -1.   For regular KCs that represent positive skills and concepts, the  effect of prior practice opportunities is relatively straightforward  to count for the purposes of modeling. Any problem step on which  a regular KC is tagged constitutes a practice opportunity for that  KC. For misconception KCs, it is less obvious how to count  practice opportunities. Any problem step tagged as -1 by the  misconception KC is an opportunity to make a misconception- driven error and, as such, could weaken the misconception upon  receiving negative feedback. Conversely, any problem step tagged  as a 1 by the misconception KC is an opportunity to yield a  misconception-driven correct answer. As such, it could strengthen  the misconception due to the student receiving positive feedback  for applying it. In the present modeling approach, we treated     opportunities to weaken a misconception and opportunities to  strengthen a misconception as separate parameters to allow them  to be weighted differently.   In the following case study, we apply these methods to model a  well-documented misconception in fraction arithmetic. We create  a KC to represent how this misconception should affect  performance outcomes on different problem steps. We then  compare the following three models: (1) AFM with the Original  KC Model, (2) AFM with the original KCs plus an added  misconception KC intercept (termed the Misconception KC  Model), and (3) AFM with the original KCs plus both a  misconception KC intercept and opportunity-driven weights on  the misconception KC (termed the Opportunity-Weighted  Misconception KC Model). We compare the models in terms of  how well they predict student performance in the dataset. We also  offer qualitative interpretations of how the model predictions are  changed by the additions of a misconception KC intercept and  opportunity-driven misconception KC weights, respectively.    4. CASE STUDY: MODELING A  MISCONCEPTION IN FRACTION  ARITHMETIC LEARNING  4.1 The Fraction Arithmetic Dataset  In order to explore whether modeling a misconception as a KC in  the method described in Section 3 can improve a models  predictive fit to real data and yield interpretable insights, we  applied the method to a subset of a fraction arithmetic dataset  [16], available from the PSLC DataShop [9]. In this subset, 40  students experienced various fraction arithmetic problem types in  a mixed, or interleaved, order. Each student completed 24  problems in which they multiplied fractions (Multiply), 10  problems in which they added fractions with same denominators  (AddSameDen), and 14 problems in which they added fractions  with different denominators (AddDiffDen). Figure 1 shows the   interface that students saw for each problem type.   The specific misconception that we modeled here is an incorrect  strategy that students commonly apply to fraction arithmetic  problems known as the Independent Whole Number Strategy [8,  15]. In this strategy, students simply apply the operator to the two  numerators to generate the numerator of the solution and apply the  operator to the two denominators to generate the denominator of  the solution. For example, given a problem like 0< +  < =, the   Independent Whole Number Strategy yields an incorrect answer  of =>. In some cases, like in fraction multiplication problems (e.g.,  0     = > ), it yields the correct answer (   = <  ).   For our baseline KC model, we used one that was generated by  hand by a domain expert. The way these KCs map to the different  problem steps in the dataset is shown in Table 1 (under the  Regular KCs heading). Here, problem steps map directly onto  the interface elements present in the fraction arithmetic tutor  (Figure 1).  The Convert-Checkbox KC represents the skill of  knowing when one does or does not need to convert the  denominators to a common denominator. There is a KC for  knowing and carrying out the procedures for each of the three  problem types (Multiply, AddSameDen, and AddDiffDen).  Finally, there are two additional KCs that are required for certain  problem steps in AddDiffDen problems. These skills are finding  the least common denominator (LCD) and finding equivalent  fractions (i.e., filling in the numerators correctly after computing  the LCD). They were tagged for the problem steps involving the  fractions in the top row of Figure 1d, because they are skills that  are more complex than simply operating on whole numbers or  copying numbers.   4.2 The IWNS Misconception KC  The misconception KC was coded to predict what the outcomes  would be for each problem step if students were simply applying  the Independent Whole Number Strategy. It is coded as 1 for   Figure 1: Example interfaces for the fraction arithmetic tutor: (a) AddSameDen problems, (b) Multiply problems, (c) AddDiffDen problems before the student checks the box to convert, and (d)  AddDiffDen problems after the student checks the box to convert.     problem steps on which applying the Independent Whole Number  Strategy yields the correct answer and -1 for problem steps on  which applying it yields the incorrect answer. Problem steps for  which it does not apply are coded as 0.   The IWNS happens to yield the correct numerator and  denominator for Multiply problems. Thus, it is coded as 1 for  those Multiply problem steps. It happens to yield the correct  numerator but the wrong denominator for AddSameDen  problems. For example, applying the strategy to the problem 0# +< #  yields   = 0<, whereas the correct answer should be   = #. Thus, its   coded as 1 for the AddSameDen numerator but -1 for the  AddSameDen denominator. Applying the strategy yields the  wrong answer for both the initial numerator and denominator of  AddDiffDen problems (top row, left fraction), so its coded as -1  for those steps. In an AddDiffDen problem (e.g., 0 +  > # ), once the   student has computed the least common denominator (12) and the   equivalent fractions containing that denominator (  =0< + 0  0< ),   applying the strategy to the two equivalent fractions yields the  correct final numerator (13) but the incorrect final denominator  (24). Thus, the correct final numerator of an AddDiffDen problem  is coded as 1 and the correct final denominator is coded as -1.   Finally, applying the strategy typically involves bypassing the  conversion checkbox, which happens to be the correct move for  Multiply and AddSameDen problems (coded as 1 for the Convert- Checkbox step on those problems) but incorrect for AddDiffDen  problems (coded as -1 for the Convert-Checkbox step on those  problems).   4.3 Model Comparison Results  We fit the Additive Factors Model with different KC models to  the data and evaluated relative model fit using the Akaike  Information Criterion (AIC) and the Bayesian Information  Criterion (BIC). Both criteria are likelihood-based measures of     Table 1: Representation of how the Regular KCs and the IWNS Misconception KC map to the different problem steps in the  dataset.     predictive fit that penalize for model complexity. AIC, in  particular, is known to be asymptotically equivalent to cross- validation [2]. For both criteria, lower numbers indicate a better  relative model fit. Models were fit using the glmer() function from  the lme4 package in R [18].   We first fit the Original KC Model (AFM with the 6 regular KCs  only). The model fit the data with an AIC of 4238 and a BIC of  4328. Its parameters estimates are shown in Table 2. For each KC,  the first line shows the intercept estimates (the baseline easiness  of the KC) and the subsequent line shows the slope estimates  (how much students improve on that KC with each additional   practice opportunity). In this model, all slope estimates are  positive and statistically significant at p < 0.05, indicating general  improvement on all KCs as students progressed through the tutor.   We then fit the Misconception KC Model (AFM with the regular  KCs plus an intercept for the IWNS Misconception KC). The  IWNS Misconception KC intercept estimates the overall strength  of the misconception throughout the tutor. In this model, there  was no parameter allowing the strength of the misconception to  change across practice opportunities. This model fit the data with  an AIC of 4119 and a BIC of 4216. The drop in these values  indicates a major improvement in model fit with the addition of an     Table 2: Coefficient estimates and p-values for the three models, which differed in their inclusion of misconception KC  related parameters. Misconception KC related parameter estimates are bolded and italicized.     intercept parameter for the IWNS Misconception KC. An  ANOVA chi-square test between the Original KC Model and the  Misconception KC Model showed that the Misconception KC  Model was a significantly better fit to data (p < 0.0001).   The parameters estimates of the Misconception KC Model are  shown in Table 2. The coefficient estimate for the IWNS  Misconception KC intercept was 0.626 and was a significant  predictor (p < 0.0001). Thus, the model estimates that the IWNS  misconception was a significant contributor to explaining  performance across students use of the fraction arithmetic tutor.   The comparison between the Original KC Model and the  Misconception KC Model predictions, aggregated across students  and plotted separately for each regular KC, is shown in Figure 2.  Differences between the models predictions are observed for the  Convert-Checkbox, AddSameDen Procedure, AddDiffDen  Procedure, and Find Equivalent Fraction KCs. These are the KCs  for which the IWNS Misconception KC makes divergent  predictions about performance (i.e., has -1 or 0 on problem steps  where the regular KC predicts 1).   In particular, the Misconception KC Model predicted specifically  poor performance on the top row left-fraction numerator, relative  to performance on the top row right-fraction numerator, in   AddDiffDen problems. The Original KC Model predicted  equivalent performance on these two problem steps because they  involve application of the same positive skill (finding an  equivalent fraction). The divergence in these two models  predictions is evident in the smooth vs. jagged prediction lines for  the Find Equivalent Fraction KC (Figure 2). The Misconception  KC Models prediction line is jagged because the top row left- fraction numerator, on which it predicts worse performance,  constitutes every other problem step for the Find Equivalent  Fraction KC.   Finally, we fit the Opportunity-Weighted Misconception KC  Model (AFM with the regular KCs, an IWNS Misconception KC  intercept, and separate slopes reflecting the influence of  opportunities to strengthen vs. opportunities to weaken the  misconception. This model fit the data with an AIC of 4093 and a  BIC of 4203. The further drop in these values reflects an  improvement in the fit of this model, even compared to The  Misconception KC Model. An ANOVA chi-square test between  the models showed that the Opportunity-Weighted Misconception  KC Model was a significantly better fit than the Misconception  KC Model (p < 0.0001).   The parameters estimates of the Opportunity-Weighted  Misconception KC Model are shown in Table 2. The coefficient   Figure 2:  A comparison between the Original KC Model (thicker blue line) predictions and the Misconception KC Model  predictions (thinner red line). The thin, dotted black line represents the actual data, aggregated across students and plotted  for each regular KC. Points at which the model predictions diverge reflect problem steps where the IWNS Misconception  KC predicts different performance than the regular KCs do.     estimate for the IWNS Misconception KC intercept was 0.366 and  was a significant predictor at p < 0.0001). The coefficient estimate  for opportunities to strengthen the misconception was 0.038 and  was significant at p < 0.0001. This reflects an increase in the  weight of the misconception with each additional opportunity on  which applying it yields the correct answer, thus creating positive  feedback for the misconception. The coefficient estimate for  opportunities to weaken the misconception was -0.052 and was  significant at p < 0.0001. This reflects a decrease in the weight of  the misconception with each additional opportunity to apply it,  yield an incorrect answer, and receive negative feedback. These  coefficient estimates are consistent with our intuitions about the  effect of different types of practice opportunities on strengthening  and weakening the misconception.   Although the predictions of the Misconception KC Model and the  Opportunity-Weighted Misconception KC Model are similar, the  intensity of the effects on predicted performance is modulated  across opportunity counts in the latter model. Sometimes the  effects of the misconception are more extreme in the Opportunity- Weighted Misconception KC Model, likely reflecting the local  effects of opportunities to strengthen the misconception. At other  points, the effects of the misconception are milder, likely  reflecting the local effects of opportunities to weaken the  misconception.   4.4 Individual Differences in Misconception  Weighting  One of the more interesting applications of modeling  misconceptions is the potential to detect individual student-level  misconception weightsthat is, to discover differences in the  degree to which individuals have this misconception. Here, we  test the ability of our misconception modeling method to detect  these differences. We then validated the models estimates against  individual students proportion IWNS errors in their pre-test  scores. Previous evidence shows that fitting slopes to individual  students leads to overfitting in the Additive Factors Model [14].  Thus, here we focused on modeling individual student-level  misconception intercepts only.   To this end, we extended the Misconception KC Model to not  only include an overall intercept for the IWNS Misconception KC  but an additional separate intercept for the misconception for each  student, modeled as a random effect. We then examined the  relationship between individual students estimated misconception  intercepts and the proportion of their overall errors that were  IWNS errors at pre-test.   A Spearmans correlation between the models individual student  estimates for the IWNS Misconception KC intercept and their  proportion of IWNS errors at pre-test was statistically significant  (R=0.46, p=0.002). This provides evidence that the model can  produce externally valid estimates of the degree to which each  student is likely to have a particular misconception.   4.5 Summary & Discussion  In the present case study, we fit the Additive Factors Model with  differing KC models to data from a fraction arithmetic tutor. In  particular, we examined the effects of adding a misconception to  the KC Model, or Q-Matrix. Here, the Independent Whole  Number Strategy (IWNS) Misconception KC was coded as 1 for  any problem steps on which applying the IWNS yields the correct  answer and coded as -1 for any problem steps on which applying  the IWNS yields the incorrect answer.   Results showed that adding an intercept for this IWNS  Misconception KC improved the models fit to data, as evidence  by a significant drop in model likelihood (with significance  assessed via ANOVA chi-square tests), even after controlling for  adding a parameter. Qualitative analyses of the problem steps on  which the Original KC Model and the Misconception KC Model  diverge in their predictions show that explicitly modeling a  misconception KC can yield different instructional implications.  For example, in AddDiffDen problems, the Misconception KC  Model selectively predicts poor performance on the top row left- fraction numerator but not the top row right-fraction numerator.  The Original KC Model predicted equivalent performance on  these two problem steps because they involve application of the  same positive skill, finding an equivalent fraction. The critical  insight is that the Original KC Model and Misconception KC  Model differentially attribute the cause of errors on the top row  right-fraction numerator, and the different causes imply different  instructional strategies. The Original KC Model attributes the  errors to a students lack of ability to fill in the numerator by  finding the fraction equivalent to the one given in the problem.  Based on this, an instructional implication might be to give the  student further practice on finding equivalent fractions. The  Misconception KC Model, on the other hand, would attribute the  errors to the presence of the IWNS misconception. Based on this,  an instructional implication might be to explicitly draw the  students attention to their misconception and explain why its ill  conceived.   Adding parameters to separately estimate the influence of  opportunities to strengthen and opportunities to weaken the  misconception also significantly improved model fit beyond just  including the KC intercept. The models parameter estimates  showed that opportunities to strengthen the misconception did  significantly increase its weight, and opportunities to weaken the  misconception did significantly decrease its weight. Based on this,  one instructional implication might be to ensure that students do  not receive too many opportunities in a row that can potentially  strengthen the misconception before intervening with  opportunities that can potentially weaken it.   Finally, we extended the model to estimate individual student  IWNS Misconception KC intercepts. The extended model yielded  coefficient estimates that were significantly correlated with  students proportion of IWNS errors at pre-test. This shows that  this method of modeling misconceptions can produce externally  valid estimates of the degree to which each student is likely to  have a particular misconception.   5. GENERAL DISCUSSION  We have demonstrated a method of explicitly representing  misconceptions in KC models and illustrated some of the potential  benefits of doing so. In the fraction arithmetic case study,  explicitly modeling a misconception allowed the  disproportionately high number of errors in the top row left- fraction numerator of AddDiffDen problems to be attributed to the  IWNS misconception as opposed to a difficulty with finding  equivalent fractions. This changes the implications for instruction,  which may then be targeted towards alleviating or inhibiting the  misconception rather than giving students more practice on  finding equivalent fractions.   This method of incorporating misconception KCs into a KC  model, or Q-Matrix, makes it possible for the student model that  drives the outer loop of Intelligent Tutors to track not only  positive skills and concepts but also ill-conceived skills and     concepts. This may affect the way an Intelligent Tutor adaptively  selects problems. For example, if a student has a high estimated  misconception weight, the tutor might selectively give him/her  more practice opportunities in which applying the misconception  results in negative feedback, to weaken the misconception.  Alternatively, the tutor might adaptively present erroneous  worked examples [1] of the associated misconceptions.   This method allows for a direct assessment of the strength of a  misconception within an instructional condition, as well as for  individual students (as shown in Section 4.4). It also allows us to  assess changes in the strength of that misconception with learning.   More generally, this modeling method contributes significantly to  learning analytics by extending the flexibility of KC modeling. It  incorporates the ability to use 1 and -1 coding within a single KC  to model the ability for a (mis)conception to have both positive  and negative effects, depending on the problem step. In future  work, misconception-like KCs could also be discovered and  refined through automated, data-driven methods like Learning  Factors Analysis [4, 11]. Standardizing the use of misconception- like KCs in educational data mining workflow tools such as those  available in DataShop [9] could be a pathway to enhancing KC  model development.   There are, however, some interesting limitations to the present  method of misconception KC modeling. In order for a logistic  regression model such as AFM to reasonably fit using both  regular and misconception KCs, the misconception KCs must  predict a pattern of performance that is unique from any that can  be formed by linearly combining the predictions of regular KCs in  the existing KC model. This often requires data with a detailed  grain size. In the case study presented here, the high level of detail  came from tracking the correctness of student actions at every  unique location in the fraction arithmetic interface. This allowed  for the misconception KC to generate a unique set of predictions  across problem steps that no linear combination of regular KCs  predictions could reproduce. This is what allows the model to  attribute performance patterns to a misconception KC rather than  capturing it via a particular combination of the regular KCs in the  original KC model. It is also for this reason that we attempted  only to model one misconception within this dataset. Each  additional misconception KC that is added to the model creates  more challenges for the proper attribution of performance  patterns. Other than the Independent Whole Number Strategy,  which account for approximately 25% of all numeric (i.e., non- hint-request) errors in the process data, we did not find evidence  that any other common fraction arithmetic misconceptions  accounted for a significant proportion of errors. Thus, we focused  on modeling the most common misconception KC here as a proof  of concept. Future work that models multiple misconceptions, on  datasets with appropriately detailed performance information for  reasonable attribution to both conceptions and misconceptions,  will be fruitful.   The level of detail required to uniquely differentiate conceptions  from misconceptions could also come from explicitly modeling  the specific errors that students make (i.e., what incorrect number  they tried on their attempt at a problem step). Popular student  models such as Bayesian Knowledge Tracing (and its many  contemporary variants), the Additive Factors Model, and the  Performance Factors Model [17] typically estimate only binary  outcomes, with no differentiation among incorrect responses. An  interesting line of further research would be to extend student  models to better capture different error types. This would allow   for better attribution of errors to the appropriate KCs from which  they are generated.   Logistic regression models such as AFM may also be sub-optimal  in the way they attribute both success and failure on problem steps  on which there are multiple KCs tagged [5]. Although this can  lead to misattribution for any kind of KC, it may especially be  problematic for the attribution of success/failure to regular vs.  misconception KCs. For example, when a student gets any one of  the three Multiply problem steps correct, AFM will attribute this  success equally to the positive Multiply KC and the IWNS  Misconception KC. However, this intuition seems incorrect. Most  likely, the success should be primarily attributed to either the  positive Multiply KC or the IWNS Misconception KC. A  Bayesian model that can attribute successes and failures to KCs  probabilistically, depending on the relative estimated strength of  each possible KC, may better capture this intuition. This is a  worthwhile endeavor for future research.   6. ACKNOWLEDGMENTS  This work was supported in part by a Program in Interdisciplinary  Education Research (PIER) Post-doctoral Training Grant  (#R305B110003) awarded to RL, and a PIER Pre-doctoral  Training Grant (#R305B090023) awarded to RP. Both sources of  funding are supported by the US Department of Education.   7. REFERENCES  [1] Adams, D., McLaren, B. M., Durkin, K., Mayer, R. E.,   Rittle-Johnson, B., Isotani, S., & Van Velsen, M. (2014).  Using erroneous examples to improve mathematics learning  with a web-based tutoring system. Computers in Human  Behavior, 36, 401-411.   [2] Akaike, H. (1985). Prediction and entropy. In Atkinson, A. &  Fienberg, S., (Eds.), A Celebration of Statistics. Springer:  New York, 124.   [3] Barnes, T. (2005). The Q-matrix Method: Mining Student  Response Data for Knowledge. Proceedings of AAAI 2005:  Educational Data Mining Workshop.   [4] Cen, H., Koedinger, K. R., & Junker, B. (2006). Learning  Factors Analysis: A general method for cognitive model  evaluation and improvement. Proceedings of the 8th  International Conference of Intelligent Tutoring Systems,  164-175.   [5] Cen, H., Koedinger, K. R., Junker, B. (2008). Comparing  two IRT models for conjunctive skills. Proceedings of the  9th International Conference of Intelligent Tutoring Systems.   [6] Corbett, A. T., & Anderson, J. R. (1995). Knowledge  Tracing: Modeling the Acquisition of Procedural  Knowledge. User Modeling and User-Adapted Interaction,  4, 253-278.   [7] Embretson, S.E., & Reise, S.P. (2000). Item Response  Theory for Psychologists. Mahwah, NJ: Erlbaum.   [8] Gelman, R., & Williams, E. (1998). Enabling constraints for  cognitive development and learning: Domain specificity and  epigenesis. In D. Kuhn and R. Siegler, (Eds.), Cognition,  perception and language. Vol. 2. Handbook of Child  Psychology. New York: John Wiley and Sons, 575-630.   [9] Koedinger, K. R., Baker, R. S. J. d., Cunningham, K.,  Skogsholm, A., Leber, B., & Stamper, J. (2010). A Data  Repository for the EDM community: The PSLC DataShop.  In Romero, C., Ventura, S., Pechenizkiy, M., Baker, R.S.J.d.  (Eds.) Handbook of Educational Data Mining. Boca Raton,  FL: CRC Press.     [10] Koedinger, K.R., Corbett, A.C., & Perfetti, C. (2012). The  Knowledge-Learning-Instruction (KLI) framework: Bridging  the science-practice chasm to enhance robust student  learning. Cognitive Science, 36(5), 757-798.   [11] Koedinger, K. R., McLaughlin, E. A., & Stamper, J. C.  (2012). Automated Student Model Improvement.  Proceedings of the 5th International Conference on  Educational Data Mining.   [12] Koedinger, K. R., McLaughlin, E. A., Stamper, J. C., &  Nixon, T. (2013). Using data-driven discovery of better  student models to improve student learning. Proceedings of  the 16th International Conference on Artificial Intelligence  in Education.   [13] Kulikowich, J. M., & Alexander, P. A. (1994). Evaluating  students errors on cognitive tasks: Applications of  polytomous Item Response Theory and log-linear modeling.   In Reynolds, C. R. (Ed.), Cognitive Assessment: A  Multidisciplinary Perspective. New York, NY: Springer  Science, 137-154.   [14] Liu, R., & Koedinger, K. R. (2015). Variations in learning  rate: Student classification based on systematic residual error  patterns across practice opportunities. Proceedings of the 8th  International Conference on Educational Data Mining.   [15] Ni Y., & Zhou, Y. D. (2005). Teaching and learning fraction  and rational numbers: the origins and implications of whole  number bias. Educational Psychologist, 40, 2752.   [16] Patel, R. Fraction Addition and Multiplication.  pslcdatashop.web.cmu.edu/DatasetInfodatasetId=1190.   [17] Pavlik, P. I., Jr., Cen, H., & Koedinger, K. R. (2009).  Performance factors analysis  A new alternative to  knowledge tracing. Proceedings of the 14th International  Conference on Artificial Intelligence in Education, 531538.   [18] R Core Team (2013). R: A language and environment for  statistical computing. R Foundation for Statistical  Computing, Vienna, Austria. http://www.R-project.org/.   [19] Siegler, R. S., & Crowley, K. (1991). The microgenetic  method: a direct means for studying cognitive development.  American Psychologist, 46(6), 606620.   [20] Tatsuoka, K. K. (1983) Rule space: An approach for dealing  with misconceptions based on item response theory. Journal  of Educational Measurement, 20, 345-354.   [21] Van Lehn, K. (2006). The behavior of tutoring systems.  International Journal of Artificial Intelligence in Education,  16, 227-265.       "}
{"index":{"_id":"46"}}
{"datatype":"inproceedings","key":"Bakharia:2016:RSL:2883851.2883882","author":"Bakharia, Aneesha and Kitto, Kirsty and Pardo, Abelardo and Gavsevi'c, Dragan and Dawson, Shane","title":"Recipe for Success: Lessons Learnt from Using xAPI Within the Connected Learning Analytics Toolkit","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"378--382","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883882","doi":"10.1145/2883851.2883882","acmid":"2883882","publisher":"ACM","address":"New York, NY, USA","keywords":"CLA toolkit, CLRecipe, architecture, learning analytics, learning record store, xAPI","abstract":"An ongoing challenge for Learning Analytics research has been the scalable derivation of user interaction data from multiple technologies. The complexities associated with this challenge are increasing as educators embrace an ever growing number of social and content-related technologies. The Experience API (xAPI) alongside the development of user specific record stores has been touted as a means to address this challenge, but a number of subtle considerations must be made when using xAPI in Learning Analytics. This paper provides a general overview to the complexities and challenges of using xAPI in a general systemic analytics solution - called the Connected Learning Analytics (CLA) toolkit. The importance of design is emphasised, as is the notion of common vocabularies and xAPI Recipes. Early decisions about vocabularies and structural relationships between statements can serve to either facilitate or handicap later analytics solutions. The CLA toolkit case study provides us with a way of examining both the strengths and the weaknesses of the current xAPI specification, and we conclude with a proposal for how xAPI might be improved by using JSON-LD to formalise Recipes in a machine readable form.","pdf":"Recipe for Success  Lessons Learnt from Using xAPI  within the Connected Learning Analytics Toolkit  Aneesha Bakharia Information Systems School  Queensland University of Technology  Brisbane, Australia aneesha.bakharia@qut.edu.au  Kirsty Kitto Information Systems School  Queensland University of Technology  Brisbane, Australia kirsty.kitto@qut.edu.au  Abelardo Pardo School of Electrical and Information Engineering The University of Sydney  Sydney, Australia abelardo.pardo@sydney.edu.au  Dragan Gaevic Moray House School of Education and School of  Informatics University of Edinburgh  Edinburgh, Scotland dgasevic@acm.org  Shane Dawson Teaching Innovation Unit  University of South Australia Adelaide, Australia  Shane.Dawson@unisa.edu.au  ABSTRACT An ongoing challenge for Learning Analytics research has been the scalable derivation of user interaction data from multiple technologies. The complexities associated with this challenge are increasing as educators embrace an ever grow- ing number of social and content-related technologies. The Experience API (xAPI) alongside the development of user specific record stores has been touted as a means to ad- dress this challenge, but a number of subtle considerations must be made when using xAPI in Learning Analytics. This paper provides a general overview to the complexities and challenges of using xAPI in a general systemic analytics solution - called the Connected Learning Analytics (CLA) toolkit. The importance of design is emphasised, as is the notion of common vocabularies and xAPI Recipes. Early decisions about vocabularies and structural relationships be- tween statements can serve to either facilitate or handicap later analytics solutions. The CLA toolkit case study pro- vides us with a way of examining both the strengths and the weaknesses of the current xAPI specification, and we con- clude with a proposal for how xAPI might be improved by using JSON-LD to formalise Recipes in a machine readable form.  Categories and Subject Descriptors D.2.8 [Software Engineering]: Metricscomplexity mea- sures, performance measures; D.2.10 [Software Engineer- ing]: DesignRepresentation; E.2 [Data Structures]: Data Storage RepresentationComposite structures, Linked rep-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883882  resentations, Object representation  Keywords xAPI, CLA toolkit, CLRecipe, Architecture, Learning Ana- lytics, Learning Record Store  1. INTRODUCTION Learning Analytics has evolved as a field of research that  uses data driven methods to improve student learning pro- cesses and outcomes [21]. However, the learning process is complex and influenced by a wide variety of contextual and personal factors. Researchers have suggested that the true potential to oer meaningful insight comes from combin- ing data from across dierent sources [17]. However, stu- dent learning data is commonly generated from numerous platforms, often with dierent underlying data structures. Hence, establishing a combined data set can be a challeng- ing task. For example, a video annotation platform may be able to provide detailed accounts of its specific events (e.g., play, pause, stop, annotate, and comment), but when this data is combined with another dataset extracted from a social network platform the intersection in vocabulary may not overlap or be consistent. If these two data sets are to be used in the context of a learning experience, their terms, objects and actions need to be reconciled into a common notation, often a time consuming and dicult task.  The challenges involved in collating and analysing dis- tributed learning events are well documented in the Learn- ing Analytics literature, and numerous data formats have been proposed as potential solutions, including: Contextu- alised Attention Metadata [19], learning context ontologies (LOCO framework) [13], and ontologies for organizational learning (IntelLEO framework) [20]; and the very recently (October 2015) released IMS Caliper [3].  http://dx.doi.org/10.1145/2883851.2883882   1.1 Experience API The Experience API (xAPI), provides a platform-neutral  formalism to collect events occurring in any learning experi- ence. xAPI was released in 2013 as the outcome of an ADL project that aimed to both: (i) improve interoperability be- tween elearning systems that collect and exchange student learning data, and (ii) overcome the limitations of SCORM [2]. The xAPI specification [4] describes the format to rep- resent discrete learning activities (as JSON statements) and the requirements for Learning Record Stores (LRS) that are able to collate and exchange learner records. The xAPI statement data format is based on WC3 Activity Streams 1.0 [1] with notable changes made to include results and context for a learning activity [8]. The design of xAPI has been influenced by the socio-cultural framework of Activity Theory [22] with the unit of analysis being the activity. As Activity Theory is closely related to constructivist learning theory, Kevan and Ryan [14] suggest that xAPI is ideally suited to tracking constructivist learning activities. How- ever, the xAPI specification has no defined core vocabulary. In this case, the community is required to both define and share the structure of xAPI statements and the vocabulary as Recipes specific to a domain. These Recipes are analogous to the semantic definitions included in ontologies. Without them, xAPI only provides the syntactical structure to com- pose statements.  Example Recipes currently exist for attendance1, video interaction2 and open badges3. However, the current fo- cus in the xAPI community upon data collection tends to mean that the analytics implications of vocabulary choice are often not considered. More complex Recipes need to be defined, and we consider it essential that the Learning An- alytics (LA) community participate in this process, as poor design decisions will make it far more dicult to implement LA systems. Here, we share knowledge and lessons learned from participating in this process with the design of the Connected Learning Analytics (CLA) toolkit. We provide some of the best practice lessons that we have learned, along with guidelines for the appropriate capture and analysis of learning records.  2. EXAMPLE: THE CLA TOOLKIT The Connected Learning Analytics (CLA) toolkit [16] is  currently being created as a part of an Australian Govern- ment funded Oce for Learning and Teaching project. This project aims to collate and analyse student behaviour within defined learning activities that are run in the wild using standard social media platforms, i.e. beyond an institutions adopted Learning Management System (LMS).  The CLA toolkit is open source (GPL3.0), and imple- mented in Python, using the Django web framework. It consists of two main components:  Data Collection is achieved by interfacing with standard social media APIs to retrieve specific data about stu- dent participation in a pre-defined learning activity. This data is stored in a Learning Records Store (LRS) using the xAPI format. Full functionality is currently  1http://xapi.trainingevidencesystems.com/recipes/ attendance/0 0 1/ 2https://registry.tincanapi.com/#profile/44 3https://tincanapi.com/recipes-designers/  Figure 1: Simplified xAPI statement schema.  implemented with the Facebook, Twitter, YouTube and Wordpress platforms.  Analytics and Reporting are enabled by pulling data out of the LRS and storing it in a secondary database (presently PostgreSQL) which provides full functional- ity for querying the xAPI JSON document structure. Section 5 discusses the current reporting capabilities of the CLA toolkit, which has both student and in- structor facing dashboards.  3. DESIGNING XAPI STATEMENTS In this section, an overview of the structure of xAPI state-  ments will be presented followed by a discussion on the advantages and disadvantages that xAPI syntax flexibility brings. xAPI statements contain 3 main elements, namely metadata (i.e., id, timestamp), descriptive information (i.e., actor, verb and object) and complementary data (i.e., con- text). xAPI statements are made up of <subject>, <verb> and <object> triplets. Each verb and object in an xAPI statement requires a unique identifier that resolves to a URL that contains the required metadata although no specific schema (i.e., typing) needs to be defined.  The <subject>, <verb>, <object> triplet representation is an oversimplification of the xAPI syntax, as well as be- ing misleading in terms of the actual data required for an- alytics. Our experience in developing the CLA toolkit has shown that the correct modelling and population of contex- tual data within xAPI statements is critical. This is the context sub-section within an xAPI statement, which in- cludes the instructor, team, and other important informa- tion about the learning context. In particular, the inclusion of the platform and the ability to link the activity with a course, a course section and an instructor are all equally important from an analytics perspective and should not be neglected when designing the mandatory fields that must be populated in xAPI statements designed for Learning Ana- lytics. We consider it essential that all statements include a reference to uniquely identify the learning experience (e.g., course, event, field trip) and platform (e.g., Moodle). All statements generated by the CLA toolkit include the course code and the originating social media platform.  When comparing xAPI statements to traditional tabular log formats (e.g., Apache Server access logs or the Accumu- lator table in the Blackboard LMS), an additional advantage of xAPI emerges; the xAPI statement is in a JSON docu- ment format and is therefore able to encode multiple rela- tionships. These can be included in the grouping, parent and other sections of contextactivities. The CLA toolkit takes advantage of this functionality and is able to include multi- ple @mentions, hashtags and tags with an activity occurring on a social media platform as contextactivities.other. While the ability to include multiple objects as items in the group-  http://xapi.trainingevidencesystems.com/recipes/attendance/0_0_1/ http://xapi.trainingevidencesystems.com/recipes/attendance/0_0_1/ https://registry.tincanapi.com/#profile/44 https://tincanapi.com/recipes-designers/   ing and parent sections of contextactivities provides much more flexibility than tabular log formats, there is ambiguity as to how these relationships should be used in analytics, because multiple objects of dierent types can be included.  The extensibility of xAPI means that new formats can be defined as JSON sub-documents and incorporated into the structure of an xAPI statement. For example, the CLA toolkit uses the rating extension to include numeric ratings of social media content4. However, the ability to define new JSON constructs as extensions without the use of a man- dated JSON schema is problematic from an analytics per- spective. The provision of a JSON schema for extensions would allow the LRS and subsequent analytically process- ing code to use the information contained within the ex- tension in an automated way. There is ongoing discussion on whether JSON for Linked Data (JSON-LD) which incor- porates object and value typing should become part of the xAPI specification. We shall not discuss this point here, but will return to this question in Section 6.  4. THE IMPORTANCE OF XAPI RECIPE DESIGN  The xAPI specification initially included a core vocabu- lary but this was removed from version 0.95 onwards with ADL favouring a community driven approach to defining verbs and activities [14]. Rustici Software currently hosts a repository5 with community submitted verbs, objects and Recipes. From an analytics perspective, using a common vocabulary to represent similar activity is not just desir- able; it is a necessity given that LRSs are designed to collate xAPI statements originating from disparate systems. xAPI Recipes have been proposed to address this need [7].  As no Recipe unifying the description of learning events in social media was available, the CLA toolkit project has de- signed an open Connected Learning (CL) Recipe. CLRecipe has played a crucial role in creating a consistent data model for social media activity, and its consistency has been tested through the ecient creation of analytics and visualisations showing temporal activities, content evolution, and social network analysis. CLRecipe describes a variety of dierent learning scenarios using a unified vocabulary:  Microblogging on platforms such as Twitter and Face- book, where users only post short notes.  Content Authoring of any long text that is written by a single user (e.g. a blog post made on Wordpress).  Content Curation of a collection of artefacts (e.g., docu- ments, audio, video, images, etc.).  Table 1 contains the current mapping, which has simpli- fied aggregate analytics across social media platforms. En- forcing the mapping in the Recipe played a key role in sim- plifying the processing required to obtain social media activ- ity by platform and verb at a course and individual student level. The verb and object vocabulary used in CLRecipe were all selected from the core W3C Activity Streams 1.0 vocabulary, which was designed to provide streams of social media activity [1]. A description of each verb is available in the CLRecipe readme.MD file [15].  4http://id.tincanapi.com/extension/quality-rating 5https://registry.tincanapi.com/  While xAPI statements represent discrete social media ac- tivities, these do not occur in isolation. This is because on social media, students might interact with content created by other students (e.g., they might like and share content), or directly comment on or reply to posts to create threaded discussions. Shares, likes and replies must include a refer- ence back to the object being mentioned using contextAc- tivities.Parent, which was chosen over using contextActivi- ties.Grouping because the xAPI specification says that con- textActivities.Grouping indicates an indirect relation while contextActivities.Parent represents a direct parent-child re- lation. The inclusion of the parent id creates a reference to the statement containing the post being commented on, replied to, liked or shared. Including the parent id in the xAPI statement allows for the construction of hierarchical relationships and is used to model threaded discussions. The use of a parent id to model a tree structure in a relational database is known as the adjacency list model [9].  Recipes are very loosely defined by a textual description of the verbs, objects, extensions used. No formal schema is enforced and relationships between statements need to be manually inferred before automated analysis can be per- formed. Invariably design decisions need to be made about which elements of an xAPI statement are used and these decisions need to be known by the system performing the analysis of xAPI statements (i.e. the design decisions and rules that a Recipe serves to enforce are not described in a machine readable manner).  5. PERFORMING ANALYTICS WITH XAPI STATEMENTS  xAPI statements are stored in a Learning Record Store (LRS). The CLA toolkit uses Learning Locker, which is an open source LRS built on MongoDB (a NoSQL database). A frequent complaint about the xAPI standard concerns the limited reporting functionality of LRSs [18]. The xAPI spec- ification does not provide a RESTful interface to perform ag- gregate queries (e.g., counts of verbs and object) against the statements in a LRS. Only simple queries are allowed and all matching statements are returned in full. The inability to directly perform aggregate queries using the xAPI LRS doc- ument interface was a stumbling block for the CLA toolkit project. xAPI statements are now stored as JSON docu- ments in a PostgreSQL database where aggregate queries can easily be performed using SQL syntax. PostgreSQL has been chosen over MongoDB because of its ability to store re- lational data (required by the CLA toolkit web application) and JSON documents.  5.1 Temporal Analysis Within the CLA toolkit, temporal analysis involves ag-  gregating social media activity over time. The CLA toolkit creates graphs showing verb use (i.e., like, share, post and comment) by platform over a specified time period. In terms of processing, the star schema commonly used in traditional business intelligence (BI) applications to create high dimen- sional cubes, was used to perform aggregate counts by date. For example, a table containing dates (i.e., the date dimen- stion) is joined to a table with extracted core fields from an xAPI statement (i.e., the fact table) which is then joined to tables containing xAPI context information such as in- structor, parent and grouping (i.e., the other dimensions for    Table 1: xAPI Verb Mapping in CLRecipe.  Create Like Share Tag Rate Comment Add Facebook Post Like Share Tag - Reply - Google+ Post Like Share Tag - Reply - Twitter Tweet Favorite Retweet Hashtag - - Blog Post - - Tag Rate Comment - Pinterest Board Like Share - - - Pin YouTube Video Like Share - - Comment -  analysis). This relational design is able to facilitate dimen- sional cube creation and provide aggregates by time of day, day in week, month, and year. The inclusion of other infor- mation, such as the social media platform, a related course, and contextual information (such as a tag or @mention) pro- vides additional dimensions for analysis. While these addi- tional fields are often seen as optional in an xAPI statement, CLRecipe mandates their inclusion for the purpose of adding further meaning and insight into the analysis.  5.2 Content Analysis CLA toolkit includes algorithms for content analysis. At  present we have implemented: Topic Modelling using the Latent Dirichlet Allocation algorithm; sentiment analysis via the Valence Aware Dictionary and sEntiment Reasoner (VADER) algorithm [12]; and a Cognitive Presence classifi- cation from the Community of Inquiry model [11]. None of these analyses can be performed without access to the con- tent associated with an xAPI entry. The xAPI specification only mandates that a unique URL for each object involved in the activity is provided, and not that it is accessible by the LRS or the system processing the xAPI statements. As such, if the LRS does not store the content from xAPI statements that originate in a firewalled system, then this functionality will not be available in the CLA toolkit. For this reason, storing the content of a social media items is recommended by the CLRecipe, although care should be taken to meet the legal terms and conditions of dierent social media.  5.3 Social Network Analysis The CLA toolkit can perform SNA and displays sociograms  that are filterable by platform and date at both a course and individual level. Forum replies, blog comment threads, @mentions, likes and shares are all stored as social relation- ships between the users performing the activities. In the CLRecipe, we refer to the post being shared, liked or com- mented on, using a parent id. This addition is essential as it creates a relationship with the posts statement and allows data such as the creators details to accessed and analysed. A social relationship table is built in the CLA toolkit which contains the post creator, creation date, platform and verb. This allows for a social network to be filtered by platform, date, and user, as well as to include dierent edge relation- ships (i.e., like, share, comment and mention). In Section 4, the use of an adjacency list model to represent the parent- child relationship between statements using a parent id was discussed. The adjacency list model however requires re- cursive queries in order to rebuild the hierarchical tree from individual statements. A few other more ecient techniques have been proposed, such as the nested set model, which will be reviewed for representation within xAPI syntax as the de- velopment of the CLA toolkit progresses. This model would  facilitate far more ecient network reconstruction, as the ability to process threaded discussions is essential.  6. IMPROVING XAPI While the xAPI statement specification is both flexible  and extensible, within this paper, we have illustrated sev- eral shortcomings in relation to the way Recipes are cur- rently described. These include a lack of strict typing (see Section 3) in extensions and Recipes as well as the lack of a machine readable way to communicate the relationship be- tween statements (see Section 4). As the xAPI Data Inter- operability Standards Consortium (DISC) forms [23], we see a chance emerging to start thinking about how xAPI might be improved, and propose that this could be done with an extension of the notion of Recipes. We propose the adop- tion of JSON-LD [6] as a solution to these issues, and here we will discuss the manner in which JSON-LD introduces stricter typing and how JSON-LD framing [5] can make the relationship between xAPI statements in a Recipe explicit and therefore machine readable.  In xAPI statements, each verb and object must include the identifier for the metadata describing the main properties. However, the metadata that is required by xAPI does not include data type information and this becomes problem- atic for non-trivial statements that use extensions and/or are part of larger Recipes. JSON-LD uses the @context property (not to be confused with the context in xAPI state- ments) to specify a URI with details on each property and its associated data type. JSON-LD also includes specific object types (e.g., Person, Place and Event) and data types (e.g., date, temperature, coordinates and floating point numbers) which would be beneficial from a xAPI statement process- ing point of view. The schema that JSON-LD provides will have similar advantages to XML schema in terms of valida- tion and compliance.  {   @context : {   as :  http :// www.w3.org/ns/  activitystreams ,   ex :  http :// example.org/vocab#   },   @type :  as:Blog ,   ex:contains : {   @type :  as:Like   }  }  Figure 2: A Frame for a Activity Stream.  The example JSON-LD frame in Figure 2 can be pro-    grammatically applied to give structure to a collection of statements that adhere to a Recipe. The frame ensures that the structure is predictable and that there is only one way for the programming code to be implemented even though the relationship between xAPI statements can take multi- ple forms. Combining the stricter typing of the JSON-LD with frames for removing the ambiguity in the encoding of statement relationships, we gain a useful way of sharing ma- chine readable xAPI Recipes. JSON-LD can also easily be translated into RDF which opens up opportunities for linked semantic student knowledge graph processing [10].  7. CONCLUSION The modelling of xAPI statements explored in this paper  is based on our experiences in building learning analytics and visualisations for the CLA toolkit. Key to the cur- rent success in our project has been the careful attention paid to creating a Connected Learning Recipe. This con- sideration has facilitated the easy creation of a variety of reports common to standard Learning Analytics solutions. The key take away is that while xAPI is flexible and ex- tensible, it is essential that analytics be considered when modelling xAPI statements using Recipes. In particular, we have found adding contextual information (which is usually seen as optional extra) is key to the provision of additional dimensions for temporal analysis. In terms of social network analysis and discourse analysis, attention needs to be given to the way relationships between statements are modelled, particularly for threaded discussions. While we have empha- sised the importance of considering analytics in the creation of Recipes, the lack of machine readable Recipes is a core weakness inherent in the current specification of xAPI. For this reason we have proposed that xAPI be extended with the JSON-LD framework that has already been adopted by the Activity Streams 2.0 specification.  8. ACKNOWLEDGEMENTS Support for this project has been provided by the Aus-  tralian Government Oce for Learning and Teaching. The views in this project do not necessarily reflect the views of the Australian Government Oce for Learning and Teach- ing.  9. REFERENCES [1] Activity streams. http://activitystrea.ms/. Accessed:  2015-10-12. [2] ADL Initiative, Project TinCan.  http://www.adlnet.gov/tla/tin-can. Accessed: 2015-10-12.  [3] Caliper Analytics. http://www.imsglobal.org/activity/caliperram. Accessed: 2015-10-29.  [4] Experience API. https://github.com/adlnet/ -xAPI-Spec/blob/master/xAPI.md. Accessed: 2015-10-12.  [5] Json-ld framing specification. http://json-ld.org/spec/latest/json-ld-framing/. Accessed: 2015-10-23.  [6] Json-ld specification. http://json-ld.org/spec/. Accessed: 2015-10-23.  [7] Recipes. https://tincanapi.com/recipes-designers/. Accessed: 2015-10-23.  [8] M. Bowe. Tin Can vs. Activity Streams. http://tincanapi.com/tin-can-vs-activity-streams/, 2013. Accessed: 2015-10-24.  [9] J. Celko. Some answers to some common questions about SQL trees and hierarchies. http: //www.ibase.ru/devinfo/DBMSTrees/sqltrees.html. Accessed: 2015-10-12.  [10] S. Dietze, S. Sanchez-Alonso, H. Ebner, H. Qing Yu, D. Giordano, I. Marenzi, and B. Pereira Nunes. Interlinking educational resources and the web of data: A survey of challenges and approaches. Program, 47(1):6091, 2013.  [11] D. R. Garrison, T. Anderson, and W. Archer. Critical thinking, cognitive presence, and computer conferencing in distance education. American Journal of distance education, 15(1):723, 2001.  [12] C. Hutto and E. Gilbert. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth International AAAI Conference on Weblogs and Social Media, 2014.  [13] J. Jovanovic, D. Gasevic, C. Knight, and G. Richards. Ontologies for eective use of context in e-learning settings. Journal of Educational Technology & Society, 10(3):4759, 2007.  [14] J. M. Kevan and P. R. Ryan. Experience API: Flexible, decentralized and activity-centric data collection. Technology, Knowledge and Learning, pages 17.  [15] K. Kitto and A. Bakharia. CLRecipe. https://github.com/kirstykitto/CLRecipe, 2015.  [16] K. Kitto, S. Cross, Z. Waters, and M. Lupton. Learning analytics beyond the LMS: the connected learning analytics toolkit. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 1115. ACM, 2015.  [17] S. Knight, S. B. Shum, and K. Littleton. Epistemology, assessment, pedagogy: where learning meets analytics in the middle space. Journal of Learning Analytics, 1(1):2347, 2014.  [18] SaLTBOX. Why reporting in the LRS http://blog.saltbox.com/blog/2015/09/23/ why-reporting-in-the-lrs/, 2015.  [19] H.-C. Schmitz, M. Wolpers, U. Kirschenmann, and K. Niemann. Contextualized attention metadata. Human attention in digital environments, pages 186209, 2011.  [20] M. Siadaty, D. Gasevic, J. Jovanovic, K. Pata, N. Milikic, T. Holocher-Ertl, Z. Jeremic, L. Ali, A. Giljanovic, and M. Hatala. Self-regulated workplace learning: A pedagogical framework and semantic web-based environment. Journal of Educational Technology & Society, 15(4):7588, 2012.  [21] G. Siemens. Learning Analytics: The Emergence of a Discipline. American Behavioral Scientist, 57(10):13801400, Aug. 2013.  [22] A. Silvers. Answers: How do i get started with xAPI http://makingbetter.us/2014/11/ answers-how-do-i-get-started-with-xapi/, 2014.  [23] A. Silvers. The way of xapis consortium. xAPI Quarterly, 2015.  http://activitystrea.ms/ http://www.adlnet.gov/tla/tin-can http://www.imsglobal.org/activity/caliperram https://github.com/adlnet/-xAPI-Spec/blob/master/xAPI.md https://github.com/adlnet/-xAPI-Spec/blob/master/xAPI.md http://json-ld.org/spec/latest/json-ld-framing/ http://json-ld.org/spec/ https://tincanapi.com/recipes-designers/ http://tincanapi.com/tin-can-vs-activity-streams/ http://www.ibase.ru/devinfo/DBMSTrees/sqltrees.html http://www.ibase.ru/devinfo/DBMSTrees/sqltrees.html https://github.com/kirstykitto/CLRecipe http://blog.saltbox.com/blog/2015/09/23/why-reporting-in-the-lrs/ http://blog.saltbox.com/blog/2015/09/23/why-reporting-in-the-lrs/ http://makingbetter.us/2014/11/answers-how-do-i-get-started-with-xapi/ http://makingbetter.us/2014/11/answers-how-do-i-get-started-with-xapi/   Introduction  Experience API   Example: The CLA toolkit  Designing xAPI Statements  The Importance of xAPI Recipe Design  Performing Analytics with xAPI Statements  Temporal Analysis  Content Analysis  Social Network Analysis   Improving xAPI  Conclusion  Acknowledgements  References   "}
{"index":{"_id":"47"}}
{"datatype":"inproceedings","key":"Robinson:2016:FSA:2883851.2883932","author":"Robinson, Carly and Yeomans, Michael and Reich, Justin and Hulleman, Chris and Gehlbach, Hunter","title":"Forecasting Student Achievement in MOOCs with Natural Language Processing","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"383--387","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883932","doi":"10.1145/2883851.2883932","acmid":"2883932","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCS, learning analytics, motivation","abstract":"Student intention and motivation are among the strongest predictors of persistence and completion in Massive Open Online Courses (MOOCs), but these factors are typically measured through fixed-response items that constrain student expression. We use natural language processing techniques to evaluate whether text analysis of open responses questions about motivation and utility value can offer additional capacity to predict persistence and completion over and above information obtained from fixed-response items. Compared to simple benchmarks based on demographics, we find that a machine learning prediction model can learn from unstructured text to predict which students will complete an online course. We show that the model performs well out-of-sample, compared to a standard array of demographics. These results demonstrate the potential for natural language processing to contribute to predicting student success in MOOCs and other forms of open online learning.","pdf":"Forecasting Student Achievement in MOOCs with Natural  Language Processing  ABSTRACT  Student intention and motivation are among the strongest  predictors of persistence and completion in Massive Open Online  Courses (MOOCs), but these factors are typically measured  through fixed-response items that constrain student expression.  We use natural language processing techniques to evaluate  whether text analysis of open responses questions about  motivation and utility value can offer additional capacity to  predict persistence and completion over and above information  obtained from fixed-response items. Compared to simple  benchmarks based on demographics, we find that a machine  learning prediction model can learn from unstructured text to  predict which students will complete an online course. We show  that the model performs well out-of-sample, compared to a standard  array of demographics. These results demonstrate the potential for  natural language processing to contribute to predicting student  success in MOOCs and other forms of open online learning.    Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in  Education; Distance Learning  Massive Open Online  Course, MOOC, Learner Motivation  General Terms  Algorithms, Measurement,    Keywords  MOOCS, Learning Analytics, Motivation    1.INTRODUCTION  In massive open online courses (MOOCs), scholars have  predicted achievement-related dimensions such as persistence and  completion through tracking log data to predict which students  drop-out [2,6,13], and when [12,22]. While generally successful,  these lines of inquiry require several days or weeks of activity  data to make reliable predictions. This lag may, however, be too long.  The probability of dropout is especially high in the first days and weeks  of a course [8,13,16], before tracking log based models have sufficient  data to make good predictions. Effective early intervention efforts,  therefore, may require reliable preliminary predictors.    Additionally, it is not clear how to interpret drop-out prediction  from activity logs. This is because the activity logs are also used  to determine whether someone has dropped out. So does this kind  of model produce a leading indicator of a future drop-out, or a  lagging indicator of a recent decision to drop out This  uncertainty constrains the ability of these models to understand  the psychology of why people fail to meet their educational goals.    Many MOOC courses collect pre-course survey data on demographics  and students motivations. Research has shown that the strongest  predictor of MOOC completion at the outset of a course are students  ratings of whether they intend to complete the course, and  demographics have also been useful, to some degree [12,16]. But  beyond that, many structured survey items have proven weak  predictors of persistence and completion [5].    While intentions are a strong predictor of course completion, it is  clear that many people who intend to complete MOOCs do not do  so [8,16]. The gap between intention and action is psychologically  rich [4], and offers the most promising applications for prediction  tools. Students who wish to complete, but whose self-assessments  indicate they are unlikely to do so, may be the most receptive for  behavioral interventions, compared to students who do not have  course completion as a goal. How, then, might we be able to use  pre-course survey data to predict their likelihood of falling into  that gap   To answer this question we model course completion in line with  Eccles and colleagues expectancy-value theory [3]. In addition to   declaring their intentions, students also described how useful and  relevant the course would be to their lives (i.e., the utility value  of the course). These reasons are important for translating intention into  action. Prior empirical work showed that students perceived utility  value of their classes is correlated with their achievement [1, 9]   Carly Robinson  Harvard University   Cambridge, MA  carlyrobinson@g.harvard.edu   Michael Yeomans  Harvard University   Cambridge, MA  yeomans@fas.harvard.edu   Justin Reich  MIT   Cambridge, MA  jreich@mit.edu   Chris Hulleman  University of Virginia   Charlottesville, VA  csh3f@virginia.edu  Hunter Gehlbach  University of California, Santa Barbara    Santa Barbara, CA  hgehlbach@education.ucsb.edu  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom   Copyright is held by the owner/author(s). Publication rights licensed to  ACM.   ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883932  http://dx.doi.org/10.1145/2883851.2883932 mailto:csh3f@virginia.edu mailto:hgehlbach@education.ucsb.edu http://dx.doi.org/10.1145/2883851.2883932   and interventions that increase students perceived utility value can  have a causal effect on student success [9]. So it is possible that  heterogeneity in students utility value is also related to the  heterogeneity in their course completion rates.   One problem with this empirical approach is that open-ended text  is high-dimensional, which reflects the wide range of motivations  and goals that students have - especially in a MOOC, where the  student body is quite diverse. Still this presents a problem of how  to measure utility value quantitatively, when students are writing  qualitative statements. In the current research we employ new  methods to predict which students follow through on their  intentions to earn certification in a MOOC. Specifically, we take  advantage of the Natural Language Processing (NLP) toolbox to  better understand how students unstructured text responses in a  precourse survey can predict their later course success.    We build upon recent work that takes a similar methodology and  argues for using NLP in MOOC analyses [11,17, 21, 23]. Our own  research extends this literature in two ways. First, we apply NLP  to pre-course data, rather than in-course data, which could open  novel approaches to deploying early in-course interventions to  help learners. Second, we are using NLP to understand differences  between all people who state an intention to complete the course.  This ensures that differences in language are not simply reflecting  different intentions. Instead, they reflect variation in why students  have come upon those intentions, and how students will strive to  achieve them.   In this paper we develop an NLP model to parse structured text and  predict course completion in a MOOC. Compared to simple  benchmarks based on demographics, we find that our machine  learning prediction model makes reliable predictions of future  student achievement, even when controlling for stated intentions.  These results show that unstructured language data can predict  student success and suggest new ways to improve student  outcomes.    2. METHODS   Data for this study came a HarvardX online course that was  education-focused, and included a utility value prompt as part of  an experiment that was attempting to improve student persistence  In this class (N = 41,946 enrolled students), 47% were female,  25% lived in the United States, and 69% had a Bachelors degree.    2.1 PROCEDURES  2.1.1  Data collection  The precourse survey covered basic information about the students  and the course, including: student intentions and motivations in the  course, their prior experience with online learning, and  demographic questions. In addition, the pre-course survey also  included an open-ended utility value prompt. The prompt was  written to elicit the students expectations of the value they would  get from the course (see Figure 1; based on [10]). Specifically,  students were asked to name (a) what you are learning that is  useful to you, or (b) A specific situation in which you will use the   knowledge/skills. Based on the initial experimental design, this prompt  was randomly assigned to half the students in this course as a  manipulation [16].    2.1.2  Population filters   We restrict our analyses to a subset of students, based on several  population filters, and the full set of filters are reported in Table 1.  First, we focus only on students who enroll in the class during the  first two weeks of its six-week run, which captures most learners  who were eligible for certificates, and which excludes students  playing catch-up, which may a very different experience (the  results below are robust across a range of similar cut-offs).  Second, we cull that sample to include only the people who  finished the pre-course survey, and who were randomly assigned  to see the utility value question, since these responses were going  to be the focus of our prediction model. Third, we drop people  who do not self-report their written English as fluent, since we  do not want the automated text analysis to simply learn  differences in language skill. Finally, people who do not intend to  complete the course, or who do not respond to the utility value  prompt were dropped.   It is worth considering the consequences of our thorough filters.  We are substantially limiting the heterogeneity in our target  population, relative to the full sample of learners. This is a   deliberate choice, for two reasons. First, the narrow scope allows  us to model how otherwise-similar students vary in terms of their  utility value, that is relatively un-confounded with other sources  of variation in the broader population. Furthermore, it is worth  noting that every single filter selects for students with a higher  likelihood of completing the course. The resulting population are  the very students who should have the highest expectations of  success in the course. This makes their subsequent failure to  achieve success all the more interesting.    Table 1: List of filters applied to the student population. This  table shows the number of students remaining after each filter  is applied, and also shows the average certification rate at each   step among the remaining students.   Population Filter Students % Complete  Unique enrolled students 41,946 9.3%  Started in first two weeks 33,396 9.8%  Pre-Course Survey 9,862 28.9%  Saw Utility Value 4,930 28.9%  Fluent in Writing English 3,139 30.4%  Intends to Complete the Course 2,097 38.4%  Wrote more than one word 1,730 40.9%  Figure 1: Text of utility value question    The most important filter was to exclude those who did not intend  to complete the course. Every HarvardX survey asks a multiple  choice question about students intentions [16]. The question asks:  People register for HarvardX courses for different reasons.  Which of the following best describes you Students choose  from four response anchors:    Here to browse the materials, but not planning on  completing any course activities (watching videos,  reading text, answering problems, etc.).    Planning on completing some course activities, but not  planning on earning a certificate.    Planning on completing enough course activities to earn  a certificate.    Have not decided whether I will complete any course  activities.   Students answers to this question are plotted in Figure 2. One  clear result is that intentions matter: most of the students who  certify are ones who intended to certify, and a higher percentage of  students who intend to certify eventually do so, compared to those  who do not intend to certify. However, the results also show that  most of the students who intend to certify do not achieve that goal.  The rest of this research is focused on modeling the variation in  outcomes solely among those who say they intend to complete the  course.   2.2 Measures  We collected two types of measures in our study. First, we  collected a set of demographic and background measures from  pre-course study, shown in Table 2. These measures include  student age, gender, level of education, experience with MOOCs,  experience with the subject material, work and student status,  parental education, and residence. For the most part, students in  this course were similar to students in many edX MOOCs, in that  they are older than traditional college students, and very likely to  have a bachelors or advanced degree [7]. There is one notable  difference from the MOOC norm: the course had a majority of  female students.    In addition to collecting demographic information on participants,  the primary source of data for our analyses are the open-ended  responses from the utility value prompt. We combined all text in  both boxes to form a single document for each student. We   excluded anyone who wrote less than one word and, among the  remainder who wrote something, the average document was 34.6  words long.    The remaining documents were cleaned and compiled using a  standard NLP 9-step process. All texts were spellchecked by hand (with  software assistance), all characters were converted to lowercase, all  contractions were expanded, all punctuation was removed, common  function words (stopwords) were removed, every remaining word was  stemmed using the standard Porter stemmer, the series of stemmed words  was processed into uni- and bi-grams, a feature count matrix was  constructed using all sets of features, all features which appeared in less  than 2% of documents were removed.    This process produced a feature count matrix, in which each  document (i.e. each student) was assigned a row, while each n- gram feature (each word or phrase) was assigned a column, and the  value of each cell represented the number of times that word or  phrase was used in that document. In addition to the n-gram  features, we also included two summary features for each  document: word count, and the Flesh-Kincaid readability score.  These data comprised the entire set of language features on which  our model would train.   Demographic Population Gender (% female) 56.2%  Age 38.8 (12.4) Previous MOOCs enrolled 3.2 (3.7)  Previous MOOCs completed 2.2 (3.3) Familiar with Material (1-5) 2.8 (1.1)  Currently Employed 82.4% Currently Enrolled in School 24.8%  Bachelors Degree 77.6% Advanced Degree 50.0%  Parent with Bachelors Degree 57.9% Parent with Advanced Degree 34.0%  Lives in N. America 52.3% Lives in Europe 13.5%  Lives in Latin America 6.8% Lives in Oceania 5.3% Lives in Africa 5.0% Lives in Asia 16.8%  Table 2: Demographic characteristics of students remaining in  sample. Cell contents are percentages or else response means   (standard deviations in parentheses where applicable).  Figure 2: Course completion by student intention    2.3 Analytic Procedures  2.3.1  Model Estimation  Our natural language processing model was designed to predict  who completed the course. The underlying statistical estimator we  used was a lasso-regularized logistic regression [19]. This is a  relatively simple model within machine learning that strikes a  balance between bias and variance to make good out-of-sample  predictions in high-dimensional environments where the solution  is likely to be sparse. The lasso regularizer functions as a penalty  for complexity, and the size of that penalty is determined  empirically, as the penalty which minimizes prediction error in a  100-fold cross-validation loop.   2.3.2  Alternative Prediction Models   In addition to the NLP model, we tested several other prediction  models using different feature sets, as benchmark comparisons.  One of those benchmarks was to compare the NLP results to a  model trained on demographic variables. To do that, we created a  feature set from the demographics listed in Table 2, chosen based  on availability and on previous research [12]. We also tested a  hybrid model which used both demographic and NLP features, to  see whether they complemented one another, or merely overlapped  in what they said about students likelihood of success.   We also tested the accuracy of another language-based prediction  method, the LIWC, which uses a predefined dictionary to extract  features from the text, rather than learning features directly from the  data  [15]. In our research we did not find any combination of  LIWC features to be useful for out-of-sample predictions of  course completion, so we do not report our LIWC analyses in  detail.    3. RESULTS  The gold-standard test for model performance is prediction  accuracy among out-of-sample cases, i.e., those cases not used  in training. To accomplish this in our dataset, we used a nested  cross-validation loop, separate from the cross-validation used to  estimate the lasso parameter [20]. We used a 20-fold outer cross- validation loop, stratified to balance the proportion of successful  students in each fold. We repeated the procedure five times to  even out model instability, and report the averaged result as our  out-of-sample prediction for each student.   Because of the imbalance in outcomes (i.e. more drop-outs than  certified students) the accuracy of each set of predictions was  measured as the AUC metric generated from an ROC curve [18].  The full NLP model performed well, and better than chance  (AUC=56.4; p<.001), where students responses to the utility value  prompt predicted course completion in the hold-out sample. The  demographic predictors also performed better than chance  (AUC=56.1; p<.001), and the model put weight on only one  feature  the number of previous MOOCs completed. Importantly,  the signal from the demographic features and NLP features did not  overlap entirely  a model trained on the combined set of features  did an even better job predicting course completion than  demographics alone (AUC=59.8; p<.02). These results confirm  that the content of the students language responses to the utility  value prompt reveals an important new source of variance in their  future achievement.    The selected features are listed in Table 3, along with their  regularized coefficients within the model, as well as the  prevalence of each feature in the documents. Each row represents a  different language feature selected by the regularizer during  training. The signs of the coefficients give an indication of how the  model is learning to distinguish completers from non-completers,   though the magnitude of the coefficients are not directly  interpretable. The final column counts the prevalence of each  feature in the document set for each course, as a percent of total  documents. Of the 21 features in the model, 78% of students used  at least one. The model made a baseline prediction for the  remaining 22% of students.    To give an example of how to interpret the features, the n-gram  believ indicates all the derivatives of the word stem believe (i.e.,  belief, believing, believed, etc.). In general the contents of the  model suggest that students most interested in extrinsic rewards of the  class (e.g. get, need, career) were less likely to earn a certificate.  By contrast, students who described ways in which the material might  be applied on-the-job (e.g. engage, innovate, impact, teach)  were most likely to follow through on their intentions and complete the  course.   One interesting feature is modephys, which indicates students using  the phrase Models of Physical Design, which is the name of a unit in  the course. In fact, it is the final unit, so the negative coefficient  suggests that students who were particularly interested in this unit were  unlikely to wait through the rest of course to get to the desired material.  This was not the case for the names of earlier units in the course (e.g.   N-Gram Coefficient Prevalence get -0.2619 5.7%  modephysic -0.0985 2.8% career -0.0761 5.9% area -0.0761 2.4% need -0.0729 7.8%  practic 0.0118 5.0% set 0.0224 4.5%  new 0.0245 18.1% present 0.0281 2.5%  theorilearn 0.0337 6.1% will 0.0417 76.3%  modelearn 0.0428 10.2% see 0.0640 3.9%  profession 0.0649 5.2% teach 0.0707 26.4% believ 0.0735 6.1%  leadershipwill 0.1222 2.6% impact 0.1296 3.1%  learnbetter 0.1303 2.9% engag 0.1604 6.2% innov 0.1816 5.0%  Table 3: Language features selected by the lasso logistic  regression model. Each row represents a different language   feature selected during training. The final column counts the  prevalence of each feature, as a percent of total documents.    Theories of Learning), though further research is needed to unpack  this relationship more precisely.   4.  DISCUSSION   Compared to simple benchmarks based on demographics, we find  that a machine learning prediction model can learn from  unstructured text to predict which students will complete an online  course. We show that the model performs well out-of-sample  within a single course, and better than demographic benchmarks.  These results demonstrate the potential for NLP to contribute to  predicting student success.    It is worth discussing some limitations to our approach. First, we are only  able to make predictions for a small subset of students (i.e. those who  passed all of our population filters).  This excludes a lot of students who  are unlikely to succeed in the course. However, those students may also be  the least interested in behavioral interventions, so the applications of a  prediction model among these students may be of limited value. By  contrast, our work reveals insights into the minds of those who may want  the most help translating their intentions into achievement.    Another clear limitation is that our research is data-intensive, and requires  a large student body from which to draw language samples. However, in  both MOOCs and traditional education settings, student data is  increasingly a trackable entity. Schools increasingly administer  student perception surveys [14] and NLP methods generate new  possibilities to gain insights from student data by asking  psychologically motivated open-ended questions. This research  demonstrates that these questions can predict student persistence  and completion over and above similar fixed response items, and  this expands the scope of external validity for our results, since  language might be collected from all kinds of naturally-occurring  educational contexts.    REFERENCES  1. Bong, M. (2001). Role of self-efficacy and task-value in   predicting college students' course performance and future  enrollment intentions. Contemporary educational  psychology, 26(4), 553-570.   2. Brooks, C., Thompson, C., & Teasley, S. (2015). A time  series interaction analysis method for building predictive  models of learners using log data. Paper presented at the  Proceedings of the Fifth International Conference on  Learning Analytics And Knowledge.    3. Eccles, J. S., Wigfield, A., & Schiefele, U. (1998).  Motivation to succeed. In N. Eisenberg (Ed.), Handbook of  child psychology (Vol. 4, pp. 1017-1095). New York: John  Wiley & Sons.   4. Gilbert, D. T., & Wilson, T. D. (2007). Prospection:  Experiencing the future. Science, 317(5843), 1351-1354.    5. Greene, J. A., Oswald, C. A., & Pomerantz, J. (2015).  Predictors of Retention and Achievement in a Massive Open  Online Course. American Educational Research Journal,  0002831215584621.    6. Halawa, S., Greene, D., & Mitchell, J. (2014). Dropout  prediction in MOOCs using learner activity features.  Experiences and best practices in and around MOOCs, 7.    7. Ho, A. D., Chuang, I., Reich, J., Coleman, C. A., Whitehill,  J., Northcutt, C. G., . . . Petersen, R. (2015). HarvardX and  MITx: Two Years of Open Online Courses Fall 2012- Summer 2014. doi: 10.2139/ssrn.2586847   8. Ho, A. D., Reich, J., Nesterko, S. O., Seaton, D. T.,  Mullaney, T., Waldo, J., & Chuang, I. (2014). HarvardX and  MITx: The first year of open online courses, fall 2012- summer 2013. (HarvardX and MITx Working Paper No. 1).    9. Hulleman, C. S., Durik, A. M., Schweigert, S. B., &  Harackiewicz, J. M. (2008). Task values, achievement goals,  and interest: An integrative analysis. Journal of Educational  Psychology, 100(2), 398.    10. Hulleman, C. S., Godes, O., Hendricks, B. L., &  Harackiewicz, J. M. (2010). Enhancing interest and  performance with a utility value intervention. Journal of  Educational Psychology, 102(4), 880.    11. Joksimovi, S., Dowell, N., Skrypnyk, O., Kovanovi, V.,  Gaevi, D., Dawson, S., & Graesser, A. C. (2015, March).  How do you connect: Analysis of social capital  accumulation in connectivist MOOCs. In Proceedings of the  Fifth International Conference on Learning Analytics And  Knowledge (pp. 64-68). ACM.   12. Kizilcec, R., & Halawa, S. (2015). Attrition and Achievement  Gaps in Online Learning. Proc. of ACM Learning at Scale,  15, 14-15.    13. Kloft, M., Stiehler, F., Zheng, Z., & Pinkwart, N. (2014).  Predicting MOOC dropout over weeks using machine  learning methods. Empirical Methods on Natural Language  Processing (EMNLP) 2014, 60.    14. MET Project. (2012). Asking Student about Teaching.  15. Pennebaker, J. W., Francis, M. E., & Booth, R. J. (2001).   Linguistic inquiry and word count: LIWC 2001. Mahway:  Lawrence Erlbaum Associates, 71, 2001.    16. Reich, J. (2014). MOOC completion and retention in the  context of student intent. EDUCAUSE Review Online.    17. Reich, J., Tingley, D. H., Leder-Luis, J., Roberts, M. E., &  Stewart, B. (2014). Computer-Assisted Reading and  Discovery for Student Generated Text in Massive Open  Online Courses.    18. Robin, X., Turck, N., Hainard, A., Tiberti, N., Lisacek, F.,  Sanchez, J.-C., & Mller, M. (2011). pROC: an open-source  package for R and S+ to analyze and compare ROC curves.  BMC bioinformatics, 12(1), 77.    19. Taddy, M. (2013). Multinomial inverse regression for text  analysis. Journal of the American Statistical Association,  108(503), 755-770.    20. Varma, S., & Simon, R. (2006). Bias in error estimation  when using cross-validation for model selection. BMC  bioinformatics, 7(1), 91.   21. Wen, M., Yang, D., & Rose, C. (2014). Sentiment Analysis in  MOOC Discussion Forums: What does it tell us Paper  presented at the Educational Data Mining 2014.   22. Whitehill, J., Williams, J. J., Lopez, G., Coleman, C. A., &  Reich, J. (2015). Beyond Prediction: First Steps Toward  Automatic Intervention in MOOC Student Stopout.    23. Yang, D., Sinha, T., Adamson, D., & Rose, C. P. (2013). Turn  on, tune in, drop out: Anticipating student dropouts in  massive open online courses. Paper presented at the  Proceedings of the 2013 NIPS Data-Driven Education  Workshop.    "}
{"index":{"_id":"48"}}
{"datatype":"inproceedings","key":"Koedinger:2016:DEC:2883851.2883957","author":"Koedinger, Kenneth R. and McLaughlin, Elizabeth A. and Jia, Julianna Zhuxin and Bier, Norman L.","title":"Is the Doer Effect a Causal Relationship?: How Can We Tell and Why It's Important","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"388--397","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883957","doi":"10.1145/2883851.2883957","acmid":"2883957","publisher":"ACM","address":"New York, NY, USA","keywords":"course effectiveness, doer effect, learn by doing, learning engineering, prediction","abstract":"The doer effect is an association between the number of online interactive practice activities students' do and their learning outcomes that is not only statistically reliable but has much higher positive effects than other learning resources, such as watching videos or reading text. Such an association suggests a causal interpretation--more doing yields better learning--which requires randomized experimentation to most rigorously confirm. But such experiments are expensive, and any single experiment in a particular course context does not provide rigorous evidence that the causal link will generalize to other course content. We suggest that analytics of increasingly available online learning data sets can complement experimental efforts by facilitating more widespread evaluation of the generalizability of claims about what learning methods produce better student learning outcomes. We illustrate with analytics that narrow in on a causal interpretation of the doer effect by showing that doing within a course unit predicts learning of that unit content more than doing in units before or after. We also provide generalizability evidence across four different courses involving over 12,500 students that the learning effect of doing is about six times greater than that of reading.","pdf":"Is the Doer Effect a Causal Relationship   How Can WE Tell and Why Its Important   Kenneth R. Koedinger   Carnegie Mellon University   5000 Forbes Avenue  Pittsburgh, PA 15201   koedinger@cmu.edu   Elizabeth A. McLaughlin  Carnegie Mellon University   5000 Forbes Avenue  Pittsburgh, PA 15201   mimim@cs.cmu.edu  Norman L. Bier   Carnegie Mellon University  5000 Forbes Avenue  Pittsburgh, PA 15201  nbier@cmu.edu      Julianna Zhuxin Jia  Carnegie Mellon University   5000 Forbes Avenue  Pittsburgh, PA 15201   zhuxinj@andrew.cmu.edu         ABSTRACT  The doer effect is an association between the number of online  interactive practice activities students do and their learning  outcomes that is not only statistically reliable but has much higher  positive effects than other learning resources, such as watching  videos or reading text. Such an association suggests a causal  interpretation--more doing yields better learning--which requires  randomized experimentation to most rigorously confirm. But such  experiments are expensive, and any single experiment in a  particular course context does not provide rigorous evidence that  the causal link will generalize to other course content. We  suggest that analytics of increasingly available online learning  data sets can complement experimental efforts by facilitating  more widespread evaluation of the generalizability of claims  about what learning methods produce better student learning  outcomes. We illustrate with analytics that narrow in on a causal  interpretation of the doer effect by showing that doing within a  course unit predicts learning of that unit content more than doing  in units before or after. We also provide generalizability evidence  across four different courses involving over 12,500 students that  the learning effect of doing is about six times greater than that of  reading.     CCS Concepts   Applied computing  E-learning     Applied computing  Computer - managed instruction   Keywords  Learn by doing; prediction; course effectiveness; doer effect;  learning engineering       1. INTRODUCTION  One general challenge for learning analytics in particular, and  for learning science and practice in general, is how to reliably  determine what are the most effective methods for supporting  learning and under what circumstances do those methods work.  We argue that learning analytics has something distinct and  important to offer as an answer to these questions. Whereas  random assignment controlled experimentation is considered the  gold standard for determining whether a method of learning is  effective [9], it does not provide evidence on whether that  learning method will generalize to other courses and course  contexts. In contrast, the increasing availability of process and  outcome data from online courses [16] makes it possible to  investigate the generalizability of associations between learning  method and outcomes. Because such data comes from naturally  occurring variations in use rather than from random assignment,  we cannot be sure that those associations are causal. However,  such data adds evidence for generalization (or lack thereof) that  comes at  a  much  lower  monetary  and social/ethical  cost  than  would be needed to do random assignment experiments across all  of these naturally occurring contexts [cf., 3].   In more technical terms, an experiment provides strong internal  validity for causal inference but provides no external validity for  generalization of the method to contexts not sampled in that  experiment. Analysis of associations of method and outcome  threatens internal validity, but doing so across many naturally  occurring contexts provides external validity at a much lower cost  than doing experiments in all these contexts.    Without the costs of designing and executing any experiments, we  have five data sets that were collected as a natural part of five  courses from four different content areas. While most  experiments typically evaluate one method (with two conditions),  these data sets allow us to analyze outcomes associated with three  different methods (doing activities, reading text, or watching  video lectures) for one course and two different methods (doing  and reading) for four courses. To do 11 controlled experiments in  these real world contexts would be a much more costly  undertaking.    Another key point of the current paper is to explore analytic  techniques that help eliminate alternative causal interpretations so  as to get closer to causal inference even when the data is  correlational in character. In particular, we explore the use of  intermediate course unit quiz data to evaluate whether the same  student reveals variation in choices across units that is associated     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK 16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04...$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883957     with better learning outcomes. This cross-unit analysis is also  another test of generalization of learning method effects across the  different content types and goals of each unit.   Lower Cost Analytics Enhances Generalizability  In a randomized controlled experiment of the effectiveness of a  method for supporting learning, students are randomly assigned  to either a treatment condition, where the method is used, or a  control condition, where it is not. The effect of the conditions are  compared on how well students do on some common outcome  measure of learning achievement. Many random assignment  experiments on learning have been run in labs [e.g., 11]. To use an  example relevant to this paper, some experiments have compared  whether having students practice retrieving facts (e.g., What is the  Chinese word for teacher) yields better learning than having  students study facts (e.g., The Chinese word for teacher is  lao3shi1). Such experiments have demonstrated, with statistical  reliability, better long term learning outcomes from retrieval  practice (also called testing) than from studying [12, 14].  Random assignment experiments like these in the lab have high  internal validity [1, 18] meaning that we can be confident that the  method (e.g., more testing) is causing the better outcomes.    Some critics wonder whether these lab results generalize to the  classroom and thus randomized experiments run within real  courses are sometimes performed, though often at greater cost in  terms of both real and social/ethical capital. Classroom studies of  the testing effect [e.g., 8] add ecological validity by being  performed with real students and in the context of real courses.  Even here the few experiments in a limited number of course  contexts may leave a critic wondering whether these testing  effects generalize across all course content or might be limited to  certain kinds of content or contexts. In fact, many of these studies  have focused on the learning of facts and verbally communicated  content [6, 11]. Perhaps the testing effect is less relevant to the  learning of skills or principles. We do not have sufficient external  validity evidence. Research on worked examples [e.g., 4, 11, 13,  15] suggests limits on the testing effect -- these studies find that  too much practice and not enough studying can yield poorer  learning outcomes. More generally, the KLI Framework [6]  outlines empirical and theoretical reasons to believe that many  methods for learning and instruction do not generalize across  course content, that is, they work well for some kinds of  knowledge acquisition but not for others (e.g., sense making  support is best for learning principles but less for skills and  arbitrary facts, inductive learning support is best for skills but not  for facts, and memory supports are best for facts).   In addition to evidence for the testing effect, there is broader  advocacy and evidence for related notions of learning by doing  [e.g., 2] and active learning [e.g., 20]. These terms are arguably  less precise than the testing effect as they cover a wider set of  approaches and are more loosely defined. Testing effect studies  typically involve immediate feedback on responses that provide  students with an example correct response if they fail to retrieve  one themselves. In cases where retrieval practice conditions do  not involve feedback, smaller learning outcomes are observed  (e.g., see Table 1 in [14]). Timely feedback is employed in many  learning by doing and active learning applications, however,  learning by doing is also often used to refer to project-based,  constructivist, or more open-ended inquiry approaches where  instructional supports such as immediate interactive feedback on  student progress are deemphasized and even discouraged [c.f., 4].  There is some large-scale evidence of classroom-based benefits of  interventions emphasizing learning-by-doing or active learning   including quasi- experiments with non-randomly assigned  controls [e.g., 5, 20, 21] and classroom-based experimental trials  with random assignment [e.g., 10,17]. However, in all these  cases, the treatment conditions vary in many ways from the  control conditions so as to not isolate active doing as a causal  ingredient.   It should be clear that determining causal relationships is  important for scientific and practical reasons because causal  relationships provide a path toward explanatory theory and a path  toward reliable and replicable practical application. Further, we  need evidence for causes that generalize across a wide variety  instructional contexts and course content. If we can  be  increasingly certain a learning method is causally related to more  optimal learning across a wide variety of contexts and content,  then that method should be used to guide course design and  students should be encouraged to use it. Coupling evidence from  both experiments and analysis of naturally occurring high-volume  data appears an effective way to increase generalizable certainty.   Toward Explaining the Doer Effect and Exploring its  Generality  In prior work, we found that different student choices of  learning methods (e.g., doing interactive activities, reading online  text, or watching online lecture videos) are associated with  learning outcomes [7]. More usage in general is associated with  higher outcomes, but especially for doing activities which has an  estimated 6x greater impact on total quiz and final exam  performance than reading or video watching. One open question  regarding this doer effect is whether the observed association is  indicative of a causal relationship, that is, that students learn more  as a consequence of doing more. Alternatively, there may be no  causal link between the two, but rather some  third  variable  common cause, such as general student motivation to learn, that  leads to both more doing and better learning. The fact that the  effect of doing activities is much stronger than that of watching  videos or reading text suggests that the third variable cannot be  general to all learning methods but would have to be particular to  doing -- something like: students who are generally good learners  desire to demonstrate their competence (show off) and doing is  a better way to do so than reading or watching.    In this paper we introduce an analysis approach designed to probe  any such general student trait explanation in contrast to the causal  explanation. This technique relies on course data involving  repeated unit assessments throughout the course with process data  on student use of different learning methods relevant to the unit  between these assessments. If the causal explanation is correct,  then the amount of doing a student chooses to engage in during a  unit should be predictive of their performance on that unit  assessment above beyond any effect of the amount of doing  outside that unit. In contrast, if some general trait is an  explanation then the amount of doing outside a unit should be  equally predictive (or more because there is more data outside a  unit) of that units assessment results as the amount of doing  within that unit. Statistically, a regression model should reveal no  within-unit effect above and beyond the outside-unit effect.   A second open question we explore is whether the doer effect  generalizes across multiple online courses. We do so with data  from four Open Learning Initiative online courses (Concepts in  Computing, Introduction to Biology, Introduction to Psychology,  and Statistical Reasoning) on associations between student  variation in the amount of doing and their learning outcomes and  between student variation in the amount of reading and their     learning outcomes. We focus our investigations on these specific  research questions:   1. Can we use cross-course performance data to narrow down the  possible causal interpretations of the doer effect   a. How do individual student resource choices vary across units  of a course   b. Is student performance in each unit better predicted by how  much they do in those units than by how much they do in other  units   c. Might course unit prerequisite relationships contribute to  cross-unit doer effects   2. Does the doer effect generalize to different courses or to different  students using similar course materials      2. CROSS-COURSE PERFORMANCE  DATA TOWARD BETTER CAUSAL  EVIDENCE FOR THE DOER EFFECT  2.1 Method: Context and Nature of Data  This analysis involves data from students taking the Introduction  to Psychology as a Science MOOC course offered by Georgia  Institute of Technology through Coursera. This is the same data  used in [7] and more details about the course, about student  characteristics, and about factors leading to drop out can be found  in that paper. Here we focus on the students who finished the  course and took all or most of the 11 quizzes (N=1154). Of these  students, most of them (N=1051) opt to make at least some use of  corresponding online materials (readings and interactive  activities) from the Open Learning Initiative (OLI) course titled  Introduction to Psychology offered by Carnegie Mellon  University.   The course is designed to introduce college students to the broad  topics in the discipline of psychology. The 12-week course  includes video lectures on each topic (e.g., biopsychology,  sensation and perception, learning) presented by the course  professor. In addition, each topic is aligned with modules from  the Introduction to Psychology OLI course that students are  encouraged to use as an online textbook and practice  environment. The course syllabus maps the topics of the lectures  to the OLI modules for each week. Thus, if students take  advantage of the course offerings their learning environment  includes watching videos in Coursera, reading OLI text pages,  and doing OLI interactive activities. The Coursera portion also  includes a discussion board, which we do not analyze here, but it  is addressed in other research [19].   Interactive activities are aligned with course learning objectives  and are embedded in the course content.  They provide  opportunities for students to test their understanding of concepts  and to practice skills. Such learning opportunities take various  formats (e.g., multiple choice questions, interactive simulations,  drop and drag, matching, and other options) and deliver  immediate tailored feedback as-needed (e.g., when a selected  answer is incorrect) or as-requested (e.g., in the form of a hint).  Many activities are multiple-choice questions, but others, like  those shown in Figure 1, provide other forms of response  selection (1a) and response construction, including the open- ended submit and compare (1b).  In all cases, students have  immediate access to correct responses.   After each video lecture for the first 11 weeks of the course, there  was a quiz. After the final week, students took a cumulative final  exam. For our by-unit analysis of student activity to outcome  associations, we focus on their activities in the 11 units associated  with the 11 weekly quizzes. Each quiz had 10 items on it. Across  all students and all quizzes the average quiz performance was 8.2  out of 10.   Although the course had weekly quizzes aligned to the content for  that week, students had some flexibility for when they took a  specific weeks quiz. Given this autonomy and the resulting  variance in quiz start times, resource data frequencies were  individualized per student. Two factors were used for attaching  unit resource data to a weekly quiz: time and content. Data was  mapped to course content using the syllabus and OLI modules for  reading and doing, and urls were used for watching (videos had an  assigned unit number). For each student, all relevant data per quiz  was tallied until the start time for that quiz, all remaining data was  considered irrelevant. Therefore, all readings, activities and videos  associated with the content before a quiz is taken are deemed  relevant.    2.2 Results:  Variation in Student Choices  Before investigating whether within-unit choices better predict  unit outcomes than outside-unit activities, we first verify that there  is sufficient variation in individual student resource use choices  across units of the course to justify our further analysis. We  wanted to determine whether students vary in how active they are  during each weekly unit of the course. If students who do a lot of  activities always do a lot and those that do few always do few,  then the by-unit analysis we propose will be uninformative.   To check for variability, we used the activity data from the 1051  students who accessed at least some pages or activities in OLI  (103 students in our sample did not). Each of these students  worked through 11 units, producing 11,561 (= 1051 x 11) student-  unit combinations. For each of these student-unit combinations  we compared students level of activity within the target unit to  their activity outside of it. No surprise, these measures are highly  correlated, R = .68. However, there is variation. To investigate  how much, for each quiz we grouped students into 5 groups  (quintiles) based on their within-unit activity and 5 quintile groups  based on their outside-unit activity. As shown in the bottom row  of Table 1, outside-unit quintile boundaries produce reasonably  consistently sized groups (a consequence of having lots of  opportunity for different levels of outside-unit activity counts  from the 10 units outside each unit). About 20% of students are in  each of the lowest or 1st quintile (below about 8 outside-  activities), the 2nd quintile (below about 185), 3rd quintile (below  about 426), 4th quintile (below about 538), and the highest or 5th  quintile (at or above about 538). Within-unit quintile boundaries  vary more because the number of activities available and done  within a unit changes quite a bit and in some cases is  small  enough to yield issues where whole number quintile cut-offs  produce quintile groups of different sizes (e.g., in units  9-11  where there is a median of 15, 11, and 2 activities done, more than  40% of the students did 0 activities so there is no way to  differentiate the first and second quintile -- all such students are in  the 1st quintile and no students are in the second).   In 6266 instances or 54% of the student-unit combinations, the  within-unit activity quintile was different from the outside-unit  activity. In 1464 instances or 13% of the cases, the quintile was  different by two levels (a difference of more between 20-40  percentile points). For example, of all the student-units in the 3rd     (a)     (b)     Figure 1: A sample of interactive activities from the Brain Regions module of the OLI course used in the Psychology MOOC. The  example on top (a) illustrates a machine-gradable alternative to multiple-choice with vastly more choices (8! = 40,320). The bottom  of the figure (b) is an example of an open-ended submit and compare question, where students can compare their submitted  response to an example correct response. In all OLI activities, students have immediate access to correct responses.     Table 1: Student activity within each unit compared with their activity outside that unit.   Outside-Unit Activity Quintile    Within-Unit  Activity Quintile   Low 2nd 3rd 4th High  20% 20% 20% 20%  20%   Within  Totals    2244 1406 473 82 26    57 308 135 32 11   53 505 1176 880 598   5 34 401 989 1080   1 22 127 338 578         quintile of outside-unit activity (2312 of them), there are  473 cases (20%) where the within-unit activity is quite a bit  lower (20- 40 percentile points) than outside-unit activity and  127 cases (5%) where the within-unit activity is quite a bit  higher than outside activity. In summary, we do find lots of  cases where students chose to do many fewer or many more  activities than they tend to do otherwise. This natural  variability opens the door to analyzing whether within-unit  activity is predictive of learning unit content above and  beyond outside-unit activity.   2.3 Results: Association of Within-Unit and  Outside-Unit Choices with Learning  Outcomes  To investigate the association of within-unit and outside-unit  choices with learning outcomes, we used mixed effect linear  regression modeling, implemented using lmer function in R,  an open statistical application. We aggregated log data from  Coursera and OLI into a file with 11561 rows for the 1051  students and each of the 11 units. The outcome or  dependent measure is the unit quiz score for the given student  and unit. To derive predictor (or independent) measures we  developed an analytic script to extract from the log data the  number of activities started, pages accessed, and video started   within each unit.  (Note: Doing so was no small effort,  motivating a need for analytic script sharing that  learnsphere.org is being designed to support.) These resource  use counts were constrained to both be resources within the  course content associated to that unit (i.e., a Coursera video  within this units section of the syllabus or an OLI page or  activity within an associated OLI unit) and used before the  student took the associated quiz. For example, a resource  done in week 1 but associated with unit 2 that is done before  the quiz (even if in week 1) gets counted toward unit 2, but  that same resource done after the unit quiz 2 is not counted.  All student resource use that is not counted as within-unit by  the above criteria is than counted as outside-unit  (e.g., for   unit  2  any  resource  associated  with  a different unit and  any unit 2 resource used after the unit 2 quiz). Thus, for  each student-unit row, we had within-unit and outside- unit  counts for each of doing, reading, and watching. As in [7], we  adjusted each students reading score to only count pages  accessed beyond the estimated minimum needed to access  the number of activities that student did. We also  converted all measures to Z scores (standard deviations  from the mean) to aid interpretation, namely, to facilitate  direct comparison of model parameter estimates.      Table 2: Within-unit and outside-unit effects of resource use on unit quiz performance.   Learning  method   Location Parameter  Estimate   Std.  Error   DF t value Pr(>|t|)    (Intercept) -0.015 0.068 12 -0.218 0.8312   Doing Within-unit 0.195 0.011 9969 17.475 < 0.000001 ***   Outside-unit 0.196 0.022 1389 8.736 < 0.000001***   Reading Within-unit 0.015 0.009 10226 1.676 0.0937 .   Outside-unit -0.006 0.021 1184 -0.283 0.7770   Watching Within-unit 0.036 0.009 10244 4.174 <0.00003 ***   Outside-unit -0.002 0.020 1215 -0.103 0.9182   Lowest 20%      4231   2nd 20%      543   3rd 20%      3212   4th 20%      2509   Highest 20%      1066   Outside Totals 2360 2275 2312 2321 2293 11561     Shown below is the R formula we used for this analysis,  indicating both the statistical method, a linear mixed effect  regression (lmer), and the regression formula (variables with  NR, for non-relevant, indicate the outside-unit counts):   lmer(Z.quiz.correct ~ (1|user.name) + (1|quiz.num) +  Z.Activities + Z.NR.Activities +  Z.Readings + Z.NR.Readings +   Z.Video + Z.NR.Videos, data = b_a)   To adjust for general differences in student performance and   unit quiz difficulty we included random effects in the model  for both student and unit (coded as quiz.num). We report on  analysis for the subset of registered OLI students (N = 939),  since only OLI students have the option of doing and reading.  All the significant results remain the same when we include all  students.   The key findings are shown in Table 2. There are  significant effects of within-unit and outside-unit doing, and  within-unit video watching. Within-unit reading is marginal.  Outside-unit reading and outside-unit watching are not  significant.   We find that within-unit doing remains a large and higher  significant predictor even after controlling for non-relevant  choices. This result is consistent with a causal interpretation.  Inspecting the parameter values we see, as before, a much  larger association of doing with outcomes than watching or  reading with outcomes. Whereas the prior whole course  analysis [7] found about a 6 times greater effect of doing on  outcomes than reading or watching, here we find a 13 times  bigger effect of doing than reading and a greater than 5   times effect of doing than video watching.   We also see, at least for doing, a significant effect of outside- unit resourse use. This result may indicate some third  variable yielding both higher general doing and better   outcomes. One possibility is that this third variable is indeed  some general student trait -- a third variable account for the  causing better learning, is that there are prerequisite. activity  effect. Another possibility, consistent with doing causing  better learning, is that there are prerequisite relationships  between units such that doing more activities in an earlier  unit, say unit 4, not only improves learning of that content  but better prepares the student for better learning from a  related subsequent unit, say unit 6.   2.4 Results:  Might  Prerequisites Account  for the Doer Effect for Outside-Unit Doing  To test for  the  possibility  that the  large  outside-unit effect   of  doing  may  be  a  consequence  of  better  learning  of  prerequisites, we split the outside-unit counts into before-unit  and after-unit counts. We ran a related R analysis as shown  below (as above, the model normalizes for general student  competence and for quiz difficulty by including random  effects for these):   lmer (Z. quiz.correct ~ (1|user.name) + (1|quiz.num) +  Z.BF.act + Z.activities + Z.AF.act +  Z.BF.reading + Z.reading + Z.AF.reading +  Z.BF.Video + Z.Video + Z.AF.Video, data = b_a2)   We report on results considering only units 2 to 10 in this  before and after analysis, since resources are not available  before unit 1 and the resources used after unit 11 are of a  different character (most being accessed to study for the final  exam). As above, we limited to only registered OLI students.  All the significant results remain the same when we include  either or both  all  students and/or all units.   We find that the doer effect is now strongest for doing within  the target unit as compared to doing before or after. This  stronger effect for doing within than before or after comes     Table 3: Before within, after unit effects of resource use on unit quiz performance.   Learning  method     Location   Normalized  Estimate   Std. Error df t value Pr(>|t|)    (Intercept) 0.059 0.049 11 1.344 0.20506        Doing   Before 0.143 0.016 1861 8.840 < 0.00 ***   Within 0.181 0.011 8892 16.973 < 0.00 ***   After 0.078 0.014 3063 5.402 0.00 ***        Reading   Before 0.008 0.015 1635 0.579 0.56278   Within 0.010 0.008 9233 1.224 0.22116   After -0.013 0.012 2603 -1.028 0.30410        Watching   Before 0.054 0.014 2432 3.853 0.00012 ***   Within 0.025 0.008 9463 3.123 0.00180 **   After 0.033 0.013 2876 2.474 0.01343 *        despite there being about 1/5 as much data, namely, 1 unit of  activity compared to 5 units on average contributing to the  before and after counts. The effect of doing outside the target  unit is stronger when more doing occurs before the unit  (0.143) than after the unit (.078). This difference is  consistent with prerequisite relationships between units. In  other work, we are exploring how a combination of a finer  grain analysis (estimating effects of every unit on every  other unit) and text processing might produce a yield  discovery or validation of prerequisite links between units.   Interestingly, there is still a significant effect for doing after  the target unit. This effect suggests that, in addition, to  possible causal effects of within and before unit practice  on improved outcomes, there may also be some general  student trait (e.g., a conscientious doer/learner) that yields  more practice and more learning (through some mechanism  not captured in the observed data, such as greater mental  effort).      For watching video, the before effect is strongest, within next,  and after. Perhaps watching the videos is important to produce   a level of sense making that better enhances preparation for  learning from future units. Nevertheless, we once again see  a much stronger effect on learning of doing than reading or  watching. In fact, the ratios are even bigger with the effect  of doing being 18 times more than the (non-significant)  effect of reading and 7 times more than the effect of video  watching.   3. TESTING THE GENERALIZATION OF  THE DOER EFFECT TO OTHER COURSES  To evaluate whether the doer effect generalizes to other  courses, we analyzed data from OLI for four courses:  Computing, Biology, Statistics, and Psychology. In the case of  Psychology this is the same OLI content (online readings and  interactive activities) as in the MOOC, but this data comes  from a different student population (those enrolled in a       Figure 2: Biology learn-by-doing activity supporting classical genetics learning outcomes. Such activities often involve multiple  related steps and integrate different forms of interaction, such as the radio box multiple choice at the top and the drag-and- drop selection at the bottom. Students get immediate feedback on their entries and can ask for hints if they are stuck.    university course rather than in a MOOC) and does not  involve the video content found in the Coursera course. We   evaluated how student choices to do activities and read (or at  least access) pages were associated both with their total quiz  scores and their course final grade. Given the similarity in  surface characteristics between the online activities and the  quizzes, these provide a kind of near transfer assessment of  learning. The final grade is a more subjective and a more   coarse measure of student learning involving more instructor  judgment and making fewer distinctions between students as  there are only 5 levels (A, B, C, D, or F). However, final  grade has the benefit of assessing learning more broadly and  serves as an intermediate (if not far) transfer assessment of  learning.   3.1 Method  The University of Maryland University College (UMUC) ran a   study to examine the effect of OLI resource materials on  distance learning. OLI log data was collected from six courses in  four disciplines (2 biology courses, 2 statistics courses, 1  computing course, 1  psychology course). Demographics (e.g.,  age, race, gender) were mostly evenly distributed in all classes.  Inclusion criteria for our analysis consisted of (1) OLI registered  students (i.e., non-OLI class sections were excluded) and (2) only  students who completed the course and received a final grade (i.e.,  students who withdrew, failed due to non-attendance, got an  incomplete, etc. were excluded).  Table 4 shows some general characteristics of the courses  involved. They all have a high number of available interactive  activities and readings and in all cases students tend to use a  substantial number of them. Figure 2 shows an example of an  interactive activity in the biology course.    Table 4: Characteristics of course use showing substantial activity use and variation.    Students using  online materials   Activities  available   Activities done  mean (stnd dev)   Readings  available   Readings done  mean (stnd dev)   Information Systems 7739 153 52 (43.7) 151 94.4 (88.8)   Biology 4564 544 200 (133.8) 881 571 (416.4)   Statistics 359 428 312 (116.0) 441 688 (420.2)   Psychology 123 687 621 (114.5) 545 510 (236.9)     3.2 Results  Figure 3 provides a scatterplot of the total pageviews (reading  estimate) and total activities started (doing estimate) for the  Biology course. It illustrates that while there is a positive  association between reading and doing (R^2 = .342), there is also  variation with some students doing more and reading less and  others reading more and doing less. The scatterplots for the other  courses are similar. The triangular white space on the right in the  scatterplot illustrates that students must access a minimal number  of pages to reach the number of activities they do. Thus, as  mentioned above, to improve the estimate of reading we adjusted  each students reading score by  subtracting this  minimum  (computed as a ratio of the activities the student did).   To pursue the question of whether the doer effect generalizes to  the UMUC data, we ran regression models across the courses to  assess how strongly student differences in resource use were  associated with learning outcomes. These models are simpler than  those in [7] to facilitate a uniform approach across datasets and  because some data is not available (e.g., video watching and pre-  test results) in most datasets. We performed two linear regressions  for each course, one where the outcome variable was students  total quiz score and another where it was their final grade  converted to numeric score (F = 1 to A = 5). We converted  predictor and outcome variables to Z scores as above. The R calls  used can be summarized as follows (where the brackets [ ]  indicate options selected in 10 separate calls):   lm([totalQuiz.z, final_grade_in_number.z] ~  activities.z + non_activities_reading.z,  data = [Statistics, Biology, IFSM, Psych, Psych MOOC])      The results are shown in Table 5. As shown, the doer effect is  consistently observed. The standardized coefficient of the effect  of doing on outcomes is always significant and much higher than  the standardized coefficient of the effect of reading (not always  significant). The ratio of the size of the doing to reading effect  goes from 2.2 to infinity (because in one case, quiz score in  Statistics, the reading effect is not positive) with median of 6 --  the same ratio we found previously! In other words, the effect of  doing is generally about 6 times greater than the effect of reading  across four different courses and involving over 12,500  students.     F i g u r e  3 :  Scatterplot of pageviews (reading) and activities  started (doing) by 4564 biology students showing that many  students do more and read less (top left) and others read  more and do less (lower right and middle).    Table 5: Model fit, standardized coefficients, and doer effect ratio for 5 courses and 2 outcomes.   Quiz Final Grade    Adj  R^2   Doing  std coef   Reading  std coef   Effect  ratio   Adj  R^2   Doing  std coef   Reading  std coef   Effect  ratio   InfoSystems 0.49 0.642 0.124 5.2 0.08 0.227 0.105 2.2   Biology 0.39 0.571 0.114 5.0 0.16 0.340 0.109 3.1   Statistics 0.24 0.519 -0.127  0.11 0.327 0.020 16.4   Psychology 0.64 0.781 0.092 8.5 0.45 0.654 0.085 7.7   Psy MOOC 0.25 0.467 0.069 6.8 0.08 0.259 0.054 4.8      4. DISCUSSION AND CONCLUSIONS  Determining causal relationships is important for scientific and  practical reasons because causal relationships provide a path  toward explanatory theory as well as reliable and replicable  practical application: If we can be certain a learning method is  causally related to more optimal learning, then that method should  be used to guide course design and students should be encouraged  to use it. There are lots of laboratory experiments of the testing  effect that provide high internal validity support that, in the  content and contexts sampled, there is a strong causal impact of  doing on longer-term learning. The content in these studies has  typically been facts (even arbitrary associations involving non-  words like zep with house [14]) and the assessments of learning  have typically involved delayed retrieval of those facts. Even in  the classroom studies, the orientation has been toward facts and  other forms of verbal expression of concepts. Given the  importance of skills and principles in many domains and given  other experimental results in such domains that point to less  testing and more study [4, 6, 15], it is critical that we expand  efforts to test the generalizability of learning by doing. One of the  strengths of these results is that the doer effect is demonstrated  across four different content domains (information systems,  biology, statistics, and psychology).  One of these is in the  humanities (psychology) but none are in the arts and it is worth  investigating whether the doer effect is found in less well-defined  domains, such as law or design.   Such efforts are particularly important in the context of MOOCs  where so much emphasis has been placed on online lecture video.  We have identified a doer effect, an association between more  doing and more learning, in data  from  multiple  online  courses. We have also shown that this effect cannot be explained  solely by some global student trait, a particular third variable  alternative to a causal explanation (e.g., a motivation to both do  and learn). Such an explanation does not predict that, for the  same student, within-unit activity will predict learning on unit  content above and beyond outside-unit activity. Of course, other  third variable explanations are still possible (e.g., interest in a  particular unit content produces more doing and more learning)  and experimentation is warranted, especially as so-called A/B  testing is becoming easier to do online.   MOOC providers and online course developers should not only be  pushing to be sure to have a large volume of activities, but to  provide guidance and incentives to students to do them. Further,   they should be exploring what are the best ratios of active doing to  passive      study      through      reading      text      or      watching  lectures Analytics can help. We suspect that detailed online  course data of the kind we analyzed can inform this question. In  particular, one can investigate, for a fixed student time allocation,  what ratio of doing to study is associated with the most learning.   5. ACKNOWLEDGMENTS  This work was supported by a National Science Foundation grant  (ACI-1443068) toward the creation of LearnSphere.org and by  funding from Google.   6. REFERENCES  [1] Campbell, D.T., & Stanley, J.C. (1963). Experimental and   quasi-experimental designs for research on teaching.  Chicago: Rand McNally    [2] Dewey,J. (1916), (2007 edition). Democracy and Education.  Teddington: Echo Library.k.    [3] Hill, P. (2013). Emerging Student Patterns in MOOCs: A  (Revised) Graphical View. e-literate. Available online:   http://mfeldstein.com/emerging-student-patterns-in-moocs-a-  revised-graphical-view/.   [4] Kirschner, P. A., Sweller, J., & Clark, R. E. (2006). Why  minimal guidance during instruction does not work: An  analysis of the failure of constructivist, discovery, problem-  based, experiential, and inquiry-based teaching. Educational  Psychologist, 41, 7586. doi:10.1207/s15326985ep4102_1   [5] Koedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark,  M. A. (1997). Intelligent tutoring goes to school in the big  city. International Journal of Artificial Intelligence in  Education, 8, 30-43.    [6] Koedinger, K. R., Corbett, A. C., & Perfetti, C. (2012). The  Knowledge-Learning-Instruction (KLI) framework: Bridging  the science-practice chasm to enhance robust student  learning. Cognitive Science, 36 (5), 757-798.  doi:10.1111/j.1551-6709.2012.01245.x   [7] Koedinger, K. R., Kim, J., Jia, J., McLaughlin, E. A., & Bier,  N. L. (2015 Learning is Not a Spectator Sport: Doing is  Better than Watching for Learning from a MOOC. In  Proceedings of the Second (2015) ACM Conference on  Learning at Scale, 111-120.       [8] McDaniel, M.A., Anderson, J. L., Derbish, M. H., &  Morrisette, N. (2007). Testing the testing effect in  theclassroom. European Journal of Cognitive Psychology,  19(4/5), 494-513. doi:10.1080/09541440701326154   [9] National Research Council. (2002). Scientific research in    education. Committee on Scientific Principles for Education  Research. Shavelson, R.J., and Towne, L., Editors. Center for  Education. Division of Behavioral and Social Sciences and  Education. Washington, DC: National Academy Press.   [10] Pane, J.F., Griffin, B., McCaffrey, D.F. & Karam, R. (2014).  Effectiveness of Cognitive Tutor Algebra I at Scale.  Educational Evaluation and Policy Analysis, 36 (2), 127 -  144. doi:10.3102/0162373713507480   [11] Pashler, H., Bain, P., Bottge, B., Graesser, A., Koedinger,  K., McDaniel, M., & Metcalfe, J. (2007). Organizing  Instruction and Study to Improve Student Learning  (NCER 2007-2004). Washington, DC: National Center  for Education Research, Institute of Education Sciences,  U.S. Department of Education.   [12] Pavlik, P. I., Jr., & Anderson, J. R. ( 2005). Practice and  forgetting effects on vocabulary memory: An activation-  based model of the spacing effect. Cognitive Science, 29,  559-586.doi:10.1207/s15516709cog0000_14   [13] Renkl, A., Stark, R., Gruber, H., & Mandl, H. (1998).  Learning from worked-out examples: the effects of  example variability and elicited self-explanations.  Contemporary Educational Psychology, 23, 90108.  doi:10/1006/ceps.1997.0959   [14] Roediger, H. L. & Karpicke, J. D. (2006). The power of  testing memory: Basic research and implications for  educational practice. Perspectives on Psychological   Science, 1, 181-210. doi:10.1111/j.1745- 6916.2006.00012.x   [15] Salden, R.J.C.M., Koedinger, K.R., Renkl, A., Aleven, V., &  McLaren, B.M. (2010). Accounting for beneficial effects of  worked examples in tutored problem solving. Educational  Psychology Review. doi: 10.1007/s10648-010-9143-6   [16] Singer, S.R. & Bonvillian, W.B. (2013). Two Revolutions in   Learning. Science 22, Vol. 339 no. 6126, p.1359.   doi:10.1126/science.1237223   [17] Lovett, M., Meyer, O., & Thille, C. (2008). The Open  Learning Initiative: Measuring the effectiveness of the OLI  statistics course in accelerating student learning. Journal of  Interactive Media in Education, 2008 (1), 1-16.  http://doi.org/10.5334/2008-14   [18] Trochim, W. M. (2009). Evaluation Policy and Evaluation  Practice. New Directions for Evaluation, 123, 13-  32.doi:10.1002/ev.303   [19] Wang, X., Wen, M., Ros, C. P. (2016). Towards triggering  higher-order thinking behaviors in MOOCs, in Proceedings  of Learning, Analytics, and Knowledge '16.   [20] Wieman, C. E. (2014). Large-scale comparison of science  teaching sends clear message. Proceedings of the National  Academy of Science, 111(23), 8319  8320.  doi:10.1073/pnas.1407304111   [21] Zhu, X., & Simon, H.A. (1987). Learning mathematics  from examples and by doing. Cognition and Instruction,  4, 137- 166. doi:10.1207/s1532690xci0403_1             "}
{"index":{"_id":"49"}}
{"datatype":"inproceedings","key":"Wang:2016:TTH:2883851.2883964","author":"Wang, Xu and Wen, Miaomiao and Ros'e, Carolyn P.","title":"Towards Triggering Higher-order Thinking Behaviors in MOOCs","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"398--407","numpages":"10","url":"http://doi.acm.org/10.1145/2883851.2883964","doi":"10.1145/2883851.2883964","acmid":"2883964","publisher":"ACM","address":"New York, NY, USA","keywords":"LDA topic modeling, coding manual, discussion, learning analytics, propensity score matching, regression analysis","abstract":"With the aim of better scaffolding discussion to improve learning in a MOOC context, this work investigates what kinds of discussion behaviors contribute to learning. We explored whether engaging in higher-order thinking behaviors results in more learning than paying general or focused attention to course materials. In order to evaluate whether to attribute the effect to engagement in the associated behaviors versus persistent characteristics of the students, we adopted two approaches. First, we used propensity score matching to pair students who exhibit a similar level of involvement in other course activities. Second, we explored individual variation in engagement in higher-order thinking behaviors across weeks. The results of both analyses support the attribution of the effect to the behavioral interpretation. A further analysis using LDA applied to course materials suggests that more social oriented topics triggered richer discussion than more biopsychology oriented topics.","pdf":"Towards triggering higher-order thinking behaviors in  MOOCs   Xu Wang  School of Computer Science  Carnegie Mellon University   5000 Forbes, Pittsburgh, PA, 15213   xuwang@cs.cmu.edu   Miaomiao Wen  School of Computer Science  Carnegie Mellon University   5000 Forbes, Pittsburgh, PA, 15213   mwen@cs.cmu.edu   Carolyn P. Ros  School of Computer Science  Carnegie Mellon University   5000 Forbes, Pittsburgh, PA, 15213  cprose@cs.cmu.edu        ABSTRACT  With the aim of better scaffolding discussion to improve learning  in a MOOC context, this work investigates what kinds of  discussion behaviors contribute to learning. We explored whether  engaging in higher-order thinking behaviors results in more  learning than paying general or focused attention to course  materials. In order to evaluate whether to attribute the effect to  engagement in the associated behaviors versus persistent  characteristics of the students, we adopted two approaches. First,  we used propensity score matching to pair students who exhibit a  similar level of involvement in other course activities. Second, we  explored individual variation in engagement in higher-order  thinking behaviors across weeks. The results of both analyses  support the attribution of the effect to the behavioral  interpretation. A further analysis using LDA applied to course  materials suggests that more social oriented topics triggered richer  discussion than more biopsychology oriented topics.    Author Keywords  Discussion; Learning analytics; LDA topic modeling; regression  analysis; coding manual; propensity score matching;   ACM Classification Keywords  K.3.1 Computer Uses in Education     1. INTRODUCTION  Previous work in the field of CSCL (Computer Supported  Collaborative Learning) has demonstrated that discussion can  facilitate learning in contexts such as classrooms or intelligent  tutoring systems [7,9]. However, unlike traditional educational  settings, discussions in MOOCs (Massive Open Online Courses)  are large-scaled and asynchronous in nature, and therefore more  difficult to control. In order to design interventions to increase the  quality of discussion in MOOCs, we begin by investigating the  following two research questions.  1) What kinds of discussion  behaviors are associated with more learning 2) What kinds of  learning materials appear to trigger more of these discussion  behaviors  These findings have the ability to inform development   of interventions to improve discussion and learning in MOOCs.  Driven by the first research question, we conducted an extension  of a previous study to explore the relationship between  cognitively relevant discussion behaviors and learning. In [26], we  developed a coding scheme based on Chis [8] ICAP (Interactive- Constructive-Active-Passive) framework to categorize students  cognitive engagement displayed in their conversations. In the  exploratory study, we found that on-topic discussion is correlated  with more learning than off-task discussion. However, contrary to  Chis earlier findings, we did not find a demonstrable order of  strength between interactive, constructive and active discussion  behaviors as we had expected.   One potential explanation is that the previous version of the  coding manual was a low inference coding manual that may have  missed subtle nuances that are germane to the distinctions made in  the original ICAP work. For example, one case that posed a  challenge for the low inference approach was distinguishing cases  where students describe a life event for the purpose of explaining  a psychological concept from cases where they are just socialized,  or cases whether they are explaining to someone else vs.  explaining to themselves. In this paper, we present a higher  inference approach to coding designed to overcome these  difficulties.  Despite the increased difficulty of achieving high reliability with a  high inference coding manual, we achieved a high reliability after  a 3 month concerted effort. Two coders coded 40 randomly  sampled posts, and achieved Kappa of 0.721 and weighted Kappa  of 0.864.  The coding manual makes fine-grained distinctions  between types of active, constructive, and interactive behavior.   However, constructive and interactive behaviors are rare.   Therefore, in our analysis building on this coding, we aggregate  constructive and interactive behaviors together, referring to them  as higher-order thinking behaviors, which involve constructing  ideas that go beyond what is explicitly covered in the course  materials, reasoning about course materials, and referring to  peers ideas constructively. We define active behaviors as  attending to course materials in any way, including indications  that the student directly repeats or paraphrases course content, or  indirectly implies he/she is paying attention to course materials.  We offer this coding manual as one contribution of this work to  aid the work of this community in investigating research questions  related to the connection between discussion and learning.   In this paper, we present an analysis in which we observe a larger  effect of higher-order thinking behaviors on learning compared  with attending to course materials in a less engaged manner.  We  then asked the question of whether the students who displayed  more higher-order thinking behaviors were simply different kinds  of learners who were more competent, or whether a more   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883964.     stringent analysis would still suggest that these students learnt  more through cognitively engaging with course materials as  shown by their discussion.  To answer this question, we adopted two approaches. First, we  used propensity score matching to pair students who have a  similar level of involvement in other activities in the course, and  compared learning between the matched control and treatment  groups. Second, we performed a week-level within-subject  analysis. We investigated whether individual learners had higher  course performance during weeks in which they displayed more  higher- order thinking behaviors in the discussion forum.  Both of  these analyses show that students learnt more through engaging  with higher-order thinking behaviors.  Building on these results, we then investigated what is different in  the course materials across weeks that lead to different levels of  cognitive engagement, and what topics may trigger cognitively  richer discussion among students. To answer this question, we  used LDA (Latent Dirichlet Allocation) [4] topic modeling to fit a  topic model in order to identify which topics are associated with  the occurrence of higher-order thinking behaviors. We then follow  up on this analysis with a qualitative analysis to contrast the  composition of texts that are associated with cognitively richer  discussion and cognitively poorer discussion.  In the remainder of the paper, we begin by briefly introducing the  ICAP framework that grounds our coding manual. We also  discuss current learning analytics approaches applied in a MOOC  context, and studies of what kinds of materials trigger more  higher-order thinking behaviors in general learning settings. In the  data preparation section, we introduce the course that provided the  data for this research, explain the coding manual, and introduce  how we prepared our data. Next we present our analyses and  results. We conclude by discussing the limitations and  implications of this work.     2. RELATED WORK  2.1 Introduction to ICAP Framework  In our analytic work, we start with a foundation in the ICAP  framework [8], in which the authors focus on the amount of  cognitive engagement that can be detected by observation of fine- grained behaviors. The ICAP framework proposes that overt  behaviors, such as discussion behaviors, can reveal students level  of engagement with learning materials. The authors explain,  Although far from perfect, overt behaviors are a good proxy to  reflect different modes of engagement that teachers can use to  ascertain whether a student is in fact engaged in a specific mode  for a given activity. [8]  In a traditional education context, researchers have the  opportunity to directly observe learners in person, which allows  them to capture a larger variety of overt activities, such as  students body gestures or their nuanced interactions with their  collocated peers. In a MOOC context, although we have access to  a large volume of clickstream data, we lack an up-close view of  student behavior illustrating their learning processes. Adopting a  content analysis approach applied to student posts is one way we  might estimate students cognitive engagement in the course.  In brief, Chi and Wylie [8] propose that there are different modes  or categories of active learning, corresponding to different overt  behaviors that elicit differential knowledge acquisition or learning  processes. They propose that learning activities and their resulting  overt engagement behaviors can be differentiated into one of four  modes: interactive, constructive, active or passive.    In our work we borrow the definitions (italicized) given in the Chi  and Wylie [8] paper to explain the differences between these four  categories.   Passive: Passive mode of engagement is defined as learners being  oriented toward and receiving information from the instructional  materials without overtly doing anything else related to learning.  Active: Learners engagement with instructional materials can be  operationalized as active if some form of overt motoric action or  physical manipulation is undertaken.   Constructive: Constructive behaviors are defined as those in  which learners generate or produce additional externalized  outputs or products beyond what was provided in the learning  materials.   Interactive: Interactive behaviors should meet two criteria (a)  both partners utterances must be primarily constructive, and (b)  a sufficient degree of turn taking must occur.  This framework has been widely adapted in capturing students  behaviors and conversations in learning science studies. In this  work, we developed a coding manual to operationalize these  behaviors in a MOOC discussion forum to better capture students  cognitive processes in these settings.    2.2 Learning analytics approaches in MOOCs  The data trace that is produced when students participate in a  MOOC is multi-faceted, and specialized analytics have been  developed for each type of data.   2.2.1 Clickstream data analytics  Clickstream data is produced whenever a student clicks on  anything on the MOOC platform.  This is the most frequent data  from MOOCs that is analyzed in prior work.  For example,  Ferguson [12] observed seven distinct patterns of engagement in 4  MOOCs based on patterns identified in clickstream data. In [17],  the authors developed a classification method to identify 4  prototypical trajectories of engagement based on students click  data. Anderson [2] found that students who participated in other  platform activities (videos, quizzes, etc.) participated more in the  forum as well. In [18], the authors used a machine leaning method  to classify students as successful or risk students based on  their performance in the discussion forums, including number of  posts, comments, votes, etc. These analyses provide insights into  how students general behaviors in a MOOC look like.   In [19] and [20], Koedinger et al. claimed a causal effect of  learning-by-doing activities on learning by analyzing clickstream  data of the psychology MOOC, the same course we did our  analysis on. In this course, the clickstream data were far richer  than usual because of learning-by-doing activities integrated into  the platform, each of which produced their own clickstream data.   The findings from clickstream data collected from the Psychology  MOOC were very effective in explaining variation in student  learning patterns. In our work, in order to probe more deeply into  student ideas and reasoning, we explore in the same MOOC what  is revealed through student discussion behavior.      2.2.2 Discourse analytics  Compared with log data, textual data is less structured, but it also  contains large amount of information about students engagement  with the course. With the advances in machine learning and  language technologies, development of analytic techniques to be  applied to student discussion data with the aim of providing  adaptive support is an up-and-coming area of learning analytics.     Allen et al. [1] used two NLP (Natural Language Processing)  tools to investigate the potential of NLP techniques to perform  stealth assessments of students reading comprehension skills.  Hsiao [16] proposed a novel Topic Facet Model, which treats all  words in a single sentence as emanating from one topic facet.  They also prototyped a visual analytics interface to present online  discussion forum semantics, which helps users examine post  content by viewing the topic facets. In [23], the authors used LDA  topic modeling to extract topics from discussion forums, and used  text mining techniques to find what are the place names students  talked about most in the class, which provided insight to inform  future development of the course. In [5], the authors proposed a  generative model to cluster threads and rank them based on  relevance to address the problem that there are far more threads  than one can read.  These methods and techniques help visualize discussions in  MOOCs. While there is great potential for applying machine  learning methods and language technologies to improve learning  environments, one downside of these automated methods is that  they lose valuable information in the modeling process. In [11], in  order to compensate for this problem, the authors adopted a mixed  method approach, by comparing the result from machine  modeling and human annotation. They did a clustering analysis of  MOOC discussion posts, and also qualitatively evaluated the  clusters and compared the clustering result with manual  annotations, for comparison and contrast.  Several studies on discussion forums in MOOCs have examined  how social factors affect attrition [24].  While study of attrition is  important, one reason why so much of the recent research has  focused on this issue is because of convenience.  Frequently it is  not possible to evaluate learning because no formal pre and post- test are administered to students.  In our work we are able to  explore the connection between discussion behavior and learning  in a MOOC context because the MOOC we are studying included  a formal pre and post-test.      2.3 Higher-order thinking behaviors  In the second half of this paper, we investigate what kinds of  materials appear to trigger richer discussion. In line with this  purpose, we also examine existing literature on the relationship  between learning materials and discussion behaviors or cognitive  engagement, especially in an online learning context.  McKendree et al. [21] suggested that the social, participatory and  shared verbal activity in online environments is a trigger for  higher-order thinking, because learners can see their peers and  tutors modeling the process of interpretation and application; they  can analyze and compare their own understanding to that of  others. This suggests that MOOCs have a natural deficiency in  triggering higher order thinking behaviors, because there are few  opportunities for students to get intensively involved in social,  participatory or shared verbal activity compared with other  learning environments. This study also suggests that more work  should be done to support students participatory and shared  verbal activities in MOOCs.  Zhu [27] listed many variables that may influence interaction and  cognitive engagement in an online discussion, including the  instructors presence, role, discussion questions, etc. In addition,  student intrinsic motivation and prior knowledge of and interest in  the topic may also influence levels of cognitive engagement and  interaction with peers during the discussion. Students with a  higher level of prior knowledge of the subject may feel bored,  while students with limited prior knowledge may find interesting  in contributing to the discussion.    In addition to course-level variables, other studies show the effect  of text or topics on learners engagement. For example,  McNamara et al. [22] found that readers who know little about the  domain of the text benefit from a coherent text, whereas high- knowledge readers benefit from a minimally coherent text. They  argue that the poorly written text forces the knowledgeable  readers to engage in compensatory processing to infer unstated  relations in the text.  These studies inform us about variables that could have an  influence on the level of cognitive engagement triggered by  learning materials, including students interest, prior knowledge,  and coherence of the text. This prior work raises questions about  what we might see specifically in the differential observed  engagement in cognitive behavior associated with different parts  of the material in this course.   3. DATA PREPARATION  3.1 Introduction to Psychology as a science  The dataset we used in this analysis is from the course  Introduction to Psychology as a Science offered through  Coursera collaboratively by Georgia Institute of Technology and  Carnegie Mellon University. The course incorporated elements  from the OLI (Open Learning Initiative) Introduction to  Psychology learning environment. One special characteristic of  the course was that it administered a pre/post test with the  intention to support research.   Course materials included video lectures, assigned MOOC  activities, learning-by-doing activities in the OLI environment,  and weekly high-stakes quizzes. We present the topics of the  course by week in Table 1 to give readers a sense of what  psychology topics are covered in the course.   Table 1. Topics in each week of the course   Week Topic Week Topic   1 Experiment methods 2  Biopsychology,   neuroscience   3 Sensation and perception 4 Learning   5 Memory 6 Language and intelligence   7 Development 8 Motivation and emotion  9 Personality 10 Social environment  11 Disorders 12 Treatment      3.2 Course involvement data  Altogether, the dataset contains data from 27,750 registered users,  and a total of 7,990 posts and comments. Of these, 1079 learners  have both pretest and posttest on record, 491 of whom  participated in the discussion forum, generating 3864 posts and  comments in total.   In addition to forum records, students clicks related to course  materials are also recorded in the clickstream data. The course  record contains 1487665 student clicks. This data provides us with  the opportunity to monitor students interaction with course  materials over their participation trajectory.     3.3 Coding Manual  One contribution of our work is a coding manual to capture  students discussion behaviors that are associated with learning  based on Michelene Chis ICAP framework [8]1.  However, to operationalize discussion behaviors in a MOOC  context, we need to consider specific characteristics of MOOC  forums. In this coding manual, we explained how MOOC forum  data is different from classroom conversation data, and how we  adapted the definitions from Chis original ICAP framework.  In a MOOC discussion forum, as long as the student contributes  an on-topic post, which in our coding manual requires that  students show evidence of at least attending to learning materials  in some way, we consider it to be active behavior.  Our definition  precludes posts being identified as on-topic and passive.  Thus, we  do not include the passive category in our coding manual. Our  definition of interactive behavior is also slightly different from  Chis framework. As students rarely take turns in conversations in  the discussion forum, we hardly see perfect interactive activities  as defined.  Thus, we categorize a post as interactive if the post is  constructive and the student is referring to someone elses idea  expressed earlier in the conversation. Although in a MOOC  context, posts often receive a nominal reply, there are nevertheless  very few posts that can be considered as genuinely interactive in  the sense intended in ICAP. Most frequently, students post to the  discussion forums as a form of personal reflection, or self- explanation, without pointing to or making connection with their  peers expressed ideas.  We only code interactive behaviors as  such when they are constructive and show that students are  pointing to, building upon, making connections to, or challenging  someone elses ideas.  Thus, this code is rarely used in our coding.  As described in the introduction section, we have developed a  high-inference coding manual to accurately capture students  discussion behaviors corresponding to their underlying cognitive  processes. On the one hand, this means we provide definitions of  each category that are content-focused instead of linguistics- focused. On the other hand, in this new version of coding manual,  we provide coders with a context, including the thread starter and  previous post, to better infer whether the content is on-topic, or  whether the student is referring to someone elses ideas. In the  previous coding manual, in which only one single post was  provided for coding without further reference to its context, it can  be difficult to make decisions that would depend upon knowing  what the author of a post was referring to, even in identifying  whether the post is on task.    A decision tree used in the coding manual is shown in Figure 1. In  the first stage of the coding process, the coder is asked to decide  whether a post is on-task or off-task. In the second stage, for on- task discourse, the coder is then asked to decide to which specific  categories of cognitive engagement each post belongs.   Note that interactive and constructive behavior have to be course  content related (in this case, psychology) in our coding. For  example, a student reasoning about a scoring rubric would be  categorized as displaying active behavior only, because the  student is paying attention to course materials, but is not engaged  in constructive behavior for the purpose of learning.  In this decision tree presented here, we provide only a brief  definition of each category. In practice, we provide our coders  with more detailed definitions, examples, and failed examples,                                                                     1 http://dance.cs.cmu.edu/MOOC-ICAP-Manual.pdf   which can be viewed in the coding manual found at the URL  mentioned above.       Figure 1. Decision Tree of the ICAP coding manual   3.4 Hand-coded dataset  Because a reliability test indicated high inter-rater reliability with  the new coding scheme, for our analysis presented in this paper,  we had one coder code all 3864 posts for students who have  pretest and posttest on record in the psychology course. There are  six mutually exclusive categories, O, A2, A1, C2, C1, I, as shown  in Figure 1. In our analysis, we grouped A2, A1, and C2, C1 into  A and C respectively.  As explained earlier, we see little intensive interaction between  students in the MOOC context. Consistent with that, interactive  behavior in our dataset is very rare. Thus we do not distinguish  between interactive and constructive behaviors in our further  analysis. Instead, we group them together, collectively referring to  them as higher-order thinking behaviors. We also refer to active  behavior as paying general or focused attention to course  materials, as indicated in the coding manual.   4. WILL ENGAGING IN HIGHER-ORDER  THINKING BEHAVIORS ASSOCIATED   WITH MORE LEARNING  In a previous study [26], we explored the relationship between  cognitively relevant discussion behaviors and learning. We  observed an effect of on-topic discussion over off-topic  discussion, but we did not see a rank ordering in the effect of  interactive, constructive, and active behaviors on learning as  indicated by Chi [8]. We partly attributed that to the deficiency in  the coding manual. As illustrated earlier, we revised the coding  manual to more accurately capture students discussion behaviors.  In this analysis, we hypothesize that higher-order thinking     behaviors would have a larger effect size on learning compared  with paying general or focused attention to course materials.   Driven by this hypothesis, we did a regression analysis as  described in section 4.1, and observed that students who display  higher-order thinking behaviors have more learning gains than  those who did not display any higher-order thinking behaviors but  show that they are paying active attention to course materials from  their posts in the forums; And the students whose discussion  behaviors indicate they are paying active attention to course  materials also have higher learning gains than students who are  constantly being off-topic in the forums.  We posit two possible explanations for the observed phenomenon.  First, it might be that the students who displayed higher-order  thinking behaviors are a different kind of learners who are better  at argumentation and expressing themselves. It might be not that  engaging in higher level thinking in discussion forums is causing  learning gains for these learners, but rather, that such learners tend  to engage at higher cognitive levels that correlates with higher  learning gains. Alternately, it might be the case that by  participating in higher-order thinking in the discussion forums,  these students engaged with learning materials deeper, and  learned more during the course as a result. While the definitive  answer can only be gained through a manipulation study, we can  gain stronger suggestive evidence through more fine-grained  analysis. First, we adopted the propensity score matching  approach to account for individual differences. We used  propensity score matching to pair students who are categorized as  similar learners by their involvement in other course activities,  and we will describe this analysis in section 4.2. Second, we  conducted a within-subject analysis over weeks to observe the  effect of higher-order thinking behaviors on individual students  learning outcomes. We will describe this analysis in section 4.3   4.1 Are higher-order thinking behaviors  associated with more learning: a regression   analysis  The purpose of our first analysis is to measure an effect of higher- order thinking behaviors over paying general or focused attention  to course materials. To this end, we created three mutually  exclusive groups of students based on the highest level of  cognitively relevant discussion behaviors they displayed in the  forum. The three binary group variables are defined below.   Group2[higher-order]: A binary variable, which equals 1 if the  student has contributed at least one constructive or interactive post  during the course, otherwise it equals 0.  Group1[paying-attention]: A binary variable, which equals 1 if  the student has contributed at least one active post during the  course but has not displayed any constructive or interactive posts,  and otherwise equals 0.  Group0[off-topic]: The group associated with this binary variable  contains the rest of the students, i.e., students who have not  contributed any on-topic discussion during the course.  We arranged our dataset such that the student is the unit of  analysis. By including only the students who have both pretest  and posttest on record, and have participated in the discussion  forum, we arrive at a final sample size of 491. Below, we explain  our dependent and control variables.  Dependent variable:    Post-test: The students standardized post-test score.      Control variables:   Pre-test:  The students standardized pre-test score.   Numpost: The total number of posts the student has contributed  throughout the course.  OLI-registration: A binary variable indicating whether the  student has registered for OLI (Open Learning Initiative), which  offered them supplementary learning-by-doing activities in each  unit. [19,20]  Video: The students number of videos clicked on at least once.   Quiz: The students number of quizzes clicked on at least once.   OLIsite: The students number of clicks on the OLI website.   Forum: The students number of clicks on the forum.   We standardized all four variables based on clickstream data.    Table 2. Regression model of discussion behaviors on learning   Control/Indep. Variable Model 1  (N=491)    Group2[higher-order] 0.456***   Group1[paying-attention] 0.264*   Pretest 0.236***  Numpost 0.049   Video -0.015   Quiz -0.035   OLIsite 0.043   Forum 0.019   OLI_registration 0.290*   (p<0.001***, p<0.01**, p<0.05*)  The result shows that controlling for the number of other activities  the student engaged in during the course, including watching  videos, doing quizzes, visiting the forum, and vising the OLI  website, for students who contributed the same number of posts,  those who displayed higher-order thinking behaviors had higher  learning gains than students who did not display higher-order  thinking behaviors (p-value= 0.001). Similarly, students whose  discussion behaviors indicated paying active attention to course  materials had more learning gains than student who did not show  active attention to course materials (p-value=0.031). We further  contrasted between the higher-order thinking group and the  paying attention group by computing the relative effect size  associated with these binary variables. We found that the higher- order thinking group had an effect size of 0.36 in comparison with  the off-topic group, while the paying-attention group had an effect  size of 0.26.  In both cases, we computed effect size using the  Cohens d method. Due to this difference in their relative effect  sizes, we consider that displaying higher-order thinking behaviors  in discussion forums is associated with more learning gains than  displaying discussion behaviors that show general or focused  attention to course materials, which in turn is associated with  more learning gains than posting in the forum but being off-topic  all the time. Using the new high inference coding, we see a rank  of the effect of discussion behaviors in learning, as proposed in  Chis [8] framework.     4.2 Accounting for individual difference using  propensity score matching  Propensity-score matching is a type of nonrandomized study that  can be used to minimize selection bias and estimate the effects of  treatments on outcomes [14]. It works by matching students inside  the treatment group with doppelgangers in a comparator group.  Only corresponding students with a high degree of similarity on  relevant variables should be paired [3, 25].   The propensity score  matching method has been recently applied to MOOCs in part  because the availability of a large student population makes it  feasible to identify well-matched comparator subjects [6]. In our  work we use propensity score matching in order to minimize  potential selection bias.  Using a propensity score matching method, we sought to match  pairs of students who engaged in the same number of other  activities in the course, but varied in either displaying or not  displaying higher-order thinking behaviors. The students who  displayed higher-order thinking behaviors are considered to be in  the treatment group, while the students who did not display such  higher-order thinking are in the control group.  In this way we can  evaluate the association between the discussion behaviors and  learning, while holding other important effort related variables  constant.  We used all the control variables as introduced earlier as features  of the students engagement in the course, and built a logistic  regression model to predict the students probability of displaying  higher-order thinking behaviors in the course. We trained the  model and then did propensity score matching with this model on  a sample of 1079 students, which is the total number of students  in the dataset that have a pre-test and post-test on record.  Table 3. Comparison between treated and control groups on   relevant variables   Variable Mean Treated Control  Numpost 7.6471 7.6555  Pretest 11.269 11.857   OLI_registration 0.85714 0.88235  Video 6.2689 6.9664  Quiz 2.9328 3.1933   OLIsite 17.504 19.857  Forum 25.58 31.697     Table 4. Regression model of treatment [higher-order   thinking behaviors] on learning   Control/Indep. Variable Model 2  (N=119*2)   Treatment 0.300*   Pretest 0.183**  Numpost -0.091   OLI_registration 0.533**   Video 0.079   Quiz -0.030   Forum 0.125   (p<0.001***, p<0.01**, p<0.05*)  119 pairs of students were matched in the process. As displayed in  Table 3, we compared the difference between the matched  treatment and control groups. None of the features we used for the   matching are significantly different between the treatment and  control groups in the matched set, which demonstrates a  successful match for further analysis.   We then fitted a regression model using students standardized  posttest score as a dependent variable, the binary treatment  variable as a main effect, and the other variables as covariates,  which are measurements of students engagement in course  activities. The result is displayed in Table 4. Being in the  treatment group, which indicates the student has displayed higher- order thinking behaviors in the course, has a significant effect on  learning (p-value=0.019). The average posttest score for the  treatment group is 28.5 (s.d. = 4.56), and the average posttest  score for the control group is 27.3 (s.d. = 5.51). One thing to  notice is that as displayed in Table 3, although in this matching,  there were no significant differences between groups on any of the  matching variables, the trend was always in favor of the control  group, giving them an advantage.  Nevertheless we still find the  treatment group learned more.  The result here shows that for a pair of students who are  categorized as similar leaners by their prior knowledge and  engagement level in the course, the one who displayed higher- order thinking behaviors had higher learning gains than the one  who did not. We also computed the effect size of the treatment  effect using the Cohens d method, which is 0.227 in this case.    4.3 Within-subject analysis over weeks  The second approach we adopted to account for individual  difference is a within-subject analysis method. It is to investigate  for each individual, whether the weeks he/she displayed more  higher-order thinking behaviors would be associated with more  learning than the other weeks for the same student.    4.3.1 Data preparation  In this analysis, we break down the students activities in the  whole course into 12 weeks. And we use the time students submit  each quiz to segment behavior into different weeks. For example,  the activities the student did between the time he submitted quiz2  and quiz3 is considered the activities he did for week3. Using this  method, each student will have one entry of aggregated data per  week. We will introduce the variables we used as follows.   Variables that have a value per student per week:  QuizScore: This is the students quiz score of the week. In the  case of the final week, this would be the post-test score.  Typically  each quiz consists of 10 questions related to the weeks material.   The post-test consists of 25 questions for the whole course. Since  the final quiz contains questions that do not belong to the material  of that week, we dropped the final week data in our analysis for  consistency. Thus, in our analysis, there are 11 weeks data per  student.  Numpost: This is the number of posts the student contributed in  that week.  For activities that come from the OLI website, it is possible to  determine which content unit of the course each click is associated  with. Two variables, namely activities and pageviews, are the  number of activities and pages the student attempted on the topic  of that week before he submitted the quiz of that week.   Activities: This is the number of activities on the OLI site the  student attempted in that week.  Pageviews: This is the number of pages of OLI textbook the  student viewed that week.   Playvideo: the number of videos the student played that week.     Variables that have a value per student:  Pre-test: As there are no pretests for each week, we used the  pretest score of the course as a proxy for prior knowledge for each  week.  All the above variables are standardized.  In order to compare the effect of higher-order thinking behaviors  with that of paying general or focused attention to course  materials, similar to the course-level analysis, we generated two  binary group variables per student per week based on the highest  level of discussion behavior the student displayed in the forum.   Group2[higher-order]: A binary variable, which equals to 1 if  the student has contributed at least one constructive or interactive  post during the week, otherwise it equals 0.  Group1[paying-attention]: A binary variable, which equals to 1  if the student has contributed at least one active post during the  week but has not displayed any constructive or interactive post,  otherwise it equals 0.  We sampled out students who have taken both pretest and  posttest, and who have participated in the discussion forum at  least once during the 11 weeks. Altogether, there are 404 such  students used in our analysis. We then fitted a linear mixed-effect  model using SPSS.   The major capabilities that differentiate mixed-effects models  from general linear models are that mixed-effects models handle  correlated data and unequal variances more effectively. In our  case, as each individual has multiple entries that are considered to  be correlated, we need to use a mixed-effect model.   In our model, we used students QuizScore as a dependent  variable, treated the students CourseraID as a random effect, and  treated all other variables as fixed effects. By doing this, we are  accounting for participant effect, and observing the main effect of  the factors related to each participant. The parameters of the  mixed-effect model are shown in Table 5.   Table 5. Tests of Fixed Effects in the Mixed-Effect Model   Source Numerator  df   Denominator  df   F   Group2   [higher-order]   1 4315.65 5.031*   Group1   [paying-attention]   1 4210.21 2.515   Intercept 1 434.261 21.111***   week 10 4101.57 37.367***   Pretest 1 398.322 19.844***   Activities 1 4137.68 148.022***   Pageview 1 4411.26 13.677***   Playvideo 1 4379.43 37.114***   (p<0.001***, p<0.01**, p<0.05*)  The result shows that when controlling for participant effect, for  each individual, in the weeks he/she displayed higher-order  thinking behaviors, he had better performance in the quiz relative  to what was expected based on pretest score and other control  variables than the weeks he did not display any higher-order  thinking behaviors. However, in this model we do not see an  effect of showing active attention to course materials.   Based on the results shown by propensity score matching, and  now this within-subject analysis, we are more inclined to explain   the effect we see of higher-order thinking behaviors on learning as  demonstrating that students are engaged in a more effective way  with the course materials when they demonstrate these higher- order thinking behaviors, which leads to better learning.    5. WHICH TOPICS TRIGGER RICHER  DISCUSSION  From the analyses just shown, we see an effect of higher-order  thinking behaviors on learning, and we also see that students  level of cognitive engagement varies from week to week. This  drives us to ask a final research question, namely, which kind of  learning materials trigger richer discussion  In order to answer this question, we adopted a content analysis  approach to extract topics from the textbook of the course and  investigate which topics are associated with more high-order  thinking behaviors.   5.1 Topic Modeling Setup  We used LDA (Latent Dirichlet Allocation) [4] topic modeling to  extract topics from the OLI textbook. Latent Dirichlet Allocation  (LDA) is a statistical generative model that can be used to  discover hidden topics in documents as well as the words  associated with each topic.   There are 16 chapters in the OLI textbook, although the last  chapter is not used in the course. We trained a topic model using  the LDA algorithm (provided in the Mallet package2) on the 15  chapters of the textbook. We split each chapter into 3 sentences  units, and used it as the unit of analysis. We treated each unit as a  document such that there are 2946 documents in total. The model  was set to estimate 15 latent topics. We applied the trained topic  model back to the documents and obtained a topic distribution  over the 15 topics for each document. Each corpus unit is then  represented as a vector of topics, as shown in (1). Corpus units  represent the learning materials of each chapter of the textbook,  we thus have a topic representation of the content of the learning  materials.   ! #$%&'( =*+,  .,                                           (1) 34  ,53    Based on our annotation, we have counted the number of higher- order thinking behaviors students have displayed in each week.  The course syllabus provides a mapping from 12 course weeks to  the 15 chapters of the textbook. Based on this mapping, we could  associate each document with the number of higher-order thinking  behaviors students displayed in the corresponding week.   We then fitted a regression model using the corpus unit data, with  the 15 topics as independent variables, and the number of higher- order thinking behaviors for each unit as a dependent variable, as  shown in (2). By doing this, we can see which topics have a  higher weight in predicting the number of higher-order thinking  behaviors. Among the 15 topics, 1 was dropped in the regression  model. For the rest of the topics, 5 are positively associated with  richer discussion, 5 are negatively associated with richer  discussion, and 3 do not have a significant effect.   6$%(789&;  ;<&; (8'=8'9) =*>,  .,              (2) 34  ,53                                                                      2 http://mallet.cs.umass.edu/topics.php     5.2 Contrasting corpus units  We picked out two topics that have the highest positive weights,  and two topics that have the highest negative weights as  representatives for further analyses. The two topics that have the  highest positive weights are development and intelligence.  The two topics that have the highest negative weights are  neuroscience/brain and memory.  We then looked into the OLI textbook to retrieve corpus units that  have high and low weights on the positive and negative topics to  explore the difference. In the topic model we built earlier, each  corpus unit has a distribution over the 15 topics, for a given topic,  e.g., development, we rank the corpus units based on their  weight on this topic, and the units that have the highest weight or  lowest weight are chosen as examples and displayed below.  We adopted a more qualitative approach to analyze the difference  between these units. We present and contrast some illustrative  examples below.  Based on the regression model we fitted, we consider that text  units that have a higher weight on topic development and  intelligence are associated with richer discussion, and that text  units which have a lower weight on topic development and  intelligence are associated with less cognitively rich discussion.  We looked into text units in the chapter of development and  intelligence, and found some examples in these two chapters that  either have a high weight on these two topics or a low weight on  these two topics. In the following example paragraphs, we use a  symbol + to indicate that the text unit is positively associated  with higher-order thinking behaviors; and we use a symbol -- to  indicate the text unit is negatively associated with higher-order  thinking behaviors. For the positive examples, we underlined  phrases that are daily life phenomenon; and for the negative  examples, we underlined technical terms that we do not frequently  use in daily life.  Here is an example of a unit that has a high weight on the topic of  development in the development chapter:   (+)During this stage children desire to experience pleasure  through bowel movements, but they are also being toilet trained to  delay this gratification. Freud believed that if this toilet training  was either too harsh or too lenient, children would become fixated  in the anal stage and become likely to regress to this stage under  stress as adults. If the child received too little anal gratification  (i.e., if the parents had been very harsh about toilet training), the  adult personality will be anal retentivestingy, with a compulsive  seeking of order and tidiness.  In the same chapter, here is a unit that has a low weight on the  topic of development:  (--)The medical research was a 1998 study published in the  prestigious medical journal, The Lancet, by a British physician  named Andrew Wakefield. He and his colleagues reported data  allegedly collected from twelve children who had diagnoses of  regressive autism, 11 of whom also had a diagnosis of non- specific colitis. Wakefields paper claimed that this new brain and  bowel disease syndrome (autistic enterocolitis was the term  coined by Wakefield for the paper) started very soon after  administration of MMR vaccine, as reported to Wakefields team  by the parents of children in the study.   Here is an example of a unit that has a high weight on the topic of  intelligence in the intelligence chapter:  (+) Once the standardization has been accomplished, we have a  picture of the average abilities of people at different ages and can   calculate a persons mental age, which is the age at which a  person is performing intellectually. If we compare the mental age  of a person to the persons chronological age, the result is the  intelligence quotient (IQ), a measure of intelligence that is  adjusted for age.  In the same chapter, here is a unit that has a low weight on the  topic of intelligence:  (--)Severe and profound mental retardation is usually caused by  genetic mutations or accidents during birth, whereas mild forms  have both genetic and environmental influences. One cause of  mental retardation is Down syndrome, a chromosomal disorder  leading to mental retardation caused by the presence of all or  part of an extra 21st chromosome. The incidence of Down  syndrome is estimated at 1 per 800 to 1,000 births, although its  prevalence rises sharply in those born to older mothers.   We also extracted some units that have high weight on the topics  brain and memory for comparison. We consider the units that  have a high weight on the topics brain and memory are  negatively associated with rich discussion. Here are two examples  that have a high topic representation of brain and memory  respectively.  (--)As you can see in the following figure, neurons consist of  three major parts: a cell body, or soma, which contains the  nucleus of the cell and keeps the cell alive; a branching, treelike  fiber known as the dendrite, which collects information from other  cells and sends the information to the soma; and a long,  segmented fiber known as the axon, which transmits information  away from the cell body toward other neurons or to the muscles  and glands. Some neurons have hundreds or even thousands of  dendrites, and these dendrites may be branched to allow the cell  to receive information from thousands of other cells. The axons  are also specialized, and some, such as those that send messages  from the spinal cord to the muscles in the hands or feet, may be  very longeven up to several feet in length.   (--)Short-term memory (STM) is the place where small amounts  of information can be temporarily kept for more than a few  seconds but usually for less than one minute. The cognitive  psychologist George Miller referred to seven plus or minus two  pieces of information as the magic number in short-term  memory. Information in short-term memory is not stored  permanently but rather becomes available for us to process, and  the processes that we use to make sense of, modify, interpret, and  store information in STM are known as working memory.  In the two positive examples in the chapter of development and  intelligence, the content is explaining life phenomenon that are  familiar, for example, toilet training or IQ tests. And there are no  technical terms in the text. All words are what we use in daily life.  By contrast, even in the same chapter, the two examples that have  a lower weight over the topics development and intelligence  include more technical/medical terms that we seldom use in daily  life. It is the same case for the two examples on the topic of  brain and memory. The two examples are about bio- psychological processes that we seldom experience in life. And  there are a lot of unfamiliar technical terms in the text, making it  more challenging to read.  In the following table, we display the words that are associated  with topics that have a strong positive or negative association with  the prevalence of higher-order thinking behaviors students  displayed.       Table 6. Words associated with positive and negative topics   Positive Negative   Intelligence; People; Children;  Development; Parents; Adults;  Relationships; Social;  Language; Group; Personality;  Behavior; Arousal; Experience;  Emotions; Women; Body;  System   Memory; information;  brain; neurons; cortex;  cells; nerve; hemisphere;  disorders; psychological;  mental; symptoms; visual;  sound; eye; perceptual;   By comparing and contrasting the words associated with positive  and negative topics, we can see that more social oriented topics  are positively associated with richer discussion. These examples  show that these topics use more daily life words and explain  human phenomenon people are more familiar with. On the other  hand, more neuroscience oriented topics are negatively associated  with richer discussion, and the examples show that these topics  use more technical terms that are not usually used in daily life.   They also explain animal experiments or biopsychology  phenomenon that are not commonly familiar.   6. DESIGN IMPLICATIONS  Based on our findings and discussion, we propose the following  design implications, which could be a next-step intervention built  into MOOC discussion forums to support richer discussion.  1) We see that content related to daily life and social experience is  associated with higher-order thinking behaviors. As explained in  motivation theories [10,15], real-world connection helps support  situational interest which could lead to higher intrinsic motivation  in the student. For the units that are not connected to personal  interests and experiences, e.g., neuroscience, we can think of  manipulations to increase student motivation. Some practices may  include, incorporating blocks of real-life stories in the unit, or  designing collaborative tasks that require students to connect the  concepts to personal life.  2) We see that the contents that have more technical terms are  associated with cognitively poorer discussion. We explained this  phenomenon as people tend to discuss less when they understand  less. This may suggest that support for reading comprehension  where technical terms come up might improve productive  engagement with the material. Further, scaffolding for explanation  should be provided in the units that have less of a tendency to  elicit explanation behavior.    3) We also found that a lot of discussions in the forums are off- topic. One way to address this is to take the off-topic conversation  that comes and try to channel it towards course content.  As it  stands, off-topic conversation may not be valuable for learning,  but if we can use it to draw attention to personal connections with  the course material, it could become valuable. Providing scripts  [13] to achieve this could also be a future direction.  4) In this paper, we developed a coding manual to categorize  students discussion behaviors by the cognitive engagement  displayed in their discussion. In a next step, we want to build  machine learning models to automatically label students  discussion behaviors as in our prior work [26], thus enabling the  provision of just-in-time feedback. For example, if the analysis  reveals that a student is constantly engaging in off-topic  discussion in the forum, we could respond by providing scripts to  direct him towards course content. Or if we see a student is  constantly repeating or paraphrasing course materials without  constructing ideas on top of that, we could also provide scripts for  these students, e.g, asking probing questions to get them think  about the content they are posting.   7. CONCLUSION AND LIMITATION  In order to better support discussion in a MOOC context, we  investigate what kinds of discussion behaviors are associated with  learning and what types of learning materials trigger richer  discussion. We developed a coding manual based on Chis [8]  ICAP framework and situated it in a MOOC context to categorize  students posts in discussion forums based on different observed  levels of cognitive engagement. We achieved high reliability  when applying this coding manual in a psychology MOOC.   Driven by the first research question, we found that students who  displayed more higher-order thinking behaviors learnt more  through deeper engagement with course materials displayed by  their discussion behaviors. In this course, students who displayed  higher-order thinking behaviors learnt more than students who  simply directed their attention to course materials.  These students  in turn also learnt more than students who were constantly off- topic in the forums.  The follow-up analysis with LDA topic modeling applied to  course materials informs us that social oriented topics triggered  richer discussion compared with biopsychology oriented topics,  and that higher-order thinking behaviors tend to appear together  within threads in the forums. We have suggested design  recommendations based on these observations.  However, there are limitations in this work, which could be  addressed and improved in future research.  1) There are relatively few instances of interactive behaviors in  the dataset, so we grouped together constructive and interactive  behaviors as higher-order thinking behaviors. Because of this, we  are not able to compare the effect between interactive behaviors  and constructive behaviors on learning. A next step is to apply this  coding manual to additional courses so that we will have more  data to address this sparsity issue.  In doing this, we will also have  more training data to build machine learning models to detect the  discussion behaviors automatically.  2) In our approach, we are only able to estimate students  cognitive engagement if they display it by posting in the  discussion forum. It is possible that students are highly engaged  with course materials even if they never display those thinking  behaviors in the discussion forum. Although we have already  controlled for students engagement in other course activities, we  are losing information about invisible learning traces. Discussion  data would be more valuable as a lens on engagement if more  students posted to the forum.  In our future research through  deploying interventions, we aim to engage more students in the  forums so that assessment based on discussion behavior can be  applied to a higher proportion of students in the course.    8. ACKNOWLEDGEMENTS  This project is funded by NSF Grant ACI-1443068 and funding  from Google.    9. REFERENCES  [1] Allen, L. K., Snow, E. L., & McNamara, D. S. (2015). Are   you reading my mind: modeling students' reading  comprehension skills with natural language processing  techniques. In Proceedings of the Fifth International  Conference on Learning Analytics And Knowledge. ACM.  DOI= http://dx.doi.org/10.1145/2723576.2723617.   [2] Anderson, A., Huttenlocher, D., Kleinberg, J., & Leskovec,  J. (2014). Engaging with massive online courses. In  Proceedings of the 23rd international conference on World  wide web (pp. 687-698). International World Wide Web     Conferences Steering Committee. DOI=  http://dx.doi.org/10.1145/2566486.2568042   [3] Austin, P. C. (2011). An introduction to propensity score  methods for reducing the effects of confounding in  observational studies. Multivariate behavioral research,  46(3), 399-424. DOI=  http://dx.doi.org/10.1080/00273171.2011.568786   [4] Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent  dirichlet allocation. the Journal of machine Learning  research, 3, 993-1022.   [5] Brinton, C. G., Chiang, M., Jain, S., Lam, H. K., Liu, Z., &  Wong, F. M. F. (2014). Learning about social learning in  MOOCs: From statistical analysis to generative model.  Learning Technologies, IEEE Transactions on, 7(4), 346- 359. DOI= http://dx.doi.org/10.1109/TLT.2014.2337900.   [6] Brooks, C., Chavez, O., Tritz, J., & Teasley, S. (2015).  Reducing selection bias in quasi-experimental educational  studies. In Proceedings of the Fifth International Conference  on Learning Analytics And Knowledge (pp. 295-299). ACM.  DOI= http://dx.doi.org/10.1145/2723576.2723614   [7] Chi, M. T. H., Siler, S., Jeong, H., Yamauchi, T.,  &Hausmann, R. G. (2001). Learning from human tutoring.  Cognitive Science, 25, 471533. DOI=  http://dx.doi.org/10.1016/S0364-0213(01)00044-1   [8] Chi, M. T., & Wylie, R. (2014). The ICAP framework:  Linking cognitive engagement to active learning outcomes.  Educational Psychologist, 49(4), 219-243. DOI=  http://dx.doi.org/10.1080/00461520.2014.965823   [9] Cohen, E. G. (1994). Restructuring the classroom:  Conditions for productive small groups. Review of  Educational Research, 64, 135. DOI=  http://dx.doi.org/10.3102/00346543064001001   [10] Durik, A. M., & Harackiewicz, J. M. (2007). Different  strokes for different folks: How individual interest moderates  the effects of situational factors on task interest. Journal of  Educational Psychology, 99(3), 597. DOI=  http://dx.doi.org/10.1037/0022-0663.99.3.597   [11] Ezen-Can, A., Boyer, K. E., Kellogg, S., & Booth, S. (2015).  Unsupervised modeling for understanding MOOC discussion  forums: a learning analytics approach. In Proceedings of the  Fifth International Conference on Learning Analytics And  Knowledge (pp. 146-150). ACM. DOI=  http://dx.doi.org/10.1145/2723576.2723589   [12] Ferguson, R., & Clow, D. (2015). Examining engagement:  analysing learner subpopulations in massive open online  courses (MOOCs). In Proceedings of the Fifth International  Conference on Learning Analytics And Knowledge. ACM.  DOI= http://dx.doi.org/10.1145/2723576.2723606   [13] Fischer, F., Kollar, I., Stegmann, K., Wecker, C. &  Zottmann, J. (2013). Collaboration scripts in computer- supported collaborative learning. The international handbook  of collaborative learning, 403-419.   [14] Greer, J., & Mark, M. (2015). Evaluation Methods for  Intelligent Tutoring Systems Revisited. International Journal  of Artificial Intelligence in Education, 1-6.   [15] Hidi, S., & Renninger, K. A. (2006). The four-phase model  of interest development. Educational psychologist, 41(2),  111-127. DOI=  http://dx.doi.org/10.1207/s15326985ep4102_4   [16] Hsiao, I. H., & Awasthi, P. (2015). Topic facet modeling:  semantic visual analytics for online discussion forums. In  Proceedings of the Fifth International Conference on  Learning Analytics And Knowledge (pp. 231-235). ACM.  DOI= http://dx.doi.org/10.1145/2723576.2723613   [17] Kizilcec, R. F., Piech, C., & Schneider, E. (2013).  Deconstructing disengagement: analyzing learner  subpopulations in massive open online courses. In  Proceedings of the third international conference on  learning analytics and knowledge (pp. 170-179). ACM.  DOI= http://dx.doi.org/10.1145/2460296.2460330.   [18] Klusener, M., & Fortenbacher, A. (2015). Predicting  students' success based on forum activities in MOOCs. In  Intelligent Data Acquisition and Advanced Computing  Systems: Technology and Applications, 2015. IEEE. DOI=  http://dx.doi.org/10.1109/IDAACS.2015.7341439   [19] Koedinger, K. R., Kim, J., Jia, J. Z., McLaughlin, E. A., &  Bier, N. L. (2015, March). Learning is Not a Spectator Sport:  Doing is Better than Watching for Learning from a MOOC.  In Proceedings of the Second (2015) ACM Conference on  Learning@ Scale (pp. 111-120). ACM. DOI=  http://dx.doi.org/10.1145/2724660.2724681   [20] Koedinger, K. R., McLaughlin, E. A., Jia, J. Z., & Bier, N. L.  (2016). Is the Doer Effect a Causal Relationship  How Can  We Tell and Why Its Important. In Proceedings of the 6th  International Conference on Learning, Analytics and  Knowledge.   [21] McKendree, J., Stenning, K., Mayes, T., Lee, J., & Cox, R.  (1998). Why observing a dialogue may benefit learning.  Journal of Computer Assisted Learning, 14(1), 110-119.  DOI= http://dx.doi.org/10.1046/j.1365-2729.1998.1420110.x   [22] McNamara, D. S., Kintsch, E., Songer, N. B., & Kintsch, W.  (1996). Are good texts always better Interactions of text  coherence, background knowledge, and levels of  understanding in learning from text. Cognition and  instruction, 14(1), 1-43. DOI=  http://dx.doi.org/10.1207/s1532690xci1401_1   [23] Robinson, A. (2015). Exploring Class Discussions from a  Massive Open Online Course (MOOC) on Cartography. In J.  Brus et al. (eds.), Modern Trends in Cartography, Lecture  Notes in Geoinformation and Cartography. DOI=  http://dx.doi.org/10.1007/978-3-319-07926-4_14    [24] Ros, C. P., Carlson, R., Yang, D., Wen, M., Resnick, L.,  Goldman, P., & Sherer, J. (2014, March). Social factors that  contribute to attrition in moocs. In Proceedings of the first  ACM conference on Learning@ scale conference. ACM.  DOI= http://dx.doi.org/10.1145/2556325.2567879   [25] Rosenbaum, P. R., & Rubin, D. B. (1985). Constructing a  control group using multivariate matched sampling methods  that incorporate the propensity score. The American  Statistician, 39(1), 33-38. DOI=  http://dx.doi.org/10.1080/00031305.1985.10479383   [26] Wang, X., Yang, D., Wen, M., Koedinger, K., & Ros, C. P.  (2015). Investigating how students cognitive behaviors in  MOOC discussion forums affect learning gains. In  Proceedings of the 8th International Conference on  Educational Data Mining.   [27] Zhu, E. (2006). Interaction and cognitive engagement: An  analysis of four asynchronous online discussions.  Instructional Science, 34(6), 451-480.     "}
{"index":{"_id":"50"}}
{"datatype":"inproceedings","key":"The:2016:SEF:2883851.2883871","author":"The, Benedict and Mavrikis, Manolis","title":"A Study on Eye Fixation Patterns of Students in Higher Education Using an Online Learning System","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"408--416","numpages":"9","url":"http://doi.acm.org/10.1145/2883851.2883871","doi":"10.1145/2883851.2883871","acmid":"2883871","publisher":"ACM","address":"New York, NY, USA","keywords":"cognitive activity, eye tracking, human-computer interaction, instructional design, online learning","abstract":"We study how the use of online learning systems stimulate cognitive activities, by conducting an experiment with the use of eye tracking technology to monitor eye fixations of 60 final year students engaging in online interactive tutorials at the start of their Final Year Project module. Our findings show that the students' visual scanning behaviours fall into three different types of eye fixation patterns, and the data corresponding to the different types relates to the performance of the students in other related academic modules. We conclude that this method of studying eye fixation patterns can identify different types of learners with respect to cognitive activities and academic potentials, allowing educators to understand how their instructional design using online learning environments can stimulate higher-order cognitive activities.","pdf":"A Study On Eye Fixation Patterns of Students in Higher  Education Using an Online Learning System      Benedict The   School of Computing  National University of Singapore   13 Computing Drive  Singapore 117417   +65 65168354  the.benedict@nus.edu.sg     Manolis Mavrikis  UCL Knowledge Lab  23-29 Emerald Street  London WC1N 3QS   United Kingdom  +44 (0) 20 7907 4634   m.mavrikis@ucl.ac.uk  ABSTRACT  We study how the use of online learning systems stimulate  cognitive activities, by conducting an experiment with the use of  eye tracking technology to monitor eye fixations of 60 final year  students engaging in online interactive tutorials at the start of their  Final Year Project module. Our findings show that the students  visual scanning behaviours fall into three different types of eye  fixation patterns, and the data corresponding to the different types  relates to the performance of the students in other related academic  modules. We conclude that this method of studying eye fixation  patterns can identify different types of learners with respect to  cognitive activities and academic potentials, allowing educators to  understand how their instructional design using online learning  environments can stimulate higher-order cognitive activities.     Categories and Subject Descriptors   Applied computing~Interactive learning environments       Social and professional topics~Information technology education   General Terms  Experimentation, Measurement, Performance   Keywords  Eye Tracking, Human-Computer Interaction, Instructional Design,  Cognitive Activity, Online Learning   1. INTRODUCTION  The continuous advancement of teaching and learning with the  increasing use of technology within the vast selections of modules  offered in higher education has been evident over the years. Most  examples of the advancement are contributed by the increasing use  of online learning systems in the learning environment. The rapid  embracing of such technology and its pervasive use in teaching and  learning have been brought about by strong supporting changes,  ranging from institutional vision and philosophy towards  encouraging the use of technology, to the rising focus of building   and developing educators competencies in using technologies to  enhance instructional design.    A common use of learning systems in education is the wide range  of multimedia functionalities, which offer many features that can  enhance student learning. One study uses computers multimedia  capabilities to lend a sensory component that help reinforce  concepts and appeal to a wider variety of approaches to learning  [31]. It is highlighted that graphical aspects help students visualize  two- and three-dimensional geometric figures and represent  mathematical ideas such as the nature of arithmetic versus  exponential growth. It is further emphasized that students can make  conjectures and experiment with these graphical representations to  see the results. Another team of researchers has also explored the  use of technology to explore the impact of multimedia resources  situated in a national e-learning portal to improve overall science  learning experiences [7].   For the School of Information Technology, Nanyang Polytechnic  in Singapore, the institution where our empirical research was  conducted, the introduction of online learning systems was focused  towards online tutorial and problem solving activities. The  introduction of online learning systems for teaching and learning of  computer programming for students in higher education has  benefited both the educators and the students. For the educators,  they are able to conduct their lessons using such systems as tools  for course material management, assignment submission, setting  and conducting of assessment, monitoring of grade performance  and student feedback. For the students, their learning deepens with  online collaborative work with peers, timely performance  feedback, instantaneous access to online course material, and  interactive engagement of online assignments. Although a lot has  been done to facilitate teaching and learning with the use of  technology as a communicative and collaborative channel, very  little study has been done on cognitive processes of students as they  learn using online tools, particularly in the field of human-computer  interaction, on how the instructional design stimulates higher-order  cognitive activities.   Learning is also optimized when accompanied by problem solving  activities [6], and problem solving induces higher-order cognitive  activities learning as it forces students to be active participants in  their learning rather than passive information receivers [2]. Online  courses such as those provided by lyndaCampus and Codecademy  are integrated into various academic modules to heighten the  learning experience of the students. However, individual  differences among the students would account for a variance in  learning outcomes [18, 25]. There is thus a need to address the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  http://dx.doi.org/10.1145/2883851.2883871.     diversity in the students learning approaches, the broad spectrum  of individual abilities, and the diversity of socioeconomic and  cultural backgrounds [1]. Another study also highlights that  learners cognitive load fluctuates during interaction while using  online learning tools [4]. From prior research studies, cognitive  activities and deep thinking has corresponded to measurements  such as eye fixations, saccades, dwell time percentage, pupil  diameter and blink rate [4, 24, 27, 28, 29]. It is unclear, however,  as to how these measurements eye movements by learners are  related to their learning experiences, and if learning indeed do  occurs, how such readings are related to individual academic results  in recent past, or use as a predictive method for future academic  performances. To our knowledge, no research work has been done  to explore these connections. Our study will thus attempt to  determine the different patterns of eye fixations and saccades of  students learning computer programming from such online learning  systems, in particular Codecademy, to their individual academic  abilities and performances via grading achieved from related  programming modules. The result will seek to help educators  understand the effectiveness of the online learning systems towards  achieving higher-order cognitive processing, when applied to their  students.   2. RELATED WORK  The use of eye tracking technology in the study of cognitive  processing has seen an increasing amount of interest in recent  research works. Most of the literature are based on two theoretical  assumptions about the relation between eye movement and  cognitive processing, namely the immediacy assumption and the  eye-mind assumption, referring to the work of Just & Carpenter  [19]. In immediacy assumption, information processing is  immediate and occurs when the information is encountered. In eye- mind assumption, the visual information that falls within the focus  of attention is also being processed, and the direction of gaze is  closely associated with the focus of attention. We have thus  conducted our research with these theoretical assumptions in mind.   Before the use of eye tracking technology in educational  psychology and educational research, thinking aloud was the  method employed in most studies on cognitive processing [3, 5,  16]. In the attempt to understand what happens during learning,  some studies have generally used the think-aloud research  methodology while readers are involved in text processing.  However, this methodology is known to have a limitation of being  intrusive. It is suggested that thinking aloud may alter the process  of thinking itself, because it requires cognitive resources that  should be used in carrying out the primary task of learning [22].  Eye tracking does not have this limitation. It does not interrupt  normal reading, and thus will not lead to disruptions in cognitive  processing. The eye tracking data collected will completely account  for the allocation of visual attention of the learner during task  execution, and such measures can be used to draw inferences about  cognitive processes.   Prior research works on eye tracking collects fixation duration and  gaze duration measurements to draw inferences about cognitive  processes [26]. Single fixation duration refers to cases where only  a single fixation is made on a word, and gaze duration refers to the  sum of all fixations on a word prior to moving to another word. In  general, researchers agree that visual attention is related to  cognitive processing activities. It is through the interpretation of  eye movement data that researchers attempt to understand the  relationship of such measures and the underlying cognitive  processes.   In our study of students interaction using a technology enhanced  learning environment, we are inspired by the works of Hyn &  Lorch [13], where they have highlighted that the various segments  that comprise a text are not given equal processing time. Another  similar research paper has also indicated that a text segment that  introduces a new topic or a new narrative episode is attended more  than the segments that are continuations of the same topic or  episode [21]. The conclusive segment at the end of the text has also  been found to receive more attention, and the end sentences have  been considered as the location of gaze for wrap up processing  [11]. This inequality of attention was also highlighted in other  similar works [9, 10], where it was reported that an increase in  cognitive load leads to longer fixation duration and an increase in  the number of fixations.    In another of such research work, eye fixations were recorded  during the reading of information of three pulley system  configurations of increasing complexity [11]. The analysis from the  collected data indicated that the readers from the university  community often reread the text to process specific information  about a component or set of components before building a spatial  mental model. Another particularly appropriate measure for our  study is suggested by Nielson [23], who has highlighted important  findings on the sequence of scanning objects on screen. From his  research, he assumes that headlines are examined first, then pictures  as well as diagrams and visual examples, followed finally by text.  This assumption will also be looked into in our study as well.   An important consideration for our study, as suggested by  Kruschke et al. [20], is that the pattern of learning varies across  individuals but is relatively stable within individuals. Similar  findings have also highlighted differences in pattern of learning,  where some participants achieved high accuracy very rapidly in  simple tasks, while others learn gradually [30], as well as  distinguishing between experts and novices eye movement  profiles [17] For this study, we will attempt to identify distinctive  characteristics in learning among the students.   In analysing the transition of eye fixation patterns, prior works have  used various methods towards determining a wide spectrum of  pattern types [32]. Methods such as network analysis on the  transition of eye fixations have highlighted different combinations  of the transition patterns of fixations.   However, to the best of our knowledge, the detailed studies above  did not conclude if there were any distinctive connections among  the patterns of learning and the performance of the learners in  related academic modules. This knowledge is advantageous to both  educators and learners, especially in understanding academic  performance, and improving on instructional design. Our empirical  research in this paper attempts to address this gap.   3. METHODS  3.1 Research Questions  Following the introduction and literature review in the previous  sections, we are particularly interested in the following research  questions:   Q1. What are the distinctive eye fixation patterns of the students  engaging in the online tutorials   Q2. How are the fixation patterns connected with the students  performances in related academic modules     3.2 Experiment Setup  A total of 60 final year students from the School of Information  Technology, Nanyang Polytechnic, were involved in the  experiment. They were at the start of their Final Year Project  module, and were selected to complete a series of online tutorials  to learn how to program in PHP using an online learning tool called  Codecademy. Each student did not have prior knowledge of  programming in PHP, and were new to the online learning  environment of Codecademy. The students were also selected  randomly. English as the medium of instruction was not highlighted  as a problem for any of the students.   3.3 The Codecademy Tutorials   The students were to individually complete 13 tasks regarding  different programming functions and features of PHP, a popular  programming language. These tasks were made available in the  form of an online interactive learning module provided by  Codecademy. An example of the online learning environment user  interface is shown in Figure 1.             Figure 1: User Interface of Codeacademy   For the online interactive PHP tutorials, the user interface is divided  into 5 main blocks. Introduction(A), Instructions(B) and Hints(C)  are arranged in sequence on the left column of the screen. The  programming Editor(D) is provided at the centre of the screen. A  programming Output(E) is displayed on the top right corner of the  screen to show the results generated by the program.   The 13 tasks were presented to each student in sequence.  Introduction presented the title and fundamentals of the task,  followed by Instructions of how to solve the task. Once the students  completed each task, they would click the Save and Submit Code  button, to proceed to the next task. If a task was done incorrectly,  an error message would be displayed at the bottom of the screen,  and the student would not be allowed to proceed to the next task.  An option for hints was provided for students who were not able to  proceed and thus needed some guiding help. Hints were not  displayed unless requested for.    Experimental sessions were conducted individually for each  student using a setup as shown in Figure 2 and Figure 3, using an  eye tracking device from Eye Tribe connected to a laptop in a  computer laboratory.   Each data collection session for a student to complete all require  tasks lasted from 15 to 30 minutes. At the start of each session, the  eye tracking device would be calibrated for each student, before the  student proceeded to complete the tasks online. A video screen  capture was also recorded while the student proceeded with  completing the online tasks. This was to validate eye tracking   records with matching movements and inputs as they complete the  online tutorials.        Figure 2: Eye Tribe scanner connected to a laptop           Figure 3: A student during an experimental session   3.4 Methodology  Our study focuses on the investigation of eye fixation patterns of  students engaging in online learning tool, with the use of eye  tracking analysis as the basic method [8, 12]. There are three main  eye tracking parameters used in this study, namely the number of  fixations, the duration of fixations and sequence of saccades.  Fixation plots are generated to reveal the areas of interest of each  participant as they progress through their online tutorials. Scan  paths that display the series of saccades and the time duration for  each fixation are also generated and analysed.   3.5 Data Collection Using Eye Tribe  The Eye Tribe eye tracking device came with a software that helped  capture eye movement coordinates, determine fixations, and record  a time stamp for each fixation. The records were generated into a  text file at the end of each session. After conducting data collection  for all students, the text files were retrieved and stored for further  analysis.   The text files containing eye movement data were subsequently  imported into Tableau Desktop 9.0 to generate fixation plots across  different areas. Our research found that the fixation plots can be  classified into 3 distinctive patterns (Figure 4.1  4.3), which were  generated from the students based on the number of fixations,  corresponding to the 5 main blocks in Codecademys user interface,  as they complete the online tutorials assigned. This is done by  visual observation of the plot patterns generated by the  visualisation software.   Eye Tribe  Scanner   (A)   (B)   (C)   (D) (E)          Figure 4.1: Pattern with fixations across all blocks (Type FP1)          Figure 4.2: Pattern without many fixations on Introduction  and Instructions, but a lot on Hints (Type FP2)           Figure 4.3: Pattern with many fixations on all blocks except  Hints (Type FP3)   The first type of pattern (Type FP1) produced a consistent  distribution of fixations to all blocks while completing the tasks.  Students who displayed this pattern of engagement started by  attempting to comprehend the topics as explained in the  Introduction, and subsequently tried to complete the tasks  displayed in the Instructions block. However, the high count in  fixations to Hints suggested that these students had failed in   completing the tasks, and had requested for help within the Hints  block. This is due to the students finding the tasks too advanced or  having misconceptions in the tasks.   Students who displayed the pattern similar to that of the second type  (Type FP2), with low count of fixations on Introduction and  Instructions but a lot on Hints, may generally had completed the  tasks by copying the solutions from the hints. Their fixation plot  suggested that they did not value the need to understand the  rudiments of the topics, and favoured more towards completing the  tasks quickly. It could thus be inferred that since these students  appear not to participate in cognitive activities with respect to the  understanding of the topics, they had not effectively learned from  Introduction or Instructions blocks.    Students who displayed the pattern that corresponded with the third  type (Type FP3), with little fixations to Hints, generally completed  the tasks by reading through the introduction and instructions. Their  fixation plot suggested that they attempted to comprehend the  fundamentals of the topics, and followed the instructions  accordingly. The low count in fixations to Hints suggested that the  learning strategy of these students was to process the requirements  within the text of the instructions and to try to produce the solution  according to their understanding. These students were thus  considered to have higher-order cognitive activities while fixing  their attention on learning from Introduction and Instructions  throughout their sessions [10, 12].   Scan paths were also generated for our study, to observe the  sequences of attention focus, as well as the patterns of look-back  and rereading behaviours. Such patterns of scanning behaviours  were studied extensively [15], where 4 types of reading strategy  were identified with their distinctive features. Fast linear readers  did not make return fixations on previous texts. Slow linear  readers made many rechecks before moving on. Non-selective  readers made many look-backs to previous sentences. Topic  structure readers paid close attention to headings and were also  those who produce the most accurate text summaries, a measure of  knowledge retention by the participants. These similar findings  were again highlighted in another study, where it was found that  the quality of recalls of the main ideas presented in the text also  correlated with the amount of time spent on look-back and  rereading by the participants [14].   Scan plots of the data collected from all the sessions were  generated. It was observed that the participants scan plots largely  fell into 3 most profound and distinguished types (Figure 5.1 -   Figure 5.3).   For the first type (Type SP1) of scan path in Figure 5.1, it mirrored  characteristics similar to slow linear readers, as students started  by placing their attention to headings in Introduction and  Instructions, and made many rechecks as they move their attention  between the Introduction and Instructions blocks. The darker the  shades of the plots around a localized area, the longer is their gaze.  For the second type (Type SP2) of scan path in Figure 5.2, it  mirrored characteristics similar to fast linear readers, and is  similar to the previous type in terms of its linear scan path sequence.  However, these students quickly move their attention to Editor and  Output as they complete their tasks without frequent rechecks to  headings in Introduction and Instructions. For the third type (Type  SP3) of scan path in Figure 5.3, students displayed a significant  amount of look-backs to Instructions as the students complete their  task on the Editor. This type of scan path resembled that of topic  structure readers.          Figure 5.1: Scan path showing characteristics of slow linear  readers with darker plots (Type SP1)           Figure 5.2: Scan path showing characteristics of fast linear  readers with lighter plots (Type SP2)           Figure 5.3: Scan path showing characteristics of topic  structure readers (Type SP3)      3.6 The Datasets  We conducted Linear Regression analyses using records gathered  to form 3 datasets. For the first dataset (snapshot in Table 1), we  are able to derive from the eye fixations data the number of counts  of Fixation Plot Type (FP1  FP3) from the set of completed  tutorials for each student.         Table 1: Excerpt of data showing counts of Fixation Plot Type  for tutorials completed by each student.    FP1 FP2 FP3   Student01 4 2 7   Student02 3 4 6   Student03 6 3 4          Student59 2 3 8   Student60 3 7 3     Similarly, in the second dataset (snapshot in Table 2), from the eye  fixations data, we again derived the number of counts of Scan Path  Type (SP1  SP3) from the set of completed tutorials for each  student.   Table 2: Excerpt of data showing counts of Scan Path Type  for tutorials completed by each student.    SP1 SP2 SP3   Student01 2 3 8   Student02 3 3 7   Student03 6 6 1          Student59 1 3 9   Student60 4 5 4     The third dataset (snapshot in Table 3) is a collection of the  academic grades of all programming project modules in which the  students were enrolled during their course of study. As a third  (final) year student, all students will have completed 4 semesters of  project modules (Proj01  Proj04).    From the grades score of the 4 project modules, we further  aggregated the score to obtain the average score (ProjAve). We also  recorded the students Grade Point Average (GPA) of all academic  modules which the students obtained by the end of Year 2  (Yr2GPA).   Table 3: Excerpt of data showing results of students past  grades from project modules and their GPA.     Proj 01   Proj 02   Proj 03   Proj 04   Proj  Ave   Yr2 GPA   Student01 79 67 77 79 75.50 3.72   Student02 92 84 75 78 82.25 3.68   Student03 63 55 63 64 61.25 1.74             Student59 73 61 91 78 75.75 2.73   Student60 72 80 73 70 73.75 3.56           4. RESULTS  This study seeks to understand how the fixation plot and scan path  are related to the project and academic grades of the students, we  used SPSS Statistics version 22 to run regression analysis on the  datasets collection mentioned in the previous section. For this  study, the multiple regression model used are having three  independent variables, namely FP1  FP3 for fixation plot analysis,  and SP1  SP3 for scan path analysis. The following sections will  show the effects of both fixation plot and scan path on project  average and end of Year 2 GPA scores, by deriving the coefficient  of the three independent variables for the linear regression analyses.   4.1 Fixation Plot and Project Average  Table 4 below shows the coefficients of the independent variables  FP1  FP3 effect on the project average (ProjAve).   Table 4: Coefficients of the independent variables FP1  FP3  effect on the project average (ProjAve).   Coefficientsa,b   Model   Unstandardized   Coefficients   Standardized   Coefficients   Sig. B Std. Error Beta   1 FP1 3.730 .336 .195 .000   FP2 6.206 .351 .370 .000   FP3 6.574 .147 .534 .000   a. Dependent Variable: ProjAve   b. Linear Regression through the Origin  The Adjusted R Square value for this analysis is 0.996, which  means that our linear model fits a set of observations very well.  From the results above, it can be seen that FP3 has the highest  standardized coefficient value that contributes positively to the  prediction of project average scores of the pool of students. It can  also be seen that FP1, having the lowest standardized coefficient  value, provides the least in the prediction of the students project  average grades. The randomness of the residual vs fitted plot below  shows that regression analysis is not biased.      Figure 6: Residual vs Fitted Plot for Fixation Plot Type effect  on Project Average   4.2 Fixation Plot and GPA  Table 5 below shows the coefficients of the independent variables  FP1  FP3 effect on the end of Year 2 GPA (Yr2GPA).   Table 5: Coefficients of the independent variables FP1FP3  effect on the end of Year 2 GPA (Yr2GPA).   Coefficientsa,b   Model   Unstandardized   Coefficients   Standardized   Coefficients   Sig. B Std. Error Beta   1 FP1 .029 .033 .037 .386   FP2 .296 .035 .424 .000   FP3 .312 .015 .608 .000   a. Dependent Variable: Yr2GPA   b. Linear Regression through the Origin  For this analysis, the Adjusted R Square value is 0.978, which also  signifies the strength of the relationship between the model and the  response variables. FP3 is shown again to have the highest  standardized coefficient value of 0.608, suggest that it provides the  most effect for the students GPA performance. Likewise, 0.37 for  FP1 may suggest the lowest in contribution towards the prediction  of GPA grades. However, since the p-value for SP1 is 0.386, and  since it is over the common alpha value of 0.05, it also indicates  that it is not statistically significant. Figure 7 shows the randomness  of the residual vs fitted plot.     Figure 7: Residual vs Fitted Plot for Fixation Plot Type effect   on GPA   4.3 Scan Path and Project Average  Table 6 below shows the coefficients of the independent variables  SP1  SP3 effect on the project average (ProjAve).   For this analysis, it is again noted that the Adjusted R Square value  is 0.996. the standardized coefficient for SP3 is highest at 0.54,  suggesting that students with scan path type associated to Topic  Structure Readers may also have the highest retention of  knowledge and skillsets, and thus lead to higher project average  scores. The standardized coefficient of SP1 is the lowest at 0.195,  suggesting that it contributes the least in prediction towards project  performance. Figure 8 shows randomness of residual vs fitted plot.   -10  -5  0  5  10  15  50 60 70 80 90R es id ua ls  Predicted Value  -1.5  -1  -0.5  0  0.5  1  1.5  1 2 3 4 5Re sid  ua ls  Predicted Value    Table 6: Coefficients of the independent variables SP1  SP3  effect on the project average (ProjAve)   Coefficientsa,b   Model   Unstandardized   Coefficients   Standardized   Coefficients   Sig. B Std. Error Beta   1 SP1 3.758 .364 .195 .000   SP2 6.220 .358 .371 .000   SP3 6.543 .142 .540 .000   a. Dependent Variable: ProjAve   b. Linear Regression through the Origin     Figure 8: Residual vs Fitted Plot for Scan Path Type effect on   Project Average   4.4 Scan Path and GPA  Table 7 below shows the coefficients of the independent variables  SP1  SP3 effect on the end of Year 2 GPA (Yr2GPA).   Table 7: Coefficients of the independent variables SP1SP3  effect on the end of Year 2 GPA (Yr2GPA)   Coefficientsa,b   Model   Unstandardized   Coefficients   Standardized   Coefficients   Sig. B Std. Error Beta   1 SP1 .035 .035 .044 .330   SP2 .290 .035 .416 .000   SP3 .312 .014 .620 .000   a. Dependent Variable: Yr2GPA   b. Linear Regression through the Origin  The linear regression report for this analysis also shows a high  Adjusted R Square value of 0.976. Similarly, from the above table,   SP3 has the highest standardized coefficient of 0.620, which  implies that it strongly supports GPA performances. It can also be  seen that for SP1, with a p-value of 0.330, it is not a significant  variable to affect students end of Year 2 GPA performance. Figure  9 below shows the randomness of the residual vs fitted plot.     Figure 9: Residual vs Fitted Plot for Scan Path Type effect on   GPA   5. DISCUSSION  The analysis from this research has enabled us to understand that  there are strong relations among different types of eye fixation  measurements and behavioural patterns in association with the  level of engagement in learning and the type of learners and their  traits towards learning. The results also suggested that students who  display different visual scanning behaviours have significantly  achieve different scores in their GPA and project grades. This may  give rise to early detection of students who may need more help and  assistance in learning well before the assessments at the end of the  semesters.   We can further suggest that although the approach towards learning  among the students varies, we found that students who are  identified as engaged learners using the online learning system, are  also high achievers in related academic modules. This finding  indicates that the students exhibit consistent learning traits across  modules conducted in the traditional methods of teaching, and  modules using online learning as a teaching tool. Therefore, by  capturing eye fixation measurements, educators may be able to  identify, even before any form of assessments, students who may  need more help in achieving better academic performance.  Educators can also use the findings as a measure of how effective  the instructional design of their online tutorials are in enhancing  learning with higher-order cognitive activities.   The limitations of this research study lie in the dependencies of the  results with the particular online UI of CodeAcademy. However, as  these UI features, namely onscreen blocks of Introduction,  Instructions, Hints, programming Editor and Outcome are  generally found in other Integrated development environments  (IDEs) for programming, our findings and insights remain useful  and applicable.. Other communicative and collaborative channels,  such as forum discussions and synchronous or asynchronous chat  features, may provide richer insights on the visual scanning  behaviours and traits of the students. In such cases, more variations  of eye fixation pattern types may arise, which could expand the  understanding of cognitive processing of different groups of  learners.   In order to deepen the understanding of the relationship between  eye fixation behaviours and cognitive activities, a more general   -10  -5  0  5  10  15  50 60 70 80 90R es id ua ls  Predicted Value  -1.5  -1  -0.5  0  0.5  1  1 2 3 4  Re sid  ua ls  Predicted Value    approach towards determining fixation plot and scan path types  should also be done to handle a larger variation of patterns [31].  Methods such as network analysis on the transition patterns of eye  fixations may result in more combinations of fixation plot and scan  path.   Another concern about the data collected is the inference of  cognitive activities and meaningful learning primarily from the  capturing of eye movement of the learners. During the course of  data collection, it is observed that students could also be in deep  thoughts while looking elsewhere, for example an open window, a  ceiling fan etc. Although these actions can also be moments  contributing to higher-order cognitive activities of the learners, the  Eye Tribe sensor did not capture them. Other types of detection  techniques, such as neuroimaging technique using  Electroencephalography (EEG) may be combined together with  eye tracking sensor data to increase the accuracy of the results.   6. CONCLUSION  This study has shown that online learning systems stimulate  cognitive activities, and that different students have displayed  different levels of engagement patterns through eye fixations.  These eye fixation patterns are also able to characterize different  types of learners, from the analysis of the dataset from eye tracking  technology that monitored the eye fixations of 60 final year  students engaging in online interactive tutorials at the start of their  Final Year Project module. Our findings further suggest that the  students visual scanning behaviours fall into three different types  of eye fixation patterns, with the data which corresponded to the  different types of learners having strong relations to the  performance of the students in other related academic modules. the  study thus concludes that this method of analysing eye fixation  patterns can identify different types of learners with respect to their  cognitive activities and academic potential, and also allow  educators to understand how their instructional design using online  learning environment can stimulate higher-order cognitive  activities.   For practical usage, the findings from this study have potentials of  allowing educators to understand the behaviours and attributes of  their students through the use of online learning tools. Although  this study uses the modest size of a datasets from the selected final  year students, it has nonetheless produce insights on the  possibilities of assessing and monitoring students learning  progress and performance apart from relying on traditional  assessment tools such as test and examination papers.   For future works, educators and researchers can also perform  research studies by providing different sets of instructional design  to facilitate online learning by different learners, and analyse the  patterns of other neuroscientific measurements of the students. The  measurements can be further studied to correlate with other  behavioural and/or academic performance metrics, to understand  how best to conduct online teaching and learning. It will be most  apparent for MOOCs, which are Massive Open Online Courses  designed for worldwide engagement, students demographical data,  time zones, language competencies etc., may also affect online  engagement behaviours.   7. REFERENCES  [1] Apostol, S., Sofronia, A., Mihailescu, D. & Petrescu, L.   (2012), One size does not fit all! A theoretical model for  adaptive tutoring systems. The 8th International Scientific  Conference eLearning and software for Education,   Bucharest, April 26-27, 2012, 44-49. DOI=   http://dx.doi.org/ 10.5682/2066-026X-12-098.   [2] Bradsford, D., Brown, A., & Cocking, R. (Eds.). (2000).  How people learn: Brain, mind, experience, and school  committee on developments in the science of learning,  Washington, DC: National Academy Press.   [3] Butcher, K. R. (2006). Learning from text with diagrams:  Promoting mental model development and inference  generation. Journal of Educational Psychology, 98, 182-197.  DOI= http://dx.doi.org/10.1037/0022- 0663.98.1.182.   [4] Chen, A. & Epps, J. (2014). Using Task-Induced Pupil  Diameter and Blink Rate to Infer Cognitive Load, Human- computer Interaction, 2014(29), 390413. DOI=  http://dx.doi.org/10.1080/07370024.2014.892428.   [5] Cromley, J. G., Snyder-Hogan, L. E., & Luciw-Dubas, U. A.  (2010), Reading comprehension of scientific text: A domain- specific test of the direct and inferential mediation model of  reading comprehension. Journal of Educational Psychology,  102, 687-700. DOI= http://dx.doi.org/10.1037/a0019452.   [6] D'Mello, S. K., Lehman, B. A., & Person, N. (2010).  Monitoring affect states during effortful problem solving  activities. International Journal of Artificial Intelligence in  Education, 20(4), 361389. DOI=  http://dx.doi.org/10.3233/JAI-2010-012.   [7] Elliot, D., Wilson, D., & Boyle, S. (2014). Science learning  via multimedia portal resources:The Scottish case, British  Journal of Educational Technology, 45(4), 571-580. DOI=  http://dx.doi.org/10.1111/bjet.12085.   [8] Duchowski, A. T. (2003). Eye Tracking Methodology:  Theory and Practice. Springer-Verlag, London, United  Kingdom.   [9] Findlay, J. M., & Kapoula, Z. (1992). Scrutinization, spatial  attention, and the spatial programming of saccadic eye  movements. The Quarterly Journal of Experimental  Psychology A: Human Experimental Psychology, 45A(4),   633-647. DOI=  http://dx.doi.org/10.1080/14640749208401336.   [10] Graf, W. & Krueger, H. (1989). Ergonomic evaluation of  user-interfaces by means of eye-movement data. In M. J.  Smith & G. Salvendy (Eds.), Work with computers:  Organizational, management, stress and health aspects, 659- 665.   [11] Hegarty, M. & Just, M. A. (1993). Constructing mental  models of machines from text and diagrams. Journal of  Memory and Language, 32, 717-742. DOI=  http://dx.doi.org/10.1006/jmla.1993.1036.   [12] Holmqvist, K (2015). Eye Tracking: A comprehensive guide  to methods and measures. Oxford University Press, Oxford,  United Kingdom.   [13] Hyn, J. & Lorch, R. F. (2004). Effects of topic headings on  text processing: Evidence from adult readers eye fixation  patterns. Learning and Instruction, 14, 131-152. DOI=  http://dx.doi.org/10.1016/j.learninstruc.2004.01.001.   [14] Hyn, J. & Nurminen, A. M. (2006). Do adult readers know  how they read Evidence from eye movement patterns and  verbal reports. British Journal of Educational Psychology,  97, 31-50. DOI=  http://dx.doi.org/10.1348/000712605X53678.     [15] Hyn, J., Lorch, R. F., Jr., & Kaakinen, J. (2002). Individual  differences in reading to summarize expository text:  Evidence from eye fixation patterns. Journal of Educational  Psychology, 94, 44-55. DOI=  http://dx.doi.org/10.1037//0022-0663.94.1.44.   [16] Jacob, R. J. K. & Karn, K. S. (2003). Eye tracking in human- computer interaction and usability research: Ready to deliver  the promises. In J. Hyona, R. Radach & H. Deubel (Eds.),  The mind's eye: Cognitive and applied aspects of eye  movement research, 573-605.   [17] Jarodzka, H., Scheiter, K., Gerjets, P., & Van Gog, T. (2010).  In the eyes of the beholder: How experts and novices  interpret dynamic stimuli. Learning and Instruction, 20, 146 154. DOI=  http://dx.doi.org/10.1016/j.learninstruc.2009.02.019.   [18] Jonassen, D. H. & Grabowski, B. L. 1993. Handbook of  Individual Difference, Learning, and Instruction. Lawrence  Erlbaum Associates, Publishers, Hillsdale, NJ.   [19] Just, M. A. & Carpenter, P. A. (1980). A theory of reading:  From eye fixations to comprehension. Psychological Review,  87, 329-354. DOI= http://dx.doi.org/10.1037/0033- 295X.87.4.329.   [20] Kruschke, J. K., Kappenman, E. S., & Hetrick, W. P. (2005).  Eye gaze and individual differences consistent with learned  attention in associative blocking and highlighting. Journal of  Experimental Psychology: Learning, Memory, and Cognition  2005, 31(5), 830-845. DOI= http://dx.doi.org/10.1037/0278- 7393.31.5.830.   [21] Lorch Jr., R. F., Lorch, E. P., & Matthews, P. D. (1985). On- line processing of the topic structure of a text. Journal of  Memory and Language, 24, 350-362. DOI=  http://dx.doi.org/10.1016/0749-596X(85)90033-6.   [22] McCrudden, M. T., Magliana, J. P., & Schraw, G. (2011),  The effect of diagram on online reading processes and  memory. Discourse Processes, 48, 69-82. DOI=  http://dx.doi.org/10.1080/01638531003694561.   [23] Nielson, J., (2001). Designing Web Usability.  Markt+Technik Verlag, Mnchen, Germany.   [24] Nisiforou, E. A. & Laghos, A. (2013). Do the eyes have it  Using eye tracking to assess students cognitive dimensions,  Educational Media International, 50(4), 247-265. DOI=  http://dx.doi.org/10.1080/09523987.2013.862363.   [25] Pozzi, S. & Bagnara, S. (2013). Individuation and diversity:  the need for idiographic HCI, Theoretical Issues in  Ergonomics Science, 14(1), 1-21. DOI=  http://dx.doi.org/10.1080/1464536X.2011.562564.   [26] Rayner, K. (1998). Eye movements in reading and  information processing: 20 years of research. Psychological  Bulletin, 124(3), 372-422. DOI=  http://dx.doi.org/10.1037//0033-2909.124.3.372.   [27] Rayner, K., Loschky, L. C. & Reingold, E. M. (2014). Eye  movements in visual cognition: The contributions of George  W. McConkie, Visual Cognition, 22(3-4), 239-241. DOI=  http://dx.doi.org/10.1080/13506285.2014.895463.   [28] Reingold, E. M. (2014). Eye tracking research and  technology: Towards objective measurement of data quality,  Visual Cognition, 22(3-4), 635-652. DOI=  http://dx.doi.org/10.1080/13506285.2013.876481.   [29] Vansteenkiste, P., Cardon, G., Philippaerts. R. & Lenoir, M.  (2015) Measuring dwell time percentage from head-mounted  eye-tracking data  comparison of a frame-by-frame and a  fixation-by-fixation analysis, Ergonomics, 58(5), 712-721.  DOI= http://dx.doi.org/10.1080/00140139.2014.990524.   [30] Webb, M. R. & Lee, M. D. (2004). Modeling individual  differences in category learning. In K. Forbus, D. Gentner, &  T. Regier (Eds.), Proceedings of the 26th annual meeting of  the Cognitive Science Society, 1440-1445.   [31] Wiest, L. R. (2001). The Role of Computers in Mathematics  Teaching and Learning, Computers in the Schools:  Interdisciplinary Journal of Practice, Theory, and Applied  Research, 17(1-2), 41-55. DOI=  http://dx.doi.org/10.1300/J025v17n01_05.   [32] Zhu, M. & Feng, G. (2015). An exploratory study using  social network analysis to model eye movements in  mathematics problem solving, In Proceedings of the 5th  international conference on learning analytics and  knowledge LAK15, 110-116. DOI=  http://dx.doi.org/10.1145/2723576. 2723591.         "}
{"index":{"_id":"51"}}
{"datatype":"inproceedings","key":"Sharma:2016:GLA:2883851.2883902","author":"Sharma, Kshitij and Alavi, Hamed S. and Jermann, Patrick and Dillenbourg, Pierre","title":"A Gaze-based Learning Analytics Model: In-video Visual Feedback to Improve Learner's Attention in MOOCs","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"417--421","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883902","doi":"10.1145/2883851.2883902","acmid":"2883902","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, eye-tracking, student attention, video based learning","abstract":"In the context of MOOCs, With-me-ness refers to the extent to which the learner succeeds in following the teacher, specifically in terms of looking at the area in the video that the teacher is explaining. In our previous works, we employed eye-tracking methods to quantify learners' With-me-ness and showed that it is positively correlated with their learning gains. In this contribution, we describe a tool that is designed to improve With-me-ness by providing a visual-aid superimposed on the video. The position of the visual-aid is suggested by the teachers' dialogue and deixis, and it is displayed when the learner's With-me-ness is under the average value, which is computed from the other students' gaze behavior. We report on a user-study that examines the effectiveness of the proposed tool. The results show that it significantly improves the learning gain and it significantly increases the extent to which the students follow the teacher. Finally, we demonstrate how With-me-ness can create a complete theoretical framework for conducting gaze-based learning analytics in the context of MOOCs.","pdf":"A Gaze-based Learning Analytics Model: In-Video Visual Feedback to Improve Learners Attention in MOOCs  Kshitij Sharma CHILI Lab, EPFL  RLC D1 740, Station 20 1015 Lausanne, Switzerland kshitij.sharma@epfl.ch  Hamed S. Alavi CHILI Lab, EPFL  RLC D1 740, Station 20 1015 Lausanne, Switzerland  hamed.alavi@epfl.ch  Patrick Jermann CEDE, EPFL  RLC D1 740, Station 20 1015 Lausanne, Switzerland patrick.jermann@epfl.ch  Pierre Dillenbourg CHILI Lab, EPFL  RLC D1 740, Station 20 1015 Lausanne, Switzerland  pierre.dillenbourg@epfl.ch  ABSTRACT  In the context of MOOCs, With-me-ness refers to the ex- tent to which the learner succeeds in following the teacher, specifically in terms of looking at the area in the video that the teacher is explaining. In our previous works, we em- ployed eye-tracking methods to quantify learners With-me- ness and showed that it is positively correlated with their learning gains. In this contribution, we describe a tool that is designed to improve With-me-ness by providing a visual- aid superimposed on the video. The position of the visual- aid is suggested by the teachers dialogue and deixis, and it is displayed when the learners With-me-ness is under the average value, which is computed from the other students gaze behavior. We report on a user-study that examines the effectiveness of the proposed tool. The results show that it significantly improves the learning gain and it sig- nificantly increases the extent to which the students follow the teacher. Finally, we demonstrate how With-me-ness can create a complete theoretical framework for conducting gaze- based learning analytics in the context of MOOCs.  Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in EducationCollaborative learning  Keywords  Eye-tracking, video based learning, MOOCs, Student atten- tion  1. INTRODUCTION The new wave of online learning  Massive Open Online  Courses  has brought with it new challenges as well as new  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.  LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00  DOI: http://dx.doi.org/10.1145/2883851.2883927  opportunities for learning analytics on a population of learn- ers with high diversity of profiles. We specifically look at the teacher-student interaction in MOOCs, and frame it from a dyadic interaction point of view: How can we measure and improve students attention in MOOC lectures The so- lution we propose in this contribution works independent of the learning topics or the students backgrounds. In ad- dition, it can stay at the periphery, keeping the students focused on the learning content.  In our previous works, we have shown that, with eye- tracking methods one can get insight into the learners per- formance irrespective of their background or the learning content [13]. Grounded in those results, this contribution presents a tool that, through visual feedback, drives the stu- dents attention to the part of the displayed content that is being explained by the teacher.  In two previous studies, we captured students attention as a response to what the teacher was saying. We addressed this situation from the teachers perspective: How much the student is with me We called this gaze-measure With- me-ness which identifies the extent to which the student was following the lecturer, i.e. paying attention to the parts of the display that correspond to the instant behaviour of the teacher. We selected two aspects of teachers be- haviour that could influence the students attention: the teachers dialogue and the deictic references.  In this paper, we build on our previous results and present a gaze-aware feedback tool to notify the learners about their levels of With-me-ness, while they watch a MOOC lecture. In addition, we construct a discussion that is structured within the Learning Analytics (LA) models with three phases [3, 9] [2]: 1) collecting data from the learners, 2) analysing different variables and their relation with learning processes and outcomes, and 3) providing the feedback to the learners and further study the change in their behaviour, the learn- ing processes and outcomes. We show how the proposed tool contributes to the third phase and completes the dif- ferent LA loops. Given the results of our previous studies coupled with the results of the study presented in this pa- per, we will argue that the notion of With-me-ness can create a complete theoretical framework for con- ducting gaze-based learning analytics in the context of MOOCs.    2. IN-VIDEO VISUAL FEEDBACK  2.1 Background In our previous studies we analyzed the learners gaze be-  havior while watching MOOC videos and developed the no- tions of perceptual and conceptual With-me-ness as follow- ing:  Perceptual With-me-ness: Is the extent to which the learner succeeds in following the teachers explicit deictic gestures. It is defined as combination of three components: 1) entry time is the temporal lag between the times a refer- ring pointer appeared on the screen and stops at the referred site (x,y); and the time student first looked at (x,y); 2) first fixation duration is how long the student gaze stopped at the referred site for the first time; 3) revisits are the number of times the students gaze came back to the referred site.  Conceptual With-me-ness: Is the extent to which the learner succeeds in following the content that is being ex- plained through other channels such as dialogue. The teacher may also verbally refer to the different objects on the dis- play. We measured how often a student looked at the object (or the set of objects) verbally referred to by the teacher during the whole course of time (the complete video dura- tion). In order to have a consistent measure of conceptual With-me-ness we normalised the time a student looked at the overlapping content (the verbal reference and the slide content) by slide duration.  User studies have shown that both the levels of With-me-ness are significantly correlated (positive) with the learning gain.  2.2 Visual feedback tool The visual feedback that we developed consists of a set  of red rectangular wire-frames highlighting the area of the screen which the teacher was talking about (Figure 1). This is made visible to the learner only when her With-me-ness levels went below the baseline.  The baseline was calculated as the average of the With- me-ness levels of the participants from our previous exper- iment (with the same video content). This baseline was cal- culated for each second of the video lecture. To calculate the baseline we took the participants from the previous experi- ment whose leaning gains were between 33 and 66 percentiles of the overall learning gains of the previous experiment. The reason for selecting this range of scores was that we wanted to give the feedback based on the typical behaviour of the students. In the remaining part of this paper this group is called the baseline group. The learning gains of the two groups are comparable as they had the same pretest and posttest. We considered only a subset of this group to de- fine our baseline, however, to compare the learning gains we will use the complete set (with 50 students).  2.3 The present Study We conducted an eye-tracking study where 27 participants  attended a MOOC lecture and received the visual feedback as described in the previous subsection. Students received the feedback whenever their With-me-ness was less than the baseline at any given point of time in the video. The hypothesis is that the gaze-aware feedback increases stu- dents With-me-ness; and thus their learning gains. Pre- cisely, through this study we address the following research questions:  Figure 1: Example of the feedback used in the ex- periment. The circumscribing red rectangle were shown if the With-me-ness of the participant went below the baseline With-me-ness at any given in- stant during the video.  1. How does the gaze-aware feedback affect the gaze pat- terns (With-me-ness) while watching the video  2. How does the gaze-aware feedback affect learning gains  2.4 Experiment  2.4.1 Participants and procedure There were 27 bachelor students from Ecole Polytech-  nique Federale de Lausanne, Switzerland participating in the present study. There were 6 females among the partici- pants. The participants were compensated with an equiva- lent of $30 for their participation in the study.  Upon their arrival in the laboratory, the participants signed a consent form. Then the participants took a pretest about the video content. Then the participants watched two videos about Resting membrane potential. Finally, they took a posttest. The videos were taken fromKhan Academy. The total length of the videos was 17 minutes and 5 seconds. It is worth mentioning that the teacher was not physically present in the video. The participants were told that the feedback would appear only when they were not paying at- tention to what the teacher was saying or writing.  2.4.2 Dependent variables Learning Gain: The learning gain was calculated as the  difference between the individual pretest and posttest scores. The minimum for each test was 0, and the maximum for the pretest was 9 and for the posttest was 10.  With-me-ness: We used the same method as described Section 2.1, to calculate students With-me-ness levels, in this experiment, in real time.  2.5 Results and interpretation Feedback and Learning Gain: We observed a sig-  nificant improvement in learning gain for the experimental group over that for the baseline group (t (df = 49.88) = - 2.50, p = .02, figure 2). The two populations (in the baseline and the experimental) were largely similar (the participant recruitment was done using the same university channel, and there was no drastic changes in student populations) in the two conditions.  Immediate effect of feedback on gaze: We observed a significant improvement in With-me-ness levels for par-    ticipants (within the experimental group) before (mean = 0.31, sd = 0.08) and after (mean = 0.57, sd = 0.16) display- ing the feedback (F [1, 26] = 310, p < .001, figure 3). The duration of each instance of the displayed feedback was min- imum 2 seconds. The With-me-ness levels were significantly higher after showing the feedback than before showing the feedback. It can be explained by the salient nature of the feedback: since the red rectangles appeared as a salient vi- sual feature for the participants, their attention was drawn towards the feedback.  0 .3  5 0 .4  0 0 .4  5 0 .5  0  Experimental conditions  L e a r n in  g  g  a in     (n o r m  a li s e d  b  e tw  e e n  0   a n d  1  )  Baseline  group  Experimental   group  n=50 n=27  Figure 2: Learning gain for the experimental and baseline conditions.  0 .0  0 .2  0 .4  0 .6  0 .8  1 .0  Feedback timing  W it h   m e   n e s s  l e v e ls  1.Before   feedBack  2.After  feedBack  n=27 n=27  Figure 3: Immediate effect of feedback on With-me- ness.  Overall effect of feedback on gaze: In order to find the overall effect of the feedback on the participants gaze, we divided the whole video in one minute episodes. Results from a linear mixed effect model showed that on average, participants With-me-ness increased by 1% every minute. This improvement was significant over time (F [1, 26] = 32.60, p < .0001). Figure 4 shows the temporal evolution for the difference between the observed mean With-me-ness and the baseline With-me-ness for the participants; and the average number of times the feedback was shown to the par- ticipants. We can see in Figure 4 that, towards the end of the video, the difference increased and the number of feed- back displayed decreased. This showes that the participants became more aware of the fact that they should follow the  0.0  0.2  0.4  0.6  0 5 10 15  Time (minutes)  Figure 4: Overall effect of feedback on the gaze. The whole video was divided into one minute episodes. The red curve shows the difference between the ob- served and baseline With-me-ness (smoothed using a two minute rolling window). The bars denote the number of required feedback per participant per minute.  teacher in an efficient manner. The significant long term effect on the With-me-ness in-  dicates that the feedback had an effect on participants at- tention in the terms of how well they follow the teacher in both the deictic and dialogue spaces. One plausible in- terpretation of increase in With-me-ness over time could be that the participants internalized the fact that following the teacher during the MOOC video is important to understand the content and thus they started following the teacher more closely than before. This effect is also evident from Figure 4, where we can see that the difference between the baseline With-me-ness and the observed With-me-ness was higher during the second half of the video.  3. RELATED WORK In this section, we present the previous studies demon-  strating the relation between 1) gaze and dialogues, 2) gaze- awareness, and 3) learning analytics models proposed in the literature.  3.1 Gaze in communication and referencing Gaze and speech are coupled. Mayer et. al.,[11] showed  that the time duration between looking at an object and naming it is between 430 and 510 milliseconds. [7] showed that there exists an eye-voice span of about 900 millisec- onds. The eye-voice span denotes the time between looking at a picture and start to provide a short explanation to it. Allopenna et. al.,[1] showed that the mean delay between hearing a verbal reference and looking at the object of ref- erence (the listeners voice-eye span) was between 500 and 1000 ms.  Richardson et. al.,[4] proposed the eye-eye span as the difference between the time when the speakers started look- ing at the referent and the time when listeners looked at the referred object. This time lag was termed as the cross- recurrence between the participants. The results show that    the cross recurrence was correlated with the correctness of the answers given by the listeners in a comprehension quiz. The average cross-recurrence was found to be between 1200 and 1400 milliseconds.  Jermann and Nussli[10] extended the concept of cross- recurrence in a pair programming task, by enabling the re- mote collaborators to share their selections on the screen. Results showed that the cross-recurrence levels were higher when there was a selection present on the screen than the times when there was no selections on the screen. Moreover, the cross-recurrence was higher, in the case, when a selection was followed by a verbal explanation.  Gergle et. al.,[6] conducted a dual eye-tracking study where the participants completed a collaborative reference elicitation task. The participants were given four replicas for the same sculpture. The key task for the participants was to find the correct replica. The authors found that the gaze overlap between the partners was lowest when the ref- erences were local as compared to when the references had location modifiers.  The notion of With-me-ness builds upon the combined notion of gaze-speech and gaze-deixis coupling. The two levels of With-me-ness, perceptual and conceptual capture the gaze-speech and gaze-deixis couplings respectively, for a teacher-student dyad.  3.2 Gaze-awareness Gaze-awareness had been used to build intelligent tutor-  ing systems [5, 15, 8], online collaboration support [12, 14]. DMello et. al., [5] used students real time gaze informa- tion to inform the tutor about the boredom and engagement levels for selecting the dialogue moves for the virtual tutor accordingly. The authors found that the gaze-aware tutor was more effective in terms of both maintaining a higher en- gagement level and achieving a higher learning gain. Wang wt. al, [15] used students gaze information to infer the tu- tors strategy in terms of the instruction and feedback to be given, and the emotions of the tutor. The authors also used gaze as the interaction modality for students to interact with the system. In a preliminary usability test the authors found that such a feedback improved students involvement with the learning processes. Gaze-awareness was also shown to be effective in improving the quality of online collaboration [12, 14].  The gaze-aware feedback tool that we propose, gives real- time feedback to the students based on their gaze. The key difference from the reviewed similar work is that we give the feedback directly to the students rather than providing it to a tutor. The system computes students With-me-ness levels and gives them a visual feedback on the video lecture, if their With-me-ness levels fall below a certain threshold.  3.3 Learning analytics models Clow [3], proposed a learning analytics cycle as a four-step  loop: 1) identifying the learner population, 2) generating and capturing the data from the learners, 3) analysing the data to get insights about the learning processes, and 4) providing interventions to the learners based on the insights acquired.  Jermann [9], proposed learning analytics as a cybernetic control with four phases: 1) learner data collection, 2) se- lecting one or more indicators to represent the current state of learner, 3) diagnosis of the current state by comparing it  to a desired state, and 4) proposition for a remedial action. Chatti et. al., [2], proposed learning analytic process as  a loop consisting of three phases: 1) learner data collection and pre-processing (finding patterns in the data), 2) data exploration based on the learning analytics objectives and taking appropriate actions, such as, prediction, assessment and recommendation, and 3) post-processing the newly ac- quired data.  4. GENERAL DISCUSSION In this section, we revisit the learning analytics models  summarized in Section 3.3 and use the results of the pre- sented study, to argue that the notion of With-me-ness can create a complete theoretical framework for conducting gaze- based learning analytics in the context of MOOCs.  First, we consider the model proposed by Clow [3]. The example model for a gaze-based LA model is shown in Fig- ure 5. The learner population is a subset of MOOC students (step 1). In our previous two experiments, we collected stu- dents gaze data while they watched the MOOC videos (step 2). Further, we developed With-me-ness as a gaze-based in- dicator and we found that it is positively correlated to the learning gain (step 3). The experiment described in this pa- per uses With-me-ness as an indicator for building a gaze- aware feedback tool to notify students about their attention level (step 4). The results show that the feedback tool not only helped students to learn more but also improved their attention levels.  Figure 5: A gaze-based learning analytics model in compliance with the model proposed by Clow [3].  Second, we consider the model proposed by Jermann [9]. The gaze-based cybernetic control system is shown in Fig- ure 6. We collect the gaze data of the students while they watch the videos (phase 1). We define With-me-ness as an indicator of the students current state (phase 2). We create the desired learner state using the With-me-ness levels of the students from our previous studies (phase 3). Finally, the proposed gaze-aware feedback tool provides the feed- back to the students about which part of the display that students should look at, if the measured With-me-ness is lower than desired With-me-ness at any instant (phase 4). The gaze-aware feedback tool acts as the key element in this gaze-based cybernetic control system.  Finally, we consider the LA model proposed by Chatti and colleagues [2]. The gaze-based LA model is shown in Figure 7. We collect the gaze data of the students and measure their With-me-ness (step 1). We found that students With-me- ness is positively correlated with their learning gains (step 2). The gaze-aware feedback tool, we presented, monitors the With-me-ness levels of the students and give them the feedback about how much attention they are paying to the teacher (step 3).    Figure 6: A gaze-based learning analytics cybernetic control model in compliance with the model pro- posed by Jermann [9].  Figure 7: A gaze-based learning analytics model in compliance with the model proposed by Chatti and colleagues [2].  Succinctly, With-me-ness as a gaze-based variable can be used as a accurate learning analytic indicator for two pur- poses: 1) to quantify learners attention, and 2) to design intervention tools to provide feedback to the learners to in- crease their attention levels as well as learning outcome. Moreover, the feedback tool completes the LA loops by in- tervening the learning process during the moments of lack of attention.  5. CONCLUSIONS In a nutshell, the gaze-aware intervention in the learn-  ing process of the students had a positive effect on their attention. Provided that such a feedback is used during reg- ular MOOC studies, this might have a long term impact on students overall attention. Regarding our general research question about how to improve the attention of the stu- dents during MOOC videos; gaze-aware feedback emerged as a influencing tool for intervention.  Finally, we propose, as future work, to create the visual- feedback directly from the heat-map of students gaze pat- tern, rather than eliciting it from the teachers dialogue and deixis (as implemented in the presented system). This can construct a reliable method especially in the context of MOOCs as the number of students increases.  6. REFERENCES [1] P. Allopenna, J. Magnuson, and M. Tanenhaus.  Tracking the time course of spoken word recognition using eye movements: Evidence for continuous mapping models* 1,* 2,* 3,* 4,* 5. Journal of memory and language, 38(4), 1998.  [2] M. A. Chatti, A. L. Dyckhoff, U. Schroeder, and H. Thus. A reference model for learning analytics. International Journal of Technology Enhanced Learning, 4(5-6):318331, 2012.  [3] D. Clow. The learning analytics cycle: closing the loop effectively. In Proceedings of the 2nd international conference on learning analytics and knowledge, pages 134138. ACM, 2012.  [4] R. D. D.C. Richardson and N. Kirkham. The art of conversation is coordination. Psychological Science, 18(5):407413, 2007.  [5] S. DMello, A. Olney, C. Williams, and P. Hays. Gaze tutor: A gaze-reactive intelligent tutoring system. International Journal of human-computer studies, 70(5):377398, 2012.  [6] D. Gergle and A. T. Clark. See what im saying using dyadic mobile eye tracking to study collaborative reference. In In Proceedings of the ACM 2011 conference on Computer supported cooperative work (pp. 435-444). ACM., 2011.  [7] Z. Griffin and K. Bock. What the eyes say about speaking. Psychological science, 11(4), 2000.  [8] N. Jaques, C. Conati, J. M. Harley, and R. Azevedo. Predicting affect from gaze data during interaction with an intelligent tutoring system. In Intelligent Tutoring Systems, pages 2938. Springer, 2014.  [9] P. Jermann. Computer support for interaction regulation in collaborative problem-solving. Unpublished Ph. D. thesis, University of Geneva, Switzerland, 2004.  [10] P. Jermann and M.-A. Nussli. Effects of sharing text selections on gaze cross-recurrence and interaction quality in a pair programming task. In In Proceedings of Computer Supported Collaborative Work 2012, 2012.  [11] A. S. Meyer, A. M. Sleiderink, and W. J. Levelt. Viewing and naming objects: Eye movements during noun phrase production. Cognition, 66(2):B25B33, 1998.  [12] A. Oh, H. Fox, M. Van Kleek, A. Adler, K. Gajos, L.-P. Morency, and T. Darrell. Evaluating look-to-talk: a gaze-aware interface in a collaborative environment. In CHI02 Extended Abstracts on Human Factors in Computing Systems, pages 650651. ACM, 2002.  [13] K. Sharma. Gaze analysis methods for learning analytics. PhD thesis, Ecole Polytechnique Federale de Lausanne, 2015.  [14] K.-H. Tan, I. Robinson, R. Samadani, B. Lee, D. Gelb, A. Vorbau, B. Culbertson, and J. Apostolopoulos. Connectboard: A remote collaboration system that supports gaze-aware interaction and sharing. In Multimedia Signal Processing, 2009. MMSP09. IEEE International Workshop on, pages 16. IEEE, 2009.  [15] H. Wang, M. Chignell, and M. Ishizuka. Empathic tutoring software agents using real-time eye tracking. In Proceedings of the 2006 symposium on Eye tracking research & applications, pages 7378. ACM, 2006.    "}
{"index":{"_id":"52"}}
{"datatype":"inproceedings","key":"Pardo:2016:ERS:2883851.2883883","author":"Pardo, Abelardo and Han, Feifei and Ellis, Robert A.","title":"Exploring the Relation Between Self-regulation, Online Activities, and Academic Performance: A Case Study","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"422--429","numpages":"8","url":"http://doi.acm.org/10.1145/2883851.2883883","doi":"10.1145/2883851.2883883","acmid":"2883883","publisher":"ACM","address":"New York, NY, USA","keywords":"SEM, higher education, learning analytics, self-regulation","abstract":"The areas of educational data mining and learning analytics focus on the extraction of knowledge and actionable items from data sets containing detailed information about students. However, the potential impact from these techniques is increased when properly contextualized within a learning environment. More studies are needed to explore the connection between student interactions, approaches to learning, and academic performance. Self-regulated learning (SRL) is defined as the extent to which a student is able to motivationally, metacognitively, and cognitively engage in a learning experience. SRL has been the focus of research in traditional classroom learning and is also argued to play a vital role in the online or blended learning contexts. In this paper, we study how SRL affects students' online interactions with various learning activities and its influence in academic performance. The results derived from a naturalistic experiment among a cohort of first year engineering students showed that positive self-regulated strategies (PSRS) and negative self-regulated strategies (NSRS) affected both the interaction with online activities and academic performance. NSRS directly predicted academic outcomes, whereas PSRS only contributed indirectly to academic performance via the interactions with online activities. These results point to concrete avenues to promote self-regulation among students in this type of learning contexts.","pdf":"Exploring the relation between Self-regulation, Online  Activities, and Academic Performance: A case study   Abelardo Pardo1  Abelardo.pardo@sydney.edu.au   Feifei Han2  Feifei.han@sydney.edu.au   Robert A. Ellis2  Robert.ellis@sydney.edu.au  1School of Electrical and Information Engineering  2Institute for Teaching and Learning   The University of Sydney, NSW 2006, Australia     ABSTRACT   The areas of educational data mining and learning analytics focus  on the extraction of knowledge and actionable items from data  sets containing detailed information about students. However, the  potential impact from these techniques is increased when properly  contextualized within a learning environment. More studies are  needed to explore the connection between student interactions,  approaches to learning, and academic performance. Self-regulated  learning (SRL) is defined as the extent to which a student is able  to motivationally, metacognitively, and cognitively engage in a  learning experience. SRL has been the focus of research in  traditional classroom learning and is also argued to play a vital  role in the online or blended learning contexts. In this paper, we  study how SRL affects students online interactions with various  learning activities and its influence in academic performance. The  results derived from a naturalistic experiment among a cohort of  first year engineering students showed that positive self-regulated  strategies (PSRS) and negative self-regulated strategies (NSRS)  affected both the interaction with online activities and academic  performance. NSRS directly predicted academic outcomes,  whereas PSRS only contributed indirectly to academic  performance via the interactions with online activities. These  results point to concrete avenues to promote self-regulation  among students in this type of learning contexts.   Categories and Subject Descriptors   Applied computing~Computer-assisted instruction  Applied  computing~E-learning   General Terms  Measurement, Performance, Human Factors, Theory.   Keywords  Learning analytics, Self-regulation, Higher education, SEM.   1. INTRODUCTION  A significant portion of the Higher education sector has  experienced a rapid change through the adoption of Internet and  Web-based technologies as an integral part of the student learning   experience. This change has resulted in the widespread use of  blended (or hybrid) learning contexts. Advances in research have  redefined the boundaries of online learning, which is now seen as  learning that is distributed over time and place using various  technologies, engaging students in multiple forms of interaction  [14]. Nowadays, students in higher education institutions are  participating in learning experiences that go beyond sitting in a  traditional classroom context. Blended learning is not only  quickly filling the new educational demand, but is also providing  new ways for student engagement and to promote their interests  and motivation [13, 15, 30]. Research has indicated that students  who attend hybrid courses which combine online and face-to-face  delivery performed better and perceived learning being more  effective than students in traditional face-to-face classroom  learning (e.g., [30]). However, researchers argued that effective  online learning often requires students to be self-disciplined and  self-regulated [11, 21, 24]. These factors may be equally effective  when translated to blended learning contexts.  Several research areas related to education have tried to avoid the  one size fits all problem proposing multiple techniques to adapt a  learning experience to the needs of each learner. The areas of  learning analytics and educational data mining approach this  problem using comprehensive collections of data about students  and the use of algorithms to derive the knowledge and insight to  help understand and improve their overall experience [5]. But  data does not speak by itself, and pedagogical and epistemological  assumptions need to be taken into account to fully understand  how to improve a learning experience [26]. The research  community has identified the need to bring multiple disciplines  with different conceptions about learning in what it has been  defined as the middle space, in order to maximize the potential of  learning analytics solutions [43]. However, there is a need for  more studies that combine research methodologies from these  areas. This paper attempts to explore further this middle space  through a study to explore the relationship between self-regulated  learning, digital traces of student interaction with online activities,  and academic performance.  The rest of the document is organized as follows. Section 2  describes the related work in the areas of self-regulated learning,  learning analytics, and the combination of data and educational  theories. Section 3 describes the methodology used for the case  study. Section 4 presents the results obtained in the study. Section  5 includes a discussion of the main consequences of these results.  The paper concludes with a set of conclusions and future avenues  to explore described in Section 6.   2. RELATED WORK  The widespread use of Learning Management Systems in higher  education institutions has made large quantities of data traces   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom   Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883883     available. Data is now being obtained from virtual appliances,  social networks, learning management system or enrolment  processes. This data was soon identified as a potential source of  information to improve the quality of a learning experience [36,  47]. This detailed information has the potential of facilitating the  understanding of how students learn [5, 26, 29]. The abundant log  data recorded by technologies offers insightful information as to  students interactive patterns with a wide range of online learning  activities [29]. There are numerous studies in this area reporting  relations between academic performance and information derived  from data traces [32].  Data mining techniques have been used to discover relationships  among factors in a learning experiences to, for example, create  models to predict student academic performance (e.g., [1, 16-17,   41] or detecting students at risk [44].    2.1 Combining educational data mining,  analytics and educational theories   However, relying solely on low level data capturing the  interaction of students with online activities reveals partial  information and does not provide insight about the underlying  problems while students learn [42]. To address this limitation,  researchers in learning analytics and educational data mining have  identified the need for a synergistic approach combining  educational data mining techniques and other fields such as  educational psychology, curriculum and pedagogy, and sociology  in education. The objective is to better identify which factors  facilitate or hinder effective behavior in students while they  interact in a learning environment (e.g., [42-43]). The study  described in this paper explores how data self-reported by  students about their self-regulation strategies can be combined  with traces of interaction with online activities to predict  academic performance.   2.2 Self-regulated learning  Self-regulation is defined as the extent to which students are  motivationally, metacognitively, and cognitively engaged in their  learning processes [12, 52]. There are various models for self- regulated learning (SRL), each placing emphasis in different  aspects (e.g., [3, 8, 12, 37, 48, 53]). For instance, Winne places  the focus on the cognitive aspect of SRL, whereas the model  proposed by [33] is established based on sociocultural  perspectives. Some researchers perceive SRL as an event-based  phenomenon (e.g., [2, 4, 22]), whereas others view SRL as a  progression through metacognitive monitoring and control (e.g.,  [48, 51]). Adopting a social-cognitive perspective, Duncan and  McKeachie argued that SRL should be treated as dynamic and  contextually bound [15], wherein SRL behaviors are not a static  trait of students per se. This means that students may change their  motivation and SRL behavior and strategy depending on the  nature of the course, its structure and the proposed learning  activities. In this paper we have adopted this conceptualization of  SRL. In other words, we assume that the perception that students  have of the learning environment and task characteristics (external  environment) together with their own state of mind (internal  environment) may either facilitate or hamper their use of self- regulating strategies [37-39]. Even though SRL encompasses the  areas of motivation, cognition, behavior, and context, this paper  explores how students use self-regulated strategies as a whole.  Self-regulation of cognition and behavior forms an essential part  of the learning processes. Past research has consistently shown   that students self-regulated learning behaviors affect their level of  academic success [6, 31, 55, 57]. Able self-regulated learners are  often described as actively setting learning goals, employing  effective and efficient learning strategies, making appropriate  learning plans, adapting their approach  from task to task,  monitoring their learning persistently, and making adjustments  when needed [10, 35, 40, 54]. While there are numerous  publications exploring the design of online learning activities to  facilitate self-regulated student behavior (e.g., [14, 19, 27, 46],  fewer studies investigate the self-regulation strategies when  combined with detailed data about student t interactions with  online learning activities, and their academic performance. Winne  [51] identified the need to adopt a wider lens to SRL specially  when technology is used to support a learning experience where  students make choices and decisions about which tools they are  going to use and how are they going to use them.  This paper presents the results of a study that explores precisely  this type of relation in the context of a first year engineering  course. Students are given set online resources that requires them  a certain level of interaction. The study explores the relationship  between their agency, the type of interactions with the online  environment, and their academic performance.   3. METHOD  3.1 Research Context  The study was conducted in a semester-long course on computer  system for first year engineering students. By the end of the  course, students should be able to design, build and configure an  electronic system, demonstrate their understanding of how  computers work, and write reports about the design process and  its results. In addition to these skills, the course has been designed  to promote independent inquiry abilities, effective  communication, information literacy, and an understanding of  ethical issues in the profession. The course was designed as a  blended learning experience consisting of a face-to-face  component and online learning component, both of them with  their corresponding assessments. The face-to-face learning part  involved a weekly two-hour lecture, a weekly two-hour tutorial,  and a weekly three-hour laboratory session. The online learning  component was required to be completed by students in their own  time through a custom-designed learning management system able  to trace, monitor, and record all the online learning activities by  students unique login identification number. The online activities  were designed with an estimated total dedication of 4 hours per  week. An average of seven activities per week were scheduled  throughout the semester. This portion of the course was assessed  as 15% (1.5% per week) of the overall course mark.   3.2 Participants  The study was conducted with 145 (n = 145) first year students  who were studying a four year Bachelor of Engineering Degree in  a research-intensive university in Australia.    3.3 Instrument  Three methods were used to collect the data in the study.   3.3.1 Self-regulated strategy use questionnaire  Students were asked to answer the questions included in the Self- Regulation Section of the Motivated Strategies for Learning  Questionnaire (MSLQ, [38]). The data was extracted from nine  items answered on a 7-point Likert scale, with 1 indicating never     true of me and 7 representing always true of me. Three of the  nine items were negatively worded. The results were collected  during the first week of the course.   3.3.2 Interactions with online activities  The course required students to interact with an online  environment providing resources such as videos, multiple choice  questions, summative and formative assessment, etc. The learning  management system hosting these resources recorded the  interaction of the students while learning. At the end of the  semester the accumulated number of events was obtained for each  student in the following categories:    Access to any HTML page of the course material  (Resource).    Expand/Collapse of a section within a page. Some pages  had headings with the content collapsed that was exposed  when clicking in the title (Col-Exp).    Events while using the embedded videos: play, pause,  begin, and end a video (Video)    Events while answering the multiple choice questions next  to the videos (VMCQ).    Events while answering the multiple choice questions in the  course notes (MCQ).    Access to a page containing a dashboard illustrating the  level of interaction with the course activities (Dboard).   3.3.3 Academic performance  The academic performance was obtained from the final course  mark. This mark was calculated by taking into account the  following assessments: online lecture preparation activities (10%),  online tutorial preparation and participation (10%), written report  about laboratory session (5%), collaborative project (15%), mid- term examination (20%), and final examination (40%), on a scale  from 1 to 100. The Mean (M) of the final mark was 65.50 with a  Standard Deviation (SD) of 16.12.   3.4 Procedure  After ethics approval was obtained for the study, we obtained the  written consent for the voluntary participation in the first session  of the semester. The collection of the data related to the  interaction with the online activities was collected all throughout  the semester. Students were required to submit partial work to be  assessed every week. The data about academic performance was  collected upon course completion.   3.5 Data Analysis  The data analysis has been carried out in five stages. The first  stage included Exploratory Factor Analysis (EFA) of the data  obtained from the Self-regulation strategy use questionnaire  (SRSUQ). Principal Component Analysis was used followed by  varimax rotation to examine the factor structure of the results.  Following suggestions of [18], we deleted from the result those  items whose coefficients were < .40 within a factor and those with  high multiple coefficients loaded across factors. In the second  stage we examined the reliability of the retained scales.  The third  stage included the use of correlation analysis to see the  relationship between self-regulated strategy use, the interaction  with the online activities, and academic performance at the  variable level. In the fourth stage we first conducted a hierarchical  cluster analysis to identify subgroups of participants where the   similarities and the differences in their self-regulated strategy use  and academic performance could be maximized. On the basis of  the identified clusters, one-way ANOVA was performed to see  whether learners in different clusters exhibit different patterns for  interactions with online activities. This allowed us to examine the  relationship between self-regulated strategy use, online learning  activities, and academic performance at the student level. In the  final stage of the study we constructed a structural equation model  (SEM) to examine the predictions of self-regulated strategy use,  tool use, and academic performance.    4. RESULTS  4.1 EFA and reliability of the scales for self- regulated strategy use   The results of the EFA for the SRSUQ are displayed in Table 1.  The obtained rotated factor loadings translated in the selection of  seven of the nine items from the original questionnaire. The final  two factors included 4 items in the Positive Self-regulated  Strategy (PSRS) scale, and 3 items in Negative Self-regulated  Strategy (NSRS) scale. The eigen-values of the PSRS and the  NSRS were 2.07 and 1.96 respectively, explaining 29.51 % and  27.94% of the total variance respectively. The values of  Cronbachs alpha were .68 and .72 for the PSRS and the NSRS,  implying that the two scales were reliable.   Table 1: Results of EFA for the SRSUQ   Scales  (Cronbach) Item Description   Rotated  factor   loadings   PSRS  (.68)   I ask myself questions to make sure I  know the material I have been  studying.   .63    I work on practice exercises and  answer end of chapter questions even  when I dont have to.   .74    Even when study materials are dull  and uninteresting, I keep working  until I finish.   .77    I work hard to get a good grade even  when I dont like a class. .69    NSRS  (.72)   When work is hard, I either give up or  study only the easy parts.  .73   I often find that I have been reading  for class but dont know what it is all  about.    .84   I find that when the teacher is talking  I think of other things and dont really  listen to what is being said.    .81   Values less than .40 removed; KMO: .83   4.2 Correlation analysis  The results of correlation analyses are shown in Table 2. While  PSRS did not significantly relate to student academic performance  (r = -.02, p = .85), the NSRS significantly and negatively  associated with academic performance (r = -.20, p < .01).  However, the PSRS showed significant and positive association  with three of the events registered in the online environment,  namely Dboard (r = .18, p < .05), Expand/Collapse-of sections in  the course notes (r = .21, p < .05), and Col-exp (r = .23, p < .01).  This association means that the more students adopted positive  self-regulated strategies in the course, the more likely they used     the above three online activities. In contrast, the correlations  between NSRS and the access to online activities turned to be  non-significant. Additionally, academic performance was found to  be positively related to most of the indicators derived from online  interactions, including Dboard (r = .24, p < .01), Col-exp (r = .35,  p < .01), Resource (r = .44, p < .01), and MCQ (r = .28, p < .01).  This positive correlation suggests that the more frequently an  individual engaged with these learning activities, the more likely  they were to obtain a higher course score in the course. The  significant and positive correlation between PSRS and online  activities, and the positive relation between these and academic  performance may indicate that the PSRS could contribute to the  academic performance via the use of online tools. This means that  it is possible that the PSRS contributed to the academic  performance indirectly rather than directly. The relation between  these factors was used to build a structural equation model as  discussed in Section 4.4.   Table 2: Correlation analysis    PSRS NSRS AP  NSRS .13 --- ---  AP -.02 -.20** ---  D-board .18* -.03 .24**  Col-Exp .21* .15 .35**  Resource .23** .10 .44**  Video .14 -.03 .14  MCQ .17 -.01 .28**  VMCQ .06 -.11 .14  Notes: ** p < .01, * p < .05,    4.3 Cluster and ANOVA analyses  A hierarchical cluster analysis using Wards method was  conducted using as criteria PSRS, NSRS, and the academic  performance. The purpose of this analysis is to see if it is possible  to identify subgroups of students with differences in their self- regulated strategy that also have different academic performance.  The cluster analysis resulted in three solutions: two, three, and  four clusters. Using the increasing value of the squared Euclidean  distance between clusters, a two-cluster solution was sought, and  the results of ANOVA are presented in Table 3.    As it can be seen, the 145 students were classified into a group of  86 High Self-regulated and High-performing students (cluster 1),  and a group of 59 Low Self-regulated and Low-performing  students (cluster 2). The students in cluster 1 differed from those  in cluster 2 in terms of the PSRS use (F (1, 144) = 90.20, p < .01,  2 = .39), NSRS use (F (1, 144) = 37.20, p < .01, 2 = .21), and  the academic performance (F (1, 144) = 24.60, p < .01, 2 = .15).  To be more specific, the High Self-regulated and High-performing  students reported adopting more of the positive self-regulated  strategies (M = 0.51), less of the negative self-regulated strategies  (M = -0.38), and achieved a relatively higher final score (M =  0.32) than the Low Self-regulated and Low-performing students.  On the other hand, the Low Self-regulated and Low-performing  students adopted a less positive self-regulated strategy (M = - 0.75), more negative self-regulated strategy (M = 0.55), and their  academic performance was relatively poorer (M = -0.46).   Based on the cluster membership, a series of additional ANOVA  were performed to examine whether students in the two clusters  had differed interaction with the online activities. The results  reported from the sixth row in Table 3 reveal that the two groups  of students had significant differences in the frequencies of   interactions with five of the seven online activities. The factors  Dboard (F (1, 144) = 9.82, p < .01, 2 = .06), Col-Exp (F (1, 144)  = 5.76, p < .05, 2 = .04), Resource (F (1, 144) = 10.47, p < .01,  2 = .07), MCQ (F (1, 144) = 7.92, p < .01, 2 = .05), and Exercise  (F (1, 144) = 15.17, p < .01, 2 = .10) had statistically significant  differences between the clusters. The two remaining factors Video  and VMCQ did not have a statistically significant difference  among clusters. The High Self-regulated and High-Performing  students interacted significantly more frequently with those five  activities than the students with Low Self-regulated and Low  performance. These results demonstrate that there is a relationship  between the use of a self-regulation strategy, academic  performance, and interactions with online activities at the level of  individual students.   Table 3: Summary statistics of the two-cluster solution   Variable High (86) Low (59) F p 2   Mean Mean      PSRS 0.51 -0.75 90.20 .00 .39  NSRS -0.38 0.55 37.20 .00 .21   AP 0.32 -0.46 24.60 .00 .15  D-board 0.21 -0.30 9.82 .00 .06  Col-Exp 0.16 -0.24 5.76 .02 .04  Resource 0.22 -0.31 10.47 .00 .07   Video 0.12 -0.17 2.89 .09 .02  MCQ 0.19 -0.28 7.92 .01 .05   VMCQ 0.11 -0.17 2.74 .10 .02  Notes: High = High Self-regulated and High Performing learners,  Low = Low Self-regulated and Low Performing learners.   4.4 Structural Equation Model  To examine how PSRS, NSRS, and the interactions with online  activities contribute to a student academic performance, we  constructed a SEM. The three variables in the model were derived  from PSRS, NSRS, and the interaction with online activities. The  PSRS and NSRS variables were constructed by computing the  mean of each scale. The variable Activity was constructed by  aggregating the frequencies of interactions with all the activities.  The resulting model is shown in Figure 1.      Figure 1: The resulting structural equation model   The following criteria were used to evaluate the fit of the SEM. A  non-significant chi-square value led to acceptance of the null  hypothesis that the model fits the population [20]. The goodness- of-fit statistics were also consulted for model evaluation. We used  the Tucker-Lewis Index (TLI, [45]), the Comparative Fit Index  (CFI, [7]), and the root mean square error of approximation  (RMSEA, [9]) as our primary goodness-of-fit statistics. The  values of TLI and CFI are in the range from 0.00 to 1.00, with  values greater than .90 as an acceptable fit to the data [23]. In  terms of the RMSEA, according to [9], a value of .06 and below is  indicative of a good fit between the hypothesized model and the  observed data [9, 23].      The results of the SEM revealed that our data fit the hypothesized  model:  (2) = 0.95, p = .62, CFI = 1.00, TLI = 1.06, RMSEA =  .05. The resulting paths are shown in Figure 1. As it can be seen,  the NSRS has a significant and negative path to the academic  performance ( = -.18, p < .05), whereas the interaction with  online activities has a significant and positive path to students  academic performance ( = .40, p < .01). At the same time, PSRS  significantly and positively predicted interaction with online  activities ( = .27, p < .01). The model also shows that the PSRS  has significant and positive indirect effect on the academic  performance ( = .11, p < .01). Even though the direct  contribution of PSRS to the academic performance is not high, it  has a more significant indirect contribution through the  interactions with online activities.    5. DISCUSSION  This study has investigated the relationship between students  self-regulated strategy use, interactions with online learning  activities, and academic performance in a first year engineering  course. The results showed that while PSRS did not show a  significant correlation with academic performance, NSRS did  have a negative relation with academic performance. Additionally,  the academic performance was found to be positively associated  with frequencies of interactions with a number of online learning  activities. The cluster analysis further identified two groups of  students based on their self-reported use of self-regulated strategy  and academic performance. The students in the High Self- regulated and High performing group adopted more PSRS, less  NSRS, obtained higher final marks in the course, and tended to  interact more frequently with online learning activities. The  results are the opposite for the Low Self-regulated and Low- performing cluster. The results derived from the structural  equation model show indicate that while the NSRS negatively  contributed to the academic performance, and the interactions  with online activities positively predicted the students final  course mark. Additionally, the PSRS had an indirect but  significant path to the academic performance. These results are  consistent with previous research in SRL that self-regulated  learning behavior is a significant factor affecting students  learning outcomes [6, 31, 55, 57]. We found, however, that PSRS  and NSRS contributed to the learning outcomes differently. While  NSRS was a significant factor, which directly predicted  performance, PSRS only affected students online learning  behaviors directly, and indirectly impacted students academic  performance. These results may mean that interventions that target  NSRS could possibly enhance students learning outcomes, but  this postulation needs to be empirically tested via a longitudinal  design or intervention studies. The interactions with online  learning activities were also found to be a significant contributor  to students final course marks, suggesting that quantity of  engagement with online learning is important in this blended  learning context.  Although it is difficult to point out directions of causality between  PSRS and interactions with online activities with our cross- sectional data, it is plausible that the features of online learning  activities are able to foster an increased level of self-regulated  learning and enable learners to become more metacognitively  aware of learning processes (e.g., [27, 28, 50]). It is also possible  that learners who tend to adopt more positive self-regulated  strategies in learning also have a tendency to interact with online  learning activities more frequently. One way to identify the  direction of causality would be through a well-conceptualized   intervention study. Researchers may compare the interactions of  online activities among four groups of learners: a control group of  students interacting with normally designed online activities; a  control group of students interacting with online activities whose  features are designed to encourage SRSU; an experiment group of  students who will receive instructions on how to improve self- regulated strategy use interacting with normally designed online  activities; and an experiment group of students interacting with  online activities whose features are designed to encourage SRSU.  Only through this kind of experiment, may we know the casual  relation between SRSU and online learning.   The results of the study offer some practical implications for  university lecturers to consider. According to our results, lecturers  should consider strategies to reduce students negative self- regulated strategy use and promote positive self-regulated strategy  use. As argued by [29], students need to be made aware of self- regulated strategies, and initially use them in guided and  structured manner. (p. 1303). These authors further identified  three aspects for self-regulated strategies to be built in the  learning environment in order to enhance students self- regulation, namely: activities for self-regulation, resources for  self-regulation, and supports for self-regulation. At the level of  activities, those activities, which are able to stimulate learners  reflections on the process of problem-solving are believed to  enhance learners self-regulated learning [8]. Therefore,  instructors may use reflective journals or diaries as part of  normative assessment for learners to reflect upon their learning  processes. At the level of resources, McMahon and Oliver [34]  advocate to use multiple sources, which are relevant and  challenge, so that these learning sources require learners to  process information deeply using self-regulated strategies. At the  level of support, instructors should provide learners with  constructive and useful feedback to improve their self-regulated  learning. There is also a place for instructors to instruct self- regulation directly so that students awareness of self-regulated  learning can be raised.   6. CONCLUSION  Self-regulation has been identified to play a very important role in  online and blended learning scenarios contexts. Although there  are numerous publications exploring the design of learning  activities to foster self-regulation, few studies explore the  connection between self-reported self-regulation data with  quantitative data about student interaction with activities and their  relation with academic performance. This paper presents the  results obtained from a case study deployed at a first year  engineering course with a blended, active learning strategy.  Three data sources were used for the study. The subset of items  inquiring about self-regulation in the Motivational and Self- Regulated Learning Questionnaire were used to encode two  factors: Positive and Negative self-regulation strategies (PSRS  and NSRS). The number of events accumulated the digital traces  produced by the students when interacting with the online  activities provided six additional factors. Finally, the academic  performance was represented by a single factor with the final  score in the course.  An initial correlation analysis showed a lack of significant  correlation between PSRS and academic performance. On the  other hand, there was a statistically significant negative  correlation between NPRS and academic performance. Four of the  six factors derived from the digital traces showed also a strong     statistically significant correlation with academic performance. A  clustering algorithm produced two robust clusters: users with both  high self-regulation and performance, and users with both low  self-regulation and performance. The factors derived from digital  traces had significantly different means in these clusters  suggesting that students in these clusters clearly interact  differently with the activities.  Finally, a structural equation model was created that provided a  more detailed vision of the relation between these factors. NSRS  affected negatively academic performance. The interaction with  online activities had a very strong positive effect on the academic  performance. The model also showed an indirect effect of PSRS  on the interaction with online activities.  The main conclusion of these results is that instructors could re- design the course to foster a higher adoption of self-regulation  through the online activities. As future work, we envision a more  detailed study that allows clarifying the causality relation between  self-regulation and how students behave in this blended learning  context.     7. ACKNOWLEDGEMENT  The authors wish to acknowledge the financial support of the  Australian Research Council through grant DP150104163.     8. REFERENCES  [1] Antunes, C. 2010. Anticipating students failure as soon as   possible. In Handbook of Educational Data Mining, C.  Romero, S. Ventura, M. Pechenizkiy, and R. Baker, Eds.  CRC Press, Boca Raton, FL. 353-362.   [2] Azevedo, R. 2009. Theoretical, methodological, and  analytical challenges in the research on metacognition and  self-regulation: A commentary. Metacognition and Learning.  4, 1 (2009), 87-95.   [3] Azevedo, R., Moos, D. C., Greene, J. A., Winters, F. I., and  Cromley, J. G. 2008. Why is externally-facilitated regulated  learning more effective than self-regulated learning with  hypermedia Educational Technology Research and  Development. 56, 1 (Feb. 2008), 45-72.   [4] Azevedo, R., Moos, D. C., Witherspoon, A. M., and  Chauncey, A. D. 2010. Measuring cognitive and  metacognitive regulatory processes used during hypermedia  learning: Theoretical, conceptual, and methodological issues.  Educational Psychologist. 45, 4 (2010), 1-14.   [5] Baker, R. and Siemens, G. 2014. Educational data mining  and learning analytics. In The Cambridge Handbook of the  Learning Sciences (2nd ed.), R. K. Sawyer, Ed. Cambridge  University Press, New York, NY, 253-274.   [6] Beishuizen, J. and Steffens, K. 2011. A conceptual  framework for research on self-regulated learning. In Self- regulated Learning in Technology Enhanced Learning  Environments: A European Perspective, R. Carneiro, P.  Lefrere, K. Steffens, K. and J. Underwood, Eds. Sense  Publishers, Rotterdam, 3-20,   [7] Bentler, P. M. 1990. Comparative fit indexes in structural  models. Psychological Bulletin. 107, 2 (1990), 238-246.   [8] Boekaerts, M. 1997. Self-regulated learning: A new concept  embraced by researchers, policy makers, educators, teachers,  and students. Learning and Instruction. 7, 2 (June. 1997),  161-186.   [9] Browne, M. W. and Cudeck, R. 1993. Alternative ways of  assessing model fit. In Testing Structural Equation Models,  K. A. Bollen and J. S. Long, Eds. Sage, Beverly Hills, CA,  136-162.   [10] Butler, D. L and Winne, P. H. 1995. Feedback and self- regulated learning: A theoretical synthesis. Review of  Educational Research. 65, 3 (1995), 245-281.   [11] Collis, B. 2003. Course redesign for blended learning:  Modern optics for technical professionals. International  Journal of Continuing Engineering Education and Lifelong  Learning. 13, 1-2 (2003), 22-38.   [12] Corno, L. and Mandinach, E. B. 1983. The role of cognitive  engagement in classroom learning and motivation.  Educational Psychologist, 18, 2 (1983), 88-108.   [13] Dabbagh, N. and Kitsantas, A. 2005. Using web-based  pedagogical tools as scaffolds for self-regulated learning.  Instructional Science. 33, 5-6 (2005), 513-514. DOI=  http://doi.acm.org/10.1145/332040.332491.   [14] Dabbagh, N. and Kitsantas, A. 2013. Using Learning  Management Systems as metacognitive tools to support self- regulation in higher education contexts. In International  Handbook of Metacognition and Learning Technologies, R.  Azevedo and V. Aleven, Eds. Springer, New York, NY, 197- 212.   [15] Duncan, T. G. and McKeachie, W. J. 2005. The making of  the Motivated Strategies for Learning Questionnaire.  Educational Psychologist. 40, 2 (2005), 117-128.   [16] Essa, A. and Ayad, H. 2012a. Improving student success  using predictive models and data visualisations. Research in  Learning Technology. 5, (Aug. 2012), 58-70.   [17] Essa, A. and Ayad, H. 2012b. Student success system: Risk  analytics and data visualization using Ensembles of  Predictive Models. Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (New  York, USA, April 2012). DOI=10.1145/2330601.2330641.   [18] Field, A. 2013. Discovering Statistics Using SPSS (4th Ed).  Sage, London.   [19] Ge. X. 2013. Designing learning technologies to support  self-regulation during III-Structured Problem-solving  Processes. In Handbook of Metacognition and Learning  Technologies, R. Azevedo and V. Aleven, Eds. Springer,  New York, NY, 213-228.   [20] Geiser, C. 2013. Data Analysis with Mplus. Guilford, New  York, NY.   [21] Graham, C. R. 2006. Blended learning systems: Definitions,  current trends, and future directions. In Handbook of  Blended Learning: Global Perspectives, Local Designs, C. J.  Bonk and C. R. Graham, Eds. Pfeiffer Publishing, San  Francisco, CA, 1-21.   [22] Greene, J. A., & Azevedo, R. 2010. The measurement of  learners self-regulated cognitive and metacognitive  processes while using computer-based learning     environments. Educational Psychologist. 45, 4 (2010), 203- 209.   [23] Hu, L. and Bentler, P. M. 1999. Cutoff criteria for fit indexes  in covariance structure analysis: Conventional criteria versus  new alternatives. Structural Equation Modelling. 6, 1 (1999),  1-55.   [24] Huang, R. and Zhou, Y. 2006. Designing blended learning  focused on knowledge category and learning activities. In  Handbook of Blended Learning: Global Perspectives, Local  Designs, C. J. Bonk and C. R. Graham, Eds. Pfeiffer  Publishing, San Francisco, CA, 296-310.   [25] Kitsantas, A. and Dabbagh, N. 2010. Learning to Learn with  Integrative Learning Technologies (ILT): A Practical Guide  for Academic Success.  Information Age, Greenwich, CT.   [26] Knight, S., Shum, S. B., and Littleton, K. 2014.  Epistemology, assessment, pedagogy: Where learning meets  analytics in the middle space. Journal of Learning Analytics.  1, 1 (2014), 23-47.   [27] Lajoie, S. P., Naismith, L., Poitras, E. Hong, Y-J, Cruz- Panesso, I.et al. 2013. Technologies-rich tools to support  self-regulated learning and performance in medicine. In  Handbook of Metacognition and Learning Technologies, R.  Azevedo and V. Aleven, Eds. Springer, New York, NY, 229- 242.   [28] Lester, J. C., Mott, B. W., Robison, J. L., Rowe, J. P., and  Shores, L. R. 2013. Supporting self-regulated science  learning in narrative-centered learning environments. In  Handbook of Metacognition and Learning Technologies, R.  Azevedo and V. Aleven, Eds. Springer, New York, NY, 471- 484.   [29] Lockyer, L., Heathcote, E., and Dawson, S. 2013. Informing  pedagogical action: Aligning learning analytics with learning  design. American Behavioral Scientist. 57, 10 (Mar. 2013),  1439-1459.   [30] Lovett, M., Meyer, O., and Thille, C. 2008. The Open  Learning Initiative: Measuring the effectiveness of the OLI  statistics course in accelerating student learning. Journal of  Interactive Media in Education.  DOI=http://doi.org/10.5334/2008-14.   [31] Lyn, L., Cuskelly, M., OCallaghan, M., and Grey, P. 2011.  Self-regulation: A new perspective on learning problems  experienced by children born extremely preterm. Australian  Journal of Educational & Developmental Psychology. 11, 1- 10.    [32] Macfadyen, L. P. and Dawson, S. Mining LMS data to  develop an early Warning System for educators: A proof of  concept. Computers & Education. 54, (Feb. 2010), 588-599.   [33] McCaslin, M. and Hickey, D. T.  2001. Educational  psychology, social constructivisim, and educational practice:  A case of emergent identity. Educational Psychologist. 36, 2  (2001), 133-140.   [34] McMahon, M. and Oliver, R. 2001. Promoting self-regulated  learning in an on-line environment. In Proceedings of World  Conference on Educational Multimedia, Hypermedia and  Telecommunications 2001, C. Montgomerie and J. Viteli  Eds. AACE, Chesapeake, VA, 1299-1305.    [35] Meltzer, L. 2007. Executive Function in Education: From  Theory to Practice. The Guilford Press, New York, NY.   [36] Newlin, M. H. and Wang, A. Y. (2002). Predictors of  performance in the virtual classroom: Identifying and helping  at-risk cyber-students. Technological Horizons in Education.  29, 10 (2002), 21.   [37] Pintrich, P. R.  2000. The role of goal orientation in self- regulated learning. In Handbook of Self-regulation, M.  Boekaerts, P. R. Pintrich, and M. Zeidener, Eds. Academic  Press, San Diego, CA, 451-521.   [38] Pintrich, P. R. and De Groot, E. V. 1990. Motivational and  self-regulated learning component of classroom academic  performance. Journal of Educational Psychology. 82, 1  (1990), 33-40.   [39] Pintrich, P. R. Wolters, C., and Baxter, G. 2000. Assessing  metacognition and self-regulated learning. In Issues in the  Measurement of Metacognition, G. Schraw and J. Impara,  Eds. Institute of Mental Measurements, Lincoln, NE, 531- 566.   [40] Puustinen, M. and Pulkkinen, L. 2001. Models of self- regulated learning: A review. Scandinavian Journal of  Educational Research. 45, 3 (2001), 269-286.   [41] Romero, C., Lpez, M.-I., Luna, J.-M. and Ventura, S. 2013.  Predicting students final performance from participation in  on-line discussion forums. Computers & Education. 68,  (Oct. 2013), 458-472.    [42] Shum, S. B. and Crick, R. D. 2012. Learning dispositions  and transferable competencies: Pedagogy, modelling and  learning analytics. Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (New  York, USA, April 2012). DOI=10.1145/2330601.2330629.    [43] Suthers, D. and Road, E.W. 2013. Learning analytics as a  Middle Space. Proceedings of the 3rd International  conference on Learning Analytics and Knowledge (Leuven,  Belgium, April 08-12, 2013).  DOI=10.1145/2460296.2460298   [44] Tanes, Z., Arnold, K. E., King, A. Z., and Remnet, 2011. M.  A. Using signals for appropriate feedback: Perceptions and  practices. Computers & Education. 57, 4 (Dec. 2011), 2414- 22.   [45] Tucker, L. R. and Lewis, C. 1973. A reliability coefficient  for maximum likelihood factor analysis. Psychometrika. 38,  1 (Mar. 1973), 1-10.   [46] Venkatesh, V., Shaikh, K., Zuberi, A., Urbaniak, K., Gallant,  T., et al. 2013. Development of task understanding and  monitoring in information retrieval environments:  Demystifying metacogntive and self-regulatory mechanisms  in graduate learners using topic maps indexing technologies  to improve essay-writing skills. In Handbook of  Metacognition and Learning Technologies, R. Azevedo and  V. Aleven, Eds. Springer, New York, NY, 277-292.   [47] Wang, A. Y., and Newlin, M. H. 2000. Characteristics of  students who enroll and succeed in psychology web-based  classes. Journal of Educational Psychology, 92, (Mar. 2000),  137-143.   [48] Winne, P. H. 2001. Self-regulated learning viewed from  models of information processing. In Self-regulated  Learning and Academic Achievement: Theoretical  Perspectives (2nd ed.), G Cshra and J. Impara Eds. Buros  Institute of Mental Measurements, Lincoln, NE, 43-97.     [49] Winne, P. H. 2006. How software technologies can improve  research on learning and bolster school reform. Educational  Psychologist. 41, 1 (2006), 5-17.   [50] Winne, P. H. and Hadwin, A. F. 2013. nStudy: Tracing and  supporting self-regulated learning in the Internet. In  Handbook of Metacognition and Learning Technologies, R.  Azevedo and V. Aleven, Eds. Springer, New York, NY, 293- 310.   [51] Winne, P. H. and Perry, N. E. 2000. Measuring self- regulated learning. In Handbook of Self-regulation, M.  Boekaerts, P. R. Pintrich, and M. Zeidner, Eds. Academic  Press, San Diego, CA, 531-566.   [52] Zimmerman, B. J. 1989. A social cognitive view of self- regulated academic learning. Journal of Educational  Psychology. 81, 3 (1989), 329-339.   [53] Zimmerman, B. J. 2000. Attaining self-regulation: A social  cognitive perspective. In Handbook of Self-regulation, M.  Boekaerts, P. R. Pintrich, and M. Zeidener, Eds. Academic  Press, San Diego, CA, 13-39.   [54] Zimmerman, B. J. 2001. Theories of self-regulated learning  and academic achievement: An overview and analysis. In  Self-regulated Learning and Academic Achievement:  Theoretical Perspectives (2nd ed.), B. J. Zimmerman and D.  H. Schunk, Eds. Lawrence Erlbaum Associates, New York,  NY, 1-38.   [55] Zimmerman, B. J. 2008. Investigating self-regulation and  motivation: Historical background, methodological  developments, and future prospects. American Educational  Research Journal. 45, 1 (2009), 166-183.   [56] Zimmerman, B. J., and Schunk, D. H. 2008. Motivation: An  essential dimension of self-regulated learning. In Motivation  and Self-regulated Learning: Theory, Research and  Applications, B. J. Zimmerman, D. H. Schunk, Eds.  Lawrence Erlbaum Associates, New York, NY, 1-30.   [57] Zimmerman, B. J. and Schunk, D. H. 2011. Self-regulated  learning and performance. In Handbook of Self-Regulation of  Learning and Performance, B. J. Zimmerman and D. H.  Schunk, Eds. Routledge, New York, NY, 1-12.          "}
{"index":{"_id":"53"}}
{"datatype":"inproceedings","key":"Tan:2016:FCL:2883851.2883965","author":"Tan, Jennifer Pei-Ling and Yang, Simon and Koh, Elizabeth and Jonathan, Christin","title":"Fostering 21st Century Literacies Through a Collaborative Critical Reading and Learning Analytics Environment: User-perceived Benefits and Problematics","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"430--434","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883965","doi":"10.1145/2883851.2883965","acmid":"2883965","publisher":"ACM","address":"New York, NY, USA","keywords":"21st century skills, CSCL, critical literacy, learning analytics","abstract":"The affordances of learning analytics (LA) are being increasingly harnessed to enhance 21st century (21C) pedagogy and learning. Relatively rare, however, are use cases and empirically based understandings of students' actual experiences with LA tools and environments at fostering 21C literacies, especially in secondary schooling and Asian education contexts. This paper addresses this knowledge gap by 1) presenting a first iteration design of a computer-supported collaborative critical reading and LA environment and its 16-week implementation in a Singapore high school; and 2) foregrounding students' quantitative and qualitative accounts of the benefits and problematics associated with this learning innovation. We focus the analytic lens on the LA dashboard components that provided visualizations of students' reading achievement, 21C learning dispositions, critical literacy competencies and social learning network positioning within the class. The paper aims to provide insights into the potentialities, paradoxes and pathways forward for designing LA that take into consideration the voices of learners as critical stakeholders.","pdf":"Fostering 21st century literacies through a collaborative  critical reading and learning analytics environment: User-  perceived benefits and problematics Jennifer Pei-Ling Tan, Simon Yang, Elizabeth Koh, Christin Jonathan   National Institute of Education, Nanyang Technological University, Singapore  jen.tan@nie.edu.sg  ABSTRACT  The affordances of learning analytics (LA) are being increasingly  harnessed to enhance 21st century (21C) pedagogy and learning.  Relatively rare, however, are use cases and empirically based  understandings of students actual experiences with LA tools and  environments at fostering 21C literacies, especially in secondary  schooling and Asian education contexts. This paper addresses this  knowledge gap by 1) presenting a first iteration design of a  computer-supported collaborative critical reading and LA  environment and its 16-week implementation in a Singapore high  school; and 2) foregrounding students quantitative and qualitative  accounts of the benefits and problematics associated with this  learning innovation. We focus the analytic lens on the LA  dashboard components that provided visualizations of students  reading achievement, 21C learning dispositions, critical literacy  competencies and social learning network positioning within the  class. The paper aims to provide insights into the potentialities,  paradoxes and pathways forward for designing LA that take into  consideration the voices of learners as critical stakeholders.     CCS Concepts   Human-centered computing~User centered design     Applied computing~Collaborative learning   Keywords  Learning analytics, CSCL, critical literacy, 21st century skills.   1. INTRODUCTION  Critical reading development is an essential component of strong  language and literacy skills, and thus fundamental to young  peoples productive participation in the 21C global knowledge  economy. But students commonly struggle with relating and  responding to English language (EL) texts on a personal level,  thereby failing to engage deeply and critically in reading tasks [1].   This has been attributed to two key reasons. First, texts used in EL  lessons are still predominantly print-based even though students  now interact largely with multimodal texts. Further, in  conventional EL curriculum and pedagogy, students have limited  opportunities to extend their reading practices in socially,  cognitively and technologically engaging ways, even though they  engage continually with rich and fluid social media platforms in   their personal time [2].  Second, students currently have limited  access to meaningful and timely formative feedback regarding  their reading engagement and their broader literacy development  [3], thereby significantly constraining their capacity to self- evaluate and positively modify their learning behaviors. Until  these issues are effectively addressed, students low levels of EL  engagement and weak critical reading skills are likely to persist,  with adverse effects on their language proficiencies, and 21C  capacitiescritical thinking, collaboration and communication.  This challenge is the impetus for our design, implementation and  evaluation of WiREAD, a web-based collaborative critical reading  and LA environment aimed at deepening critical reading  engagement levels, promoting self-regulated and collaborative  knowledge construction in EL among Singapore Secondary 3  (Grade 9) students, during and beyond formal class time.   2. TECHNO-PEDAGOGICAL DESIGN  WiREAD was designed with the primary objective of motivating  and helping students to develop richer dialogue and quality  interactions with peers around multimodal texts, thereby  deepening their appreciation of and connection to collaborative  and critical reading as a generative social practice. So, the techno- pedagogical design of WiREAD focused on 2 key learning  affordances: online peer interactions around reading, and the LA  dashboard.   2.1 Online Social Reading and Discussion  Space  In its pedagogical and curricular design, the web-based social  reading and discussion tool is underpinned by Vygotskian socio- constructivist theories, informed by a Multiliteracies pedagogical  framework, which foregrounds 4 essential dimensions of effective  contemporary literacy enculturation in learners: 1) situated  practice, 2) overt instruction, 3) critical framing, and 4)  transformed practice [4; 2]. The micro-level of pedagogical  scaffolding scripts drew from Paul-Elders wheel of critical  reasoning [5] and our own work on dialogic indicators of  collective creativity and criticality [6] to develop 7 critical lenses  (message, purpose, audience, assumption, point of view,  inference, impact of language/visuals) and 5 critical talk types (I  think that, I think so because, I agree, I disagree, I need to ask).   Together, these served as a schema for guiding students  collaborative critique of texts on WiREAD in that students are  required to tag each of their comments/replies with 1 critical lens  and 1 critical talk type (Figure 1). Furthermore, each critical lens  and critical talk type tag contained a popover that provided  students with question prompts and sentence starters. These  served as a constant referential resource reminding students how  each tag could be used to critique texts more deeply. A new  teacher-nominated multimodal text was uploaded each week over  a 16-week term. Students had a weekly 30-minute reading period  in school and personal time beyond curriculum time to read,  comment and reply to others posts on the texts.    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than the author(s) must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom.  Copyright is held by the owner/author(s). Publication rights licensed to  ACM. ACM 978-1-4503-4190-5/16/04...$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883965       Figure 1: Texts, critical lenses, talk types, and popovers   Architecturally, WiREAD is built on WordPress, an open-source  PHP website creation tool, with additional plugins and in-house  programming codes to customize the functionalities for our  purposes. A major plugin, BuddyPress, enhanced the social  learning aspect where users can join singular or multiple groups,  and utilize interactive features including activities feed, personal  messaging and friendship system. As depicted in Figure 2, all data  are stored in a mysql database, with structural modifications made  to capture additional data required for the LA dashboard  component of the learning platform (see Section 2.2.).   Figure 2: WiREAD system architecture   2.2 Learning Analytics Dashboard  The LA affordance of WiREAD was designed with the aspiration  of providing rich, meaningful and timely formative feedback to  students and teachers throughout the term to help monitor reading  engagement and progress, and modify learning strategies and  pedagogical practices to improve outcomes. We focus here on the  student dashboard, which consisted of 4 separate components/tabs  visualizing a blend of EL achievement, dispositional, discourse,  content and social network analytics [7]:  (i) My Reading Achievement: data on students reading grades   on school-based assessments throughout the term captured in  teachers centralized information portal;   (ii) My Learning Attitudes and 21C Skills: data from student  self-report questionnaires administered at the start and end of  each term, using pre-validated measures of productive 21C  learning dispositions [8], including mastery/performance goal  orientations, deep/surface learning, task value, self-efficacy  and engagement, student-teacher relatedness, cognitive  playfulness (curiosity and creativity), criticality, collaboration  and communication;   (iii)  My WiREAD Critique & Discussion Profile: data on  students online engagement based on the frequency and   length of comments and replies, and frequency of each  critical lens and critical talk type across texts;   (iv) My Social Learning Network Map: sociograms reflecting  students position and influence within the WiREAD  learning network (ties based on students online discussion of  texts with others) and the class learning network (ties based  on students nomination of who and how often they  approached classmates to discuss texts critically).   These data indicators and corpuses were retrieved using PHP  functions integrated into the WordPress framework. In this  process, data was checked and sorted. Application algorithms  were applied to the data, with processed data parsed to the  visualization handler where the final data is presented to the end  user in interactive visualized models (Figure 3).     Figure 3: Learning dashboard visualizations (selected only)   3. STUDENTS EVALUATIVE ACCOUNTS  OF LA DASHBOARD  To develop more nuanced insights into the potentialities,  paradoxes and pathways forward for designing LA, we ask how  do students describe the benefits of and problems with the various  LA dashboard components and visualizations   3.1 Methods and Measures  In this study, we used a design-based research approach  supplemented by a quasi-experimental design. Pre/post-tests and  self-reported questionnaires, and qualitative feedback forms and  focus groups conducted at the start and end of the 16-week  innovation term together formed the impact evaluation of  WiREAD relating to its online social reading and discussion  space, and its LA dashboard. The Secondary 3 (Grade 9, 15-year  old) student participants comprised 3 WiREAD classes (N=116)  and 3 control classes (N=92). Here, we report and discuss the  WiREAD classes mixed methods evaluative accounts of the  student LA dashboard component.   Evaluative measures in the quantitative questionnaire included  (but were not limited to) a perceived ease of use scale (4 items), a  perceived usefulness scale (16 items measuring 4 key productive     learning dimensions [9]: i) socialization, (ii) expression of identity  & opinions, (iii) development of 21C skills & dispositions, (iv)  academic learning & performance), and a 4-item scale of how  helpful each of the four LA dashboard components were for  students learning and growth. All scales reported strong internal  consistency and confirmatory factor analysis scores (available  upon request). Student focus groups were conducted with 30  students (10 from each of the 3 WiREAD classes), purposively  selected to reflect diverse users with a range of low to high levels  of WiREAD usage, reading achievement, and vocality in class.  Remaining students (N=86) completed a qualitative feedback  form on the aspects of WiREAD they found to be useful or  otherwise for their learning and why.   4. RESULTS AND DISCUSSION  4.1 Quantitative Evaluative Findings  Descriptive analysis of questionnaire responses showed that  students found the LA dashboard easy to use, navigate and  understand (M=5.54 on a scale of 1-very difficult to 7-very easy,  SD=1.02). In general, they were moderately positive regarding  each dashboard components helpfulness to their learning and  growth (Figure 4), and regarding the overall dashboards  usefulness for enhancing our identified key 21C productive  learning dimensions (Figure 5).      Figure 4: Dashboard components helpful for learning/growth      Figure 5: Dashboard useful for enhancing 21C learning   4.2 Qualitative Evaluative Accounts & Insights  Students qualitative accounts of the LA dashboard provide  deeper insights into the experienced realities, promises and  problematics of such contemporary learning affordances.   4.2.1 User-perceived benefits of LA dashboard  We elaborate here on the 3 overarching themes that emerged from  students qualitative accounts of the LA dashboards perceived  learning benefits, namely, 1) generating greater awareness and   reflection of learning profile and progress, 2) enhancing learning  motivation and engagement (cognitive, affective and behavioral),  and 3) cultivating reciprocal social citizenry in learning.   Students repeatedly described the LA dashboard and its various  components as most informative in terms of making visible their  learning progress, dispositions and behaviors, which they implied  were usually vague or invisible to them: The [WiREAD  Critique/Discussion Profile] section shows how much comments  and replies we have done each week. This is very useful as it  serves as a reminder to write comments on texts we have not  done. It also encourages me to be more active as I often visit this  section to see my progress and effort (St8, 3R7). Similarly, St22  (3R6) stated, My WiRead Discussion Data and My Social  Learning Network Profile were also useful in monitoring my  progress and to encourage me to post and comment more and  have better quality comments. They also help to show which skills  and [critical] lenses I have not used, which allows me to know  which skills I should practice more on, and for Reading  Achievement dashboard, I found the EL achievement data most  usefulas I was able to gauge how much progress for EL I have  made as well as how I fair against other students. Through this, I  know what I can work on in order to improve (St19, 3R1).   Students frequently converged on the Learning Attitudes and 21C  Skills dashboard as the most eye-opening in helping them  discover and find out strengths and weaknesses that they did  not previously know about themselves: I found that My Learning  Attitudes and 21C Skills were the most useful. This dashboard  allowed me to realize what type of attitude I had towards learning  English and the way that I was trying to achieve my EL goals. I've  come to realize that my attitude towards EL was not the best as I  only learnt for the sake of doing well in exams not for the sake of  understanding the language. Thus looking at it, I am trying to  change my learning habits and attitude towards English in a  better and more positive direction (St1, 3R7). This reinforces the  need for schools to place more overt emphasis on developing  students 21C literacies and dispositions, given their salience in  predicting academic, employment and life outcomes [10, 11].   In all of the accounts, frequently heard among the participant  cohort were students attributions of strong links between greater  learner awareness as a precursor for enhanced learning motivation  and engagementcognitive, affective and behavioral [12]:  shows me where I standmotivates me to work harder,  motivate me to help or get help from my friends, force me  to think deeper, and makes me more motivated to comment so  that my [social network] dot can be bigger and brighter, and I  will know who to look to for help. This concurs with much LA  research on the potential of visible learning for promoting self- regulated learning [13], providing further empirical confirmation  that learner autonomy, competence and relatedness are vital to  intrinsic motivation and sustained behavioral change [14].  Also noteworthy was the recognition among students that the LA  dashboard, especially its social network analytics, bore much  value for nurturing a disposition we term reciprocal social  citizenry. This refers to a capacity to shift ones focus away from  self-interest and self-sufficiency to engage in productive help- giving and help-seeking learning behaviors: the spider web, some  of our names, the bubbles were bigger, right So it showed us like  who can we find like if we ever need any help in English. And  some of the people, whose bubbles are like smaller, they are out of  the web thing, so we can like we can just ask them whether they  need any help. But such insights on the value of the social  network map in promoting social responsibility and peer support     in learning were not common among students. We elaborate on  this problematic in Section 4.2.2.2.   4.2.2 Two corresponding problematics  Students qualitative accounts of the benefits gained from the LA  dashboard, however, drew opposing counterpoints from other  peers, thus providing insights into the ambivalence reflected in  responses on the usefulness of the dashboard for their learning and  growth. 2 key problematics were gleaned from the student talk: 1)  the ironic nature of norm-referenced vs. criterion-referenced  visualizations for motivating learning, and 2) the inherently low  levels of connective literacy among students.  4.2.2.1 LA visualizations a double-edged sword  Students were polarized in their views of norm-referenced visual  analytics as indicators of academic and social standing among  peers, and thus as a motivating mechanism for learning. For  many, knowing where they stand in class, if they are below  the average or at the edge of the network made them more  determined to increase [their] level of participation, to work  harder, be more active and improve their learning behaviors  online. Further, students often alluded to the inherent fun and  interesting nature of the visualizations as stimulating a form of  healthy peer pressure and informal competition that drives  them to engage with learning in more substantive ways: the  spider web would like, for me, it looks like a mini game, so that  its a race to get to the center and to get as big as you can (St12,  3R7). Higher-performing students in the classes even called for a  more formalized ranking system or hall of fame, asserting that  this leads to advancement and constant usage of WiREAD  (St25, 3R7).   There were, however, opposing views, and some pointed to the  adverse affective effects of clearly seeing oneself in relation to  others in the class, especially if one is at the bottom (My  Reading Achievement), or at the outside (My Social Learning  Network Profile): It was demoralizing sometimes. You look  like you are below average on the chart then the class average  is so high, and the maximum mark you are supposed to get is so  high, then you are below averageit was demoralizing  depressing to see that classmates are better at commenting or  thinking skills (St5, 3R7), and after several classes, I have  found out that working to a brighter and larger spot has turned  into a competitionthis provides added stress that we do not  need, as people have their own pace of learning and may be  slower. The slow pace of learning may be laughed at by  classmates and thus it is not as useful for learning as it may  emotionally affect others (St14, 3R6).   Students holding this view often asserted that learning  visualizations are most helpful when they are criterion-based and  self-referenced, rather than norm-referenced to peers: seriously, I  think what matters most is actually yourself instead of other  people it actually has some psychology effect. You shouldn't  compare with anybody else cause even though this is  collaborative learning, its really more of self-learning in the end  (Stu3, 3R1); and the purpose of comparing with others is to make  you improve to so-called match other peoples level, but  comparing with yourself would, can actually have the same effect  cause you are trying to improve based on your past. Comparing  with yourself will be more suitable. If like for example, the week  before you commented less, then you think like Oh yes, I actually  commented more [this week], I feel better, instead of comparing  with others like What Im last, what is this Not fair then you  cry (St8, 3R1).    This reinforces the use of intrinsic motivation mechanisms in the  design of LA to foster a mastery-oriented mindset that has been  shown to result in more adaptive outcomes relative to a  performance-oriented mindset [15]. It also foregrounds recent  positive psychology understandings that emphasize personal  best achievement goals as a highly salient predictor of students  academic motivation and engagement [16]. Moreover, it reminds  LA designers to be cognizant of the restrictive effects of one- sized-fits-all approaches to assessment and pedagogy in  conventional schooling [17], and the need to develop richer, more  nuanced and proximal multi-dimensional analytics [18], such that  differentiated instruction can become an experienced reality for  students, with purposefully-designed LA serving to compress,  rather than exacerbate, the learning and achievement gap between  thriving and struggling students.   4.2.2.2 Fostering connective literacy an imperative  for adolescent learners  A second issue became apparent from students talk around their  experiences with the dashboardlow levels of connective  literacy amongst adolescent learners. This has implications for  not only LA designers and pedagogues, but for a wider  educational community committed to helping students become  productive and engaged citizens beyond school. We conceptualise  and define connective literacy as the capacity to see learning as  an adaptive and fluid networked activity, where social positioning  is seen as constituting a form of learning capital requiring active  nurturing and reciprocal transactivity with significant others.   In other words, connectively-literate students recognize that the  opportunity to see their individual positioning within the larger  class network can prompt them to 1) find out the people that  [they] can actually approach for English, to seek help from  prominent others so as to improve and learn from them; 2) to  diversify and expand the circle of people whom [they] discuss  English texts with to take part in more conversations with the  whole class, instead of just the people [they] know better as  friends, thereby mak[ing them] think more, and also 3) to show  consideration to others at the side with hardly any connection  [to] try to see their comments more, and reply to their comments.   But this view was relatively nascent and yet to emerge among  many student participants. Most students consistently privileged  individual learning outcomes and skill acquisition as way more  important than social learning connections: To me, it doesnt  matter who Im connected to. As long as I can approach the text  with different critical lenses, it is useful and helpful. My Social  Learning Network Profile does not help me learn anything (St8,  3R1); others referred to social learning network profile as a  redundant system, because all it really did was show who  worked with who more. It had no real significant purpose to help  with my learning (St29, 3R1), with yet others stating that all Im  interested in is the content of the comments (St1, 3R6). To many  of these students, social networks have little bearing for learning  and growth, with network centrality or prominence being  trivialized as promoting a superfluous social butterfly  syndrome, at the expense of more serious learners those who  truly like to comment and learn (St10, 3R6).   So, we acknowledge the need for a critical evaluation of  WiREADs social learning network dashboard design and  pedagogical value. Nevertheless, these students accounts reflect a  highly nave, scientifically erroneous view of the power of social  networks and how these extensively influence ideas, emotions,  behaviors, learning and more [19]. This in turn brings to our  attention an often-overlooked imperativethat educators of     young leaners today need to be much more intentional and  purposeful in efforts to develop an empirically-informed  appreciation for the power of adaptive and reciprocal social  networked capitalas a form of new fundamental literacy  essential to successful learning, living and earning in  contemporary societies.   5. LIMITATIONS & WAYS FORWARD  To conclude, we highlight some limitations and ways forward.  WiREAD is only in its first design iteration, and we are conscious  of the LA dashboards rudimentary visualizations. There is room  for further enhancements to its user interface and aesthetic  features. Second, we are consulting with colleagues to incorporate  richer and more proximal indicators and analytics, and in  particular automated semantic and discourse analytics [7] that can  better assess the quality of critical reading and thinking reflected  in students discursive practices. Relying on students self-tagging  of critical lenses and critical talk types leads to large variations  and may be of limited validity. We are also working on  developing the teacher dashboard, to improve the adaptability of  pedagogical strategies with regard to learners needs and interests.  We aim to improve the pedagogical sensitivity and responsiveness  of both the LA dashboard and the teacher-pedagogue, in  recognition of the tight coupling between extrinsic pedagogical  scaffolding and intrinsic self-regulated learning [20].   The preliminary findings reported in this paper relate only to the  first of three design, implementation and evaluation cycles to be  conducted over the next 18 months. We hope, however, that the  data and discussion presented here fill some gaps in LA  researchby foregrounding secondary student-users perceptions  and experiences associated with the design and impact of LA for  their overall learning and growth, while shedding some light on  relevant aspects of Asian edu-cultural contexts. We have also  attempted to underscore some less discernible educational  problematics that underlie LA design, relating to both  instrumental and conceptual debates that are featured in the fields  scholarly discourse. It is in the collective deliberation of these  problematics that the transformative power of LA can be further  realized for more meaningful, equitable and future-relevant  educational outcomes and social life trajectories for learners.   6. ACKNOWLEDGMENTS  This paper draws from project NRF2013-EDU001-EL019, funded  by the edulab Research Program, Singapore National Research  Foundation. The views expressed in this paper are the authors  and do not necessarily represent the views of NIE Singapore.   7. REFERENCES  [1] Garcia, A., Mirra, N., Morrell, E., Martinez, A. and Scorza, D.   2015. The Council of Youth Research: Critical Literacy and  Civic Agency in the Digital Age. Reading & Writing  Quarterly, 31, 2, 151-167.   [2] Tan, J.P-L., and McWilliam, E. 2009. From Literacy to  Multiliteracies: Diverse Learners and Pedagogical  Practice. Pedagogies: An International Journal, 4, 3, 213-225.    [3] Davison, C. 2013. Innovation in assessment: Common  misconceptions and problems. Innovation and change in  English language education, 263-275.   [4] Cope, B. and Kalantzis, M. 2015. A Pedagogy of  Multiliteracies: Learning by Design. Palgrave Macmillan   [5] Paul, R. and Elder, L. 2001. Critical Thinking: Tools for  taking charge of your learning and your life. Prentice Hall,  Upper Saddle River, NJ.   [6] Tan, J.P-L., Caleon, I.S., Jonathan, C.R., and Koh, E. 2014. A  dialogic framework for assessing collective creativity in  computer-supported collaborative problem-solving tasks.  Research and Practice in Technology Enhanced Learning, 9,  3, 411-437.   [7] Ferguson, R. and Buckingham Shum, S. 2012. Social  learning analytics: five approaches. In Proceedings of the  2nd International Conference on Learning Analytics &  Knowledge, 2333.   [8] Tan, J.P-L. and Nie, Y.Y. 2015. The role of authentic tasks in  promoting 21st century learning dispositions in Mathematics.  In Authentic Problem Solving and Learning in the 21st  Century, Y. H. Cho, I. S. Caleon and M. Kapur, Eds.  Springer Science+Business Media Singapore Pte Ltd,  Singapore, 19-39.   [9] Tan, J. P. L. 2009. Digital kids, analogue students: a mixed  methods study of students' engagement with a school-based  Web 2.0 learning innovation. Doctoral thesis. ID Code:   30396, Queensland University of Technology.   [10] Tempelaar, D. T., Rienties, B., and Giesbers, B. 2015. In  search for the most informative data for feedback generation:  Learning Analytics in a data-rich context. Computers in  Human Behavior, 47, 157-167.   [11] Levin, H. M. 2012. More than just test scores. Prospects, 42,  3, 269-284.   [12] Wang, M. T. and Eccles, J. S. 2012. Adolescent behavioral,  emotional, and cognitive engagement trajectories in school  and their differential relations to educational success.  Journal of Research on Adolescence, 22, 1, 31-39.   [13] Lockyer, L., Heathcote, E. and Dawson, S. 2013. Informing  pedagogical action: Aligning learning analytics with learning  design. American Behavioral Scientist, 0002764213479367.   [14] Wise, A. F. 2014. Designing pedagogical interventions to  support student use of learning analytics. In Proceedings of  the 4th International Conference on Learning Analytics And  Knowledge, 203-211.   [15] Dweck, C. 2012. Mindset: How you can fulfil your potential.  Hachette UK.   [16] Martin, A. J. and Elliot, A. J. 2015. The role of personal best  (PB) and dichotomous achievement goals in students  academic motivation and engagement: a longitudinal  investigation. Educational Psychology, 1-18.   [17] Gaevi, D., Dawson, S., Rogers, T. and Gasevic, D. 2015.  Learning analytics should not promote one size fits all: The  effects of instructional conditions in predicating academic  success. The Internet and Higher Education, 28, 68-84.   [18] Dawson, S. and Siemens, G. 2014. Analytics to literacies:  The development of a learning analytics framework for  multiliteracies assessment. The International Review of  Research in Open and Distributed Learning, 15, 4.   [19] Christakis, N. A. and Fowler, J. H. 2009. Connected: The  surprising power of our social networks and how they shape  our lives. Little, Brown.   [20] Azevedo, R., Moos, D. C., Greene, J. A., Winters, F. I. and  Cromley, J. G. 2008. Why is externally-facilitated regulated  learning more effective than self-regulated learning with  hypermedia Educational Technology Research and  Development, 56, 1, 45-       "}
{"index":{"_id":"54"}}
{"datatype":"inproceedings","key":"Pardos:2016:IEA:2883851.2883949","author":"Pardos, Zachary A. and Xu, Yanbo","title":"Improving Efficacy Attribution in a Self-directed Learning Environment Using Prior Knowledge Individualization","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"435--439","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883949","doi":"10.1145/2883851.2883949","acmid":"2883949","publisher":"ACM","address":"New York, NY, USA","keywords":"Bayesian knowledge tracing, education, efficacy attribution, individualization, massive open online courses (MOOCs), prior knowledge, self-directed learning, self-selection bias","abstract":"Models of learning in EDM and LAK are pushing the boundaries of what can be measured from large quantities of historical data. When controlled randomization is present in the learning platform, such as randomized ordering of problems within a problem set, natural quasi-randomized controlled studies can be conducted, post-hoc. Difficulty and learning gain attribution are among factors of interest that can be studied with secondary analyses under these conditions. However, much of the content that we might like to evaluate for learning value is not administered as a random stimulus to students but instead is being self-selected, such as a student choosing to seek help in the discussion forums, wiki pages, or other pedagogically relevant material in online courseware. Help seekers, by virtue of their motivation to seek help, tend to be the ones who have the least knowledge. When presented with a cohort of students with a bi-modal or uniform knowledge distribution, this can present problems with model interpretability when a single point estimation is used to represent cohort prior knowledge. Since resource access is indicative of a low knowledge student, a model can tend towards attributing the resources with low or negative learning gain in order to better explain performance given the higher average prior point estimate. In this paper we present several individualized prior strategies and demonstrate how learning efficacy attribution validity and prediction accuracy improve as a result. Level of education attained, relative past assessment performance, and the prior per student cold start heuristic were employed and compared as prior knowledge individualization strategies.","pdf":"Improving efficacy attribution in a self-directed learning  environment using prior knowledge individualization  Zachary A. Pardos            University of California, Berkeley              zp@berkeley.edu           Yanbo Xu  Johns Hopkins University  asnoppy@gmail.com     ABSTRACT  Models of learning in EDM and LAK are pushing the boundaries  of what can be measured from large quantities of historical data.  When controlled randomization is present in the learning platform,  such as randomized ordering of problems within a problem set,  natural quasi-randomized controlled studies can be conducted,  post-hoc. Difficulty and learning gain attribution are among factors  of interest that can be studied with secondary analyses under these  conditions. However, much of the content that we might like to  evaluate for learning value is not administered as a random stimulus  to students but instead is being self-selected, such as a student  choosing to seek help in the discussion forums, wiki pages, or other  pedagogically relevant material in online courseware. Help seekers,  by virtue of their motivation to seek help, tend to be the ones who  have the least knowledge. When presented with a cohort of students  with a bi-modal or uniform knowledge distribution, this can present  problems with model interpretability when a single point estimation  is used to represent cohort prior knowledge. Since resource access  is indicative of a low knowledge student, a model can tend towards  attributing the resources with low or negative learning gain in order  to better explain performance given the higher average prior point  estimate. In this paper we present several individualized prior  strategies and demonstrate how learning efficacy attribution  validity and prediction accuracy improve as a result. Level of  education attained, relative past assessment performance, and the  prior per student cold start heuristic were employed and compared  as prior knowledge individualization strategies.   Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Distance learning   Keywords  Massive Open Online Courses (MOOCs), Self-directed learning,  Self-selection bias, Bayesian Knowledge Tracing, Efficacy  attribution, Prior knowledge, Individualization, Education   1. INTRODUCTION  A principle value proposition of Educational Data Mining (EDM),  and to an extent Learning Analytics, has been that the advent of  ever increasing volumes of education data will bring with it a  commensurate opportunity for significant discovery about learning  and human or computer-aided instruction. However, much of what   we would like to discover about learners pertain to causal  relationships that are notoriously difficult to derive from analysis  of historical data that was collected absent of necessary controls.   When elements of randomization do exist in a dataset, or the ability  to de-conflate a confounding variable, windows of opportunity for  discovery with statistical methods and tools from EDM open. Just  as Item Response Theory (IRT) has become the tool of choice for  measuring stationary constructs of ability in testing scenarios, so  too has Knowledge Tracing (KT) become a de facto tool for  measuring dynamics of knowledge in learning scenarios. Using  extensions of KT as a model for discovery [6], investigators have  exploited the randomization of 8th grade mathematics problem  orderings to attribute learning efficacy to ordered pairs of problems  [11] as well as different within-subject administered scaffolding  strategies [12]. KT has also been used to measure transfer between  skills in the domain of scientific inquiry [13]. While the hypotheses  of causality in those works were built into the design of the model,  recent work has sought to infer the appropriate causal structure  from data [8].   In this paper we introduce a method for improving learning value  attribution with models by de-conflating the confound of individual  prior knowledge. The data we use to validate the method come from  two popular post-secondary online courses, which constitute  partially self-directed learning environments [1]. Self-directed  learning environments give the learner a high degree of freedom in  selecting how she wishes to acquire information. The benefit of this  type of interaction is that there is now variation in the pathways  learners take to acquire knowledge. This variation means that there  is the potential to find signal that would not be present in a system  that delivers instruction in a fixed, linear sequence. The challenge  in analyzing such data is that it is wrought with self-selection bias.    The objective of our analysis is to accurately attribute learning to  the different pedagogical components of online courseware. The  selection bias may dictate, however, that the lower the prior  knowledge of a student, the more components she accesses. This  can easily lead to a model that attributes a component with zero or  even negative learning if a single point estimate is used to  characterize the cohorts prior knowledge. The results reported in  this paper show how different strategies for seeding individual prior  knowledge parameters facilitate improvements in learning  attribution as evidenced by improved predictive accuracy. The  strategies employed for prior knowledge individualization were:  level of education obtained from survey data, split based on global  assessment performance, and bootstrapping the prior using the  students first response to a question of a particular skill to  determine which prior class she belongs to [7].   1.1 Related Work  The following describes four examples of where investigators  measured the learning value of an activity in a self-directed or  partially self-directed learning environment and encountered     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883949       results that appeared to suffer from the selection bias native to self- directed environments.   Colleagues [9] working with a reading tutor, Project LISTEN,  measured the learning value of the systems help button, which  pronounces the word in question when pressed, using three  different analysis methods: an experimental design with pre and  post test, a regression based analysis named learning  decomposition, and a Bayesian assessment approach. In two of the  three approaches, the help button was deemed harmful to learning,  with only the Bayesian approach providing positive parameter  values associated with the voluntary help intervention.   Other investigators [3] looked at the learning value of nine different  activities found within a social learning platform for Algebra and  8th grade English Language Arts, Grockit. They found that five of  the activities lead to positive gains, while three did not contribute  to any gains, and one contributed to a loss.    Lastly, colleagues [2] evaluated two Massive Open Online courses  (MOOC) using Item Response Theory and correlated the quantity  of resources accessed by learners in the course with their  corresponding ability level as calculated by IRT. They found a  strong negative correlation between access of the resources and  ability. An approach based on Knowledge Tracing applied to  MOOC data from the same course found resource access to be  harmful to the accuracy of student response prediction [4].   In all of the cases described, the objective was to evaluate student  self-selected activities, interventions, or resources for their learning  value but in all cases the methodologies produced null or negative  correlations between some of the self-selected stimuli and learning.  We hypothesize that the stimuli did not likely result in negative  learning (forgetting or interference) but instead was the result of not  controlling for the confounding variable of prior knowledge.    2. DATASETS  The two datasets used in our study come from BerkeleyX, the brand  for Massive Open Online Courses (MOOCS) offered by the  University of California at Berkeley via the edX platform. The first  course dataset, Stat2.1x Introduction to Statistics: Descriptive  Statistics, contained 53 thousand enrolled participants (with 8,182  certificate earners) producing 17 million logged actions. The  actions include events such as:     Forum endorse    Forum view    Problem view    Problem answer    Video play    Video pause   These events are associated with a timestamp, username, and object  of the action taken. For example, a learner with username  marti411 playing video, S1L3.2, at 11:06am on April 1st, 2013.  The dataset also included the level of education reported by the  student upon signing up for an account on edX. The level of  education distribution consisted of 33% Bachelors, 32% Masters,  13% High School, with 6% non responding and the remainder  spread across a variety of additional levels.    The second course dataset, College Writing 2.1x: Principles of  Written English, contained around 60 thousand participants (with  4,614 certificate earners) and produced 9.8 million logged actions  with similar event types as the Stats course. The level of education  distribution for this course was nearly identical to the stats course.  These two courses represent the two most popular courses offered  by BerkeleyX and these data both come from the Spring 2013  offering of these courses. The 27 problems in the College writing   course were mostly multiple choice while the 131 Stats class  problems were mostly numeric answer. All problems were included  in the analysis, including non-exam questions where students were  allowed to answer until correct.   3. MODELS & METHODOLOGY  The objective of our study was to estimate the learning value that  should be attributed to the various components of the course:  treating the resources such as videos and forum posts as learner self  selected stimulus. We hypothesized that modeling individual  student knowledge would improve the accuracy of the learning  attribution estimate and that such an improvement would be  reflected by an improvement in prediction accuracy.      Figure 1. Learning efficacy attribution model based on KT   Take, for example, the event sequence shown at the top of Figure  1. A student answers a problem incorrectly, then proceeds to watch  a video, S1V2-5, then another video, S1V2-6, before returning to  the question and answering it correctly. The goal of our learning  efficacy model was to estimate the influence of each component  (the videos in this example) in getting the student to a state in which  she could answer the problem correctly. We utilized an extension  of the Bayesian Knowledge Tracing (KT) model to estimate this  learning efficacy associated with the various pedagogical  components. The model assumes that knowledge is binary, either  acquired or not, and that there exists some probability that a student  will transition from the unacquired state to the acquired state  between events. In a traditional Intelligent Tutor context, the  probability of transition is constant across events of the same skill  (or knowledge component). This transition parameter, referred to  as P(T), primarily captures the probability that a student will learn  due to receiving feedback on the correctness of their answer and  receiving any hints that may have been available after answering  incorrectly. In a MOOC context, there is significantly more variety  of pedagogical material. Consequently, the extension of the model,  instead of using just one transition probability, models a transition  probability for each pedagogical component + the original  transition probability associated with answering a question and  receiving feedback. The formula for calculating the updated  probability of knowledge given a transition is as followed:     #$%& '  #$%()|+,-./01/& 2  #3$%()|+,-./01/&   #5&  Where the probability of knowledge at event n is equal to the  probability of knowledge after observing the previous event  (posterior probability at n-1) plus the compliment of that value  multiplied by the transition probability associated with the event  that just occurred (eg. P(T) given a problem answer event, video  playing event, etc). A preliminary version of this model, without  prior knowledge individualization, was piloted on a dataset of  2,000 students in [4] and used to detect video relevance in [5]. The  relationship between the value of the latent and the predicted     probability of a correct answer to a problem is expressed with the  guess, P(G), and slip, P(S), parameters. These are equivalent to the  emission probabilities in a Hidden Markov Model (HMM) and  represent the noise that exists between knowledge and  performance. The last of the parameters is the prior knowledge  parameter, P(L0). This represents the probability of an arbitrary  student being in the acquired knowledge state before interacting  with the skill or learning objective at hand. In the standard KT  model, this is a single point estimate. In the work presented in this  paper, we will evaluate three different strategies for individualizing  the prior parameter. Individualization of the prior means assigning  students to one of two or more priors based on past performance or  external survey information. In order to train these parameters,  Expectation Maximization (Baum-Welch) is used to search for the  set of parameter values that maximizes the likelihood fit to the data.  The trained model is then used to predict every student response in  test set. We use student level 5-fold cross validation and use the  extended Bayesian Knowledge Tracing software package, xBKT1  (Xu, Johnson & Pardos, 2015) to conduct fast training and  prediction. One hundred random EM restarts were used and the best  log-likelihood model was chosen within each cross-validated  training phase.   3.1 Prior Individualization Strategies  Our hypothesis was that models learning attribution and prediction  accuracy would improve with prior knowledge individualization.  For each of the two datasets we evaluated the model using four  different prior knowledge strategies.    1. Single Prior: Only use a single EM learned prior point  estimate. This is the standard BKT modeling approach  (baseline).   2. Prior Per Student (PPS) Cold Start Heuristic: Assign  each learner to one of two prior groups based on the  correctness of their first response, then use EM to learn  the point estimates of those two groups.   3. Average Performance Split: Assign each learner to one  of two prior groups based on if their total percent correct  across all problems, besides the one being predicted, was  above or below the median, then use EM to learn the  point estimates of those two groups.   4. Level of Education: Assign each learner to one of 11 prior  groups depending on their level of education stated in  their survey response and use EM to learn the priors.   We ran two different attribution models for each prior strategy: One  where each individual pedagogical component received its own  transition probability, such as a transition probability for video  S1V2-5, and another, coarser-grained model, where transition  probability was associated instead with the event action, such as:  Video play, Problem answer, and Forum view. This coarser- grained model would take care of potential sparsity issues in the  fine-grained model and would still provide substantive feedback to  instructors on which types of actions were leading to learning and  which were not. The fine-grained model, if successful, would allow  for instructor feedback on individual components and open up the  possibility of inserting pointers to particular high efficacy  components. The skill model used for all models treated the  problem as the skill (or KC). In edX, a problem consists of multiple  question sub-parts and often a shared figure. This can be seen as a                                                                     1 https://github.com/CAHLR/xBKT   learning efficacy attribution model that estimates the probability of  various pedagogical components getting a learner to the state where  she can answer elements of the problem correctly. This was the best  KC model among those derived strictly from course structure  information [4].   In the case of the prior per student cold start heuristic; the first  response dictates the prior category, an incorrect first response  assigns the students to the low prior, a correct response to the high  prior. In order not to peek at the data when predicting the first  response of the test folds, the marginal probability of prior  knowledge is used in place of the appropriate prior; low prior  multiplied by the prior frequency of the low prior + high prior  multiplied by the prior frequency of the high prior. Average  performance median split is determined by calculating the percent  correct on first attempts across KCs for each student, not counting  the KC being predicted. For students who only answered a single  KC, the marginal is again used.   4. RESULTS  For each of the two datasets we evaluated eight different attribution  models using four different prior knowledge individualization  strategies (single prior, prior per student, average performance  split, and level of education) and two different pedagogical  component granularity representations (parameter per component  and parameter per event action type). A model using the single prior  and ignoring all non-answer events was also included as baseline.  Root Mean Squared Error (RMSE) was used as the error metric to  score prediction. If the attribution models outperformed the model  using only answers, then this suggests that the event or component  transition probabilities are generalizing from the training set to  unseen students. We investigate if prior knowledge  individualization further improves parameter generalization and  prediction. Results of model evaluation are shown in Table 1 (Stats  2.1) and Table 2 (College Writing 2.1).    Table 1. Stats2.1 dataset: model prediction results for four  prior knowledge strategies and two transition granularities   Individual prior  knowledge  strategy   Transition  granularity RMSE   Avg. perf. split Event Action 0.404675  Avg. perf. split Component 0.405385  Level of education Event Action 0.416106  Single prior Event Action 0.416371  PPS cold start Event Action 0.416618  Level of education Component 0.416932  Single prior Component 0.417262  PPS cold start Component 0.417266  Single prior Only answers 0.417308     The standard KT model that used only answer correctness to train  produced the lowest RMSE (0.4173). Adding information about  when a student visits a particular components reduced the error but  not as much as taking into account resource visits at the event action  level. Neither resource abstractions reduced error statistically  significantly using a paired t-test. The prior knowledge     individualization strategy of assigning priors based on average  performance median split amplified the prediction performance  gain of both the efficacy attribution models over the single prior  event action efficacy model (p << 0.01). Level of education as a  prior individualization was the third best in performance but was  not statistically separable from the single prior event action model.  The prior per student models did not add any performance, with  results nearly identical to the respective single prior models.  Table 2. ColWri2.1 dataset: model prediction results for four  prior knowledge strategies and two transition granularities   Individual prior  knowledge  strategy   Transition  granularity RMSE   Avg. perf. split Event Action 0.413567  Avg. perf. split Component 0.413683  Level of education Event Action 0.425777  Level of education Component 0.425807  Single prior Event Action 0.426266  Single prior Component 0.426307  Single prior Only answers 0.426421   PPS cold start Event Action 0.426840  PPS cold start Component 0.426844     The results in College Writing 2.1 mostly echoed  those in the Stats  class. Event action and Component efficacy attribution models with  a single prior lower error (insignificantly) from the answer only  model. The average performance median split individualization  amplified the efficacy attribution models over their single prior  counter parts (p < 0.05) with level of education, much like with  Stats, being the third best performing model, but not statistically  separable from the single prior attribution model. In the case of  College Writing, the prior per student individualizations performed  the worse (not stat sig. from single prior).   Table 3.  Prior group values for Stats and College Writing   Prior Stats 2.1 College Writing  2.1   PPS (comp.) high 0.9537 0.8950  PPS (comp.) low 0.0452 0.1037  Avg. perf. high 0.5498 0.5669  Avg. perf. low 0.3322 0.4201     Table 3 shows the group prior parameter values for the two  datasets. In Stats, top two prior groups for level of education (event)  were Doctorates and Masters degrees, with 0.55 and   0.54 priors.  College Writings top two levels of education were Doctorate, with  0.51, followed by High school, with 0.50.    5. CONTRIBUTIONS  In this paper we address the challenging endeavor of using  historical data analysis to try to evaluate the learning efficacy of  various learner activities. We used a novel dataset, consisting of  two online courses (BerkeleyX Intro Statistics 2.1 and College  Writing 2.1), each with tens of thousands of learners, to investigate  whether or not controlling for individual prior knowledge could  mitigate the self-selection biases found in data from self-directed   learning environments. Our results, which were consistent across  the two datasets, support our hypothesis that individualizing  student knowledge in the BKT model can improve learning  efficacy attribution and subsequently improve performance  prediction. Particularly salient was the average performance split  strategy using coarse-grained transition parameters in the both  courses. The prior per student model performed no differently from  the single prior model in the Stats class and below the single prior  in the College Writing. The underperformance in ColWri may be  due to the multiple-choice nature of the assessments in that class  which are easier to guess and therefore the first question, used to  seed the individual prior, is less indicative of what the student  knows about the more general topic. All models, including prior per  student, would be expected to improve in accuracy with KC models  based on subject matter expert tagging. More precise efficacy  attribution would be expected in this case, as well.    This work serves to demonstrate that accounting for different priors  among students can improve the accuracy of models which  attribute learning value of individual resources and event types.  Accurate efficacy is important for potential recommender systems  which use modeling techniques. Such systems could suggest  components or actions to struggling students that are associated  with high learning rates, P(T).Additionally, instructors can receive  more accurate feedback regarding the resources that lead to higher  rates of learning, and which lead to the lowest and for which groups  of students. Lastly, where prior individualization works, it suggests  ways in which future course offerings may be adapted based on  either background education or performance demonstrated in class.   Acknowledgements  A Google Faculty Research Award (#2014_R1_446) provided  support for this collaboration.   REFERENCES  [1] Gureckis, T. M., & Markant, D. . (2012). Self-directed   learning a cognitive and computational  perspective. Perspectives on Psychological Science, 7(5),  464-481.   [2] Champaign, J., Colvin, K. F., Liu, A., Fredericks, C., Seaton,  D., & Pritchard, D. E. (2014). Correlating skill and  improvement in 2 moocs with a student's time on tasks.  In Proceedings of the first ACM conference on Learning@  scale conference (pp. 11-20). ACM.   [3] Bader-Natal, A., Lotze, T., & Furr, D. (2011). A comparison  of the effects of nine activities within a self-directed learning  environment on skill-grained learning. In Artificial  Intelligence in Education (pp. 15-22). Springer Berlin  Heidelberg.   [4] Pardos, Z.A., Bergner, Y., Seaton, D., Pritchard, D.E. (2013)  Adapting Bayesian Knowledge Tracing to a Massive Open  Online College Course in edX. DMello, S. K., Calvo, R. A.,  and Olney, A. (eds.) Proceedings of the 6th International  Conference on Educational Data Mining (EDM). Memphis,  TN. Pages 137-144.   [5] MacHardy, Z.M, Pardos, Z.A. (Accepted) Evaluating The  Relevance of Educational Videos using BKT and Big Data.  To Appear in the Proceedings of the 8th International  Conference on Educational Data Mining (EDM). Spain.    [6] Baker, R. S., & Yacef, K. (2009) The state of educational  data mining in 2009: A review and future visions. JEDM- Journal of Educational Data Mining, 1(1), 3-17.     [7] Pardos, Z. A., Heffernan, N. T. (2010) Modeling  Individualization in a Bayesian Networks Implementation of  Knowledge Tracing. In P. De Bra, A. Kobsa, D. Chin (eds.)  Proceedings of the 18th International Conference on User  Modeling, Adaptation and Personalization (UMAP). Big  Island of Hawaii. Springer. pp 255-266.   [8] Fancsali, E.S. (2014) Causal Discovery with Models:  Behavior, Affect, and Learning in Cognitive Tutor Algebra.  In Stamper, J., Pardos, Z., Mavrikis, M., McLaren, B.M.  (eds.) Proceedings of the 7th International Conference on  Educational Data Mining (EDM). London, UK. Pages 28-35.   [9] Beck, J. E., Chang, K. M., Mostow, J., & Corbett, A. (2008).  Does help help Introducing the Bayesian Evaluation and  Assessment methodology. In Intelligent Tutoring Systems  (pp. 383-394). Springer Berlin Heidelberg. Chicago   [10] Koedinger, K. R., Booth, J. L., & Klahr, D. (2013).  Instructional complexity and the science to constrain  it. Science, 342(6161), 935-937.   [11] Pardos, Z. A., Heffernan, N. T. (2009) Determining the  Significance of Item Order in Randomized Problem Sets. In  Barnes, Desmarais, Romero & Ventura (eds.) Proceedings of  the 2nd International Conference on Educational Data  Mining (EDM). Cordoba, Spain. Pages 111-120.   [12] Pardos, Z.A., Dailey, M. & Heffernan, N. (2011) Learning  what works in ITS from non-traditional randomized  controlled trial data. The International Journal of Artificial  Intelligence in Education, 21(1-2):45-63.   [13] Sao Pedro, M., Baker, R., & Gobert, J. (2013). Incorporating  Scaffolding and Tutor context into Bayesian Knowledge  Tracing to predict inquiry skill acquisition. In Proc. of the  6th International Conference on Educational Data Mining,  Memphis, TN (pp. 185-192)       "}
{"index":{"_id":"55"}}
{"datatype":"inproceedings","key":"Taraghi:2016:BMS:2883851.2883895","author":"Taraghi, Behnam and Saranti, Anna and Legenstein, Robert and Ebner, Martin","title":"Bayesian Modelling of Student Misconceptions in the One-digit Multiplication with Probabilistic Programming","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"449--453","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883895","doi":"10.1145/2883851.2883895","acmid":"2883895","publisher":"ACM","address":"New York, NY, USA","keywords":"Bayesian modelling, learning analytics, one-digit multiplication, probabilistic programming","abstract":"One-digit multiplication errors are one of the most extensively analysed mathematical problems. Research work primarily emphasises the use of statistics whereas learning analytics can go one step further and use machine learning techniques to model simple learning misconceptions. Probabilistic programming techniques ease the development of probabilistic graphical models (bayesian networks) and their use for prediction of student behaviour that can ultimately influence learning decision processes.","pdf":"Bayesian Modelling of Student Misconceptions in the  one-digit Multiplication with Probabilistic Programming  Behnam Taraghi  Educational Technology  Graz University of Technology  Mnzgrabenstrasse 35, 8010  Graz, Austria  b.taraghi@tugraz.at  Anna Saranti  Institute of Theoretical  Computer Science  Graz University of Technology  Inffeldgasse 16b/I, 8010  Graz, Austria  s0473056@sbox.tugraz.at  Robert Legenstein  Institute of Theoretical  Computer Science  Graz University of Technology  Inffeldgasse 16b/I, 8010  Graz, Austria  legi@igi.tugraz.at  Martin Ebner  Educational Technology  Graz University of Technology  Mnzgrabenstrasse 35, 8010  Graz, Austria  martin.ebner@tugraz.at  ABSTRACT One-digit multiplication errors are one of the most exten- sively analysed mathematical problems. Research work pri- marily emphasises the use of statistics whereas learning an- alytics can go one step further and use machine learning techniques to model simple learning misconceptions. Prob- abilistic programming techniques ease the development of probabilistic graphical models (bayesian networks) and their use for prediction of student behaviour that can ultimately influence learning decision processes.  CCS Concepts Mathematics of computing ! Bayesian networks; Probabilistic algorithms; Variable elimination; Expectation maximization; Computing methodologies ! Bayesian network models; Applied computing ! E-learning;  Keywords Learning Analytics; Bayesian Modelling; Probabilistic Pro- gramming; One-Digit Multiplication  1. INTRODUCTION After analysing the most prevalent error types that are  observed in one-digit multiplication, we carried out a de- tailed analysis [23], [22], [19]. With the use of heat maps and diagrams we depicted those misconceptions that are of higher relevance, because they occur more often, and we provided hints at probable reasons. But the presentation  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883895  of these elementary statistics, also used in other extensive analysis research work such as in [24], is not build with the means to adapt. If we are to build a data-driven learning application that bases its further analysis and decision mak- ing on those outcomes, we need an adaptive mechanism that contains not only quantitative but also qualitative informa- tion about the interdependencies of the elements involved as well as a structure that can be updated straightforwardly. Even sophisticated statistics, providing only current repre- sentations (snapshot), are inflexible and not easy to extend in the face of new ideas and information. These require- ments are indeed met by Bayesian Networks, also known as Probabilistic Graphical Models [8].  The answers of the students that were used in this analysis were provided by the 1x1 trainer application [17] 1. We were provided with answers of one-digit multiplication questions, over a period of two years. These data were used to train and test the performance of our model. Apart from answering data, the application does not store any demographic values about the users such as age or gender. This means that we have to make conclusions with very limited information.  Section 2 of this paper presents related work in the area of learning applications that use Bayesian Networks. Section 3 covers our current work and what has been implemented so far. Finally, the planned future research work is described in section 4.  2. RELATED WORK The creation of probabilistic student models with the use  of Bayesian Networks is a method that is used by several re- search groups; representative literature reviews can be found in [4] and [12]. More information about Bayesian networks can be found in [8], [1] and [13].The most common use of these networks is the representation of students behaviour [16], knowledge, cognitive [14] and mental state as well as incorrect knowledge and misconceptions [5], [6]. One benefit of Bayesian Network Modelling is the inference of informa-  1http://schule.learninglab.tugraz.at/einmaleins/index/play last access 31 October 2015    Learning  State  Answer of  Question 9*9  Answer of  Question 1*1  Answer of  Question 8*5... ...  8*5    Probability  Operand  0.11  Intrusion  0.11  Consistency  0.12  Off-By  0.21  Add/Sub/Div  0.11  Pattern  0.11  Unclassified  0.11  Correct  0.11                        ...  30  37  38  39  40  41  42  43  ...  Operand  ...  0.083  0.0  0.0  0.0  0.0  0.0  0.083  0.0  ...  Intrusion  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  Consistency  ...  0.058  0.0  0.058  0.058  0.0  0.063  0.058  0.058  ...  Off-By  ...  0.0  0.0  0.20  0.20  0.0  0.38  0.20  0.0  ...  Add/Sub/Div  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  Pattern  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  Unclassified  ...  0.0  0.02  0.0  0.0  0.0  0.0  0.0  0.0  ...  Correct  ...  0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  ...                Figure 1: The Bayesian student network repre-  senting the one-digit multiplication misconceptions.  The random variable Learning State and its corre-  sponging conditional probability distribution (CPD)  table is the parent node for the ninety Answers of  Questions random variables, each one for the 1x1  to 9x9 posed questions. Their CPD tables repre-  sent the conditional probability distribution of these  variables, where the conditioning is on all possible  Learning State outcomes.  tion that cannot be observed directly but can be inferred from the gathered student data and the model, such as the reasons of the learners answers, hidden cognitive states and meta-cognitive skills in a diagnostic manner [9], [10], [11].  The design of the model requires domain knowledge about the topic that is represented by it. In many cases the struc- ture and the parameters to begin with are provided by the teachers. Prior information can also be acquired directly by the learners by a pre-test [5], the teachers [25], or a quiz that is done to initialize the model before its actual use. The models are usually integrated in learning applications that are updating the model frequently (if possible in real-time) to improve the accuracy of the representation. The evalua- tion typically consists of validation by measuring the success of the predictions of the model during the subsequent use of the application or by a post-test [5].  Some of these applications go beyond the quantification of factors that play a role in learning and their dependen- cies; they use the models to manage personalisation in the learning application. The creation of feedback and targeted recommendations to the students [16] is driven by the infor- mation contained in and elicited from the model.  3. PROBABILISTIC STUDENT MODEL  3.1 Model Structure We have considered six specific error types of the one-  digit multiplication: operand, intrusion, consistency, o-by-  1 and o-by-2, pattern, as well as confusion with addi- tion, subtraction, and division operation errors, as described in [3] and [19]. The collection of these mistakes consist of our bug library [4] or taxonomy of misconceptions [5] which, at this point, is created by enumeration and is driven by domain knowledge. Any false answer that cannot be as- signed to one of these categories, is put in a new category called unclassified errors. Any result that was negative or greater than 100 was considered more or less careless play- ing around (because the mathematical principles of one- digit multiplication are known to the users) and was not taken into account. Correct answers assume the absence of any error type and are categorized as correct. The model belongs to the category of so-called perturbation models [4], that represent various kinds of mal-knowledge. Although the category correct seems like a misfit in a perturbation model, it is defined as the absence of any misconception or error type and is included to help the prediction process.  The first step in quantifying the extend of each error type for each student is the creation of the probabilistic student model. The representation of misconceptions in this model is build based on minimal structural assumptions about the in- terdependencies of the dierent error types; we do not begin by considering them dependent. This approach was chosen in other related research work too [18], [6]. There are how- ever answers to questions that could belong to many error types, for example a student may answer the question 8 5 with a 41, which could be both a consistency and o-by-+1 error. The more data we have, the more certain we will be which error type is more dominant.  The model was conceived under specific requirements. It is not a rigid representation of the learners state of knowl- edge, since this aspect changes over time, even during the application is used. Furthermore, it is not considered to be a perfect and absolute measurement of the students miscon- ceptions; it is a representation of our estimation about them. Therefore, it can certainly change over time; the more data we observe the more reliable our conclusions (that we make based on it) are. Moreover, it must acknowledge that each learner should be handled dierently , has its own problems and learning diculties, but at the same time one expects similarities to occur. When facing a new user, the system is not completely agnostic; it has already observed the be- haviour of other users and merely needs some individualisa- tion information.  The highest level of the model, as seen in figure 1, is a random variableLearning Statethat represents the current learning state of the student. The possible outcomes of this random variable are the specific error types as well as the correct category, which are acquired during learning (for ex- ample in our case in a math class). One can represent these outcomes in a table and make statements of the form: A particular person, when dealing with one-digit multiplica- tion problems, makes 20% operand errors, 10% consistency errors and so on. As we will see in section 4, these state- ments can be extended to the whole set of students.  The children of this node are ninety random variables named Answers of Questions 1x1 to Answers of Ques- tions 9x9. The answers given by a student depend on the kind of misconceptions (and lack thereof in some cases) that he or she acquired during math learning. The edges in the graph represent this causal relationship. So, these variables depend directly on the Learning State, their outcomes are    Learning State  Answers of  Question 9x9  Answers of  Question 1x1 ...  Q1x1|LS Q9x9|LS  LS  Answering Data  Figure 2: Meta-network of the student model us-  ing plate notation. The random variables and their  dependencies (as in figure 1) are depicted together  with their corresponding (Dirichlet) hyper parame-  ters  Q1x1|LS . . . Q9x9|LS. The nodes that are shaded  are the ones that we can observe whereas the non-  shaded ones arehiddenorlatentand their values  must be inferred.  conditioned on it and therefore the table is called a con- ditional probability table. For every value of the random variable Learning State , the probabilities of the answers in the random variable Answers of Question need to be defined.  3.2 Learning the Models Parameters Although our assumption about the dependencies define  the structure of the model (functional forms of these depen- dencies), it does not state how and to which extend each of them generates the answers we observe in the data. The quantification of this relationship is to be learned from the data itself.  At first, the model is trained with assumptions about the misconceptions of the student. It is assumed that each error type and the unique correct option are equally likely. The same applies to the Answers of Questions random vari- ables; we do not assume that given a specific error type there is any preference or tendency towards a particular answer, which is greater than the others produced by this miscon- ception. As the answering data is gathered, the probability of the error type(s) and the correct category, associated with each particular answer, change. A straightforward choice is to parametrize the random variables as depicted in figure 1 by   LS  and  Q1x1|LS . . . Q9x9|LS using Dirichlet hyper pa-  rameters both for Learning State and for each line of the Answers of Questions tables. More information about the  Dirichlet distribution and its use for the definition of hyper parameters can be found in [8], [2] and [15].  Unfortunately, we do not have data from pre-tests or lon- gitudinal studies. On the other hand, the model will either way adapt during data processing, so we decided to start with a non-informative uniform prior, where each miscon- ception (error type) or correct conception is equally likely; in our case, each of them has probability 0.125 to exist. We applied the same principle to the Answers of Questions variables; each answer that is a result of a specific miscon- ception, starts with the same probability. So our starting prior for every line in the Table CPDs of these variables is also uniformly defined. We trained the model with the data of all students and we set this result (the posterior parameter distribution) as the new, updated prior for further learning (which is of course dierent from the starting uniform prior). This updated prior will be the new starting point for each student, which will be individualized in a much lesser degree and faster than the starting uninformed prior.  To help the model learn the parameters that most accu- rately represent the student answering behaviour, we used the expectation-maximization (EM) - Algorithm. Since it is out of scope of this paper to describe the details of the algorithm we refer to [2], [8] and [15]. Basically the algo- rithm starts either with random or pre-defined parameters and continuously improves its estimation. Returning to our previous example, where the student confronted the ques- tion 8x5 and answered 41, the probabilities of consistency and o-by errors therefore subsequently increase as seen in figure 1. The rest of the probabilities have the same value. The eect of this particular data point is that we infer which error types and how much have caused this result.  As a result we see that this change in probability of ob- serving an error is not the same for all error types because they dont have the same number of answers generated from them. For example, consistency error produces a total of 17 whereas the o-by error 4 incorrect answers. That means that this model needs more evidence to state that the preva- lent misconception of a person is a consistency rather than an o-by error. Although it seems as if there is an imbal- ance, this phenomenon is perfectly explainable by the terms by which we defined the model; each misconception is gen- erally manifested by a dierent number of faulty answers. It is questionable whether teachers follow the same logic, or rather consider each evidence as equally important. In any case, it is something that we are aware of and do not bypass in the future steps of our analysis.  The implementation of the model was made with the func- tional and object-oriented Scala programming language 2, using the probabilistic programming library Figaro 3 [15].  4. FUTURE WORK The common mistakes that students make in one-digit  multiplication have been thoroughly analysed. They are the first starting point when creating the bug library. Our anal- ysis does not only show which of the error types are more serious, but also unveils patterns inside a specific error type. For example, one does not only know that a particular stu- dent makes more often the operand error mistake, but also  2http://www.scala-lang.org/ last access 31 October 2015 3https://www.cra.com/work/case-studies/figaro last access 31 October 2015    which faulty answer in this category is the most prevalent. So one could continue searching for patterns in the unclas- sified errors category as well as altering the structure of the model correspondingly to validate in case the change produces more accurate evaluation results. Of course, this would change the structure of the model and therefore it needs to be evaluated as well as compared with the previous version.  It would be interesting to find out whether there are groups of learners (stereotypes [4])that have similar problems when given these questions, as described in [21] or not. We may expect that the distribution of the random variable Learn- ing State of learners of each group is similar, and the whole set created by all groups together is parametrised by a Mix- ture of Dirichlet distributions.This means that we will be able to state that, for example, the Learning State of 20% of our students has values near to a particular Dirichlet dis- tribution, 30% equals a completely dierent Dirichlet dis- tribution and so on. This would add another level to our model, and the current Learning State will depend on it.  The probabilistic model can be used to define a generative process, where all random variables will generate values ac- cording to their distribution. After the learning is done, the model can predict the answer that will be given to the next posed question. Several learning applications use this infor- mation to decide what the next learning topic will be [18]. Probabilistic and maximum a posteriori (MAP) queries can be used to influence all kinds of factors in a learning applica- tion such as hints, helping notes, rearrangement of questions sequence and so on.  Furthermore, the model can become the central element of the application architecture because the eectiveness of any such change for a potential improvement of the learning process can be directly evaluated. So the ultimate goal of the modelling process is not to remain a flexible and adaptable analysis tool that provides information to its stakeholders (teachers, students), but to eect the learning process itself. Bayesian Networks can be extended to undertake decisions under uncertainty and to influence actions that fulfil some optimality criterion(for example posing the question that is diculty appropriate next) [8], [7] in a more sound and straightforward way than in [20].  5. REFERENCES [1] D. Barber. Bayesian Reasoning and Machine  Learning. Cambridge University Press, The Edinburgh Building, Cambridge, United Kingdom, 2012.  [2] C. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.  [3] J. I. Campbell. Mechanisms of simple addition and multiplication: A modified network-interference theory and simulation. Mathematical Cognition, 1(2):121164, January 1995.  [4] K. Chrysafiadi and M. Virvou. Student modeling approaches: A literature review for the last decade. Expert Systems with Applications, 40(11):47154729, September 2013.  [5] G. Goguadze, S. A. Sosnovsky, S. Isotani, and B. M. McLaren. Evaluating a bayesian student model of decimal misconceptions. In Proceedings of the 4th International Conference on Educational Data Mining, pages 301306, July 2011.  [6] G. Goguadze, S. A. Sosnovsky, S. Isotani, and B. M. McLaren. Towards a bayesian student model for detecting decimal misconceptions. In Proceedings of the 19th International Conference on Computers in  Education, pages 3441. Asia-Pacific Society for Computers in Education, November 2011.  [7] M. J. Kochenderfer. Decision Making Under Uncertainty : Theory and Application. The MIT Press, Cambridge, Massachusetts, London, England, 2015.  [8] D. Koller and N. Friedman. Probabilistic Graphical Models : Principles and Techniques. The MIT Press, Cambridge, Massachusetts, London, England, 2009.  [9] E. Milan, J. M. Agosta, and J.-L. P. de la Cruz. Bayesian student modeling and the problem of parameter specification. British Journal of Educational Technology, 32(2):171181, March 2001.  [10] E. Millan and J.-L. P. de la Cruz. A bayesian diagnostic algorithm for student modeling and its evaluation. User Modeling and User-Adapted Interaction, 12(2):281330, June 2002.  [11] E. Millan, T. Loboda, and J.-L. P. de la Cruz. Bayesian networks for student model engineering. Computers & Education, 55(4):16631683, December 2010.  [12] J. Nakic, A. Granic, and V. Glavinic. Anatomy of student models in adaptive learning systems: A systematic literature review of individual dierences from 2001 to 2013. Journal of Educational Computing Research, 51(4):203234, January 2015.  [13] R. E. Neapolitan. Learning Bayesian Networks. Prentice Hall, Northeastern Illinois University Chicago, Illinois, 2003.  [14] Y. Nouh, P. Karthikeyani, and R. Nadarajan. Intelligent tutoring system-bayesian student model. In Proceedings of the 1st International Conference on  Digital Information Management, pages 257262. IEEE, December 2006.  [15] A. Pfeer. Practical Probabilistic Programming. Manning Publications, New York, United States of America, 2016.  [16] S. Schiano, P. Garcia, and A. Amandi. eteacher: Providing personalized assistance to e-learning students. Computers & Education, 51(4):17441754, December 2008.  [17] M. Schon, M. Ebner, and G. Kothmeier. Its just about learning the multiplication table. In Proceedings of the 2nd International Conference on Learning  Analytics and Knowledge, pages 7381. LAK 2012, October 2012.  [18] K. Stacey, E. Sonenberg, A. E. Nicholson, T. Boneh, and V. Steinle. A teaching model exploiting cognitive conflict driven by a bayesian network. In Proceedings of the 9th International Conference on User  Modelling, pages 352362. Springer, June 2003. [19] B. Taraghi, M. Frey, A. Saranti, M. Ebner, V. Muller,  and A. Groman. Determining the causing factors of errors for multiplication problems. In Immersive Education, Edition: Communications in Computer  and Information Science 486, pages 2738. Springer, August 2015.  [20] B. Taraghi, A. Saranti, M. Ebner, V. Muller, and A. Groman. Adaptive learner profiling provides the    optimal sequence of posed basic mathematical problems. In Proceedings of the 9th European Conference on Technology Enhanced Learning,  EC-TEL 2014, pages 592593. Springer International Publishing, September 2014.  [21] B. Taraghi, A. Saranti, M. Ebner, V. Muller, and A. Groman. Towards a learning-aware application guided by hierarchical classification of learner profiles. Journal of Universal Computer Science, 21(1):93109, January 2015.  [22] B. Taraghi, A. Saranti, M. Ebner, and M. Schon. Markov chain and classification of diculty levels enhances the learning path in one digit multiplication. In Learning and Collaboration Technologies. Designing and Developing Novel Learning Experiences, pages 322333. Springer International Publishing, June 2014.  [23] B. Taraghi, A. Saranti, M. Ebner, and M. Schon. On using markov chain to evidence the learning structures and diculty levels of one digit multiplication. In Proceedings of the Fourth International Conference on  Learning Analytics And Knowledge, pages 6872, March 2014.  [24] S. H. van der Ven, M. Straatemeier, B. R. Jansen, S. Klinkenberg, and H. L. van der Maas. Learning multiplication: An integrated analysis of the multiplication ability of primary school children and the diculty of single digit and multidigit multiplication problems. Learning and Individual Dierences, 43:4862, October 2015.  [25] J.-D. Zapata-Rivera and J. E. Greer. Interacting with inspectable bayesian student models. International Journal of Artificial Intelligence in Education, 14(2):127163, January 2004.    "}
{"index":{"_id":"56"}}
{"datatype":"inproceedings","key":"Wang:2016:EER:2883851.2883910","author":"Wang, Yan and Ostrow, Korinn and Beck, Joseph and Heffernan, Neil","title":"Enhancing the Efficiency and Reliability of Group Differentiation Through Partial Credit","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"454--458","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883910","doi":"10.1145/2883851.2883910","acmid":"2883910","publisher":"ACM","address":"New York, NY, USA","keywords":"data mining, group differentiation, partial credit, randomized controlled trial, resampling","abstract":"The focus of the learning analytics community bridges the gap between controlled educational research and data mining. Online learning platforms can be used to conduct randomized controlled trials to assist in the development of interventions that increase learning gains; datasets from such research can act as a treasure trove for inquisitive data miners. The present work employs a data mining approach on randomized controlled trial data from ASSISTments, a popular online learning platform, to assess the benefits of incorporating additional student performance data when attempting to differentiate between two user groups. Through a resampling technique, we show that partial credit, defined as an algorithmic combination of binary correctness, hint usage, and attempt count, can benefit assessment and group differentiation. Partial credit reduces sample sizes required to reliably differentiate between groups that are known to differ by 58%, and reduces sample sizes required to reliably differentiate between less distinct groups by 9%.","pdf":"Enhancing the Efficiency and Reliability of Group  Differentiation through Partial Credit     Yan Wang, Korinn Ostrow, Joseph Beck, Neil Heffernan   Worcester Polytechnic Institute  Worcester, MA 01609   {ywang14, ksostrow, josephbeck, nth} @wpi.edu       ABSTRACT  The focus of the learning analytics community bridges the gap  between controlled educational research and data mining. Online  learning platforms can be used to conduct randomized controlled  trials to assist in the development of interventions that increase  learning gains; datasets from such research can act as a treasure  trove for inquisitive data miners. The present work employs a data  mining approach on randomized controlled trial data from  ASSISTments, a popular online learning platform, to assess the  benefits of incorporating additional student performance data  when attempting to differentiate between two user groups.  Through a resampling technique, we show that partial credit,  defined as an algorithmic combination of binary correctness, hint  usage, and attempt count, can benefit assessment and group  differentiation. Partial credit reduces sample sizes required to  reliably differentiate between groups that are known to differ by  58%, and reduces sample sizes required to reliably differentiate  between less distinct groups by 9%.     Categories and Subject Descriptors  K: Applications to Education. K.3: Computers and Education. I.6  Simulation and Modeling.   General Terms  Measurement, Experimentation, Reliability.   Keywords  Partial Credit, Group Differentiation, Resampling, Randomized  Controlled Trial, Data Mining.   1. INTRODUCTION  The learning analytics and educational data mining communities  have established a variety of well-vetted models to predict student  knowledge and trace performance both within and across  knowledge components (i.e., skills). The gold standard for student  modeling, Knowledge Tracing (KT), has maintained its reign for  almost a quarter-century despite relying on a rudimentary  sequence of correct and incorrect responses to estimate the  probability of student knowledge [2]. Attempts to enrich this  approach have included supplemental estimates of prior  knowledge to individualize predictions to each student [9],   supplemental estimates of item difficulty to individualize to each  problem [10], and the implementation of flexible correctness via  consideration of hint usage and attempt count [12, 13, 7]. Despite  these excursions, popular learning systems, including the  Cognitive Tutor series, still largely rely on traditional KT to  inform mastery learning [4].   In parallel, enthusiastic support has been growing for the use of  randomized controlled trials embedded within online learning  platforms to investigate best practices and enhance the user  experience. Randomized controlled trials are the soundest  approach to social science, allowing researchers to postulate  causal relationships between independent and dependent  variables. Within the realm of education, experimental design has  historically been longitudinal, with formal pre- and post-tests,  highly controlled curricula, and vast sample populations required  for class-level or even school-level randomization. However, the  expanding popularity of online learning platforms used for  classwork and homework offers researchers an opportunity to  gather data more efficiently, with fewer logistic constraints, and  requiring smaller samples due to random assignment at the  student-level.   The present work employs data mining methodologies on  randomized controlled trial data from ASSISTments, a popular  online learning platform, to assess the benefits of incorporating  additional student performance data when attempting to  differentiate between two user groups. The platform, created in  2002, now supports over 50,000 users around the world, providing  students with immediate feedback and enhancing assessment for  teachers [3]. The ASSISTments platform is an easily accessible  shared tool for educational research that offers the unique  opportunity to bridge the gap between the analysis of randomized  controlled trials and more traditional data mining. Considering  student performance variables for the purpose of group  differentiation is arguably a worthy venture for both realms.   Many learning platforms assess student performance using  standard binary correctness (i.e., a students accuracy on her first  solution attempt). Instead, we argue for a combination of features  that better define the learning process: initial accuracy, feedback  usage, and attempts required for success. The present work  suggests that such features can be combined to establish a partial  credit metric to enhance analytic efficiency when attempting to  differentiate between two user groups (i.e., experimental  conditions). It is not surprising that a more robust view of student  performance can alter a researchers ability to pinpoint the  effectiveness of an intervention.  Modeling numerous features per  data point requires fewer data points to arrive at distinct  conclusions (i.e., posttests could simultaneously be shortened and  yet made more robust for both students and researchers). Previous   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than ACM must be honored. Abstracting with  credit is permitted. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom     2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00    DOI: http://dx.doi.org/10.1145/2883851.2883910    mailto:Permissions@acm.org http://dx.doi.org/10.1145/2883851.2883910   work has also suggested that infusing controlled assessment with  learning opportunities (i.e., providing feedback or allowing  multiple attempts) directly benefits robust student learning [1].  However, many researchers hesitate when considering the  allowance of these features within posttests. As such, the present  work seeks to validate the allowance of partial credit within  randomized controlled trial posttests.   Although ASSISTments employs binary scoring, feedback usage  and attempts required for success can be considered in the  algorithmic calculation of partial credit scores. Recent research  within ASSISTments has examined the potential benefits of  partial credit scoring for student modeling [7] and has validated  partial credit penalizations using an extensive grid search of  possible scoring procedures [6]. We extend this work by asking:  Does partial credit scoring enhance the efficiency with which  significant differences can be detected between groups of students  within a randomized controlled trial We define enhanced  efficiency as a reduction in the sample size required to reliably  observe significant differences between groups (akin to enhancing  power, or reducing Type II error).   2. DATASET  The dataset is comprised of log files from a previously published  randomized controlled trial on the effects of interleaving skill  content within a brief homework assignment [8]. The original  study was conducted with a group of participating teachers from a  suburban middle school in Massachusetts. Researchers worked  with teachers to select content for three skills (A, B, C). A  practice session comprised of twelve questions (four per skill) was  presented to students in one of two possible linear presentations:  blocked or interleaved. Students randomly assigned to the blocked  condition received questions grouped by skill (A1, A2, A3, A4, B1,  B2, B3, B4, C1, C2, C3, C4), while those randomly assigned to the  interleaved condition received the same questions in a mixed skill  pattern (A1, A2, B1, B2, C1, C2, A3, B3, C3, B4, C4, A4). All  students partook in a follow-up assignment containing three  questions (A5, B5, C5) as a delayed posttest. The posttest was  presented with tutoring in the form of on-demand hint messages  and students were allowed multiple attempts to achieve accuracy.   The original work presented an Analysis of Covariance  (ANCOVA) on the average posttest performance of 146 students  (n Blocked = 60, n Interleaved = 86) based on binary scoring.  Results only trended toward significance across the full sample,  but split file analyses revealed significant learning gains for low  skill students who had received the interleaved assignment. In a  parallel analysis, average hint usage and attempt counts at posttest  were considered through a Multivariate Analysis of Covariance  (MANCOVA), with results suggesting a significant multivariate  effect driven by a reduction in posttest hint usage for students in  the interleaved condition. These results inspired the present work.  Binary scoring alone could not consistently allow for reliable  group differentiation until controlling for student skill level.   Additionally, robust value was added via consideration of posttest  variables that define partial credit in the present work. How would  results have differed if the authors of the original work had  considered algorithmic partial credit scoring   3. METHODOLOGY  To examine the potential for using partial credit as a metric to  more efficiently differentiate between groups, the dataset was  processed using a definition of partial credit scoring previously  validated within ASSISTments. Past research on modeling student  performance within ASSISTments has revealed that certain  definitions of partial credit significantly outperform others when  attempting to predict next problem performance [6]. The  algorithm presented in Figure 1, originally defined in [7], has been  proven as an effective definition in the context of modeling  student performance [7]. This algorithm establishes a score  categorization based on logged information regarding the  students performance: the number of attempts required to reach  an accurate response (attempt), the number of hints requested  (hint_count), and whether or not the student was provided the  answer through the bottom out hint (bottom_hint). A version of  this algorithm was recently implemented within the ASSISTments  platform.   After passing the dataset through the algorithm presented in  Figure 1, the resulting file contained categorical partial credit  scores (0, 0.3, 0.6, 0.7, 0.8, 1.0) for each students performance on  each problem in the practice and posttest sessions.  Students could  still earn full credit in the traditional sense (i.e., answering  correctly on the first attempt), but only lost full credit if they made  more than five attempts or were provided the answer through the  bottom out hint. An example of the processed data, with variables  from the original file as well as the resulting penalizations and  partial credit scores, is presented in Table 1. The processed dataset  has been stripped of student identifiers and is available at [11] for  reference.   When considering user groups, this dataset offered two clear  opportunities for group differentiation: experimental condition  and discretized student performance level. The latter metric  defines students as either high performing or low performing     IF attempt = 1 AND correct = 1 AND hint_count = 0         THEN 1  ELSIF attempt < 3 AND hint_count = 0        THEN .8  ELSIF (attempt <= 3 AND hint_count=0)  OR (hint_count = 1 AND bottom_hint != 1)        THEN .7  ELSIF (attempt < 5 AND bottom_hint != 1)  OR (hint_count > 1 AND bottom_hint != 1)        THEN .3  ELSE 0   Figure 1: Partial credit algorithm originally defined in [7].    Table 1: Randomized controlled trial data with partial credit algorithm employed  Student Condition Problem Binary Hints Bottom Out Attempts Penalization Partial Credit Score  Student 1 Interleaved A1 0 1 0 2 0.3 0.7  Student 1 Interleaved B1 0 0 0 2 0.2 0.8  Student 1 Interleaved C1 1 0 0 1 0.0 1.0  Student 2 Blocked A1 0 3 1 3 1.0 0.0  Student 2 Blocked A2 0 0 0 3 0.3 0.7  Student 2 Blocked A3 0 1 0 4 0.7 0.3       Figure 2: The resampling process used to create samples of n students from each population. Each set of samples was used in a    t-test and significance values were recorded. This process was repeated 5,000 times for each group of n students.     based on a measure of prior knowledge calculated using the  ASSISTments database. Prior knowledge is established by  considering the average accuracy (in the binary sense) of all  problems that a student has ever solved within ASSISTments. A  median split can then be applied to this metric within a dataset to  discretize groups of generally high performing and generally  low performing students. In previous research, these groups  have been found to exhibit significantly different performance,  with low performing students logging reliably lower accuracy,  more hints, and more attempts [8]. Thus, while observing  differentiation between experimental conditions is subject to the  success of the intervention, grouping students by skill level offers  an obvious differentiation to test the efficacy of partial credit.    The full sample (146 students) was used to test differentiation  between student performance levels. Equivalent samples of  students were randomly selected from each performance level in  single student increments (i.e., 5 students, 6 students, 7 students,  etc.) For each set of equivalent samples of size n, an independent  samples t-test was performed to compare the difference in partial  credit scores between Sample 1 (a subset, n, of high performing  students) and Sample 2 (a subset, n, of low performing students).  A p-value denoting level of significance was recorded. This  process was repeated to examine differences between Sample 1  and Sample 2 when considering binary scoring. These trials  were repeated 5,000 times per sampling increment. This process is  depicted visually in Figure 2. For both partial and binary credit,  sets of resulting p-values were then analyzed to determine the  percentage of trials in which significant differences were observed  between samples (p < .05). Findings were graphed for a visual  comparison of the two scoring methods. Analyses and mappings  were conducted using MATLAB [5] via code available for further  consideration at [11].    This procedure was also used to differentiate between students  based on experimental condition: blocked or interleaved. As the  original work suggested that experimental condition only  significantly altered achievement in low performing students, the  present analysis considers only this subset of the original sample.  Resampling with replacement was then used to establish artificial  groups as large as desired. Please note that resampling is not  employed in the present work to draw conclusions regarding the  strength of a particular subsample or condition. The sole purpose  of our analysis is to show that partial credit scoring can be used to  reduce the sample sizes required to reliably differentiate between  groups.    4. RESULTS  Results suggest that partial credit is exceptionally efficient in  differentiating between distinct groups. Table 2 presents the  differences in average correctness, hint usage, and attempt count  observed when students are discretized into high and low  performance levels - two groups that we know to be quite  discernible and are therefore used here to validate our approach.  Figure 3 depicts the percentage of samples in which significant  differences were observed between these two groups.  As these  groups show obvious distinctions, both binary and partial credit  scoring allow for 100% reliability of group differentiation with  samples of fewer than 60 students. However, it should be noted  that partial credit (red/dashed line) requires consistently smaller  samples and attains reliability far more efficiently than binary  scoring (blue/solid line). The resampling procedure suggested that    Table 2: Means and SDs for average correctness, hints, and  attempts across performance levels   Group Correctness Hints Attempts  Low Performing 0.54 (0.28) 0.72 (0.69) 2.05 (1.11)  High Performing 0.75 (0.22) 0.08 (0.21) 1.40 (0.43)        Figure 3: Significant differentiation in Performance Levels  using Binary Scoring and Partial Credit Scoring. In groups  with a known significant difference, differentiation is more   efficient using partial credit. Sample size required for  significant differentiation in 90% of trials is reduced by 58%.   0 10 20 30 40 50 60 20  30  40  50  60  70  80  90  100  Number of students  P er  ce nt   o f e  xp er  im en  ts  w  ith  p   v al  ue  <   0 .0  5  High performance VS Low performance        Partial credit Binary credit  (13,90%) (31,90%)      Figure 4: Significant differentiation in Condition using Binary   Scoring and Partial Credit Scoring. In groups with a less  substantial difference, differentiation is still almost always  more efficient using partial credit. Sample size required for  significant differentiation in 90% of trials is reduced by 9%.      Table 3: Means and SDs for average correctness, hints, and  attempts across conditions for low performing students   Condition Correctness Hints Attempts  Blocked 0.48 (0.25) 0.89 (0.67) 1.98 (0.58)   Interleaved 0.56 (0.29) 0.62 (0.67) 2.16 (1.37)   when using partial credit, equivalent groups of 13 students offer  enough power to observe significant differences between  performance levels in 90% of trials, while equivalent groups of 31  students were required when using binary scoring. Thus, within  this context, using partial credit allowed sample sizes to be  reduced by 58% while still obtaining the same result.   Although significant differences between experimental conditions  within low performing students were more difficult to discern, as  limited by the strength of the intervention, partial credit continued  to offer more robust group differentiation when considering these  user groups, as depicted in Figure 4. An analysis of means for the  variables that combine to form partial credit revealed that low  performing students in the interleaved condition were more  accurate on average at posttest with fewer hints, as displayed in  Table 3. Resampling suggested that when using partial credit,  equivalent groups of 175 students offer enough power to observe  significant differences between performance levels in 90% of  trials, while equivalent groups of 192 students were required when  using binary scoring. Thus, within this context, using partial credit  allowed sample sizes to be reduced by 9% while obtaining the  same result.    5. METHOD VALIDATION  When smaller equivalent sample sizes are required to differentiate  between groups, Type II error is reduced for consistent sample  sizes across scoring metrics. Before celebrating this finding, it is  necessary to evaluate whether partial credit scoring in turn  increases Type I error.    If no actual difference exists between two groups and we maintain  a threshold of p < .05 in determining a significant difference, the  Type I error rate, or alpha, should be 5%. In order to determine  whether   partial  credit  has   reduced  Type  II   error  simply   by     Figure 5: Type I error when resampling students from a   solitary population using Binary Scoring and Partial Credit  Scoring. Measures show roughly similar trends, suggesting   that while partial credit allows for more robust group  differentiation, it does not significantly impact Type I error.   increasing Type I error, we simulated a null experiment with our  dataset.  The full sample population (146 students) was subjected  to the resampling (with replacement) process, without predefining  students as having high or low performance or as belonging to a  particular experimental condition. Thus, for every sample  increment, n, Sample 1 and Sample 2 were randomly selected  from the full population (establishing samples that were not  distinctly different). An independent samples t-test was conducted  to analyze the difference in partial credit scores between  subsamples. This trial was repeated 5,000 times, with p-values  recorded for each trial. Complimentary trials were conducted  using binary correctness. The percentage of trials resulting in  significantly different subsamples is charted in Figure 5. Both  measures show roughly similar trends, with approximately 5% of  trials resulting in significant findings. This finding suggests that  while partial credit allows for more robust group differentiation, it  does not significantly influence Type I error.   6. DISCUSSION & FUTURE WORK  The present work sought to examine whether partial credit scoring  could be used to enhance the efficiency of group differentiation  within a previously published randomized controlled trial. Results  confirmed our expectations, suggesting that partial credit is a  more robust measure of student performance that increases the  reliability of group differentiation and reduces the sample size  required to observe significant differences (or, enhances power).    Partial credit scoring held merit for differentiating both between  student performance levels and between experimental conditions.  The lack of strength in the latter finding may be correlated with  the efficacy of the intervention itself; differentiation based on a  learning intervention should not be expected to be as robust as  differentiation based on a mathematically established  dichotomy.  Still, trends in reliability for both scoring metrics  follow the standards of a power analysis: if sample sizes in the  original work had been larger, the intervention would have proven  reliably significant.   It should be noted that while we observed consistent positive  effects for partial credit, it is mathematically possible for the  metric to underperform binary scoring. When using t-test   0 50 100 150 200 250 0  10  20  30  40  50  60  70  80  90  100  Number of students  P er  ce nt   o f e  xp er  im en  ts  w  ith  p   v al  ue  <   0 .0  5 Interleaving VS Blocking--Low performing        Partial credit Binary credit  (175, 90%)  (192, 90%)  0 50 100 150 200 250 4.2  4.4  4.6  4.8  5  5.2  5.4  5.6  5.8  6  Number of students  P er  ce nt   o f e  xp er  im en  ts  w  ith  p   v al  ue  <   0 .0  5  Differentiation of students in one group       Partial credit Binary credit    comparisons, smaller p-values are obtained as t-statistics increase.  T-statistics are inflated when mean differences between groups  are large while variance within groups is low. Mathematically, the  use of partial credit reduces within group variance while  increasing the mean for each group. With this increase in means,  it would be possible for binary scoring to outperform partial credit  in a heavily skewed dataset.    A potential limitation of this approach can be found in the balance  between enhancing group differentiation by adding measures of  student performance and overfitting student performance. One  could argue that to most efficiently differentiate between groups,  all available student data could be collapsed into a partial credit  metric, perhaps using a regression model. While this would likely  result in better differentiation, the overly robust definition of  partial credit would fail to generalize to other online learning  platforms, or possibly even to other content or user populations  within the ASSISTments platform.  Future work should consider  the pros and cons of supplementing partial credit scoring with  additional measures of student performance.   Another potential limitation of this work is that students habits  within the ASSISTments tutor are normative to those of a binary  system; the majority of students understand that they will lose all  credit if they request tutoring feedback or make more than one  attempt.  Thus, any definition of partial credit that uses a data  mining approach to work backwards toward group differentiation  should be considered potentially skewed. As partial credit was  recently implemented within ASSISTments, future work should  consider how the real-time effects of partial credit scoring impact  the power of randomized controlled trials.    Future research should also consider how our partial credit  approach contends with latent group differentiation, in an attempt  to outperform modeling techniques like Knowledge  Tracing.  Even if latent, when two groups are qualitatively  different (i.e., learned vs. unlearned, denoting skill mastery within  KT) our method may be feasible to observe patterns leading to  more reliable group differentiation. Future work should examine  this paradigm, and consider the generalizability of using partial  credit scoring within the context of other platforms and domains.   7. CONTRIBUTION  The work presented herein is novel in that it sought to bridge the  gap between educational research and data mining by applying  post hoc mining methods to the results of a previously published  randomized controlled trial. Results suggested a substantial  benefit of considering partial credit scoring within online learning  platforms: increased efficiency in group differentiation which  translates to increased power and reduced Type II error. Our  findings further confirm the notion that allowing students to learn  during assessment is beneficial to students and researchers alike.  Student performance metrics that are typically lost on traditional  posttests can actually improve data analysis. Further, our results  suggest that by using robust measures of student performance, the  number of items or opportunities analyzed need not be large to  result in significant group differentiation, offering evidence for  short, minimally invasive assessments. These findings translate to  real world implications: significant outcomes can be observed  with smaller samples and with fewer overall data points, reducing  the many of the costs and constraints of experimental research.   8. ACKNOWLEDGMENTS  We acknowledge funding from multiple NSF grants (ACI- 1440753, DRL-1252297, DRL-1109483, DRL-1316736, DRL- 1031398), the U.S. Dept. of Ed. (IES R305A120125,  R305C100024, GAANN), ONR, and the Gates Foundation.   9. REFERENCES  [1] Attali, Y. & Powers, D. (2010). Immediate feedback and   opportunity to revise answers to open-end questions.  Educational and Psychological Measures, 70 (1), 22-35.   [2] Corbett, A.T., Anderson, J.R. (1995). Knowledge Tracing:  Modeling the Acquisition of Procedural Knowledge. User  Modeling and User-Adapted Interaction, 4, 253-278.   [3] Heffernan, N. & Heffernan, C. (2014). The ASSISTments  Ecosystem: Building a Platform that Brings Scientists and  Teachers Together for Minimally Invasive Research on  Human Learning and Teaching. Int J of AIED, 24(4), 470- 497.   [4] Koedinger, K.R. & Corbett, A.T. (2006). Cognitive tutors:  Technology bringing learning science to the classroom.  In  K. Sawyer (Ed.), The Cambridge handbook of the learning  sciences (61-78). New York: Cambridge University Press.   [5] MATLAB version R.2013.a (2013). Natick, Massachusetts:  MathWorks, Inc. Accessible at www.mathworks.com   [6] Ostrow, K., Donnelly, C., & Heffernan, N. (2015).  Optimizing Partial Credit Algorithms to Predict Student  Performance. In Santos, et al. (eds.) Proc of the 8th Int Conf  on EDM, 404-407.    [7] Ostrow, K., Donnelly, C., Adjei, S., & Heffernan, N. (2015).  Improving Student Modeling Through Partial Credit and  Problem Difficulty.  In Russell, D.M., Woolf, B., & Kiczales,  G. (eds.) Proc of the 2nd ACM Conf on L@S, 11-20.    [8] Ostrow, K., Heffernan, N., Heffernan, C., Peterson, Z.  (2015). Blocking vs. Interleaving: Examining Single-Session  Effects within Middle School Math Homework. In Conati,  Heffernan, Mitrovic, & Verdejo (eds.) Proc of the 17th Int  Conf on AIED. Springer International, 388-347.    [9] Pardos, Z.A. & Heffernan, N.T. (2010). Modeling  Individualization in a Bayesian Networks Implementation of  Knowledge Tracing. In De Bra, Kobsa, & Chin (eds.) Proc of  the 18th Int Conf on UMAP, 255-266.   [10] Pardos, Z.A., & Heffernan, N.T. (2011). KT-IDEM:  Introducing Item Difficulty to the Knowledge Tracing  Model. In Joseph A. Konstan et al. (Eds.), Proc of the 19th Int  Conf on UMAP, 243-254.   [11] Wang, Y. (2015). Data and Code for Enhancing the  Efficiency and Reliability of Group Differentiation through  Partial Credit: http://tiny.cc/LAK2016-Resampling   [12] Wang, Y. & Heffernan, N.T. (2011). The Assistance  Model: Leveraging How Many Hints and Attempts a Student  Needs. In Proc of the 24th Int FLAIRS Conf.   [13] Wang, Y. & Heffernan, N. (2013). Extending Knowledge  Tracing to Allow Partial Credit: Using Continuous versus  Binary Nodes. In K. Yacef et al. (Eds.) AIED 2013, LNAI  7926, 181-188.     http://www.mathworks.com/   1. INTRODUCTION  2. DATASET  3. METHODOLOGY  4. RESULTS  5. METHOD VALIDATION  6. DISCUSSION & FUTURE WORK  7. CONTRIBUTION  8. ACKNOWLEDGMENTS  9. REFERENCES   "}
{"index":{"_id":"57"}}
{"datatype":"inproceedings","key":"Adjei:2016:PSP:2883851.2883867","author":"Adjei, Seth A. and Botelho, Anthony F. and Heffernan, Neil T.","title":"Predicting Student Performance on Post-requisite Skills Using Prerequisite Skill Data: An Alternative Method for Refining Prerequisite Skill Structures","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"469--473","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883867","doi":"10.1145/2883851.2883867","acmid":"2883867","publisher":"ACM","address":"New York, NY, USA","keywords":"learning maps, placements, prerequisite structures, refinements, skill relationships","abstract":"Prerequisite skill structures have been closely studied in past years leading to many data-intensive methods aimed at refining such structures. While many of these proposed methods have yielded success, defining and refining hierarchies of skill relationships are often difficult tasks. The relationship between skills in a graph could either be causal, therefore, a prerequisite relationship (skill A must be learned before skill B). The relationship may be non-causal, in which case the ordering of skills does not matter and may indicate that both skills are prerequisites of another skill. In this study, we propose a simple, effective method of determining the strength of pre-to-post-requisite skill relationships. We then compare our results with a teacher-level survey about the strength of the relationships of the observed skills and find that the survey results largely confirm our findings in the data-driven approach.","pdf":"Predicting Student Performance on Post-requisite Skills  Using Prerequisite Skill Data: An Alternative Method for   Refining Prerequisite Skill Structures     Seth A. Adjei  Worcester Polytechnic Institute   100 Institute Road  Worcester, MA 01609-2280   saadjei@wpi.edu    Anthony F. Botelho   Worcester Polytechnic Institute  100 Institute Road   Worcester, MA 01609-2280  abotelho@wpi.edu     Neil T. Heffernan   Worcester Polytechnic Institute  100 Institute Road   Worcester, MA 01609-2280  nth@wpi.edu     ABSTRACT  Prerequisite skill structures have been closely studied in past years  leading to many data-intensive methods aimed at refining such  structures. While many of these proposed methods have yielded  success, defining and refining hierarchies of skill relationships are  often difficult tasks.  The relationship between skills in a graph  could either be causal, therefore, a prerequisite relationship (skill  A must be learned before skill B). The relationship may be non- causal, in which case the ordering of skills does not matter and  may indicate that both skills are prerequisites of another skill. In  this study, we propose a simple, effective method of determining  the strength of pre-to-post-requisite skill relationships. We then  compare our results with a teacher-level survey about the strength  of the relationships of the observed skills and find that the survey  results largely confirm our findings in the data-driven approach.   Categories and Subject Descriptors   Mathematics of computing  Regression analysis;  Applied  computing  Computer-assisted instruction;  Mathematics of  computing  Causal networks      Keywords  Prerequisite Structures, learning maps, skill relationships,  refinements, PLACEments   1. INTRODUCTION  Prerequisite skill structures represent the ordering of skills in a  given knowledge domain. The learning sequences represented in  prerequisite skill structures have become an area of interest over  the past few years. As a prelude to the objective of learning  prerequisite skill structures from data, Tatsuoka [12] developed  and proposed the Q-Matrix, a structure that represents the  mapping of items on a test to specific skills. Others have built on  this structure to find relationships between the skills and items  represented in the Q-matrices [3,11] or proposed methods for  refining Q-Matrices [7]. Brunskel presented preliminary work in   which she used students noisy data to infer prerequisite structures  [4]. Additionally, Scheines, et al. [11] present an extension of a  causal structure discovery algorithm in which the assumption of  pure items are relaxed to reflect real data, and use that relaxed  assumption to infer prerequisite skill graphs from students  response data.    The focus of other researchers in the community has been on  refining the prerequisite structures developed either by domain  experts or through data mining approaches, as used by Barnes [3].  Cen, et al. [5] proposed Learning Factors Analysis (LFA) as a  method for refining cognitive models. Their approach includes  statistical techniques, human expertise, and combinatorial search  to refine cognitive models. Following the proposals made by Cen  et al. in [5], Adjei et al. [1] developed a combinatorial search  algorithm based on LFA and found simplified prerequisite  structures, which have equally good predictive power as the  originals.    Desmarais, et al. [6] introduced a method for determining partially  ordered knowledge structures (POKS) from student data. The  main idea behind this approach is to compare pairs of items in a  test in order to determine any interactions existing between each  pair. The interactions serve as a basis for determining the  relationship between the skills represented by the items. Pavlik  and his colleagues applied POKS to analyze item-type covariances  and proposed a hierarchical agglomerative clustering method to  refine the tagging of items to skills [9] and later proposed  Learning Factors Transfer Analysis [10] as a means for generating  domain models. Adjei and Heffernan [2] used randomized control  experiments to identify links within prerequisite skill structures  that require further scrutiny. All of this effort that has been  expended in the quest to find skill structures from data have  yielded varied degrees of success.   The desire to find the best representation of skills (i.e., the  prerequisite skill structure) is important for a number of reasons. It  informs domain experts about the optimal sequencing of  instruction in order to achieve the best tutoring for students.  Additionally, this should help researchers in the education  research community to better model students knowledge and  performance in intelligent tutoring systems more accurately. Such  strategies and models can benefit students understanding of new  skills by supplying them with the optimal foundations for the  material.  Likewise, better student models can lead to improved  intervention design for those students requiring further aid.   This current study proposes a simple method for identifying  problematic links in a prerequisite skill structure, pointing domain  experts to the ordering of instructions that may be creating   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than ACM must be honored. Abstracting with  credit is permitted. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires prior specific permission and/or a fee. Request  permissions from Permissions@acm.org.    LAK '16, April 25-29, 2016, Edinburgh, United Kingdom     2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00    DOI: http://dx.doi.org/10.1145/2883851.2883867     mailto:Permissions@acm.org   problems for students. In this study we use linear regression of  students performance on items presented to students in the order  of a given prerequisite skill structure and make suggestions about  the strength of the relationships between the skills.    This paper starts out by describing PLACEments, the adaptive  testing system from which data was collected for use in this study.  This is followed by a description of the methods we employed and  the results of the studies. We then present the results of a teacher  survey that we conducted and compare the results of the survey  with the findings of our data mining. The paper concludes with a  discussion of the results and possible future work in this area.   2. INTRODUCTION TO PLACEments  PLACEments, a free mathematics adaptive testing system, is a  feature of ASSISTments (a free web-based Intelligent Tutoring  System (ITS)). When assigning a PLACEments test, an initial set  of skills are selected for the test. Students are tested on the initial  set of skills and depending on their performance, the system  traverses a skill graph to present problems from the prerequisite  skills of the initial set of skills. The test adapts to the students  performance as well as the underlying prerequisite skill graph. If a  student performs poorly on an item in the test, they are presented  with items from the prerequisite skills required to solve the  original problem. PLACEments uses a prerequisite skill structure  created by one of the experts who developed the Common Core  Standard for mathematics. Portions of this structure are currently  being used by websites like AchieveTheCor.org  [http://www.achievethecore.org/coherence-map/]. The developers  of the site call it the Coherence Map.   PLACEments has an additional feature that assigns remediation  assignments to students who perform poorly on a test. These  remediation assignments are intended to build the students  understanding of the skills they performed poorly on, during the  test. The remediation assignments are released in the order of the  arrangement of skills in the prerequisite skill structure. Students  are assigned lower grade level prerequisite skills first, and until  they complete those remediation assignments, post-requisite skills- related remediation assignments are not released. This ensures that  the students gradually build on their knowledge of skills until they  eventually reach a desired level of mastery of the skills in the  given domain.   To illustrate how PLACEments works, Figure 1 shows a  hypothetical prerequisite skill graph where the letters A through H  each represents a skill. The graph additionally shows a typical  configuration of a students navigation through the prerequisite  skill structure in the process of taking a PLACEments test. Skills  A, B and C are the initial skills assigned on the test.      Figure 1: A Typical Students navigation in PLACEments   In this case the student performed poorly on skill A and so is  asked questions for skills D and E. Since the student could not  demonstrate understanding on skill E, he is further asked  questions from skill H, which he performs poorly on as well.   PLACEments creates remediation assignments for each of the  skills the student performs poorly on (A, E and H). For this  particular example, the remediation assignment for skill H is  released before any other remediation assignments are released.  The assignment for skill E is released after the student completes  that previous skills assignment.   For the purpose of this study, we focus only on the remediation  assignment management feature. This is the feature that provides  us with data for determining how strong prerequisite skill  relationships are. The remediation assignments are typically  assignments in which students practice a number of similarly  designed problems to help them master a particular common core  skill. In the course of the assignments, students are allowed to ask  for help (in the form of hints) as they progressively answer the  questions. The student is deemed to have mastered the skill if  he/she correctly answers n consecutive problems in the assignment  without asking for hints. The value of n typically ranges from  three to five depending on the designer of the problem set. If after  a set number of problems (typically called the daily limit), the  student is unable to reach the mastery criterion, the system pauses  the practice session until the next day when the student can  continue with the assignment.     3. METHODOLOGY  3.1 Dataset  The remediation assignment feature of PLACEments served as the  source of data for the current study. The dataset includes students  performance on remediation assignments. There were 495  prerequisite skill links from the prerequisite skill structure  described above. In this study we focused our attention only on  skills that have exactly one prerequisite skill, but it is important to  note that our approach is not inherently limited to such skills. Of  the 104 skills that have exactly one prerequisite skill, we had 24 of  the links that had data for a minimum of 50 students. For each of  the prerequisite skill links examined, there was an average of 120  students who were assigned remediation assignments of both the  prerequisite and post-requisite skills of the link.    Each row in the dataset has a students performance on the pre-  and post-requisite skills (measured by the percent correct of the  Skill Builder, and the number of items it took them to complete  the Skill Builder typically referred to as the students mastery  speeds) and the student's prior performance on all problems in  ASSISTments. The latter is to help us account for the students  knowledge level. The data set also includes the skill difficulty  values for both the pre- and post-requisite skill. These difficulty  values are the percent correct for all the items tagged with that  skill in ASSISTments. Table 1 shows a sample of the dataset that  was used for this study. Each row in the dataset represents a  student's performance on the remediation assignments related to a  given PLACEments test. If the student had a similar pair of  assignments in another placements test, that information was  ignored because we did not want to duplicate the data for a given  student. In all, the dataset had 5803 instances of students  performance on pre- and post-requisite skills, involving 1567  students who have completed placements tests.    3.2 Regression Models  We ran linear regression to predict students performance on the  post-requisite skill. To avoid bias caused by student performance  and differences in skill difficulty, we included each students prior     performance, mastery speed of the prerequisite skill, and the  difficulty of both the pre- and post-requisite skills into our models.  The dependent variable was the mastery speed of post-requisite  skills.   Table 1: Sample data set. 1   SID PsSk Pre Sk   Pos MS   Pre MS   StPr  Pre  Dif   Pos  Dif   23412 57 50 4 5 0.75 0.32 0.40   24321 87 50 3 5 0.86 0.58 0.67     The following equation illustrates the regression model learned  from the data for each of the links:    (1)  where i indicates the metric for the post-requisite skill and k  indicates the prerequisite for student j.  The term m represents  mastery speed,  represents the intercept, K represents the prior  knowledge, and d represents skill difficulty. , ,  and   represent the coefficients of the independent features in the  regression.    We considered a links model only when the model was found to  be statistically significant (p<0.05) with R-Square above 0.1. All  those models with R-Square values below 0.1 were considered to  be suggestive of non-existence of a believable link between the  two skills. For the models that met the above criterion, a  prerequisite relationship was considered to exist when there is a  positive standardized beta coefficient for the prerequisite skills  mastery speed (i.e., > 0) and is significant (p<0.01 in many  cases and p<0.05 in a few).    Since outliers in the dataset could skew the results, we used two  data transformation methods to minimize the effects of outliers in  the dataset. The first method was to winsorize the mastery speeds  in which all mastery speeds above 10 had their values set to 10.  Skill Builders in ASSISTments have this feature where a daily  limit of 10 is set to prevent students from banging their heads  when they are unable to master the skill within 10 opportunities.  This is the reason we chose 10 as the cut off number in order to  fairly account for student performance. More than 80% of the data  we used had mastery speeds below 10 so the impact of this  transformation was not very significant. The second data  transformation method we used was a log transform of the mastery  speeds. We then used each of the transformations to predict the  correspondingly transformed mastery speeds and present both  results in the results section.                                                                     1 The complete dataset can be found at   http://tiny.cc/mslinkstrength. SID is the unique student  identifier, PsSk is the post requisite skill id, PreSk has the  prerequisite skill id, PrMS and PosMS contain the students  mastery speed of the pre- and post-requisite skill respectively,  StPr is the students prior percent correct (an indication of the  student knowledge level), and PreDif and PosDiff is the  difficulty of the pre- and post-requisite skills. The column names  have been shortened for lack of space.   In the case of the transformed data, we replaced the raw mastery  speeds in the model with the transformed mastery speeds. We run  linear regression models similar to equation (1) above with the  mastery speeds,  and , respectively replaced with the  transformed data,  and . Equation (1) in this case  becomes:    .. (2)   3.3 Teacher Survey  To verify the results of our findings, we ran a survey of 45  randomly selected domain experts and teachers who use  ASSISTments and asked about their perceptions of the strength of  the 24 prerequisite skill relationships, including the 14 links we  studied in the regression study. A sample survey question is  shown in Figure 3. The survey had 26 different questions, the first  question introduced the survey and the last was complimentary.  There was a survey question for each of the 24 prerequisite skill  links. For each prerequisite skill link, we presented a sample  problem for each of the post- and pre-requisite skills and asked  teachers to rate, on a scale of 1 to 7 (1 not important; 7 extremely  important), how important it is for a student to know the  prerequisite skill to be able to answer the problem from the post- requisite skill. Even though the questions give the impression that  we are trying to figure out how related the skills are, we  intentionally did not use the terms pre- and post-requisite skills in  order not to confuse the respondents, or to point them in a  particular direction. A link is considered to exist if the mean of the  responses for that link was approximately 5 and a standard  deviation of less 1 or less. We then compare the results of the  survey with the findings of the study and report on the  comparison.    4. RESULTS  4.1 Regression   The results of the regression study are illustrated in Figure 2,  which shows that several of the links could be found to be  problematic and require further scrutiny. When the mastery speeds  were transformed to take care of the outliers, the models do a  better determination of the good links than was the case when the  raw speeds were used. The takeaway from this graph is that we do  a better job at finding both good and bad links in a prerequisite  structure (and thus refine the structure) when we transform the  mastery speeds. By transforming the data, we increase the good  links by about 10 percentage points, as shown in Figure 2.       Figure 2: Percentages of identified good and problematic links  based on mastery speed transformation methods   The graph in figure 4 shows that of the 24 prerequisite links, the  regression method identified 25% of the links (6 links) in which  the students performance on the prerequisite skills significantly  predicts their performance on the post-requisite skills irrespective  of the data transformation method used. 36% of the prerequisite     skills (in 8 links) are significant predictors (p-value < 0.05) of the  post-requisite skills if we used any two of the transformation   methods. This suggests that a prerequisite skill relationship truly  exists between the two skills in each of those links.         Figure 3: Sample Teacher Survey question      Figure 4: Agreement between mastery speed transformation  methods   4.2 Teacher Survey  We received responses from 21 of the 45 teachers invited to  respond to the survey, representing a response rate of 47%.  All  respondents completed the survey in its entirety, and the resulting  scores were averaged per link.  Those links found to have an  average score greater than or equal to 5 with a standard deviation  approximately equal to or less than 1 were viewed as exhibiting a  prerequisite relationship. This is concluded as we used a 7-point  scale, with those scores greater than 4 indicating at least some  importance for one skill to be presented before the other.  On the  basis of these criteria, the survey found 67% of the links (16 links)  are good, while the remaining 33% (8 links) are bad.         Figure 5: A comparison of model predictions with teacher   survey about link strength.  Using these values as ground truth, we compare the results to our  regression models. In Figure 5, we observe our models in terms of  precision, recall, and accuracy metrics against the ground truth  values.  An interesting find of those results shows that the   accuracy of the regression models was not influenced by the  transformation method used.  This could indicate that the  transformations alter the data in similar ways, or simply that there  were too few instances affected by the transformations to view an  effect.   Figure 6 illustrates each methods ability to identify correct links  when compared to the ground truth values.  We see that the  method is generally successful in identifying links.  This is the  case, even as the accuracy seen in Figure 6 has room for  improvement.      Figure 6: Strength of Methodology. This graph shows counts  of the good and bad links correctly identified by the method   grouped by data transformation method.   5. DISCUSSION OF RESULTS  Prerequisite skill structures in any knowledge domain are very  important for instruction and for preparing students for future  learning. Almost every knowledge domain has one or a couple,  which are created by domain experts. It is important to note that  several of these prerequisite skill structures need to be refined.  Data is currently being generated that affords us the opportunity  to use data-centered methods to refine these structures.   In this study we used data generated from PLACEments, which is  an adaptive testing feature of ASSISTments, to propose one  method for refining these structures. In this current study, we used  a simple linear regression method in which we use the student  performance on the prerequisite skill to predict their performance  on the post-requisite skill. We found that for some of the links,  this method was effective at identifying both good and bad links  in the structure. Comparing the results of the method with the  survey provided a ground truth with which to compare the  findings from the study. The results have shown that if we have     performance data, in the form of mastery speeds, we can achieve  more accurate results by transforming the dataset in some format  in order to take care of outliers that can easily ruin the findings.  [8] The methodology affords prerequisite skill structure creators  i.e., domain experts, the opportunity to identify and refine the  order of the skills in these structures.    It must however be noted though that the method was not perfect.  A few of the links could not be correctly identified. Additionally,  the criterion for determining whether a regression model is worth  examining is relatively low. In view of these, further studies are  required to ascertain the reasons behind that finding and to  propose refinements of the method. There could be interaction  effects and other relevant predictors that have been ignored, but  which may be necessary to ensure better accuracy for the  prediction models.    6. CONTRIBUTION  The main contribution of this study is the provision of a simple  and effective linear regression based method for refining  prerequisite skill structures. With this method, we are able to  identify problematic arcs in the structure and make these findings  available to domain experts who will then use this information to  further refine the maps.  Additionally, we have introduced a  system that provides us with a very good source of data for  refining prerequisite skill structures.   7. CONCLUSION  Several authors in the educational data mining and learning  analytics community have attempted to learn prerequisite  structures from students response data. Others have looked for  methods to refine already existing, domain-expert-made  prerequisite skill structures using methods like LFA, etc. It has so  far been difficult to get datasets that present students response  data in the order of their underlying students performance. This  paper uses such a dataset available from PLACEments, an  adaptive testing system that traverses a prerequisite skill structure  for item selection. The data from students performance on  remediation assignments was used to learn the strength of  prerequisite skill relationships existing between skills and to make  suggestions regarding these arcs.    We have shown that using simple linear regression and with the  right dataset, we can relatively tell how strong the prerequisite  skill relationship between two skills are and, based on that make  suggestions, regarding which links domain experts may need to  investigate and refine.   This study does have some limitations. The data used for this  analysis has come solely from PLACEments. There are not many  such adaptive testing systems that generate the kind of data we  used in this study. It will be interesting to study another dataset  that is generated in the same or similar format as PLACEments, in  order to apply this simple linear regression method to make  statements about the strength of prerequisite skill relationships.   We view this study as a preliminary step in our goal of finding  optimal prerequisite structures using PLACEments data.   8. ACKNOWLEDGEMENTS   We thank multiple NSF grants (ACI-1440753, DRL-1252297,  DRL-1109483, DRL-1316736, DGE-1535428 & DRL-1031398),  the US Dept. of Ed (IES R305A120125 & R305C100024 and  GAANN), and the ONR.   9. REFERENCES  [1] Adjei, S., Selent, D., Heffernan, N., Pardos, Z., Broaddus,   A., & Kingston, N. (2014). Refining Learning Maps with  Data Fitting Techniques: Searching for Better Fitting  Learning Maps. In J. Stamper, Z. Pardos, M. Mavrikis, & B.  M. McLaren (Eds.), Proceedings of the 7th International  Conference on Educational Data Mining (pp. 413 414).  Accessed on October 31, 2015, from  http://educationaldatamining.org/EDM2014/uploads/procs20 14/posters/89_EDM-2014-Poster.pdf     [2] Adjei, S. A., Heffernan, N. T (2015) Improving Learning  Maps Using an Adaptive Testing System: PLACEments. In  Conati, C., Heffernan, N., Mitrovic, A., Verdejo, M.F. (Eds.)  Proceedings of the 17th International Conference (pp. 517- 520), AIED 2015, Madrid, Spain, June 22-26, 2015.    [3] Barnes T. (2005). The Q-matrix Method: Mining Student  Response Data for Knowledge. In: Proceedings of AAAI  2005 Educational Data Mining Workshop.    [4] Brunskel, E. (2011). Estimating prerequisite structure from  noisy data In Proceedings of International Conference on  Educational Data Mining (EDM)   [5] Cen, H., Koedinger, K. R., & Junker, B. (2005). Learning  Factors Analysis: A general method for cognitive model  evaluation and improvement. In M. Ikeda, K. Ashley, & T.  Chan (Eds.), Intelligent Tutoring Systems 8th International  Conference (pp. 164175). Berlin: Springer.   [6] Desmarais, M., Maluf, A., and Liu, J. (1996). User-expertise  modeling with empirically derived probabilistic implication  networks. User Modeling and User-Adapted Interaction 5,  283315.    [7] Desmarais, M.C., Xu, P. and Beheshti, B. (2015). A partition  tree approach to combine techniques to refine item to skills  Q-matrices 8th Conference on Educational Data Data  Mining (EDM 2015), Madrid, Spain.   [8] Osborne, Jason W. & Amy Overbay (2004). The power of  outliers (and why researchers should always check for them).  Practical Assessment, Research & Evaluation, 9(6).  Retrieved from http://PAREonline.net/getvn.aspv=9&n=6   [9] Pavlik Jr., P.I., Cen, H., Wu, L., Koedinger, K.R.: Using  Item-type Performance Covariance to Improve the Skill  Model of an Existing Tutor. In: Baker, R.S., Beck, J.E.  (Eds.) Proceedings of the 1st International Conference on  Educational Data Mining, 2008. Montreal, Canada, p. 77-86    [10] Pavlik, P.I., Cen, H., Koedinger, K.R.: Learning factors  transfer analysis: using learning curve analysis to  automatically generate domain models. In: Barnes, T.,  Desmarais, M.C., Romero, C., Ventura, S. (eds.) 2nd  International Conference on Educational Data Mining-- EDM2009, pp. 121-130, Cordoba, Spain, 1-3 July 2009   [11] Scheines, R., Silver, E., Goldin, I.: Discovering prerequisite  relationships among knowledge components. In: Stamper, J.,  Pardos, Z., Mavrikis, M., McLaren, B. (eds.) Proceedings of  the 7th International Conference on Educational Data  Mining. pp. 355356. European Language Resources  Association (ELRA), May 2014   [12] Tatsuoka, K K. (1983). Rule space: An approach for dealing  with misconceptions based on item response theory. Journal  of Educational Measurement. 20(4) 345-354     1. INTRODUCTION  2. INTRODUCTION TO PLACEments  3. METHODOLOGY  3.1 Dataset  3.2 Regression Models  3.3 Teacher Survey   4. RESULTS  4.1 Regression  4.2 Teacher Survey   5. DISCUSSION OF RESULTS  6. CONTRIBUTION  7. CONCLUSION  8. ACKNOWLEDGEMENTS  9. REFERENCES   "}
{"index":{"_id":"58"}}
{"datatype":"inproceedings","key":"Pardo:2016:GAP:2883851.2883870","author":"Pardo, Abelardo and Mirriahi, Negin and Martinez-Maldonado, Roberto and Jovanovic, Jelena and Dawson, Shane and Gavsevi'c, Dragan","title":"Generating Actionable Predictive Models of Academic Performance","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"474--478","numpages":"5","url":"http://doi.acm.org/10.1145/2883851.2883870","doi":"10.1145/2883851.2883870","acmid":"2883870","publisher":"ACM","address":"New York, NY, USA","keywords":"feedback, learning analytics, personalization, recursive partitioning","abstract":"The pervasive collection of data has opened the possibility for educational institutions to use analytics methods to improve the quality of the student experience. However, the adoption of these methods faces multiple challenges particularly at the course level where instructors and students would derive the most benefit from the use of analytics and predictive models. The challenge lies in the knowledge gap between how the data is captured, processed and used to derive models of student behavior, and the subsequent interpretation and the decision to deploy pedagogical actions and interventions by instructors. Simply put, the provision of learning analytics alone has not necessarily led to changing teaching practices. In order to support pedagogical change and aid interpretation, this paper proposes a model that can enable instructors to readily identify subpopulations of students to provide specific support actions. The approach was applied to a first year course with a large number of students. The resulting model classifies students according to their predicted exam scores, based on indicators directly derived from the learning design.","pdf":"Generating Actionable Predictive Models   of Academic Performance   Abelardo Pardo  Faculty of Engineering and IT   The University of Sydney, Australia  Abelardo.Pardo@sydney.edu.au           Jelena Jovanovic  Department of Software Engineering   University of Belgrade, Serbia  jeljov@fon.rs   Negin Mirriahi  School of Education    and Learning & Teaching Unit  University of New South Wales,   Australia  Negin.Mirriahi@unsw.edu.au      Shane Dawson  Teaching Innovation Unit   University of South Australia, Australia  Shane.Dawson@unisa.edu.au   Roberto Martinez-Maldonado  Connected Intelligence Centre   University of Technology Sydney,  Australia   Roberto.Martinez- Maldonado@uts.edu.au      Dragan Gaevi   School of Education and Informatics   The University of Edinburgh, UK  dgasevic@acm.org     ABSTRACT  The pervasive collection of data has opened the possibility for  educational institutions to use analytics methods to improve the  quality of the student experience. However, the adoption of these  methods faces multiple challenges particularly at the course level  where instructors and students would derive the most benefit from  the use of analytics and predictive models. The challenge lies in  the knowledge gap between how the data is captured, processed  and used to derive models of student behavior, and the subsequent  interpretation and the decision to deploy pedagogical actions and  interventions by instructors. Simply put, the provision of learning  analytics alone has not necessarily led to changing teaching  practices. In order to support pedagogical change and aid  interpretation, this paper proposes a model that can enable  instructors to readily identify subpopulations of students to  provide specific support actions. The approach was applied to a  first year course with a large number of students. The resulting  model classifies students according to their predicted exam scores,  based on indicators directly derived from the learning design.   CCS Concepts    Applied computing  Education  Interactive learning  environments.   Keywords  Learning analytics; personalization; feedback; recursive  partitioning   1. INTRODUCTION  To date, research contributions in the field of learning analytics  have broadly aimed to further our understanding of the learning  process [21]. Central to this work has been the development of  predictive models of student learning behavior and performance  [1, 2, 6, 15, 19]. The models commonly aid in identifying  potential relationships among various behavioral, demographic,  and performance-based factors in large data sets. A well-adopted   example lies in the research associated with the early  identification of students at-risk of academic performance or  attrition [10]. While these types of predictive models have been  well utilized for developing early indicators, there has been  minimal attention investigating how such information can be best  deployed to promote reflection and action among teaching staff  and students. The data used to create such models requires  substantial pre-processing to comply with the requirements  imposed by the algorithms. Furthermore, once the predictions are  obtained, they usually require non-trivial interpretations. These  requirements increase the barrier for adoption by instructors and  students.    In order for learning analytics to have widespread impact and  uptake as a discipline, predictive models need to offer intuitive  actionable insight for both instructors and students, to overcome  some of the existing concerns reported in the literature. For  instance, there are claims that the uptake of learning analytics has  resulted in little improvements in the quality of student feedback,  and only increased the frequency of such feedback [22].  Furthermore, the design and deployment of interventions derived  from collected data has been relative unexplored [24]. This paper  proposes how a recursive partitioning technique can offer  instructors a predictive model to help them derive data-informed  pedagogical interventions and personalized feedback to different  student subpopulations.    The rest of the paper is organized as follows. Section 2 reviews  the work in the area of predictive algorithms and their potential to  drive interventions. Section 3 describes the method followed for  the recursive partitioning statistical analysis. The resulting models  are described in Section 4 together with some discussion about the  interpretation and performance of the models. Section 0 contains  the conclusions of the study and some ideas for future work.   2. RELATED WORK  There has been much research in the fields of learning analytics  and educational data mining dedicated to predictive modeling.  This has most commonly involved the development of models  associated with the prediction of students at risk of failing a  course and the prediction of students grades [8]. These  predictions are largely based on the use of data stored in  institutional and learning management systems. For example,  Agudo et al. [1] used classification algorithms to examine the  effect of three categories of interactions with academic  performance. Alstete and Beutell [2] examined student   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that open  copies are not made or distributed for profit or commercial advantage and  that copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than ACM must  be honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom    2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883870     performance in online education in management with respect to  several qualitative indicators, but did not elaborate on the actions  to be derived from these relations. Barber and Sharkey [6]  described one of the increasingly present models to predict when  students are at risk of abandoning the institution using a Nave  Bayes algorithm. The study aimed to detecting students at risk of  academic failure in order to provide timely and early support  interventions. A similar application was presented by Jayaprakash  et al. [15] in the context of an open source initiative across  multiple institutions including a portfolio of interventions that  may help address academic concerns regarding student retention.  In this case, the techniques were applied at an institutional level in  an attempt to reduce overall student attrition. To date, the work on  predictive modeling has demonstrated promising results with  respect to the level of prediction accuracy. Moreover, researchers  have also shown that the application of predictive models can lead  to significant educational gains and increased student retention  over an extended period of time [3].    Educational data mining [4] has specialized in the study of  algorithms to discover new relations or knowledge that are not  evident from large data sets. Although data mining algorithms are  being applied to numerous areas, the applications have mostly  focused on detecting adequate relationships rather than on the  implications stemming from the use of such algorithms or how to  make the resulting models available to instructors to derive  learning support actions (see for example [5, 16]). A  comprehensive description of how to use data mining techniques  was presented by Romero et al [19, 20]. However, the  interventions are only discussed as potential actions to be decided  by the instructor.    Timely and effective feedback has been identified as a key factor  to influence student learning [13, 14]. It has also been considered  as an inherent part of self-regulated learning as it affects cognitive  engagement with tasks [7]. Formative assessment provides  opportunities to give students feedback as they progress in their  learning experience, helping them identify areas needing attention  or improvement [12]. The increasing use of student-centered and  active-learning teaching methodologies to improve learning  outcomes [9, 23] provide additional opportunities to offer highly  personalized feedback. Data-supported predictive models, such as  the one presented in this paper, can offer instructors with valuable  insights on the type of feedback and how to deliver it.   While the work on predictive modeling shows promise for  advancing teaching practice, there is much less research available  on how to actually make use of such predictive models to improve  feedback strategies. The study conducted by Tanes et al. [22]  explored feedback messages sent to students as a response to early  warning alerts triggered by a predictive model of the Signals  software tool. The study showed that while the early warning  system increased the frequency of summative feedback (i.e.,  informing students how they stand with respect to meeting certain  criteria and standards), it did not increase the quality of feedback  (i.e., sending formative and instructional feedback to help students  identify how they could improve their learning). A likely reason  for the low influence on instructional practice can be attributed to  the lack of actionable insight these models provide [11]. Although  the traffic light metaphor used in Signals is intuitive, there is  insufficient transparency related to the reasons why and how  certain predictions are made. This transparency in risk  calculations is essential in order for instructors and students to  understand how best to act upon the predictions.   Further, predictive models are often created without taking into  account the nuanced factors found in different learning situations.  With the aim to maximize scalability and reusability, predictive  models are frequently constructed to generalize across different  courses and subject domains. However, research shows that such  generalized models cannot detect factors that can inform teaching  and learning if the instructional conditions of individual courses  are not considered [10]. Likewise, assumptions are frequently  made about learners on average rather than taking into account  student agency and individual differences (e.g., metacognitive  skills and prior knowledge) in order to tailor personalized  feedback for individual students and student sub-populations [17].    In order to explore the need for new approaches in presenting the  results of predictive models, this paper reports the findings of a  study that examined the use of a recursive partitioning technique  to produce a predictive model. The model is suitable to inform  instructors about the estimated future academic performance of  the students based on engagement with course material. The  algorithms based on recursive partitioning provide a good match  between their requirements and the factors typically available in a  learning environment [18]. The algorithms of this type do not  require the specification of any model in advance, and can handle  an arbitrarily large number of numeric features with different  scales. This is particularly useful in multimodal environments in  which multiple data sources are obtained from a variety of tasks.  Conventional algorithms require data to be categorized,  normalized, or transformed into the required format. These  algorithms are capable to handle variables with no restrictions.  The algorithms also perform automatic selection of the most  relevant features from a potentially large collection. This is also  an advantage when a rich set of features is obtained from a  learning experience but only some of them are relevant for  predictive purposes. Additionally, the resulting model, a tree,  provides instructors with a visualization that can help them  translate the findings into appropriate learning support actions.  The following sections outline a case study describing the adopted  technique used to predict student academic performance and  identify subpopulations of students for personalized feedback.   3. METHOD  3.1 Study Design  The case study described in this section collected data from a 13  week first year course at a large research intensive university in  Australia. The course design involved students engaging in  weekly activities comprised of both formative and summative  assessment items over eleven core weeks of the course. All course  components were offered in electronic form and made available  following a blended learning strategy mediated by the corporate  Learning Management System. The data for the study was derived  from the interactions of the students with the course components.  The study was carried out in the context of a natural experiment as  the participation of the students was beyond the control of the  researchers.   3.2 Materials  The study used several data sources derived from the interaction  of the students with the course components, namely four types of  activities. The first type (VID) consisted of an interactive HTML  page with a video clip introducing new course concepts. All play  and pause events in the video clips were recorded. The second  type of activities (VEQ) were included immediately next to the  video clip and consisted of a formative assessment in the form of  multiple-choice questions related to the concepts covered in the     adjoining video clip. Students could answer each question, review  whether the answer was correct, or request to see all the answers.  This last option was offered only if an answer was provided. The  events correct, incorrect and show were recorded denoting the  three possible actions. The third type of activity (EQT) required  students to read text in an HTML document with additional  formative assessment in the form of multiple-choice questions  (identical to the ones previously described). The fourth set of  activities (EXC) required students to solve a sequence of exercises  and provide their answer through a multiple-choice question. The  platform would select an exercise randomly from a pre-defined  sequence. If the exercise was answered correctly, it no longer  appeared in the sequence. If the answer was incorrect, the exercise  remained as part of the sequence. The students advanced through  the sequence (and therefore repeated exercises that were answered  incorrectly) and a score was calculated as the percentage of  exercises answered correctly. This exercise was a summative  assessment and the score was directly added to the course marks.  The server registered the events correct and incorrect for each  exercise attempted by each student. Each event generated in any  of the assessment items has a unique identifier that allows its  differentiation from the rest of items. The three types of  assessment (VEQ, EQT, and EXC) were treated as separated  categories in the analysis. These activities were made available to  the students gradually every week and remained available for the  remainder of the semester. Two additional data sources were  considered in the study: the results of the midterm examination  (scheduled in Week 6 of the semester) and final examinations.  These exams contributed 20% and 40% respectively towards the  final course mark.   3.3 Procedure   The data was collected in the 2014 offering of a large first year  engineering course (n = 272). The weekly schedule included one  2-hour lecture, one 2-hour tutorial and a 3-hour laboratory  session. The data was extracted from the server logs and fully  anonymized. The course design included the same pattern of  activities for the core weeks 2-5 and 7-13. The activity sequence  was comprised of an initial set of formative assessment activities  of types VID, VEQ and EQT followed by a summative  assessment activity of type EXC with submission deadlines before  the start of the lecture to encourage in-class preparation.  Additionally, another set of EQT activities and an additional EXC  activity were scheduled with deadlines before the start of the  tutorial sessions. Students were provided with real-time indicators  of their participation in these activities showing the percentage of  activities that were attempted (if formative) and their score (if  summative).   3.4 Variables and Measures  The variables used as predictors to build the models were derived  from the interaction students had with the various learning design  resources available in an online platform. Since the course has  tasks and assessments due every week, the variables used for the  analysis are the weekly counts for the activity events across  eleven core course weeks. More precisely, for each week and each  student, the following counts were extracted from the server logs:    VID.PL/VID.PA: Number of play/pause events in the  videos for the week.    VEQ.CO/VEQ.IN/VEQ.SH: Number of times a question  next to a video clip was answered correctly, incorrectly or  the answer was shown.    EQT.CO/EQT.IN/EQT.SH: Number of times a question   in the course notes was answered correctly, incorrectly or  the answer was shown.    EXC.CO/EXC.IN: Number of exercises answered  correctly/incorrectly.   For each week of the course, a set of variables was extracted and  used to calculate the predictive models. For example, the model  for week 3 was calculated with the set of variables reflecting the  number of events recorded during that week (ignoring the rest).  These models (eleven of them) were divided into two categories:  those predicting student performance on the midterm exam (for  weeks 2 to 5) and those predicting performance on the final exam  (weeks 7 to 13). The midterm and final exam scores are numerical  variables with values in the range [0-20] and [0-40] respectively.   3.5 Data Analysis  The study aimed to show the feasibility of obtaining a predictive  model of the performance of the students in the midterm and final  examination using the data about their interaction with the online  activities. Essentially, the study explored the suitability of  predictive techniques that can handle numeric indicators and  provide a simple interpretation of the results. The performance of  the model was measured using the Mean Absolute Error (MAE)  defined as the mean of the differences between the predicted and  real scores, and the Root of the Mean Squared Error (RMSE)  defined as the square root of the mean of the square differences  between the predicted and real scores (MSE). The model  validation was performed following a leave one out cross- validation strategy. For each of the samples a model was  calculated leaving one sample out and its error in the estimation  (on the left-out sample) was obtained. The errors were then  combined to calculate MAE and RMSE.   4. RESULTS  The descriptive statistics (mean and standard deviation) for each  of the variables during the eleven core weeks of the course are  shown in Table 1. The mean and standard deviation of the  midterm and final scores were 13.3 (4.1) and 19.1 (8.8),  respectively. Predictive models were calculated for all eleven  weeks of the semester using recursive partitioning. For each  model, a collection of rules and a tree structure was produced. The  rules were stated as conditions on the input variables (event  counts). Each node of the tree was labeled with a condition and  two outgoing edges. As an example, the resulting predictive tree  for Week 10 is shown in Figure 1.   4.1 Instructional Interpretation   As illustrated in Figure 1, the predictive tree for Week 10,  obtained through recursive partitioning, contains 13 nodes and 6  rules. The recursive structure of the tree represents how the entire  population of students is recursively partitioned into subsets. The  final partition is represented by the leaf nodes at the bottom of the  figure. The top node represents the entire cohort and shows the  condition used to divide it into two subsets represented by the  sub-trees. The left branch includes 108 students who provided  more than 21 incorrect answers to the exercise sequence  (condition EXC >=22 and node number 2 in the tree). The right  branch of the top node contains 167 students who did not satisfy  the condition (i.e. less than 22 errors in the exercise sequence) and  with an average predicted score of 9.6 over 40. The partitioning is  then performed recursively at the level of the sub-nodes. At each  point in the tree, a different feature is chosen to divide the group.   The final partitioning of the cohort is represented by the leaves at  the bottom of the tree. This model has divided the group into        Table 1: Descriptive statistics of the event count variables of the study  Week VID.PL VID.PA VEQ.CO VEQ.IN VEQ.SH EQT.CO EQT.IN EQT.SH EXC.IN EXC.CO  W2 8.9(16.1) 8.9(18.5) 6.4(6.7) 3.7(4.5) 2.1(2.9) 6.8(12.6) 4.6(9.6) 2.3(6.7) 16.7(11.0) 15.2(14.6)  W3 9.4(16.8) 9.5(20.9) 9.1(10.3) 6.1(7.9) 2.5(5.0) 11.9(14.5) 7.8(10.4) 3.6(7.1) 25.5(13.5) 38.6(25.3)  W4 13.2(26.1) 12.7(24.7) 8.6(11.5) 5.7(9.3) 2.0(4.1) 6.8(12.8) 4.4(8.6) 1.8(5.3) 25.6(12.4) 37.9(28.7)  W5 6.1(14.9) 5.9(15.9) 4.2(6.9) 2.7(4.7) 1.0(2.9) 7.3(13.3) 5.4(11.1) 2.1(5.2) 22.9(11.0) 33.5(25.4)  W7 7.3(21.2) 7.2(24.0) 3.8(6.3) 3.2(5.5) 1.2(3.2) 1.6(4.3) 1.0(2.9) 0.4(1.8) 19.9(7.4) 26.4(21.1)  W8 8.1(21.0) 9.1(27.8) 3.0(5.7) 2.5(4.9) 0.7(2.3) 2.3(4.5) 1.5(3.2) 0.4(1.3) 18.2(8.5) 26.8(21.6)  W9 6.9(15.7) 7.2(19.1) 3.0(5.8) 2.7(5.4) 0.9(2.6) 2.4(5.6) 1.6(3.7) 0.7(2.5) 19.0(8.3) 33.6(26.2)   W10 6.0(16.4) 6.8(21.0) 2.5(5.5) 2.6(5.3) 0.8(2.4) 1.8(5.3) 1.3(3.3) 0.3(1.4) 11.7(18.7) 18.2(21.1)  W11 5.5(11.9) 5.5(15.1) 1.9(3.5) 1.9(3.9) 0.6(1.7) 2.1(7.0) 1.8(5.4) 0.7(3.3) 15.4(8.4) 23.5(23.8)  W12 4.2(13.0) 4.3(15.1) 2.4(5.2) 2.3(5.1) 0.9(2.9) 1.0(3.6) 0.7(2.7) 0.4(2.1) 15.5(7.3) 23.0(19.2)  W13 44.4(95.8) 38.6(94.8) 14.2(26.8) 11.4(20.7) 3.8(8.4) 30.1(39.5) 17.0(20.5) 6.7(10.9) 71.1(96.6) 52.0(63.7)              seven subpopulations with a different number of students per  subgroup and with predicted scores on the final examination  ranging from 6 (left most node) to 15 (right most node). An  equivalent interpretation of the model is that every path from the  root node to a leaf provides the set of conditions satisfied by that  subset of students. The conjunction of these conditions would  create the rule used to identify the students. For example, the 90  students represented by node number 4 are identified by the  property that EXC.IN >=22 AND VID.PL < 8.5, or in other  words, the number of incorrect answer to one of the test is larger  than 21, and the number of play events in the videos less than 8.5.   The main advantage of this model is that the inputs for the  algorithm (features) are indicators derived directly from the  learning design. Additionally, the result divides the cohort into a  manageable number of subpopulations. An instructor may now  use the predicted score to provide personalized feedback to each  subset of students. For example, the suggestions given to the 20  students with a predicted score of 15 (out 40) would be different  from those given to the 90 students with a predicted score of six.   A more in-depth analysis of the resulting decision tree may also  guide the instructor towards a higher level of personalization. For  example, students in the subpopulation with the lowest predicted  score have a high number of incorrect exercise submissions (node  1) and a low number of play video events (node 2). An instructor  may refer to this information when customizing the feedback   inviting them to increase their engagement with the videos, which  may help reduce the number of their incorrect exercise  submissions.   4.2 Model Performance  The performance of the resulting model is assessed based on the  accuracy of the score predictions in the leave nodes. This  information can be conveyed to the instructors so that they have  an estimate of the error in the predictions. The performance  analysis is shown in Table 2.    Table 2: Performance of the model for the midterm   MSE RMSE MAE   W2 15.826 3.978 3.14  W3 15.476 3.934 3.053  W4 15.14 3.891 3.013  W5 14.469 3.804 3.007   The MAE for all four models calculated is stable at approximately  15% of the midterm score (3 out of 20). The RMSE, on the other  hand, offers a higher value (approximately 4) but still below 20%  (4 out of 20). These figures have a simple interpretation with  respect to the model. The score predicted for a subgroup may have  an average of 15 or 20% error. Although this type of error would  not be accepted in other disciplines, when applied to the provision  of feedback in a learning context it helps to reduce the uncertainty  when trying to identify the appropriate sets of students.   Table 3: Performance when estimating the final exam score   MSE RMSE MAE   W7 29.182 5.402 4.4  W8 32.358 5.688 4.726  W9 28.905 5.376 4.481   W10 32.101 5.666 4.665  W11 30.436 5.517 4.482  W12 25.476 5.047 4.097  W13 32.037 5.66 4.503   The performance of the models improves when predicting the  score for the final exam. Table 3 shows the performance for the  remaining seven weeks of the semester. In this case, since the  score is in the range [0-40] the MAE is below 11.8% (4.726 over  40) whereas the RMSE is below 14.22% (5.688 over 40).   5. CONCLUSION  The predictive models derived from data captured from learning  environments usually require complex data-preparation processes  and produce results that are difficult to interpret by instructors and  students. This paper presented a case study that used recursive  partitioning to process a large number of numeric features derived  from the student engagement with the learning environment and   Figure 1: Decision tree for Week 10.     automatically select those that provide a robust classification with  respect to their predicted academic performance. The estimated  error in the prediction is within reasonable bounds. Although the  results should not necessarily be used in the exact form as shown  in this paper, they provide a transparent characterization of a  student cohort based on indicators extracted from the learning  environment thus facilitating its translation into actions. These  models may serve as the basis to explore more efficient sense- making solutions to support instructors in the provision of  frequent and effective formative and personalized feedback.   6. REFERENCES  [1] Agudo-Peregrina, .F., Iglesias-Pradas, S., Conde-Gonzlez,   M.. and Hernndez-Garca, . 2014. Can we predict  success from log data in VLEs Classification of  interactions for learning analytics and their relation with  performance in VLE-supported F2F and online learning.  Computers in Human Behavior. 31, (Feb. 2014), 542550  doi:10.1016/j.chb.2013.05.031.   [2] Alstete, J.W. and Beutell, N.J. 2004. Performance indicators  in online distance learning courses: a study of management  education. Quality Assurance in Education. 12, 1 (Mar.  2004), 614 doi:10.1108/09684880410517397.   [3] Arnold, K.E. and Pistilli, M.D. 2012. Course signals at  Purdue: using learning analytics to increase student success.  Proceedings of the 2nd International Conference on  Learning Analytics and Knowledge (New York, NY, USA,  2012), 267270 doi:10.1145/2330601.2330666.   [4] Baker, R.S.J. 2010. Data Mining. International Encyclopedia  of Education. Elsevier. 112118.   [5] Baker, R.S.J. d., DMello, S.K., Rodrigo, M.M.T. and  Graesser, A.C. 2010. Better to be frustrated than bored: The  incidence, persistence, and impact of learners cognitive affective states during interactions with three different  computer-based learning environments. International  Journal of Human-Computer Studies. 68, 4 (Apr. 2010),  223241 doi:10.1016/j.ijhcs.2009.12.003.   [6] Barber, R. and Sharkey, M. 2012. Course Correction: Using  Analytics to Predict Course Success. Proceedings of the  2Nd International Conference on Learning Analytics and  Knowledge (New York, NY, USA, 2012), 259262  doi:10.1145/2330601.2330664.   [7] Butler, D.L. and Winne, P.H. 2014. Feedback and Self- Regulated Learning: A Theoretical Synthesis. Review of  Educational Research. 65, 3 (2014), 245281  doi:10.3102/00346543065003245.   [8] Dawson, S., Gaevi, D., Siemens, G. and Joksimovic, S.  2014. Current State and Future Trends: A Citation Network  Analysis of the Learning Analytics Field. International  Conference on Learning Analytics and Knowledge (2014),  232240 doi:10.1145/2567574.2567585.   [9] Freeman, S., Eddy, S.L., McDonough, M., Smith, M.K.,  Okoroafor, N., Jordt, H. and Wenderoth, M.P. 2014. Active  learning increases student performance in science,  engineering, and mathematics. Proceedings of the National  Academy of Sciences of the United States of America. (May  2014), 16 doi:10.1073/pnas.1319030111.   [10] Gaevi, D., Dawson, S., Rogers, T. and Gasevic, D. 2016.  Learning analytics should not promote one size fits all:  The   effects of instructional conditions in predicting learning  success. The Internet and Higher Education. 28, (2016), 68 84.   [11] Gaevi, D., Dawson, S. and Siemens, G. 2015. Lets not  forget: Learning analytics are about learning. TechTrends.  59, 1 (2015), 6471.   [12] Gibbs, G. and Simpson, C. 2004. Conditions under which  assessment support students learning. Learning and  Teaching in Higher Education. 1, 1 (2004), 331.   [13] Hattie, J. and Jaeger, R. 1998. Assessment and Classroom  Learning: a deductive approach. Assessment in Education:  Principles, Policy & Practice. 5, 1 (Mar. 1998), 111122  doi:10.1080/0969595980050107.   [14] Hattie, J. and Timperley, H. 2007. The Power of Feedback.  Review of Educational Research. 77, 1 (Mar. 2007), 81112  doi:10.3102/003465430298487.   [15] Jayaprakash, S.M., Moody, E.W., Laura, E.J.M., Regan,  J.R. and Baron, J.D. 2014. Early Alert of Academically At- Risk Students: An Open Source Analytics Initiative. Journal  of Learning Analytics. 1, 1 (May 2014), 647.   [16] Jiang, Y., Paquette, L., Baker, R.S., Street, W., Clarke- midura, J. and Hill, O.M. 2015. Comparing Novice and  Experienced Students within Virtual Performance  Assessments. International Conference on Educational  Data Mining (2015).   [17] Kovanovi, V., Gaevi, D., Joksimovi, S., Hatala, M. and  Adesope, O. 2015. Analytics of communities of inquiry:  Effects of learning technology use on cognitive presence in  asynchronous online discussions. The Internet and Higher  Education. 27, (Oct. 2015), 7489  doi:10.1016/j.iheduc.2015.06.002.   [18] Lantz, B. 2013. Machine Learning with R. Packt Publishers.  [19] Romero, C., Lpez, M.-I., Luna, J.-M. and Ventura, S. 2013.   Predicting students final performance from participation in  on-line discussion forums. Computers & Education. 68,  (Oct. 2013), 458472 doi:10.1016/j.compedu.2013.06.009.   [20] Romero, C., Ventura, S. and Garca, E. 2008. Data mining in  course management systems: Moodle case study and  tutorial. Computers & Education. 51, 1 (Aug. 2008), 368 384 doi:10.1016/j.compedu.2007.05.016.   [21] Siemens, G. 2013. Learning Analytics: The Emergence of a  Discipline. American Behavioral Scientist. 57, 10 (Aug.  2013), 13801400 doi:10.1177/0002764213498851.   [22] Tanes, Z., Arnold, K.E., King, A.S. and Remnet, M.A. 2011.  Using Signals for appropriate feedback: Perceptions and  practices. Computers & Education. 57, 4 (Dec. 2011),  24142422 doi:10.1016/j.compedu.2011.05.016.   [23] Wieman, C.E. 2014. Large-scale comparison of science  teaching methods sends clear message. Proceedings of the  National Academy of Sciences of the United States of  America. 111, 23 (Jun. 2014), 831920  doi:10.1073/pnas.1407304111.   [24] Wise, A.F. 2014. Designing Pedagogical Interventions to  Support Student Use of Learning Analytics. Proceedings of  the International Conference on Learning Analytics and  Knowledge (2014).        1. INTRODUCTION  2. RELATED WORK  3. METHOD  3.1 Study Design  3.2 Materials  3.3 Procedure  3.4 Variables and Measures  3.5 Data Analysis   4. RESULTS  4.1 Instructional Interpretation  4.2 Model Performance   5. CONCLUSION  6. REFERENCES   "}
{"index":{"_id":"59"}}
{"datatype":"inproceedings","key":"Ringtved:2016:LDF:2883851.2883856","author":"Ringtved, Ulla and Milligan, Sandra and Corrin, Linda","title":"Learning Design and Feedback Processes at Scale: Stocktaking Emergent Theory and Practice","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"479--480","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883856","doi":"10.1145/2883851.2883856","acmid":"2883856","publisher":"ACM","address":"New York, NY, USA","keywords":"crowd-sourced learning, feedback, learning analytics, learning at scale, learning design, management performance, scaled courses","abstract":"Design for learning in scaled courses is shifting away from replication of traditional on-campus or online teaching towards exploiting the distinctive characteristic and potentials of scale to transform both teaching and learning. Scaled learning environments such as MOOCs may represent a new paradigm for teaching. This workshop involves consideration of the how learning occurs in scaled environments, and how learning designers and analysts can assist. It will explore questions at the heart of effective learning design, using expert panelists and collaborative knowledge-building techniques to arrive at a stocktake of thinking.","pdf":"Learning design and feedback processes at scale:  stocktaking emergent theory and practice    DesignLAK16 workshop: https://sites.google.com/site/designlak16  Ulla Ringtved   Aalborg University & University  College Northern Denmark,  P.O. Box 71 Denmark -9100   Aalborg, Denmark  +61 (0) 72698000  ulr@hum.aau.dk   Sandra Milligan  University of Melbourne   Melbourne Graduate School of  Education, 234 Queensberry St,   Parkville VIC 3010 Australia  +61 (0) 402115005   s.milligan@unimelb.edu.au   Linda Corrin  University of Melbourne, Melbourne   Centre for the Study of Higher  Education, Elisabeth Murdoch   Building, Parkville VIC 3010 Australia  +61 (03) 90359685   lcorrin@unimelb.edu.au     ABSTRACT  Design for learning in scaled courses is shifting away from  replication of traditional on-campus or online teaching towards  exploiting the distinctive characteristic and potentials of scale to  transform both teaching and learning. Scaled learning  environments such as MOOCs may represent a new paradigm for  teaching. This workshop involves consideration of the how  learning occurs in scaled environments, and how learning  designers and analysts can assist. It will explore questions at the  heart of effective learning design, using expert panelists and  collaborative knowledge-building techniques to arrive at a stock- take of thinking.   Categories and Subject Descriptors  H.1: Models and Principles   General Terms  Management Performance, Design, Human Factors, Theory.   Keywords  learning analytics, learning design, scaled courses, feedback,  crowd-sourced learning, learning at scale.   1. INTRODUCTION  Since the advent of MOOCs and other forms of learning at scale,  attention has focused on how leaning occurs in these contexts, and  how to improve it. Scaled courses lack the normal affordances of  the traditional higher education classroom. There is no collegiate  community of like-minded peers with similar backgrounds and  aspirations with whom the experience is shared. Nor are there  teachers or tutors to whom the student can turn, who know the  student, and their capability and who monitor and guide their  leaning over a period of time, sometimes years. Learning in a  MOOC is, at first glance, an individual task, tackled alone and  apart from others. Some like it and prosper. For others it is  difficult, even alienating. Some people generate high levels of  learning, others cant or dont.    Early examination of success in MOOCs has focused on what  works, both in terms of MOOC design, and the characteristics of   successful learners. Participants who prosper are a small  proportion of learners who are highly self-regulated, and able to  harness quality learner feedback from the crowd or from  automated teaching agents of the platform [1-3].They have a set of  learning skills, made up of a constellation of attitudes, values,  beliefs, understanding and knowledge about leaning, and  awareness of how these skills work for them. They create an  environment for themselves that provides the social and  pedagogical support they need.    Equally important is to understand the needs of learners whose  learning skills in this environment are low, and how their learning  skills can be developed. Learning design needs to give full rein to  the self-regulated learner and simultaneously guide the dependent  majority of learners to become more self-regulated.   Learning design is not simple, in any environment. Arranging  feedback, assessment and pedagogy that works in learning is hard,  effects often impossible to predict. The utility of feedback can be  marginal, its effects transient and negative [8-10]. Organising the  right feedback to a student at the right time is the holy grail of  most teachers, and in this, MOOCs have advantages: design teams  can use learning analytics, the crowd, and peers as well as input  from teaching staff, aligned with quality designs to provide a  tireless, constant stream of quality, relevant feedback to the  learner at the time they need it. The combined power of good  feedback, good assessment, good collaboration and learning  analytics may well provide a step towards that holy grail of  interest to all teachers, not just those designing MOOCs.       2. WORKSHOP QUESTIONS  A range of research projects and studies are seeking insight into   how to effect good design in scaled open courses [4-7], delving  into what self-regulated learners need to generate higher order  learning, and what teaching staff can to do provide and support it.  The field is fast-moving and challenging.    Teaching in scaled open courses is a design task, a team  activity in which technology and analytics are central elements.  Increasingly, the perspective of teachers working in the area is  shifting away from replication of the traditional on-campus or  online teaching-learning relationship towards exploiting the  distinctive characteristics and potentials of scale to transform both  teaching and learning.    This workshop will explore the theory and practice of designing  for learning in digital scaled courses, aimed to support the self- regulated learner. Questions at the heart of this are: Do scaled  learning environments such as MOOCs represent a new paradigm   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883856     for teaching What are the significant features of scaled  teaching/learning that set it apart What are the challenges facing  the learner in scaled learning environments What is known about  the behaviours of successful learners What distinctive learning  skills do they require Can they be developed, and how What are  the challenges facing the designer What are the key design  approaches in scaled courses How can learning analytic  help What are some priorities for action Where are the gaps in  knowledge/understanding that could be informed by research  Where are the gaps in analytics tools and techniques      3. WORKSHOP METHODS  This workshop is to distill the emergent principles guiding best   practice in use of learning analytics in learning design for scaled  courses. It is designed for researchers and teaching practitioners  interested and/or expert in the process of teaching, learning and  assessment in MOOCs and other forms of learning at scale. It  aims to synthesis views about learning analytics, feedback,  assessment and its role from researchers and practitioners in areas  of learning analytics in MOOCs, self-regulation and learning  skills, learning design, and online learning platform design.    The workshop is structured around several elements. A series of  invited expert panelists from research teams working in the area  will give overviews of their work, to frame the debate. Working  sessions will be organised around the questions, outlined above.  Participants are expected to actively input their own experience  and insights via a fast-moving, participative, knowledge-building  process, the world caf, (http://www.theworldcafe.com). Such  processes are designed to synthesise and develop knowledge and  understanding of a diverse range of participants from different  disciplines and experiential backgrounds, different nationalities  and different sectors. The process will build on various groupings  that emerge from the participant base, and ensure that each  individual has an opportunity to interact at some depth with  people from similar backgrounds, and people from different  backgrounds as well. The process is effective in knowledge  building and also effective for building networks and connections,  and promoting cross-disciplinary and cross-contextual  understanding. Knowledge aggregations methods will be used,  promising to be energetic, engaging and fun as well. In a final  panel, rapporteurs will feed back to the groups some of the key  ideas that emerged. Each rapporteur will also produce a brief  report synthesizing the key themes in their area after the workshop  for distribution to participants and publication in the workshop  proceedings.    3. ABOUT THE ORGANISERS  Ulla Ringtved is organizer of the Learning Analytics Summer  Institute 2013 and 2015 at Aalborg University, Denmark. Her  research focuses on automation of feedback and assessment in  Technology Enhanced Learning (TEL), and on the  implementation of open educational resources in learning and  teaching in Higher Education, including in MOOCs. She teaches  at UCN and AAU in the areas of technology and learning, and  professional development for learning professionals, focusing on  feedback, assessment and learning analytics. She is an  experienced workshop facilitator and teaches and researches in a  diversity of TEL environments.      Sandra Milligan is a member of the Learning Analytics  Research Group at the University of Melbourne; is Convener of a  University of Melbourne MOOC targeting professional learning  and research engagement of teachers; and is conducting research  on new approaches to assessment and certification in MOOCs.  She works under the auspices of the Science of Learning Research  Centre and the Assessment Research Centre. She is a publisher,  has taken to market three of her own technology start-up  companies, worked at senior levels in large multi-national  companies and in government, and served as director for a range  of organisations. She is an experienced presenter, and writer.   Linda Corrin has been involved in educational technology- related research, curriculum development, and academic  development in higher education for the past 12 years. Lindas  research interests include examining students engagement with  technology, learning analytics, student feedback, and learning  design. Her current research includes a crossinstitutional study  which aims to develop a better understanding of the ways in  which learning analytics can be interpreted, applied and actioned  by teachers to improve teaching and learning practices. Linda was  a founding member of the Victorian and Tasmanian Learning  Analytics Network in Australia.   4. REFERENCES   [1] Milligan, S. K. Crowd-sourced learning in MOOCs:   learning analytics meets measurement theory. In Proceedings of  the Learning Analytics and Knowledge Conference Poughkeepsie,  NY, USA, 2015. ACM.    [2] Ferguson, R. and Sharples, M. Innovative Pedagogy at  Massive Scale: Teaching and Learning in MOOCs. Springer,  2014.   [3] Lockyer, L. and Dawson, S. Where Learning Analytics  Meets Learning Design Workshop. In Proceedings of the LAK'12  Conference, ACM Vancouver, B Canada, 29 April- 2 May, 2012.    [4] Milligan, C. and Littlejohn, A. Professional Learning in  Massive Open Online Courses:Design Recommendations, 2015.   [5] Milligan, S. K. and Griffin, P. Mining a MOOC: What our  MOOC taught us about professional learning, teaching and  assessment. IGI Global, City, 2015.   [6] Kennedy, G., Corrin, L., Lockyer, L., Dawson, S., Williams,  D., Mulder, R., Khamis, S., & Copeland, S. Completing the loop:  returning learning analytics to teachers. In B. Hegarty, J.  McDonald, & S.-K. Loke (Eds.), Rhetoric and Reality: Critical  perspectives on educational technology. Proceedings ascilite  Dunedin 2014 (pp. 436-440).    [7] Milligan, C., Littlejohn, A. and Margaryan, A. Patterns of  Engagement in Connectivist MOOCs. Journal of Online Learning  & Teaching, 9, 2 2013), 149-159.    [8] Kluger, A. and DeNisi, A. The effects of feedback  interventions on performance: a historical review, a meta-analysis,  and a preliminary feedback intervention theory. Psychological  Bulletin, 119, 2 1996), 30.   [9] Black, P., McCormick, R., James, M. and Pedder, D.  Learning How to Learn and Assessment for Learning: a  Theoretical Inquiry. Research Papers in Education, 21 (02) 2006.   [10] Hattie, J., & Timperley, H. (2007). The power of feedback.  Review of Educational Research, 77(1), 81-112.        "}
{"index":{"_id":"60"}}
{"datatype":"inproceedings","key":"Shum:2016:CPW:2883851.2883854","author":"Shum, Simon Buckingham and Knight, Simon and McNamara, Danielle and Allen, Laura and Bektik, Duygu and Crossley, Scott","title":"Critical Perspectives on Writing Analytics","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"481--483","numpages":"3","url":"http://doi.acm.org/10.1145/2883851.2883854","doi":"10.1145/2883851.2883854","acmid":"2883854","publisher":"ACM","address":"New York, NY, USA","keywords":"education, natural language processing, writing","abstract":"Writing Analytics focuses on the measurement and analysis of written texts for the purpose of understanding writing processes and products, in their educational contexts, and improving the teaching and learning of writing. This workshop adopts a critical, holistic perspective in which the definition of the system and success is not restricted to IR metrics such as precision and recall, but recognizes the many wider issues that aid or obstruct analytics adoption in educational settings, such as theoretical and pedagogical grounding, usability, user experience, stakeholder design engagement, practitioner development, organizational infrastructure, policy and ethics.","pdf":"Critical Perspectives on Writing Analytics     Simon Buckingham Shum, Simon Knight   Connected Intelligence Centre   University of Technology Sydney  PO Box 123, Broadway, Ultimo    NSW 2007, AUS  first.lastname@uts.edu.au   Danielle McNamara, Laura Allen   Science of Learning and Educational Technology Lab   Arizona State University  Tempe, AZ, USA   Danielle.McNamara@asu.edu  LauraKAllen@asu.edu   Duygu Bektik   Knowledge Media Institute   The Open University  Walton Hall   Milton Keynes, MK7 6AA, UK  duygu.bektik@open.ac.uk   Scott Crossley   Department of Applied Linguistics/ESL   34 Peachtree St. Suite 1200   One Park Tower Building   Georgia State University  Atlanta, GA 30303, USA  scrossley@gsu.edu     ABSTRACT  Writing Analytics focuses on the measurement and analysis of  written texts for the purpose of understanding writing processes  and products, in their educational contexts, and improving the  teaching and learning of writing. This workshop adopts a critical,  holistic perspective in which the definition of the system and  success is not restricted to IR metrics such as precision and  recall, but recognizes the many wider issues that aid or obstruct  analytics adoption in educational settings, such as theoretical and  pedagogical grounding, usability, user experience, stakeholder  design engagement, practitioner development, organizational  infrastructure, policy and ethics.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   General Terms  Design, Experimentation, Human Factors, Measurement   Keywords  Education, Writing, Natural Language Processing   1. WORKSHOP INTRODUCTION  The focus of this workshop is on the topic of writing analytics.  Broadly defined, writing analytics involves the measurement and  analysis of written texts for the purpose of understanding writing  processes and products, in their educational contexts. Writing  analytics are ultimately aimed at improving the educational  contexts in which writing is most prominent. The principal goal of  writing analytics is to move beyond assessment of texts divorced  from contexts, transitioning instead to a more nuanced  investigation of how analytics may be effectively deployed in   different writing contexts.    Writing analytics thus aims to employ learning analytics to  develop a deeper understanding of writing skills. Thus, workshop  discussions will focus on writing from a number of different  perspectives. In particular, we will discuss analytics that can help  to better understand both the writing process as well as the final  product, as well as their interactions with task demands, such as  essay genre and voice.    An additional focus of this workshop will be on the pedagogical  context in which writing analytics should take place. Our aim is  not simply to focus on the automated scoring of written essays.  Rather, we aim to discuss writing analytics that can be  meaningfully embedded within a pedagogical context. These  discussions can relate to a number of issues, such as the delivery  of feedback and adaptive instruction.   1.1 Writing as a window onto the mind  Effective writing is not only central to education and the  workplace, but also a lifelong citizenship skill necessary for  effectively engaging with society. A large majority of academic  disciplines focus on the development of learners skills in critical  review, conceptual synthesis, reasoning, and  disciplinary/professional reflection. In these subjects, writing  arguably serves as the primary window into the mind of the  learner. Huge effort is invested in literacy from the earliest  schooling, extending into higher education. Yet educators and  employers alike recognize the challenge of cultivating this ability  in graduates, with poor written communication skills a common  cause of complaint.   Extending beyond scholarly academic writing, many educators  also have a keen interest in disciplined, autobiographical  reflective writing as a way for students to review and consolidate  their learning, thus providing a means for assessing the deepest  kinds of shifts that can occur in learner agency and epistemology.  Such approaches are also common in the training and  development of professional reflective practitioners.   Writing is, however, time consuming, labor-intensive to assess,  difficult for students to learn, and not something that all educators  can coach well, or even consider their job to coach. It is in  addressing these systemic limitations that Writing Analytics is   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.    Copyright is held by the owner/author(s).   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom.   ACM 978-1-4503-4190-5/16/04.  http://dx.doi.org/10.1145/2883851.2883854     attracting significant educational interest and commercial  investment.   1.2 Harnessing NLP  Natural language processing (NLP) techniques have been  proposed as one of the most effective methods for analyzing  writing. In particular, NLP can provide detailed information about  the properties of students writing at multiple levels of the text.  For instance, NLP tools have been developed to provide  information about the words in the text, such as their imageability  and frequency in the English language. Additionally, NLP tools  have been developed to calculate various other aspects of text,  such as the difficulty of the sentences, and the presence of  cohesion at multiple different levels of the text (e.g., between  sentences, paragraphs, etc.).   As NLP moves beyond research labs and into mainstream  products, the Learning Analytics community has the opportunity  and challenge of harnessing language technologies, and delivering  them in effective ways that enhance learning.    NLP approaches are, of course, the key enabling capability of  these technologies, but they are just one piece of the puzzle for an  effective learning analytics solution: these approaches need to be  tuned by theories of how writing and learning shape each other,  the scholarship of teaching writing, appropriate pedagogical  practices and user interface design, and evidence from empirical  evaluation of the total system, not just algorithmic metrics.    The LAK community should be in a position to guide educators  and students on the evidence of impact in this new space. What  questions should be asked before buying a new product or  trialling a new research prototype What are the options for  evaluating such tools What staff competencies are required to  ensure that such tools have the maximum chances of success Do  students need orientation or training What pedagogical contexts  can this tool be applied to These are the often-ignored  constraints around a potentially disruptive technology.   1.3 Promises and pitfalls  Ultimately, for the tools to be successful, educators and students  must trust them, and the effort of learning these new tools must  pay back. Computational assessment of writing elicits strong  reactions from many educators. For skeptics, handing the task of  assigning feedback or grading essays to a machine crosses a  boundary line marking the limits of artificial intelligence (AI). An  important research question is whether or not such skepticism is  justified.   Writing Analytics have in common similar potential and pitfalls to  other learning analytics applications. At the most optimistic level,  the promise of writing analytics is the kind of 24/7 personalized  feedback that is currently only available to a privileged minority  via detailed, timely feedback from educators as they draft texts.  However, this workshop will take a systemic perspective,  problematizing the contexts that Writing Analytics are deployed  within, and partially constituted by. Evaluation of Writing  Analytics is thus framed as a design problem, raising questions  about conventional metrics (such as precision, recall), alongside:  socio-technical concerns; pedagogic and assessment contexts; and  ethical issues.   1.4 Critical, systemic perspectives   Learning Analytics as a field sits at the confluence of existing  research tributaries. The LAK Discourse-Centric Learning  Analytics (DCLA) workshops forged connections with CSCL  discourse researchers to ensure that DCLA built on existing work.   DCLA workshops have had a couple of papers on extended  student writing, which we now argue merits its own workshop.   This workshop thus seeks to build similar bridges to existing  research communities. There is a decades tradition of Workshops  on Innovative Use of NLP for Building Educational Applications,  operating within the computational linguistics research paradigm,  with evaluation based on information retrieval (IR) metrics, but  applied specifically to educational texts. The Computer Assisted  Assessment community has a tradition of research into student  writing, and has a strong educational researcher presence.  Research in Computer-Supported Collaborative Learning has a  primary focus on student discourse.    The workshop aims to reflect the distinctive contribution that  SoLAR and LAK bring, namely, a holistic perspective in which  the definition of the system and success is not restricted to IR  metrics such as precision and recall, but recognizes the many  wider issues that aid or obstruct analytics adoption in educational  settings, such as theoretical and pedagogical grounding, usability,  user experience, stakeholder design engagement, practitioner  development, organizational infrastructure, policy and ethics.   1.5 Submissions & workshop format  Thus, in this first Writing Analytics workshop, we aim to bring  together active researchers and reflective practitioners in the field  from both academia and industry, spanning K-12, higher  education, and the workplace. This forum will provide the chance  for newcomers to see the state of the art in a range of approaches,  as well as appreciate the issues that arise around writing.    The workshop problematizes writing analytics providing space for  critical reflection from a range of communities on the  development and application of such techniques. We welcome  contributions from technical and educational perspectives,  inviting theoretical and empirical evidence, alongside critical  perspectives. Issues include:    Pedagogically-grounded requirements for language  technologies to support a specific genre of writing (even if  these are extraordinarily challenging)    Design and validation of analytics for different genres of  academic writing (e.g., literature review; debate analysis;  personal reflection)    The relationship between assessment regimes and choice of  writing analytics (e.g., summative grading for high stakes  tests; formative feedback on open-ended reflection;  individual versus collaborative peer review)    Writing analytics in support of wider pedagogic context  (e.g., using writing to recommend readings; writing  analytics to support peer assessment)    Arguments for the potential benefits (or damage) of  engaging with writing analytics (e.g., Might rapid feedback  disrupt critical reflection processes Is automated feedback  perceived differently by students than human feedback)    Compelling (even fun) user interfaces for engaging with  automated writing feedback (e.g., annotations;  visualizations of content and structure, games)    Empirical evaluations of research prototypes and  commercial products    Principles for embedding software tools into practice (e.g.,  student and staff orientation; common misconceptions)    Organizational adoption case studies   Ethical issues specific to writing analytics (e.g., given the   range of ideas and emotions that can be expressed)     Following an innovative format we have used in past workshops,  brief position statements and presentations will be invited to  address the above themes, with participants in each thematic  session assigned different roles to provoke multidisciplinary  dialogue, such as analytics researcher, tool developer, data  provider, writing researcher, writing educator, and commentator.   2. AUDIENCE AND PARTICIPANTS  This workshop will be of interest to a wide range of LAK  delegates including: students and researchers actively engaged in  writing research, text analytics or writing analytics specifically;  educators in schools, universities and businesses; leaders and  policymakers; and companies active or potentially active in the  field. As the first workshop devoted to this topic at LAK, it will  also serve as a community-building event. Participants will be  expected to leave with a clearer understanding of, and critical  perspectives on, the range of purposes for which Writing  Analytics may be deployed, the current state of the art, criteria  and methods for evaluation, and organizational adoption issues.   3. WORKSHOP CHAIRS  Simon Buckingham Shum is Professor of Learning Informatics  at the University of Technology Sydney, where he directs the  Connected Intelligence Centre. His research focuses on learning  analytics for  higher order competencies such as academic writing,  argumentation and collaboration. He served as LAK12 Program  Co-Chair, and co-chaired the LAK13/14 workshops on Discourse- Centred Learning Analytics.   Simon Knight is a Research Fellow in Writing Analytics at the  Connected Intelligence Centre, University of Technology Sydney.  His research focuses on the relationship of analytics to  epistemology, pedagogy and assessment, discourse analytics, and  epistemic cognition, particularly around information seeking,  work which has been presented at LAK and ICLS. He co-chaired  the ICLS14 Workshop on Learning Analytics for Learning and  Becoming in Practice and LAK15 Workshop on Temporal  Analyses of Learning Data.    Danielle McNamara is a Professor of Psychology at Arizona  State University, where she directs the Science of Learning and  Educational Technology Lab. Her research focuses on discovering  new methods to improve students' ability to understand text, learn  new information, and convey their thoughts in writing. Her work  integrates various approaches and methodologies including the  development of intelligent tutoring systems and the development  of natural language processing tools.    Laura Allen is a Doctoral Student in the Psychology Department  at Arizona State University. The overarching aim of her research  is to better understand the cognitive processes involved in  language comprehension, writing, knowledge acquisition, and  conceptual change, and to apply that understanding to educational  practice by developing and testing educational technologies. Her  research has been presented at LAK15 and other conferences  related to writing analytics.   Duygu Bektik is a Doctoral Student at the Knowledge Media  Institute, Open University UK. Her research investigates whether  computational techniques can automatically identify the attributes  of good academic writing in undergraduate student essays within  different disciplines; and if this proves possible, how best to  feedback actionable analytics to support educators in their essay  assessment processes, which has been presented at LAK14/15.   Scott Crossley is Associate Professor of Applied Linguistics at  Georgia State University. His primary research focus is on natural   language processing and the application of computational tools  and machine learning algorithms in language learning,  writing,  and text comprehensibility. His main interest area is the  development and use of natural language processing tools in  assessing writing quality and text difficulty. Professor Crossley  works as a senior researcher on Writing Pal, an intelligent tutoring  system under development at Arizona State University.    4. REFERENCES  The following represent the chairs research in this topic.   [1] Allen, L., Jacovina, M. and McNamara, D. in press.  Computer-based writing instruction. In C. A. MacArthur, S.  Graham, & J. Fitzgerald (Eds.), Handbook of Writing  Research. The Guilford Press: New York, NY.   [2] Allen, L., Snow, E. and McNamara, D. S. 2015. Are you  reading my mind Modeling students reading  comprehension skills with Natural Language Processing  techniques. In: 5th International Learning Analytics &  Knowledge Conference (Poughkeepsie, NY, USA, March 16  - 20, 2015). LAK15. ACM, New York, NY, 246-254. DOI=  http://dx.doi.org/10.1145/2723576.2723617    [3] Buckingham Shum, S., . Sndor, R. Goldsmith, X. Wang,  R. Bass and M. McWilliams (2016). Reflecting on reflective  writing analytics: assessment challenges and iterative  evaluation of a prototype tool. 6th International Learning  Analytics & Knowledge Conference, (Edinburgh, UK, April  25 - 29 2016). LAK16. ACM, New York, NY. DOI=  http://dx.doi.org/10.1145/2883851.2883955   [4] Knight, S. and K. Littleton (2015). Discourse-centric learning  analytics: mapping the terrain. Journal of Learning Analytics,  2 1, 185-209.   [5] Knight, S., S. Buckingham Shum and K. Littleton (2014).  Epistemology, assessment, pedagogy: where learning meets  analytics in the middle space. Journal of Learning  Analytics, 1, 2, 23-47.   [6] McNamara, D., Crossley, S., Roscoe, R., Allen, L. and Dai,  J. 2015. A hierarchical classification approach to automated  essay scoring. Assessing Writing, 23 (Jan. 2015), 35-59.  DOI= http://dx.doi.org/10.1016/j.asw.2014.09.002    [7] McNamara, D., Graesser, A., McCarthy, P. and Cai, Z.  (2014). Automated evaluation of text and discourse with  Coh-Metrix. Cambridge: Cambridge University Press.   [8] Roscoe, R.D., Varner, L.K., Crossley, S.A. and McNamara,  D.S. (2013). Developing pedagogically-guided algorithms  for intelligent writing feedback. International Journal of  Learning Technology, 8, 4, 362-381. DOI=  http://dx.doi.org/10.1504/IJLT.2013.059131    [9] Roscoe, R., Varner, L., Weston, J., Crossley, S. and  McNamara, D. (2014). The Writing Pal Intelligent Tutoring  System: Usability Testing and Development. Computers and  Composition, 34 (Dec. 2014), 39-59. DOI=  http://dx.doi.org/10.1016/j.compcom.2014.09.002    [10] Simsek, D., Buckingham Shum, S, Sndor, , De Liddo, A.,  and Ferguson, R. (2013) XIP Dashboard: Visual Analytics  from Automated Rhetorical Parsing of Scientific  Metadiscourse. 1st Int. Workshop on Discourse-Centric  Learning Analytics, LAK13. Leuven (Apr. 8-12, 2013).    [11] Simsek, D., Sandor, ., Buckingham Shum, S., Ferguson, R.,  De Liddo, A. and Whitelock, D. (2015). Correlations  between automated rhetorical analysis and tutors grades on  student essays. In: 5th International Learning Analytics &  Knowledge Conference (Poughkeepsie, NY, USA, March 16  - 20, 2015). LAK15. ACM, New York, NY, 355-359. DOI=  http://dx.doi.org/10.1145/2723576.2723603    "}
{"index":{"_id":"61"}}
{"datatype":"inproceedings","key":"Ley:2016:LAW:2883851.2883860","author":"Ley, Tobias and Klamma, Ralf and Lindstaedt, Stefanie and Wild, Fridolin","title":"Learning Analytics for Workplace and Professional Learning","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"484--485","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883860","doi":"10.1145/2883851.2883860","acmid":"2883860","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, workplace learning","abstract":"Recognizing the need for addressing the rather fragmented character of research in this field, we have held a workshop on learning analytics for workplace and professional learning at the Learning Analytics and Knowledge (LAK) Conference. The workshop has taken a broad perspective, encompassing approaches from a number of previous traditions, such as adaptive learning, professional online communities, workplace learning and performance analytics. Being co-located with the LAK conference has provided an ideal venue for addressing common challenges and for benefiting from the strong research on learning analytics in other sectors that LAK has established. Learning Analytics for Workplace and Professional Learning is now on the research agenda of several ongoing EU projects, and therefore a number of follow-up activities are planned for strengthening integration in this emerging field.","pdf":"Learning Analytics for Workplace and   Professional Learning   Tobias Ley  Tallinn University   Center for Educational Technology  Narva Mnt 29, 10120 Tallinn, Estonia   tley@tlu.ee     Stefanie Lindstaedt  Graz University of Technology   Knowledge Technologies Institute  Inffeldgasse 22, 8010 Graz, Austria   slind@tugraz.at    Ralf Klamma  RWTH Aachen University   Advanced Community Information Sytems  Ahornstr. 55, 52056 Aachen, Germany  klamma@dbis.rwth-aachen.de     Fridolin Wild   Oxford Brookes University  Oxford OX33 1HX  United Kingdom   fridolin.wild@gmail.com       ABSTRACT  Recognizing the need for addressing the rather fragmented  character of research in this field, we have held a workshop on  learning analytics for workplace and professional learning at the  Learning Analytics and Knowledge (LAK) Conference. The  workshop has taken a broad perspective, encompassing  approaches from a number of previous traditions, such as adaptive  learning, professional online communities, workplace learning  and performance analytics. Being co-located with the LAK  conference has provided an ideal venue for addressing common  challenges and for benefiting from the strong research on learning  analytics in other sectors that LAK has established. Learning  Analytics for Workplace and Professional Learning is now on the  research agenda of several ongoing EU projects, and therefore a  number of follow-up activities are planned for strengthening  integration in this emerging field.   CCS Concepts  Applied computingEducation   Keywords  Learning Analytics; Workplace Learning   1. INTRODUCTION  While learning analytics for all types of educational settings have  been striving in the past years [7], analytics for workplace  learning has been much less in the focus of the learning analytics  community. While Learning Analytics in educational settings very  often follow a particular pedagogical design, workplace learning  is much more driven by demands of work tasks or intrinsic  interests of the learner, by self-directed exploration and social      exchange that is tightly connected to processes and the places of  work [5]. Hence, learning interactions are to a large extent not  embedded into a pedagogical scenario. At the same time,  workplace learners can benefit from being exposed to their own  and others learning processes and outcomes as this potentially  allows for better awareness and tracing of learning, sharing  experiences, and scaling informal practices.   Recently, several different approaches to learning analytics in the  workplace have been suggested. Some of these have been coming  from the tradition of adaptive learning systems [4][6] or self- directed learning environments [9][10] for workplace learning or  lifelong learning [8], some from learning in professional  communities [1][3]. Recently, the topic of performance analytics  or analytics in smart industries has extended the focus to more  traditional work settings [11]. Yet, up to this point, the research  has been rather fragmented. New research challenges also abound  in workplace scenarios, such as the introduction of new  technologies (augmented interfaces, large scale collaboration  platforms), or the new challenges that derive from the need to  make informal learning processes better traceable and  recognizable.  We recognize that workplace learning scenarios can benefit from  existing research in education-based learning analytics approaches  and technologies. At the same time, we are convinced that the  community would benefit from a closer exchange around the  specificities of workplace learning, such as the unconstrained and  less plannable learning processes, the challenge to integrate  learning systems in work practices, or a methodological focus on  design-oriented research approaches with smaller samples in real  life settings.   At the same time, we think that researchers in the educational  domain can benefit from this workshop at LAK, as the clear  boundaries between formal and informal learning are increasingly  vanishing, and a focus on lifelong learning is increasingly being  established [8].   2. OBJECTIVES  The objective of this workshop is therefore to provide a forum for  researchers in the area of learning analytics who specifically  address learning at the workplace or in professional settings in  different forms and flavors. This includes on the job training, self- directed informal learning, community-based learning or   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.  Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883860      continued professional education. We also particularly encourage  submissions that use learning analytics for enhancing the  integration of formal and informal learning process for lifelong  learning or workplace performance.  The LAK conference is a particularly well suited venue for this  workshop as several researchers in workplace learning have been  members of the LAK community, but a common focus point to  identify synergies and reflect about the state of research has up to  now been missing. Learning Analytics for Workplace and  Professional Learning is now on the research agenda of several  ongoing EU projects, and therefore the workshop provides a  timely mechanism for strengthening integration in this emerging  field.   3. TARGET GROUP  We expect researchers and professionals in the training or HR  sectors that are particularly focusing on workplace learning,  professional learning or performance support and are starting to  recognize the value of data-driven and evidence-based  interventions. We are also targeting researchers developing  learning analytics applications that have been tested in informal  work related settings, or those from the educational domain who  are starting to apply their approaches in more informal settings.  We take a broad view of workplace or professional learning which  could encompass all of the following    Informal self-directed forms of competence  development or performance support at the workplace    ad hoc and on the job training    learning in professional communities    collaborative learning and problem solving at the  workplace    continued professional education     connections of individual and organizational learning   4. ACKNOWLEDGMENTS  Updates on this workshop can be found at http://learning- layers.eu/laforwork/. The workshop is supported by EU-FP7  projects Learning Layers, TELL-ME and MIRROR, and H2020  projects WEKIT and CEITER.   5. REFERENCES  [1] De Laat, M., & Schreurs, B. (2013). Visualizing Informal   Professional Development Networks: Building a Case for  Learning Analytics in the Workplace. American Behavioral  Scientist, 57(10), 14211438.   [2] Drachsler, H., Verbert, K., Manouselis, N., Vuorikari, R.,  Wolpers, M., & Lindstaedt, S. (2012). Preface [Special issue   on dataTELData Supported Research in Technology- Enhanced Learning].   [3] R. Klamma. Community learning analytics: challenges and  opportunities. In Proceedings of Advances in Web-Based  Learning (ICWL), pages 284 - 293, Kenting, Taiwan, 2013.  Springer.   [4] B. Kump, C. Seifert, G. Beham, S. Lindstaedt, and T. Ley.  Seeing What the System Thinks You Know -Visualizing  Evidence in an Open Learner Model. In Proceedings of the  2nd International Conference on Learning Analytics and  Knowledge (LAK 12), pages 153-157, Vancouver, Canada,  2012. ACM.   [5] Ley, T., Cook, J., Dennerlein, S., Kravcik, M., Kunzmann,  C., Pata, K., Trattner, C. (2014). Scaling informal learning at  the workplace: A model and four designs from a large-scale  design-based research effort. British Journal of Educational  Technology, 45(6), 10361048. doi:10.1111/bjet.12197   [6] T. Ley and B. Kump. Which User Interactions Predict Levels  of Expertise in Work-integrated Learning In D. Hernandez- Leo, T. Ley, R. Klamma, and A. Harrer, editors, Proceedings  of the 8th European Conference on Technology-enhanced  Learning (EC-TEL 2013), pages 178-190, Paphos, Cyprus,  2013.   [7] N. Nistor, M. Derntl, and R. Klamma. Learning Analytics:  Trends and Issues of the Empirical Research of the Years  2011-2014. In Proceedings of the 10th European Conference  on Technology Enhanced Learning (EC-TEL 2015), pages  453 - 459, Toledo, Spain, 2015.   [8] Manh Pham, Raf Klamma; Data Warehousing for Lifelong  Learning Analytics, Bulletin of the Technical Committee on  Learning Technology, Volume 15, Issue 2, April 2013, 6-9.   [9] Dominik Renzel, Ralf Klamma, Milos Kravcik et al. (2015)  Tracing Self-Regulated Learning in Responsive Open  Learning Environments. ICWL 2015: 14th International  Conference, Guangzhou, China, November 5-8, 2015,  Proceedings. Springer, Berlin-Heidelberg, pp 155164   [10] Dominik Renzel, Ralf Klamma: From Micro to Macro:  Analyzing Activity in the ROLE Sandbox, Proceedings of  the Third Conference on Learning Analytics and Knowledge,  LAK'13, Leuven, Belgium, April 8-12, 2013   [11] Wild, F., Scott, P., Lefrere, P., Karjalainen, J., Helin, K.,  Naeve, A., & Isaksson, E. (2014). Towards data exchange  formats for learning experiences in manufacturing  workplaces. In CEUR Workshop Proceedings (Vol. 1238, pp.  23-33).           "}
{"index":{"_id":"62"}}
{"datatype":"inproceedings","key":"Martinez-Maldonado:2016:CLA:2883851.2883855","author":"Martinez-Maldonado, Roberto and Hernandez-Leo, Davinia and Pardo, Abelardo and Suthers, Dan and Kitto, Kirsty and Charleer, Sven and Aljohani, Naif Radi and Ogata, Hiroaki","title":"Cross-LAK: Learning Analytics Across Physical and Digital Spaces","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"486--487","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883855","doi":"10.1145/2883851.2883855","acmid":"2883855","publisher":"ACM","address":"New York, NY, USA","keywords":"integration, learning analytics, monitoring, seamless learning","abstract":"It is of high relevance to the LAK community to explore blended learning scenarios where students can interact at diverse digital and physical learning spaces. This workshop aims to gather the sub-community of LAK researchers, learning scientists and researchers from other communities, interested in ubiquitous, mobile and/or face-to-face learning analytics. An overarching concern is how to integrate and coordinate learning analytics to provide continued support to learning across digital and physical spaces. The goals of the workshop are to share approaches and identify a set of guidelines to design and connect Learning Analytics solutions according to the pedagogical needs and contextual constraints to provide support across digital and physical learning spaces.","pdf":"Cross-LAK: Learning Analytics Across   Physical and Digital Spaces   Roberto Martinez-Maldonado  University of Technology Sydney,   NSW Australia  Roberto.Martinez-  Maldonado@uts.edu.au     Dan Suthers  University of Hawaii at Manoa, USA   Suthers@hawaii.edu     Naif Radi Aljohani  King Abdulaziz University, Saudi   Arabia  Nraljohani@kau.edu.sa   Davinia Hernandez-Leo  Universitat Pompeu Fabra,   Barcelona, Spain  Davinia.Hernandez   @upf.edu     Kirsty Kitto  Queensland University of   Technology, Brisbane, Australia  Kirsty.Kitto@qut.edu.au   Abelardo Pardo  The University of Sydney, NSW   Australia  Abelardo.Pardo  @sydney.edu.au     Sven Charleer   KU Leuven, Belgium  Sven.Charleer@kuleuven.be     Hiroaki Ogata   Kyushu University, Japan  Hiroaki.Ogata@gmail.com      ABSTRACT  It is of high relevance to the LAK community to explore  blended learning scenarios where students can interact at  diverse digital and physical learning spaces. This  workshop aims to gather the sub-community of LAK  researchers, learning scientists and researchers from other  communities, interested in ubiquitous, mobile and/or face- to-face learning analytics. An overarching concern is how  to integrate and coordinate learning analytics to provide  continued support to learning across digital and physical  spaces. The goals of the workshop are to share approaches  and identify a set of guidelines to design and connect  Learning Analytics solutions according to the pedagogical  needs and contextual constraints to provide support across  digital and physical learning spaces.    CCS Concepts    Information systems ! Information systems applications  ! Collaborative and social computing systems and tools    Keywords  Learning analytics, seamless learning, integration, monitoring    1. INTRODUCTION AND MOTIVATION  Students learning commonly occurs in spaces and at moments  that go beyond formal education, and this learning is not  constrained to a single physical or digital environment [5, 10].  Research in the learning sciences (LS), computer-supported  collaborative learning (CSCL) and technology-enhanced  learning (TEL) has revealed the pedagogical benefits of letting   students experience different types of content,  real world   challenges, and physical and social interactions with educators  or other learners [2, 8]. Increasing access to emerging  communication technologies have made it possible for students  to make use of a wide range of devices and educational (and  non-educational) software applications. At the same time,  educational providers, including schools and universities,  deploy a variety of educational technologies and pedagogical  resources in both online and face-to-face settings [9]. These  technologies allow learners to get remote access to educational  resources from different physical spaces (ubiquitous learning  support) or to enrich their learning experiences in the classroom  in ways that were not previously possible (face-to-face learning  support). In short, there is an increasing interest in providing  support for students learning across physical and digital spaces.   It is of high relevance to the LAK community to explore  students activity in blended learning scenarios where they can  interact at diverse digital and physical learning spaces. The  challenge is to find the best approaches that can be applied to  automatically capture traces of students activity, and understand  how learning analytics techniques can be used in this context to  exploit these (often) heterogeneous data.  This workshop aims to  gather the sub-community of LAK researchers interested in  ubiquitous, mobile and/or face-to-face learning analytics in  conjunction with learning scientists and researchers from other  communities who have explored the perspective of learning  across digital and physical spaces. This aim is aligned with the  theme of LAK 2016 by focusing on the Convergence of  Communities for Grounding and Implementation of Learning  Analytics solutions across physical and digital spaces.   Providing continued support in the classroom, for mobile  experiences and using web-based systems has been explored to  different extents and each poses its own challenges [9, 11]. An  overarching concern is how to integrate analytics across all  those three spaces in a coordinated way. The goal of the  workshop is to share approaches and exchange information  about how educational data science can be brought to bear to  provide continued support across varied spatiotemporal learning  situations and identify a set of guidelines to design and connect  Learning Analytics solutions according to the pedagogical needs  and contextual constraints of practitioners.    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  http://dx.doi.org/10.1145/2883851.2883855       2. WORKSHOP THEMES  The workshop will focus on the following four themes:    1. Learning analytics across digital spaces (e.g. [4]).   Examples of application of learning analytics in educational  settings where multiple digital technologies (digital spaces,  learning environments, tools such as  e-books, learning  management systems, e-portfolio systems, social network  platforms, intelligent tutoring systems, etc), connected or  disconnected, are used to facilitate different learning  activities.   2. Learning analytics bridging physical (and digital) spaces  (e.g. [6]). Examples of application of learning analytics in  educational settings with an significant face-to- face component (e.g. the classroom, the computer lab,  collocated experimental settings, etc), a blend of collocated  face-to-face and remote learning activities, or experimental  approaches to gather collaborative students data while  interacting with various systems.    3. Mobile and ubiquitous learning analytics (e.g. [1, 7]).  Examples of application of learning analytics in ubiquitous  educational settings with an important component of  students mobility and where the learning activity spans  across various physical and digital spaces.   4. Data integration of heterogeneous learning data sources  (e.g. refer to [3, 4]). Examples learning analytics solutions to  collect, gather and synchronise students activity data  coming from varied heterogeneous data sources.    3. EXPECTED OUTCOMES  The expected outcomes of the workshop are the following:     1. Definition of the research gap. This workshop also will aim  to help identify still unsolved challenges and future research  lines in Learning Analytics across Spaces and set grounding  for possible joint research efforts.   2. Guidelines or principles for R&D. The workshop will bring  together the sub-communities within CSCL/LS, TEL, and  LAK with the goal of contributing with expert  guidelines/principles that can help guide future research and  development to create learning analytics and monitoring  tools that can provide support for each of the four themes  posed above.    3. Dissemination of R&D of LA across spaces. All the  outcomes of the workshop, including the outcomes listed  above and the papers submitted, will be made available  through the workshops own website or an online wiki so  other members of the LAK community can benefit and  further contribute to the design space.   4. CONCLUSION  While this workshop can be considered to be grounded on a  consolidated line interest on the topic of learning across spaces,  in this case the focus is on the particular challenges to provide  continued support to students by using learning analytics  techniques. More detailed information about the workshop  organisers, authors, program and outcomes of the workshop can  be consulted in the workshop website1.                                                                      1 Cross-LAK 2016 - https://sites.google.com/site/crosslak2016   5. ACKNOWLEDGEMENTS   Particular workshop organisers are partially supported by  the Spanish Ministry of Economy and Competitiveness  (TIN2014-53199-C3-3-R), the Erasmus+ programme, and Key  Action 2 Strategic Partnerships of the European Union under  grant agreement 2015-1-UK01-KA203-013767   ABLE  project.   6. REFERENCES   [1] Aljohani, N. R. and Davis, H. C. 2012. Learning analytics in   mobile and ubiquitous learning environments. In  Proceedings of the World Conf. on Mobile and Contextual  Learning (Helsinki, Finland, 16-18 October 2013). mLearn  '12. NY: ACM, 70-77   [2] Delgado Kloos, C., Hernndez-Leo, D., and Asensio-Prez,  J.I. 2012. Technology for Learning across Physical and  Virtual Spaces. Journal of Universal Computer Science, 18,  15, 2093-2096.   [3] Ellis, C. 2013. Broadening the scope and increasing the  usefulness of learning analytics: The case for assessment  analytics. BJET, 44, 4, 662-664. DOI= 10.1111/bjet.12028   [4] Kitto, K., Cross, S., Waters, Z., and Lupton, M. 2015.  Learning analytics beyond the LMS: the connected learning  analytics toolkit. In Proceedings of the Int. Conf. on  Learning Analytics and Knowledge (Poughkeepsie, USA,  March 16-20). LAK '15. NY: ACM, 11-15. DOI=  10.1145/2723576.2723627   [5] Looi, C. K., Wong, L. H., & Milrad, M. 2015. Guest  Editorial: Special Issue on Seamless, Ubiquitous, and  Contextual Learning. IEEE TLT, 1, 2-4. DOI=  10.1109/TLT.2014.2387455   [6] Martinez-Maldonado, R., Clayphan, A., Yacef, K., and  Kay, J. 2015. MTFeedback: providing notifications to  enhance teacher awareness of small group work in the  classroom. IEEE TLT, 8, 2 (Jun 2015), 187-200.  DOI=10.1109/tlt.2014.2365027   [7] Melero, J., Hernndez-Leo, D., Sun, J., Santos, P., Blat,  J. 2015. How was the activity A data visualization  support for a case of location-based learning design,  BJET, 46, 2, 317-329. DOI= 10.1111/bjet.12238   [8] Prez-Sanagustn, M.; Ramrez-Gonzlez, G.;  Hernndez-Leo, D.; Muoz-Organiero, M.; Santos, P.;  Blat, J.; Delgado-Kloos, C. 2012. Discovering the  campus together: a mobile and computer-based learning  experience. Journal of Network and Computer  Applications, 35, 1. 176-188.  DOI=10.1016/j.jnca.2011.02.011   [9]  Rogers, Y. 2008. Using external visualizations to extend  and integrate learning in mobile and classroom settings. In  Visualization: Theory and practice in science education.  J.K. Gilbert, M. Reinder and M. Nakhleh Eds. Springer:  Netherlands, 89-102. DOI= 10.1007/978-1-4020-5267-5_5   [10] Sharples, M. and Roschelle, J. 2010. Guest editorial:  Special section on mobile and ubiquitous technologies for  learning. IEEE TLT, 1, 4-6.  DOI=10.1109/TCAD.2015.2410671   [11] Wang, M., Shen, R., Novak, D., & Pan., X. 2009. The  impact of mobile learning on students learning behaviours  and performance: Report from a large blended classroom.  BJET, 40, 4, 673-695. DOI=10.1111/j.1467- 8535.2008.00846.x        "}
{"index":{"_id":"63"}}
{"datatype":"inproceedings","key":"Drachsler:2016:EPI:2883851.2883933","author":"Drachsler, Hendrik and Hoel, Tore and Cooper, Adam and Kismih'ok, G'abor and Berg, Alan and Scheffel, Maren and Chen, Weiqin and Ferguson, Rebecca","title":"Ethical and Privacy Issues in the Design of Learning Analytics Applications","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"492--493","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883933","doi":"10.1145/2883851.2883933","acmid":"2883933","publisher":"ACM","address":"New York, NY, USA","keywords":"data ownership, ethics, learning analytics, legal rights, privacy, surveillance","abstract":"Issues related to Ethics and Privacy have become a major stumbling block in application of Learning Analytics technologies on a large scale. Recently, the learning analytics community at large has more actively addressed the EP4LA issues, and we are now starting to see learning analytics solutions that are designed not only as an afterthought, but also with these issues in mind. The 2nd EP4LA","pdf":"Ethical and Privacy Issues   in the Design of Learning Analytics Applications   Hendrik Drachsler  Welten Institute   Open University of the Netherlands  hendrik.drachsler@ou.nl     Gbor Kismihk   Center of Job Knowledge Research  Amsterdam Business School   University of Amsterdam  G.Kismihok@uva.nl     Weiqin Chen   Oslo and Akershus University College  of Applied Sciences   Oslo, Norway  Weiqin.Chen@hioa.no      Tore Hoel  Oslo and Akershus University College   of Applied Sciences  Oslo, Norway   tore.hoel@hioa.no     Alan Berg  ICT Services   University of Amsterdam  a.m.berg@uva.nl                Adam Cooper  CETIS   United Kingdom  adam@cetis.org.uk     Maren Scheffel   Welten Institute  Open University of the Netherlands   Maren.Scheffel@ou.nl     Rebecca Ferguson  Institute of Educational Technology,   The Open University, UK  rebecca.ferguson@open.ac.uk        ABSTRACT  Issues related to Ethics and Privacy have become a major  stumbling block in application of Learning Analytics technologies  on a large scale. Recently, the learning analytics community at  large has more actively addressed the EP4LA issues, and we are  now starting to see learning analytics solutions that are designed  not only as an afterthought, but also with these issues in mind.  The 2nd EP4LA@LAK16 workshop will bring the discussion on  ethics and privacy for learning analytics to a the next level,  helping to build an agenda for organizational and technical design  of LA solutions, addressing the different processes of a learning  analytics workflow.   CCS Concepts  Security and privacy  Privacy protections,   General and reference  Design     Security and privacy  Social aspects of security and privacy     Security and privacy  Privacy protections     Applied computing  E-learning   General Terms  Algorithms, Measurement, Design, Human Factors, Legal Aspects   Keywords  Learning analytics, ethics, privacy, legal rights, data ownership,  surveillance     1. INTRODUCTION  Since the Ethics & Privacy for Learning Analytics (EP4LA)   workshop at LAK15 it has become more and more clear that  Ethics and Privacy issues play a major role in preparing for large- scale adoption of learning analytics. Cases of moral panics are  still used to create attention to these important issues  the  infamous inBloom case in USA [1], and the less known Stiching  Snapped case in the Netherlands [2] are both cases in point. Now  we start to see that ethics and privacy are not only an afterthought  when something has gone wrong; it starts to be built into the new  designs from the very beginning. This workshop will focus on  how ethics and privacy concerns would form the basis, on which  technical architectures and educational practices will be built.   Literature review for the EP4LA@LAK15 workshop found few  papers published related to ethics and privacy in the research field  known as learning analytics, and even fewer policies or  guidelines regarding privacy, legal protection rights or other  ethical implications [3]. Now a special issue of Journal of  Learning Analytics on ethics and privacy is in press, with a  number of articles originating from the LAK workshop [4].  Furthermore, projects funded to do research and developments  within the field of learning analytics now have deliverables  addressing ethics and privacy [5].      2. ETHICS & PRIVACY IN APPLICATION  DESIGN  Another recent trend is implementation of Codes of Practice for  Learning Analytics. JISC, a charitable organization supporting the  use of digital technologies in UK education and research,  supported by LACE and other European projects has developed a  Code of Practice covering the main issues institutions need to  address in order to progress ethically and in compliance with the  law [6]. The first university to establish an institutional Code of  Practice for Learning Analytics was the Open University, UK [7].   A Code of Practice is only the first step in developing a new  ecosystem for learning analytics that allows large-scale adoption.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. Copyrights for components  of this work owned by others than the author(s) must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to post  on servers or to redistribute to lists, requires prior specific permission and/or  a fee. Request permissions from Permissions@acm.org.   LAK '16, April 25 - 29, 2016, Edinburgh, United Kingdom   Copyright is held by the owner/author(s). Publication rights licensed to  ACM.   ACM 978-1-4503-4190-5/16/04$15.00   DOI: http://dx.doi.org/10.1145/2883851.2883933     Open architectures are needed that provide technical solutions to  the challenges of data ownership, consent to use, and ethical  management and stewardship of the data. We are now starting to  see these architectures emerge [8].   The Workshop at LAK16 will take stock of current ethics and  privacy-led design for learning analytics and contribute to an  agenda for large-scale implementations that goes beyond mere  policy guidelines.   3. WORKSHOP DESCRIPTION  3.1 Overall Motivation  The Workshop will develop an agenda for EP4LA-based design  of organizational and technical infrastructures based on the recent  awareness of the importance of ethical and privacy issues related  to big data. This years workshop will build on and strengthen the  impact of the EP4LA@LAK15 workshop and drive the discussion  in the direction of actions that could be implemented in  educational cultures and technologies.      3.2 Workshop Objectives  The Workshop builds on the series of EP4LA workshops1, which  for the one in LAK15 gathered around 50 participants discussing  five papers. The target audience for the Workshop this year is  LAK participants, both managers and developers, who want to see  EP4LA crystalized as a set of requirements and blueprints for  organizational and technical designs that could be implemented in  different sectors of the educational ecosystem. Following the path  of data sharing through the different processes of a LA system,  e.g., Learning Activity, Data Collection, Data Storing and  Processing, Analyzing, Visualization, and Feedback Actions  (Figure1), the objective of the Workshop is to give the  participants input to build an agenda for ethics and privacy for all  these processes.     Figure 1: Processes in a Learning Analytics cycle      3.3 Workshop Organization  A Call for Papers will be issued outlining the objectives of the  Workshop. The organizers are confident that they have a  community outreach that would result in paper submission  covering the full range of the LA ecosystem. The 3 hours  workshop will be organized as both paper presentation and  discussions along the lines of the LA process model.      3.4 Workshop Facilitators   The workshop will be organized jointly by the FP7 EU LACE  project (http://www.laceproject.eu), the Apereo Foundation, the  Asian-Pacific Society of Computers in Education (APSCE) SIG  on Learning Analytics  (https://sites.google.com/site/  apscesiglaedm/) and the European Association for Technology  Enhanced Learning (EATEL) SIG dataTEL (http://ea-tel.eu/sig-                                                                    1 http://www.laceproject.eu/ethics-privacy-learning-analytics/   datatel/). All partners aim at advancing the learning analytics field  by coming up with practical solutions and guidelines for ethical &  privacy issues that are a critical part of most data-driven research  in Education. The main goals are to increase the knowledge and  awareness about ethical and privacy boundaries of Learning  Analytics research and practice, to identify existing theories of  trust and privacy, to promote the re-use of best practice solutions  on privacy and ethics, to foster the cooperation between different  Learning Analytics research units, and to develop a kind of code  of honor for Learning Analytics research supported by IT based  legal tools.       4. REFERENCES  [1] New York Times. 2014. InBloom student data reporitory to   close. News story 2014-04-21, online at  http://bits.blogs.nytimes.com/2014/04/21/inbloom-student- data-repository-to-close, retrived 2015-01-15   [2] CBP. 2014. College bescherming persoonsgegevens  Onderzoek. CBP naar de verwerking van persoonsgegevens  door Snappet Rapport definitieve bevindingen van 14 juli  2014 met corrigendum van 27 augustus 2014 Juli 2014  Online at  https://cbpweb.nl/sites/default/files/downloads/mijn_privacy/ rap_2013_snappet.pdf, accessed 2015-10-30   [3] Ethical and Privacy Issues in the Application of Learning  Analytics, LAK15, March, 2015, Poughkeepsie, NY, USA.   [4] Drachsler, H.,  Ferguson, R.,  Hoel, T., & Scheffel, M (eds.)  In press. Special issue on Ethics and Privacy in Learning  analytics of Journal of Learning Analytics   [5] Steiner, C., Masci, D., Johnson, M., Trker, A., Drnek, M.,   & Kickmeier-Rust, M. 2014. Privacy and Data Protection  Policy. Deliverable D2.3 from LEAs BOX project. Online at  http://css-kmi.tugraz.at/mkrwww/leas- box/downloads/D2.3.pdf, accessed 2015-10-30   [6] Sclater, N., & Bailey, P. 2015. Code of Practice for Learning  Analytics. Jisc. Available at  http://www.jisc.ac.uk/guides/code-of-practice-for-learning- analytics   [7] Open University. 2014. Policy on Ethical Use of Student  Data for Learning Analytics. Available at  http://www.open.ac.uk/students/charter/essential- documents/ethical-use-student-data-learning analytics-policy   [8] Sclater, N. 2015. Jiscs Learning Analytics Architecture   whos involved, what are the products and when will it be  available Available at  http://analytics.jiscinvolve.org/wp/2015/06/15/jiscs-learning- analytics-architecture-whos-involved-what-are-the-products- and-when-will-it-be-available/         Learning Activity  Data Collection  Data Storing &  Processing Analyzing Visualization  Feedback Actions    "}
{"index":{"_id":"64"}}
{"datatype":"inproceedings","key":"Greer:2016:LAC:2883851.2883899","author":"Greer, Jim and Molinaro, Marco and Ochoa, Xavier and McKay, Timothy","title":"Learning Analytics for Curriculum and Program Quality Improvement (PCLA 2016)","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"494--495","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883899","doi":"10.1145/2883851.2883899","acmid":"2883899","publisher":"ACM","address":"New York, NY, USA","keywords":"ACM proceedings, LATEX, text tagging","abstract":"This workshop on Learning Analytics for Curriculum and Program Quality Improvement investigates how LAK can drive improvements in teaching practices, instructional and curricular design, and academic program delivery. This workshop brings forward research and examples of how LAK can help build the case for instructional, curricular, or programmatic change and further how LAK can be used to foster acceptance of change processes by teachers, administrators, and other stakeholders in the educational enterprise.","pdf":"Learning Analytics for Curriculum and Program Quality  Improvement (PCLA 2016)  Jim Greer University of Saskatchewan  E232 Administration University of Saskatchewan  Saskatoon, Canada jim.greer@usask.ca  Marco Molinaro University of California, Davis  3470 Trousdale Parkway, WPH-601A  Davis, CA, USA mmolinaro@ucdavis.edu  Xavier Ochoa Escuela Superior Politcnica  del Litoral Va Perimetral Km. 30.5  Guayaquil, Ecuador xavier@cti.espol.edu.ec  Timothy McKay University of Michigan  450 Church Street Ann Harbor, MI, USA  tamckay@umich.edu  ABSTRACT This workshop on Learning Analytics for Curriculum and Program Quality Improvement investigates how LAK can drive improvements in teaching practices, instructional and curricular design, and academic program delivery. This work- shop brings forward research and examples of how LAK can help build the case for instructional, curricular, or program- matic change and further how LAK can be used to foster acceptance of change processes by teachers, administrators, and other stakeholders in the educational enterprise.  CCS Concepts Applied computing ! Education;  Keywords ACM proceedings; LATEX; text tagging  1. MOTIVATION Much of the research in LAK to date has been aAIJs-  tudent facingaAI, that is, using data to better understand learners and their need or to create interventions that di- rectly support or influence learners. This workshop takes the perspective on how LAK can drive improvements in teach- ing practices, instructional and curricular design, and aca- demic program delivery. While this does influence student outcomes in the long term, the data gathered and evidence generated is more instructor and administrator facing. We have seen examples of how LAK can help build the case for instructional, curricular, or programmatic change and fur-  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 16 April 25-29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883899  ther how LAK can be used to foster acceptance of change processes by teachers, administrators, and other stakehold- ers in the educational enterprise. When successful, these kinds of changes are often associated with educational re- form or culture shifts in educational practice.  2. OBJECTIVES This workshop oers those in the LAK community an op-  portunity to share and explore how educational data, its analysis and visualization, and the evidence derived can change/improve the context of learning. The main research and practice questions to be addressed in this workshop are:  1. how to provide relevant and actionable information to faculty, teaching assistants, departmental and col- lege administrators to encourage a greater emphasis on student learning and use of evidence-based practices, thus encouraging a continuous improvement approach to teaching and learning.  2. how to create visualization and data collection tools and approaches that encourage a community of in- structors and administrators to engage in making deci- sions based on data to improve student learning whether at the activity, lesson, course, series, department, col- lege or university-wide levels.  3. how to extract information from the multiple modali- ties used in instructional environments and help cap- ture, represent and evaluate faculty instructional ap- proaches, student-faculty engagement, student-student interactions and student-technology interactions.  4. how to represent, summarize and mobilize data from human interactions and student-technology interactions to motivate change and quality improvement. Is there a way to make the data and representations more use- ful for promoting sustainable change  5. how to change the evaluation of instructional activities to be more formative, actionable and multi-dimensional    in nature, emphasizing individual and group improve- ment rather than a one-size fits all student survey by which instructors or courses are judged and compared. Ideally such evaluation systems would go well beyond student satisfaction as the sole measure of good teach- ing and should include learning outcomes, utility of outcomes, applicability to future learning, match or fit between learner and instructor, and more.  This is the first time that a workshop focused on Learn- ing Analytics for Curriculum and Program Improvement has been held.  3. DISSEMINATION STRATEGY Whether in face-to-face, blended, or fully online instruc-  tional situations, learning analytics for instructional and program improvement makes sense. Researchers, practition- ers, faculty and administrators interested in improvement in learning environments would be interested in this workshop. This workshop invites members of the LAK community in- terested in this topic and as well as those interested in data- informed approaches to institutional change in teaching and learning. The proceedings from this workshop, a collection of peer-reviewed papers, will appear in the CEUR Workshop Proceedings.  4. WORKSHOP FORMAT After peer review, approximately half a dozen papers were  accepted for presentation at this half-day workshop. The format follows a model of short presentations by authors of the papers followed by round-table smaller-group discussions of issues arising from these short presentations. A World- cafe of discussion on emerging themes will close the work- shop. We expect that an introduction to the proceedings will be written by the workshop organizers as a preamble to the published prodceedings.  5. PROGRAM COMMITTEE AND PARTIC- IPANTS  In addition to the four organizers, a program committee was established to provide peer reviews of the various pa- pers. Three reviews were invited for each submission.   Christopher Brooks, University of Michigan   Katherine Chiluiza, ESPOL, Ecuador   Cristian Cechinel, UFPel, Brazil   Stephanie Frost, University of Saskatchewan   Vive Kumar, Athabasca University   Phil Long, University of Texas at Austin   Leah McFayden, Unversity of British Columbia   Emily Miller, Association of American Universities   Christopher Pagliarulo, University of California, Davis   Craig Thompson, University of Saskatchewan   Catherine Uvarov, University of California, Davis   Katrien Verbert, KULeuven, Belgium  6. SYNOPSIS OF PAPERS PRESENTED The accepted papers all aim at improving decisions about  learning and learning environments based on data-driven methods. Some aim at providing enhanced information to instructors to better understand their students and how their teaching is impacting student learning [4] [5] [1]. Oth- ers are more directly aimed at instructional designers, to learn from successful design strategies in certain courses and to apply that knowledge to other course-design scenarios [2] [7]. The third type of paper is aimed at providing actionable information to administrators for better managing curricu- lum and program change [3] [6].  Issues include instructional designs that lead to optimal use of learning management systems, developing metrics for better analysing curricula, risk mitigation approaches to cur- ricular change, and visualization methods for improving de- cision making about academic programs. In order to ad- dress these types of issues, the data literacy of instructors, instructional designers, and administrators is a point of con- cern. It is important that users of program and curriculum learning analytics (PCLA) tools understand the capabili- ties and limitations of these tools and the outputs they pro- vide. Appropriate interpretations of statistical models or predictions are an important prerequisite to eective use of data-driven approaches. Poor statistical literacy and mis- understanding of probabilities can lead to bad decisions, systematic discrimination, and prejudicial abuse of learners. Along with powerful analytics tools, there is a responsibility to ensure that those using and interpreting the results from such tools are suitably trained to use them well.  The reference list below contains the the accepted work- shop papers, which will appear in the CEUR Proceedings.  7. REFERENCES [1] D. G. Blazenka Divjak and M. Maretic. Assessment  analytics for peer-assessment: A model and implementation. In Program and Curricular Learning Analytics Workshop, 2016.  [2] J. Fritz. Lms course design as learning analytics variable. In Program and Curricular Learning Analytics Workshop, 2016.  [3] J. Greer, C. Thompson, R. Banow, and S. Frost. Data-driven programmatic change at universities: What works and how. In Program and Curricular Learning Analytics Workshop, 2016.  [4] D. Y. Liu, C. E. T. a ndAdam J. Bridgeman, and K. Bartimote-Auick. Empowering instructors through customizable collection and analyses of actionable information. In Program and Curricular Learning Analytics Workshop, 2016.  [5] M. Molinaro, M. Steinwachs, Q. Li, and A. Guzman-Alvarez. Promoting action from instructors to departments via simple, actionable tools and analyses. In Program and Curricular Learning Analytics Workshop, 2016.  [6] X. Ochoa. Simple metrics for curriculum analytics. In Program and Curricular Learning Analytics Workshop, 2016.  [7] W. Y. Wong and M. Lavrencic. Using a risk management approach in analytics for curriculum and program quality improvement. In Program and Curricular Learning Analytics Workshop, 2016.    "}
{"index":{"_id":"65"}}
{"datatype":"inproceedings","key":"Bull:2016:LWL:2883851.2883852","author":"Bull, S. and Ginon, B. and Kay, J. and Kickmeier-Rust, M. and Johnson, M. D.","title":"LAL Workshop: Learning Analytics for Learners","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"496--497","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883852","doi":"10.1145/2883851.2883852","acmid":"2883852","publisher":"ACM","address":"New York, NY, USA","keywords":"dashboards, learning analytics for learners, learning data for learners, open learner models, visual learning analytics","abstract":"With the arrival of 'big data; in education, the potential was recognised for learning analytics to track students' learning, to reveal patterns in their learning, or to identify at-risk students, in addition to guiding reform and supporting educators in improving teaching and learning processes [1]. Learning Analytics dashboards have been used at all levels, including institutional, regional and national level [2]. In classroom use, while learning visualisations are often based on counts of activity data or interaction patterns, there is increasing recognition that learning analytics relate to learning, and should therefore provide pedagogically useful information [3]. While increasing numbers of technology-enhanced learning applications are embracing the potential of learning analytics at the classroom level, often these are aimed at teachers. However, learners can also benefit from learning analytics data (e.g. [4][5]).","pdf":"LAL Workshop: Learning Analytics for Learners  S. BULL   University College London, UK  s.bull@ucl.ac.uk   B. GINON  University of Birmingham, UK   b.ginon.1@bham.ac.uk   J. KAY   University of Sydney, Australia    judy.kay@sydney.edu.au   M. KICKMEIER-RUST  Technische Universitt Graz, Austria   michael.kickmeier-rust@tugraz.at   M.D. JOHNSON  University of Birmingham, UK  M.D.Johnson.1@bham.ac.uk     CCS Concepts   Human-centered computing Human computer interaction  (HCI); user model, user studies Visualization; Visual ana- lytics; Visualization systems and tools.   Keywords  Visual Learning Analytics; Dashboards; Open Learner Models;  Learning Analytics for Learners; Learning data for learners.   1. MOTIVATION  With the arrival of big data in education, the potential was  recognised for learning analytics to track students learning, to  reveal patterns in their learning, or to identify at-risk students, in  addition to guiding reform and supporting educators in improv- ing teaching and learning processes [1]. Learning Analytics  dashboards have been used at all levels, including institutional,  regional and national level [2]. In classroom use, while learning  visualisations are often based on counts of activity data or inter- action patterns, there is increasing recognition that learning  analytics relate to learning, and should therefore provide peda- gogically useful information [3]. While increasing numbers of  technology-enhanced learning applications are embracing the  potential of learning analytics at the classroom level, often these  are aimed at teachers. However, learners can also benefit from  learning analytics data (e.g. [4][5]).   Open learner models have long been showing learners infor- mation about their learning, often with the aim of encouraging  metacognitive behaviours such as reflection, planning, self- assessment and self-directed learning [6]. Benefits of showing  learning data to learners for such purposes are now also being  investigated in learning analytics (e.g. [7][8]). Nevertheless,  despite a few exceptions, there is limited reference to both open  learner models and learning analytics in the same publications.  One of the aims of this workshop, therefore, is to raise awareness  of the overlap, as well as differences, in approaches to, and pur- poses of visualising learning data in these two fields. In particular,   we will bring our substantial expertise in open learner model re- search, together with our more recent experience in learning ana- lytics for learners, to facilitate discussion and exchange of expe- riences amongst participants.   2. OBJECTIVES  The first objective of this workshop is to bring together people  working in learning analytics generally, open learner models,  and learning dashboards, to raise awareness of projects and ap- proaches across research areas, that are relevant to learning  analytics for learners.   The second objective of the workshop is to organise a state of  the art review on learning analytics for learners, to be co- authored with key participants at the workshop. This aims to  cover the themes in the first objective, but may be adapted to  incorporate expertise and interests of workshop participants.   Finally, we aim to continue supporting research on learning  analytics for learners in future workshops, and would like to  gather suggestions for key topics for the next LAL proposal.  This will allow us to foster and stimulate ideas in areas of inter- est to participants that may not yet be mature enough for inclu- sion in the review article or workshop proceedings.   In short: we would like to bring our understanding of work re- lated to learning analytics for learners to the learning analytics  community, to learn from others work on learning analytics for  learners, and together build a vision for learning analytics to  directly support and facilitate the learning process for learners.   3. TOPICS  The workshop topics of interest include but are not limited to  the following:   ! Open learner models  ! Learning analytics dashboards to support learning  ! Use cases and applications of learning analytics for learners   4. TARGET GROUP  The workshop is of interest not only to those who are currently  working on learning analytics for learners, but also those whose  primary interest is with other stakeholders. The topic is timely,  with recognition of its importance growing (e.g. [7][8][9]). We  are targeting mainstream learning analytics researchers as well  as those who are also involved in related fields such as artificial  intelligence in education. We accepted eight submissions as  interactive presentations and demonstrations from authors with  advanced research in this area, to inform broader discussion  through the workshop. These come from researchers with expe- rience in designing or using learning analytics, and ideas about   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and  that copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than ACM  must be honored. Abstracting with credit is permitted. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee. Request permissions  from Permissions@acm.org.  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   2016 ACM. ISBN 978-1-4503-4190-5/16/04$15.00  DOI: http://dx.doi.org/10.1145/2883851.2883853       how these might usefully support learners directly. These in- clude demonstrations of learning analytics for learners. In addi- tion, the workshop format was created to be attractive to those  beginning their path in learning analytics for learners, as a space  to learn about related approaches and to have an opportunity to  gain feedback on their early ideas. We anticipated this to be a  particularly useful place for PhD researchers to become actively  involved in the community. At the same time, the workshop  aimed to draw upon those who are more experienced, who wish  to contribute to the growing work in this timely and important  topic.     The international Program Committee comprises researchers  from the learning analytics community, with some also in relat- ed fields. All submissions were reviewed by three program  committee members. The following form the Program Commit- tee:   ! Simon Buckingham Shum, University of Technology, Syd- ney, Australia    ! Susan Bull, University College London, UK  ! Eva Durall, Aalto University, Finland   ! Albrecht Fortenbacher, HTW Berlin, Germany  ! Alyssa Friend Wise, Simon Fraser University, Canada   ! Dragan Gasevic, University of Edinburgh, UK  ! Blandine Ginon, University of Birmingham, UK  ! Dai Griffiths, University of Bolton, UK   ! Sharon Hsiao, Arizona State University, USA.  ! Stphanie Jean-Daubias, University Claude Bernard of   Lyon, France   ! Matthew Johnson, University of Birmingham, UK  ! Judy Kay, University of Sydney, Australia  ! Michael Kickmeier-Rust, Technische Universitt Graz,   Austria  ! Symeon Retalis, University of Piraeus, Greece   ! Ravi Vatrapu, Copenhagen Business School, Denmark   5. FORMAT  The workshop is a full day workshop, with the provisional  schedule as below. The first half of the workshop comprises  research paper, position paper and tool demonstrations, with a  focus on interactive presentation styles.    9.00 Workshop introductions: participants and themes  9.30   Interactive presentations and demonstrations 10.00  10.30  11.00 Coffee break  11.30   Interactive presentations and demonstrations 12.00  12.30  13.00 Lunch break 13.30  14.00 Theme discussions in small groups 14.30  15.00 Coffee break  15.30   Overall discussion and writing of a synthesis 16.00  16.30  17.00 Workshop end      The second half of the workshop will be mainly devoted to ex- change and relevant theme discussion in small groups. The  themes reflect the given workshop topics and/or interests arising  during the workshop. Group discussions will last 20 minutes,  with groups switching topic/theme after each discussion. Groups   may remain together, or participants may distribute themselves  differently across themes, according to their interests. One of the  organisers or participants will record discussion, and facilitate  discussion as required. The final part of the workshop will in- clude summaries from the group facilitators, further discussion  amongst the larger group, and identification and planning of a  joint journal article on Learning Analytics for Learners, and a  YouTube video highlighting the potential for, and existence of  learning analytics for learners.     The main criteria for acceptance were as follows:   ! Relevance to the themes of the LAL workshop  ! Presentation and argumentation  ! Technical content (as applicable for demos / research papers)  ! Strength of evaluation (for research papers)  ! Novelty of the proposition  ! Interest for the Learning Analytics community  ! Overall quality of the paper     Further information about the workshop can be found on the  workshop website:   http://css-kmi.tugraz.at/mkrwww/leas-box/lakws16/home.html.    The workshop proceedings will be available as a CEUR Work- shop volume.   REFERENCES  [1] Siemens, G. & Long P. 2011. Penetrating the Fog: Analyt-  ics in Learning and Education. EDUCAUSE Review 46 (5),  30-38.   [2] West, D. 2012. Big Data for Education: Data Mining, Data  Analytics, and Web Dashboards. Governance Studies at  Brookings. 1-10.   [3] Gaevi, D., Dawson, S., & Siemens, G. (2015). Lets Not  Forget: Learning Analytics are about Learning. Techtrends,  59(1), 64-71. DOI: 10.1007/s11528-014-0822-x   [4] Ferguson, R. and Buckingham Shum, S. (2012). Social  Learning Analytics: Five Approaches. LAK 2012. 2333.  DOI: 10.1145/2330601.2330616   [5] Vozniuk, A., Govaerts, S., & Gillet, D. 2013. Towards Portable  Learning Analytics Dashboards. ICALT 2013, IEEE, 412-416.  DOI: 10.1109/icalt.2013.126    [6] Bull, S. & Kay, J. 2013. Open Learner Models as Drivers  for Metacognitive Processes. In R. Azevedo & V. Aleven  (eds), International Handbook of Metacognition and  Learning Technologies, Springe,r New York, 349-365.  DOI: 10.1007/978-1-4419-5546-3_23   [7] Dawson, S., Macfadyen, L., Risko E., Foulsham, T. &  Kingstone, A. 2012. Using Technology to Encourage Self- Directed Learning: The Collaborative Lecture Annotation  System (CLAS). ASCILITE 2012.   [8] Durall, E. & Gros, B. 2014. Learning Analytics as a Meta- cognitive Tool. Proceedings of CSEDU 2014, 380-384.   [9] Kay, J & Bull, S. 2015. New Opportunities with Open  Learner Models and Visual Learning Analytics. In C.  Conati, N. Heffernan, A. Mitrovic, M.F. Verdejo (eds)  AIED, Springer Int. Publishing, Switzerland, 666-669.  DOI: 10.1007/978-3-319-19773-9_87       "}
{"index":{"_id":"66"}}
{"datatype":"inproceedings","key":"Ochoa:2016:MLA:2883851.2883913","author":"Ochoa, Xavier and Worsley, Marcelo and Weibel, Nadir and Oviatt, Sharon","title":"Multimodal Learning Analytics Data Challenges","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"498--499","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883913","doi":"10.1145/2883851.2883913","acmid":"2883913","publisher":"ACM","address":"New York, NY, USA","keywords":"data challenge, multimodal, multimodal datasets","abstract":"This is a proposal for organizing a Multimodal Learning Analytics (MLA) data challenge as part of the workshop offering of the Learning Analytics and Knowledge (LAK) conference. It explains the motivation of the event, its objectives, target groups, expected format, organization, dissemination strategy and schedule.","pdf":"Multimodal Learning Analytics Data Challenges  Xavier Ochoa Escuela Superior Politcnica  del Litoral Va Perimetral Km. 30.5  Guayaquil, Ecuador xavier@cti.espol.edu.ec  Marcelo Worsley University of Southern  California 3470 Trousdale Parkway,  WPH-601A Los Angeles, CA, USA worsley@usc.edu  Nadir Weibel University of California, San  Diego La Jolla, CA, USA  weibel@ucsd.edu  Sharon Oviatt Incaa Designs  11140 Wing Point Drive, N.E. Bainbridge Island, WA, USA  sharon.oviatt@incaadesigns.org  ABSTRACT This is a proposal for organizing a Multimodal Learning An- alytics (MLA) data challenge as part of the workshop oer- ing of the Learning Analytics and Knowledge (LAK) confer- ence. It explains the motivation of the event, its objectives, target groups, expected format, organization, dissemination strategy and schedule.  CCS Concepts Applied computing ! Education;  Keywords Multimodal, Data Challenge, Multimodal datasets  1. MOTIVATION Learning is an innately multimodal activity. Students col-  laborate through face-to-face conversations, while using dig- ital and non-digital representations to support their ideas. Teachers use digital multimedia to explain a topic, while the students take notes on a multitude of digital and non- digital platforms. Students work together to build phys- ical projects, while annotating drawings and taking digi- tal photographs of their developments. Learning Analytics research, however, has concentrated mainly on computer- based learning contexts, where tools tend to automatically capture, in a fine-grained level of detail, the interactions with their users. The relative abundance of readily available data and the low technical barriers to process it, make computer- based learning systems the ideal place to conduct Learning Analytics research. On the contrary, in learning contexts where computers are not used, the actions of the actors of  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 16 April 25-29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883913  the learning process are not automatically captured. With- out traces to be analyzed, computational models and tools used traditionally in Learning Analytics are not applicable. While this bias towards computer-based learning context has helped the initial stages of Learning Analytics, computer- based learning still represents a small subset of the learning contexts. Multimodal Learning Analytics[2] (MLA) seeks to expand the current scope of Learning Analytics, focusing on the analysis of learning processes that happen on the phys- ical or physical/virtual world and require the capture, pro- cessing and analysis of more natural signals such as speech, writing, sketching, facial expressions, hand gestures, object manipulation, tool use, artifact building, etc. This workshop is an opportunity to introduce members of the Learning An- alytic community to methodologies, techniques and tools to capture, process and analyze multimodal learning traces.  2. OBJECTIVES Facilitate access to multimodal datasets: One of the  main barriers to start research in MLA is access to high- quality, annotated multimodal recordings. By oering these datasets to any interested researcher, the MLA community seeks to expand the available human talent capable of con- ducting learning analytics studies with multimodal signals.  Sharing advanced approaches and techniques: The ability to contrast and compare approaches and techniques to analyze diverse multimodal signals is a result of working on common datasets and questions. Research teams are able to directly learn from the developments of other teams and the current state-of-the-art is easily determined.  Disseminate the state of MLA research: A goal for the workshop this year is to disseminate the current capabil- ities of MLA to analyze non-computer-based learning con- texts among the wide Learning Analytics community.  Identify new datasets: Including ones involving addi- tional modalities, languages, and learning activities.  3. DATASETS This year, two multimodal datasets will be shared. Each  dataset will have challenges at two levels. To facilitate the participation of researchers already used to work with log data, the first level will provide features already extracted    from the dierent modalities. The research question at this level will be to combine those features into a model that can infer a specific aspect of the learning process. The second level is oriented to signal processing researchers and consists of extracting relevant learning traces from raw recordings of dierent media (audio, video, digital pen, etc.). For both levels, the datasets will also provide ground truth against which the models and techniques can be evaluated. The datasets to be provided will be:  The Math Data Corpus[1]: This dataset involves 12 sessions, with small groups of three English-speaking stu- dents collaborating while solving mathematics problems. Data were collected on their natural multimodal communication and activity patterns during these problem-solving and peer tutoring sessions, including students speech, digital pen in- put, facial expressions, and physical movements. This dataset has been expanded with full manual and automatic tran- scripts of the students speech. It also contains more than 10.000 annotations of the students diagrams during problem solving. The main research questions behind this dataset will be automatic estimation of solution correctness, individ- ual expertise, group performance, group collaboration level and evidence of learning.  Oral Presentation Quality Corpus: This challenge includes a data corpus that involves 40 oral presentations of Spanish-speaking students in groups of 4 to 5 members presenting projects. The following data is available: speech, facial expressions and physical movements in video, skele- tal data gathered from Kinect for each individual, and slide presentation files. In addition grading for individuals when doing their presentations is included as well as a group-grade related to the quality of the slides used when doing each pre- sentation. This challenge seeks determine how multimodal techniques can help in evaluating presentation skills.  4. TARGET GROUPS Existing MLA researchers: This event is the annual  meeting venue for the existing MLA community. The datasets that will be part of this challenge are among the most re- searched topics in MLA.  New MLA researchers: It is one of the objectives of this workshop to attract new researchers from the Learning Analytics community to MLA. The low-barriers to partici- pate in the challenge will enable researchers that are used to alphanumerical-only data to test their approaches and techniques on data generated from multimodal signals.  LA researchers and practitioners: This workshop is also directed to the general Learning Analytics community. Researchers could attend the workshop to discover how to in- corporate new modes (such as speech, writing, gestures, etc.) to their current monomodal analysis. Practioners could at- tend the workshop to establish how MLA could help them to measure and feedback learning on real-world, non-computer- based contexts.  5. DISSEMINATION STRATEGY Apart from the dissemination of the call through tradi-  tional channels (mailing lists, web-site and relevant discus- sion fora), the organizers will identify and personally contact researchers both from the MLA and general Learning An- alytics communities to invite to apply their approaches to one or more of the challenges.  6. PROGRAM COMMITTEE The program committee is formed with members of the  MLA community, researchers of Multimodal Interaction and Learning Analytics. These members are: Michael Johnston (Interactions), Alejandro Andrade (Indiana University), Kate Thompson (University of Sydney), Engin Bumbacher (Stan- ford University), Mirko Raca (Ecole polytechnique federale de Lausanne), Richard Davis (Stanford University), Bertrand Schneider (Stanford University), Shuchi Grover (SRI Inter- national), Saad Khan (Educational Testing Service), Lei Chen (Educational Testing Service), Katherine Chiluiza (Es- cuela Superior Politecnica del Litoral)  7. FORMAT Following the successful experience of the previous MLA  challenges, this event will be organized around the analy- sis of two dierent datasets. The results of all the partici- pant teams will be collected, compared and contrasted. The participating teams and the audience are invited to discuss about the level of success of the presented approaches to solve the challenge and possible collaborations to improve the results.  8. SCHEDULE This will be a full-day workshop (6 hours). The sched-  ule will be: Introduction (15 minutes). Presentation of the datasets and challenges (30 minutes). Each tema Work on the challeges (2 hours). Lunch break. Oral Presentation Corpus at Level 1 and 2 (30 minutes). Critique of the ap- proaches for Oral Presentation Corpus Level 1 (30 minutes). Critique of the approaches for Math Data Corpus Level 2 (30 minutes). General discussion about the results for the Math Data Corpus (30 minutes). Coee break. Discussion about future collaboration and the MLA community (1 hour). Fi- nal remarks, future steps and conclusions (30 minutes).  9. REFERENCES [1] S. Oviatt, A. Cohen, and N. Weibel. Multimodal  learning analytics: description of math data corpus for icmi grand challenge workshop. In Proceedings of the 15th ACM on International conference on multimodal  interaction, pages 583590. ACM, 2013. [2] M. Worsley. Multimodal learning analytics: enabling  the future of learning through multimodal data analysis and interfaces. In Proceedings of the 14th ACM international conference on Multimodal interaction, pages 353356. ACM, 2012.    "}
{"index":{"_id":"67"}}
{"datatype":"inproceedings","key":"Wolff:2016:DLL:2883851.2883864","author":"Wolff, Annika and Moore, John and Zdrahal, Zdenek and Hlosta, Martin and Kuzilek, Jakub","title":"Data Literacy for Learning Analytics","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"500--501","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883864","doi":"10.1145/2883851.2883864","acmid":"2883864","publisher":"ACM","address":"New York, NY, USA","keywords":"analysis, communication, data literacy, learning analytics, visualization","abstract":"This workshop explores how data literacy impacts on learning analytics both for practitioners and for end users. The term data literacy is used to broadly describe the set of abilities around the use of data as part of everyday thinking and reasoning for solving real-world problems. It is a skill required both by learning analytics practitioners to derive actionable insights from data and by the intended end users, such that it affects their ability to accurately interpret and critique presented analysis of data. The latter is particularly important, since learning analytics outcomes can be targeted at a wide range of end users, some of whom will be young students and many of whom are not data specialists. Whilst data literacy is rarely an end goal of learning analytics projects, this workshop aims to find where issues related to data literacy have impacted on project outcomes and where important insights have been gained. This workshop will further encourage the sharing of knowledge and experience through practical activities with datasets and visualisations. This workshop aims to highlight the need for a greater understanding of data literacy as a field of study, especially with regard to communicating around large, complex, data sets.","pdf":"Data literacy for Learning Analytics  Annika Wolff, John Moore   Computing and Communications, Faculty of Maths,  Computing and Technology    The Open University Milton Keynes, MK7 6AA, UK   {annika.wolff, john.moore}@open.ac.uk     Zdenek Zdrahal, Martin Hlosta, Jakub Kuzilek  Knowledge Media Institute   The Open University Milton Keynes, MK7 6AA, UK   {zdenek.zdrahal, martin.hlosta,   jakub.kuzilek}@open.ac.uk        ABSTRACT  This workshop explores how data literacy impacts on learning  analytics both for practitioners and for end users. The term data  literacy is used to broadly describe the set of abilities around the  use of data as part of everyday thinking and reasoning for solving  real-world problems. It is a skill required both by learning  analytics practitioners to derive actionable insights from data and  by the intended end users, such that it affects their ability to  accurately interpret and critique presented analysis of data. The  latter is particularly important, since learning analytics outcomes  can be targeted at a wide range of end users, some of whom will  be young students and many of whom are not data specialists.  Whilst data literacy is rarely an end goal of learning analytics  projects, this workshop aims to find where issues related to data  literacy have impacted on project outcomes and where important  insights have been gained. This workshop will further encourage  the sharing of knowledge and experience through practical  activities with datasets and visualisations. This workshop aims to  highlight the need for a greater understanding of data literacy as a  field of study, especially with regard to communicating around  large, complex, data sets.    Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1 [Computer  Uses in Education] Computer-assisted instruction (CAI)    General Terms  Measurement, Design, Human Factors   Keywords  Learning analytics, data literacy, communication, analysis,  visualization   1. Motivation  Data analytics are coming out of the lab and into the mainstream.  As more of the general population start interacting with complex  data sets it is time to start taking data literacy seriously.   This is of particular importance within the field of learning  analytics, where the results of data analysis are targeted towards a  multitude of different end users, from young students, teachers,  faculty and administration. Learning analytics aims to provide  actionable insights to educators and learners on different aspects  of learning. Whilst the remit of learning analytics can be quite  broad, in each case there must be a communication of the data  analysis through which the important insights can be easily  understood by the target audience. Both the people producing the  data analysis and the people on the receiving end must have a  foundational level of data literacy in order to be able to  communicate effectively via the data.  But how can this be defined What are the specific competences  of the different stakeholders and how, within the field of learning  analytics, is it possible to support their acquisition or to develop  an understanding such that it is possible to develop dashboards  and visualisations of data that are easily understood by the target  audience  In early stages of the field of learning analytics a lot of focus has  been on the types of data available and the types of analysis that  can be applied to it. This workshop explores the issues around  data literacy for learning analytics and asks the question do  students and educators need a certain level of data literacy to  understand the outputs of learning analytics And crucially, do  most students and educators achieve this level    2. Workshop Objectives And Topics  The objective of the workshop is to develop a broader  understanding of data literacy, particularly with respect to  different aspects of learning analytics. The workshop aims to  develop this as a field of study in its own right within the learning  analytics community and to make it a more prominent  consideration when designing LA systems.  Topics of interest include, but are not limited to:    Creating dashboards and visualisations for non-data  experts    Making data and algorithms visible as a way to improve  collected data and improve peoples data-contributing  behaviour    Does increasing visibility of algorithms change learner  behaviour    Gaming the learning analytics  do data literate learners  start to play the system    Visualizing analytic processes for novice users.     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883864     The workshop website can be found at:  http://events.kmi.open.ac.uk/dataliteracy4la/     3. Target Group  The target group are primarily member of the LA community who  need to communicate outputs of data analysis, who are interested  to understand how data literacy aligns with the process of  conducting analysis of learner data, or who wants to understand  how data literacy might affect the behavior of various LA  stakeholders.     4. Format  The workshop is intended to bring participants together for one  day.  We want to make the workshop as interactive as possible. In  the morning we will have some short presentations based on  submitted papers, but with a focus very much on speakers  presenting information directly related to data literacy angles of  projects. In the afternoon participants will work in groups on one  of three practical tasks (depending on participant numbers, there  may be several groups working on the same type of task). Tasks  are focused on understanding how data literacy can impact on  communicating outputs of learning analytics, from different  starting points. There will be flexibility on the day to allow for  additional topics of interest to be explored if there is a big enough  group and appropriate resource for it. Proposed tasks are:    1. Analyzing and visualizing learner data. Participants  will be provided, in advance of the workshop and via  the workshop website, an extract of learner data and  some suggested analyses aimed to get participants to  focus on important aspects of the data. This will be from  the anonymised Open University Learning Analytics  dataset (https://analyse.kmi.open.ac.uk/open_dataset)  which contains data about courses, students and their  interactions with Virtual Learning Environment (VLE)  for seven selected courses. This data will have been  already used within a mashup event  (https://www.participant.co.uk/open_data_mashup_day)  and insights from this experience will be used to inform  the running of this session. During the workshop,  participants will work together to analyse and produce  insight from a more extensive version of this dataset,  which they will then try to convey to other workshop  participants. It will be suggested they bring their own  devices and preferred tools for data analysis and  visualization, although we will also create a list of  useful free and easily downloadable tools. Prior to the  workshop we will also use the workshop website to  solicit for additional datasets from workshop attendees.  We would intend that at least some participants of this  group would be proficient in data analysis and  visualization, however we would aim to mix the group   to also include non data experts. Participants might  choose to visualize data towards individuals, about  groups of students, or to combine the two. The types of  insights we might expect from this group would be a  developing understanding of how choices of analysis  impact on the types of visualisations and  communications that are available from data.  Data  analysis experts may be prompted to reflect more  carefully on how they choose to communicate the  outcome of their analyses with respect to the assumed  competences of their target audience.    2. Communicating from aggregated data. Participants in  this group will be provided with a pre-digested output  from the Open Universities (OU) own analytics work on  a) predicting learner failure from interactions in the  VLE b) providing student focused recommendations.  The output will be derived from the OUs existing  visualisations af data analyses, yet will be presented in a  non visual way. Participants will be asked to identify  ways to communicate this data to different types of user,  including expert and non experts, or faculty, tutors, or  students. They will present their outputs to other groups  to assess how successful they are in conveying the  intended message. Outputs could be created by  technology or drawn onto paper as a creative design, or  constructed in 3D. Participants are invited to be  particularly creative in this session. The types of  insights we might expect from this group would be  around how to communicate to different audiences from  complex datasets and the creation of potentially new  modes of visualizing learner data. We hope to create a  better understanding of how to visualize and  communicate to students themselves the outcome of  recommenders.   3. Data and visualization clinic. In this group,  participants share their experience of creating  dashboards and visualisations for different types of  users. Participants will be solicited to bring examples of  visualisations they have used with end users as a point  for discussion, to understand if the outputs are  communicating effectively and to collectively explore  better ways for visualizing, especially for non-expert  audiences. The types of insight expected from this  group are towards developing best practice for  communicating about data, based on real life examples.   Groups will share their insights amongst each other at the end of  the day, insights will be captured and used to define more clearly  what data literacy means in the field of learning analytics and  what are the biggest problems to tackle.          "}
{"index":{"_id":"68"}}
{"datatype":"inproceedings","key":"Giannakos:2016:SEA:2883851.2883898","author":"Giannakos, Michail N. and Sampson, Demetrios G. and Kidzi'nski, Lukasz and Pardo, Abelardo","title":"Smart Environments and Analytics on Video-based Learning","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"502--504","numpages":"3","url":"http://doi.acm.org/10.1145/2883851.2883898","doi":"10.1145/2883851.2883898","acmid":"2883898","publisher":"ACM","address":"New York, NY, USA","keywords":"interaction design, learning analytics, smart environments, video-based learning, visual analytics","abstract":"The International Workshop of Smart Environments and Analytics on Video-Based Learning (SE","pdf":"Smart Environments and Analytics on Video-Based Learning  Michail N. Giannakos  Norwegian University of  Science and Technology   Trondheim, Norway  michailg@idi.ntnu.no   Demetrios G. Sampson  Curtin University  Perth, Australia   demetrios.sampson@curt in.edu.au   ukasz Kidziski  Ecole Polytechnique   Fdrale de Lausanne,  Lausanne, Switzerland   lukasz.kidzinski@epfl.ch   Abelardo Pardo  The University of Sydney   Sydney, Australia  abelardo.pardo@sydn  ey.edu.au     ABSTRACT  The International Workshop of Smart Environments and  Analytics on Video-Based Learning (SE@VBL) aims to connect  research efforts on Video-Based Learning with Smart  Environments and Analytics to create synergies between these  fields. The main objective is to build a research community  around the intersection of these topical areas. In particular,  SE@VBL aims to develop a critical discussion about the next  generation of video-based learning environments and their  analytics, the form of these analytics and the way they can be  analyzed in order to help us to better understand and improve the  value of educational videos to support teaching and learning.  SE@VBL is based on the rationale that combining and analyzing  learners' interactions with other available data obtained from  learners, new avenues for research on video-based learning have  emerged. This can have a significant impact in current educational  trends such as Massive Open Online Courses (MOOCs) and  Flipped Classroom.   Categories and Subject Descriptors  K.3.1 [Computer Uses in Education] Computer-assisted  instruction (CAI), Distance learning; J.1 [Administrative Data  Processing] Education   General Terms  Measurement, Design, Experimentation, Human Factors,   Keywords  Video-Based Learning, Learning Analytics, Smart Environments,  Visual Analytics, Interaction Design.   1. BACKGROUND  With the widespread adoption of video-based learning systems  such as Khan Academy and edX, new research in the area of  Learning Analytics and Smart Learning Environments has  emerged. Even new for-profit companies, such as Coursera and  Udacity, have started offering forms of instruction that are  primarily video-based. Most universities are implementing video  lectures in a variety of ways, such as delivering lecture recordings  before class to conserve class time or to flip the day for hands-on  activities. Video-based learning environments (such as Massive  Open Online Courses MOOCs) are becoming widely popular,   sometimes with unjustified promises. Yet, the educational added- value of video-based learning environments, is lacking strong  evidences, as learners do not always use them as expected and in  an effective way [1].   As a step toward improving learners experience and engagement  with video-based learning systems; students activity might be  converted via analytics into useful information and benefit smart  environments efficiency and ultimately learners experience and  performance. Capturing, sharing and analyzing these learners  interactions can clearly provide scholars and educators with  valuable information [2]. In addition, the combination of other  information of the individual learner and/or groups of learners can  provide opportunities for adding value to learning analytics and  smart video environments.   To explore the future of video-based technologies for teaching  and learning, we aim to build a research community around this  topical area, to brainstorm about what the next generation of  video-based learning environments might look like, what kind of  data can be collected, and how these data can help us to better  understand and improve the value of video-based learning.   2. OBJECTIVES  The advances of technology-supported open access to education  indicate an increased use of video but only when pedagogically  appropriate and designed purposely to facilitate teaching and  learning. From current research, it is difficult to tell what aspects  of the video-lectures and video-based learning systems can have a  positive impact. In order to employ videos that serve as powerful  pedagogical tools, care should be taken to examine their impact  on the overall learner experience. As such, the purpose of this  workshop is to explore how smart environments and analytics can  improve video-systems learning potential.   In particular, we seek to answer the following two questions:   RQ1. How smart environments and analytics can  improve video-based learning experience, adoption and  learning outcome   RQ2. How video based learning environments can be  better developed for taking full advantage of the  opportunities and challenges they provide   One of our main objectives is to bring together researchers who  are interested on Smart Environments, Learning Analytics and  their application on Video-based Learning. Specifically,  SE@VBL aims to provide an environment where participants will  get opportunities to: develop their research skills; increase their  knowledge base; collaborate with others in their own and  complementary research areas; and discuss their own work. In  particular, guiding questions and themes include:   O1. What might next generation of smart environments  and analytics enhanced video learning tools look like   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  http://dx.doi.org/10.1145/2883851.2883898      O2. What kind of data can be collected from video- based learning environments    O3. How these data can help us to better understand and  improve the educational value of video-based learning   O4. How emerging data analyses techniques (e.g.,  machine learning) as well as data visualizations can help  us to provide reflection and insights to learner, teacher,  manager, researcher, etc   O5. How can the affordances of video-based learning  coupled with learning analytics and help instructors to  redesign their teaching materials and practices   3. LEVERAGING VIDEO INTERACTION  DATA  The phenomenon of MOOCs introduced irreversible changes to  education. Lecture videos, which constitute the main part of an  online course, allow us to analyze students interaction from a  different perspective at a large scale - unimaginable in educational  science few years ago. This analysis is beneficial for all MOOC  stakeholders: Practitioners, can design their classes in a more  engaging way, service providers can build platforms which, for  example, simplify access to the most demanded content, and  educational researchers can understand incentives of a student  participating in a lecture.    We gather large amounts of data in the form of log files, database  entries, cookies etc., distributed among MOOC providers, clouds  and personal devices of students. Although this provides us a  great opportunity to understand the learning processes, it also  poses new technological and research challenges, which can only  be addressed by joint efforts of data science and educational  communities.   Classical statistical frameworks are now often reassessed. On one  hand, the notion of statistical significance is blurred  the  detection of an effect is simpler and we are more likely to discover  small effects, irrelevant in practice. Conversely, recent  advancements in machine learning allow us to discover new types  of patterns in data, useful in practice but difficult to prove  formally.   The process of addressing research questions became more  complex and involves more technical expertise. Instead of the  classical path hypothesis    experiment design    testing,  we now often need to take additional, nontrivial step of an  adequate representation of the data, as presented in Figure 1.      Fig. 1. The flow of data-driven educational research is now  altered by information retrieval step, where we find an adequate  representation of a vast dataset.   It is clear that we have reached into a significant turning point, a  new era of educational research. Not only we significantly  improve the quality of global education but also we have the  capacity to answer questions which were not even addressable few  years ago.   4. CONCLUSIONS AND THE WAY  AHEAD  The roles of 1) analytics on helping individuals to make sense of  the learning procedures and the 2) smart environments on  providing feedback and diverse smart functionalities have  drawn the interest of many scholars and practitioners in the last  years. In particular, analytics have proven their ability to help us  to understand (make sense) many complex learning phenomena in  the past [3]. However, comparing with research on text and  discourse analytics, the research on video analytics is still on an  early stage. Video analytics have an enormous potential,  especially given what is currently happening around the explosion  of MOOCs. As most of the MOOCs are using videos as their  primary content delivery mechanism, research on MOOCs will  heavily influence video-based learning research. So we believe  that the topic of SE@VBL is very timely with great potential.  This potential will grow as MOOC platforms, like Coursera and  Edx make their data publicly available to the research community.   Although research on video based learning has been increased in  the last years [1], a number of questions remain unexplored  regarding the use and design of videos for learning. In particular,  little research has been conducted on the functionalities and the  characteristics of learning videos and video based learning  systems. Characteristics like quality of feedback, cognitive load,  engagement and tone of voice, pace, length , and segmentation  need to be examined in more detail in order to improve the  effectiveness of video as learning medium [1, 4].   With respect to the viewing patterns of learners, some interesting  preliminary work was noted [4] including when and where  students watch learning videos as well as how they view materials  (e.g., in small chunks). Future research can focus on a more  detailed analysis of viewing patterns and its impact on learning  outcomes. For example, students who skip or re-watch segment  may integrate less knowledge than students who view videos more  systematically. To this end, sophisticated video analytic systems  as well as smart video-based environments can be used and help  us to make sense and improve how students learn with the  assistance of videos.   5. ACKNOWLEDGMENTS  Our thanks to thank the Research Council of Norway for the  financial support (project number: 248523/H20).   6. REFERENCES  [1] Giannakos, M. N. Exploring the video-based learning   research: A review of the literature. British Journal of  Educational Technology, 44(6), 191-195 (2013)   [2] Giannakos, M. N. et al. (2013). Analytics on video-based  learning. In Proceedings of LAK, ACM Press, 283-284.   [3] Siemens. G. (2012). Learning analytics: envisioning a  research discipline and a domain of practice. In Proceedings  of LAK, ACM Press, 4-8.     [4] Guo, P. J., Kim, J., & Rubin, R. (2014). How video  production affects student engagement: An empirical study  of mooc videos. In Proceedings of Learning@ scale, 41-50.     "}
{"index":{"_id":"69"}}
{"datatype":"inproceedings","key":"Brooks:2016:IDM:2883851.2883879","author":"Brooks, Christopher A. and Thompson, Craig and Kovanovi'c, Vitomir","title":"Introduction to Data Mining for Educational Researchers","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"505--506","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883879","doi":"10.1145/2883851.2883879","acmid":"2883879","publisher":"ACM","address":"New York, NY, USA","keywords":"data mining, learning analytics, predictive modeling","abstract":"The goal of this tutorial is to share data mining tools and techniques used by computer scientists with educational social scientists. We broadly define educational social scientists as being made up of people with backgrounds in the learning sciences, cognitive psychology, and educational research. The learning analytics community is heavily populated with researchers of these backgrounds, and we believe those that find themselves at the intersection of research, theory, and practice have a particular interest in expanding their knowledge of data driven tools and techniques.","pdf":"Introduction to Data Mining for Educational Researchers  Christopher A. Brooks   School of Information  University of Michigan   brooksch@umich.edu   Craig Thompson  Department of Computer Science   University of Saskatchewan  craig.thompson@usask.ca   Vitomir Kovanovi  School of Informatics   University of Edinburgh  v.kovanovic@ed.ac.uk       CCS Concepts  Computing methodologies  Machine learning  Machine  learning approaches   Keywords  Data mining, learning analytics, predictive modeling   1. INTRODUCTION & MOTIVATION  The goal of this tutorial is to share data mining tools and  techniques used by computer scientists with educational social  scientists.  We broadly define educational social scientists as  being made up of people with backgrounds in the learning  sciences, cognitive psychology, and educational research.  The  learning analytics community is heavily populated with  researchers of these backgrounds, and we believe those that find  themselves at the intersection of research, theory, and practice  have a particular interest in expanding their knowledge of data- driven tools and techniques.   A version of this tutorial was presented by Brooks, Pardos, and  Thompson at LAK14 in Indianapolis. That tutorial sold out, and  participant feedback was strong enough that the tutorial was  offered again at LASI14 at Harvard. This proposal is heavily  based on the LAK14 proposal, and includes updates to the  curriculum based on participant feedback.   One popular tool used by computer scientists for data mining is  the Weka toolkit.  It is open source, free, and easy to use.  To  apply Weka to educational datasets correctly, however, one  requires knowledge of the features, limitations, and subtleties of  the underlying techniques.  These aspects of the data mining  process are often not immediately clear, and even just a little time  exploring these techniques with experts using realistic datasets  can be beneficial in increasing practical data mining skills.  The  Weka toolkit offers a variety of advanced machine learning  mechanisms, allowing attendees to further explore the field further  after the tutorial.   We intend for this tutorial to be one way to facilitate this cross- disciplinary discussion.  With a focus on practical skill building  and hands on learning, we aim to explore with attendees the   different data mining techniques can be applied at this intersection  of research, theory, and practice.  We expect attendees will leave  the tutorial with not only a deeper understanding of how data  mining can be applied to their research questions, but a feeling of  empowerment in being able to carry out initial analysis directly.   2. OBJECTIVES  By the end of this course we expect that attendees are able to:   1. Describe the differences between supervised and  unsupervised classification   2. Understand how to choose a classification method for a  particular research question   3. Frame different kinds of educational datasets in a way  that is appropriate for data mining   4. Contextualize the results of a J48 decision tree to their  research questions   5. Contextualize the results of k-means clustering to their  research questions   6. Apply knowledge of the Weka toolkit to create decision  trees or clusters of new educational datasets   While there are many education and learning conferences that  include computer scientists and educational social scientists (e.g.  EDM, AIED, ITS, LAK, ICLS, CSCL, etc.), to our knowledge  there has been no other tutorial series focused explicitly on  introductory methods of data mining for practitioners. The closest  relevant resource we have observed is the recently offered free  course on Data Mining with Weka offered by the University of  Waikato1. While this content is an excellent way to learn the  Weka toolkit, it has not been largely advertised in the educational  technology and learning sciences communities, and it does not  focus on educational datasets or questions of learning outcomes.   We view our proposed tutorial at Learning Analytics and  Knowledge as one way of contextualizing the kind of instruction  that is available in this complimentary online course.   3. TARGET GROUP  This tutorial is aimed at educational researchers who are from  traditional social science backgrounds.  Explicitly, it is not  intended for computer scientists who have a background in  statistical analysis or artificial intelligence. Nonetheless, we  expect that junior graduate students in the computer sciences may  find this tutorial useful, and experience suggests that they are able  to use this opportunity to share their own experiences with data  mining methods in education.   Administrators, program directors, and higher education decision  makers are a third sub-population of the LAK community.  This                                                                     1 See http://wekamooc.blogspot.com/           Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   ACM 978-1-4503-4190-5/16/04.   DOI: http://dx.doi.org/10.1145/2883851.2883879      tutorial expects only basic understanding of statistics (mean, error,  ANOVA, etc.), and we believe will be accessible to this group of  attendees as well. While we have not finalized the datasets we will  use for the tutorial, we expect data at the institutional level (e.g.  student admission and registration data) to be part of this, making  the tutorial inclusive for attendees who are interested in  institutional educational analytics as well. One significant revision  to the tutorial from previous iterations is that we aim to use a real  educational dataset that is similar to the datasets any university or  college would have available for analysis.   4. FORMAT  A schedule for this half-day tutorial2 is as follows:    A short overview of what kinds of educational problems  classification in data mining has been applied to, along  with a description of research questions that  classification is not well suited to answering (20 mins)    A guided example of building J48 decision trees using  Weka.  Attendees will be given access to real datasets,  and walked through the various parameters (leaf size,  ten fold testing, etc.) available in the Weka toolkit. (40  mins)    A discussion of understanding the results of the decision  tree process, such as how to read the confusion matrix,  understanding rules in the tree, etc.  Participants will be  encouraged to explore parameters of the classification,  and relate these to outcomes. (30 mins)    Coffee Break (15 mins)    An introduction to what cluster analysis is, and a guided  example of applying k-means to real datasets.   Attendees will be walked through the process, and will  cluster the data using various kinds of parameters. (40  mins)    A discussion of understanding the results of clustering.   E.g. what is a centroid, how to measure the quality of  the cluster analysis, applications of clustering in other  educational areas. (30 mins)    A wrap up discussion reviewing the objects of the  tutorial, next steps for participants who wish to learn  more, and general questions about educational data (20  mins)   Depending upon the background, number of individuals, interest  of attendees, and time, additional content may be provided:    Comparison with other statistical techniques for  classification (e.g. logistic regression)    Attribute selection with weka (e.g. CFS)    Balancing datasets with SMOTE    Applying trained models to new classify data    Using Weka to clean datasets  As this will be a hands-on tutorial, attendees are expected to bring  with them computing equipment such as laptops, and attendance                                                                     2 The schedule for previous iterations of this tutorial, as well as   recordings of those tutorials, can be found online at  https://sites.google.com/a/umich.edu/lak-2014-tutorial- introduction-to-data-mining-for-educational-researchers/   should be limited to 20 persons.  The organizers will provide  copies of the Weka toolkit (open source and cross platform), as  well as a selection of educational datasets. A projector for the  presentations, as well as tables for attendees to work at, are  minimum equipment requirements.   This tutorial is not only beneficial for established researchers, but  also for graduate students.  The LAK community is very much  global in nature and, given travel limitations, we are interested in  exploring whether the event can be recorded or webcast as a  SOLAR Storm3 or high quality captured content analogous to a  mini-MOOC.  While there are logistics and additional costs (high  speed network connection, potentially camera and audio rental  equipment) required with such an activity, we believe the impact  this tutorial will have on the broader LAK community will be  multiplied with a persistent online resource.  If this ends up being  a viable option, we will seek other off-site collaborators to help  manage the remote help requests using screensharing, live  discussion forums, etc.   5. BIOGRAPHIES  Christopher Brooks is a Research Assistant Professor in the  School of Information, and Director of Learning Analytics and  Research in the Office of Digital Education and Innovation, at the  University of Michigan. His research focuses on using machine  learning for predictive modeling along with information  visualization to build novel solutions for understanding student  learning.   Craig Thompson is a Ph.D. candidate and Data Analyst at the  University of Saskatchewan. His doctoral research interests  include Machine Learning and Image Recognition applied to  labelling images on the web. As a Data Analyst, Craig is pursuing  a variety of projects involving Learning Analytics and Data  Mining with institutional enrolment information, student survey  responses, and LCMS data.   Vitomir Kovanovi is a PhD student and research assistant at  School of Informatics, University of Edinburgh, United Kingdom,  and a research assistant at the Learning Innovation and Networked  Knowledge Research Lab at University of Texas, Arlington.  Vitomirs research is in the area of Learning Analytics and  Educational Data Mining focuses on the development of novel  learning analytics methods based on the trace data collected by  learning management systems and their use to improve inquiry- based online education. He authored and co-authored several  book chapters, conference papers, and journal articles. Vitomir is  a recipient of two best paper awards (LAK15 and HERDSA15  conferences) and scholarships by the Serbian ministry of  Education, Simon Fraser University, and the University of  Edinburgh.                                                                     3 http://www.solaresearch.org/storm/     "}
{"index":{"_id":"70"}}
{"datatype":"inproceedings","key":"Agnihotri:2016:EDM:2883851.2883857","author":"Agnihotri, Lalitha and Mojarad, Shirin and Lewkow, Nicholas and Essa, Alfred","title":"Educational Data Mining with Python and Apache Spark: A Hands-on Tutorial","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"507--508","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883857","doi":"10.1145/2883851.2883857","acmid":"2883857","publisher":"ACM","address":"New York, NY, USA","keywords":"big data, data mining, educational data mining, exploratory data analysis, learning analytics, machine learning, parallel computing, predictive analytics, python, simulation, spark, visualization","abstract":"Enormous amount of educational data has been accumulated through Massive Open Online Courses (MOOCs), as well as commercial and non-commercial learning platforms. This is in addition to the educational data released by US government since 2012 to facilitate disruption in education by making data freely available. The high volume, variety and velocity of collected data necessitate use of big data tools and storage systems such as distributed databases for storage and Apache Spark for analysis. This tutorial will introduce researchers and faculty to real-world applications involving data mining and predictive analytics in learning sciences. In addition, the tutorial will introduce statistics required to validate and accurately report results. Topics will cover how big data is being used to transform education. Specifically, we will demonstrate how exploratory data analysis, data mining, predictive analytics, machine learning, and visualization techniques are being applied to educational big data to improve learning and scale insights driven from millions of student's records. The tutorial will be held over a half day and will be hands on with pre-posted material. Due to the interdisciplinary nature of work, the tutorial appeals to researchers from a wide range of backgrounds including big data, predictive analytics, learning sciences, educational data mining, and in general, those interested in how big data analytics can transform learning. As a prerequisite, attendees are required to have familiarity with at least one programming language.","pdf":"Educational Data Mining with Python and Apache Spark: A Hands-on Tutorial  Lalitha Agnihotri McGraw-Hill Education  281 Summer Street Boston, Massachusetts, US  lalitha.agnihotri @mheducation.com  Shirin Mojarad McGraw-Hill Education  281 Summer Street Boston, Massachusetts, US  shirin.mojarad @mheducation.com  Nicholas Lewkow McGraw-Hill Education  281 Summer Street Boston, Massachusetts, US  nicholas.lewkow @mheducation.com  Alfred Essa McGraw-Hill Education  281 Summer Street Boston, Massachusetts, US  alfred.essa @mheducation.com  ABSTRACT Enormous amount of educational data has been accumu- lated through Massive Open Online Courses (MOOCs), as well as commercial and non-commercial learning platforms. This is in addition to the educational data released by US government since 2012 to facilitate disruption in education by making data freely available. The high volume, variety and velocity of collected data necessitate use of big data tools and storage systems such as distributed databases for storage and Apache Spark for analysis.  This tutorial will introduce researchers and faculty to real- world applications involving data mining and predictive an- alytics in learning sciences. In addition, the tutorial will introduce statistics required to validate and accurately re- port results. Topics will cover how big data is being used to transform education. Specifically, we will demonstrate how exploratory data analysis, data mining, predictive analytics, machine learning, and visualization techniques are being ap- plied to educational big data to improve learning and scale insights driven from millions of student's records.  The tutorial will be held over a half day and will be hands on with pre-posted material. Due to the interdisciplinary nature of work, the tutorial appeals to researchers from a wide range of backgrounds including big data, predictive analytics, learning sciences, educational data mining, and in general, those interested in how big data analytics can transform learning. As a prerequisite, attendees are required to have familiarity with at least one programming language.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.   2016 ACM. ISBN 978-1-4503-2138-9. DOI: 10.1145/1235  CCS Concepts Information systems ! Data mining; Data cleaning; Mathematics of computing!Exploratory data anal- ysis; Computing methodologies ! MapReduce al- gorithms; Modeling and simulation; Modeling method-  ologies; Model verification and validation;  Keywords educational data mining, learning analytics, python, spark, predictive analytics, machine learning, exploratory data anal- ysis, big data, data mining, visualization, simulation, paral- lel computing  1. MOTIVATION A number of commentators, including Clayton Christensen,  have argued that education is prime for disruption [1]. Ed- ucation has become a part of big data revolution as educa- tional data meets the four main elements of big data includ- ing volume, velocity, variety and veracity. Large volume of educational data is collected through online learning plat- forms such as MOOCs and commercial e-learning products. This data is oftentimes generated real-time as students ac- tivities happen on the e-learning platform and comes in dif- ferent formats including clickstreams, text and comments, short answers, attendance, performance, etc. The uncer- tainty in educational data including biases, noise and ab- normality necessitates a process in which the data is stored and mined meaningful to the problem being analyzed.  Educational data mining (EDM) is an interdisciplinary field entailing data mining, machine learning and statis- tics, and their application to education settings to transform teaching and learning. EDM has received much attention from researchers to derive insights from learners'activities and has been adopted by many institutions to improve the services they provide and for increasing student grades and retention.  Since LAK's primary focus is learning analytics, this tu- torial will be beneficial for the majority of audience, provid- ing accessible tools to perform predictive analytics and data mining on big educational data sets.  10.1145/1235   2. OBJECTIVES Given the focus on practical skill-building, the primary  objectives of the proposed tutorial are:   Understand the basics of exploratory data analysis (EDA).   Learn to use Python programming language.   Learn how to build and validate predictive models.   Learn the foundations of parallel computing for work- ing with large datasets and large scale models.  While there are several conference series (e.g., LAK, EDM) focusing on the intersection between educational and com- puting research, to the best of our knowledge there have not been any tutorials covering all aspects of data analysis including EDA, predictive analytics, reporting results using appropriate statistical measures, and visualizing results. A subset of this tutorial was presented at ECTEL 15 (Euro- pean Conference on Technology Enhanced Learning) as a part of a half day tutorial. The session was very well re- ceived by the audience and the recording is available online at http://educate.gast.it.uc3m.es/wapla/.  Most of the available tools for predictive analytics require enviable knowledge of technical details of underlying algo- rithms. However, there are also attempts to develop plat- forms for broader communities of researchers. Weka is a widely used data mining platform in java. Although easy to use, Weka's main packages are not optimized for memory intensive tasks on large data sets. Our goal in this tutorial is to make programming and mining big data in education using Python accessible for a wide range of audience. The Python programming language - with its predictive model- ing and visualization libraries such as scikit-learn and Bokeh - provides all the necessary components for mining big data in an ecient and eective manner.  3. AGENDA The preliminary tutorial agenda is as follow:   Exploratory data analysis (45 minutes).   Predictive modeling (60 minutes).   Applying predictive models to educational datasets (60 minutes).   Introduction to Spark (30 minutes).  4. TARGET GROUP The tutorial appeals to researchers from a wide range of  backgrounds including big data, predictive analytics, learn- ing sciences, educational data mining and researchers inter- ested in learning more about how analytics can transform teaching and learning. As a prerequisite, attendees are re- quired to have familiarity with at least one programming language. According to previous experience from presenting simi-  lar tutorial at ECTEL, we will be expecting an estimated number of 20 participants. We will have our colleagues from McGraw-Hill Education to help with the logistics and to help the participants with the installation and use of material.  5. FORMAT Each of the four objectives mentioned above will be cov-  ered by a corresponding IPython notebook. Participants can download the data and notebooks before or during the session from our GitHub and stay engaged by running the notebook as we walk through them. At the end of each ob- jective, we will form multiple groups and will assign a mini project to each group to solve. We will discuss the outcome of the exercises before moving to the next objective.  6. REFERENCES [1] Christensen, C. M., Horn, M. B., Caldera, L.,  and Soares, L. Disrupting college: How disruptive innovation can deliver quality and aordability to postsecondary education. Innosight Institute (2011).  7. ORGANIZERS BIOGRAPHY Lalitha Agnihotri is a Senior Data Scientist at McGraw-  Hill Education. Lalitha Agnihotri holds a Ph.D. from Columbia University in Computer Science and has over 15 years of experience in the Data Mining/Modeling area. She has au- thored over 40 peer reviewed conference and journal papers and has presented at several international conferences. She has applied a wide variety of learning algorithms to huge amounts of data to enable applications related to prediction of outcomes.  Shirin Mojarad is a Data Scientist at McGraw-Hill Ed- ucation. She has wide experience in framing and conduct- ing complex analyses and experiments using large datasets to find trends in diverse data sources and analyze behav- ioral patterns using advanced statistical modeling and data mining techniques. Shirin was formerly a senior analytics specialist in the Advanced Analytics team at the Canadian Imperial Bank of Commerce (CIBC) and prior to that, a data mining consultant with a leading software company in predictive analytics. She received her Ph.D. in Electrical Engineering and her M.Sc. in Communications and Signal Processing from Newcastle University U.K., where she spe- cialized in predictive modeling and artificial neural networks.  Nicholas Lewkow is a Data Scientist at McGraw-Hill Edu- cation. He holds a PhD in computational astrophysics from the University of Connecticut. Additionally, he has con- ducted research in astrophysics and high performance com- puting at Oak Ridge National Laboratory and the Harvard- Smithsonian Center for Astrophysics. Nicholas is currently interested in big data analytics, parallel computing, and ma- chine learning.  Alfred Essa is Vice President R&D and Analytics at McGraw- Hill Education. Previously he was Director of Analytics Re- search & Strategy at Desire2Learn, where he led product development of the Student Success System the acquisition of Degree Compass and the architecture of the joint ana- lytics solution with IBM. He was Associate Vice Chancellor and Deputy CIO at Minnesota State Colleges & Universities where he led academic online strategy, enterprise infrastruc- ture services, network security academic technologies and web development. Previously he was CIO at MIT's Sloan School of Management, where he won an MIT Excellence Award, was Principal Investigator of the iLearn project, and founded an Open Source project called dotLRN.  http://educate.gast.it.uc3m.es/wapla/   Motivation  Objectives  Agenda  Target Group  Format  References  Organizers' Biography   "}
{"index":{"_id":"71"}}
{"datatype":"inproceedings","key":"Clow:2016:LF:2883851.2883918","author":"Clow, Doug and Ferguson, Rebecca and Macfadyen, Leah and Prinsloo, Paul and Slade, Sharon","title":"LAK Failathon","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"509--511","numpages":"3","url":"http://doi.acm.org/10.1145/2883851.2883918","doi":"10.1145/2883851.2883918","acmid":"2883918","publisher":"ACM","address":"New York, NY, USA","keywords":"failure, negative results, positive results, publication bias","abstract":"As in many fields, most papers in the learning analytics literature report success or, at least, read as if they are reporting success. This is almost certainly not because learning analytics research and activity are always successful. Generally, we report our successes widely, but keep our failures to ourselves. As Bismarck is alleged to have said: it is wise to learn from the mistakes of others. This workshop offers an opportunity for researchers and practitioners to share their failures in a lower-stakes environment, to help them learn from each other's mistakes.","pdf":"      LAK Failathon  Doug Clow1*, Rebecca Ferguson1, Leah Macfadyen2, Paul Prinsloo3, Sharon Slade1    1The Open University   Walton Hall, Milton Keynes  MK7 6AA, UK   Firstname.Surname@open.ac.uk     2The University of British Columbia,   Faculty of Arts,  1866 Main Mall,    Vancouver, BC, V6T 1Z1  leah.macfadyen@ubc.ca     3University of South Africa    3-15, Club 1, PO Box 392, Unisa   0003, South Africa   prinsp@unisa.ac.za       ABSTRACT  As in many fields, most papers in the learning analytics literature  report success or, at least, read as if they are reporting success.  This is almost certainly not because learning analytics research  and activity are always successful. Generally, we report our  successes widely, but keep our failures to ourselves. As Bismarck  is alleged to have said: it is wise to learn from the mistakes of  others. This workshop offers an opportunity for researchers and  practitioners to share their failures in a lower-stakes environment,  to help them learn from each others mistakes.    Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in   Education.   General Terms  Management, Human Factors.   Keywords  Failure, Publication bias, Positive results, Negative results.   1. MOTIVATION AND OBJECTIVES  1.1 The learning analytics literature is biased  Publication bias is a well-known problem in many empirical  disciplines, notably health and medicine. This is sometimes called  the file drawer effect. Researchers are incentivized to analyse  their data to find positive results; positive results are more likely  to be written up; and positive results are more likely to be  accepted as publications. Negative results are more likely to  languish, unloved, in file drawers.   The EU-funded Learning Analytics Community Exchange  (LACE) project1 is building an Evidence Hub for learning  analytics. The LACE Evidence Hub focuses on evidence for and  against a set of key propositions about learning analytics.  Evidence presented on the site is categorised according to whether  it supports or detracts from a particular proposition.                                                                         1 http://www.laceproject.eu      Preliminary results suggest that learning analytics is no exception  to the general trend: more than half the evidence in the Hub is  classified as positive, and most of the rest is neutral or mixed. (A  full account of this work is in preparation.)   Anecdotally, we know that not all learning analytics research  yields positive results, and that the majority of large-scale projects  encounter at least some serious problems. There are a few  examples of these issues being reported in the learning analytics  literature, such as Dawson & Macfadyens paper [1], but such  reports remain unusual.   Further, publication bias is a problem for well-conducted studies.  Very few outlets exist for those who want to publish accounts of  studies that failed to generate interpretable results because of  mistakes by the researchers.    Similar considerations apply to projects and activities applying  learning analytics: successful work is given far more prominence  than failure.   In summary, we tend to publicise our successes, and keep our  failures to ourselves.   1.2 This prevents effective learning within the  community  The printed literature enables us to learn of and from success.  Failure can be an extremely rich source of learning. Indeed, some  learning theories make explicit use of mistakes and failures in the  learning process.   Learning from ones own mistakes can be a very powerful source  of expertise. However, it is more efficient  and less unpleasant   to learn from other peoples mistakes too. But this is difficult  without access to information about failure.   1.3 Strong pressures keep it that way  Why is it like this   The incentives that contribute to publication bias are considerable,  and hard to change. In the medical field, there are moves such as  requiring pre-registration of trials and protocols, but the evidence  that these solve the problem is limited to date. Some journals now  explicitly welcome negative results, and some devoted purely to  negative results have sprung up.    However, the human pressure to publicise success but downplay  failure is likely to persist. Organisational pressures on researchers  and practitioners seem likely to increase.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, to republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.   LAK 16 Edinburgh, Scotland   Copyright 2016 ACM           1.4 A failure workshop can help  Such powerful social and systemic forces cannot be changed  quickly or easily.    However, there are opportunities for learning from each others  failures outside formal routes. The social spaces at LAK provide  informal opportunities for sharing these sorts of experience.    This workshop aims to offer a more explicit and structured space  for this to happen. We hope to create a space where researchers  and practitioners can learn from each others mistakes.   Many teachers will be familiar with learners being reluctant to  admit mistakes in public. This is one major rationale for the  existence of closed discussion forums (Learning Management  Systems/Virtual Learning Environments) in education: if the  discussion is entirely open to the world, learners may be too  reticent about failure to contribute. For this reason, this workshop  will be semi-private, held under the Chatham House Rule (see  below).   2. TARGET GROUP  This workshop is targeted at all researchers and practitioners in  learning analytics who are interested in learning from other  peoples failures, and are willing to share their own experience of  failure in the workshop.   No specific preparation is required, but participants will need to  be ready to talk about their own mistakes, and respect the  confidentiality of the session.   3. FORMAT  This will be a half-day workshop.   In the first session, there will be a series of presentations from the  organisers and other experienced learning analytics practitioners  (see below), who will present accounts of their own failures, with  time for discussion. After the coffee break, participants will  discuss their own failures in small groups, reporting summary  feedback to a short final plenary session.   To encourage free and frank discussion, the meeting will be held  under the Chatham House Rule:   When a meeting, or part thereof, is held under the Chatham  House Rule, participants are free to use the information received,  but neither the identity nor the affiliation of the speaker(s), nor  that of any other participant, may be revealed.2   Thus, participants can talk about or publish or act on what they  hear from other participants, but may not say who said it or what  organization or project it referred to. For the avoidance of doubt,  this applies to social media use as well: it is fine to tweet  interesting or amusing anecdotes, but not to attribute them  (directly or by implication) to either the person, the project, or the  organization.   4. ORGANISERS & PRESENTERS  This workshop is organized by the EU-funded Learning Analytics  Community Exchange (LACE) project. The organisers are  confirmed presenters in the morning session. Those listed as  presenters have agreed in principle to contribute.                                                                        2 https://www.chathamhouse.org/about/chatham-house-rule   4.1 Organisers & Presenters  Doug Clow, The Open University, UK   Doug Clow is a Senior Lecturer working on learning analytics at  The Open University, UK, and has more than 20 years  experience of projects harnessing new technology to improve  learning. He has published some of his successes at LAK and  tends to tell only his best friends about his failures. Doug is  currently part of a large-scale transformatory analytics programme  at the OU, and the LACE project. He has been co-organiser of  three SoLAR Flares in the UK.   Rebecca Ferguson, Open University, UK   Rebecca Ferguson is a Senior Lecturer at The Open University in  the UK, focused on educational futures, learning analytics,  MOOCs, augmented learning and online social learning. She is a  member of the steering committee of the Society for Learning  Analytics Research (SoLAR) and a Programme Chair of the  Practitioner Track at LAK16. She leads the LAEP project, which  is considering the implications and opportunities of learning  analytics for European educational policy. She co-chaired the 1st  and 2nd International Workshops on Discourse-Centric Learning  Analytics, held in Belgium and the US, as well as three SoLAR  Flares held in the UK.    Leah Macfadyen, The University of British Columbia   Leah Macfadyen is Program Director, Evaluation & Learning  Analytics  in the Faculty of Arts at the University of British  Columbia. Her applied research projects include development of  visual models of student enrollment pathways to assist with  curriculum re-development, social network analysis of learner  engagement patterns in MOOCs and LMS-based courses, and  visualization of themes in unstructured data (course evaluation  comments) as well as continued testing of models of student  activity and fine-grained indicators of achievement in online  courses. Her experience of the challenges of implementing  learning analytics in her large institution has pushed her to write  and think about strategic approaches for implementing learning  analytics at scale. She is also a member of the SoLAR Executive.  Paul Prinsloo, UNISA   Paul Prinsloo is a Research Professor in Open Distance Learning  (ODL) in the College of Economic and Management Sciences,  University of South Africa (Unisa). His academic background  includes fields as diverse as theology, art history, business  management, online learning, and religious studies. Paul is an  established South African National Research Foundation (NRF)  rated researcher and has published numerous articles in the fields  of teaching and learning, student success in distance education  contexts, learning analytics, curriculum development and  corporate citizenship. He was awarded international fellowships to  the Open University in 2007, 2009, and 2010 and received the  Unisa Chancellors Award for Outstanding Research in 2008.   Sharon Slade, Open University, UK   Sharon is a Senior Lecturer and Regional Manager in the Open  University Business School. She leads and participates in projects  which feed into teaching and learning across the Open University.  For example, she acted as the Business Lead for 3 institutional  projects linked to the provision of tailored curriculum-based  student support. She is also chairing the team which has recently  developed a new OU policy for the ethical use of learning           analytics. Her current research interests relate to ethical issues in  learning analytics.   4.2 Presenters  (Subject to timetabling constraints with other workshops at LAK.)   Shane Dawson, University of South Australia   Shane Dawson is the Director of the Teaching Innovation Unit at  the University of South Australia. Shane's research focuses on the  use of social network analysis and learner ICT interaction data to  inform and benchmark teaching and learning quality. Shane is a  founding executive member of the Society for Learning Analytics  Research and past conference chair of the International Learning  Analytics and Knowledge conference. He is a co-developer of  SNAPP an open source social network visualization tool designed  for teaching staff to better understand, identify and evaluate  student learning, engagement, academic performance and creative  capacity.   Hendrik Drachsler, Open University, NL   Hendrik Drachsler is Assistant Professor at the Welten Institute of  the Open University of the Netherlands. His research interests  include Learning Analytics, Personalisation technologies,   Recommender Systems, Educational data, Open Science, mobile  devices, and their applications in the fields of technology  enhanced learning, science 2.0, and health 2.0. He is chairing the  EATEL SIG dataTEL and the national SIG Learning Analytics of  the Dutch umbrella organisation SURF. He is WP2 leader of the  LinkedUp project and the scientific coordinator of LACE project.   Maren Scheffel, Open University, NL   Maren Scheffel is a researcher and PhD candidate at the Welten  Institute of the Open University of the Netherlands. She is a  computational linguist and has been working in the field of  technology enhanced learning (TEL) for several years where she  was involved in several national and international TEL projects.  Currently, she focuses her research on learning analytics,  reflection and awareness support, personalisation and educational  data. She is currently working on the LACE project.    5. REFERENCES  [1] Macfadyen, L.P. and Dawson, S. 2012. Numbers Are Not   Enough. Why e-Learning Analytics Failed to Inform an  Institutional Strategic Plan. Educational Technology &  Society. 15, 3 (2012), 149163.         "}
{"index":{"_id":"72"}}
{"datatype":"inproceedings","key":"Mol:2016:LTG:2883851.2883859","author":"Mol, Stefan and Kobayashi, Vladimer and Kismih'ok, G'abor and Zhao, Catherine","title":"Learning Through Goal Setting","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"512--513","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883859","doi":"10.1145/2883851.2883859","acmid":"2883859","publisher":"ACM","address":"New York, NY, USA","keywords":"goal setting, learning analytics, learning record store","abstract":"Despite the mounting evidence supporting the role that goal setting has on the learning process, there seems to be only a handful of studies that directly investigate goal setting in the context of Learning Analytics (LA). Although investigations have incorporated elements of goal setting, the attention afforded to theory and operationalization have been modest. In this workshop we plan to position goal setting at the forefront of LA research. The workshop will serve as a venue to bring together researchers interested in advancing Goal Setting (GS) research in the LA field. Topics include: (1) GS theory and measurement; (2) analysis and visualization of GS data; (3) strategies for integrating GS in the learning experience; and (4) implementation of GS technologies. Participants who need tools to execute their GS ideas and those who already have tools and are exploring better ways to integrate a goal setting feature can gain a lot from this workshop. Moreover, participants will have the opportunity to contribute to the conceptualization and staging of GS ideas in LA research.","pdf":"Learning Through Goal Setting  Stefan Mol   University of Amsterdam  Plantage Muidergracht 12   1018TV Amsterdam, Netherlands  S.T.Mol@uva.nl   Vladimer Kobayashi  University of Amsterdam  Plantage Muidergracht 12   1018TV Amsterdam, Netherlands  V.Kobayashi@uva.nl   Catherine Zhao  UNSW Australia   Kensington Campus  Kensington UNSW, Australia 2033   catherine.zhao@unsw.edu.au     Gbor Kismihk  University of Amsterdam  Plantage Muidergracht 12   1018TV Amsterdam, Netherlands  G.Kismihok@uva.nl        ABSTRACT  Despite the mounting evidence supporting the role that goal  setting has on the learning process, there seems to be only a  handful of studies that directly investigate goal setting in the  context of Learning Analytics (LA). Although investigations have  incorporated elements of goal setting, the attention afforded to  theory and operationalization have been modest. In this workshop  we plan to position goal setting at the forefront of LA research.  The workshop will serve as a venue to bring together researchers  interested in advancing Goal Setting (GS) research in the LA  field. Topics include: (1) GS theory and measurement; (2)  analysis and visualization of GS data; (3) strategies for integrating  GS in the learning experience; and (4) implementation of GS  technologies. Participants who need tools to execute their GS  ideas and those who already have tools and are exploring better  ways to integrate a goal setting feature can gain a lot from this  workshop. Moreover, participants will have the opportunity to  contribute to the conceptualization and staging of GS ideas in LA  research.   CCS Concepts   Applied ComputingEducationE-learning.  Keywords  Goal Setting; Learning Analytics; Learning Record Store   1. MOTIVATION  Since learning is oftentimes a goal oriented process, Goal Setting  (GS) theory with its firm track record in educational research,  specifically in studies about student motivation, has the potential  to address one of the aims of Learning Analytics, that is, to  optimize learning [2, 4, 5, 7]. Learning goals guide learners to  where learning is relevant and enable them to focus their effort  and attention to goal-related activities [3]. The focus is made   possible by highlighting aspects important for learning while  ignoring irrelevant ones.  Another way in which goal setting might contribute to advancing  LA is that it may be used to facilitate self-reflection through goal  progress tracking. When learners set goals they engage in what is  called a discrepancy-creating process [6]. The subsequent process  of resolving this discrepancy allows learners to reflect on their  performance and to adjust the magnitude and direction of their  effort to match goal requirements. Goal setting as a self-reflection  mechanism also has important implications for self-regulated  learning (SRL). In a study involving high achieving students, it  was found that an increase in mastery goals led to the increasing  use of SRL strategies [1].   Goal reasoning, a framework dedicated to the application of goal  setting in the context of intelligent systems, is investigating ways  of giving artificial systems the autonomy to formulate, select and  manage goals. LA, being at  the crossroad of educational research  and information technology, has a lot to gain from this framework  [8]. As learners interact with educational technologies that  support goal setting, these technologies may collect and analyze  learners goal orientations and goal setting activities. Using this  setup it may be feasible to provide students with goal  recommendations that are tailored to their own context,  personality, preferences, and orientations. With the  implementation of such technologies, goal setting may contribute  to yet another aim of LA, that is, prediction which refers to  forecasting learners next move. One of its applications is in  predicting student drop-out or attrition. In the context of goal  setting, goal prediction refers to forecasting what goals that  students might want to pursue next in order to achieve a learning  outcome. What we are envisaging then is that goals which were  shown to lead to better performance in one cohort of students are  recommended to individual students in the subsequent cohort.  A final way in which goal setting might contribute to advancing  LA is that it has the potential to generate primary data pertaining  to students offline learning activities, thereby addressing an  important limitation of those LA studies that are based on  secondary data generated by administrative systems or electronic  learning environments.  Given these potential contributions, the time is ripe to bring  together LA and Goal Setting researchers in an open marketplace  of ideas. This workshop will be an opportunity to consolidate  research findings from educational research, learning technologies  and research within LA itself.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  users, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883859     2. OBJECTIVES  The main objectives of the workshop is to bring together  researchers, so as to discuss ongoing research and identify  promising directions for studies that apply GS in the LA context.  It hopes to bring together researchers from both the learning side  and analytics side to discuss the underpinning rationale for  design choices in the context of data-driven learning and teaching.  Specifically we will discuss the theoretical and empirical aspects  of traditional GS as applied to education. Moreover, the workshop  will also serve as a venue where technologies supporting goal  setting can be demonstrated. In this regard, the workshop will  showcase a work in-progress experiment in which goal-setting is  embedded in courses using a standalone dashboard. It will  demonstrate the quality of LMS data with regard to applying LA  techniques to extract meaning. Lastly, analytics applied to data  collected through goal setting technologies will be addressed.   3. WORKSHOP FORMAT  3.1 Audience  The workshop is open to all LA researchers with an interest in  GS. Relevant fields on the learning side are learning science,  educational psychology, assessment and evaluation, curriculum  and pedagogy design, management (because goal-setting has been  widely used in HR management across industries, experts in this  field bring in knowledge of this concept). Relevant fields on the  data side are educational data mining, machine learning, HCI,  computer sciences, and educational statistics.  There will be a call for papers1 inviting contributions relevant to  the workshop topics. The topics are aggregated into four topical  groups, namely, (1) GS theory, (2) GS measurement and analytics,  (3) the relationship between GS and other educational outcomes  such as interest, motivation, engagement, and academic  performance, and (4) GS technologies. Accepted papers will be  published in the CEUR workshop proceedings.   3.2 Format  We propose a half-day workshop with at most four paper  presentations and at most two software demonstrations.  The first part of the workshop will be an introduction of the  workshop by the organizers. This will be followed by a keynote  about Goal Setting in the academic context. Afterwards, workshop  participants will be assigned to three groups. Each group will  discuss one of these three topic groups: GS Theory, GS  measurement and analytics, and Relationship between GS and  other education outcomes.  The lead discussants of each group are the authors of the accepted  papers assisted by the organizers. Authors of accepted papers are  requested to prepare a 10-15 minute presentation and the  presentations should incorporate what have transpired during the  group discussion.   Group discussion will last for an hour. While having discussions  the participants can pause for a coffee break. After group  discussions, the presentations will start. There will be a short  question and answer portion after every presentation. Aside from  the presentation there will be a demo of GS technologies and  sample use cases. The demo will cover the fourth topical group  which is the GS technologies. After all presentations are done,                                                                    1 see https://sites.google.com/site/lakgoalsetting/   there will be an open forum. The open forum will be directed to  further elaborate the role of GS in the LA setting and discuss  outstanding issues. The workshop will end by wrapping up the  main points that have emerged during the discussions and explore  the possibility of establishing a GS Special Interest Group (SIG)  within LA. The SIG will have the primary mandate of advancing  and popularizing GS research in the context of LA.   3.3 Tentative Schedule  13:00-13:15 Welcome and Introduction (by the Organizers).  13:15-14:00 Keynote.  14:00-15:00 Break-out Groups with Coffee Break.  15:00-15:30 Group Presentations  15:30-15:45 Demo of Goal Setting Software.  15:45-16:15 Open Forum.  16:15-16:30 Wrapping up and formation of the SIG.   4. ACKNOWLEDGMENTS  The authors acknowledge(s) the financial support of the  Eduworks Marie Curie Initial Training Network Project (PITN- GA-2013-608311) of the European Commissions 7th Framework  Program.   5. REFERENCES  [1] Ablard, K.E. and Lipschultz, R.E. 1998. Self-regulated   learning in high-achieving students: Relations to advanced  reasoning, achievement goals, and gender. Journal of  Educational Psychology. 90, 1 (1998), 94101. DOI=  http://dx.doi.org/10.1037/0022-0663.90.1.94   [2] Gegenfurtner, A. and Hagenauer, G. 2013. Achievement  goals and achievement goal orientations in education.  International Journal of Educational Research. 61, (2013),  14. DOI= http://dx.doi.org/10.1016/j.ijer.2013.08.001   [3] Latham, G.P. and Locke, E.A. 2007. New Developments in  and Directions for Goal-Setting Research. European  Psychologist. 12, 4 (Jan. 2007), 290300. DOI=  http://dx.doi.org/10.1027/1016-9040.12.4.290   [4] Lavasani, M.G., Weisani, M. and Ejei, J. 2011. The role of  achievement goals, academic motivation, and learning  strategies in statistics anxiety: Testing a causal model.  Procedia - Social and Behavioral Sciences. 15, (2011),  18811886. DOI=  http://dx.doi.org/10.1016/j.sbspro.2013.12.810    [5] Leake, D.B. and Ram, A. 1993. Goal-Driven Learning:  Fundamental Issues: A Symposium Report. AI Magazine.  14, 4 (1993), 67. DOI=   http://dx.doi.org/10.1609/aimag.v14i4.1069   [6] Locke, E.A. and Latham, G.P. 2002. Building a practically  useful theory of goal setting and task motivation. A 35-year  odyssey. The American Psychologist. 57, 9 (Sep. 2002),  705717. DOI= http://dx.doi.org/10.1037/0003- 066X.57.9.705   [7] Meece, J.L., Anderman, E.M. and Anderman, L.H. 2006.  Classroom Goal Structure, Student Motivation, and  Academic Achievement. Annual Review of Psychology. 57,  1 (Jan. 2006), 487503. DOI=  http://dx.doi.org/10.1146/annurev.psych.56.091103.070258   [8] Vattam, S., Klenk, M., Molineaux, M. and Aha, D.W. 2013.  Breadth of approaches to goal reasoning: A research  survey. DTIC Document.     "}
{"index":{"_id":"73"}}
{"datatype":"inproceedings","key":"vanLeeuwen:2016:LAF:2883851.2883874","author":"van Leeuwen, Anouschka","title":"Learning Analytics in a Flipped University Course","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"514--515","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883874","doi":"10.1145/2883851.2883874","acmid":"2883874","publisher":"ACM","address":"New York, NY, USA","keywords":"blended learning, formative assessment, higher education, learning analytics, web lectures","abstract":"In this poster, we describe the design of a university course with a blended learning character. Learning analytics were used both within the course to facilitate effective teacher-student interaction, as well as after the course to examine patterns between students' activities during the course and their performance on the test and the group assignment at the end of the course.","pdf":"Learning analytics in a flipped University course  Anouschka van Leeuwen   Utrecht University  Utrecht, the Netherlands  A.vanLeeuwen@uu.nl                   ABSTRACT  In this poster, we describe the design of a university course with a  blended learning character. Learning analytics were used both  within the course to facilitate effective teacher-student interaction,  as well as after the course to examine patterns between students  activities during the course and their performance on the test and  the group assignment at the end of the course.    ACM classification   Applied computing   Law, social and behavioral sciences    Psychology  Human-centered computing   Visualization    Visualization design and evaluation methods   Keywords  Higher education, blended learning, learning analytics, web  lectures, formative assessment   1. INTRODUCTION  Blended learning (BL) is defined as the combination of online and  face-to-face learning activities [1]. BL has many possible forms of  implementation, differing according to the ordering, frequency,  and type of teacher-learner interaction within these activities.    One type of BL that is increasingly often implemented in higher  education is the so called Flipped Classroom, in which students  rotate on a fixed schedule between face-to-face teacher-guided  practice on campus and online delivery of content and instruction  of the same subject from a remote location (often home) after  school [1]. In this context, online activities serve as a tool for  studying learning materials in preparation for face-to-face  meetings, so that more time can be spent on practicing skills and  receiving feedback instead of rather than a teacher explaining the  material. The main idea is that each medium  face-to-face and  online settings  is used optimally: face-to-face meetings for  interaction and feedback with a teacher, and the online medium to  study material in a variety of ways, making use of the possibilities  of for example multimedia and discussion fora [2].   Because BL involves online learning activities, there are  opportunities to automatically log and analyze these activities and  use the results to further enhance learning and teaching processes,  for example by providing the teacher with summaries of students  activities [3]. When student data are used in this way, this is  called learning analytics (LA) [4]. Within the present study, LA  are used in two ways.    First of all, within the course, LA are used as indicated above,  namely to inform teachers of students online activities. Secondly,  on a higher level, LA are used to examine the relationship  between students activities and their final performance within the  course. The results may then be used in the subsequent year to  predict and timely intervene when students show behavior  indicating the possibility of failing the course [5].     Thus, we investigate the usefulness of LA during the course, but  also use LA techniques at the end of the course to examine  relationships between process and outcome of learning. These two  aspects are reflected in the two research questions.   Research questions:    To what extent do blended learning elements and the use  of  learning analytics enhance teaching and learning processes during  the course   What is the relationship between (online and face-to-face)  learning activities and (individual and group) performance in the  course   2. METHOD  2.1 Course description   The course under study is about Educational Design, and was  offered at Utrecht University in the Netherlands. Between 100 and  150 bachelor students are registered for the course. The goals of  the course are for students to gain theoretical knowledge about  designing processes, as well as for them to practice these skills by  working on designing an actual training as requested by a  company or school in the Universitys network. Besides working  on this collaborative assignment, students also complete an  individual test at the end of the course.   2.2 Blended elements / Course design  The course consists of the following weekly activities (see Figure  1). First of all, there are plenary lectures, for which the students  prepare in two ways. They are supposed to 1) study the material  belonging to that weeks topic, either by reading the book or by  viewing the web lectures that explain the same content, and 2) to  formulate one question about the material for other students to test  whether they understood the material.   In the lecture, the focus is on deepening understanding of the  material by offering insights from recent research and by offering  practical examples.    Permission to make digital or hard copies of part or all of this work for personal or  classroom use is granted without fee provided that copies are not made or  distributed for profit or commercial advantage and that copies bear this notice and  the full citation on the first page. Copyrights for third-party components of this work  must be honored. For all other uses, contact the Owner/Author.    Copyright is held by the owner/author(s).   LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00   DOI: 10.1145/2883851.2883874     A few days after the lecture, students have working groups. In  these sessions, the questions that students formulated are used as  formative assessments. The answers to the questions are given by  means of electronic voting system, which means the teacher is  able to see at a glance which answers are given most. Each  question and its possible answers are discussed. The remainder of  the working group is devoted to working on the group  assignment.     Preparation   Lecture  Working   group  - Viewing web  lectures /  studying book   - Formulating  one question  about the  material    - Deepening  understan- ding of  course  material    - Formative  assessment  - Discussion of  course material  - Working on  group  assignment     Figure 1: Weekly schedule of activities within the course.      2.3 Learning analytics within the course  During the course, teachers will be provided with summaries of  students activities in two ways. First of all, teachers are able to  see how often the web lectures are viewed, and which parts of the  web lecture are re-viewed most (see Figure 2) [6]. This could  indicate that particular concepts need additional explanation  during the working groups. Furthermore, teachers receive  immediate information on students performance on the formative  assessments, because students participate in these tests using  electronic voting systems.       Figure 2: Dashboard of web lecture viewing patterns.   2.4 Collected data  Data is collected about each type of activity, see Table 1. Viewing  of web lectures is automatically logged by the system (see section  2.3). Attendance of the lectures is obtained by manually keeping a  list each week. The questions that students submit for the  formative assessment are coded for the type of question using  Blooms taxonomy (for example, questions concerning definitions  of concepts versus applying concepts). The performance on  formative assessments during the working groups is also  automatically logged (see section 2.3). The performance measures  are obtained by teacher grading of both individual tests and group  product.  Teacher usage of LA that are provided to them is investigated by  means of teacher interviews. Students perceptions of the  effectivity of BL elements in the course are examined using focus  group interviews.   Table 1: Overview of collected data.   Web  lectures Lectures   Formative  assessment   Course  performance   - Viewing  frequency  - Density of  re-viewing  within web  lecture   - Attendance  rate   - Type of  submitted  questions  - Performance  on  assessments   - Score on  final test  - Score on  group  assignment   General measures  - Observation of teaching processes, using interview/log   book methodology  - Focus group interview with set of students   3. ANALYSIS / EXPECTED RESULTS  3.1 Using learning analytics within the course  To investigate whether the use of blended learning and LA  enhanced teaching and learning processes (research question 1),  we will code the student and teacher interviews using thematic  analysis and combine these with the observed activities during the  course. This will lead to a qualitative description of the teaching  and learning processes that occurred during the course.    3.2 Learning analytics at overall course level  To investigate patterns between learning activities and course  performance (research question 2), we will use the individual test  grade and group performance grade as dependent variables, and  the learning activities as outlined in Table 1 as independent  variables in a structural equation model.    4. PRESENTATION AT LAK16  The course in which the data is collected will run from February 8  until April 15, 2016, which means we will have just finished data  collection at the start of the LAK16 conference. Preliminary  analyses will be presented, especially concerning the within  course use of learning analytics (see section 3.1)., and the opinion  of other scholars will be welcome in deciding on the analyses at  course level (section 3.2).   5. REFERENCES  [1] Staker, H., & Horn, M. B. (2012). Classifying K-12 Blended   Learning. Innosight Institute.  [2] Bishop, J. L., & Verleger, M. A. (2013). The flipped   classroom: A survey of the research. Proceedings of the  ASEE Annual Conference, 2326 June 2013, Atlanta, GA.   [3] Van Leeuwen, A., Janssen, J., Erkens, G., & Brekelmans, M.  (2014). Supporting teachers in guiding collaborating  students: Effects of learning analytics in CSCL. Computers  & Education, 79, 2839.    [4] Siemens, G., & Gasevic, D. (2012). Guest editorial   learning and knowledge analytics. Educational technology &  Society, 15(3), 1-2.   [5] Tanes, Z., Arnold, K. E., King, A. S., Remnet, M. A. (2011).  Using Signals for appropriate feedback: Perceptions and  practices. Computers & Education, 57(4), 2414-2422.   [6] Obtained from http://Lecturenet.sites.uu.nl     "}
{"index":{"_id":"74"}}
{"datatype":"inproceedings","key":"Grover:2016:MAS:2883851.2883877","author":"Grover, Shuchi and Bienkowski, Marie and Tamrakar, Amir and Siddiquie, Behjat and Salter, David and Divakaran, Ajay","title":"Multimodal Analytics to Study Collaborative Problem Solving in Pair Programming","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"516--517","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883877","doi":"10.1145/2883851.2883877","acmid":"2883877","publisher":"ACM","address":"New York, NY, USA","keywords":"collaboration, collaborative problem solving, k-12 computer science education, kinect, multimodal analytics, pair programming","abstract":"Collaborative problem solving (CPS) is seen as a key skill in K-12 education---in computer science as well as other subjects. Efforts to introduce children to computing rely on pair programming as a way of having young learners engage in CPS. Characteristics of quality collaboration are joint exploring or understanding, joint representation, and joint execution. We present a data driven approach to assessing and elucidating collaboration through modeling of multimodal student behavior and performance data.","pdf":"Multimodal Analytics to Study Collaborative Problem  Solving in Pair Programming   Shuchi Grover  SRI International  Menlo Park, CA   shuchi.grover@sri.com  Behjat Siddiquie   SRI International  Princeton, NJ   behjat.siddiquie@sri.com   Marie Bienkowski  SRI International  Menlo Park, CA   marie.bienkowski@sri.com  David Salter    SRI International  Princeton, NJ   david.salter@sri.com     Amir Tamrakar  SRI International   Princeton, NJ  amir.tamrakar@sri.com   Ajay Divakaran  SRI International   Princeton, NJ  ajay.divakaran@sri.com     ABSTRACT  Collaborative problem solving (CPS) is seen as a key skill in K-12  educationin computer science as well as other subjects. Efforts  to introduce children to computing rely on pair programming as a  way of having young learners engage in CPS. Characteristics of  quality collaboration are joint exploring or understanding, joint  representation, and joint execution. We present a data driven  approach to assessing and elucidating collaboration through  modeling of multimodal student behavior and performance data.   CCS Concepts   Human-centered computing ~ Empirical studies in  collaborative and social computing     Keywords  Collaboration, Collaborative Problem Solving, Pair programming,  Kinect, Multimodal analytics, K-12 Computer Science Education.      1. INTRODUCTION  Collaborative problem solving (CPS) is increasingly  acknowledged as an important 21st century skill across all  disciplines and especially in STEM subjects [7]. Recent efforts to  identify the core skills that constitute computational thinking in  K-12 computer science (CS) [2] have highlighted both the  collaborative and the problem-solving nature of CS.    Programming is an essential part of the CS problem solving and  design process, and a central feature of most introductory CS  curricula is learning to program. Pair programming has been  shown to increase programming confidence and satisfaction in  college settings [3, 5], and is now being advocated in pre-college  settings by US advocacy organizations such as NCWIT and  Code.org. A collaborative approach to computer programming  where two people work sharing the same computer and mouse,  pair programming provides a unique context to better understand  the process of CPS in introductory CS classrooms in K-12.    Past research shows that the quality of student interactions has a  direct influence on their learning during collaborative activities  and that successful collaboration does not occur by itself and   needs to be supported [4]. Measuring and assessing interaction  and collaboration is challenging but necessary if it needs to be  supported in learning environments. The quality of pair  programming in process remains relatively understudied. Self- reports and reflections are helpful but imprecise and incomplete.  Traditional ethnographic and qualitative research techniques are  painstaking and do not scale, in addition to providing information  only post-hoc. Multimodal analytics can play a significant role in  this area, since they aim at deriving high-quality information from  audiovisual and sensorial input (Figure 1), which can help identify  interactions and behaviors that afford the drawing of inferences  based on the temporal patterns in the multimodal signals.     Figure 1: Range of information that can be captured by   unobtrusive instrumentation.   This paper presents our preliminary work in studying the quality  of collaboration using multimodal data gleaned from unobtrusive  sensors as pairs solve programming puzzles. We define quality  collaboration as the process of sharing understanding, and pooling  skills, effort, and knowledge to solve problems [7]. This work  represents a first step in combining data from the physical  environment with process data from the programming  environment to study how collaboration impacts the programming  process, with the view to eventually providing feedback to the  teachers and learners in addition to scaffolds for supporting the  complex process of CPS during programming.    2. DYADIC COLLABORATION IN PAIR  PROGRAMMING   The goal of this ongoing research is to develop and test  computational models of social interaction, in CPS environments.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom   ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883877   mailto:shuchi.grover@sri.com mailto:behjat.siddiquie@sri.com mailto:marie.bienkowski@sri.com mailto:david.salter@sri.com mailto:amir.tamrakar@sri.com mailto:ajay.divakaran@sri.com   We begin with approaches to classify the quality of collaboration  from Kinect data on the body movements and gestures of the  pair programmers working together at a computer.    2.1 Pilot Study: Procedures & Data Capture    Our pilot study involves unobtrusively capturing multimodal data  (video, audio, clickstream, and screen capture) from pairs of  children as they work together on a pair-programming task. The  tasks are those from the open source Blockly Games (blockly- games.appspot.com, on which the popular Hour of Code activities  are based as well as other applications). We developed an  instrumented version of Blockly (developers.google.com/blockly/)  to record clickstream data on programming actions for a related  learning analytics research effort. We record audio and actions on  the screen using the Camtasia screen-capture application. We  record video of the participants seated side-by-side, covering the  area from the keyboard up to include their whole face, using the  Kinect (Figure 2). The Kinect captures a video recording of the  scene as well as sensing and recording body position: position of  the head, arms, and torso; in addition to eye gaze and audio.   We have currently captured data from 4 pairs of children aged  between 11 and 14 as they worked on the 10 levels of the Maze  and Pond Blockly games. For each pair, we introduced pair  programming as the way they should approach the problems,  which uses driver and navigator roles. The driver controls the  mouse and keyboard, and the navigator provides support by  discussing and checking the driver's work. Both partners are  responsible for design and development of the solution. The pairs  were instructed to switch driver and navigator roles after they  solved each level of the Blockly Games (10 levels in each). We  collected 8 videos varying in length between 30 and 60 minutes.  .    Figure 2: Data capture with Kinect mounted on display.   2.2 Data Analyses  The first step in creating the computational models was to  manually code some of the recorded videos to use as training  examples for the machine learning algorithms. Based on an initial  viewing of the videos captured from different pairs, we decided  that using a 1-minute time interval would help us make reliable  judgments about the pair interaction. We qualitatively coded  videos for evidence of collaboration, roughly averaged across a 1- minute interval, as High, Medium, or Low. The coding scheme  was based on observables used in past research [3, 8]. For pair  programming, our observables include screen pointing, leaning  forward, joint attention (looking at screen), taking the mouse  (with or without consent) and synchrony in body position. These  were used to train machine learning algorithms that classify the  ongoing collaboration into the three aforementioned categories.    We leverage past work modeling multimodal predicates such as  engagement by capturing their spatiotemporal constituents [1].   We use the raw features derived from the skeleton tracking of the  two subjects with the goal of predicting the level/strength of their  collaboration. Since the level of collaboration can be low,  medium, or high, random classification accuracy is 33%. Using  the 13 joints from the upper body, we extracted a set of first order  static and dynamic handcrafted skeleton features. Given these  features we trained a Support Vector Machine based classifier to  perform classification. Preliminary evaluations on four sessions  consisting of two Pond and two Maze games containing a total of  about 117 annotated one minute segments, show that we can  detect the level of collaboration with 48% accuracy which is  significantly better than random. The accuracy was determined  using ten-fold randomized cross-validation. While further work is  required to conclusively establish the effectiveness of our  classifiers and features, our preliminary results are encouraging.    3. CONCLUSION AND FUTURE WORK  Our ongoing work on this pilot will further our understanding of  computational models that can automatically detect collaboration  from gesture, posture, and body movement data, and assess its  quality from pairs of students working on programming. Future  work includes additional data capture and adding analyses of eye  gaze and speech. Furthermore, we plan to apply Discriminative  Conditional Restricted Boltzmann Machines [1] that will allow us  to identify constituents of high collaboration interactions e.g.,  synchronized arm motions of the pair. Eventually, we aim to  combine these with clickstream data from the software  environment that we are capturing in order to get a holistic picture  of how the collaboration impacts the problem solving involved in  the programming task. Although this work is exploratory and  broader adoption depends on data sharing issues that the field is  working to resolve, such analyses have the potential to allow us to  assess and scaffold the collaboration in complex problem solving  settings such as programming.   REFERENCES  [1] Amer, Mohamed R., et al. 2015. Human Social Interaction   Modeling Using Temporal Deep Networks. arXiv preprint  arXiv:1505.02137 (2015).   [2] Astrachan, O., & Briggs, A. (2012). The CS principles  project. ACM Inroads, 3(2), 38-42.   [3] Hannay, J. E., Dyb, T., Arisholm, E., & Sjberg, D. I. 2009.  The effectiveness of pair programming: A meta-analysis.  Information and Software Technology, 51(7), 1110-1122.   [4] Hesse, F., Care, E., Buder, J., Sassenberg, K., & Griffin, P.  2015. A framework for teachable collaborative problem  solving skills. In Assessment and teaching of 21st century  skills (pp. 37-56). Springer Netherlands.   [5] McDowell, C. et al. 2006. Pair programming improves  student retention, confidence, and program quality.  Communications of the ACM, 49(8), 90-95.   [6] Meier, A., Spada, H., & Rummel, N. 2007. A rating scheme  for assessing the quality of computer-supported collaboration  processes. International Journal of Computer-Supported  Collaborative Learning, 2(1), 63-86.   [7] Organization for Economic Co-operation and Development  (OECD). 2013. PISA 2015 draft collaborative problem  solving framework.   [8] Rummel, N., & Spada, H. 2005. Learning to collaborate: An  instructional approach to promoting collaborative problem  solving in computer-mediated settings. The Journal of the  Learning Sciences, 14(2), 201-241.     1. INTRODUCTION  2. Dyadic Collaboration in Pair Programming  2.1 Pilot Study: Procedures & Data Capture  2.2 Data Analyses   3. Conclusion and Future Work  Our ongoing work on this pilot will further our understanding of computational models that can automatically detect collaboration from gesture, posture, and body movement data, and assess its quality from pairs of students working on programming. Futu...  References   "}
{"index":{"_id":"75"}}
{"datatype":"inproceedings","key":"Hu:2016:AAC:2883851.2883963","author":"Hu, Xiao and Ng, Tzi-Dong Jeremy and Tian, Lu and Lei, Chi-Un","title":"Automating Assessment of Collaborative Writing Quality in Multiple Stages: The Case of Wiki","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"518--519","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883963","doi":"10.1145/2883851.2883963","acmid":"2883963","publisher":"ACM","address":"New York, NY, USA","keywords":"automated assessment, metadiscourse, wiki","abstract":"This study attempts to investigate to what extent indicators of academic writing and cognitive thinking can help measure the writing quality of group collaborative writings on Wikis. Particularly, comparisons were made on Wiki content in different stages of the projects. Preliminary results from a multiple linear regression analysis reveal that linguistic indicators such as engagement markers and self-mention were significant predictors in earlier stages to the projects, whereas verbs indicating cognitive thinking in the evaluation level were significant in later project stages.","pdf":"Automating Assessment of Collaborative Writing Quality  In Multiple Stages: The Case of Wiki   Xiao Hu1, Tzi-Dong Jeremy Ng1, Lu Tian1, Chi-Un Lei2  1 Faculty of Education, The University of Hong Kong, Pokfulam, Hong Kong   2 Technology-Enriched Learning Initiative, The University of Hong Kong, Pokfulam, Hong Kong  {xiaoxhu, jeremyng, u3522789, culei}@hku.hk     ABSTRACT  This study attempts to investigate to what extent indicators of  academic writing and cognitive thinking can help measure the  writing quality of group collaborative writings on Wikis.  Particularly, comparisons were made on Wiki content in different  stages of the projects. Preliminary results from a multiple linear  regression analysis reveal that linguistic indicators such as  engagement markers and self-mention were significant predictors  in earlier stages to the projects, whereas verbs indicating cognitive  thinking in the evaluation level were significant in later project  stages.   CCS Concepts   Human-centered computingWikis    Social and  professional topicsStudent assessment    Computing  methodologiesNatural language processing    Applied  computingCollaborative learning   Keywords  Wiki,  automated assessment, metadiscourse.   1. INTRODUCTION  Wiki is a platform where users or visitors can create new pages,  modify existing ones, and re-structure the hierarchy of pages,  serving as a collaborative authoring environment for  complementing and enhancing online collaboration. The  simplicity and flexibility of Wikis make it a predominant tool  employed by instructors in this age of collaborative learning.   However, the use of Wikis might raise the complexity of the  learning environment for the instructors, since they have to  monitor quantity and quality of the shared written work.  Automated evaluation of writing quality can hence be  experimentally applied to scoring Wikis in terms of their contents.  In view of the under-studied topic of automating the assessment of  writing quality on collaborative Wikis, this study attempts to  explore to what extent common linguistic features of academic  writing and cognitive thinking can be used to predict the quality  of students online collaborative writing. In addition, as one of the  major advantages of Wiki is to be able to record all historical  versions of the pages, it provides an ideal case to study the  evolution of student collaborative writing. Therefore, in this study  we examine Wiki pages at different stages of the collaborative   writing projects. Specifically, this study aims to answer: to what  extent can linguistic indicators of academic writing and cognitive  thinking help measure the quality of collaborative writing in  different stages   2. LEARNING ANALYTICS IN ASSESSING  COLLABORATIVE WRITING  Various existing studies have been conducted to design analytical  tools to help teachers observe and monitor the progress and  contributions of students on their group assignments, but most of  them focused on the quantitative aspects such as revision counts  and words added. The concern remains as to whether the quality  of collaborative writing on Wiki is automatically assessable.  Previous studies applied Natural Language Processing (NLP)  techniques to automatically assessing free-text answers in  examinations. Dowell et al. [3] analyzed the use of discourse  features to predict group performance during collaborative  learning interactions. However, they explored the content of group  chats rather than the writing itself. In another LA-infused study,  Kim et al. [6] analyzed the topic of each collaborative Wiki page,  but did not assess the quality of writing. Another pertinent study  [2] probed numerous quality indicators and looked into how  capable these indicators were to assess the quality of Wikipedia  articles. Coming close to the conceptual framework of the present  study, their study paves the way for our study to evaluate to what  extent textual features (i.e., quality indicators) of collaborative  writing can help accurately and comprehensively assess Wikis  created by university students for knowledge co-construction,  instead of the Wikipedia articles created by the public.   3. METHODOLOGY  Wikis in three undergraduate courses at the University of Hong  Kong were selected for the present study, namely two under the  Bachelor of Science in Information Management programme  (BSIM3004  Information Retrieval; BSIM4018 Data Warehousing  and Data Mining) and another being a common core course  (CCST9003 Everyday Computing and the Internet) that all  undergraduate students could enroll in. Although the courses had  different group collaborative tasks (e.g., compiling an annotated  bibliography, writing a report on an innovative computing  technology), we combined the Wikis in these courses together on  the premise that all tasks involved collaborative academic writing.  It is our intention to explore indicators that can be applied to  various disciplines and tasks. Seventy-eight group Wikis were  obtained from these courses, as well as their final marks given by  the instructors.   To investigate student collaborative writings in different stages of  group projects, for each Wiki we collected its snapshots at the first  quarter of the project, second quarter and third quarter, in addition  to its latest content. The project period was established as starting  from the time of the first editing among Wikis of one course, and  ending at the time of the last editing. This segmentation was   Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on the  first page. Copyrights for third-party components of this work must  be honored. For all other uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  http://dx.doi.org/10.1145/2883851.2883963     intended to examine the changes in measures along the project  process. We first extracted the pages from the Wikis, then HTML  tags and redundant blanks were eliminated before a set of  measures of quality indicators were calculated. In the meantime,  the performance scores of the student groups given by the  instructors were taken as the scores of human evaluation. The  measurement output was then for further statistical analysis.   3.1 Quality indicators  By identifying and selecting linguistic features from multiple  sources, this study attempts to explore the potential of these  indicators in building a model for predicting human scores of  Wiki writing quality.   Metadiscourse features: Metadiscourse refers to the linguistic and  rhetorical manifestations in the text. Social constructionist  researchers have been using metadiscourse as an orientation to  analyzing discourse, while Wikis are regarded as an online  environment that embodies social-constructivist principles  through co-construction of knowledge [7]. In our view, this  denotes that metadiscourse features serve as important markers in  not only general academic text-styles but also in collaborative  writing (i.e., Wiki). Specifically, these features include attitude  markers, boosters, code glosses, endophoric markers, engagement  markers, evidentials, frame markers (goal-announcing,  sequencing, stage-labelling and topic-shifting) hedges, qualifiers,  person markers (self-mentioning) and transition markers [5]. Our  study treats these 14 linguistic features as the potential quality  indicators of academic writing in the Wiki context.   Blooms Taxonomy: Bloom's Taxonomy is a well-defined and  broadly accepted tool for categorizing types of thinking into six  different levels: knowledge, comprehension, application, analysis,  synthesis, and evaluation, where this Taxonomy has been widely  used in educational settings to measure students ability [4]. In  relation to Wikis, an objective of this collaborative writing  practice is to prompt students critical thinking. Knowledge  building using Wikis has also been shown to have connections  with students cognitive processes [1]. The present study therefore  treats the above six features as the possible quality indicators of  cognitive thinking embodied in Wiki writing.   3.2 Statistical Analysis  For each of the datasets, a multiple linear regression analysis was  conducted using the quality indicators as the independent  variables and the scores as the dependent variable, in an attempt to  build a model to predict scores of human evaluation.   4. RESULTS AND DISCUSSION  Table 1 below shows the quarter-based results of the linear  regression analysis. Results from the first quarter are not shown  due to the absence of significant values. Evaluation (in the  Blooms Taxonomy) was statistically highly significant (p < 0.01)  in both the third quarter and in the full-data context, meaning that  this feature possessed the explanatory power for the dependent  variable of human score. Evaluation words (e.g. compare,  describe, justify, etc.) would likely lead to a higher score, making  this feature a relatively significant predictor of the scores of Wikis.  Other moderately statistically significant (0.01 < p < 0.05)  features included Engagement markers and Self-mention  which were prominent in the second and third quarter,  corresponding to the earlier stage of students progress on the  Wikis. It is noteworthy that Self-mention had a negative  regression coefficient, meaning that this indicator contributed to  the scores negatively.    Table 1: Quarter-based regression analysis for selected indices  predicting Wiki scores   Note: * indicates significance at p < 0.10; ** at p < 0.05; *** at p < 0.01.   5. CONCLUSION  This study aimed to explore the potential of indicators of  academic writing and cognitive thinking in measuring the quality  of Wiki collaborative writing in different stages of group projects.  Results demonstrated that a few textual features, especially  towards the final stage of the Wiki project, appeared to have the  predictive power on the human scores. Future work is needed to  explore other quality indicators and their generalizability across  courses of different disciplines.   6. ACKNOWLEDGEMENTS  The work was partially supported by a Teaching Development  Grant from the University of Hong Kong.   REFERENCES  [1] Cress, U. and Kimmerle, J. 2008. A systemic and cognitive   view on collaborative knowledge building with wikis.  International Journal of CSCL. 3, 2, 105-122. DOI:  http://dx.doi.org/10.1007/s11412-007-9035-z   [2] Dalip, D. H., Gonalves, M. A., Cristo, M., and Calado, P.  2009. Automatic quality assessment of content created  collaboratively by web communities: a case study of  wikipedia. In Proceedings of the 9th JCDL. ACM, 295-304.  DOI:http://dx.doi.org/10.1145/1555400.1555449   [3] Dowell, N. M., Cade, W. L., Tausczik, Y., Pennebaker, J.,  and Graesser, A. C. 2014. What works: Creating adaptive  and intelligent systems for collaborative learning support.  In ITS (Jan 2014). Springer International Publishing. 124- 133. DOI: http://dx.doi.org/10.1007/978-3-319-07221-0_15    [4] Forehand, M. 2010. Blooms taxonomy. Emerging  perspectives on learning, teaching, and technology. 41-47.   [5] Hyland, K. 2005. Metadiscourse. John Wiley & Sons, Inc..  DOI:http://dx.doi.org/10.1002/9781118611463.wbielsi003   [6] Kim, J., Shaw, E., Xu, H., and Adarsh, G. V. 2012. Assisting  instructional assessment of undergraduate collaborative Wiki  and SVN activities. In Proceedings of the 5th EDM, 10-16.   [7] Su, F., and Beaumont, C. 2010. Evaluating the use of a wiki  for collaborative learning. Innovations in Education and  Teaching International. 47, 4, 417-431. DOI=  http://dx.doi.org/10.1080/14703297.2010.518428   2nd quarter 3rd quarter  Latest version   Features Coefficients (Std. Error)  Coefficients  (Std. Error)   Coefficients  (Std. Error)   Application -0.273 (15.616)  -0.244*  (11.707)   -0.140  (11.834)   Comprehension 0.042 (39.824)  0.173   (25.428)  0.191*   (30.580)  Engagement   Markers  0.469*   (13.113)  0.326**  (11.476)   0.205  (12.219)   Evaluation -0.265 (36.903)  0.367***  (31.282)   0.565***  (42.207)   Qualifiers 0.723** (23.020)  0.056   (17.523)  -0.100   (20.662)   Self-mention -0.441** (43.778)  -0.319**  (41.057)   -0.257  (42.540)   Topic-shift -0.587* (17.500)  -0.158   (14.292)  -0.154   (13.714)  Transition  Markers   0.498**  (16.643)   0.040  (15.332)   0.085  (15.832)   R2 0.561 0.507 0.525   http://dx.doi.org/10.1007/s11412-007-9035-z http://dx.doi.org/10.1145/1555400.1555449 http://dx.doi.org/10.1007/978-3-319-07221-0_15 http://dx.doi.org/10.1002/9781118611463.wbielsi003 http://dx.doi.org/10.1080/14703297.2010.518428   "}
{"index":{"_id":"76"}}
{"datatype":"inproceedings","key":"Ferguson:2016:LAC:2883851.2883878","author":"Ferguson, Rebecca and Clow, Doug","title":"Learning Analytics Community Exchange: Evidence Hub","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"520--521","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883878","doi":"10.1145/2883851.2883878","acmid":"2883878","publisher":"ACM","address":"New York, NY, USA","keywords":"ethics, evidence, learning, learning analytics, take-up, teaching","abstract":"This poster sets out the background and development of the LACE Evidence Hub, a site that gathers evidence about learning analytics in an accessible form. The poster also describes the functionality of the site, summarises its quantitative and thematic content to date, and assesses the state of evidence. In addition, it encourages people to add to and make use of the Hub.","pdf":"Learning Analytics Community Exchange: Evidence Hub     Rebecca Ferguson  The Open University  Milton Keynes, UK   Rebecca.ferguson@open.ac.uk   Doug Clow  The Open University  Milton Keynes, UK   Doug.clow@open.ac.uk    ABSTRACT  This poster sets out the background and development of the  LACE Evidence Hub, a site that gathers evidence about learning  analytics in an accessible form. The poster also describes the  functionality of the site, summarises its quantitative and thematic  content to date, and assesses the state of evidence. In addition, it  encourages people to add to and make use of the Hub.   Categories and Subject Descriptors  A.0 [General]: Reference   General Terms  Management, Documentation, Performance, Theory, Verification   Keywords  Ethics, evidence, learning, learning analytics, take-up, teaching   1. INTRODUCTION  Educational institutions worldwide are interested in using data- informed planning and decision making to improve their learning  and teaching. Learning analytics applications offer to provide  institutions with opportunities to support learner progression,  enabling rich personalized learning at scale. The use of large  datasets, powerful analytics engines and clear visualisations could  enable institutions to use the experience of the past to create  supportive, insightful models of learning processes. However,  due to the limited number of broad scale strategic and policy  examples available across the education sector, identifying the  precise competitive advantages that analytics can bring to the  education space is a multifaceted and complex undertaking [6].    Gathering evidence about the successes and failure of  learning analytics is not easy, particularly as developers and  school and workplace educators often find themselves blocked by  a pay-wall when they try to access relevant research. Even for  those who can examine these papers and reports, with analytics  and data mining experiments in education starting to proliferate,  sorting out fact from fiction and identifying research possibilities  and practical applications are not easy [1].   In 2011, the LAK Dataset was created to help address these  problems. As its homepage states, the dataset makes publicly  available machine-readable versions of research sources from the  Learning Analytics and EDM communities, where the main goal   is to facilitate research, analysis and smart explorative  applications. Since its creation, a series of Data Challenges has  inspired researchers and developers to explore it in many ways,  providing important insights into the field of learning analytics  research [5; 7; 8]. However, the LAK Dataset does not make it  easy to obtain a coherent and consistent overview of evidence and  the conditions under which learning analytics have a  positive/negative impact on teaching and learning.   To address this gap in the resources available to the learning  analytics community, the Learning Analytics Community  Exchange project (LACE) has developed an Evidence Hub.   The LACE project brings together key European players in  the fields of learning analytics and educational data mining who  are committed to building communities of practice and sharing  emerging best practice. One of the projects main activities is the  creation and curation of the Hub, a knowledge base of evidence  that will enable the community to assess the effectiveness and  relative desirability of outcomes resulting from the use of learning  analytics tools and techniques. The development of this Hub by  LACE draws on previous work carried out by one of the project  partners, the UKs Open University (OU).   2. PREVIOUS EVIDENCE HUB WORK  The OU has developed several tools and approaches designed to  link evidence together in a reliable and robust manner. For  example, the Evidence Hub for Open Education was developed  to provide an environment to systematically interrogate the Open  Education movement on what are the people, projects,  organizations, key challenges, issues, solutions, claims and  evidence that scaffold the movement [3]. This site was designed  to address the need for better ways to pool, map and harness what  a community knows. It was developed as a collaborative  knowledge-building (specifically evidence-building) web platform  that could highlight the importance of understanding different  perspectives and support quality debates [2].   Today, the Open Education Resources Research Hub (OERRH)  provides a focus for research, designed to provide answers to the  overall question What is the impact of OER on learning and  teaching practices and identify the influence of openness. Its  content currently includes more than 6,000 responses to 20  surveys exploring the impact of open educational resources [4].   Many claims have been made about OER and the OERRH is  therefore structured around a set of 11 hypotheses that cover  performance, access, retention, support and related areas.  Research and resources included within it are always related to  this specific set of claims that people have made about OER and  their potential, and evidence is always presented as evidence for  or against one or more of these hypotheses.     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author. Copyright is held by the authors  LAK 16, April 2529, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04  DOI: http://dx.doi.org/10.1145/2883851.2883878     3. LACE EVIDENCE HUB  The LACE project has used similar principles to design its  Evidence Hub. This is organised around four key propositions,  that learning analytics:    Improve learning outcomes   Improve learning support and teaching, including   retention, completion and progression   Are taken up and use widely, including deployment at   scale   Are used in an ethical way.   Evidence that is included within the hub is always given a polarity  (positive, negative or neutral/mixed) in relation to one or more of  these propositions. However, because the field of learning  analytics is relatively new, much of the work that will lead to  substantial evidence is still in its infancy. Therefore the LACE  hub also includes a Projects section, which gathers together  ongoing work in the area   Evidence in the Hub can be visualized and interrogated in several  ways. The Search facility enables visitors to search by keyword  and to focus on a specific country or sector (informal, schools,  universities, workplace or cross-sector). Once a search has been  run, Summaries of the evidence can be viewed. Each of these  contains a link to the original evidence, which is not stored in the  Hub. Summaries can be accessed in a variety of ways.   A Country Map shows the balance of negative and positive  evidence in different countries. The Evidence Map also starts  with a view of the world, but this time it can be searched by  proposition, polarity and/or sector, or title keywords. Finally, the  Evidence Flow diagram shows how much of the evidence relates  to each proposition, how much originates in each sector of  education, and how much is positive, negative and neutral/mixed.   4. USE OF THE LACE EVIDENCE HUB  This year, for the first time, the submission system for the LAK  conference has been aligned with the Evidence Hub, so that  evidence can be directly imported to the site and added to the  evidence that is already in place on the site.    As the Evidence Hub develops, a view of the state of  learning analytics emerges that can be used to guide future  activity and research. The site is currently being used as an  evidence source by LAEP  a European project that is exploring  the implications and opportunities of learning analytics for  European educational policy.    An overview of evidence currently in the Hub, using the  Evidence Flow diagram, revealed that most of the evidence  currently available is positive or, at worst, neutral or mixed. As a  result of this finding, the LACE project has set up a Failathon  workshop at LAK16 to explore the apparent lack of negative  evidence generated by the learning analytics community.   Visitors to the poster will be encouraged to contribute to the  Evidence Hub directly at http://evidence.laceproject.eu/   5. ACKNOWLEDGMENTS  The LACE Evidence Hub is powered by WordPress and based on  the OER Research Hub developed by Rob Farrow and Martin  Hawksey in the Institute of Educational Technology at The Open  University as part of a project funded by the Hewlett Foundation.  The European Commission Seventh Framework Programme funds  the LACE project: grant number 619424.   6. REFERENCES  [1] Bienkowski, M., Feng, M., and Means, B. 2012. Enhancing   teaching and learning through educational data mining and   learning analytics: An issue brief. US Department of  Education, Office of Educational Technology.   [2] De Liddo, A. and Buckingham Shum, S., 2013. The Evidence  Hub: harnessing the collective intelligence of communities to  build evidence-based knowledge. Paper presented at Large  Scale Ideation and Deliberation Workshop (Munich,  Germany, 29 June-2 July 2013).   [3] De Liddo, A., Buckingham Shum, S., Mcandrew, P., and  Farrow, R., 2012. The Open Education Evidence Hub: a  collective intelligence tool for evidence based policy. Paper  presented at Global 2012 Conference (Cambridge, UK, 16-18  April 2012).   [4] De Los Arcos, B., Farrow, R., Perryman, L.-A., Pitt, R., and  Weller, M. 2014. OER Evidence Report 2013-2014 OER  Research Hub.   [5] Derntl, M., Gnnemann, N., and Klamma, R., 2013. A  dynamic topic model of learning analytics research. In  proceedings of LAK Data Challenge: LAK13 (Leuven,  Belgium, 2013), CEUR. URL: http://ceur-ws.org/Vol- 974/lakdatachallenge2013_01.pdf.   [6] Siemens, G., Dawson, S., and Lynch, G. 2013. Improving the  quality of productivity of the higher education sector: Policy   and strategy for systems-level deployment of learning   analytics. SoLAR.  [7] Taibi, D. and Dietze, S., 2013. Fostering analytics on   learning analytics research: the LAK dataset. In proceedings  of LAK Data Challenge: LAK13 (Leuven, Belgium, 2013),  CEUR. URL: http://ceur-ws.org/Vol- 974/lakdatachallenge2013_preface.pdf.   [8] Zouaq, A., Joksimovic, S., and Gasevic, D., 2013. Ontology  learning to analyze research trends in learning analytics  publications. In proceedings of LAK Data Challenge: LAK13,  (Leuven, Belgium, 2013), CEUR. URL:   http://bit.ly/1kN2Y9W.           http://evidence.laceproject.eu/ http://ceur-ws.org/Vol-974/lakdatachallenge2013_01.pdf http://ceur-ws.org/Vol-974/lakdatachallenge2013_01.pdf http://ceur-ws.org/Vol-974/lakdatachallenge2013_preface.pdf http://ceur-ws.org/Vol-974/lakdatachallenge2013_preface.pdf   1. INTRODUCTION  2. PREVIOUS EVIDENCE HUB WORK  3. LACE EVIDENCE HUB  4. USE OF THE LACE EVIDENCE HUB  5. ACKNOWLEDGMENTS  6. REFERENCES   "}
{"index":{"_id":"77"}}
{"datatype":"inproceedings","key":"Spikol:2016:EIH:2883851.2883920","author":"Spikol, Daniel and Avramides, Katerina and Cukurova, Mutlu and Vogel, Bahtijar and Luckin, Rose and Ruffaldi, Emanuele and Mavrikis, Manolis","title":"Exploring the Interplay Between Human and Machine Annotated Multimodal Learning Analytics in Hands-on STEM Activities","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"522--523","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883920","doi":"10.1145/2883851.2883920","acmid":"2883920","publisher":"ACM","address":"New York, NY, USA","keywords":"CSCL, learning analytics, mobile","abstract":"This poster explores how to develop a working framework for STEM education that uses both human annotated and machine data across a purpose-built learning environment. Our dual approach is to develop a robust framework for analysis and investigate how to design a learning analytics system to support hands-on engineering design tasks. Data from the first user tests are presented along with the framework for discussion.","pdf":"Exploring the interplay between human and machine  annotated multimodal learning analytics in hands-on  STEM Activities  Daniel Spikol  Malm University  Malm, Sweden  daniel.spikol@mah.se  Katerina Avramides  UCL Institute of Education  London, UK  K.Avramides@ioe.ac.uk  Mutlu Cukurova  UCL Institute of Education  London, UK  m.cukurova@ioe.ac.uk  ABSTRACT This poster explores how to develop a working framework for STEM education that uses both human annotated and ma- chine data across a purpose-built learning environment. Our dual approach is to develop a robust framework for analysis and investigate how to design a learning analytics system to support hands-on engineering design tasks. Data from the first user tests are presented along with the framework for discussion.  Keywords Mobile, Learning Analytics, CSCL  1. INTRODUCTION There is concern amongst policy makers and employers  that students are not graduating with the required skills in STEM subjects (Science, Technology, Engineering, and Mathematics). It is argued that learners must go beyond the acquisition of discipline-specific facts and skills, to develop an integrated understanding of STEM disciplines within an authentic context of collaborative problem-solving. This in- tegrated approach to STEM teaching is at the heart of in- structional practices that centre on collaborative, hands-on, engineering design problems [4]. The use of hands-on engi- neering design problems, in classroom teaching, is facilitated by physical computing kits, such as the Arduino1, and other platforms. These kits provide building blocks that make technology development more accessible to novices, thus al- lowing them to work on more complex problems. A fine- grained analysis of the collaborative problem-solving pro- cess, using learning analytics tools, can provide insight into learning to support both teachers and learners. However, it is hard to track and interpret learner activity in engineer- ing design problems, due to the hands-on and open-ended nature of such tasks [3]. This poster presents work that be-  1www.arduino.cc  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK16 April, 25-29, 2016, Endinburgh, UK c 2016 Copyright held by the owner/author(s).  ACM ISBN 123-4567-24-567/08/06. . . $15.00 DOI: http://dx.doi.org/10.475/123 4  gins to address this challenge. We discuss the development of a coding process for exploring hands-on STEM activi- ties, using physical computing kits, that uses both hand- annotated and machine data across a purpose-built learning environment. The research aim is to investigate how we can support a combination of realtime hand-annotated data col- lected with machine collected data to help understand the complexities of collaborative problem solving.  2. BACKGROUND This work is part of the PELARS project2, which explores  teaching and learning of STEM using physical computing kits and open-ended engineering design tasks. The aim of PELARS is to develop learning analytics tools that support teachers and students. There is limited research on learn- ing analytics within this context, due to the diculty of tracking and interpreting student activity [3]. Researchers, in this area, have primarily focused on using learning an- alytics tools to gain a more fine-grained understanding of patterns of problem-solving between novices and experts, generally with a strong focus on individual rather than col- laborative learning [1][3]. However, collaborative learning has a central role in formal education. There is a wealth of research that suggests collaboration, compared with indi- vidual work, can foster better problem-solving and greater learning outcomes [2]. Collaboration supports learners to ar- ticulate their thinking and listen to others, as well as resolve conflicts and build on new ideas. Therefore, opportunities for new research that combine multimodal learning analytics with hands-on collaborative learning are present for further investigation.  3. TOOLS AND SETTINGS The tools described in the poster consist of a purpose-built  learning environment with multiple sensors to collect data during practice-based activities, with added web and mo- bile tools for learners to document their learning activities. The learning environment is a designed workshop that in- cludes a specially designed table connected to a freestanding wall with a built-in display. This special work area accom- modates a small group up to 4 students. The workstation includes a computer vision system with facial and object tracking (fiducial marks), log files from the programming of physical computing kits, as well as tracking of the dierent physical components that are being used by the students.  2www.pelars-project.eu  http://dx.doi.org/10.475/123_4   Table 1: Breakdown of data collection  Data Collection Types of Data Coding Computer Vision Arm Tracking, Audio Levels, Motion, Objets Machine Visual Electronics Platform Components connected and coding actions Machine Sentiment Buttons Frequency Aective states Machine Mobile System Research coding, learner documentation Machine and Human  Audio levels are captured (in the current release), and we have created two large buttons that students can push to signify sentiment (thunder cloud and sun). These buttons trigger the system to capture a screenshot of the program- ming activity and photo of the action on the table. Stu- dents need to plan, document, and reflect on their solution by entering brief text descriptions and capture photographs and video through the mobile system. More interestingly (for this poster) the mobile system provides a tool for re- search observations to be marked (in a temporal sense) on the fly, providing a bridge between the data that the sys- tem collects. Table 1 illustrates the dierent type of data collected, what sensors, and the split between machine and human data collection and coding. The annotation data cre- ated by the researchers and the learners serves as a coarse temporal breakdown of the activities. These web-based mo- bile tools (cross platform) also provide a key tool for re- searchers to provide course realtime encoding based on an analysis framework for collaborative work. It is our intention to explore how this rougher coding done by researchers and learners could supply more automatic segmentation between the dierent design activities, thus structuring the machine collected data.  Figure 1: Detail of initial visualisation based real-  time coding  4. DATA COLLECTION As of writing this poster, the project has conducted its  first full test of the system with two groups of three stu- dents with mixed backgrounds. The user trial procedure was to set up the test and introduce the students to the system. Each of the students wore fiducial tags for the vi- sion system on their dominant arm and were shown how to use the mobile reporting system. Then the students were guided with an hands-on introduction to the visual program- ming platform (IDE) that included working with sensors and actuator blocks and programming them. The task of proto- typing an interactive toy in 30 minutes was introduced and  the learning analytics system started. The research observer using the mobile device began the coding of the activity with marking events (design stages of project scoping, project re- alisation, and reflection) while the students used the tool to capture planning, documenting, and reflecting. Figure 1 vi- sualises the breakdown of planning, building, and reflecting across the dierent data that generated by the combination of research observations and the students input. Our initial work with the data has been to generate a rough visualisa- tion as a first attempt to reflect on the data. We present the sum of all learners hand movements and the gaze at the display. We also include the number of times students in- teracted with the electronics and the IDE. We generated a simple timeline diagram divided into 3 phases illustrating the duration of these high-frequency events in the dierent phases.  5. DISCUSSION The ability to use both the researcher codes of the dier-  ent design stages [1][3] and the students own classification of their activities suggests a rapid way to break down the data collected into appropriate parts for analysis and visualisa- tion. Of course, questions and work remain to test further this process of using multimodal data and encoded annota- tions in comparison to more traditional hand coding of the video analysis in the project.  6. ACKNOWLEDGMENTS The PELARS Project has received funding from the European  Commissions Seventh Framework Programme for research, tech-  nological development and demonstration under grant agreement  no. 619738.  7. ADDITIONAL AUTHORS Bahtijar Vogel (Malmo University bahtijar.vogel@mah.se), Rose  Luckin (UCL IOE, R.Luckin@ioe.ac.uk), Emanuele Rualdi (Scuola Superiore SantAnna, e.ruffaldi@sssup.it) and Mavrikis, Manolis (UCL IOE, jm.mavrikis@ucl.ac.uk).  References [1] C. J. Atman, R. S. Adams, M. E. Cardella, J. Turns, S. Mos-  borg, and J. Saleem. Engineering Design Processes: A Com-  parison of Students and Expert Practitioners. Journal of En-  gineering Education, 96(4):359379, Jan. 2013.  [2] B. Barron. Achieving coordination in collaborative problem-  solving groups. The Journal of the Learning Sciences,  9(4):403436, 2000.  [3] M. Worsley and P. Blikstein. Analyzing Engineering Design  through the Lens of Computation. Journal of Learning Ana-  lytics, 1(2):151186, Aug. 2014.  [4] D. L. Zeidler. Dancing with maggots and saints: Visions for  subject matter knowledge, pedagogical knowledge, and peda-  gogical content knowledge in science teacher education reform.  Journal of Science Teacher Education, 13(1):2742, 2002.    Introduction  Background  Tools and Settings  Data Collection  Discussion  Acknowledgments  Additional Authors   "}
{"index":{"_id":"78"}}
{"datatype":"inproceedings","key":"Koile:2016:UMA:2883851.2883922","author":"Koile, Kimberle and Rubin, Andee and Chapman, Steve and Kliman, Marlene and Ko, Lily","title":"Using Machine Analysis to Make Elementary Students' Mathematical Thinking Visible","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"524--525","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883922","doi":"10.1145/2883851.2883922","acmid":"2883922","publisher":"ACM","address":"New York, NY, USA","keywords":"elementary education, formative assessment, learning analytics, mathematics, pen-based computing, visual representations","abstract":"The INK-12: Teaching and Learning Using Interactive Ink Inscriptions in K-12 project has been developing and investigating the use of pen-based technology in elementary math classes. This paper reports on progress made on machine analysis of students' visual representations created using digital tools developed to support learning multiplication and division. The goal of the analysis is to make student thinking visible in order to (a) better understand how students learn multiplication and division, and (b) provide feedback to teachers, e.g., about strategies students use to solve problems. Student work from a five-week trial in a third grade class provides a corpus for development and evaluation of the machine analysis routines. Preliminary findings indicate that the routines can reproduce human analyses.","pdf":"Using Machine Analysis to Make Elementary Students  Mathematical Thinking Visible  Kimberle Koile  MIT ODL   77 Mass Ave, E34-370  Cambridge, MA 02139   Andee Rubin  TERC   2067 Mass Ave   Cambridge, MA 02140     Steve Chapman     StevCode   stevcode.com  Cambridge, MA 02139          Marlene Kliman   TERC    2067 Mass Ave  Cambridge, MA 02140   Lily Ko  TERC     2067 Mass Ave  Cambridge, MA 02140   +1-617-253-6037  kkoile@mit.edu    +1-617-873-9720  andee_rubin@terc.edu             stev.code@gmail.com    +1-617-873-9654          marlene_kliman@terc.edu   +1-617-873-9859                                      lily_ko@terc.edu     ABSTRACT  The INK-12: Teaching and Learning Using Interactive Ink  Inscriptions in K-12 project has been developing and investigating  the use of pen-based technology in elementary math classes.  This  paper reports on progress made on machine analysis of students  visual representations created using digital tools developed to  support learning multiplication and division.  The goal of the  analysis is to make student thinking visible in order to (a) better  understand how students learn multiplication and division, and (b)  provide feedback to teachers, e.g., about strategies students use to  solve problems. Student work from a five-week trial in a third  grade class provides a corpus for development and evaluation of  the machine analysis routines. Preliminary findings indicate that  the routines can reproduce human analyses.   Categories and Subject Descriptors  H.1.2 I.1.1 [Artificial Intelligence]: Knowledge Representation  Frameworks and Methods; K.3.1 [Computers and Education]:  Computer Uses in Education.   Keywords  Visual representations, pen-based computing, learning analytics,  elementary education, mathematics, formative assessment   1. INTRODUCTION  Students are often asked to show their thinking when presented  with a mathematical problem to solve.  In the elementary grades,  drawing visual representations is an important method for  expressing mathematical thinking [1, 7]. Such representations  give students the opportunity to be creative and to choose their  own strategies.  Indeed, the National Council of Teachers of  Mathematics (NCTM) recommends that students create and use  representations to organize, record and communicate  mathematical ideas and select, apply and translate among  mathematical representations to solve problems [5, p. 67].  Classroom conversations around representations provide valuable  opportunities for feedback to both teachers and students.  The INK-12: Teaching and Learning Using Interactive Ink  Inscriptions  in  K-12  project  (ink-12.mit.edu)  has  been   investigating upper elementary students uses of visual  representations in multiplication and division [3, 4].     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the owner/authors. Copyright is held by the owner/authors.  LAK16, April 25-29, 2016, Edinburgh, United Kingdom   Copyright 2016 ACM 978-1-4503-4190-5/16/04.  http://dx.doi.org/10.1145/2883851.2883922     Using our tablet-based software, called Classroom Learning  Partner (CLP), students use a tablet pen to create representations  in an electronic notebook and wirelessly submit their page of  work to the teacher.  The teacher can view student work in real  time, identifying students who may need help and choosing  pedagogically interesting examples for class discussion.  But how  does a teacher make these decisions, especially when presented  with submissions from each of 20 or more students   One of our  goals is to develop machine analysis routines that will provide a  teacher with information that will enable her to make these  decisions easily and quickly. The analysis routines are also aimed  at increasing our understanding of how students learn  multiplication and division, helping to find answers to such  questions as:  How do students use and modify representations  What do use patterns reveal about student thinking    2. ENABLING MACHINE ANALYSIS   CLPs machine analysis routines provide information about a  representation and the process by which the representation was  created.  As noted by NCTM, representation refers both to  process and to product [5, p. 67].  There are three key ideas that  enable machine analysis of students visual representations:   digital tools that enable students to draw, but that also produce  structured objects that a machine can recognize; storage of objects  and the history of a students interactions with the object; and a  coding scheme for creation and interaction with representations  (history codes) and for observations about the representations  (analysis codes). Shown in Figure 1 is a third grade students  number line representation for 48  8, the history code for  creating the representation, and an analysis code for  representation correctness, i.e., whether the representation  matches the math in the problem. The history code represents the  actions of adding a number line of length 48 to the page, then  drawing jumps of size 8 from 0 to 48 along the number line. The  analysis code is the result of comparing the number line jumps6  jumps of size 8 ending at 48against an answer definition that  specifies 6 groups of 8.     Figure 1. Number line, history and analysis codes   NL [48: 8, 0-48]        NL [48: COR]               3. HUMAN CODING  The coding scheme that forms the basis of our machine analysis  was developed over several months by the authors, two of whom  are math educators, using grounded theory. The student work used  in coding was the final assessment for a five-week multiplication  and division unit in a third grade class and consisted of 21  students work on 12 problems. The coding process involved  replaying interaction histories, identifying salient features, then  creating terms that captured the features. The resulting scheme  contains 20 history codes for five types of representations and 28  analysis codes. A subset of the scheme is shown in Table 1. The  scheme was used to code all the student work, with a subset of  work coded by all authors in order to ensure inter-rater reliability.   Table 1. Example history and analysis codes    Type Meaning Example   History  Create a number line and jump by # from # to # NL [24: 4, 0-24]   History  Create an array of dimension # x # ARR [6x7]   History  Draw a group of bins BINS [8]   Analysis Fill in answer before creating representation ABR   Analysis Representation correctness: correct, partially, incorrect  NL [24: COR]  ARR [4x6: INC]       History codes correspond only to actions deemed relevant to  mathematical thinking; e.g., there is no code for resizing a  representation.  Analysis codes correspond to observations about a  representations product or process, e.g., filling in an answer  before creating a representation, which may imply the representa- tion to check or explain work, rather than in problem solving.   4. MACHINE ANALYSIS    Our machine analysis routines aim to automate human encoding  of representations.  They take as input an interaction historya  sequence of low-level actions such as adding an ink stroke or an  object to a pageand make multiple passes over the history:   identifying characteristics of objects, clustering ink strokes via  machine learning techniques, using handwriting recognition and  heuristics to add semantics to ink clusters, and combining actions  into more abstract actions that resemble human history codes. The  result is a sequence of semantic events, similar to TD indicators in  [2] and semantic events in [6]. The sequence describes the process  of creating a representation and, as a last step, is analyzed in order  to recognize salient use patterns and apply relevant analysis codes.   The interaction history for the number line in Figure 1 contains 67  low-level actions, starting with a dot at 8 and a jump from 0-8,  then dot at 16, jump 8-16, dot at 24, jump 16-24, 10 ink strokes  that are identified as arithmetic, then dot at 32 and jump 24-32.  The arithmetic and dot-jump pattern continues until 40, indicating  that the student knows her 8 times table up to 24, but must use  arithmetic for larger numbers.  Her last jump from 40-48 indicates  that she knows the jumps must end on the dividend. Two jumps of  sizes 7 and 9 are erased and replaced with jumps of size 8,  indicating that she knows that the jumps must be the same size.   The entire interaction history for this example contains 341  actions, and playback and analysis reveal something extremely  interesting: The student created two different representations and  erased them prior to creating the number line.  In fact, she did not  use the number line to compute the answer; instead she drew a  series of circles representing bins and used a strategy called   dealing out, in which a mark is added to each bin until the number  of marks matches the total, in this case 48. She then wrote the  answer, and created a 6 by 8 array with skip counting along the  bottom, presumably to check her work.  Her bin and array  representations are shown in Figure 2. She then erased everything,  rewrote the equation without an answer, created the number line,  and filled in the answer again. A teacher does not have time to  replay students work, but machine analysis routines can give her  valuable information not evident in the final work, in this case  which strategies the student tried before settling on a number line.     Figure 2. Additional representations and human encoding   5. CURRENT WORK  We are continuing human analysis of our current corpus and  extending our machine analysis routines in order to recognize  additional patterns, especially those that involve ink strokes. In  Figure 2, for example, the current routines recognize skip  counting because of the alignment of ink strokes with array  columns.   Recognizing the equation or the bins is challenging,  but clustering of strokes and shape recognition routines hold  promise. Robust machine analysis routines will enable us to move  beyond time-consuming human analysis so that thousands of  pieces of work created during several five-week trials can be  analyzed, helping to further our knowledge about how students  learn and how student mathematical thinking can be made visible.   6. ACKNOWLEDGMENTS  Research funding is via NSF DRL-1020152, DRL-1019841.   7. REFERENCES  [1] Fosnot, C. T. and Dolk, M. 2001. Young Mathematicians at   Work: Constructing Multiplication and Division.  Portsmouth,  N.H.: Heinemann.   [2] Gutierrez-Santos, S., et al. 2012. Design of teacher assistance  tools in an exploratory learning environment for algebraic  generalization. IEEE TLT. 5, 4 (Oct-Dec 2012), 1939-2382.    [3] Koile, K. and Rubin, A. 2015. Tablet-based technology to  support students understanding of division.  In Proceedings  WIPTTE 2015.  Awaiting publication.   [4] Koile, K. and Rubin, A. 2015. Machine interpretation of  students hand-drawn mathematical representations. Impact of  Pen and Touch Technology on Education. Hammond, T., Val- entine, S., Adler, A., Payton, M. (Eds.). NY: Springer. 49-56.   [5] National Council of Teachers of Mathematics. 2000.   Principles and Standards for School Mathematics. Reston.   [6] Spannagel, C. and Kortenkamp. U. 2009. Demonstrating,  guiding, and analyzing processes in dynamic geometry  Systems. In Proceedings of 9th ICTMT.   [7]  Woleck, K. R. 2001. Listen to their pictures: an investigation  of childrens mathematical drawings. The Roles of Represen-  tation in School Mathematics. Cuoco, A. (Ed.), Reston:  NCTM, 215-227.  BINS [8]   BINS deal [8 DB 1: 6]   ANS FI [6: COR]   EQN [48  8 = 6]   ARR [8x6]   ARR skip [8x6: 8, 8-46]        "}
{"index":{"_id":"79"}}
{"datatype":"inproceedings","key":"Jayaprakash:2016:BSP:2883851.2883940","author":"Jayaprakash, Sandeep M. and Laur'ia, Eitel J. M. and Gandhi, Pritesh and Mendhe, Dinesh","title":"Benchmarking Student Performance and Engagement in an Early Alert Predictive System Using Interactive Radar Charts","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"526--527","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883940","doi":"10.1145/2883851.2883940","acmid":"2883940","publisher":"ACM","address":"New York, NY, USA","keywords":"benchmarking, data mining, information visualizations, instructional assessment, intervention, interventions, learning analytics, open source, predictive analytics, visualization","abstract":"This poster synthesizes the design features of a visualization layer applied on the Open Academic Analytics Initiative (OAAI), an open source academic early alert system based on predictive analytics. The poster explores ways to convey the predictive model outputs and benchmark student performances using visually intuitive radar plots.","pdf":"Benchmarking Student Performance and Engagement in  an Early Alert Predictive System using Interactive Radar   Charts  Sandeep M. Jayaprakash, Eitel J.M. Laura, Pritesh Gandhi, Dinesh Mendhe   Marist College, Poughkeepsie, NY, USA   {Sandeep.Jayaprakash1, Eitel.Lauria, Pritesh.Gandhi1, Dinesh.Mendhe1}@marist.edu  ABSTRACT  This poster synthesizes the design features of a visualization layer  applied on the Open Academic Analytics Initiative (OAAI), an  open source academic early alert system based on predictive  analytics. The poster explores ways to convey the predictive model  outputs and benchmark student performances using visually  intuitive radar plots.    Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning, Visualization, Dashboards   General Terms  Algorithms, Measurement, Information Visualizations, Design,  Experimentation, Interventions, Predictive analytics     Keywords  Learning Analytics, Open Source, Data Mining, Visualization,  Instructional Assessment, Benchmarking, Intervention   INTRODUCTION  The Open Academic Analytics Initiative (OAAI) [1] is a project  originally supported by the EDUCAUSE Next Generation  Learning Challenges (NGLC) program and funded primarily by the  Bill and Melinda Gates Foundation. OAAI uses the Sakai  Collaboration and Learning Environment, an existing open-source  Learning Management System (LMS), and the open-source  Pentaho Business Intelligence tool suite as its core technology  platforms.  This poster briefly describes the predictive model  building process implemented by the OAAI and delivers the model  outcomes using visualization techniques (radar plots) via a  dashboard to respective stakeholders.   PREDICTIVE MODELING  Four sources of data  were considered in the model building  process: student demographic data and course enrollment data,  extracted from the student information system (SIS); and learning  management system (LMS) data, extracted from Sakai, which  included logs of interactions of the students with the LMS as well  as partial contributions to the final grade  as reported by Sakais  gradebook (Sakai includes a gradebook tool that can be set up by   instructors to automatically record grades on graded activities such  as assignments, tests and forums). The data sources were combined  and cleaned to derive an enriched dataset which constituted the unit  of analysis (students taking courses) subsequently used for  predictive model training and testing.  The data set was made up of  the following attributes: FTPT AND ONLINE are indicators of  student full-time/ part-time status, and whether the course taken by  the student is online or on the ground. ACADEMIC _STANDING  identifies probation, regular, or honors status. ENROLLMENT is  the course size. CLASS identifies the year the student is in  (freshman, sophomore, junior and senior). GPA_CUMULATIVE  is the students cumulative GPA. LMS engagement metrics  (measured as ratios to the course average):  R_SESSIONS is the  number of Sakai sessions opened by a student in a course;  R_CONTENT_READ is the number of content resources viewed  by a student; R_FORUMS is the numbers of forum contributions  of the student; R_ASSIGMENTS is the number of assignments  submitted by the student. RMN_SCORE_PARTIAL is an  aggregated metric derived from partial contributions to the final  grade as recorded in the Gradebook tool. ACADEMIC_RISK is  used as the binary classification target attribute, with a cut-off grade  (below C for undergraduates, below B for graduate students) to  identify at-risk students.    As a result of the model training and testing process,  RMN_PARTIAL_SCORE and CUMULATIVE_GPA emerged as  the strongest predictors, followed by other LMS engagement  metrics (sessions, content read, forums and assignments). Adequate  depiction and visualization of these metrics can provide a holistic  portrait on the students performance and engagement patterns  which is the main thrust of this work.   DASHBOARDS AND RADAR CHARTS  Immediate access to data through learning dashboards offers new  opportunities to instructors and students by providing prompt  feedback [2]. This opportunity to act fast is not trivial especially  when you consider the high rates of withdrawal after a single course  failure in the first year of college. Learning dashboards in  combination with early alert systems offer students feedback in  timely fashion, alerting struggling students of their academic status.   Learning Activity Radar charts  As pointed by [3], unique visuals like pictorials, graphs,  grid/matrix, trees and networks and diagrams have significantly  high memorability scores compared to common charts like lines,  bars, circles, points. Radar graphs are efficient ways of  communicating a multivariate data in an intuitive way. The graphs  are laid in a circular fashion with multiple axes representing the  different metrics under analysis. The quantitative scales that run  along the axes (identical or different for each axis) are arranged to  begin at the center corresponding to 0 and then extended outside     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883940     with increasing values.  The farther away from the center the score  is in a criteria, the better demonstration of success in the criteria.     Performance Benchmarking   At the core of the Learning Activity Radar is a data-driven  predictive model determining the key performance predictors.  These predictive metrics are then mapped to the multivariate axes  on the Learning Activity Radar: (1) Cumulative GPA (2)  Gradebook scores (3) Session Activity (4) Content Read Activity  (5) Assignment Activity (6) Forums Activity. The Cumulative  GPA and Gradebook demonstrates the students performance  assessment criteria whereas the rest of the metrics form the  engagement assessment criteria.      Figure 1: Student Learning Activity Radar (a) No Risk   student profile (b) High Risk student profile.  The example presented in this work uses data from a graduate  online program and therefore sets a threshold of good standing at a  minimum cumulative GPA of 3.0 in a 0-4 scale. The predictors  from the LMS engagement and gradebook data are derived as ratios  comparing student performance with the class average fixed at a  score of 100 in a 0-200 scale.  This allows us to map the class  averages in each of the criteria and create a Class Benchmark  comparison polygon denoted in dark gray color in Figure 1.  When  an individual student score in each criteria is mapped against the  axis we get a student performance polygon which we can compare  against the class benchmark to identify areas of excellence and  problem areas. If the students polygon is on par or beyond the class  benchmark polygon in all criteria then they demonstrate a good  balance between performance and engagement. Color coding is  used to represent the risk confidence level as predicted by the model  (No, low, medium, and high risk). See Figure 1 (a) and (b).   The individual student performance benchmark view can be used  by both instructors and students to infer learning patterns. If a  student is highly engaged but the performance is weak or vice versa  or both, suitable instructional design strategies and intervention  supports could be made available to the student to change outcomes  and mitigate the problem areas.     Additionally, instructors have access to visualize the collective  patterns of all the students in a particular Risk group by using the  filters provided in the dashboard.  All the student polygons in the  risk group are color coded and layered on the class benchmark  polygon to easily identify common problem areas. Figure 2 shows  a collective pattern benchmark for a high risk group. It is evident  that the students are falling short on multiple categories: grades are  low among the group which is a result of lack of engagement in  forums and also some of them are falling short on assignments. The   instructor can act upon this insight to try to improve engagement  and content mastery.    Figure 2:  Example - Collective patterns and benchmarking  performance in a high risk group.   ONGOING WORK  We are currently working on enhancing the interactivity of the  visual layer by adding on-click histogram capabilities to the  performance predictors. This would further inform the instructors  on the class distribution and trends.    The Learning Activity Radar chart was built using D3 and the  Angular visualization library. An enhanced version of the chart is  already embedded into an open-source Dashboard project within  the Apereo Foundation. The visualization is available as a  configurable widget with support for Learning Tools  Interoperability (LTI) standards allowing the visual to be rendered  in variety of LMSes.   Currently, the dimensions mapped on the radar are on student  activity and performance as determined by an early alert predictive  model. Going forward we would like to turn this into a Learning  Competency Radar where there could be multiple learning  analytics applications feeding dimensions (radar chart axes) of  student skills and competencies. Examples include among others  student social engagement metrics determined by social network  analysis and writing skills metrics assessed by text mining  applications.    REFERENCES  [1] Jayaprakash S., Moody E., Laura E., Regan J., Baron J.,    Early Alert of Academically At-Risk Students: An Open  Source Analytics Initiative ,  Journal of Learning Analytics.   [2] Verbert, K., Duval, E., Klerkx, J., Govaerts, S., & Santos, J.  L. (2013). Learning Analytics: Dashboard Applications.  American Behavioral Scientist. 57(10), 1500-1509.    [3] B Michelle A. Borkin, Azalea A. Vo, Zoya Bylinskii, Phillip  Isola, Shashank Sunkavalli, Aude Oliva, Hanspeter Pfister,   What Makes a Visualization Memorable , IEEE  Transactions on Visualization & Computer Graphics, vol.19,  no. 12, pp. 2306-2315, Dec. 2013      REFERENCES   "}
{"index":{"_id":"80"}}
{"datatype":"inproceedings","key":"Dillon:2016:SAD:2883851.2883960","author":"Dillon, John and Ambrose, G. Alex and Wanigasekara, Nirandika and Chetlur, Malolan and Dey, Prasenjit and Sengupta, Bikram and D'Mello, Sidney K.","title":"Student Affect During Learning with a MOOC","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"528--529","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883960","doi":"10.1145/2883851.2883960","acmid":"2883960","publisher":"ACM","address":"New York, NY, USA","keywords":"affect, data collection, technology and learning","abstract":"This paper presents affect data collected from periodic emotion detection surveys throughout an introductory Statistics MOOC called I Heart Stats. This is the first MOOC, to our knowledge, to capture valuable student affect data through self-reported surveys. To collect student affect, we used two self-reporting methods: (1) The Self-Assessment Manikin and (2) A discrete emotion list. We found that the most common reported MOOC emotion was Hope followed by Enjoyment and Contentment. There were substantial shifts in affective states over the course, notably with Anxiety and Pride. The most valuable result of our study is a preliminary description of the methods for collecting self-reported student affect at scale in a MOOC setting.","pdf":"Student Affect during Learning with a MOOC  John Dillon   Univ. of Notre Dame  jdillon5@nd.edu   G. Alex Ambrose   Univ. of Notre Dame   gambrose@nd.edu   Nirandika Wanigasekara  IBM Research, India   nwaniga4@in.ibm.com  Malolan Chetlur   IBM Research, India   mchetlur@in.ibm.com   Prasenjit Dey   IBM Research, India   prasenjit.dey@in.ibm.com   Bikram Sengupta  IBM Research, India   bsengupt@in.ibm.com   Sidney K. DMello  Univ. of Notre Dame  sdmello@nd.edu  ABSTRACT  This paper presents affect data collected from periodic emotion  detection surveys throughout an introductory Statistics MOOC  called I Heart Stats. This is the first MOOC, to our knowledge,  to capture valuable student affect data through self-reported  surveys. To collect student affect, we used two self-reporting  methods: (1) The Self-Assessment Manikin and (2) A discrete  emotion list. We found that the most common reported MOOC  emotion was Hope followed by Enjoyment and Contentment.  There were substantial shifts in affective states over the course,  notably with Anxiety and Pride. The most valuable result of our  study is a preliminary description of the methods for collecting  self-reported student affect at scale in a MOOC setting.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Distance learning    Keywords  Affect; Data Collection; Technology and Learning.   1. INTRODUCTION   Despite the burst of research focused on educational data  generated by MOOCs, little work has been done on student affect  in MOOCs. This is surprising given that previous research has  demonstrated that student affect is a critical part of the learning  process [1,7]. Although a few previous papers have considered  emotions in a MOOC context [5,8], this paper reports the first  broad study of self-reported student affect in a MOOC context.  This study is based on the University of Notre Dames  introductory statistics MOOC called I Heart Stats. Previous and  extensive research has focused on anxiety during learning  Statistics [6]. One of the learning objectives of this course was to  alleviate student anxiety towards Statistics, and in this regard, it  was an ideal opportunity to investigate affect.   2. RESEARCH QUESTIONS  Unlike previous research, which has inferred student affect based  on discussion fora [8] or click behavior [5], in this study, we  directly and periodically ask students to self-report affect.  Furthermore, we track a substantially greater number of affective  states than previous studies. We investigate three research  questions: Q1. What affective states do students experience in a  MOOC setting Q2. How do the reported affective states co- occur Q3. How do these affective states change over the  duration/modules of the course   3. SETUP AND DATA COLLECTION  There were 8 modules (or weeks) of content for I Heart Stats  and all of the content was released simultaneously. Total  enrollment was 24,279 from 183 different countries. The content  included short videos as well as multiple-choice assessments,  similar to many MOOCs. To measure student emotion in the  course we used two self-report assessments. The first (see Figure  1) was the Self-Assessment Manikins (SAM) [3] measure of  valence (i.e. negative to positive). Since English was not the first  language of a large portion of the enrolled students, SAM offered  a non-linguistic measure for valence. A second assessment (see  Figure 2) presented students with a list of 15 discrete emotions  and asked students to select two emotions that best described how  they were feeling. These two questions were asked at the start of  Weeks 0, 2, 4, and 6, as well as at the end of Week 8.       Figure 1: Self-Assessment Manikin (SAM).     Figure 2: Emotion List.     Figure 3: Distribution of Emotions.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883960   http://dx.doi.org/10.1145/2883851.2883960   4. PRELIMINARY RESULTS   Q1. What affective states do students experience in a MOOC  setting  Approximately 3,968 students completed the discrete emotion  response survey at least once. The most frequently reported  affective states (see Figure 3) were Hope, Enjoyment, and  Contentment.    Q2. How do the reported affective states co-occur   Table 1 lists the number of distinct emotion combinations for  each survey. Asking students to report two emotions affords an  analysis of co-occurring states [2]. For instance, the top five  emotion pairs from Week 0 (see Table 2) show that frequently  reported affective states such as Hope are often paired with  several different affective states. Further study, however, is  needed to determine the Lift scores of these co-occurring  emotions [2].    Table 1: Count of Emotion Combinations   Table 2: Top 5 Co-Occurring Emotion Pairs in Week 0        Q3. How do these affective states change over the  duration/modules of the course  Figure 4 presents the distribution of individual affective states  over the duration of the course. Note that Hope, Enjoyment, and  Contentment remained dominant positive affective states. Pride  increased as the course progressed, while Hope and Anxiety  decreased.      Figure 4: Distribution of Emotions Over Course.      5. CONCLUSION/FUTURE WORK   We collected affect data in a MOOC setting. We found that  Hope, Contentment, and Enjoyment are frequent and persistent  MOOC emotions. Previous research on affect and learning with   technology has largely overlooked some of these emotions,  especially Hope [4]. We have also found that the frequency of  certain emotions such as Anxiety, Relief, and Pride can change  substantially over a course. We acknowledge that our results may  be biased by the high rate of MOOC attrition. Once a student  drops out, we can no longer collect his or her self-reported  emotion. Further research to control for dropout bias in self- reporting affect surveys is needed.    We also plan to study the co-occurring affective states and  discrete states in conjunction with course log data. One practical  application of sensor-free affect detection in a MOOC context is  affect prediction and affect-based intervention. Detecting if a  student is confused or frustrated would allow designers,  instructors, and learning scientists to orchestrate personalized and  timely interventions at scale. An additional practical application  of this research is a better understanding of how students feel  about different types and samples of learning content. These  items will be pursued in future work in the context of I Heart  Stats and in other MOOCs as well.    6. ACKNOWLEDGEMENTS   We would like to thank Crystal DeJaegher and Xaiojing Duan for  their contributions in the design and data collection of this  project.    7. REFERENCES   [1] Baker, R., and Ocumpaugh, J. 2014. Interaction-Based Affect   Detection in Educational Software. The Oxford Handbook of  Affective Computing (2014), 233-246.    [2] Bosch, N., and DMello, S. 2014. Co-occurring Affective  States in Automated Computer Programming Education. In  Proceedings of the Workshop on AI-supported Education for  Computer Science at the 12th International Conference on  Intelligent Tutoring Systems (AIEDws 2014).    [3] Bradley, M., and Lang, P. 1994. Measuring emotion: the self-  assessment manikin and the semantic differential. Journal of  behavior therapy and experimental psychiatry 25, 1 (1994),  49- 59.    [4] DMello, S. (2013). A selective meta-analysis on the relative  incidence of discrete affective states during learning with  technology. Journal of Educational Psychology, 105(4),  1082.    [5] Leony, D., Muoz-Merino, P., Ruiprez-Valiente, J., Pardo,  A., and Kloos, C. 2015. Detection and Evaluation of  Emotions in Massive Open Online Courses. Journal of  Universal Computer Science 21, 5 (2015), 638655.    [6] Onwuegbuzie, A. J., Da Ros, D., & Ryan, J. M. (1997). The  Components of Statistics Anxiety: A Phenomenological  Study. Focus on Learning Problems in Mathematics, 19(4),  11-35.   [7] Pekrun, R., Goetz, T., Titz, W., and Perry, P. 2002. Academic  emodtions in students self-regulated learning and  achievement: A program of qualitative and quantitative  research. Educational psychologist 37, 2 (2002), 91105.    [8] Yang, D., Wen, M., Howley, I., Kraut, R., and Rose, C. 2015.  Exploring the effect of confusion in discussion forums of  massive open online courses. In Proceedings of the Second  (2015) ACM Conference on Learning@ Scale. ACM, 121 130.  and Contentment. Alternatively, Isolation, which we imagined  appropriate to an online learning setting, was not a frequent  reported state.    Q.2 How do the reported affective states co-occur  Table 4 lists the number of distinct emotion combinations for  each survey. The top five emotion pairs from each week (see  Table 5) show that frequently reported affective states such as  Hope are often paired with several different affective states. This  suggests the rethinking of single affective states as part of a more  complex emotional and multifaceted state [2].      Q3. How do these affective states change over the duration of  the course  Figure 6 presents the distribution of individual affective states  over the duration of the course. Note that Hope, Enjoyment, and  Contentment remained dominant positive affective states. Pride  increased as the course progressed, while Hope and Anxiety  decreased.      5. CONCLUSION/FUTURE WORK  We have developed a method for gathering affect data in a  MOOC setting. We have found that Hope, Contentment, and  Enjoyment are frequent and persistent MOOC emotions.  Previous research on affect and learning technology has largely  overlooked some of these emotions, especially Hope [4]. We  have also found that the frequency of certain emotions such as  Anxiety, Relief, and Pride can change substantially over a course.  We have identified the most common co-occurring emotion  pairs, which builds on previous research in a new context [2]. We  plan to study the co-occuring affective states and trajectories in  conjunction with course log data to draw insights and customize  MOOCs via affect prediction and affect-based intervention.      6. ACKNOWLEDGEMENTS  We would like to thank Crystal DeJaegher and Xaiojing Duan  for contributions in the design and data collection of this project.     7. REFERENCES  [1] Baker, R., and Ocumpaugh, J. 2014. Interaction-Based Affect  Detection in Educational Software. The Oxford Handbook of  Affective Computing (2014), 233-246.  [2] Bosch, N., and DMello, S. 2014. Co-occurring Affective  States in Automated Computer Programming Education. In  Proceedings of the Workshop on AI-supported Education for  Computer Science at the 12th International Conference on  Intelligent Tutoring Systems (AIEDws 2014).  [3] Bradley, M., and Lang, P. 1994. Measuring emotion: the self- assessment manikin and the semantic differential. Journal of  behavior therapy and experimental psychiatry 25, 1 (1994), 49- 59.   [4] DMello, S. (2013). A selective meta-analysis on the relative  incidence of discrete affective states during learning with  technology. Journal of Educational Psychology, 105(4), 1082.  [5] Leony, D., Muoz-Merino, P., Ruiprez-Valiente, J., Pardo,  A., and Kloos, C. 2015. Detection and Evaluation of Emotions in  Massive Open Online Courses. Journal of Universal Computer  Science 21, 5 (2015), 638655.   [6] Pekrun, R., Goetz, T., Titz, W., and Perry, P. 2002. Academic  emotions in students self-regulated learning and achievement: A  program of qualitative and quantitative research. Educational  psychologist 37, 2 (2002), 91105.   [7] Yang, D., Wen, M., Howley, I., Kraut, R., and Rose, C. 2015.  Exploring the effect of confusion in discussion forums of  massive open online courses. In Proceedings of the Second  (2015) ACM Conference on Learning@ Scale. ACM, 121130.    0  0.05  0.1   0.15  0.2   0.25  0.3   0.35  0.4   Week 0 Week 2 Week 4 Week 6 Week 8   Table 4. Count of Emotion Combinations  Figure 6. Distribution of Emotions Over Course   Table 5. Top 5 Co-Occurring Emotion Pairs Per Week   and Contentment. Alternatively, Isolation, which we imagined  appropriate to an online learning setting, was not a frequent  reported state.    Q.2 How do the reported affective states co-occur  Table 4 lists the number of distinct emotion combinations for  each survey. The top five emotion pairs from each week (see  Table 5) show that frequently reported affective states such as  Hope are often paired with several different affective states. This  suggests the rethinking of single affective states as part of a more  complex emotional and multifaceted state [2].      Q3. How do these affective states change over the duration of  the course  Figure 6 presents the distribution of individual affective states  over the duration of the course. Note that Hope, Enjoyment, and  Contentment remained dominant positive affective states. Pride  increased as the course progressed, while Hope and Anxiety  decreased.      5. CONCLUSION/FUTURE WORK  We have developed a method for gathering affect data in a  MOOC setting. We have found that Hope, Contentment, and  Enjoyment are frequent and persistent MOOC emotions.  Previous research on affect and learning technology has largely  overlooked some of these emotions, especially Hope [4]. We  have also found that the frequency of certain emotions such as  Anxiety, Relief, and Pride can change substantially over a course.  We have identified the most common co-occurring emotion  pairs, which builds on previous research in a new context [2]. We  plan to study the co-occuring affective states and trajectories in  conjunction with course log data to draw insights and customize  MOOCs via affect prediction and affect-based intervention.      6. ACKNOWLEDGEMENTS  We would like to thank Crystal DeJaegher and Xaiojing Duan  for contributions in the design and data collection of this project.     7. REFERENCES  [1] Baker, R., and Ocumpaugh, J. 2014. Interaction-Based Affect  Detection in Educational Software. The Oxford Handbook of  Affective Computing (2014), 233-246.  [2] Bosch, N., and DMello, S. 2014. Co-occurring Affective  States in Automated Computer Programming Education. In  Proceedings of the Workshop on AI-supported Education for  Computer Science at the 12th International Conference on  Intelligent Tutoring Systems (AIEDws 2014).  [3] Bradley, M., and Lang, P. 1994. Measuring emotion: the self- assessment manikin and the semantic differential. Journal of  behavior therapy and experimental psychiatry 25, 1 (1994), 49- 59.   [4] DMello, S. (2013). A selective meta-analysis on the relative  incidence of discrete affective states during learning with  technology. Journal of Educational Psychology, 105(4), 1082.  [5] Leony, D., Muoz-Merino, P., Ruiprez-Valiente, J., Pardo,  A., and Kloos, C. 2015. Detection and Evaluation of Emotions in  Massive Open Online Courses. Journal of Universal Computer  Science 21, 5 (2015), 638655.   [6] Pekrun, R., Goetz, T., Titz, W., and Perry, P. 2002. Academic  emotions in students self-regulated learning and achievement: A  program of qualitative and quantitative research. Educational  psychologist 37, 2 (2002), 91105.   [7] Yang, D., Wen, M., Howley, I., Kraut, R., and Rose, C. 2015.  Exploring the effect of confusion in discussion forums of  massive open online courses. In Proceedings of the Second  (2015) ACM Conference on Learning@ Scale. ACM, 121130.    0  0.05  0.1   0.15  0.2   0.25  0.3   0.35  0.4   Week 0 Week 2 Week 4 Week 6 Week 8   Table 4. Count of Emotion Combinations  Figure 6. Distribution of Emotions Over Course   Table 5. Top 5 Co-Occurring Emotion Pairs Per Week     "}
{"index":{"_id":"81"}}
{"datatype":"inproceedings","key":"Hagood:2016:IPA:2883851.2883958","author":"Hagood, Danielle and Ching, Cynthia Carter and Schaefer, Sara","title":"Integrating Physical Activity Data in Videogames with User-centered Dashboards","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"530--531","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883958","doi":"10.1145/2883851.2883958","acmid":"2883958","publisher":"ACM","address":"New York, NY, USA","keywords":"activity monitors (Fitbit), dashboards, health, quantified self, sociocultural theory","abstract":"To promote healthy awareness and activity learning, we gave 12-to 14-year-old youth activity monitors (Fitbits) to track their physical activity, which was then integrated into a videogame we created. The players' real-world steps transform into in-game resources needed for gameplay. In addition to requiring real-world steps for various in-game activities, a dashboard in this game presents visual representations of activity patterns, ostensibly informing students about patterns of their own activity. In this paper and poster, we discuss challenges in initial designs of our dashboard. We present findings and challenges in the process of creating a user-centered dashboard and conclude with our future design goals.","pdf":"Integrating Physical Activity Data in Videogames   with User-Centered Dashboards     Danielle Hagood   University of California, Davis  dhagood@ucdavis.edu   Cynthia Carter Ching  University of California, Davis  ccching@ucdavis.edu   Sara Schaefer  University of California, Davis  seschaefer@ucdavis.edu        ABSTRACT  To promote healthy awareness and activity learning, we gave 12-  to 14-year-old youth activity monitors (Fitbits) to track their  physical activity, which was then integrated into a videogame we  created. The players real-world steps transform into in-game  resources needed for gameplay. In addition to requiring real-world  steps for various in-game activities, a dashboard in this game  presents visual representations of activity patterns, ostensibly  informing students about patterns of their own activity. In this paper  and poster, we discuss challenges in initial designs of our  dashboard. We present findings and challenges in the process of  creating a user-centered dashboard and conclude with our future  design goals.   Categories and Subject Descriptors  H.5.2 [User Interfaces]: User Interfaces  User-centered design;  K.3.1 [Computer and Education]: Computer Uses in Education   Computer-assisted instruction (CAI); K.8.0 [Personal  Computing]: General  Games   General Terms  Design, Human Factors   Keywords  Quantified Self, Dashboards, Activity Monitors (Fitbit), Health,  Sociocultural Theory   1. INTRODUCTION  The far-reaching consequences of youth obesity and inactivity  compel contemplation of learning analytics role in supporting  healthier outcomes for youth. Since 1980 the adolescent obesity  rate has tripled, growing from 5% to 18% [1]. Youth need access  to information and resources to live healthier lives. The emerging  Quantified Self (QS) movement offers one approach to tracking  health data and exploring personal analytics [2].  A core assumption of quantified self approaches is that information  alone is inherently valuable and motivating. But, youth often fail to  find their data fascinating or useful [3]. We suggest that, by  leveraging the documented success of videogames to motivate  change, we can more successfully present health analytics [4];  incorporating dry activity data into a motivating game context to  promote behavior and awareness changes.   2. PROJECT CONTEXT  We combine our expertise of in learning analytics, health science,  and game development, to explore how game play which integrates  players own physical activity data can promote healthy awareness  and behavior.   This study spans multiple iterations of data collection with different  groups. For this paper we focus our discussion in the second year  of the project. By employing a working example methodology [5]  we discuss how our findings and design processincluding our  initial struggles to incorporate analytics meaningfully.   2.1 Study Overview  We provided youth at two school sites activity monitorsthe  commercially available Fitbit Zips. The participants wore these  activity monitors for 30 days and then played the game Terra while  continuing to use the activity monitors for another 63 days. Terra  is a tile-turning style game following the narrative of planetary  exploration and terraforming created by our research team and a  gamedesign firm. Throughout this time, we collected data from  focus groups, individual interviews, observations, activity  monitors, game servers, and publically-available site statistics.   2.2 Participants and Their Communities  Two school sites provide the second year case studies in this  project. Participants from Capitol Middle School included mainly  lower socioeconomic (SES) youth participating in an after school  program in a large urban area of Northern California (N = 40, Mage  = 12.6). Participants from Greenville Junior High included youth  enrolled an educational technology elective course from a well- resourced suburban community (N = 61, Mage = 13.7). Table 1  compares both school sites to each other and the overall state  demography.   Table 1: Demography of Capitol Middle School and  Greenville Junior High (2015)    Capitol California Overall Greenville   English Learner 34.5% 27.7% 7.4%  Receiving Free  Lunch 92.7% 59.4% 28.2%   Minority 96.5% 74.4% 50.2%  Household  IncomeMedian $53,394 $61,094 $60,114      3. DASHBOARDS FOR ACTIVITY DATA  In addition to leveraging game mechanics to promote behavior and  attitude changes relating to activity, our team developed a  dashboard that displays the activity history of the player outside of   Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on the  first page. Copyrights for third-party components of this work must  be honored. For all other uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  http://dx.doi.org/10.1145/2883851.2883958.       the game. These metrics track the students activity, which provides  in game bonuses and energy points. Here we discuss what we have  learned so far and present our current approach to creating a truly  user-centered dashboard.   3.1 User Experience: Im Confused  A main finding regarding the in-game dashboard  see one screen  shown in Figure 1  is that players seemed confused or  uninterested in our data representations. The initial dashboard  design used simplistic data representations like line graphs.  However, we found that students often needed help to decipher the  graphs, or did not make connections between what they saw on the  graphs and their everyday activities. In contrast when activity data  was transmuted and displayed as energy points for gameplay, the  students made sense of their activity data in the context of the game  narrative and considered their overall activity patterns.   While participants were not motivated to spend much time on their  online dashboard, some demonstrated deep reflections using the  live data on their fitbits displays [6]. For example, in a focus group  one student commented: [The Fitbit] it told me, how much weight,  how much calories I burned off every time I take a steps, and, um,  the times that I do for my activities.   We believe the school-like format and dry presentation of the  data itself was the problem with our initial dashboard, rather than  an unwillingness to engage with aggregate data or use inductive  reasoning about patterns.   3.2 User Context: I cant change anything  Further, analyzing the sociocultural contexts of these two sites [7]  reveals another design challenge in creating a successful  dashboard. The different communities with high and low SES offer  disparate affordances for activity in the community. We observed  that even when Capitol students in the low-income community  were engaging in their data, they reported barriers to changing their  activity. While Greenville students with access to parks, safe bike  paths, and sports clubs reported more access opportunities to  exercise based on learning from their data.   One goal of learning analytics is to promote individual learning  through reflection with personal data. But, our initial one-size  dashboard did not accommodate different activity opportunities in  different communities. Drawing on interview and focus group data  with students we reexamine providing community appropriate  feedback. Employing a standard framework for data interaction  unintentionally foregrounds the designers standard community  rather than the learners reality.         Figure 1: A Screen in the Terra Dashboard.   3.3 Design Goals and Future Work  First, to address confusion in user experience, we are now  developing a player dashboard more integrated in the game world  and game narrative. Our goal is to more richly frame the players  personal activity data in terms of game world goals and strategies.  This design allows players to set daily, weekly, or cumulative  activity goals on their game dashboard such as step count, total  mileage, improvement over baseline, intensity of activity, calories  burned, and then have those goals directly tied to progress toward  game world achievements. Drawing on the data representation  approaches already in use like the fitbit.com, integrating the youth- produced data into the game narrative makes these activity  analytics more accessible to youth.   Second, to incorporate user context, we continue to study how  young people examine and query their aggregate physical activity  data in ways that responds to unique community situations and  resources. We explore how youth engage with their dashboards and  how this engagement is connected to their physical activity. By  considering activity analytics through a sociocultural lens we can  design dashboards that address the plurality of health needs and  physical activity.   4. ACKNOWLEDGMENTS  Thanks to Mary Stewart for her instrumental collaborative writing  with me on these topics in other texts.   REFERENCES  [1] Ogden, C. L., Carroll, M. D., Kit. B. K., and Flegal, K. M.   2012. Prevalence of obesity and trends in body mass index  among US children and adolescents, 1999-2010. JAMA, 307,  5, 483-490. doi:10.1001/jama.2012.40.   [2] Swan, M. 2012. Sensor Mania! The Internet of things,  wearable computing, objective metrics, and the Quantified  Self 2.0. Journal of Sensor and Actuator Networks, 1, 3, 217- 253   [3] Lee, V.R. 2013. The Quantified Self (QS) movement and  some emerging opportunities for the educational technology  field. Educational Technology, 53, 6, 39-42.   [4] Baranowski, T., Buday, R., Thompson, D. I., and  Baranowski, J. (2008). Playing for real: video games and  stories for health-related behavior change. American journal  of preventive medicine, 34, 74-82.   [5] Barab, S., Dodge, T., & Gee, J. P. 2009. The worked  example: Invitational scholarship in service of an emerging  field. In Annual Meeting Online Program of the American  Education Research Association (San Diego, California,  April 13-16, 2009).   [6] Ching, C. C. & Schaefer, S. 2014.  Devices, dashboards,  games, and reflections: Quantitative data and the subjective  experience of fitness technologies.  In Learning and  Becoming in Practice: The International Conference of the  Learning Sciences (ICLS) 2014 (Boulder, Colorado, June 23- 27, 2014).   [7] Hagood, D., Stewart, M, Ching, C. 2015 A tale of two  schools: Terrain and resources in virtual games and physical  communities. In Proceedings of the 11th Games + Learning  + Society conference (Madison, Wisconsin, July 7  10, 20        "}
{"index":{"_id":"82"}}
{"datatype":"inproceedings","key":"Schwendimann:2016:ULG:2883851.2883930","author":"Schwendimann, Beat A. and Rodr'iguez-Triana, Mar'ia Jes'us and Vozniuk, Andrii and Prieto, Luis P. and Boroujeni, Mina Shirvani and Holzer, Adrian and Gillet, Denis and Dillenbourg, Pierre","title":"Understanding Learning at a Glance: An Overview of Learning Dashboard Studies","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"532--533","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883930","doi":"10.1145/2883851.2883930","acmid":"2883930","publisher":"ACM","address":"New York, NY, USA","keywords":"dashboards, educational data mining, information visualization, learning analytics, systematic review","abstract":"Research on learning dashboards aims to identify what data is meaningful to different stakeholders in education, and how data can be presented to support sense-making processes. This paper summarizes the main outcomes of a systematic literature review on learning dashboards, in the fields of Learning Analytics and Educational Data Mining. The query was run in five main academic databases and enriched with papers coming from GScholar, resulting in 346 papers out of which 55 were included in the final analysis. Our review distinguishes different kinds of research studies as well as different aspects of learning dashboards and their maturity in terms of evaluation. As the research field is still relatively young, many of the studies are exploratory and proof-of-concept. Among the main open issues and future lines of work in the area of learning dashboards, we identify the need for longitudinal research in authentic settings, as well as studies that systematically compare different dashboard design options.","pdf":"Understanding learning at a glance: An overview of  learning dashboard studies  Beat A. Schwendimann 1, Mara Jesus Rodrguez-Triana2, Andrii Vozniuk2, Luis P. Prieto1, Mina Shirvani Boroujeni1, Adrian Holzer2, Denis Gillet2 and Pierre Dillenbourg1  1CHILI Group, EPFL, Station 20, 1015 Lausanne, Switzerland. Email: {beat.schwendimann, luis.prieto, mina.shirvaniboroujeni, pierre.dillenbourg}@epfl.ch  2REACT Group, EPFL, Station 9, 1015 Lausanne, Switzerland. Email: {maria.rodrigueztriana, andrii.vozniuk, adrian.holzer, denis.gillet}@epfl.ch  ABSTRACT Research on learning dashboards aims to identify what data is meaningful to dierent stakeholders in education, and how data can be presented to support sense-making processes. This paper summarizes the main outcomes of a system- atic literature review on learning dashboards, in the fields of Learning Analytics and Educational Data Mining. The query was run in five main academic databases and enriched with papers coming from GScholar, resulting in 346 papers out of which 55 were included in the final analysis. Our re- view distinguishes dierent kinds of research studies as well as dierent aspects of learning dashboards and their ma- turity in terms of evaluation. As the research field is still relatively young, many of the studies are exploratory and proof-of-concept. Among the main open issues and future lines of work in the area of learning dashboards, we identify the need for longitudinal research in authentic settings, as well as studies that systematically compare dierent dash- board design options.  Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Education; H.5.2 [Information interfaces and presen- tation]: User interfaces  Keywords learning analytics, educational data mining, information vi- sualization, dashboards, systematic review  1. INTRODUCTION Visual displays are critical to sense-making as humans can  process large amounts of data if presented in meaningful ways. A current major challenge in the field of education is  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 16, April 25 - 29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883930  how data coming from learning platforms can be made ac- tionable by analyzing and presenting it in ways meaningful to dierent stakeholders [4]. Learning dashboards build on research in information visualization, learning analytics and educational data mining. Although these fields are still rela- tively young, their explosive growth already provides enough literature to justify a systematic review. Indeed, several re- views of learning dashboards already exist, although they only focus on small case studies and contrasting selected ex- amples (see, for example [5]). Thus, this paper provides a systematic literature review of research on learning dash- boards. The research questions addressed by this study are: - RQ1 : In which contexts are learning dashboards applied - RQ2 : What are the characteristics of learning dashboards developed so far, including their purpose, indicators pre- sented and technologies used - RQ3 : How mature are such eorts on learning dashboards, in terms of their evaluation  2. METHODOLOGY When conducting the review (following the guidelines pro-  posed by Kitchenham and Charters [2]), we selected five main academic databases relevant for Technology Enhanced Learning: ACM Digital Library, IEEE Xplore, Springer- Link, Science Direct, and Wiley. Google Scholar was also used in order to detect potentially relevant grey literature (technical reports and other publications outside of classic academic publishing channels). The search string used was: dashboard AND (learning analytics OR educational data mining OR educational datamining). Hence, the review focuses specifically on dashboards (a visual display of the most important information needed to achieve one or more objectives; consolidated and arranged on a single screen so the information can be monitored at a glance [1]), rather than visualizations in general. The literature search was conducted on 21st August, 2015.  A total of 246 papers were obtained from the afore- mentioned databases; additionally, the top 100 results from GScholar (from a total of 989) were added to the analysis. Each paper was reviewed by two researchers to determine if it was out of scope, of low credibil- ity, or of low quality. After this filtering, 55 pa- pers were analyzed in detail (see list of papers here https://dx.doi.org/10.6084/m9.figshare.2066793.v1).    3. RESULTS  3.1 Types of contributions While many of papers (39 papers; 71%) described the im-  plementation of a specific learning dashboard, only 3 papers presented a theoretical proposal or a framework. Interest- ingly, the definition of dashboard was addressed in just 4 papers (7%), being dierent in each one of the cases.  3.2 Learning Context When considering the context where dashboards were ap-  plied (RQ1), we identified dierent target users, learning scenarios, educational levels and pedagogical approaches dis- cussed in the papers.  Target users. Teachers (41 papers; 75%) and students (28 papers; 51%) are clearly the main users of the dash- boards, although administrators and researchers also appear in some cases.  Learning scenarios. 50 papers (91%) targeted formal learning while the rest of the papers either addressed non- formal learning or they did not specify the type of learning.  Educational level. 29 papers (53%) addressed univer- sity settings and 17 (31%) did not specify the learning con- text.  Pedagogical approach. Although the papers often did not refer to a specific pedagogical approach (31 pa- pers; 56%), there is a noteworthy appearance of computer- supported collaborative learning (7 papers; 13%), blended (5 papers; 9%), and online learning (4 papers; 7%).  3.3 Learning Dashboard Solutions To answer RQ2 (regarding current dashboard solutions),  we analysed the (1) purpose of the dashboard, (2) types of data sources used, (3) platforms the data was retrieved from, (4) indicators and (5) visualizations presented in the dashboard.  Purpose. We distinguished 3 types of dashboard pur- poses: self-monitoring (28 papers; 51%), monitoring others (39 papers; 71%) and administrative monitoring (1 paper; 2%). Three papers (5%) did not explicitly state a purpose for their dashboard.  Types of data sources. The majority of papers (47 pa- pers; 85%) mentioned logs as their data source for the dash- board. Learning artefacts were the second most frequently- used data source (16 papers; 29%), followed by information explicitly asked from the users (7 papers; 13%), institutional databases (5 papers; 9%), physical user activity (4 papers; 7%) and external APIs (3 papers; 5%). Finally, 4 papers (7%) did not specify the used data source.  Platforms. The solutions, presented in the reviewed pa- pers, relied in total on data coming from 51 distinct plat- forms, of which 38 papers each mentioned a dierent plat- form. Moodle was the most prominent platform being used in 18% of the papers.  Indicators. From the paper review we obtained over 200 dierent indicators, which we categorized into 6 groups: learner, action, content, result, context, and social-related indicators. Regarding the subject, most of the papers pre- sented indicators about individuals (47 papers; 85%), fol- lowed by indicators related to whole classes (25 papers; 45%), small groups (8 papers; 15%), or large ones such as MOOCs (5 papers; 9%).  Visualization type. The most popular five are bar  charts (33 papers; 60%), line graphs (24 papers; 44%), tables (21 papers; 38%), pie charts (15 papers; 27%) and network graphs (10 papers; 18%).  3.4 Evaluation The maturity of current learning dashboard solutions, in  terms of evaluation presented in the papers (RQ3), is rather unequal: the majority of papers (58%) contained no evalu- ation whatsoever. Most papers used mixed methods for the evaluation (15 papers; 65% of the evaluations), as opposed to purely qualitative or quantitative evaluations (four and two instances, respectively). In total, ten papers gathered evaluation information from teachers, while 19 papers tar- geted students. Surprisingly, most of the evaluations (74% of the 23 papers that had evaluations) addressed general con- structs such as usability, usefulness or user satisfaction,while very few studies actually looked at (and provided evidence for) the impact of these technologies on learning (e.g., [3] did not find statistically significant eects).  4. IMPLICATIONS AND FUTURE LINES OF WORK  The review reveals a lack of an agreed and shared dash- board definition. Thus, we propose the following definition: a learning dashboard is a single display that aggregates mul- tiple visualizations of dierent indicators about learner(s), learning process(es) and/or learning context(s). Addition- ally, we have identified certain trends and gaps that may lead to future lines of work. For example, more than half of the papers focused on university settings, which highlights a need for learning dashboard studies in other settings, such as K-12 and non-formal settings. Regarding the data sources, the reviewed papers retrieved data mainly from logs and only a few used external APIs, physical user activity or in- stitutional databases. With the development of distributed and ubiquitous learning, it will become a must to aggregate complementary data sources. The field still lacks compara- tive studies among dierent dashboards or dashboard design options as well as empirical studies on the long-term impact and aordances of learning dashboards, especially in terms of learning gains.  5. REFERENCES [1] S. Few. Information dashboard design. OReilly, 2006. [2] B. Kitchenham and S. Charters. Guidelines for  performing systematic literature reviews in software engineering. Technical report, Keele University (UK), 2007.  [3] Y. Park and I.-H. Jo. Development of the Learning Analytics Dashboard to Support Students Learning Performance. Journal of Universal Computer Science, 21(1):110133, 2015.  [4] R. Sutherland, S. Eagle, and M. Joubert. A vision and strategy for Technology Enhanced Learning. Report from the STELLAR Network of Excellence, 2012.  [5] K. Verbert, S. Govaerts, E. Duval, J. L. Santos, F. Van Assche, G. Parra, and J. Klerkx. Learning dashboards: an overview and future research opportunities. Personal and Ubiquitous Computing, 18(6):14991514, 2014.    "}
{"index":{"_id":"83"}}
{"datatype":"inproceedings","key":"Rienties:2016:RTC:2883851.2883886","author":"Rienties, Bart and Boroowa, Avinash and Cross, Simon and Farrington-Flint, Lee and Herodotou, Christothea and Prescott, Lynda and Mayles, Kevin and Olney, Tom and Toetenel, Lisette and Woodthorpe, John","title":"Reviewing Three Case-studies of Learning Analytics Interventions at the Open University UK","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"534--535","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883886","doi":"10.1145/2883851.2883886","acmid":"2883886","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative learning, distance learning, online learning settings","abstract":"This study provides a conceptual framework how organizations may adopt evidence-based interventions at scale, and how institutions may evaluate the costs and benefits of such interventions. Building on a new conceptual model developed by the Open University UK (OU), we will analyse three case-studies of evidence-based interventions. By working with 90+ large-scale modules for a period of two years across the five faculties and disciplines within the OU, Analytics4Action provides a bottom-up-approach for working together with key stakeholders within their respective contexts. Using principles of embedded case-study approaches by Yin [1], by comparing the learning behavior, satisfaction and performance of 11079 learners the findings indicated that each of the three learning designs led to satisfied students and average to good student retention. In the second part we highlighted that the three module teams made in-presentation interventions based upon real-time analytics, whereby initial user data indicated VLE behaviour in line with expectations. In 2-5 years, we hope that a rich, robust evidence-base will be presented to show how learning analytics can help teachers to make informed, timely and successful interventions that will help learners to achieve their learning outcomes.","pdf":"Reviewing three case-studies of learning analytics  interventions at the Open University UK   Bart Rienties, Avinash Boroowa, Simon Cross, Lee Farrington-Flint, Christothea Herodotou,   Lynda Prescott, Kevin Mayles, Tom Olney, Lisette Toetenel, John Woodthorpe   Open University UK   Milton Keynes, United Kingdom   [First.Lastname]@open.ac.uk  ABSTRACT  This study provides a conceptual framework how organizations  may adopt evidence-based interventions at scale, and how  institutions may evaluate the costs and benefits of such  interventions. Building on a new conceptual model developed by  the Open University UK (OU), we will analyse three case-studies  of evidence-based interventions. By working with 90+ large-scale  modules for a period of two years across the five faculties and  disciplines within the OU, Analytics4Action provides a bottom- up-approach for working together with key stakeholders within  their respective contexts. Using principles of embedded case- study approaches by Yin [1], by comparing the learning behavior,  satisfaction and performance of 11079 learners the findings  indicated that each of the three learning designs led to satisfied  students and average to good student retention. In the second part  we highlighted that the three module teams made in-presentation  interventions based upon real-time analytics, whereby initial user  data indicated VLE behaviour in line with expectations. In 2-5  years, we hope that a rich, robust evidence-base will be presented  to show how learning analytics can help teachers to make  informed, timely and successful interventions that will help  learners to achieve their learning outcomes.   CCS Concepts   Applied computing~Distance learning     Applied computing~E-learning     Keywords  Online learning settings, Collaborative Learning, Distance  Learning   1. INTRODUCTION  Increased availability of large datasets [2, 3], powerful analytics  engines, and skillfully designed visualisations of analytics results  [4] mean that institutions may now be able to use the experience  of the past to create supportive, insightful models of primary (and  even real-time) learning processes [5-7]. While substantial  progress has been made in small-scale experimental and practical  studies, various meta-reviews and our LACE evidence hub [6, 8]  indicate that most learning analytics studies have provided limited  empirical support to suggest that learning analytics tools are  currently able to provide a rich personalised experience for each  learner.    We argue that one of the largest challenges for learning analytics  research and practice still lies ahead of us, namely how to put the  power of learning analytics into the hands of teachers and  administrators. At the Open University UK, the Analytics4Action  (A4A) project is building towards a comprehensive conceptual  model called Analytics4Action Evaluation Framework (A4AEF),  which is nested within a strong evidence-base. The A4AEF  describes how teachers and administrators can use learning  analytics to make successful interventions in their own practice [9,  10].    Building on work with the A4AEF [9, 10], we will use an  embedded case-study approach developed by Yin [1] to map,  unpack and develop a fine-grained understanding of the learning  analytics experiences in three large-scale level 1 modules at the  OU. A rich understanding of the functioning of learning analytics  interventions within the OU will be explored across three modules  that have intensively used a range of ICT tools (e.g., blogs,  computer simulations, discussion forums, videoconferences,  wikis). We hope that our study will contribute to a body of insight  how learning analytics approach can maximise student retention  using evidence-based interventions.   2. ANALYTICS4ACTION  2.1 Analytics4Action Evaluation Framework  As argued by [9], the A4AEF provides an evidence-based  framework for learning analytics with which students, researchers,  educators, and policy makers can manage, evaluate, and make  decisions about which types of interventions work well, under  which conditions, and which do not. [9] indicated that an effective  evidence-based framework in learning analytics needs to adhere to  five conditions:    1) accurately and reliably identify learners at-risk/needing for  support;    2) identify learning design improvements;  3) deliver (personalised) intervention suggestions that work for   both student and teacher;   4) operate within the existing teaching and learning culture; and   5) be cost-effective.    2.2 Research questions and approach  We used an embedded case-study design [1] to understand how  teachers designed their modules and how students engaged in  these three interactive modules:   1) To what extent do existing OU learning analytics metrics and  visualisations of student journeys provide an accurate picture  of learning design, learning processes and outcomes across  the three modules   2) To what extent can these learning analytics metrics and  visualisations help teachers to implement effective  interventions   Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on the  first page. Copyrights for third-party components of this work must  be honored. For all other uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883886     3. RESULTS   Three trends can be identified from the Module Activity Chart  visualisations in Figure 1. First of all, in all three modules the  learning design led to a continuous engagement of the vast  majority of students, as illustrated by the orange lines. A second  clear trend is that assessment drives learning and learning  engagement.  A final trend is that the VLE engagement patterns  across the three modules followed subtly different trajectories.     Figure 1: VLE engagement across the three modules.   Note: Orange line = VLE Engagement per week; Blue line = registered  students; Green dots = Submission rate of assessments    Table 1: Student retention and satisfaction      Average  Level 1 SD Arts   Psycho- logy   Tech- nology   Registered  students   666.62 861.77 2234 3181 2489   After 25% 94.13 5.93 94.10 87.90 86.50   % Passed 65.02 12.73 71.30 64.40 59.70   z-score -0.19 2.87 3.80 -0.70 3.30      As illustrated in Table 1, the retention (% Passed) in the Arts  module was well above the average of level 1 modules, while the  Psychology module and Technology module were within the  range of the average of level 1 (n=114). The z-score, which takes  into consideration the relative performance of the module given  the positive/negative characteristics of the students, indicated that  both Arts and Technology performed better than expected,   whereby a score close to 4 indicated a strong performance for  both modules. The psychology module was relatively  underperforming given the characteristics of the students,  although with a small effect.    4. DISCUSSION  An important lesson from research question 1 is that just  focussing on basic learning outcomes measures might lead to ill- informed decisions by managers and teachers alike. In terms of  research question 2, at LAK 2016 we will elaborate on the follow- up interventions that were initiated based upon the discussions  with the Analytics4Action team in the three modules indicated  that small changes in learning design could be traced effectively  with current VLE data. By working together in interdisciplinary  teams consisting of teachers, learning designers, learning analytics  specialists, educational psychologists, data interpreters, IT  specialists and multi-media designers, the OU aims to  continuously refine the learning experiences of our large cohorts  of learners to meet their specific learning needs in an evidence- based manner. In the next 25 years, we hope that a rich, robust  evidence-base will be available which will demonstrate how  learning analytics approaches can help teachers and administrators  around the globe to make informed, timely and successful  interventions that will help each learner achieve their learning  outcomes.   REFERENCES  [1] Yin, R. K. Case study research: Design and methods. Sage,   2009.  [2] Arbaugh, J. B. System, scholar, or students Which most   influences online MBA course effectiveness Journal of  Computer Assisted Learning, 30, 4 2014), 349-362.   [3] Rienties, B., Toetenel, L. and Bryan, A. Scaling up  learning design: impact of learning design activities on LMS  behavior and performance. ACM, City, 2015.   [4] Gonzlez-Torres, A., Garca-Pealvo, F. J. and Thern, R.  Humancomputer interaction in evolutionary visual software  analytics. Computers in Human Behavior, 29, 2 (3// 2013),  486-495.   [5] Ferguson, R. and Buckingham Shum, S. Social learning  analytics: five approaches. ACM, City, 2012.   [6] Papamitsiou, Z. and Economides, A. Learning Analytics and  Educational Data Mining in Practice: A Systematic Literature  Review of Empirical Evidence. Educational Technology &  Society, 17, 4 2014), 4964.   [7] Arnold, K. E. and Pistilli, M. D. Course signals at Purdue:  using learning analytics to increase student success. ACM,  City, 2012.   [8] Clow, D., Cross, S., Ferguson, R. and Rienties, B. Evidence  Hub Review. LACE Project, City, 2014.   [9] Rienties, B., Boroowa, A., Cross, S., Kubiak, C., Mayles, K.  and Murphy, S. Analytics4Action Evaluation Framework: a  review of evidence-based learning analytics interventions at  Open University UK. Journal of Interactive Media in  Education, 1, 2 2016), 1-12.   [10] Rienties, B., Cross, S. and Zdrahal, Z. Implementing a  Learning Analytics Intervention and Evaluation Framework:  what works Springer, City, 2016.        "}
{"index":{"_id":"84"}}
{"datatype":"inproceedings","key":"Ruiperez-Valiente:2016:ASI:2883851.2883947","author":"Ruip'erez-Valiente, Jos'e A. and Mu~noz-Merino, Pedro J. and Kloos, Carlos Delgado","title":"Analyzing Students' Intentionality Towards Badges Within a Case Study Using Khan Academy","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"536--537","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883947","doi":"10.1145/2883851.2883947","acmid":"2883947","publisher":"ACM","address":"New York, NY, USA","keywords":"Khan academy, badges, learning analytics, modelling behavior","abstract":"One of the most common gamification techniques in education is the use of badges as a reward for making specific student actions. We propose two indicators to gain insight about students' intentionality towards earning badges and use them with data from 291 students interacting with Khan Academy courses. The intentionality to earn badges was greater for repetitive badges, and this can be related to the fact that these are easier to achieve. We provide the general distribution of students depending on these badge indicators, obtaining different profiles of students which can be used for adaptation purposes.","pdf":"Analyzing Students Intentionality towards Badges within  a Case Study using Khan Academy  Jos A. Ruiprez-Valientea,b, Pedro J. Muoz-Merinoa, Carlos Delgado Kloosa  a Universidad Carlos III de Madrid, Avenida Universidad 30, 28911 Legans (Madrid) Spain   b IMDEA Networks Institute, Av. del Mar Mediterrneo 22, 28918 Legans (Madrid) Spain  {jruipere, pedmume, cdk}@it.uc3m.es   ABSTRACT  One of the most common gamification techniques in education is  the use of badges as a reward for making specific student actions.  We propose two indicators to gain insight about students  intentionality towards earning badges and use them with data  from 291 students interacting with Khan Academy courses. The  intentionality to earn badges was greater for repetitive badges, and  this can be related to the fact that these are easier to achieve. We  provide the general distribution of students depending on these  badge indicators, obtaining different profiles of students which  can be used for adaptation purposes.    Categories and Subject Descriptors  J.1 [Computer Applications]: Administrative Data Processing   Education. K.3.2 [Computing Milieux]: Computers and  Education  Computer and Information Science Education:  Information systems education    General Terms  Algorithms, Measurement, Human Factors   Keywords  Badges, Learning Analytics, Khan Academy, Modelling Behavior   1. INTRODUCTION  Gamification techniques in education have been used broadly in  the last years. The application of learning analytics for gamified  environments in education can bring useful information for the  learning process such as students behavior through gamification  (e.g. if they are motivated or not for gamification features). One of  the most used gamification techniques in education is the  provision of badges. Badges are awards that students can receive  by doing specific actions on an educational platform (e.g. for  watching videos during some time or for solving a set of exercises  correctly). Gamification indicators are not frequently included in  those platforms, e.g. the review of indicators reported by  Dyckhoff et al. [1], does not include any.  ALAS-KA [2], a plugin  for the Khan Academy platform that provides a set of more than  21 new different indicators related to the learning process, include  some general indicators related to badges but they do not describe  the intention of students to get them.   In this work, we propose and implement indicators for analyzing  students intention for the achievement of badges, taking into   account two different types of badges: topic and repetitive ones.  There are other works [3, 4] which have also proposed models of  the students behavior towards badges in Stack Overflow and  findings seem to indicate that indeed badges affect the normal  behavior of students. The work of Grant & Betts [3], focuses in  three types of specific badges of Stack Overflow to see how these  badges influence the behavior of students  which is a similar  approach to ours as it is very challenging to focus in all types of  badges. They found that user activity increased before earning  some badges and it would decrease immediately afterwards.  However to the best of our knowledge no work has proposed  similar models about the intentionality of students towards a  badge system as we do in this proposal.   2. MODELLING STUDENTS  INTENTIONALITY TOWARDS BADGES  IN KHAN ACADEMY  Students can receive badges as an award for their interaction with  the platform. Students can also access detailed information about  each badge and the requisites to get it and the student will receive  a notification whenever they earn a badge. Our aim is to infer if a  student is intentionally completing the badge conditions in order  to acquire badges. Taking into account the diverse types of  badges, there are some cases in which we cannot infer this  information based on the events that the learning platform  provide. We focus on two types of badge: topic badges and  repetitive badges.    2.1 Intentionality on Topic Badges (ITB)  Topic badges have a set of exercises as requirement. The student  must reach a proficient level in all of the required exercises to get  the badge. In addition, in our experiment, the exercises required  are not repeated in the requisites of the different topic badges. A  specific topic badge can only be received once.   We calculate the number of badges the student has received, and  compare it with the maximum number of badges the student might  have received, taking into account the total number of proficient  exercises the user has mastered. Therefore, we would be able to  see if the student is trying to maximize the number of badges  he/she is earning according to his/her actions.   The system first needs to retrieve the number of topic badges that  the student has received. Next, we have designed an algorithm  which obtains the maximum number of badges that the student  could have received with the number of exercises that he/she has  mastered. The last step is to divide the number of badges that the  student has received with the maximum number of badges that the  student could have received with those skills, which gives the ITB  indicator.    2.2 Intentionality on Repetitive Badges (IRB)  This indicator is related to those badges that can be earned  repetitively. Our goal is to be able to detect if the specific badge  was earned as part of the natural learning process or not. Then, we   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883947     would be able to detect if the student is intentionally earning more  and more of the same type.   Students should solve exercises until the system informs them that  they have achieved a proficient level in that skill. Therefore, when  a student keeps solving exercises after achieving proficiency in  the skill and they earn a repetitive badge, we make the hypothesis  that they keep solving exercises not to learn (they have already  mastered the concept) but to earn more badges. At the end of the  process we can calculate the percentage of repetitive badges that  were intentionally earned (i.e. the level of repeated badges earned  without being part of the learning process out of the total number  of badges), which gives the IRB indicator.    3. RESULTS AND CONCLUSIONS  In this section we apply the two metrics from previous section in  an experiment with 291 students using the Khan Academy  platform at Universidad Carlos III de Madrid. These courses are  taken by freshmen students in the topics of physics, chemistry and  mathematics.  For this analysis we have set a threshold of at least 60 minutes in  order to guarantee that the students made some interaction with  the platform, discarding students who did not do enough activity.  Figure 1 shows the density distribution of both intentionality  indicators for all students. The 1st quartile of ITB and IRB  indicators is 0, as well as the median of ITB. This means that  there are a big percentage of people who despite spending at least  60 minutes, did not show much interest on earning badges,  especially topic badges. On the other hand the median for IRB is  48.44%, which seems to indicate that students show more interest  in repetitive badges. The mean value of IRB indicates that the  average user earns 39.52% of repetitive badges intentionally,  which we think is a high percentage. We must state too, that  probably many of the students near the 0% of both indicators,  probably did not interacted a lot with the platform, as a result,  they might be end up classified as having no interest for badges.   A visual look at figure 1 can help understand how these two  indicators are distributed among the students. We can see that for  ITB, a higher amount of the population is accumulated in the low  numbers of ITB, and there are not many users getting between 50- 100% of ITB, however we can see a small peak at 100%, who are  the set of students showing a lot of interest. In the case of IRB, we  can see that there is a valley between 10-30% who are probably  those students who interacted with the platform, but did not show  interest for repetitive badges. Also we can see a moderate peak  between 50-75% whom are students showing a moderate interest  in repetitive badges and 75-100% whom are those showing a high  interest. We should note out that to be able to acquire in IRB such  high values, you must really put a lot of interest in these badges.  Overall, it looks like students felt more motivated towards  repetitive badges rather than topic badges, but we should also  mention that as these badges are easier to earn, and that might be  why students might feel more motivated towards them. We also  found a moderate correlation (r = 0.445, p < 0.00) between IRB  and ITB, which makes sense, because if one student is interested  in earning one type of badge, probably the other type too.  Knowing which students are really motivated and challenged to  earn more badges, might help in order to be able to adapt their  learning process within virtual learning environments, and these  indicators can be combined with others for making adaptation e.g.  using approaches like [5]. Although these indicators have been   defined for the Khan Academy platform, they can be easily  generalized to other platforms. In order to implement these badge  indicators in a different context, the intentionality for obtaining  topic badges should consider a set of badges for which each badge  can be achieved by obtaining the proficiency in a set of predefined  exercises. Each badge cannot repeat an exercise as requisite if it is  part of another badges requisites. The intentionality for achieving  repetitive badges should consider badges that can be obtained  several times for students.     Figure 1: Density distribution of the intentionality indicators.   4. ACKNOWLEDGMENTS  This work has been supported by the eMadrid project (Regional  Government of Madrid) under grant S2013/ICE-2715, the  RESET project (Ministry of Economy and Competiveness)  under grant RESET TIN2014-53199-C3-1-R and the European  Erasmus+ SHEILA project under grant 562080-EPP-1-2015-BE- EPPKA3-PI-FORWARD   5. REFERENCES  [1] Dyckhoff, A.L., Lukarov, V., Muslim, A., Chatti, M.A. and   Schroeder, U. 2013. Supporting action research with learning  analytics. Proceedings of the Third International Conference  on Learning Analytics and Knowledge - LAK 13 (New York,  New York, USA, 2013), 220.   [2] Ruiprez-Valiente, J.A., Muoz-Merino, P.J., Leony, D. and  Delgado Kloos, C. 2015. ALAS-KA: A learning analytics  extension for better understanding the learning process in the  Khan Academy platform. Computers in Human Behavior. 47  (2015), 139148   [3] Grant, S. and Betts, B. 2013. Encouraging User Behaviour  with Achievements: An Empirical Study. 10th Working  Conference on Mining Software Repositories (2013), 6568.   [4] Anderson, A., Huttenlocher, D. and Kleinberg, J. 2013.  Steering User Behavior with Badges. 22nd international  conference on World Wide Web (2013), 95106.   [5] Muoz-Merino, P. J., Kloos, C. D., Muoz-Organero, M., &  Pardo, A. 2015. A software engineering model for the  development of adaptation rules and its application in a  hinting adaptive e-learning system. Computer Science and  Information Systems, 12(1), 203-231       "}
{"index":{"_id":"85"}}
{"datatype":"inproceedings","key":"Molenaar:2016:LAP:2883851.2883892","author":"Molenaar, Inge and van Campen, Carolien Knoop","title":"Learning Analytics in Practice: The Effects of Adaptive Educational Technology Snappet on Students' Arithmetic Skills","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"538--539","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883892","doi":"10.1145/2883851.2883892","acmid":"2883892","publisher":"ACM","address":"New York, NY, USA","keywords":"ability levels, arithmetic's, educational technologies, primary education","abstract":"Even though the recent influx of tablets in primary education goes together with the vision that educational technology empowered with learning analytics will revolutionize education, empirical results supporting this claim are scares. Adaptive educational technology Snappet combines extracted and embedded learning analytics daily in classrooms. While students make exercises on the tablet this technology displays real-time data of learner performance in a teacher dashboard (extracted analytics). At the same time, learner performance is used to adaptively adjust exercises to students' progress (embedded analytics). This quasiexperimental study compares the development of students' arithmetic skills over one schoolyear (grade 2 and 4) in a traditional paper based setting to learning with the adaptive educational technology Snappet. The results indicate that students in the Snappet condition make significantly more progress on arithmetic skills in grade 4. Moreover, in this grade students with a high ability level, benefit the most from working with this adaptive educational technology. Overall the development pattern of students with different abilities was more divergent in the AET condition compared to the control condition. These results indicate that adaptive educational technologies combining extracted and embedded learning analytics are indeed creating new education scenarios that contribute to personalized learning in primary education.","pdf":"Learning analytics in practice  The effects of adaptive educational technology Snappet on   students arithmetic skills     Inge Molenaar  Radboud University   Nijmegen, Netherlands  I.molenaar@pwo.ru.nl   Carolien Knoop-van Campen  Radboud University   Nijmegen, Netherlands  c.vancampen@pwo.ru.nl   ABSTRACT  Even though the recent influx of tablets in primary education goes  together with the vision that educational technology empowered  with learning analytics will revolutionize education, empirical  results supporting this claim are scares. Adaptive educational  technology Snappet combines extracted and embedded learning  analytics daily in classrooms. While students make exercises on  the tablet this technology displays real-time data of learner  performance in a teacher dashboard (extracted analytics). At the  same time, learner performance is used to adaptively adjust  exercises to students progress (embedded analytics). This quasi- experimental study compares the development of students  arithmetic skills over one schoolyear (grade 2 and 4) in a  traditional paper based setting to learning with the adaptive  educational technology Snappet. The results indicate that students  in the Snappet condition make significantly more progress on  arithmetic skills in grade 4. Moreover, in this grade students with  a high ability level, benefit the most from working with this  adaptive educational technology. Overall the development pattern  of students with different abilities was more divergent in the AET  condition compared to the control condition. These results  indicate that adaptive educational technologies combining  extracted and embedded learning analytics are indeed creating  new education scenarios that contribute to personalized learning  in primary education.   CCS Concepts   Applied computing~Education      Applied computing~Computer-managed instruction     Applied computing~Computer-assisted instruction      Applied computing~Learning management systems      Applied computing~E-learning   Keywords  Educational technologies, primary education, arithmetics, ability  levels   1. INTRODUCTION  Even though the recent influx of tablets in primary education goes  together with the vision that educational technology empowered  with learning analytics will revolutionize education, empirical  results supporting this claim are scares [1]. Specifically, advances  are expected in adapting education to the needs of individual  students leading to enhanced educational effectiveness and  differentiated instruction [2, 3]. Effective usage of learning  analytics, however, in primary education is largely dependent on  the teacher. Consequently, an important issue is how to augment  teacher intelligence with learning analytics valuing the dynamics  between technology and human control. Central in this discussion  is the application of both extracted and embedded learning  analytics [4]. Extracted learning analytics make data actionable  for teachers through for instance visualizations, such as teacher  dashboards [5]. This allows teachers to adjust instruction and  feedback to the ability of the learners. Moreover, embedded  learning analytics automatically adjust of learning materials to  learners abilities for example, by adjusting exercises to the  current performance of a student [4]. Learning analytics thus  support teachers in making decisions to create learning  experiences that adapt to their students abilities. Learning  analytics in practice have the potential to change teaching and  learning for students with different ability levels in primary  education. This study provides insights on how extracted analytics  combined with embedded analytics are used in practice and what  the effects on students skill development are. This increases our  understanding of the combination of two forms of learning  analytics in interaction with the human role of the teacher in the  classroom. This quasi-experimental study compares the  development of students arithmetic skills over one schoolyear  (grade 2 and 4) in a traditional paper based setting (control  condition) to learning with the adaptive educational technology  Snappet (AET condition).  The adaptive educational technology Snappet leads to the  construction of a unique educational scenario in which elements  of traditional instruction are combined with innovative use of  technology. Classroom wide teacher instruction plays an  important role in this scenario. The technology captures real-time  data of learner performance, which are displayed to the teachers  concurrently on dashboards. Teachers use this information as  input to adjust their further instruction, modelling and feedback as  well as for the selection of classroom wide practices. Next  students continue to practice with adaptive exercises. This type of  practice supports sustained individual practice at the students  ability level.   Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on the  first page. Copyrights for third-party components of this work must  be honored. For all other uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851     2. METHOD  In total, 11 schools, with 430 children from grade 2 and 4  participated (AET condition: 227 students, 47% girl, Control  condition: 203 students, 53% girl). The national standardized  arithmetic assessment, CITO Rekenen-Wiskunde [Arithmetic- Mathematics], was used to measure childrens arithmetic skills.  Based on previous CITO arithmetic scores, students were divided  in three ability levels. The high ability group represents the top  25% students of the Netherlands, the middle ability group  contains the middle 50% and finally the low ability group  represents the lowest 25% of students scoring the least. The log  files from the adaptive educational technology Snappet provided  detailed information on students practice activities. Class wide  exercises were the same for all students in the class, adaptive  exercises were adjusted to the students performance.  To examine the effect of Snappet on students arithmetics skill  development over one schoolyear, a two-way ANCOVA with  repeated measures was conducted, with time (June 2014 / June  2015) as within-subject-factor, with Condition (Snappet / control)  and ability level (high, middle, low) as between-subject-factors.  SES and Gender were included as covariates to control for the  differences between children in the AET and Control condition.   3. RESULTS  The analyses indicated no differences between the AET and the  control condition in arithmetic skill development over one  schoolyear in grade 2, see figure 1. Boys did make more progress  than girls and children with a middle and low ability level, made  more progress on arithmetic skills over one schoolyear compared  to children with a high ability level.  The analyses indicated a three-way interaction effect in grade 4:  children with a high ability level, made more progress in the AET  condition compared to children in the control condition (see  Figure 2).     Figure 1:  Grade 2 Arithmetic Scores per Condition per   Ability level.   4. DISCUSSSION  Learning with adaptive educational technology Snappet  had a  significant effect on childrens arithmetic skill development in  grade 4: also the results indicated a significantly greater progress  for high ability students arithmetic skill development in the AET  condition versus the control condition. In general, the application  of the AET supported a more divergent development pattern  between students in the three ability groups. In the control  condition a more convergence development  pattern between the  different ability groups was observed. This could be an indication   of more differentiated teaching practices of the teachers supported  with learning analytics.   Nevertheless, data showed great variation in the number of both  class wide and adaptive exercises made over different classrooms  and different schools. Although the majority of the teachers (98%)  indicated to use the EAT to personalize education, their usage of  adaptive exercises was only marginal percentages (21%) of the  total amount of exercises made by students. This indicates that the  combination of classroom wide instruction with adaptive forms of  practice is a challenge for teachers. Specifically the integration of  extracted and embedded analytic tools in daily practice deserves  special attention.       Figure 2: Grade 4 Arithmetic Scores per Condition per Ability   level.  This study showed that an adaptive educational technology that  combined extracted and embedded learning analytics indeed  created new education scenarios, which contributed to more  personalized learning in primary education. However, the large  variation in classroom wide and adaptive exercises does stress the  need for advanced efforts to support teachers in how to use  learning analytics. The challenge is to balance embedded analytics  and extracted analytics, in such a way that system strengths and  human strengths together optimally support personalized learning.   REFERENCES  [1] A. Papamitsou, Z. Economides, A. Anastasios. Learning   Analytics and Educational Data Mining in Practice: A  Systematic Literature Review of Empirical Evidence.  Journal of Educational Technology & Society, 17.4: 49- 64, 2014.   [2] D. Tempelaar, B. Rienties, B. Giesbers. In search for the  most informative data for feedback generation: Learning  Analytics in a data-rich context. Computers in Human  Behavior, 47: 157-167,2015.   [3] M. Fullan. Systems thinkers in action. Innovation. UK.  London: DfES with NCSL, 2004.   [4] A. Wis, Y, Zhou,, S. Hausknecht. Learning analytics for  online discussions: Embedded and extracted  approaches. Journal of Learning Analytics, 1.2: 48-71,  2014.   [5] K. Verbert, et al. Dataset-driven research to support  learning and knowledge analytics. Journal of  Educational Technology & Society, 15.3: 133-148,  2012.     "}
{"index":{"_id":"86"}}
{"datatype":"inproceedings","key":"Feng:2016:EDI:2883851.2883908","author":"Feng, Mingyu and Krumm, Andrew E. and Bowers, Alex J. and Podkul, Timothy","title":"Elaborating Data Intensive Research Methods Through Researcher-practitioner Partnerships","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"540--541","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883908","doi":"10.1145/2883851.2883908","acmid":"2883908","publisher":"ACM","address":"New York, NY, USA","keywords":"data intensive research, learning analytics, researcher-practitioner partnership","abstract":"Technologies used by teachers and students generate vast amounts of data that can be analyzed to provide insights into improving teaching and learning. However, practitioners are left out of the process. We describe the development of an approach by which researchers and practitioners can work together to use data intensive research methods to launch improvement efforts within schools. This paper describes elements of the first year of a researcher-practitioner partnership, highlighting initial findings, challenges, and strategies for overcoming these challenges.","pdf":"Elaborating Data Intensive Research Methods through  Researcher-Practitioner Partnerships   Mingyu Feng  SRI International   333 Ravenswood Ave  Menlo Park, CA   mingyu.feng@sri.com  Andrew E. Krumm  SRI International   333 Ravenswood Ave  Menlo Park, CA   andrew.krumm@sri.com  Alex J. Bowers  Columbia University  525 West 120th St.   New York, NY 10027  bowers@tc.edu     Timothy Podkul   SRI International   333 Ravenswood Ave  Menlo Park, CA   timothy.podkul@sri.com   ABSTRACT  Technologies used by teachers and students generate vast   amounts of data that can be analyzed to provide insights into  improving teaching and learning. However, practitioners are left  out of the process. We describe the development of an approach  by which researchers and practitioners can work together to use  data intensive research methods to launch improvement efforts  within schools. This paper describes elements of the first year of a  researcher-practitioner partnership, highlighting initial findings,  challenges, and strategies for overcoming these challenges.     Categories and Subject Descriptors  K.3.m [Computers and Education]: Miscellaneous.    General Terms  Measurement, Design, Human Factors.   Keywords  Data intensive research, researcher-practitioner partnership,   learning analytics   1. INTRODUCTION  Multiple technologies, such as learning management systems,   tutoring systems, digital curriculums, and open-access,  supplemental instructional resources, are regularly used in  schools. Many of these systems track students use, thus providing  a rich data stream that can be analyzed to better understand  teaching and learning [e.g. 1, 11]. However, often missing from  the current research landscape is the voice of practitioners. In  many ways, the field is driven by solutions (e.g., dashboards and  data mining techniques) looking for problems [5, 6, 17], with little  explication on how to productively work with practitioners in a  collaborative way. Thus, our focus is on describing the process of  developing research-practice partnerships where the voice and  needs of practitioners drive the collaborative inquiry [8]. In this  paper, we report on aspects of the first year of a project that  sought to develop a series of repeatable and scalable processes to  support researchers and practitioners collaboratively engage in  data intensive research. To begin developing tools, routines, and  processes for supporting researchers and practitioners in working  together, the approach documented in this paper builds on a  research-practice partnership between researchers and  practitioners at a charter management organization (CMO)   operating in California.1 The CMO serves diverse students, uses  advanced technologies, and has a strong commitment to making  data-driven instructional decisions. Through a customized online  platform, students access most of their learning resources across  all grades and subject areas. Students use the platform in mostly  self-directed ways, with monitoring and support from teachers.    Multiple researchers have documented how practitioners can  effectively use standardized test scores, grades, and behavioral  infractions (e.g., [2, 7]). Researchers have also demonstrated how  to use data intensive methods to identify patterns among these  same variables (e.g., [3, 4]). A next step for data intensive  research is to engage practitioners earlier in the process, building  on their insights in shaping analyses to ensure buy-in from both  researchers and practitioners [8]. This project uses a design based  research approach [15] and theories from organizational and  learning sciences to inform the development and refinement of  partnership activities. As this paper demonstrates, a partnership- based approach can be effective for developing and sustaining  change efforts in schools because it can aid practitioners in  developing new knowledge and skills applicable to future  problems and datasets after the partnership ends [9].    2. UNDERSTANDING NEEDS AND  CONTEXT   We engaged in an iterative cycle of needs analyses by jointly  identifying pressing problems of practice facing the CMO. We  started the process with joint meetings that involved researchers  and a representative team from the CMO including principals,  teacher professional development leads, and members of the  technology team. The research team initially facilitated a blue  sky, ideation activity where researchers and practitioners  brainstormed potential questions. The partnership then iteratively  triaged and refined blue sky questions. Questions were selected  based on the benefits of answering a question in relation to  potential costs for students, teachers, or administrators. The entire  process led to a number of focus areas and research questions that  were meaningful to all audience and do-able given the constraints  of time and data quality.   Another important early task was to better understand the  CMOs context, which included (1) discerning how students  interact with teachers and content and (2) identifying ways in  which teachers and administrators currently use data (i.e., what is  being looked at and when). Researchers attended professional  development sessions, exchanged emails, and conducted meetings  to clarify what instruction looks. CMO staff walked researchers  through the process they use to manage, analyze, and report on  data for various audiences. One key takeaway from these efforts  was that managing and merging datasets was a particular pain  point for CMO staff and one that prohibited potentially useful                                                                     1 In the United States, a CMO is like a traditional K-12 school district in   that they are public schools; yet they operate under a uniquely defined  charter, the specifics of constitutes a charter vary by states.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.  Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  @ 2016 ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883908   http://dx.doi.org/10.1145/2883851.2883908   analyses. Moreover, from a teachers perspective, data was shown  to them on how well students did (e.g., assessment scores) and not  necessarily what they were doing (e.g., resources accessed).    Although the processes described above took significant time  and effort, it was extremely important for the partnership and for  later data analysis work. Data analysts often have a strong desire  to jump in to mine the data to look for interesting patterns or make  accurate predictions. However, when working with practitioners  who face specific challenges, it is important to make sure that  researchers have a good understanding of the school context and  are answering meaningful questions that practitioners could  engage in and are willing and able to take action on.   3. CAPACITY BUILDING  The research team conducted preliminary data analyses and   through an iterative process, the partnership reviewed results and  identified ways data analyses could be integrated into regular  work practices in schools. Collaboratively interpreting data with  practitioners is an important element of the partnerships capacity  building efforts. We scaffolded practitioners understanding of  analyses by explicitly instructing CMO staff on researchers  thinking and the methods that were used to analyze, and visualize  data as well as interpret results. Instead of having researchers  present results, we organized discussions as hands-on exploration  opportunities for CMO staff and encouraged them to try to make  sense out of the results using data visualizations, for example, that  did not contain the CMOs data in order to promote understanding  of the techniques that were employed before engaging with CMO- specific data that used the same visualizations. We also relied on  relatively basic data exploration and modeling approaches and  supported the interpretation of various representations, such as  histograms, scatter plots, trend lines, and box plots.   4. BUILDING INFRASTRUCTURES  Throughout the first year of the project, we identified the role   of (1) partnership and (2) analytics infrastructures that appeared  necessary for supporting collaborative research These supports  include the routines and tools that researchers will potentially  need to consider in order to effectively collaborate with  practitioners. Our experience showed that the key pieces to build a  strong partnership include: (1) jointly developed goals that are  conditional and revisited; (2) intentional efforts to build capacity  and promote sustainability; (3) mutually agreed communication  routines, roles, and responsibilities; and (4) long-term data sharing  agreements. The second support included the analytics  infrastructure that we established during the early stage of the  project. Elements of this infrastructure deal with the nuts and bolts  of managing files, analyzing data, and reporting on data products.  Necessary but not sufficient conditions include (1) a shared  understanding of available data and the activity system from  which they are drawn and (2) processes for sharing data and data  products that are secure and easy to access.   5. CONCLUSION   Although the promise of practitioners drawing on data to   inform decisions is widely recognized, effective implementation  has proven challenging ([e.g. 10, 13]). We argue that one way to  overcome many of the challenges is to include practitioners as  early as possible in the process. However, engaging practitioners  intimately in multiple facets of a research project comes with its  own challenges. The approach we took was to enhance  practitioners ability to interpret and take action with data and to  focus on developing basic infrastructures that can support work  long term. The style of inquiry and the school-by-school results of  a partnership approach will likely not appeal to all researchers  nor should it. Yet, our efforts show that the skills of researchers  can be applied in multiple ways to support practitioners and that  researchers can benefit from closer ties to what is happening in  classrooms.   6. ACKNOWLEDGEMENTS  The research was supported by the NSF, through Grant DRL-  1444621. The opinions expressed are those of the authors and do  not necessarily represent views of the NSF.   REFERENCES  [1] Baker, R. S. (2013). Learning, Schooling, and Data   Analytics. In M. Murphy, S. Redding & J. Twyman (Eds.),  Handbook on innovations in learning. Charlotte, NC:  Information Age Publishing.    [2] Boudett, K. P., City, E. A., & Murnane, R. J. (2013). Data  Wise: A step-by-step guide to using assessment results to  improve teaching and learning. Revised and expanded  edition. Cambridge, MA: Harvard Education Press.   [3] Bowers, A. J. (2010). Analyzing the longitudinal K-12  grading histories of entire cohorts of students: Grades, data  driven decision-making, dropping out and hierarchical  cluster analysis. Practical Assessment Research and  Evaluation, 15, 1-18.   [4] Bowers, A. J., & Sprott, R. (2012). Examining the Multiple  Trajectories Associated with Dropping Out of High School:  A Growth Mixture Model Analysis. Journal of Educational  Research, 105, 176-195.   [5] Bryk, A. S., Gomez, L., Grunow, A., & LeMahieu, P. (2015).  Learning to improve: How Americas schools can get better  at getting better. Cambridge, MA: Harvard Education Press.   [6] Cho, V., & Wayman, J. C. (2015). Districts Efforts for Data  Use and Computer Data Systems: The Role of Sense making  in System Use and Implementation. Teachers College  Record, 116(2), 1-45.    [7] Coburn, C. E., & Turner, E. O. (2011). Research on data use:  A framework and analysis. Measurement, 9, 173-206.   [8] Coburn, C. E., Penuel, W. R., & Geil, K. E. (2013).  Research-practice partnerships. New York, NY: William T.  Grant Foundation.    [9] Coburn, C. E., Russell, J. L., Kaufman, J. H., & Stein, M. K.  (2012). Supporting sustainability: Teachers advice networks  and ambitious instructional reform. American Journal of  Education, 119(1), 137-182.   [10] Franke, M. L., Carpenter, T. P., Levi, L., & Fennema, E.  (2001). Capturing teacher's generative growth: A follow-up  study of professional development in mathematics. American  Education Research Journal, 38(3), 653-690.   [11] Koedinger, K. R., D'Mello, S., McLaughlin, E. A., Pardos, Z.  A., & Ros, C. P. (2015). Data mining and education. Wiley  Interdisciplinary Reviews: Cognitive Science.    [12] Marsh, J. A. (2012). Interventions Promoting Educators Use  of Data: Research Insights and Gaps. Teachers College  Record, 114(11), 1-48.   [13] Means, B., Chen, E., Debarger, A., & Padilla, C. (2010).  Teachers Ability to Use Data to Inform Instruction:  Challenges and Supports. Washington, DC: U.S. Department  of Education.   [14] Penuel, W. R., & Martin, C. (2015, April). Design-Based  Implementation Research as a Strategy for Expanding  Opportunity to Learn in School Districts. Paper presented at  the Research Conference of the NCTM, Boston, MA.   [15] Schutt, R., & O'Neil, C. (2013). Doing Data Science: Straight  Talk from the Frontline. Cambridge, MA: O'Reilly.       "}
{"index":{"_id":"87"}}
{"datatype":"inproceedings","key":"Jo:2016:PEL:2883851.2883912","author":"Jo, Yohan and Tomar, Gaurav and Ferschke, Oliver and Ros'e, Carolyn Penstein and Gavsevi'c, Dragan","title":"Pipeline for Expediting Learning Analytics and Student Support from Data in Social Learning","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"542--543","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883912","doi":"10.1145/2883851.2883912","acmid":"2883912","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, social learning","abstract":"An important research problem in learning analytics is to expedite the cycle of data leading to the analysis of student progress and the improvement of student support. For this goal in the context of social learning, we propose a pipeline that includes data infrastructure, learning analytics, and intervention, along with computational models for individual components. Next, we describe an example of applying this pipeline to real data in a case study, whose goal is to investigate the positive effects that goal-setting students have on their peers, which suggests ways in which we might foster these social benefits through intervention.","pdf":"Pipeline for Expediting Learning Analytics and  Student Support from Data in Social Learning  Yohan Jo, Gaurav Tomar, Oliver Ferschke, Carolyn Penstein Ros, Dragan Gaevic School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA  {yohanj, gtomar, ferschke, cprose}@cs.cmu.edu Schools of Education and Informatics, The University of Edinburgh, Edinburgh, UK  dgasevic@acm.org  ABSTRACT An important research problem in learning analytics is to expedite the cycle of data leading to the analysis of student progress and the improvement of student support. For this goal in the context of social learning, we propose a pipeline that includes data infrastructure, learning analytics, and in- tervention, along with computational models for individual components. Next, we describe an example of applying this pipeline to real data in a case study, whose goal is to inves- tigate the positive eects that goal-setting students have on their peers, which suggests ways in which we might foster these social benefits through intervention.  Categories and Subject Descriptors K.3.1 [Computer Uses in Education]  General Terms Algorithms, Human Factors, Languages  Keywords Learning Analytics, Social Learning  1. INTRODUCTION More and more recent work in educational data mining  and learning analytics refers to a virtuous cycle of data leading to insight on what students need and then improve- ments in support for learning [3]. An important goal is tight- ening this cycle. In this paper, we propose a pipeline and its component models that can achieve this goal.  In this work we are specifically interested in social learn- ing. Social learning is based on a Vygotskian theoretical frame where learning practices begin within a social space and become internalized through social interaction. That interaction may involve observation or more intensive inter- action through cycles of feedback and help exchange.  We will present a three-part pipeline for expediting data analysis and student support in social learning. The pipeline  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s). LAK 16, April 25-29, 2016, Edinburgh, United Kingdom ACM 978-1-4503-4190-5/16/04. http://dx.doi.org/10.1145/2883851.2883912  consists of the data infrastructure for a uniform interface with heterogeneous data from social interaction, a proba- bilistic model for analyzing learning pathways in social con- texts, and recommendation technology to evaluate and sup- port learning processes. We will then present a case study, in which we describe an example of applying our method to real data and summarize the findings from the case study.  2. MODELS AND CASE STUDY This section details the three models that power the sep-  arate components of the proposed pipeline as well as a case study where we applied the pipeline to data from social learning. The goal of the case study is to investigate the pos- itive eects that goal-setting students have on their peers, to measure the extent to which students are benefiting from these peer eects, and to oer a proof of concept of our recommendation system for increasing the extent to which these valuables connections between peers are made.  2.1 Course Context The data used in the case study was collected from the  ProSolo learning platform [2]. The course was oered on edX with the title Data, Analytics, and Learning (DALMOOC) from October to December 2014. In this dual layer MOOC, students had the option of choosing a more standard path through the course within the edX platform or to follow a more self-directed and social path in the external environ- ment ProSolo. The ProSolo layer encouraged students (1) to set their learning goals, (2) to write posts in the discussion forum on ProSolo or link their external posts on their own blogs and Twitter, and (3) to follow other students, so that they can easily access their followees activities and posts.  2.2 Data InfrastructureDiscourseDB Model: The significance of the data infrastructure proposed here, referred to as DiscourseDB1, lies in its ability to map diverse forms of discussion into a common representation, making it easy to apply analytic tools to dierent types of so- cial interactions across multiple platforms. Hence, analytic and intervention models can be applied with little change to any data once imported into DiscourseDB. The struc- ture of DiscourseDB is represented in a relational database as an entity-relation model of connected discourse contribu- tions organized in generic, nested discourse containers. For content information, DiscourseDB stores individual posts or messages into the contribution table, along with their past  1 http://discoursedb.github.io    revisions and relations between them, which can be arbitrar- ily typed (e.g., reply-of). User information is stored in the user table, along with information about social relationships between users and users contributions. Case study: We import the ProSolo data into DiscourseDB. Students individual goal notes, forum posts, blog posts, and tweets are mapped to contributions in DiscourseDB. Ev- ery contribution has the information about its source plat- form. The reply relations between posts are stored in dis- course relation. Students are mapped to the user table, and their follow relations are stored in the user relation table.  2.3 Learning AnalyticsSequence Model Model: We model students learning paths such that the building blocks of learning paths are induced from the data. This approach may find more representative units of student interests than predefined building blocks. For this purpose, we propose an extension of the previously published State Transition Topic Model (STTM) [1], in order to infer learn- ing paths from student behavior traces in a course. STTM is a combination of a Hidden Markov Model and Latent Dirich- let Allocation, where each state is represented as a topic distribution. Therefore, STTM can learn topics students are interested in within each state, and estimate a students state at each time point. STTM also learns the probabil- ities of state transitions, which reflect students progress. However, the original published STTM model is incapable of investigating how learning paths dier depending on the students social status, e.g., the existence or lack of connec- tion with certain peers. Therefore, we extended STTM to learn dierent transition probabilities for students depend- ing on their assigned status as well as the trend of students engagement in course topics and media in each state. Case study: We apply the model to the imported ProSolo data to analyze students learning paths conditioned on their social relationships. We are specifically interested in stu- dents who set goals using goal notes and in examining whether students who follow them have dierent learning paths from the others. The learning paths will show how such social connections make a dierence in the selection of course top- ics and social media the students choose. Our case study revealed that students following goal-setters show more in- terest in hands-on practice and subjects in the later part of the course in comparison to other students. Their transi- tions between states imply that they are more likely to link course materials learned across the course as well.  2.4 InterventionRecommendation System Model: Once we have identified patterns that distinguish successful and unsuccessful student paths, we may want to introduce interventions that we believe will increase the prevalence of successful paths. In the current work, that involves supporting students in adding connections to posi- tive role models in their network. In many learning environ- ments, discussions are the main means of social interaction among students. Our specific work proceeded by first assess- ing the extent to which students benefited from specific so- cial connections based on analysis of goal-setting behavior, and then by proposing a social recommendation approach that would enable students to find opportunities to add such connections. For this purpose, we first investigated sensitiv- ity on the part of students to identify eective role models to connect to naturally. The investigation was conducted  through link prediction by a social recommendation system that extends the matrix factorization model developed by Yang et al.[4]. The link prediction involved predicting con- nections students make through post-reply actions on dis- cussion threads. Incorporation of the goal-setting behavior of students in the model did not help this prediction task. That suggests that the students do not demonstrate a sen- sitivity to peer students goal-setting behavior while making connections. In order to help students connect to positive role models in their network, we extended the matrix factor- ization model, which already recommends connections as per preferences made by the students in the past, by introduc- ing a constraint which makes connections with positive role models (students having good goal-setting behavior) manda- tory. The model first generates recommendations which are relevant for students and then filters out some based on the constraint imposed. Case study: Our analytics have identified a positive ef- fect associated with social connection with goal-setting stu- dents. Thus, we are looking forward to opportunities to introduce interventions that would introduce more of these connections into the experiences of students. For example, we might provide students with opportunities to interact more frequently with goal-setters through discussion in the forums and through other social aordances by means of so- cial recommendation. Our model has been demonstrated to recommend learning partners that are not only relevant to a students preferences but are also qualified role models.  3. CONCLUSION We proposed a pipeline for expediting the process of mov-  ing from learning analytics to student support by using com- putational modelling approaches at every step in the pipeline. The concrete example of applying the pipeline to a case study and the result of the case study show the potential of the pipeline for various studies about social learning. We believe that social learning can benefit from technologies, such as discussion recommendation, conversational agents, and collaboration tools in MOOCs2.  4. ACKNOWLEDGEMENT This research was supported by the National Science Foun-  dation under grants ACI-1443068 and IIS-1320064, and by the Naval Research Laboratory and Google.  5. REFERENCES [1] Y. Jo and C. P. Rose. Time Series Analysis of Nursing  Notes for Mortality Prediction via a State Transition Topic Model. In CIKM 15, 2015.  [2] C. P. Rose, O. Ferschke, G. Tomar, D. Yang, I. Howley, V. Aleven, G. Siemens, M. Crosslin, D. Gasevic, and R. Baker. Challenges and Opportu- nities of Dual-Layer MOOCs: Reflections from an edX Deployment Study. In CSCL 15, pages 848851, 2015.  [3] C. Thille. Education Technology as a Transformational Innovation. White House Summit on Community Colleges: Conference Papers, pages 7378, 2010.  [4] D. Yang, D. Adamson, and C. P. Rose. Question recommendation with constraints for massive open online courses. In RecSys 14, pages 4956, 2014.  2 http://dance.cs.cmu.edu/resources/    "}
{"index":{"_id":"88"}}
{"datatype":"inproceedings","key":"Berg:2016:DXE:2883851.2883968","author":"Berg, Alan and Scheffel, Maren and Drachsler, Hendrik and Ternier, Stefaan and Specht, Marcus","title":"The Dutch xAPI Experience","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"544--545","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883968","doi":"10.1145/2883851.2883968","acmid":"2883968","publisher":"ACM","address":"New York, NY, USA","keywords":"data silos, data standardization, learning analytics, learning record store, xAPI","abstract":"We present the collected experiences since 2012 of the Dutch Special Interest Group (SIG) for Learning Analytics in the application of the xAPI standard. We have been experimenting and exchanging best practices around the application of xAPI in various contexts. The practices include different design patterns centered around Learning Record Stores. We present three projects that apply xAPI in very different ways and publish a consistent set of xAPI recipes.","pdf":"The Dutch xAPI Experience  Alan Berg Universiteit van Amsterdam  Weesperzijde 190 1097 DZ Amsterdam,  The Netherlands a.m.berg@uva.nl  Maren Scheffel Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, The Netherlands  maren.scheffel@ou.nl  Hendrik Drachsler Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, The Netherlands  hendrik.drachsler@ou.nl Stefaan Ternier Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, The Netherlands  stefaan.ternier@ou.nl  Marcus Specht Open Universiteit  Valkenburgerweg 177 6419 AT Heerlen, The Netherlands  marcus.specht@ou.nl  ABSTRACT We present the collected experiences since 2012 of the Dutch Special Interest Group (SIG) for Learning Analytics in the application of the xAPI standard. We have been experi- menting and exchanging best practices around the appli- cation of xAPI in various contexts. The practices include dierent design patterns centered around Learning Record Stores. We present three projects that apply xAPI in very dierent ways and publish a consistent set of xAPI recipes.  CCS Concepts Information systems ! Data management systems; Information storage systems; Information systems  applications; Applied computing ! Education;  Keywords learning analytics, xAPI, learning record store, data stan- dardization, data silos  1. INTRODUCTION We introduce briefly three xAPI-powered learning ana-  lytics research projects that are supported by members of the SURF SIG on Learning Analytics. These projects are UvAInform, ECO and Learning Pulse. We describe the main benefits and disadvantages of xAPI and address why it is important to provide an authoritative set of recipes. Finally, we publish the recipes used within our projects to support consistent application.  The Experience API (xAPI) formerly known as TinCan API was publicly launched in April 2012. The standard is stable, there have been no significant updates to the spec-  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). LAK 16 April 25-29, 2016, Edinburgh, United Kingdom  c 2016 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-4190-5/16/04. DOI: http://dx.doi.org/10.1145/2883851.2883968  ification since 2014 and it is increasingly being adopted1. Since 2014 we have seen numerous projects and initiatives in Europe that apply the xAPI specification as a metadata approach to securely aggregate learning events ready for di- gestion by Learning Record Stores and analytics engines. A bandwagon of xAPI showcases and systems is starting to roll in Europe as an increasing number of educational insti- tutions harvest structured and consistent data. A motivator for this is that xAPI delivers three very innovative aspects that are appealing to digital education providers in the 21st century: The xAPI approach is (1) learner activity centered, (2) system independent, and (3) straightforward to imple- ment.  2. EUROPEAN XAPI PROJECTS In 2012, the first project that applied the xAPI approach  was UvAInform at the University of Amsterdam (UvA). The second was the European project ECO. This project has played a central role by developing a complete set of xAPI statements for all activities a learner can interact with within a traditional MOOC or other distance education courses. We finish the overview with the LACE Learning Pulse study that applies xAPI methodology to biofeedback data from wearable devices. The last two projects are both running at the Open University of the Netherlands (OUNL).  In June 2012 UvA initiated a stimulus project for learning analytics known as the UvAInform project. The project in- cluded seven pilots mostly centered on dashboard building and a generic infrastructure component, a UvA-developed LRS named Larissa2. From 2012 to 2014, the central ser- vices of UvA invested in instrumenting open source xAPI connectors for Sakai3 to accelerate and experiment with the use of learning activity data and the Apereo Open Academic Environment (OAE)4. The aim was to generate wider adop- tion by researchers. Researcher involvement was seen as a key factor in understanding and developing learning analytic services and was driven by two contradictory perceptions by  1https://github.com/adlnet/xAPI-Spec 2https://github.com/Apereo-Learning-Analytics- Initiative/Larissa 3https://confluence.sakaiproject.org/display/TINCAN/Home 4http://oaeproject.org    decision makers: (1) learning analytics had the potential to improve services across a spectrum of stakeholders and (2) the lack of hard evidence in 2012 for the impact on learning analytics within the Dutch context.  In early 2014, OUNL received European funding to de- velop a learning analytics infrastructure for the ECO project5. ECO is developing a single entry portal for various MOOC providers. It contributes to increasing awareness of the ad- vantages of open online education in Europe and to develop shared technologies for the dierent MOOC providers [1]. The ECO project comprises a set of learning platforms that already have their own logging and monitoring system. Each platform can use its proprietary methodology as long as it also provides the required data according to the xAPI spec- ification. Therefore, an LRS architecture with xAPI state- ments has been established that allows the calculation of learning analytics indicators for each involved platform.  Within another European project called LACE6, OUNL and their partners collect and visualize evidences to support learning analytics best practices for K12, workplace learn- ing, and the higher education sector in Europe among other objectives. Within the LACE project, OUNL conducts ex- perimental studies focused on educational evaluation of ad- vanced analytics tools [5]. Among mobile learning analytics they are working with BioFeedback and environmental data to identify conditions for productive and unproductive learn- ing contexts. The Learning Pulse study stores data from four dierent sources such as (1) RescueTime7, a tracking tool that analyzes the tools used on a PC and applies a pro- ductivity score, (2) the heart rate of the learners measured through wearable FitBit8 devices, (3) weather data through open data weather services and, (4) user ratings about their own past activities.  3. TOWARDS XAPI CONSISTENCY The most challenging issue for xAPI is the freedom of  choice when designing xAPI statements. Anyone can on de- mand define statements and related vocabulary. This will work for an isolated solution, however, this approach gener- ates considerable issues once the barriers between data silos are broken down and xAPI datasets are combined. The interoperability issue is not a new one and has been de- scribed long before the xAPI approach for other standards such as IMS LD [3] and SCORM [4]. Nevertheless, the call for a more standardized approach to collecting data that increases the insights one gains from standardized data is still valid and becomes even more urgent with the learner activity-based data collection.  Several contemporary sources of xAPI recipes exist, the primary library is advertised on the ADLnet website9. How- ever, as of October 2015, the documented recipes are limited in extent to a number of contexts (attendance, bookmarklet, checklist, open badges, scorm to tincan, tags, video, virtual patient activities). A secondary set of recipes that expand coverage to initially support cMOOCs [2] are stored in a Github location10. Although these sources are suggestive  5https://ecolearning.eu 6http://www.laceproject.eu 7https://www.rescuetime.com 8https://www.fitbit.com 9http://tincanapi.com/recipes/  10https://github.com/kirstykitto/CLRecipe  and act as sources of guidance there is currently no clearly authoritative of one source of truth. The lack of authorita- tive guidance in selecting verbs and others metadata terms generates a huge inconsistency between single statements be- tween providers. For instance, interaction of a learner with a video could be tracked as: Learner A played the movie How to cook good xAPI versus Learner B watched the video How to cook good xAPI. Both statements express the same experience in slightly dierent semantic manners. There- fore, xAPI promotes the use of recipes to standardize the expression of experiences, because there are multiple plau- sible paths to defining that a learner has interacted with an object. xAPI thus relies on the educational community to publicize and deliver standards for these recipes.  UvAInform, ECO and LACEs Learning Pulse have cov- ered a wide range of learner interactions. All three have thus published their underlying xAPI statements. These recipes can be found in a publicly shared Google document. The overview of xAPI statements is available in two ways: (1) a registry of the complete statements in JSON format11  and (2) a spreadsheet with the most important information needed for each statement12, i.e., a more user-friendly and readable version of the same content. It describes the activ- ity, names the specific action and lists the verbs and types of objects to be used. For each statement it also provides a link to the respective JSON statement in the registry document. These recipes, if incorporated into a defacto standard, will significantly increase the range of recipes and thus support recipe standardization as it is the authors great wish to be part of an orchestrated process that delivers one authorita- tive source of xAPI recipes.  4. ACKNOWLEDGMENTS The eorts of M. Scheel, H. Drachsler and S. Ternier have  partly been funded by the ECO project (grant no. 21127) and the LACE project (grant no. 619424).  5. REFERENCES [1] F. Brouns, J. Mota, L. Morgado, D. Jansen, S. Fano,  A. Silva, and A. Teixeira. A networked learning framework for eective mooc design: the eco project approach. In Proc. 8th EDEN Research Workshop, pages 161171, Budapest, Hungary, 2014. EDEN.  [2] K. Kitto, S. Cross, Z. Waters, and M. Lupton. Learning analytics beyond the lms: The connected learning analytics toolkit. In Proc. of the Fifth Int. Conf. on Learning Analytics And Knowledge, LAK15, pages 1115, New York, NY, USA, 2015. ACM.  [3] R. Koper and B. Olivier. Representing the learning design of units of learning. Educational Technology & Society,, 7(3):97111, 2004.  [4] C. Qu and W. Nejdl. Towards interoperability and reusability of learning resource: a scorm-conformant courseware for computer science education. In Proc. of the 2nd IEEE Int. Conf. on Advanced Learning Technologies (IEEE ICALT 2002), 2002.  [5] B. Tabuenca, M. Kalz, H. Drachsler, and M. Specht. Time will tell: The role of mobile learning analytics in self-regulated learning. Journal of Computers & Education,, 89:5374, 2015.  11http://bit.ly/DutchXAPIreg 12http://bit.ly/DutchXAPIspread    "}
{"index":{"_id":"89"}}
{"datatype":"inproceedings","key":"Milligan:2016:VFC:2883851.2883956","author":"Milligan, Sandra and He, Jiazhen and Bailey, James and Zhang, Rui and Rubinstein, Benjamin I. P","title":"Validity: A Framework for Cross-disciplinary Collaboration in Mining Indicators of Learning from MOOC Forums","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"546--547","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883956","doi":"10.1145/2883851.2883956","acmid":"2883956","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOC, collaborative learning, crowd-sourced learning, learner performance, learning analytics, learning progression, measurement theory, non-negative matrix factorisation, rasch analysis, topic modelling, validity","abstract":"Two research teams from the University of Melbourne's Learning Analytics Research Group used validation as applied in educational measurement to provide a framework for collaboration. One team was focussed on defining and building measures of learning capability of MOOCs participants, and the other on using topic modelling to discover topics in MOOC forums. The collaboration explored the suitability of items discovered from MOOC forums using topic modelling as measures of learning capability of participants in MOOCs.","pdf":"Validity: a framework for cross-disciplinary collaboration  in mining indicators of learning from MOOC forums      Sandra Milligan  University of Melbourne   Melbourne, Australia  s.milligan@unimelb.edu.au   Jiazhen He   University of Melbourne   Melbourne, Australia  jiazhen@student.unimelb.edu.au   James Bailey  University of Melbourne   Melbourne, Australia  baileyj@unimelb.edu.au     Rui Zhang   University of Melbourne  Melbourne, Australia   rui.zhang@unimelb.edu.au   Benjamin I.P Rubinstein  University of Melbourne   Melbourne, Australia  Benjamin.rubinstein@unimelb.edu.au     ABSTRACT  Two research teams from the University of Melbourne's  Learning Analytics Research Group used validation as applied  in educational measurement to provide a framework for  collaboration. One team was focussed on defining and building  measures of learning capability of MOOCs participants, and  the other on using topic modelling to discover topics in MOOC  forums. The collaboration explored the suitability of items  discovered from MOOC forums using topic modelling as  measures of learning capability of participants in MOOCs.     Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Computer-assisted  learning   Keywords  Validity; MOOC, Topic Modelling, Non-Negative Matrix  Factorisation, Measurement Theory, Learning Analytics, Rasch  Analysis, Crowd-Sourced Learning, Collaborative Learning,  Learning Progression, Learner Performance     1. BACKGROUND  The University of Melbourne Learning Analytics Research  Group (LARG) supports interdisciplinary research using large  data sets relating to staff and student interactions in eLearning  environments.    An educational measurement team in LARG was focussed on  improving visibility of the learning process in MOOCs, and  had defined and measured a 21st-Century skill  the capability  to crowd-source higher order learning in MOOCs. It had  established empirically verified measures of this capability (the  Crowd-sourced Learning Scale, C-SL) [1] to infer the degree to  which any MOOC participant possessed it. The items used to  construct the assessments were coded from the digital traces of  learner behaviour in the log stream, an expensive process.     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first  page. Copyrights for third-party components of this work must be  honored. For all other uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04  DOI:  http://dx.doi.org/10.1145/2883851.2883956         Automated item mining was considered preferable, especially  in indicating the quality of what is said in forums.    A computer science team in LARG was working on the  automatic discovery of topics from MOOC discussion forums,  and seeking to use them as items that indicate educationally  meaningful attainment. This was achieved by using machine  learning topic modelling algorithms to automatically assess for  each student the degree to which the latent topics were  represented in their posts and comments in forums.  Furthermore, a standard topic modelling algorithm, Non- Negative Matrix Factorisation (NMF) [2] was adapted to  require the discovered topics to conform to a Guttman scale  [3], and the students' final grades were used to model the  students' topic response patterns [6].    Key questions for the collaboration were: is there a quicker,  cheaper and easier way than hand-mining to generate reliable,  quality items from the log stream of sufficient quality to use in  assessment of learning Could topic analysis provide an  option Are the discovered topics meaningful and useful for  teachers or other educators Could the C-SL project's  handcrafted items be used as the benchmarks of utility and  meaning for the topic analysis output    These questions were explored using posts and comments  taken from a Coursera MOOC for practicing teaching run by  the University of Melbourne in 2014, entitled Assessment and  Teaching of 21st Century Skills (ATC21S MOOC), for which  the hand crafted items and measure for each participant on the  C-SL scale were also available. Topic modelling was used to  generate indicators for each of 1729 participant for each of 10  topics. The indicator was the likelihood that the topic had been  addressed in the individuals posts and comments. Each topic  was described by its ten most frequent roots. This likelihood  was then coded to form items suitable for use in a scale.    Methodologically, the collaboration was cast as a validity  investigation aimed at developing an argument to support a  judgment of the degree to which the metrics developed by the  topic modelling are plausibly interpreted as measuring the  capability of a participant to generate higher order learning in a  MOOC.    2. APPROACH TO VALIDATION   Modern day conceptions define validity as a judgement of the  degree to which empirical and theoretical argument supports  the utility and interpretation of a metric [4, 5]. An argument,  based on evidence from investigation, is needed which  specifies the inferences and supporting assumptions needed to  get from data to score-based interpretations and uses.  Investigations cannot confirm validity but they can identify  areas of weakness in the argument. There is no generally   mailto:rui.zhang@unimelb.edu.au http://dx.doi.org/10.1145/2883851.2883956   accepted way to investigate validity or to mount an argument  for validity. Validity is a unitary concept, incorporating a range  of formerly separate ideas such as those captured in the terms  predictive validity, concurrent validity, criterion validity  or reliability and so on. Nor is validity a yes/no judgement: it  is a matter of degree. What is required is an on-balance  judgment, based on the evidence and combined with openness  to further evidence and improvements over time. Alternatively  it should not become a never-ending process of presenting a  miscellany of investigations. It should focus on priorities, on  what is genuinely in doubt from the perspective of stakeholder  audiences, and on investigations that will yield information, for  reasonable cost, on the key risks for users.    3. THE INVESTIGATIONS  A series of validity investigations were conducted, and the  questions focusing each are described below.   Questions were asked about rational and need for the measures,  including: Is there a plausible theoretically based educational  rationale for expecting the automatically derived topic items to  have utility and to be interpretable for educators Is practical  educational significance and meaning clear Evidence drew  from the literature in the learning sciences of the relationship  between topics discussed by peers in forums and learning  outcome in MOOCs.    Questions were asked about the appropriateness of data use,  including: Is the transformation of raw data into measures  transparent, meaningful and plausible Evidence in this  investigation included assessments by teaching staff of the  degree to which the topics retained meaning from the forum  texts through the extensive prepossessing (involving text  aggregation, stemming, deletion of stop words and html tags,  creation of a word-student matrix), and through application of  the statistical modelling and item coding. It included  assessment by teaching staff of the meaningfulness of the topic  outputs, including the degree to which they were reflective of  the discourse in the course, relevant to learning (and not, for  example, tapping social rather that educational dimensions of  student discussion), and reflective of how a teacher might  assess text written by students to improve teaching practice.   Questions were asked about psychometric strength, including:  Were the automatically discovered topic items of sufficient  quality, stability, replicability and generalisability to use as  indictors of learning capability Were they biased for  participant not fluent in English Evidence in this investigation  included tests of the topic items (when conjoined with hand  crafted items within the C-SL scale), using the criterion of fit to  a Rasch measurement model [7].    A question about interpretability explored alternate  interpretations of topic item meaning. Evidence for this  investigation included investigations of the predicted  relationship of the topic modelling items for each participant  with other related measures.   A further question explored whether there were better methods  for achieving the same ends. This involved review of alternate  way of measuring quality in forum posts, and related questions  of coder bias, cost, objectivity and reliability.   4. THE JUDGEMENT  On the basis of these investigations, the details of which it is  not possible to report here, a judgment was made that, with  some qualifications, the topic modelling supported automatic  generation of items that have utility in assessing learner  capability to generate higher order learning. The method was  found to be efficient and objective, resulting in items slightly  less reliable that the hand-coded items, but of sufficient   psychometric quality, and interpretable and valued by teaching  staff. No bias was detectable for MOOC participants with  different levels of English language fluency. As a group most   of the items worked well in the C-SL scale to standards  required by fit to a parsimonious measurement model.    Some qualifications on interpretation and use were identified.  Not all automatically generated topic items demonstrated  suitability to purpose: three of the 10 topics using the adapted  NMF approach did not fit to measurement modelling and were  judged unsuitable for use. None of the topic items were  suitable for use on their own: their power to measure learning  derives from integration with other items tapping other  dimension of forum activity. The topic items fitted into the C- SL scale, but did not improve its reliability. Closer examination  suggested that the misfit arose from the tight relationship built  into the topic modelling between topics discovered, and grade  on the course. The CSL scale was crafted to discriminate on  people's capability to learn, rather than their participation in  grading. The findings suggest how the automatic generation of  items can be used in assessment of capability to learn and how  the items can be interpreted, and identified further areas for  research.    5. CONCLUSION  Methodologically, validation theory provided an appropriate  framework for the cross-disciplinary investigations. In the era  of big data, when analytics generates a range of metrics,  intended for teacher and student consumption, there are risks to  student and teachers arising from use of an un-replicable,  unreliable, or irrelevant metrics. Educators should retain  scepticism about seemingly attractive new metrics: data do not  speak for themselves; it is easy to detect patterns, report  findings and impute meaning to what is found; to conflate  complex relationships such as causality or meaningfulness with  statistical indictors such as correlations or tests of significance.  Unless models and interpretations found in data are tested  and continue to pass tests of falsification, it is unwise to accept  them at face value. Validation theory provides a useful  conceptual framework for analytics project that aspire to  having utility for learning.   REFERENCES   [1] Milligan, S. K. 2015. Crowd-sourced learning in MOOCs:   learning analytics meets measurement theory. In  Proceedings of the Learning Analytics and Knowledge  Conference (Poughkeepsie, NY, USA, 2015). ACM,. 2015,  151-155.   [2] Lee, D. D. and Seung, H. S. 1999. Learning the parts of  objects by non-negative matrix factorization, Nature, 401,  788-791.   [3] L. Guttman, 1950. The basis for scalogram analysis, in  measurement and prediction: The American Soldier, S.  Stouffer, Ed. Wiley, New York.   [4] Messick, S. 1993. Foundations of Validity: Meaning and  Consequences in Psychological Assessment, Educational  Testing Service, Princeton, New Jersey.   [5] Kane,. M. T. 2013. Validating the interpretations and uses  of test scores. Journal of Educational Measurement, 50, 1,  1-73.   [6] J. He, J., Rubinstein, B.I., Bailey, J., Zhang, R.,  Milligan,  S., and Chan, J. 2016.  MOOCs meet measurement theory:  A topic-modelling approach, Proceedings of the 30th AAAI  Conference on Artificial Intelligence, Austin, USA,      [7] Masters, G. N. 1982. A Rasch model for partial credit  scoring. Psychometrica, 471, 149-174.     "}
{"index":{"_id":"90"}}
{"datatype":"inproceedings","key":"Kitto:2016:CLA:2883851.2883881","author":"Kitto, Kirsty and Bakharia, Aneesha and Lupton, Mandy and Mallet, Dann and Banks, John and Bruza, Peter and Pardo, Abelardo and Shum, Simon Buckingham and Dawson, Shane and Gavsevi'c, Dragan and Siemens, George and Lynch, Grace","title":"The Connected Learning Analytics Toolkit","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"548--549","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883881","doi":"10.1145/2883851.2883881","acmid":"2883881","publisher":"ACM","address":"New York, NY, USA","keywords":"dashboards, sensemaking, social learning analytics","abstract":"This demonstration introduces the Connected Learning Analytics (CLA) Toolkit. The CLA toolkit harvests data about student participation in specified learning activities across standard social media environments, and presents information about the nature and quality of the learning interactions.","pdf":"Recipe for Success  Lessons Learnt from Using xAPI  within the Connected Learning Analytics Toolkit  Aneesha Bakharia Information Systems School  Queensland University of Technology  Brisbane, Australia aneesha.bakharia@qut.edu.au  Kirsty Kitto Information Systems School  Queensland University of Technology  Brisbane, Australia kirsty.kitto@qut.edu.au  Abelardo Pardo School of Electrical and Information Engineering The University of Sydney  Sydney, Australia abelardo.pardo@sydney.edu.au  Dragan Gaevic Moray House School of Education and School of  Informatics University of Edinburgh  Edinburgh, Scotland dgasevic@acm.org  Shane Dawson Teaching Innovation Unit  University of South Australia Adelaide, Australia  Shane.Dawson@unisa.edu.au  ABSTRACT An ongoing challenge for Learning Analytics research has been the scalable derivation of user interaction data from multiple technologies. The complexities associated with this challenge are increasing as educators embrace an ever grow- ing number of social and content-related technologies. The Experience API (xAPI) alongside the development of user specific record stores has been touted as a means to ad- dress this challenge, but a number of subtle considerations must be made when using xAPI in Learning Analytics. This paper provides a general overview to the complexities and challenges of using xAPI in a general systemic analytics solution - called the Connected Learning Analytics (CLA) toolkit. The importance of design is emphasised, as is the notion of common vocabularies and xAPI Recipes. Early decisions about vocabularies and structural relationships be- tween statements can serve to either facilitate or handicap later analytics solutions. The CLA toolkit case study pro- vides us with a way of examining both the strengths and the weaknesses of the current xAPI specification, and we con- clude with a proposal for how xAPI might be improved by using JSON-LD to formalise Recipes in a machine readable form.  Categories and Subject Descriptors D.2.8 [Software Engineering]: Metricscomplexity mea- sures, performance measures; D.2.10 [Software Engineer- ing]: DesignRepresentation; E.2 [Data Structures]: Data Storage RepresentationComposite structures, Linked rep-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. LAK 16, April 25-29, 2016, Edinburgh, United Kingdom  c 2016 ACM. ISBN 978-1-4503-4190-5/16/04. . . $15.00 DOI: http://dx.doi.org/10.1145/2883851.2883882  resentations, Object representation  Keywords xAPI, CLA toolkit, CLRecipe, Architecture, Learning Ana- lytics, Learning Record Store  1. INTRODUCTION Learning Analytics has evolved as a field of research that  uses data driven methods to improve student learning pro- cesses and outcomes [21]. However, the learning process is complex and influenced by a wide variety of contextual and personal factors. Researchers have suggested that the true potential to oer meaningful insight comes from combin- ing data from across dierent sources [17]. However, stu- dent learning data is commonly generated from numerous platforms, often with dierent underlying data structures. Hence, establishing a combined data set can be a challeng- ing task. For example, a video annotation platform may be able to provide detailed accounts of its specific events (e.g., play, pause, stop, annotate, and comment), but when this data is combined with another dataset extracted from a social network platform the intersection in vocabulary may not overlap or be consistent. If these two data sets are to be used in the context of a learning experience, their terms, objects and actions need to be reconciled into a common notation, often a time consuming and dicult task.  The challenges involved in collating and analysing dis- tributed learning events are well documented in the Learn- ing Analytics literature, and numerous data formats have been proposed as potential solutions, including: Contextu- alised Attention Metadata [19], learning context ontologies (LOCO framework) [13], and ontologies for organizational learning (IntelLEO framework) [20]; and the very recently (October 2015) released IMS Caliper [3].  http://dx.doi.org/10.1145/2883851.2883882   1.1 Experience API The Experience API (xAPI), provides a platform-neutral  formalism to collect events occurring in any learning experi- ence. xAPI was released in 2013 as the outcome of an ADL project that aimed to both: (i) improve interoperability be- tween elearning systems that collect and exchange student learning data, and (ii) overcome the limitations of SCORM [2]. The xAPI specification [4] describes the format to rep- resent discrete learning activities (as JSON statements) and the requirements for Learning Record Stores (LRS) that are able to collate and exchange learner records. The xAPI statement data format is based on WC3 Activity Streams 1.0 [1] with notable changes made to include results and context for a learning activity [8]. The design of xAPI has been influenced by the socio-cultural framework of Activity Theory [22] with the unit of analysis being the activity. As Activity Theory is closely related to constructivist learning theory, Kevan and Ryan [14] suggest that xAPI is ideally suited to tracking constructivist learning activities. How- ever, the xAPI specification has no defined core vocabulary. In this case, the community is required to both define and share the structure of xAPI statements and the vocabulary as Recipes specific to a domain. These Recipes are analogous to the semantic definitions included in ontologies. Without them, xAPI only provides the syntactical structure to com- pose statements.  Example Recipes currently exist for attendance1, video interaction2 and open badges3. However, the current fo- cus in the xAPI community upon data collection tends to mean that the analytics implications of vocabulary choice are often not considered. More complex Recipes need to be defined, and we consider it essential that the Learning An- alytics (LA) community participate in this process, as poor design decisions will make it far more dicult to implement LA systems. Here, we share knowledge and lessons learned from participating in this process with the design of the Connected Learning Analytics (CLA) toolkit. We provide some of the best practice lessons that we have learned, along with guidelines for the appropriate capture and analysis of learning records.  2. EXAMPLE: THE CLA TOOLKIT The Connected Learning Analytics (CLA) toolkit [16] is  currently being created as a part of an Australian Govern- ment funded Oce for Learning and Teaching project. This project aims to collate and analyse student behaviour within defined learning activities that are run in the wild using standard social media platforms, i.e. beyond an institutions adopted Learning Management System (LMS).  The CLA toolkit is open source (GPL3.0), and imple- mented in Python, using the Django web framework. It consists of two main components:  Data Collection is achieved by interfacing with standard social media APIs to retrieve specific data about stu- dent participation in a pre-defined learning activity. This data is stored in a Learning Records Store (LRS) using the xAPI format. Full functionality is currently  1http://xapi.trainingevidencesystems.com/recipes/ attendance/0 0 1/ 2https://registry.tincanapi.com/#profile/44 3https://tincanapi.com/recipes-designers/  Figure 1: Simplified xAPI statement schema.  implemented with the Facebook, Twitter, YouTube and Wordpress platforms.  Analytics and Reporting are enabled by pulling data out of the LRS and storing it in a secondary database (presently PostgreSQL) which provides full functional- ity for querying the xAPI JSON document structure. Section 5 discusses the current reporting capabilities of the CLA toolkit, which has both student and in- structor facing dashboards.  3. DESIGNING XAPI STATEMENTS In this section, an overview of the structure of xAPI state-  ments will be presented followed by a discussion on the advantages and disadvantages that xAPI syntax flexibility brings. xAPI statements contain 3 main elements, namely metadata (i.e., id, timestamp), descriptive information (i.e., actor, verb and object) and complementary data (i.e., con- text). xAPI statements are made up of <subject>, <verb> and <object> triplets. Each verb and object in an xAPI statement requires a unique identifier that resolves to a URL that contains the required metadata although no specific schema (i.e., typing) needs to be defined.  The <subject>, <verb>, <object> triplet representation is an oversimplification of the xAPI syntax, as well as be- ing misleading in terms of the actual data required for an- alytics. Our experience in developing the CLA toolkit has shown that the correct modelling and population of contex- tual data within xAPI statements is critical. This is the context sub-section within an xAPI statement, which in- cludes the instructor, team, and other important informa- tion about the learning context. In particular, the inclusion of the platform and the ability to link the activity with a course, a course section and an instructor are all equally important from an analytics perspective and should not be neglected when designing the mandatory fields that must be populated in xAPI statements designed for Learning Ana- lytics. We consider it essential that all statements include a reference to uniquely identify the learning experience (e.g., course, event, field trip) and platform (e.g., Moodle). All statements generated by the CLA toolkit include the course code and the originating social media platform.  When comparing xAPI statements to traditional tabular log formats (e.g., Apache Server access logs or the Accumu- lator table in the Blackboard LMS), an additional advantage of xAPI emerges; the xAPI statement is in a JSON docu- ment format and is therefore able to encode multiple rela- tionships. These can be included in the grouping, parent and other sections of contextactivities. The CLA toolkit takes advantage of this functionality and is able to include multi- ple @mentions, hashtags and tags with an activity occurring on a social media platform as contextactivities.other. While the ability to include multiple objects as items in the group-  http://xapi.trainingevidencesystems.com/recipes/attendance/0_0_1/ http://xapi.trainingevidencesystems.com/recipes/attendance/0_0_1/ https://registry.tincanapi.com/#profile/44 https://tincanapi.com/recipes-designers/   ing and parent sections of contextactivities provides much more flexibility than tabular log formats, there is ambiguity as to how these relationships should be used in analytics, because multiple objects of dierent types can be included.  The extensibility of xAPI means that new formats can be defined as JSON sub-documents and incorporated into the structure of an xAPI statement. For example, the CLA toolkit uses the rating extension to include numeric ratings of social media content4. However, the ability to define new JSON constructs as extensions without the use of a man- dated JSON schema is problematic from an analytics per- spective. The provision of a JSON schema for extensions would allow the LRS and subsequent analytically process- ing code to use the information contained within the ex- tension in an automated way. There is ongoing discussion on whether JSON for Linked Data (JSON-LD) which incor- porates object and value typing should become part of the xAPI specification. We shall not discuss this point here, but will return to this question in Section 6.  4. THE IMPORTANCE OF XAPI RECIPE DESIGN  The xAPI specification initially included a core vocabu- lary but this was removed from version 0.95 onwards with ADL favouring a community driven approach to defining verbs and activities [14]. Rustici Software currently hosts a repository5 with community submitted verbs, objects and Recipes. From an analytics perspective, using a common vocabulary to represent similar activity is not just desir- able; it is a necessity given that LRSs are designed to collate xAPI statements originating from disparate systems. xAPI Recipes have been proposed to address this need [7].  As no Recipe unifying the description of learning events in social media was available, the CLA toolkit project has de- signed an open Connected Learning (CL) Recipe. CLRecipe has played a crucial role in creating a consistent data model for social media activity, and its consistency has been tested through the ecient creation of analytics and visualisations showing temporal activities, content evolution, and social network analysis. CLRecipe describes a variety of dierent learning scenarios using a unified vocabulary:  Microblogging on platforms such as Twitter and Face- book, where users only post short notes.  Content Authoring of any long text that is written by a single user (e.g. a blog post made on Wordpress).  Content Curation of a collection of artefacts (e.g., docu- ments, audio, video, images, etc.).  Table 1 contains the current mapping, which has simpli- fied aggregate analytics across social media platforms. En- forcing the mapping in the Recipe played a key role in sim- plifying the processing required to obtain social media activ- ity by platform and verb at a course and individual student level. The verb and object vocabulary used in CLRecipe were all selected from the core W3C Activity Streams 1.0 vocabulary, which was designed to provide streams of social media activity [1]. A description of each verb is available in the CLRecipe readme.MD file [15].  4http://id.tincanapi.com/extension/quality-rating 5https://registry.tincanapi.com/  While xAPI statements represent discrete social media ac- tivities, these do not occur in isolation. This is because on social media, students might interact with content created by other students (e.g., they might like and share content), or directly comment on or reply to posts to create threaded discussions. Shares, likes and replies must include a refer- ence back to the object being mentioned using contextAc- tivities.Parent, which was chosen over using contextActivi- ties.Grouping because the xAPI specification says that con- textActivities.Grouping indicates an indirect relation while contextActivities.Parent represents a direct parent-child re- lation. The inclusion of the parent id creates a reference to the statement containing the post being commented on, replied to, liked or shared. Including the parent id in the xAPI statement allows for the construction of hierarchical relationships and is used to model threaded discussions. The use of a parent id to model a tree structure in a relational database is known as the adjacency list model [9].  Recipes are very loosely defined by a textual description of the verbs, objects, extensions used. No formal schema is enforced and relationships between statements need to be manually inferred before automated analysis can be per- formed. Invariably design decisions need to be made about which elements of an xAPI statement are used and these decisions need to be known by the system performing the analysis of xAPI statements (i.e. the design decisions and rules that a Recipe serves to enforce are not described in a machine readable manner).  5. PERFORMING ANALYTICS WITH XAPI STATEMENTS  xAPI statements are stored in a Learning Record Store (LRS). The CLA toolkit uses Learning Locker, which is an open source LRS built on MongoDB (a NoSQL database). A frequent complaint about the xAPI standard concerns the limited reporting functionality of LRSs [18]. The xAPI spec- ification does not provide a RESTful interface to perform ag- gregate queries (e.g., counts of verbs and object) against the statements in a LRS. Only simple queries are allowed and all matching statements are returned in full. The inability to directly perform aggregate queries using the xAPI LRS doc- ument interface was a stumbling block for the CLA toolkit project. xAPI statements are now stored as JSON docu- ments in a PostgreSQL database where aggregate queries can easily be performed using SQL syntax. PostgreSQL has been chosen over MongoDB because of its ability to store re- lational data (required by the CLA toolkit web application) and JSON documents.  5.1 Temporal Analysis Within the CLA toolkit, temporal analysis involves ag-  gregating social media activity over time. The CLA toolkit creates graphs showing verb use (i.e., like, share, post and comment) by platform over a specified time period. In terms of processing, the star schema commonly used in traditional business intelligence (BI) applications to create high dimen- sional cubes, was used to perform aggregate counts by date. For example, a table containing dates (i.e., the date dimen- stion) is joined to a table with extracted core fields from an xAPI statement (i.e., the fact table) which is then joined to tables containing xAPI context information such as in- structor, parent and grouping (i.e., the other dimensions for    Table 1: xAPI Verb Mapping in CLRecipe.  Create Like Share Tag Rate Comment Add Facebook Post Like Share Tag - Reply - Google+ Post Like Share Tag - Reply - Twitter Tweet Favorite Retweet Hashtag - - Blog Post - - Tag Rate Comment - Pinterest Board Like Share - - - Pin YouTube Video Like Share - - Comment -  analysis). This relational design is able to facilitate dimen- sional cube creation and provide aggregates by time of day, day in week, month, and year. The inclusion of other infor- mation, such as the social media platform, a related course, and contextual information (such as a tag or @mention) pro- vides additional dimensions for analysis. While these addi- tional fields are often seen as optional in an xAPI statement, CLRecipe mandates their inclusion for the purpose of adding further meaning and insight into the analysis.  5.2 Content Analysis CLA toolkit includes algorithms for content analysis. At  present we have implemented: Topic Modelling using the Latent Dirichlet Allocation algorithm; sentiment analysis via the Valence Aware Dictionary and sEntiment Reasoner (VADER) algorithm [12]; and a Cognitive Presence classifi- cation from the Community of Inquiry model [11]. None of these analyses can be performed without access to the con- tent associated with an xAPI entry. The xAPI specification only mandates that a unique URL for each object involved in the activity is provided, and not that it is accessible by the LRS or the system processing the xAPI statements. As such, if the LRS does not store the content from xAPI statements that originate in a firewalled system, then this functionality will not be available in the CLA toolkit. For this reason, storing the content of a social media items is recommended by the CLRecipe, although care should be taken to meet the legal terms and conditions of dierent social media.  5.3 Social Network Analysis The CLA toolkit can perform SNA and displays sociograms  that are filterable by platform and date at both a course and individual level. Forum replies, blog comment threads, @mentions, likes and shares are all stored as social relation- ships between the users performing the activities. In the CLRecipe, we refer to the post being shared, liked or com- mented on, using a parent id. This addition is essential as it creates a relationship with the posts statement and allows data such as the creators details to accessed and analysed. A social relationship table is built in the CLA toolkit which contains the post creator, creation date, platform and verb. This allows for a social network to be filtered by platform, date, and user, as well as to include dierent edge relation- ships (i.e., like, share, comment and mention). In Section 4, the use of an adjacency list model to represent the parent- child relationship between statements using a parent id was discussed. The adjacency list model however requires re- cursive queries in order to rebuild the hierarchical tree from individual statements. A few other more ecient techniques have been proposed, such as the nested set model, which will be reviewed for representation within xAPI syntax as the de- velopment of the CLA toolkit progresses. This model would  facilitate far more ecient network reconstruction, as the ability to process threaded discussions is essential.  6. IMPROVING XAPI While the xAPI statement specification is both flexible  and extensible, within this paper, we have illustrated sev- eral shortcomings in relation to the way Recipes are cur- rently described. These include a lack of strict typing (see Section 3) in extensions and Recipes as well as the lack of a machine readable way to communicate the relationship be- tween statements (see Section 4). As the xAPI Data Inter- operability Standards Consortium (DISC) forms [23], we see a chance emerging to start thinking about how xAPI might be improved, and propose that this could be done with an extension of the notion of Recipes. We propose the adop- tion of JSON-LD [6] as a solution to these issues, and here we will discuss the manner in which JSON-LD introduces stricter typing and how JSON-LD framing [5] can make the relationship between xAPI statements in a Recipe explicit and therefore machine readable.  In xAPI statements, each verb and object must include the identifier for the metadata describing the main properties. However, the metadata that is required by xAPI does not include data type information and this becomes problem- atic for non-trivial statements that use extensions and/or are part of larger Recipes. JSON-LD uses the @context property (not to be confused with the context in xAPI state- ments) to specify a URI with details on each property and its associated data type. JSON-LD also includes specific object types (e.g., Person, Place and Event) and data types (e.g., date, temperature, coordinates and floating point numbers) which would be beneficial from a xAPI statement process- ing point of view. The schema that JSON-LD provides will have similar advantages to XML schema in terms of valida- tion and compliance.  {   @context : {   as :  http :// www.w3.org/ns/  activitystreams ,   ex :  http :// example.org/vocab#   },   @type :  as:Blog ,   ex:contains : {   @type :  as:Like   }  }  Figure 2: A Frame for a Activity Stream.  The example JSON-LD frame in Figure 2 can be pro-    grammatically applied to give structure to a collection of statements that adhere to a Recipe. The frame ensures that the structure is predictable and that there is only one way for the programming code to be implemented even though the relationship between xAPI statements can take multi- ple forms. Combining the stricter typing of the JSON-LD with frames for removing the ambiguity in the encoding of statement relationships, we gain a useful way of sharing ma- chine readable xAPI Recipes. JSON-LD can also easily be translated into RDF which opens up opportunities for linked semantic student knowledge graph processing [10].  7. CONCLUSION The modelling of xAPI statements explored in this paper  is based on our experiences in building learning analytics and visualisations for the CLA toolkit. Key to the cur- rent success in our project has been the careful attention paid to creating a Connected Learning Recipe. This con- sideration has facilitated the easy creation of a variety of reports common to standard Learning Analytics solutions. The key take away is that while xAPI is flexible and ex- tensible, it is essential that analytics be considered when modelling xAPI statements using Recipes. In particular, we have found adding contextual information (which is usually seen as optional extra) is key to the provision of additional dimensions for temporal analysis. In terms of social network analysis and discourse analysis, attention needs to be given to the way relationships between statements are modelled, particularly for threaded discussions. While we have empha- sised the importance of considering analytics in the creation of Recipes, the lack of machine readable Recipes is a core weakness inherent in the current specification of xAPI. For this reason we have proposed that xAPI be extended with the JSON-LD framework that has already been adopted by the Activity Streams 2.0 specification.  8. ACKNOWLEDGEMENTS Support for this project has been provided by the Aus-  tralian Government Oce for Learning and Teaching. The views in this project do not necessarily reflect the views of the Australian Government Oce for Learning and Teach- ing.  9. REFERENCES [1] Activity streams. http://activitystrea.ms/. Accessed:  2015-10-12. [2] ADL Initiative, Project TinCan.  http://www.adlnet.gov/tla/tin-can. Accessed: 2015-10-12.  [3] Caliper Analytics. http://www.imsglobal.org/activity/caliperram. Accessed: 2015-10-29.  [4] Experience API. https://github.com/adlnet/ -xAPI-Spec/blob/master/xAPI.md. Accessed: 2015-10-12.  [5] Json-ld framing specification. http://json-ld.org/spec/latest/json-ld-framing/. Accessed: 2015-10-23.  [6] Json-ld specification. http://json-ld.org/spec/. Accessed: 2015-10-23.  [7] Recipes. https://tincanapi.com/recipes-designers/. Accessed: 2015-10-23.  [8] M. Bowe. Tin Can vs. Activity Streams. http://tincanapi.com/tin-can-vs-activity-streams/, 2013. Accessed: 2015-10-24.  [9] J. Celko. Some answers to some common questions about SQL trees and hierarchies. http: //www.ibase.ru/devinfo/DBMSTrees/sqltrees.html. Accessed: 2015-10-12.  [10] S. Dietze, S. Sanchez-Alonso, H. Ebner, H. Qing Yu, D. Giordano, I. Marenzi, and B. Pereira Nunes. Interlinking educational resources and the web of data: A survey of challenges and approaches. Program, 47(1):6091, 2013.  [11] D. R. Garrison, T. Anderson, and W. Archer. Critical thinking, cognitive presence, and computer conferencing in distance education. American Journal of distance education, 15(1):723, 2001.  [12] C. Hutto and E. Gilbert. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Eighth International AAAI Conference on Weblogs and Social Media, 2014.  [13] J. Jovanovic, D. Gasevic, C. Knight, and G. Richards. Ontologies for eective use of context in e-learning settings. Journal of Educational Technology & Society, 10(3):4759, 2007.  [14] J. M. Kevan and P. R. Ryan. Experience API: Flexible, decentralized and activity-centric data collection. Technology, Knowledge and Learning, pages 17.  [15] K. Kitto and A. Bakharia. CLRecipe. https://github.com/kirstykitto/CLRecipe, 2015.  [16] K. Kitto, S. Cross, Z. Waters, and M. Lupton. Learning analytics beyond the LMS: the connected learning analytics toolkit. In Proceedings of the Fifth International Conference on Learning Analytics And Knowledge, pages 1115. ACM, 2015.  [17] S. Knight, S. B. Shum, and K. Littleton. Epistemology, assessment, pedagogy: where learning meets analytics in the middle space. Journal of Learning Analytics, 1(1):2347, 2014.  [18] SaLTBOX. Why reporting in the LRS http://blog.saltbox.com/blog/2015/09/23/ why-reporting-in-the-lrs/, 2015.  [19] H.-C. Schmitz, M. Wolpers, U. Kirschenmann, and K. Niemann. Contextualized attention metadata. Human attention in digital environments, pages 186209, 2011.  [20] M. Siadaty, D. Gasevic, J. Jovanovic, K. Pata, N. Milikic, T. Holocher-Ertl, Z. Jeremic, L. Ali, A. Giljanovic, and M. Hatala. Self-regulated workplace learning: A pedagogical framework and semantic web-based environment. Journal of Educational Technology & Society, 15(4):7588, 2012.  [21] G. Siemens. Learning Analytics: The Emergence of a Discipline. American Behavioral Scientist, 57(10):13801400, Aug. 2013.  [22] A. Silvers. Answers: How do i get started with xAPI http://makingbetter.us/2014/11/ answers-how-do-i-get-started-with-xapi/, 2014.  [23] A. Silvers. The way of xapis consortium. xAPI Quarterly, 2015.  http://activitystrea.ms/ http://www.adlnet.gov/tla/tin-can http://www.imsglobal.org/activity/caliperram https://github.com/adlnet/-xAPI-Spec/blob/master/xAPI.md https://github.com/adlnet/-xAPI-Spec/blob/master/xAPI.md http://json-ld.org/spec/latest/json-ld-framing/ http://json-ld.org/spec/ https://tincanapi.com/recipes-designers/ http://tincanapi.com/tin-can-vs-activity-streams/ http://www.ibase.ru/devinfo/DBMSTrees/sqltrees.html http://www.ibase.ru/devinfo/DBMSTrees/sqltrees.html https://github.com/kirstykitto/CLRecipe http://blog.saltbox.com/blog/2015/09/23/why-reporting-in-the-lrs/ http://blog.saltbox.com/blog/2015/09/23/why-reporting-in-the-lrs/ http://makingbetter.us/2014/11/answers-how-do-i-get-started-with-xapi/ http://makingbetter.us/2014/11/answers-how-do-i-get-started-with-xapi/   Introduction  Experience API   Example: The CLA toolkit  Designing xAPI Statements  The Importance of xAPI Recipe Design  Performing Analytics with xAPI Statements  Temporal Analysis  Content Analysis  Social Network Analysis   Improving xAPI  Conclusion  Acknowledgements  References   "}
{"index":{"_id":"91"}}
{"datatype":"inproceedings","key":"Hu:2016:WLA:2883851.2883966","author":"Hu, Xiao and Ip, Jason and Sadaful, Koossulraj and Lui, George and Chu, Sam","title":"Wikiglass: A Learning Analytic Tool for Visualizing Collaborative Wikis of Secondary School Students","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"550--551","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883966","doi":"10.1145/2883851.2883966","acmid":"2883966","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative writing, statistics, timeline, visualization, wiki","abstract":"This demo presents Wikiglass, a learning analytic tool for visualizing the statistics and timelines of collaborative Wikis built by secondary school students during their group project in inquiry-based learning. The tool adopts a modular structure for the flexibility of reuse with different data sources. The client side is built with the Model-View-Controller framework and the AngularJS library whereas the server side manages the database and data sources. The tool is currently used by secondary teachers in Hong Kong and is undergoing evaluation and improvement.","pdf":"Wikiglass: A Learning Analytic Tool for Visualizing  Collaborative Wikis of Secondary School Students   Xiao Hu1    Jason Ip2    Koossulraj Sadaful3    George Lui3    Sam Chu1  1. Faculty of Education  2Faculty of Business and Economics 3Department of Computer Science   The University of Hong Kong   Pokfulam Road, Hong Kong   {xiaoxhu, jasonipcy, ksadaful, luifei23, samchu}@hku.hk       ABSTRACT  This demo presents Wikiglass, a learning analytic tool for  visualizing the statistics and timelines of collaborative Wikis built  by secondary school students during their group project in  inquiry-based learning. The tool adopts a modular structure for  the flexibility of reuse with different data sources. The client side  is built with the Model-View-Controller framework and the  AngularJS library whereas the server side manages the database  and data sources. The tool is currently used by secondary teachers  in Hong Kong and is undergoing evaluation and improvement.      CCS Concepts   Human-centered computingVisual analytics    Human- centered computing~Wikis    Applied computing   Collaborative learning   Keywords  Wiki; visualization; statistics; timeline; collaborative writing  1. INTRODUCTION  Wiki is widely regarded as a useful tool to facilitate project-based  learning [1], but the amount of learning evidence made available  by the tool may discourage teachers from adopting it because of  the perceived increase of workload in continuous student  assessment [2]. To tackle this problem, tools have been built to  assist teachers in processing and making sense of the large amount  of continuous student input [3]. However, few existing tools work  with student writings in Chinese and are designed for secondary  school teachers who often have heavy workload and may not be  expert computer users. In this study, we build a learning analytic  tool, Wikiglass, to help secondary school teachers in Hong Kong  and surrounding regions to monitor student collaboration and  progress in group projects using Wikis.   For this initial round of development, teachers use PBworks as the  platform for their students to collaborate on inquiry-based projects  over a five-month period. The students are required to work in  groups each consisting of about five students. Each group write  their project report on their Wiki, with different sections of the  report (e.g., Introduction, Methodology, etc.) written on separate  pages. It is expected that the clear, accurate and timely   information provided by Wikiglass can help teachers keep track of  student progress, based on which they can provide proper  interventions to needed individual and/or groups of students  during the learning process instead of after.      2. SYSTEM ARCHITECTURE  Figure 1 shows the general structure of Wikiglass which can be  divided into two layers, client and server, and four main  components: visualization, logic, Wikiglass Application Program  Interface (API) and the database.      Figure 1: System architecture of Wikiglass.   2.1 The Client Side  The client side is implemented with the AngularJS library [4] and  the Model-View-Controller (MVC) framework, which allows for  flexibility in reuse and ease of development. This architecture also  enables distributing computation load between the server and the  client so that the server can be more efficient and robust.  The client side framework consists of two layers, the Controller  and the View (with the Model in the MVC framework being the  data from the server). The View is the visualizations the users will  see, such as graphs, tables and charts. It does not concern itself  with the requests for or the actual processing of the data. The  latter functions are accomplished within the Controller. The  Controller is responsible to direct users requests, request data  from the API, process the data and render it using the View.   2.2 The Server Side  The server side is responsible for data collection and  preprocessing. Raw data from source Wikis (e.g., PBworks) are  obtained through the APIs provided by the source, including but  not limited to student user information, page content, and revision  records. Data preprocessing involves stripping unneeded tags and  extracting useful elements. A database is set up to store the  processed raw data as well as derived statistics (e.g., number of  revisions made by each student) for different purposes of analysis  and visualizations.      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883966          To maximize the flexibility of Wikiglass, we designed a generic  and easily readable API for Wikiglass. It makes the data in the  Wikiglass database accessible quickly by different client  applications. The endpoints transfer data are in the JavaScript  Object Notation (JSON) format which nowadays is a universal  format for data exchange on the Internet. The API makes it  convenient to develop alternative client sides.    3. FEATURES  3.1 Visualization  Currently Wikiglass provides two visualization modes. The  statistics mode allows teachers to compare statistics of groups in  one class or individual students in one group (Figure 2). The  statistics include revision counts, total words in the latest versions  of Wiki pages, page counts, and number of words added and  deleted by each student. For each student, Wikiglass also shows  the number of words added and deleted across different pages.     Figure 2: Statistics mode of visualization.   The timeline mode displays statistics accumulated by dates on a  weekly basis (Figure 3).  It also includes total revision counts for  each group and each student in a group as well as number of word  amendment made by each group. The timeline visualizations  allow teachers to monitor the progress of the groups or individual  students in a clear and easy manner.     Figure 3: Timeline mode of visualization.   Nearly all visualizations are clickable, facilitating teachers  navigation in the system. For example, clicking one bar  (corresponding to one group) in the class page shown in Figure 2  will lead the user to the page of that group where contributions of  group members are displayed. Clicking one pie (corresponding to  one student) in the pie chart shown in Figure 2 will lead the user  to the page of that student where his/her contributions to different   pages are visualized. Similarly, the lines in the timeline  visualization are all clickable and clicking each line leads to the  corresponding page of the corresponding group or student.      3.2 Daily update of database  As the student projects last five months, teachers need to monitor  student progress on a continuous basis. Therefore, Wikiglass  needs to update its database regularly. To fulfil this purpose, a  task scheduler in the server side is set up to retrieve all the  updated versions of the Wikis in the source side and update the  databases accordingly. The task scheduler is currently set to  update on a daily basis as it is the estimated usage frequency of  the teachers. Once the new data are extracted and processed, the  visualizations will be automatically updated on the client side.  The task scheduler is designed to be reconfigurable to different  time slots and/or different frequency.     3.3 Event Logging  For usage statistics, users interactive activities with Wikiglass are  logged, such as duration of use, number of views of each page,  and the type of visualization mode (statistics/timeline) being used.  This information will be useful for subsequent analytics aiming to  improve user experience and system functionality.   3.4 Weekly Summary  At the end of every week, teachers will receive emails  summarizing the progress of the groups and students they teach.  In this way, teachers can have a quick review on the performance  of different groups and students, and be reminded to logon  Wikiglass for more detailed information.   4. FUTURE WORK  The next steps include identifying and visualizing quality  indicators of student collaborative writing and evaluating the  impacts of the tool on teaching and learning.    5. ACKNOWLEDGMENTS  The work was partially supported by an Early Career Scheme  grant from the Research Grants Council of the Hong Kong Special  Administrative Region, China. (Project No. HKU 27401114) and  a Teaching Development Fund by the Faculty of Education,  University of Hong Kong.   REFERENCES  [1] Chu, S.K.W., Siu, F.L.C., Liang, M., Capio, C.M. & Wu,   W.W.Y. 2013. Users experiences and perceptions on using  two wiki platforms for collaborative learning and knowledge  management. Online Information Review, 37(2), 304325.  DOI= http://dx.doi.org/10.1108/OIR-03-2011-0043   [2] Kear, K., Donelan, H., & Williams, J. 2014. Using wikis for  online group projects: Student and tutor perspectives.  International Review of Research in Open and Distance  Learning, 15(4), 70-90.   [3] Kubincov, Z., Homola, M., & Janajev, R. 2012. Tool- supported assessment of wiki-based assignments. Paper  presented at the 4th International Conference on Computer  Supported Education (CSEDU 2012), Porto, Portugal.   [4] Darwin, P. B., & Kozlowski, P. 2013. AngularJS web  application development. Packt Publishing Ltd.    "}
{"index":{"_id":"92"}}
{"datatype":"inproceedings","key":"Freeman:2016:DUS:2883851.2883903","author":"Freeman, J. D.","title":"Demonstration of the Unizin Sentiment Visualizer","booktitle":"Proceedings of the Sixth International Conference on Learning Analytics  Knowledge","series":"LAK '16","year":"2016","isbn":"978-1-4503-4190-5","location":"Edinburgh, United Kingdom","pages":"552--553","numpages":"2","url":"http://doi.acm.org/10.1145/2883851.2883903","doi":"10.1145/2883851.2883903","acmid":"2883903","publisher":"ACM","address":"New York, NY, USA","keywords":"Unizin, discussion, intervention, learning analytics, natural language processing, real time, real-time, sentiment analysis, student, text mining","abstract":"While much promise has been demonstrated in the learning analytics field with sentiment analysis, the analyses are typically post hoc. The Unizin Sentiment Visualizer demonstrates that the application of sentiment analysis in real-time provides a powerful new tool to support students in complex learning environments.","pdf":"Demonstration of the Unizin Sentiment Visualizer  J D Freeman   Unizin  720 Brazos Street, Suite 810    Austin, TX, USA  jd.freeman@unizin.org  ABSTRACT  While much promise has been demonstrated in the learning  analytics field with sentiment analysis, the analyses are typically  post hoc. The Unizin Sentiment Visualizer demonstrates that the  application of sentiment analysis in real-time provides a powerful  new tool to support students in complex learning environments.   Categories and Subject Descriptors  J.7 Computers in Other Systems (C.3)  - Real time;  K.3.1  Computer Uses in Education - Computer-assisted instruction  (CAI); I.2.7 Natural Language Processing - Text analysis   General Terms  Algorithms, Measurement   Keywords  Unizin, learning analytics, student, intervention, sentiment  analysis, discussion, natural language processing, text mining, real  time, real-time   1. INTRODUCTION  As Simon Knight and Karen Littleton recently explained in the  Journal of Learning Analytics [1], there is strong demand for  discourse analysis.    In this demonstration, Unizin will show an application where the  Canvas Live Events feed by Instructure is ingested, and content  from student contributions to discussion forums is scored for  sentiment, frustration, grammatical uncertainty (as a subset of  linguistic modality), and inquisition. The Live Events feed [2], is  compliant with the Caliper Analytics framework, as maintained by  the IMS Global Learning Consortium [3].    Charted results are available as a selectable data table (Figure 1).  Selectable fields include Course, Term, Student, Alert Status,  Thread, and Post.     Figure 1: Data Table with User Selection Menu.     The results will also be displayed in an interactive scatterplot,  where each dot represents the average sentiment of a students  contributions to any given discussion thread. Students in potential  need of intervention will be detected and highlighted in an  alternative color. The detection threshold can be customized, and  is shown as one standard deviation of negative sentiment below  the average sentiment of the entire class for the same thread.     A tooltip (Figure 2) will appear when the cursor hovers over a  dot, displaying the identifiers for both the student and the thread,  as well as a sentiment reading. While this demonstration employs  anonymized synthetic data for FERPA compliance, the instructor  would see student names.        Figure 2: Tooltip on Sentiment Analysis Datum Representing  the Average Sentiment Expressed by an Individual Student   throughout a Discussion Thread, Plotted Relative to the  Average Sentiment for the Entire Class.     The results for frustration, uncertainty, and inquisition will be  displayed in a three-dimensional dot cloud as depicted in Figure  3. The instructor will be able to zoom and spin perspective around  this dot cloud. The detection threshold can be those outliers  falling beyond the farthest boundary of the concentration zone of  a cluster (shown as shaded in Figure 3) or a trigger limit on the  Euclidean distance of each dot from origin.     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '16, April 25-29, 2016, Edinburgh, United Kingdom  ACM 978-1-4503-4190-5/16/04.  DOI: http://dx.doi.org/10.1145/2883851.2883903      http://epress.lib.uts.edu.au/journals/index.php/JLA/article/view/4043/4841 http://canvas.beta.instructure.com/doc/api/file.live_events.html http://www.imsglobal.org/activity/caliperram     Figure 3: Interactive 3D Clustering of Student Discussion   Contributions by Uncertainty, Frustration, and Inquisition.      Reports of those students contributing to the current thread and in  potential need of intervention are interactively prepared on a  different tab, showing the text in question for review. Among  others, the application finds cases where students are pleading for  help (Figure 4). This report can be printed directly from the  browser, so that the instructor may have the report in hand while  heading into the classroom.       Figure 4: The Application Detects a Student Pleading for Help   with a Printable Intervention Report.    Finally, while word clouds are not empirical visualizations, they  are popular because some instructors believe they can get an  overall sense of the class (Figure 5). Here, the instructor can  specify a word cloud to represent some or all of the discussion  posts.      Figure 5: Word Cloud.   Over time, the data available within the application will include  enough history to compare charts and word clouds across terms  and/or instructors.    While this application is intended to demonstrate the power of  real-time learning analytics, Unizin hopes that mapping the  intervention alerts will also be useful in informing early warning  systems, and thusly contribute to student retention. Lessons we  learn from this endeavor may eventually contribute to an  empirical base of knowledge within the real time analytics space  that drives alerts for other administrative efficiencies.   2. WHO WE ARE  Unizin [4] is a consortium of like-minded land grant institutions  in higher education facilitating the transition toward collaborative  digital education. Our mission is to improve the learning  experience by providing an environment built on collaboration,  data, standards, and scale.   Our offerings include Engage, an eText reader platform and  collaborative learning tool for the delivery of digital learning  materials, including Open Educational Resources, faculty- authored course packs, and publisher content.   3. ACKNOWLEDGMENTS  In keeping with this years conference theme of a convergence of  communities, the author thanks:   Dr. Vince Kellen, Senior Vice Provost and Chief Information  Officer at University of Kentucky, which is not a Unizin member  institution, for the use cases that justify this tool.   Instructure, makers of the Canvas LMS, for their support in  providing the experimental Canvas Live Events feed.   IMS Global Learning Consortium, maintainers of the Caliper  Analytics framework.   Ms. Kimberly Arnold, Senior Evaluation Consultant, University  of Wisconsin  Madison, for her editorial support.   REFERENCES  [1] S. Knight, & K. Littleton. Discourse  Centric Learning   Analytics: Mapping the Terrain. Journal of Learning  Analytics, 2(1):185  209, 2015.    [2] Canvas Live Events feed by Instructure. URL:   http://canvas.beta.instructure.com/doc/api/file.live_events.html    [3] Caliper Analytics Framework, IMS Global Learning URL:  http://www.imsglobal.org/activity/caliperram   [4] Unizin Educational Consortium. URL: http://unzin.org/  [5] Promotional Video for Unizin Sentiment Visualizer.  E-mail   author for the current URL.       "}
