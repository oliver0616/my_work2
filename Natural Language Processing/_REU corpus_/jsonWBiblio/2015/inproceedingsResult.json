{"index":{"_id":"1"}}
{"datatype":"inproceedings","key":"Martinez-Maldonado:2015:LWD:2723576.2723583 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"2"}}
{"datatype":"inproceedings","key":"Kitto:2015:LAB:2723576.2723627 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"3"}}
{"datatype":"inproceedings","key":"Scheffel:2015:DEF:2723576.2723629 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"4"}}
{"datatype":"inproceedings","key":"Eagle:2015:ENP:2723576.2723630 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"5"}}
{"datatype":"inproceedings","key":"Wang:2015:TBA:2723576.2723618 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"6"}}
{"datatype":"inproceedings","key":"SanPedro:2015:ECM:2723576.2723610 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"7"}}
{"datatype":"inproceedings","key":"Vogelsang:2015:VPG:2723576.2723633 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"8"}}
{"datatype":"inproceedings","key":"Ferguson:2015:EEA:2723576.2723606 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"9"}}
{"datatype":"inproceedings","key":"Hansen:2015:SSM:2723576.2723615 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"10"}}
{"datatype":"inproceedings","key":"Joksimovic:2015:YCA:2723576.2723604 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"11"}}
{"datatype":"inproceedings","key":"Ferguson:2015:LAE:2723576.2723637 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"12"}}
{"datatype":"inproceedings","key":"Vahdati:2015:OOQ:2723576.2723605 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"13"}}
{"datatype":"inproceedings","key":"Prinsloo:2015:SPS:2723576.2723585 author =  Within higher education, our assumptions and understanding of issues surrounding student attitudes to privacy are influenced both by the apparent ease with which the public appear to share the detail of their lives and our paternalistic institutional cultures. As such, it can be easy to allow our enthusiasm for the possibilities offered by learning analytics to outweigh consideration of issues of privacy. This paper explores issues around consent and the seemingly simple choice to allow students to opt-in or opt-out of having their data tracked. We consider how 3 providers of massive open online courses (MOOCs) inform users of how their data is used, and discuss how higher education institutions can work toward an approach which engages and more fully informs students of the implications of learning analytics on their personal data.","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"14"}}
{"datatype":"inproceedings","key":"Aguiar:2015:WML:2723576.2723619 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"15"}}
{"datatype":"inproceedings","key":"Elbadrawy:2015:CMM:2723576.2723590 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"16"}}
{"datatype":"inproceedings","key":"Asif:2015:IPS:2723576.2723579 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"17"}}
{"datatype":"inproceedings","key":"Davies:2015:UTD:2723576.2723620 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"18"}}
{"datatype":"inproceedings","key":"Bergner:2015:EAH:2723576.2723582 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"19"}}
{"datatype":"inproceedings","key":"Brooks:2015:TSI:2723576.2723581 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"20"}}
{"datatype":"inproceedings","key":"Kennedy:2015:PSL:2723576.2723593 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"21"}}
{"datatype":"inproceedings","key":"Harrison:2015:LAS:2723576.2723621 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"22"}}
{"datatype":"inproceedings","key":"Ezen-Can:2015:UMU:2723576.2723589 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"23"}}
{"datatype":"inproceedings","key":"Milligan:2015:CLM:2723576.2723596 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"24"}}
{"datatype":"inproceedings","key":"Joksimovic:2015:CPT:2723576.2723609 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"25"}}
{"datatype":"inproceedings","key":"Gweon:2015:TSP:2723576.2723608 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"26"}}
{"datatype":"inproceedings","key":"Lee:2015:BKT:2723576.2723587 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"27"}}
{"datatype":"inproceedings","key":"Xing:2015:LAO:2723576.2723602 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"28"}}
{"datatype":"inproceedings","key":"Kovanovic:2015:PBB:2723576.2723623 author = ","abstract":"All forms of learning take time. There is a large body of research suggesting that the amount of time spent on learning can improve the quality of learning, as represented by academic performance. The wide-spread adoption of learning technologies such as learning management systems (LMSs), has resulted in large amounts of data about student learning being readily accessible to educational researchers. One common use of this data is to measure time that students have spent on different learning tasks (i.e., time-on-task). Given that LMS systems typically only capture times when students executed various actions, time-on-task measures are estimated based on the recorded trace data. LMS trace data has been extensively used in many studies in the field of learning analytics, yet the problem of time-on-task estimation is rarely described in detail and the consequences that it entails are not fully examined. This paper presents the results of a study that examined the effects of different time-on-task estimation methods on the results of commonly adopted analytical models. The primary goal of this paper is to raise awareness of the issue of accuracy and appropriateness surrounding time-estimation within the broader learning analytics community, and to initiate a debate about the challenges of this process. Furthermore, the paper provides an overview of time-on-task estimation methods in educational and related research fields.","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"29"}}
{"datatype":"inproceedings","key":"Snow:2015:YGS:2723576.2723592 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"30"}}
{"datatype":"inproceedings","key":"Crossley:2015:PTF:2723576.2723595 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"31"}}
{"datatype":"inproceedings","key":"Whitelock:2015:OSD:2723576.2723599 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"32"}}
{"datatype":"inproceedings","key":"Mandran:2015:DMB:2723576.2723580 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"33"}}
{"datatype":"inproceedings","key":"Gross:2015:HRS:2723576.2723601 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"34"}}
{"datatype":"inproceedings","key":"Rogers:2015:CRL:2723576.2723631 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"35"}}
{"datatype":"inproceedings","key":"Hsiao:2015:TFM:2723576.2723613 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"36"}}
{"datatype":"inproceedings","key":"Molenaar:2015:ESS:2723576.2723586 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"37"}}
{"datatype":"inproceedings","key":"Knight:2015:DMP:2723576.2723577 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"38"}}
{"datatype":"inproceedings","key":"Allen:2015:YRM:2723576.2723617 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"39"}}
{"datatype":"inproceedings","key":"Pardo:2015:ILS:2723576.2723611 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"40"}}
{"datatype":"inproceedings","key":"Holman:2015:PSS:2723576.2723632 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"41"}}
{"datatype":"inproceedings","key":"Holman:2015:PSS:2723576.2723632 author = ","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"42"}}
{"datatype":"inproceedings","key":"Beheshitha:2015:PMA:2723576.2723628 author = ","abstract":"Research on self-regulated learning has taken main two paths: self-regulated learning as aptitudes and more recently, self-regulated learning as events. This paper proposes the use of the Fuzzy miner process mining technique to examine the relationship between students' self-reported aptitudes (i.e., achievement goal orientation and approaches to learning) and strategies followed in self-regulated learning. A pilot study is conducted to probe the method and the preliminary results are reported.","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"43"}}
{"datatype":"inproceedings","key":"Mostafavi:2015:TDM:2723576.2723622","author":"Mostafavi, Behrooz and Eagle, Michael and Barnes, Tiffany","title":"Towards Data-driven Mastery Learning","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"270--274","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723622","doi":"10.1145/2723576.2723622","acmid":"2723622","publisher":"ACM","address":"New York, NY, USA","keywords":"data-driven mastery learning, knowledge tracing, logic proof, problem selection","abstract":"We have developed a novel data-driven mastery learning system to improve learning in complex procedural problem solving domains. This new system was integrated into an existing logic proof tool, and assigned as homework in a deductive logic course. Student performance and dropout were compared across three systems: The Deep Thought logic tutor, Deep Thought with integrated hints, and Deep Thought with our data-driven mastery learning system. Results show that the data-driven mastery learning system increases mastery of target tutor-actions, improves tutor scores, and lowers the rate of tutor dropout over Deep Thought, with or without provided hints.","pdf":"Towards Data-Driven Mastery Learning  Behrooz Mostafavi Department of Computer  Science North Carolina State  University Raleigh, NC 27695  bzmostaf@ncsu.edu  Michael Eagle Department of Computer  Science North Carolina State  University Raleigh, NC 27695  mjeagle@ncsu.edu  Tiffany Barnes Department of Computer  Science North Carolina State  University Raleigh, NC 27695  tmbarnes@ncsu.edu  ABSTRACT We have developed a novel data-driven mastery learning system to improve learning in complex procedural problem solving domains. This new system was integrated into an existing logic proof tool, and assigned as homework in a de- ductive logic course. Student performance and dropout were compared across three systems: The Deep Thought logic tu- tor, Deep Thought with integrated hints, and Deep Thought with our data-driven mastery learning system. Results show that the data-driven mastery learning system increases mas- tery of target tutor-actions, improves tutor scores, and low- ers the rate of tutor dropout over Deep Thought, with or without provided hints.  Keywords Data-driven Mastery Learning, Problem Selection, Knowl- edge Tracing, Logic Proof  1. INTRODUCTION This paper describes a novel data-driven mastery learn-  ing system (DDML) that uses the knowledge tracing (KT) of tutor actions in past student-tutor performance data to regularly evaluate new student performance and select suc- cesive structured problem sets. An overview of the DDML system is shown in Figures 1 & 2. Problem sets are sepa- rated into levels based on the rules or concepts required for those problems, with the order of levels determined by the course curriculum. Each level presents an opportunity for mastery learning, requiring students to complete of a min- imum number of problems containing the required rules or concepts for that level in order to progress to the next. Be- tween levels, student performance is assessed by the knowl- edge tracing of all actions taken in the tutor (in the case of this experiment, target-rule application), weighted by the priority of those actions within the level. The resulting score from the assessment is compared to a threshold value: the average data-driven knowledge tracing (DKT) scores of cor-  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org LAK 15 March 16  20, 2015, Poughkeepsie, NY USA Copyright 2015 ACM 978-1-4503-3417-4/15/03$15.00 http://dx.doi.org/10.1145/2723576.2723622.  Exemplars (Historical)    1st Interval Problems   Knowledge Tracing of all steps in P1 to Pl   P(G)  P(S)   P(L0) P(T) P(T)   KCs KCs KCs   Last  Step Step Step          P1 P2 Pl  P1 P2 Pl   2nd Interval Problems    Interval  Performance   Compare step to best possible step, update KCs   KC 1 KC 2  KC n  Exemplar 1 0.35 0.12  0.67  Exemplar 2 0.42 0.33  0.59         Exemplar m 0.37 0.28  0.73  AVERAGE 0.46 0.26  0.71   KC Thresholds sent to DDML   Figure 1: The data-driven threshold builder. KCs for each exemplar are updated using action steps from an interval set of tutor problems. The KC score averages at each interval are used as thresholds in the DDML system.  responding problems solved by past students who have suc- cessfully completed the entire tutor (exemplars). A score value above or below the threshold determines the users relative proficiency in the subject matter, and the difficulty of the problem set given in the next level. This process con- tinues through the end of the tutor. This is a new, novel approach to achieve mastery learning.  For this experiment, we compared three versions of the Deep Thought logic tutor: the original undirected proof tool with un-ordered problem sets (DT0); DT0 with inte- grated on-demand hints (DT1); and DT0 integrated with data-driven mastery learning (DT2). We hypothesized that DT2s DDML system would result in higher learning gains and lower tutor dropout than DT0 and DT1. Analysis of the results indicate that students using DT2 had significant improvement in full tutor completion by almost 3 times over students using DT0. Students using DT2 also showed im- provement over students using DT1 with hints, although this was not statistically significant. The results show that DT2s data-driven mastery learning system is effective in improving tutor percent completion for most students over DT0 and DT1, confirming our hypothesis.  2. RELATED WORK Cognitive tutors use knowledge tracing (KT) to track stu-  dents mastery of particular Knowledge Components (KCs) based on their performance at each opportunity to apply  270    Level 1   Level 2   New Student   P1.1  P1.l   Assessment   Exemplar  KC Thresholdi   (i = KC1  KCn)   P2.1H  P2.lH   P2.1L  P2.lL   Level 3   P3.1H  P3.lH   P3.1L  P3.lL  Assessment    High Proficiency High Proficiency   Low Proficiency Low Proficiency   Interval  Performance + value   - value   Compare  performance to   interval thresholds,  weight and sum   difference.  (+/-)   Figure 2: The Data-Driven Mastery Learning system. At each level interval, new student KC scores are calculated and then compared to exemplar thresholds for corresponding problem sets (see Figure 1).  that KC. The classic model used by Corbett & Anderson uses Bayesian Knowledge Tracing (BKT) to predict proba- bility of learned components based on learning, guess, and slip parameters [3]. Once a KT model is created, it can be used to select problems which best suit the student and what he or she still needs to learn. BKT has been shown to be generally effective for predicting success on next problems (meaning it can be used for problem selection), although it is more effective in some domains than others [7].  In tutoring systems with an intelligent problem selection component, problem selection thrashing can occur. Thrash- ing is a cycle that can occur in problem selection, where students will be given a string of problems that focus on con- cepts they have already mastered until they reach a prob- lem with multiple concepts, including concepts they have not yet mastered, inevitably get the incorrect answer, and then are given a similar string of easy but unhelpful prob- lems. Koedinger et al argued that thrashing was caused by incorrect blame assignment in knowledge tracing systems; when a student attempts a problem with multiple knowl- edge components and gets an incorrect answer, knowledge tracing works on the assumption that each knowledge com- ponent must have been applied or executed equally incor- rectly [5]. This can provide greater problems in domains where knowledge components may overlap or conflict with each other, such as deductive logic proof problem-solving. The system presented in this paper was designed to avoid problem selection trashing, in that we sought to intelligently select appropriate problem sets by determining which knowl- edge components each student needed to focus on and use that determination to select appropriate problems.  2.1 Deep Thought Fig. 3 shows the interface for Deep Thought, that displays  logical premises, buttons for logical rules, and a logical con- clusion to be derived. Deep Thought was developed as a practice tool for proof construction, divided into three sets of problems.  In DT0 students were allowed to solve assigned problems at will, in any order, though assignments were ordered. As a student works through a problem, each step is logged in a data file that records the current problem, the rule being applied, any errors made (such as attempting to use a rule that is logically impossible), completion of the problem, time taken per step, and elapsed time taken to solve the problem.  Figure 3: A screen capture of the Deep Thought tutor, showing given premises at the top, conclusion at the bottom, and rules for application on the right.  Barnes & Stamper extended Deep Thought with a data- driven hint-generation system to create DT1. DT1 used a Markov decision process to automatically select hints for stu- dents upon request, based on their individual performance on specific problems [1]. In their research, Stamper et al. observed student dropout in DT0 and DT1 were high [6]. Despite improvements to the student dropout rate from DT0 to DT1 with the addition of hints, there were still significant numbers of students that did not complete the tutor. We hypothesized that the DDML system we developed for DT2 would have higher percentage completion and lower student dropout rates than DT0 and DT1 (see Table 2).  3. METHODS DT2s data-driven mastery learning system consists of  two major components: a mastery learning leveling com- ponent; and an assessment component. Their processes are described in this section.  3.1 Mastery Learning: DT2 Leveling Up DT2 breaks the problem sets of DT0/DT1 into additional  levels while maintaining the same rule applications and the same difficulty level of problems in DT0/DT1. Students  271    cumulative performance on target-rule actions at the end of a level determine whether they attempt the next level at higher or lower proficiency, as shown in Figure 2. Level 1 of DT2 contains three problems common to all students who use the tutor, and provides initial performance assessment data to the DDML model. Levels 26 of DT2 are each split into two distinct sets of problems, labeled higher and lower proficiency. The problems in the different proficiency sets prioritize the same rules judged by domain experts to be im- portant for solving the problems in that level. However, the degree of problem solving difficulty between proficiency sets is different, with problems in the low proficiency set requir- ing fewer numbers of steps for completion, lower complexity of logical expressions, and lower degree of rule application than problems in the high proficiency set. Fig. 4 shows the possible path progressions of students using DT2.  Figure 4: DT2 path progression. At each level, stu- dents are evaluated and provided either the higher or lower proficiency problem sets. Students can also be switched from the higher to lower proficiency set within a level.  Because the problem sets in DT2 are completely ordered instead of random access, DT2 allows students to temporar- ily skip problems within a level. Students who skip once in the higher proficiency set are given the next unsolved prob- lem in the set. Skipping twice in the higher proficiency set will drop students to the lower proficiency problem set in the same level, regardless of how many problems they solved in the higher proficiency set (see Fig. 4). Students in the lower proficiency set who skip a given problem are first offered an alternate version of the same problem before moving them to the next unsolved problem in the set. Students who re- peatedly skip problems in the lower proficiency set will cycle through the unsolved problems in the set.The system main- tains a maximum of three required solved problems to con- tinue to the next level, taking into account any problems that may have been solved in the harder proficiency set, and ensures that all rules required for demonstration are presented to the student. Between the two proficiency sets, and the alternate problems offered in the lower proficiency set, there are 43 problems in the DT2 problem set The DDML system can be applied to any procedural do-  main where quality of student solutions are difficult to eval-  uate. By limiting the number of problems students are re- quired to solve, the DDML system ensures that students do not spend an unreasonable amount of time solving prob- lems in the current level, allowing them to move forward in the tutor and be exposed to new concepts while receiving practice with problems that have difficulty relative to their performance.  3.2 Assessment: DKT Rule Score Updating We hypothesize that students who have completed Deep  Thought in the past have acquired a coherant skill set for proof problem solving and rule application. By splitting the DT0/DT1 problem set into levels, DT2 allows regu- lar evaluation of new students compared to the standard of what exemplars were able to accomplish at corresponding points in the tutor. DT2 uses data-driven knowledge trac- ing (DKT) of past and current students to evaluate student performance. For each student, a KT score ruleScorei for each logical rule i in the tutor is created when a student first logs into DT2, and these scores are maintained and updated at each rule application made by the student.  The ruleScorei for a given rule i is initialized with a learn- ing value p(L0) = 0.01, acquisition value p(T ) = 0.01, guess value p(G) = 0.3, and slip value p(S) = 0.1. After each observed application of rule i, ruleScorei is updated using Bayesian knowledge tracing equations for inference and pre- diction of individual skill knowledge [4].  3.3 Assessment: Proficiency Determination By the time the student has completed a level of the tu-  tor, that student will have accumulated a set of rule scores for each rule i calculated based on their performace. At the end of each level, the DKT scores for each rule ruleScorei are compared to a threshold value for that same rule. This threshold value, ruleThresholdi, was calculated using data- driven knowledge tracing of DT0 tutor logs from six 2009 Deductive Logic course sections. Only students who com- pleted the entire DT0 assignment (n = 302) were used for threshold calculation, since these exemplars demonstrated proficiency for proof problem solving using all required rules. DKT student scores were computed for problems in DT0, and mapped to the corresponding levels in DT2. The scores from DT0 for each rule score were averaged at each of these break points and set as ruleThresholdi. Students using DT2 are therefore judged to have proficiency on a rule based on how their performance compares to average student usage from previous use of Deep Thought by past exemplars.  For each student in DT2, every rulei is assigned a positive scoreSigni value if the rule score is above ruleThresholdi. Each rulei receives a negative scoreSigni value if the score is below the ruleThresholdi, meaning that they have not yet shown the same level of proficiency in deciding which rules to use as past students who successfully completed all the problems in DT0. This positive or negative value is weighted based on the priority of rule i in the current level. Rule priority is determined by the set of rules deemed necessary by domain experts to best solve the current proof problem. Rules required for completion of the proof problems in that level are weighted by 1. Rules not required for completion of the proof problems in that level are weighted by 0.5.  The weighted scoreSigni values are summed, and the sign of the sum determines whether a student is sent to the higher proficiency path or the lower proficiency path  272    4. RESULTS AND DISCUSSION DT2 was used as a mandatory homework assignment by  students in a philosophy deductive logic course (DDML group, n = 47). Students were allowed to work through the problem sets at their own pace for the entire 15-week semester. Prob- lem Levels 16 were assigned for full completion of the tutor, totalling 1318 problems depending on proficiency path pro- gression. Data from DT2 was compared to data collected in two  prior semesters of the same philosophy course using DT1 and DT0, respectively. Both courses were taught by the same instructor as the DT2-DDML group, and were assigned the same problems for full completion of the tutor. The Spring 2009 students used the version of Deep Thought  (DT1) with a hint-generation system developed by Barnes & Stamper to aid students in solving proof problems (Hint group, n = 48). Students using DT1 show a significant increase in tutor completion over the DT0 system without hints [2]. The Fall 2009 students used the original unaltered DT0 (Control group, n = 43) with no hints.  4.1 Tutor Completion Table 1(a) shows the number of total assigned problems  solved in tutor for the three groups. Students in the DT2- DDML group solved 13 problems on average  the minimum required for completion of the tutor  while students in the DT1-Hint group completed 8 assigned problems out of 13, and students in the DT0-Control group completed 6 out of 13.  Table 1: (a) Number of total assigned problems solved in tutor. (b) Percentage of tutor completion. * indicates significance over the control.  (a) DT2-DDML DT1-Hint DT0-Control Mean 13.09 of 1318 7.98 of 13 6.07 of 13 StDev 4.94 4.78 5.20  (b) DT2-DDML DT1-Hint DT0-Control Mean 79.79* 61.38 46.69 Median 100.0 61.54 46.15 StDev 29.88 36.78 40.02  Table 1(b) shows that students from the DT2-DDML group complete 33% more of the tutor on average than the DT0- Control group, and 18% more of the tutor than the DT1- Hint group, with lower variance in the final scores. The median scores show that over half (55%) of the DT2-DDML group completed the entire tutor (see Table 2 for tutor com- pletion by group). A one-way ANOVA test on percent com- pletion difference was performed on tutorial completion across all three test groups, showing significance (F (2, 120) = 7.559, p = 0.001). A Tukey post-hoc comparison shows a signifi- cant improvement of the DT2-DDML group (M = 0.80, 95% CI [0.70, 0.90]) over the DT0-Control group (M = 0.51, 95% CI [0.40, 0.62], p <0.001, Cohens d = 0.84). Although the DT2-DDML group had a higher mean percentage comple- tion than the DT1-Hint group, the results were not signifi- cant (p = 0.138, Cohens d = 0.35). However, this still in- dicates that DT2s data-driven knowledge tracing combined with individualized problem set selection can improve stu- dent percent completion as much as on-demand hints (DT1).  4.2 Student Dropout Student dropout is defined as the termination of tutor ac-  tivity prior to completion of assigned problems (DT0/DT1) or levels (DT2). Figure 5 shows the dropout trend of the three data groups over the course of the tutor. The mark- ers on the DT2-DDML line indicate the end of each level in DT2, and the markers on the DT1-Hint and DT0-Control lines indicate the end of each problem in DT1 and DT0. The horizontal axis represents the percentage completion of the tutor. The vertical axis represents the percentage of all students in each group active in the tutor at the com- pletion of each level (DT2) or problem (DT1, DT0). Both the DT1-Hint and DT0-Control groups have higher dropout than the DT2-DDML group across all problems. The DT0- Control group also has greater dropout than the DT1-Hint group, with a noted difference in higher level problems (cor- responding to Levels 46 in DT2). This is consistent with the results found by Stamper et al [6], that on-demand hints help student retention in-tutor.  Figure 5: Rate of student dropout for the DT2- DDML, DT1-Hint, and DT0-Control groups over the course of the tutor.  Table 2 summarizes the number of students who com- pleted and dropped out of the tutor across all three groups. An overall chi-square test was performed to examine the re- lationship between student dropout and group, showing sig- nificance (2(2, 123) = 11.75, p = 0.003). The DT2-DDML group was significantly less likely (by 82%) to drop out of tutor when compared to the DT0-Control group (2(1, 84) = 11.50, p = 0.001, OR = 5.31). Although the DT2- DDML group had lower student dropout than the DT1-Hint group, the difference was not significant (2(1, 86) = 3.23, p = 0.072, OR = 2.21). The DT1-Hint group had a lower dropout rate than the DT0-Control group, but not signifi- cantly (2(1, 76) = 2.74, p = 0.098, OR = 2.40). As a whole, the DDML system in DT2 is as effective in im-  proving student in-tutor performance as much as on-demand hints in DT1, and significantly better than DT0. The pur- pose of on-demand hints is to aid and improve student learn- ing, and tutor performance is a reflection of a students knowledge and abilities in the subject matter. This result is important for intelligent tutor design, indicating that the DDML system presented here can achieve or outperform the learning gain from on-demand hints, reducing tutor devel-  273    Table 2: Student completion of the tutor by group. Dropped indicates that the student did not complete Deep Thought.  Group Completed Dropped Total DT0-Control 8 (18.6%) 35 (81.4%) 43 DT1-Hint 15 (31.3%) 33 (68.7%) 48 DT2-DDML 26 (55.3%) 21 (44.7%) 47 Total 49 (35.5%) 89 (64.5%) 138  opment time. The next section explores performance in the application of rules in DT2.  4.3 Rule Score Threshold Comparison Evaluation of the overall learning effect of the DDML  system model is determined by comparing final ruleScorei scores for all i rules from the DT2-DDML group to end- of-tutor ruleThresholdi scores from the DT0 logs. End- of-tutor ruleThresholdi scores and the DT2-DDML group scores were taken only from students who completed the entire tutor, to measure successful student rule-application performance. The results are shown in Table 3. The rules listed in Table  3 are rules that were required for completion of the tutor. The Difference column shows whether the DT2-DDML rule scores are higher (+) or lower () than the ruleThreshold scores. With the exception of modus tollens (MT) and con- structive dilemma (CD), average rule scores for the DT2- DDML group are higher than the respective ruleThresholdi scores. This indicates that students using DT2 show a higher overall awareness of proper individual rule usage by end of tutor over students using DT0.  Table 3: Average rule DKT scores for DT2-DDML group students who completed the entire tutor (n = 26), compared to end-of-tutor ruleThresholdi aver- ages from DT0 (n = 302).  Rule ruleThresholdi DT2-DDML Difference MP 0.649 0.743 + DS 0.491 0.647 +  SIMP 0.734 0.947 + MT 0.352 0.238  ADD 0.426 0.747 + CONJ 0.348 0.578 + HS 0.455 0.669 + CD 0.163 0.120  DN 0.437 0.697 + DEM 0.182 0.423 + IMPL 0.463 0.703 +  TRANS 0.298 0.433 + EQUIV 0.106 0.151 +  Students using DT2 complete more of the tutor on average than students using DT0 or DT1 with hints. Students using DT2 have a lower rate of tutor dropout than students using DT0 and DT1, exposing more students to important domain concepts. DT2-DDML students are also significantly more likely to complete their entire assignment when compared to the DT0-Control group and the DT1-Hint group. This is particularly important, since success in Deep Thought is correlated with course completion for introductory deductive  logic [6]. By solving more problems, DDML students have more practice solving logic problems than students using DT0 or DT1. These students also show higher proficiency of tutor concepts.  5. CONCLUSIONS This paper presented a holistic data-driven mastery learn-  ing system that uses knowledge tracing of domain concepts in existing student-tutor performance data to regularly eval- uate current student proficiency and select problem sets. Analysis of the results show that this system can improve tutor completion and proficiency of applied target-rules com- pared to past exemplars. We conclude that the data-driven mastery learning system presented in this paper significantly aids mastery of core concepts, improves tutor scores, and lowers rate of tutor dropout for a majority of students over an undirected tutor, and at least as well as the same tutor with on-demand hints. We expect that the integration of hints with the DDML system will provide an even greater effect than what was seen in this experiment, and this is the first step of our future work.  6. ACKNOWLEDGEMENTS This material is based on work supported by the National  Science Foundation under Grants 1432156 and 0845997.  7. REFERENCES [1] T. Barnes and J. Stamper. Toward automatic hint  generation for logic proof tutoring using historical student data. In Proceedings of the 13th International Conference on Intelligent Tutoring Systems (ITS 2008), pages 373  382, 2008.  [2] T. Barnes, J. Stamper, L. Lehmann, and M. J. Croy. A pilot study on logic proof tutoring using hints generated from historical student data. In Proceedings of the 1st International Conference on Educational Data Mining (EDM 2008), pages 197  201, 2008.  [3] A. T. Corbett and J. R. Anderson. Knowledge Tracing: Modeling the Acquisition of Procedural Knowledge. User Modeling and User-Adapted Interaction, 4:253278, 1995.  [4] M. Eagle and T. Barnes. Data-Driven Method for Assessing Skill-Opportunity Recognition in Open Procedural Problem Solving Environments. In Proceedings of the 15th International Conference on Intelligent Tutoring Systems (ITS 2012), pages 615617, 2012.  [5] K. R. Koedinger, P. I. Pavlik, J. Stamper, T. Nixon, and S. Ritter. Avoiding Problem Selection Thrashing with Conjunctive Knowledge Tracing. In Proceedings of the 4th International Conference on Educational Data Mining (EDM 2011), pages 91100, 2011.  [6] J. Stamper, M. Eagle, T. Barnes, and M. J. Croy. Experimental Evaluation of Automatic Hint Generation for a Logic Tutor. In Proceedings of the 15th International Conference on Artificial Intelligence in Education (AIED 2011), pages 345352, 2011.  [7] Y. Xu, F. Ave, and J. Mostow. Comparison of methods to trace multiple subskills: Is LR-DBN best In Proceedings of the 5th International Conference on Educational Data Mining (EDM 2012), pages 4148, 2012.  274      "}
{"index":{"_id":"44"}}
{"datatype":"inproceedings","key":"Gibson:2015:ART:2723576.2723635","author":"Gibson, Andrew and Kitto, Kirsty","title":"Analysing Reflective Text for Learning Analytics: An Approach Using Anomaly Recontextualisation","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"275--279","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723635","doi":"10.1145/2723576.2723635","acmid":"2723635","publisher":"ACM","address":"New York, NY, USA","keywords":"affective computing, learning analytics, reflective text","abstract":"Reflective writing is an important learning task to help foster reflective practice, but even when assessed it is rarely analysed or critically reviewed due to its subjective and affective nature. We propose a process for capturing subjective and affective analytics based on the identification and recontextualisation of anomalous features within reflective text. We evaluate 2 human supervised trials of the process, and so demonstrate the potential for an automated Anomaly Recontextualisation process for Learning Analytics.","pdf":"Analysing Reflective Text for Learning Analytics: An Approach Using Anomaly Recontextualisation  Andrew Gibson & Kirsty Kitto Queensland University of Technology 2 George Street, Brisbane, Australia  [andrew.gibson,kirsty.kitto]@qut.edu.au  ABSTRACT Reflective writing is an important learning task to help foster reflective practice, but even when assessed it is rarely anal- ysed or critically reviewed due to its subjective and affective nature. We propose a process for capturing subjective and affective analytics based on the identification and recontex- tualisation of anomalous features within reflective text. We evaluate 2 human supervised trials of the process, and so demonstrate the potential for an automated Anomaly Re- contextualisation process for Learning Analytics.  Categories and Subject Descriptors K.3.1 [Computer Uses in Education]: Computer-assisted instruction; J.1 [Administrative Data Processing]: Ed- ucation; H.1.2 [User/Machine Systems]: Human factors  Keywords Reflective Text, Learning Analytics, Affective Computing  1. INTRODUCTION Reflective writing is used by educators to help students  develop the metacognitive capability required for effective reflective practice, an important dimension of continual im- provement in many professions [6, 5, 13]. However, despite its educational value, reflective writing presents challenges. Firstly, it is difficult to assess [13], and can require a lot of reading time on the part of the educator. Secondly, the inherent lack of structure in reflective writing, its personal style, and variability in quality [6], makes it difficult to treat in the same way as other more structured written tasks, and thus an unlikely candidate for computational analysis.  Despite these challenges, the fact that when we read text we easily recognise features such as sarcasm, humour and emotional tone, suggests that features are there, and could possibly be discovered computationally. Although external factors like body language and eye contact can help with interpretation of spoken language, they are by no means  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 15 March 16 - 20, 2015, Poughkeepsie, NY, USA Copyright is held by the author(s). Publication rights licensed to ACM. ACM 978-1-4503-3417-4/15/03 ...$15.00. http://dx.doi.org/10.1145/2723576.2723635  always necessary as human readers have an ability to in- tuitively determine subtexts, and to postulate on their un- stated meanings. In reviews of others writing we can easily understand phrases like thinly veiled contempt, dripping with sarcasm and unbridled enthusiasm, which remind us of the ease with which we perceive the affective dimensions of an authors perspective from their writing. When consider- ing reflective writing we also make interpretations about the way an author feels about a topic, their degree of openness and honesty, the extent to which the author is expressing affective ideas like conflict, fear or uncertainty, or even the degree to which an author acknowledges a community or focuses on themselves.  Although computational tools for natural language pro- cessing (NLP), such as linguistic analysis [7] and topic mod- elling [1], have proven very effective for information retrieval, they have typically been less capable with the more subjec- tive text characteristics such as emotion and affect. Sen- timent analysis [12] techniques can be used to identify af- fect, however they tend to perform best with polarised data that is more uniform in sentiment. We would prefer more complete representations of these subjective features when analysing reflective text for learning analytics.  It is not easy to see how the standard computational ap- proaches can be extended to more nuanced writing. The personal nature of reflective writing can involve the use of complex linguistic devices such as irony, which are particu- larly challenging for computational analysis. The statement Im spending my weekend marking assignments. I love it - cant imagine doing anything else is identified reasonably easily by a human reader as negative. This is because the human approach to anomalous information is to learn [15]. We tend to expand the repertoire of contexts which we draw upon to make sense of what we read. For example, when we contextualise a persons words as humour, we dont label them a liar for expressing obviously false statements.  Intriguingly, there is one exception to this typical hu- man approach for accommodating anomalous data within different contexts. Sometimes we judge the data itself to be inaccurate, in which case the tendency is to dismiss the anomaly and keep the existing context [4]. This judgement of accuracy is much closer to what we have come to expect from computational processes, which can influence our use of them. Thus, while an anomaly in the computer world is usually a violation of specific rules or conditions, an anomaly in the human world is something out of context. This is the intuition that we seek to formalise.  275    2. ANOMALY RECONTEXTUALISATION Our approach, which we call Anomaly Recontextualisation  (AR), is based on accommodating rather than eliminating anomalous data. It is fundamentally a 2 step process. Step 1 requires the identification of an anomaly within a given context. Step 2 involves recontextualising the anomaly such that it is no longer anomalous within its new context.  To illustrate our approach, consider the following metaphor about a bird called Tux:  Context: Birds have a feature of flight which is associated with a feature wings.  Data 1: Tux has feature wings, and belongs to context bird.  Data 2: Tux doesnt fly - lacks feature flight  Anomaly: Tux doesnt fly even though Tux has wings, is a bird, and birds fly.  New data: Tux has additional feature swims  New context: Penguins have a feature swims and a fea- ture wings, but lack feature flight.  Recontextualisation: Tux is a penguin!  Note that in the Tux illustration, we dont modify the orig- inal context. The birds context with feature flight still con- tains a general truth for birds. By recontextualising anoma- lies, we retain a general truth which allows us to recognise that birds fly, but to also adopt exceptions for those that do not fit precisely into the feature set of our original con- textualised definition. The alternative to AR is a semantic reconstruction that relaxes the feature set to elemintate the anomaly. In our Tux example this would mean modifying the birds fly general truth, to become some birds fly.  The problem with this kind of a semantic reconstruction approach is that it weakens the utility of our original concept (in its original context), and we lose the powerful construct that results from maintaining exceptions to general truths. This can be seen in a comparison between the computation- ally easy binary approach to resolving anomalies, and an approach that considers them in terms of their likelihood, or the probability that a certain feature would be present within a given context. For our metaphor, this would look like:  P (F (flight)|C(bird))  1 (or very high), (1)  where F denotes a feature, and C a context within a proba- bility P . Equation (1) expresses our intuitive understanding that the likelihood of something flying is very high if it is a bird. By contrast, a binary approach would assign a proba- bility equal to 1, and the context would need to be modified for the feature to hold in all cases. That is:  P (F (flight)|C(bird)) = 1 (2)  becomes:  P (F (flight)|C(flying bird)) = 1. (3)  This highlights the issue with reconstructing the semantics. While it resolves the anomaly, it actually results in changing the meaning of the context such that it doesnt provide us with any useful information. We have reached the status of a tautology: flying birds fly. A better approach would be to generate an additional context:  P (F (swims),F (flight)|C(penguin))  1. (4)  This allows Tux to be both a bird and a penguin, without altering our original understanding of birds. In fact we can even be certain that Tux is still a bird since:  P (C(bird)|C(penguin)) = 1. (5)  In order to explore the potential for a computational im- plementation of AR, we expanded on the 2 fundamental steps of the process . However, we did not assume that the AR process would be implemented in a single piece of software, nor that it would be free of any human supervision. Essentially, our objective was to outline the necessary steps that, given an anomaly in one context, allow a new context to be created in which that anomaly is resolved, without modifying the original context. This expanded AR process is as follows:  1. Identify a context based on a feature set (e.g. feath- ers, wings, flight, for context birds). A complementary context (e.g. not birds) can be identified from the ab- sense of the features.  2. Classify data based on a key feature (e.g. wings) to determine each elements membership of the context; the complementary context; or undetermined.  3. Identify anomalies through the classification using a feature strongly related to the first (e.g. flight). Anoma- lies are missing this feature but possess the first.  4. Classify the data based on a non key feature feature that is significant to the set of anomalies (e.g. swims).  5. Recontextualise successfully classified anomalies with a new context based on the associated features (e.g. wings, swims, no flight).  6. Repeat 4 and 5 until either (i) all anomalies are recon- texualised, or (ii) a set limit is reached.  The final output of the AR process is a number of con- texts which provide high level information about our data, along with the ability for that data to be understood in varying ways. For example, if we started the above example with a context that had features of doesnt fly and swims then we would could include whales, athletes and penguins. However, their membership in other contexts (e.g. ocean mammals, humans, birds) would allow us to not only differ- entiate between them, but also to understand their interre- lationships.  3. APPLICATION We applied the AR process to the analysis of reflective  texts written by first year Bachelor of IT students during a group project that spanned about 3 weeks. The cohort was offered access to GoingOK 1, a web application developed by one of us (Andrew Gibson) for recording personal reflections. At the conclusion of the project, students could download their reflections and submit them for assessment together with their project deliverables. GoingOK de-identifies the users reflective data so that it can be used for research pur- poses2. 82 students signed up to use GoingOK. Of these students, 24 recorded at least 1 reflection with reflective text during the period. Recorded reflections were of varying qual- ity, and so echoed many of the qualities that make reflective text difficult to work with computationally. 1www.goingok.com 2QUT Ethics Approval No.: 1400000151  276    GoingOK also records a reflection point with the text, a numeric value between 0 and 100 displayed to the user as a sliding scale between distressed and soaring with going ok in the middle. Some students didnt record any text reflections, and for the reflections that they did record they left the slider at the middle 50 position of going ok. These reflections (n = 58) were excluded from the data.  As our primary objective was to analyse the reflective text, we wanted to minimise the number of reflections that were recorded without text. However, excluding all non-text re- flections would not have provided a realistic sample of the data, so we selected all reflections for users where at least one of their reflections included 2 or more words. Thus, we still captured some zero word reflections, but the majority of the reflections included a text of at least 2 words. The resultant dataset included 57 records from 24 students.  We extracted a large range of features from the data set that could be used in our computational analysis. For each individual reflection we extracted features related to: reflec- tion point, date, word length, word and sentence counts, parts of speech, word frequencies, and topics. We also ex- tracted features for the group as a whole: Statistical anal- ysis of reflection point, day of week, word lengths, word and sentence counts. We also collected frequency distribu- tions of words, counts, ratios, and various parts of speech. Other features were collected by building topic models over the full collection. Using both individual and group fea- tures, we calculated a range of comparative measures such as the deviation from group mean for individual features, or the Kullback-Leibler (KL) divergence [9] for measuring the similarity or difference between distributions. Term Fre- quency / Inverse Document Frequency (TF-IDF) [14] also provided a measure of the extent to which words were unique to inidvidual reflections, or more generally used across the collection. For topic modelling, we used Latent Dirichlet Allocation (LDA) [2], which we used to identify the topical relationships between reflections and groups.  Although not part of the reflective text itself, the (nu- meric) reflection point was included in our analysis as it provided a quantitative measure indicating the users self- perceived general state of well-being at the time of writing their reflections. This proved a useful point of reference external to the text itself, a user annotation of their own text. Additionally, we calculated the descriptive statistics on the reflection points of the whole group, which provided a group norm against which individual reflection points could be compared.  The majority of the features used in analysis were gener- ated from the reflective text. These included word count, sentence count, words per sentence, verb ratio, and noun ratio. As with the reflection point, we calculated descriptive statistics across the group from individual reflective text fea- tures. Some features (including parts of speech, pronouns, words, word length, and punctuation) were recorded as a frequency distribution for each reflection, and the same for the group as a whole.  We included 2 types of semantic feature distributions. A TF-IDF analysis provided a measure of uniqueness in the content of reflections. An LDA topic analysis provided an indication of similarity between reflections, as well as a sense of which topics were most significant over the group.  With only a relatively small data set to work with, we chose to trial the AR process with different aspects of the  same data. We hypothesised that an individual record in the dataset would belong to a context if it contained some characteristic of a selected feature, or to the complementary context if it had the absence or opposite characteristic of the same selected feature. We trialled the process by looking at 2 aspects that were subjective in nature, relevant to LA, and pedagogically useful. These were: Progress satisfaction, and self/others focus.  Where possible we utilised computational tools to imple- ment the various steps of the AR process (notably Factorie [10] for POS tagging and LDA, and Scala [11] for other anal- ysis). However, due to time constraints, human supervision was involved and this is outlined below.  3.1 Trial 1: Progress satisfaction The first aspect we analysed was progress satisfaction with  the reflection point as the initial key feature. We expected that a positive polarity of the reflection point would indicate satisfaction with progress, and that a negative polarity of the reflection point would indicate a dissatisfaction.  A classifying function evaluated each reflection point in terms of its deviation from the mean reflection point for the group as a whole. This value was recorded in the results as a ratio to the standard deviation. The function classified all reflections of 0 or more deviation as positive, and vice versa. This yielded 29 reflections classified as positive, and 36 negative.  We took a supervised approach to the identification of anomalies, and reviewed the negatively classified results first by reading the text and assessing it for general negative sen- timent related to progress. This could be implemented com- putationally with sentiment analysis softare. We identified 16 anomalies in the negative results, which we further clas- sified into 3 groups: one with 3 reflections which had insuf- ficient text for a judgement, and the second had 11 reflec- tions that were generally positive but held reflection points very close to the mean. The third group had 2 reflections that appeared to be significant anomalies. Because of the large number of anomalies with points close to the mean, we determined that the classifier function should work from a lower threshold, a slight negative deviation from the mean. We calculated that a 0.2865 deviation would catch all of these anomalies. Significantly, we noted that this new point would not erroneously classify negative reflections as posi- tive. We re-ran the analysis with a split set at 30% of the standard deviation less than the mean. This resulted in an effective split point for the data set at 45.65. The re-run of the analysis resulted in 23 negative reflections and 42 pos- itive reflections. This process of finding the optimum split point could be implemented computationally using a simple error minimisation algorithm.  The positive results were almost universally reflective of progress satisfaction related remarks in the text. Of the 42 reflections classified as positive, we identified 5 anomalies. Four of these were spurious: A duplicate record, one with no text, and 2 that contained irrelevent text (e.g. lazy hol- idays. . . ), leaving only one genuine anomaly. This text was a more complex mix of negative tone, indications of progress being made, but dissatisfaction with a group members con- tribution. Despite the negative undertones, it had reflection point of 76. A portion of the text read: So we had the group meeting today. [name] didnt make it and its been nearly a month since weve all even seen him...To be honest, this  277    group meeting was pretty pointless. . . . . [user: hesnav] Of the 23 negative reflections, we identified 4 anomalies.  Two with no text, one a sarcastic expression: new place to share everything...great! (reflection point of 33), and the other was not significantly positive, but did indicate satis- faction with progress despite having a reflection point of 27. The text read: Im happy with the app functions I created in the last assignment, but I presented in a very average fash- ion. I would have liked to learn more about presenting in [unit] so that I could have expressed that knowledge through my presentation. [user: cuzguz]  The initial analysis of the data resulted in 2 contexts which could be labeled: satisfied with progress and dissatis- fied with progress. Of the 2 identified significant anomalies, both could be recontextualised with a label such as mixed feelings about progress. The other anomalies could be recon- textualised as no relevant information. The final results for this trial are summarised in Table 1.  Table 1: Results for satisfaction with progress Final Context Feature/s Qty Satisfied with progress  Positive reflection point deviation  37  Dissatisfied with progress  Negative reflection point deviation  19  Mixed feelings about progress  Sentiment, reflection point deviation  2  No relevant infor- mation  Reflection point devia- tion, empty or off-topic  7  3.2 Trial 2: Self-others balance The second trial analysed for a focus on self or oth-  ers, the balance between students focusing on themselves or including others in their reflection, with the hypothesis that students who focused solely on themselves would be less likely to perform well as a part of a team. Influenced by Campbell and Pennebakers work on Pronouns [3], we used pronoun distributions as the key feature for initial classifica- tion. We assumed that a focus on self would result in the use of less third person plural pronouns and more first person singular pronouns, and that a focus on others would reflect in greater use of third person plural and less of first person singular pronouns.  Of the total of 65 reflections, 21 of them had no pronoun distribution due to no pronouns being detected in the text. For expediency, we removed these from the data set prior to applying the classifying function, as it did not structurally alter the trial. However, in future these could be analysed for inferred pronoun useage. Reflections written in a text message style, may infer a personal pronoun without actu- ally stating it. e.g. Just finished assignment as opposed to Ive just finished my assignment.  Our initial classification function compared the reflections to the pronoun distribution for the whole group. We used KL divergence to assess the difference between individual reflection pronoun distributions and that of the group dis- tribution. We assumed the group distribution would be an appropriate balance between self and others. This proved correct as the individual reflection with the lowest KL di- vergence (k = 0.0804) contained a good mix of self and oth- ers:. . . I feel like i need to start making it a habit. Otherwise Im going to end up doing it all on one day . . . Anyway as  Table 2: Results for self/others balance Final Context Feature/s Qty Self others bal- anced  KL divergence of pro- noun distribution  9  Self focused first person singular pro- noun - high value  25  Others focused first person singular pro- noun - low value  8  Non focused text content 1 No relevant infor- mation  no distribution 21  far as the group meeting went we discussed how our obser- vations went and we came to an agreement . . .  With supervision deciding the break point, the reflections were separated at a KL divergence of 0.3. As with the first trial, error minimisation algorithms would allow us to com- plete this computationally. The selected split point resulted in 12 balanced reflections, and 32 reflections that were biased to either self or to others.  Of the 12 balanced reflections, we identified 3 anomalies. As our primary objective was to obtain analytics about the learner, we decided to resolve these anomalies by taking an student focused approach, looking for the extent of balance in the students other reflections. If other balanced reflec- tions were found, we aggregated the results and re-calculated the KL divergence. If not, we added the reflection to the unbalanced context. Only 1 reflection was by someone who had written another balanced reflection, so we recalculated the KL divergence for the aggregate and confirmed that was just over the split point (k = 3.1161). We classified the 3 remaining reflections as unbalanced requiring further resolu- tion. The final number of balanced reflections was 9. Despite the manual work in this stage of the process, much if not all of it could be implemented computationally through an it- erative process of expanding the net of analysed reflections based on the writer.  Initially the unbalanced reflections numbered 32, but the 3 balanced anomalies that were reclassified as unbalanced resulted in a total of 35. We treated them all as anomalies as we wanted to know more about their bias. To do this, we applied another classifying function to classify them as self focused or others focused. To determine self focus, we utilised a feature based on the first person singular value of the distribution. Given that the imbalance in the 3 anoma- lies separated from the balanced context was primarily re- lated to the first person singular value, we determined that the split point for this function should be the minimum value of these 3 reflections. This would position them into the self focused group by default. Running this function resulted in 25 reflections classified as self focus, 9 as others focus, and 1 duplicate.  Of the 25 self focused reflections, no further anomalies were detected so these were recontextualised as self focused. A reflection typical of this context is: I just finished a pre- sentation, so I feel great! [user: cobkev] Of the 9 others focused reflections, 1 reflection had no self or others focus and was manually recontextualised as non focus, leaving 8 that were recontextualised as others focused. A reflection typical of this context included text like: . . . We are run- ning behind on creating a roleplay . . . Also we havent heard from two group members in around two weeks, which is re-  278    ally hindering us in terms of how much we can do. [user: rutkod].  The final contexts after all recontextualisation had con- cluded are summarised in Table 2.  3.3 Findings Our 2 trials demonstrated that the AR process has some  benefits over analysis techniques which take a more binary classification approach with subjective data. The informa- tion found through the AR process could be useful to Learn- ing analytics due to its affective nature and learner focus. Rather than software that indicates that a student is gen- erally positive or negative, we have shown the potential for finding students with mixed feelings about their progress, or students that may not be working well in a group. And this has been done with the same sparse data source which was written with neither of the analysis objectives known.  Throughout our documentation of the trials we identified where human supervision was utilised and made suggestions for the way that these steps could be automated. However, a fully automated implementation of AR would be a challeng- ing endeavour. One of the most significant challenges would be to develop a suite of good context-feature models that can be implemented using the AR process. Such a system is likely to require the use of machine learning techniques that learn which models to apply in which circumstances based on historical data that has been evaluated by a human ex- pert. We anticipate that it is likely that human intervention is always likely to be required to some extent in both a data- wrangling and sensemaking [8] capacity.  An automated AR process would also require better data cleaning prior to initial classification. Although duplicated records and records without text did not significantly impact this study, these types of issues could have been a prob- lem with a larger data set, and would need to be mitigated against for an automated approach. Other pre-processing like the pronoun inferrence mentioned above would also need to be implemented.  During our application of AR, we also noted that changes to a learners reflections over time is likely to be useful. Unfortunately, the temporal dimension of this study was very limited (around 3 weeks) and only a few students com- pleted a series of reflections. However , an automated system analysing data over semesters, or even years, would benefit from models which included temporal features.  Unsurprisingly, the learner is central to the AR process and we see a great deal of promise in including user mod- elling in future work. In particular, there are opportunities to explore the interactions between users in a group sce- nario, which could make a significant contribution to LA in the area of group learning.  4. CONCLUSION The subjective and affective features of reflective text can  provide insights on students and their progress that are unique to this type of writing. Our proposal of the AR process as a way of extracting these features from reflec- tive texts showed potential when applied in the context of our study. We believe that Anomaly Recontextualisation holds promise for the enrichment of Learning Analytics and is worthy of future development.  5. REFERENCES [1] D. M. Blei. Probabilistic topic models.  Communications of the ACM, 55(4):7784, Apr. 2012.  [2] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:9931022, Mar. 2003.  [3] R. S. Campbell and J. W. Pennebaker. The secret life of pronouns: flexibility in writing style and physical health. Psychological science, 14(1):6065, Jan. 2003.  [4] C. A. Chinn and W. F. Brewer. The role of anomalous data in knowledge acquisition: A theoretical framework and implications for science instruction. Review of Educational Research, 63(1):149, Jan. 1993.  [5] K. D. Chirema. The use of reflective journals in the promotion of reflection and learning in post-registration nursing students. Nurse Education Today, 27(3):192202, Apr. 2007.  [6] J. E. Dyment and T. S. OConnell. Assessing the quality of reflection in student journals: a review of the research. Teaching in Higher Education, 16(1):8197, Feb. 2011.  [7] S. Joksimovic, D. Gasevic, V. Kovanovic, O. Adesope, and M. Hatala. Psychological characteristics in cognitive presence of communities of inquiry: A linguistic analysis of online discussions. The Internet and Higher Education, 22:110, 2014.  [8] G. Klein, B. Moon, and R. R. Hoffman. Making sense of sensemaking 1: Alternative persepectives. Ieee Intelligent Systems, 21(4):7073, 2006.  [9] S. Kullback and R. A. Leibler. On information and sufficiency. Annals of Mathematical Statistics, 22(1):7986, 1951.  [10] A. McCallum, K. Schultz, and S. Singh. FACTORIE: Probabilistic programming via imperatively defined factor graphs. In Neural Information Processing Systems (NIPS),12491257, 2009.  [11] M. Odersky and al. An Overview of the Scala Programming Language. Technical Report IC/2004/64, EPFL, Lausanne, Switzerland, 2004.  [12] B. Pang and L. Lee. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1135, 2008.  [13] C. Reidsema and P. Mort. Assessing reflective writing: Analysis of reflective writing in an engineering design course. Journal of Academic Language and Learning,3(2):A117A129, 2009.  [14] T. Roelleke and J. Wang. TF-IDF uncovered: a study of theories and probabilities. In SIGIR 08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval, July, 353442, 2008.  [15] S. B. Trickett, J. G. Trafton, and C. D. Schunn. Thats odd! How scientists respond to anomalous data. In Proceedings of the 22nd Annual Conference of the Cognitive Science Society, pages 10541059, 2001.  279      "}
{"index":{"_id":"45"}}
{"datatype":"inproceedings","key":"Ezen-Can:2015:CSD:2723576.2723588","author":"Ezen-Can, Aysu and Grafsgaard, Joseph F. and Lester, James C. and Boyer, Kristy Elizabeth","title":"Classifying Student Dialogue Acts with Multimodal Learning Analytics","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"280--289","numpages":"10","url":"http://doi.acm.org/10.1145/2723576.2723588","doi":"10.1145/2723576.2723588","acmid":"2723588","publisher":"ACM","address":"New York, NY, USA","keywords":"dialogue act modeling, multimodal learning analytics, text-based learning analytics, tutorial dialogue","abstract":"Supporting learning with rich natural language dialogue has been the focus of increasing attention in recent years. Many adaptive learning environments model students' natural language input, and there is growing recognition that these systems can be improved by leveraging multimodal cues to understand learners better. This paper investigates multimodal features related to posture and gesture for the task of classifying students' dialogue acts within tutorial dialogue. In order to accelerate the modeling process by eliminating the manual annotation bottleneck, a fully unsupervised machine learning approach is utilized for this task. The results indicate that these unsupervised models are significantly improved with the addition of automatically extracted posture and gesture information. Further, even in the absence of any linguistic features, a model that utilizes posture and gesture features alone performed significantly better than a majority class baseline. This work represents a step toward achieving better understanding of student utterances by incorporating multimodal features within adaptive learning environments. Additionally, the technique presented here is scalable to very large student datasets.","pdf":"Classifying Student Dialogue Acts with Multimodal Learning Analytics  Aysu Ezen-Can, Joseph F. Grafsgaard, James C. Lester, Kristy Elizabeth Boyer Department of Computer Science  North Carolina State University aezen, jfgrafsg, lester, keboyer@ncsu.edu  ABSTRACT Supporting learning with rich natural language dialogue has been the focus of increasing attention in recent years. Many adaptive learning environments model students natural lan- guage input, and there is growing recognition that these systems can be improved by leveraging multimodal cues to understand learners better. This paper investigates multi- modal features related to posture and gesture for the task of classifying students dialogue acts within tutorial dialogue. In order to accelerate the modeling process by eliminating the manual annotation bottleneck, a fully unsupervised ma- chine learning approach is utilized for this task. The results indicate that these unsupervised models are significantly im- proved with the addition of automatically extracted posture and gesture information. Further, even in the absence of any linguistic features, a model that utilizes posture and gesture features alone performed significantly better than a majority class baseline. This work represents a step toward achieving better understanding of student utterances by incorporating multimodal features within adaptive learning environments. Additionally, the technique presented here is scalable to very large student datasets.  Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Ed- ucation; I.2.7 [Artificial Intelligence]: Natural Language ProcessingDiscourse  General Terms Human Factors  Keywords Text-based learning analytics, Multimodal learning analytics, Tutorial dialogue, Dialogue act modeling  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. LAK 15, March 16 - 20, 2015, Poughkeepsie, NY, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-3417-4/15/03 http://dx.doi.org/10.1145/2723576.2723588 ...$15.00.  1. INTRODUCTION The research community has endeavored for several decades to build effective systems that support learners [44]. Within the learning analytics community, there has been significant work on analyzing students clicking behavior [46], engage- ment [9], interactions with the learning environment [1], and textual analysis [26, 38] to understand how students learn and how best to support them. Textual natural language data is a rich source of information that can support these goals.  Understanding students through their natural language has been the focus of researchers for a broad range of goals includ- ing assessing students science competency [26], identifying exploratory dialogue [16], identifying idea distribution [10] and relating student posts to knowledge creation [8]. For this papers goal of improving natural language interaction in a learning environment, the focus is specifically on iden- tifying dialogue acts, which represent the communicative intentions of each utterance [37, 40]. These dialogue acts have been shown to be correlated with learning [42]. For example, [the task] just means to assign the name right and spaces or no spaces are both questions, and the goal of dialogue act classification is to automatically detect their types. Automatically identifying dialogue acts has long been a goal for dialogue researchers [39], yet only very recently have multimodal features been considered for this task.  Dialogue act classification is a very important area of research not only for automated systems that adapt to learn- ers, but also for understanding the processes that underlie learning. For example, dialogue act analysis can reveal pat- terns that are particularly effective for supporting students with varying levels of self-efficacy [45], different personality profiles [41], and different learner characteristics [29]. For adaptive systems, dialogue act classification is a crucial step toward providing rich natural language within tutoring sys- tems that can bridge the gap between one-on-one tutoring and automated learning environments [17]. Moreover, scal- able dialogue act modeling techniques can be applied across massive student data, lending insights into how people learn at scale.  Multimodal learning analytics incorporate features of dif- ferent categories into the learning analytics tasks [3]. Some categories of multimodal features, including posture [21] and facial expressions indicating confusion [5], have been found useful for dialogue act classification. However, these prior approaches have relied upon supervised machine learning techniques, which suffer from a manual annotation bottleneck that is problematic for scaling these models across domains  280    or even across corpora. Therefore, how to utilize multi- modal features within unsupervised dialogue act models is an important open research question. Unsupervised machine learning approaches hold great promise for addressing this shortcoming by eliminating the need for dialogue act taxon- omy engineering and manual labeling of utterances [11,15]. In addition, the groupings produced by unsupervised models are fully data driven, which may differ from manual labels and provide important insights into the data.  This paper presents the first model to incorporate multi- modal features into an unsupervised dialogue act classifier for the learning analytics domain. Motivated by the impor- tance of analyzing the process of learning rather than the end product only [2], we analyze posture and gesture features of students in the course of tutoring and utilize this information to enhance our understanding of student dialogue. The re- sults demonstrate that information about students posture and gesture significantly improves dialogue act classification performance when judged against gold standard dialogue act labels. Furthermore, analyses show that utilizing solely posture and gesture lead to better performance than majority baseline chance, even in the absence of any linguistic infor- mation about the utterances being classified. This finding highlights the importance of multimodal features for building rich understanding models of student utterances. This work is a step toward developing tutorial dialogue systems that rely on unsupervised models to provide flexible and effective dialogue to support learning. Moreover, the techniques in- vestigated here have broad application for modeling natural language interactions that support learning because the mod- eling does not utilize manual labels: the clustering is fully data-driven and the multimodal features are automatically extracted, making it scalable for massive student data across domains.  2. RELATED WORK A growing body of findings indicates that multimodal features play an important role in learning analytics [3]. Empirical studies suggest that multimodal approaches are promising for assessing learners interaction experience [24]. Research has proceeded to identify relationships between multimodal cues and cognitive-affective states [19] and learning itself [48,49].  The importance of multimodal features in dialogue has been widely observed. For example, gaze and gesture help with modeling the flow of conversation while showing the relation of dialogue acts to turn taking [4]. The importance of nonverbal cues is also widely observed for discovering con- versational patterns [23] and for determining the addressee of an utterance [43].  Specifically, posture has been shown to be promising for recognizing affective states such as boredom, frustration [47] and disengagement [12,33,36]. Automatic tracking of these posture features has improved substantially, allowing extrac- tion of these features both from two-dimensional [12, 36] and three-dimensional [19] video using computer vision tech- niques. Following the line of investigation into postural features, gestural features have also gained attention from the community. Motivated by the cultural influence of ges- tures [28], they have also been studied in the intelligent tutoring systems community [18,33,47]. Gestures are related to student affective states: one-hand-to-face gestures are as- sociated with less negative affect whereas two-hands-to-face gestures are associated with reduced focus [19]. The growing  body of work in posture and gesture motivates research on dialogue incorporating these multimodal features.  From a dialogue perspective, dialogue act classification, the task of inferring the action and intention underlying utterances, has been extensively studied [32,39]. There is a rich body of work on supervised dialogue act classification techniques where a machine learner is trained on manual dialogue act tags. However, the work utilizing nonverbal cues constitutes a very small subset of that larger body of work, with acoustic and prosodic cues [25,32], facial expres- sions [5], pointing gestures [7] and body posture [21] among the modalities that are found promising. There is a much smaller body of work on unsupervised dialogue act modeling, most outside of the educational domain [13,34]. It is crucial to utilize fully data-driven methods for modeling student utterances for rapid development of adaptive systems. This paper is the first to consider these multimodal cues for inclu- sion within unsupervised dialogue act classifiers for learning analytics with the overarching goal of understanding students better [11,15].  3. CORPUS Tutorial dialogue is one of the most effective means of sup- porting human learning and is an important source for textual learning analytics. The work reported in this paper uses a tutorial dialogue corpus collected in a computer-mediated textual environment for task-oriented tutoring of introduc- tory computer science. The corpus consists of student-tutor interactions while collaborating on computer programming problems in the Java programming language (see Table 2 for an excerpt). This corpus reflects effective tutoring, with students correctly answering 49% of missed pretest questions on the posttest and demonstrating positive overall learning gain (p<0.001).  As shown in Figure 1, the interface consists of four panels: the task pane where the tasks to be completed are explained, the code pane in which the students implement their solutions, the output pane where students could see the output of compiling/running their programs, and the dialogue pane which allowed tutor-student textual dialogue.  The multimodal corpus includes 1,443 student dialogue utterances which were manually annotated in prior work (see Table 1) [21]. There are 7 manually labeled dialogue act tags in the corpus: Answer (A, 43.28% the majority baseline), statement (S, 20.46%), acknowledgment (ACK, 20.2%), question (Q, 14.16%), clarification (C, 0.9%), request for feedback (RF, 0.5%) and other utterances (O, 0.5%). Because the focus of this paper is on unsupervised dialogue act classification, these dialogue act tags are only used for evaluation purposes with held-out cross-validation test data, while the models are built on unlabeled data.  The students (n=37) were recorded with Kinect cameras (Figure 2), and the videos were processed to extract posture and gesture features. In prior work, the posture features were calculated from head and torso distances and gesture features include one-hand-to-face and two-hands-to-face and the performance of these algorithms compared to manual tags was 92.4%, indicating high reliability [19].  4. FEATURES The goal of this work is to investigate the extent to which posture and gesture features improve unsupervised dialogue  281    Figure 1: The JavaTutor tutorial dialogue interface with four panels.  Figure 2: The workstation with Kinect depth-sensor, webcam and tutorial dialogue interface.  act models. Because the parallel streams (student coding ac- tivities, multimodal features, and dialogue) offer rich sources of information, we hypothesize that models of student utter- ances highly benefit from utilizing these features. Four sets of features were considered within the experiments. Three of these setslexical, dialogue-context and task featureshave been shown to improve unsupervised dialogue act classifica- tion in prior work [14]. The fourth set consists of multimodal features of posture and gesture.  Lexical Features. Words and punctuation of each student utterance are provided to the model. Because the overarching goal of dialogue act classification is to understand learners effectively in real-time systems, features such as part-of- speech tags which are time-consuming to extract and have been observed not to improve the accuracy of some dialogue act models [6] are omitted, leaving only unigrams and word- orderings for consideration.  Dialogue-Context Features. Four dialogue-context fea- tures shown useful in prior work [21] are included in the model: utterance position in relation to the beginning of the dialogue, utterance length, author of the previous dialogue message (tutor or student), and previous tutor dialogue act.  282    Student Dialogue Act  Example Distr. (%)  Answer (A) pretty good, just a lot of homework  43.28  Statement (S) its very interesting to me  20.46  Acknowledgement (ACK)  alright 20.20  Question (Q) how can the errors be fixed  14.16  Clarification (C) *html messing 0.90 Request for Feedback (RF)  better 0.50  Other (O) haha 0.50  Table 1: Student dialogue act tags, sample utter- ances and their frequencies.  Because in a tutorial dialogue system the tutor moves are system-generated, their dialogue acts are known. We use the previous tutor dialogue act as feature in our models. This type of dialogue history has been shown effective for dialogue act classification [7, 14,27,35].  Task Features. The parallel task stream present in tutorial dialogue is a rich information source that may not be directly represented in the dialogue. This stream consists of task actions, in our case compiling, running of code, changing code and sending messages. Utilizing these features can help capture the whole dialogue in a more comprehensive man- ner. To do this, we use interaction traces between tutors and students to obtain task features that can help the dia- logue act classification task [14]. The programming activities logged throughout the course of tutoring include the previous task action preceding each student utterance (composing an utterance, writing/compiling/running code), the status of the most recent coding action (begin, success, error, stop, input sent), number of messages sent since the beginning of the task, and number of errors present in the student code.  Posture Features. Four posture features are utilized: head distance (distance between camera and head), mid torso, lower torso, and the average of these three features [19] as shown in Figure 3. Approximately eight frames per second were recorded from a Kinect depth camera. However, ut- terances occur less frequently. Because the granularity of posture features are different from granularity of utterances, representation constitutes a challenge. We take the average of the feature values ten seconds before an utterance and ten seconds after the previous utterance, which is the minimum granularity that allows us to observe change in the features.  Gesture Features. The gesture features include two differ- ent hand-to-face features: one-hand-to-face (see Figure 4) and two-hands-to-face (see Figure 5) indicating the hand po- sitions of students [20]. For matching the gesture features to utterances, we count the number of values detected between two utterances within a ten-second frame. For instance, for a particular utterance, the number of times the one-hand- to-face feature gets detected after the previous utterance of that particular utterance is counted which allows us to match gesture features to each utterance.  Student modifies code. Student receives a compile error. One-hand-to-face gesture recognized.  Student : which do i put first [Question] Tutor : try it. [Statement ]  Change in head depth detected. Student receives a compile error.  Tutor : what you had was close. [Statement ] Tutor : go back to that [Statement ]  Student modifies code. Student compiles code successfully.  Student : is the order wrong [Question] Tutor : no, the literal is just [Statement ] Tutor : Players name is [Statement ]  Student modifies code. Tutor : dont put your name [Hint ]  Student runs the code successfully. Tutor : that is excellent. [Positive Feedback ] Tutor : i could tell a lot of learning was going on [Statement ]  Change in mid-depth detected. Student : its very interesting to me [Statement ] Tutor : good. you are good at it. [Statement ] Tutor : try things. make mistakes. learn. [Statement ] Tutor : one more screen. [Statement ]  Table 2: Excerpt of dialogue from the corpus and the corresponding dialogue act tags.  5. METHODOLOGY For unsupervised classification of dialogue acts, we use a framework that calculates similarities between utterances using their longest-common-subsequences (explained later in this section) and then utilize those similarities within k- medoids clustering [14]. For features other than the lexical features (task, dialogue-context and multimodal features) we use Cosine similarity, which captures similarity independent of the length of utterances. K-medoids is a widely-used clustering algorithm that groups utterances according to their closest centroids within clusters [30]. For this algorithm, the number of clusters needs to be selected. k=5 was found to be the optimal number of clusters in prior work by using the Bayesian Information Criterion, which penalizes the number of parameters the model uses [14].  In addition, our prior work for representing dialogue his- tory, which was shown to significantly improve upon the prior performance of unsupervised dialogue act models, is adopted in this work [14]. Specifically, we branch the cluster- ing model by student utterances according to the previous tutor dialogue acts. Nine branches of student utterances are formed, one for each tutor dialogue act. In this way, the student utterances in the training set are clustered while taking the previous tutor move into account. Each branch has student utterances that share the previous tutor dialogue act and therefore are more granular for clustering. Then, clustering is performed within each branch. Note that each student utterance is clustered only with utterances that had the same previous tutor dialogue act.  Classifying test utterances. Once we have the clusters that are produced using the branching and clustering tech- nique in the training set, each unseen utterance from the test  283    Figure 3: Output of the posture algorithm.  set is classified using the model created in training. For each utterance in the test set, we choose the branching that it should follow in the existing model according to its previous tutor dialogue act. Having chosen the branching, the average distance between the target utterance and each cluster in the clustering group is calculated, where the clustering group represents all clusters in that particular branch. The dis- tance from the target utterance to utterances in each cluster is calculated and divided by the number of utterances in each cluster, producing one average distance to each cluster. The closest cluster which has the smallest average distance determines the target utterances dialogue act. For instance, if the previous tutor dialogue act of the test utterance was a statement, then the utterance is modeled within clusters that shared the same previous tutor dialogue act in the training set. The process is depicted in Figure 6 where the student utterance to be classified is ui with its posture and gesture features pi and gi respectively. The branching is done based on the previous tutor utterance of ui (ut1) and the chosen branch is used for clustering ui. Using this framework which has been shown to outperform previous state-of-the-art unsu- pervised dialogue act classifiers [14], the experimental results (Section 6) will demonstrate the additional benefit of using multimodal features for dialogue act classification.  Distance metric. For calculating similarities between utterances, we take word ordering into account to better capture the underlying intentions of each utterance. As an example, consider two utterances I should declare a variable and should I declare a variable. These two utterances have the same set of words when compared with a bag-of-words approach that does not take the order of words into account. However, the first utterance is a statement whereas the latter is a question. To distinguish them, it is necessary to take the word ordering into account. We utilize longest com- mon subsequence [22], shared subsequences of not-necessarily contiguous words between utterances, to calculate the simi- larity between two utterances considering word ordering [14]. Unlike any distance metric that does not exploit utterance  Figure 4: Output of the gesture algorithm showing the one-hand-to-face feature.  ordering information (Cosine, Euclidean, Manhattan, Jac- card), these two sentences are considered different by longest common subsequence. This discriminative power is desirable.  6. EXPERIMENTS The goal of unsupervised dialogue act classification is to group together utterances with the same dialogue act. There are different techniques to accomplish this in an unsupervised way including k-means clustering [34], Dirichlet process mixture model [11] and query-likelihood clustering [13]. For this work we utilized k-medoids clustering because our prior work has established that this technique outperforms its counterparts for our corpus [14].  We hypothesized that including posture and gesture infor- mation would improve dialogue act classification performance significantly. Therefore, we conducted experiments with and without these features. We created unsupervised dialogue act classifiers that utilized posture and gesture features as well as models that did not use these features. To investigate how these models compared to each other, we compared the performance of models with the same test sets. For instance, we compared how well the utterances of a student in the test set were classified using the model having access to multimodal features and using the model that did not take this information into account. In this way, we aim to draw conclusions on the importance of multimodal features for dialogue act classification.  For testing, leave-one-student-out cross-validation was per- formed: for each fold, each students utterances were either all in the test set or all in the training set, but not in both. To evaluate how well the model performed for each unseen utterance, we computed test set accuracy. Test set accuracy calculates how well the clustering model classifies the label of unseen utterances. Accepting the closest cluster as the cluster of the test utterance (as described in Section 5), the majority vote of the cluster was given as the label to the test instance. The average accuracy for the test set was computed as the number of correct classifications divided  284    Figure 5: Output of the gesture algorithm showing the two-hands-to-face feature.  by the number of utterances in the test set. The formula for test set accuracy is as follows where n is the number of utterances in each fold of the test set and ci is the cluster of utterance i (ui):n  i=1majority label of ci = label of ui  n  Because we applied leave-one-student-out cross-validation, we took an average of all students (folds) to report average test set accuracy. The t-tests were conducted comparing each classifiers performance (with and without multimodal features) for each student.  7. RESULTS AND DISCUSSION This section presents experimental results for unsupervised dialogue act classification based on multimodal features. We compared models built separately using posture and gesture features to models that did not have access to this informa- tion. Each comparison in this section was conducted with a one-tailed t-test for n = 37 students. The threshold for statistical reliability was taken as p = 0.05.  The leave-one-student-out cross-validation accuracies with respect to manual dialogue act labels were statistically sig- nificantly better with the addition of posture and gesture features (p < 0.05). The average accuracy for the model without using multimodal features was 61.8% ( = 2) and this number increased to 67% ( = 1.9) with the inclusion of multimodal features, 8% improvement. The confusion matrices for both cases are shown in Figures 7 and 8. Less frequent dialogue acts were eliminated from the confusion ma- trix because the model never predicted those acts (Request for Feedback and Other).  The experimental results show that including posture and gesture features improved unsupervised dialogue act clas- sification performance significantly. For the most frequent dialogue acts (statements, answers, questions and acknowl- edgments), only statements classification accuracy decreased    Tutor utterance ut-1  Student utterance ui  Student posture pi  Student gesture gi   Multimodal  Interaction History   If  u t-1   is    Qu es  tio n   If u t-1 is   Statement   Student  dialogue   act  clusters        Student  dialogue   act  clusters   Student  dialogue   act  clusters   Figure 6: Branching student utterances according to previous tutor dialogue act and choosing which clustering group to use for unseen utterances.  Figure 7: Confusion matrix for the model without posture and gesture (61.8% accuracy).  Figure 8: Confusion matrix for the model with pos- ture and gesture (67.05% accuracy).  285    with multimodal features. In order to gain better insights and understand which dialogue acts benefit more from mul- timodal features, we compared the two models qualitatively.  We observed that for distinguishing questions that are very similar to statements in structure, multimodal features are highly beneficial. In contrast, when multimodal sensors de- tect features that might be indicative of confusion, although students may utter statements, the models decided that they asked questions requesting help. The nature of the corpus is highly influential here: because the students are engaging in dialogue while completing a learning task, nonverbally expressed confusion may relate to the learning task and not necessarily be indicative that the student is expressing a question dialogue act. Table 3 (shown on the next page) de- picts sample utterances which were incorrectly classified with the model that did not utilize posture and gesture features and were corrected with the help of multimodal features. We provide five types of corrections in the table, two of which were more frequently seen: questions misclassified as statements and questions misclassified as acknowledgements. These results show that utilizing posture and gesture fea- tures, the dialogue act classifier became more successful in distinguishing questions. Especially for utterances that were not syntactically questions such as so the computer reads it from right to left, multimodal features helped enrich the information present in the utterance by incorporating information about students posture and gesture. For ac- knowledgements that were corrected from statements with the help of multimodal features, the students were closer to the workstation (according to lower torso distance) and both one-hand-to-face and two-hand-to-face gestures were present.  Table 4 shows sample utterances that caused the dialogue act classification model to be confused with the addition of multimodal features, i.e. utterances that were classified cor- rectly with the model that did not have access to multimodal features but were incorrectly classified with the addition of these features. Most of the misclassifications caused by multimodal features were on questions. Comparison between two models showed that increase in students mid or lower torso depth i.e., students moving farther from the camera, or one-hand-to-face gesture detection increases the chances of the model classifying the utterance as a question because this pattern is seen in other questions as well. Therefore, even though an utterance may have a statement label, ob- serving students moving farther from the computer triggered a question classification. That may be one of the reasons why a decrease in the accuracy of statements was observed when the multimodal features were incorporated.  Another important finding of the experiments is that when posture and gesture were used with no other features, the average cross-validated accuracy was 53.2%, whereas the majority chance baseline was 43%. This finding suggests that, even before knowing the content of an utterance, it is possible to predict the dialogue acts by analyzing multimodal features of students. This information can be especially helpful for systems that aim to provide remedial support without an explicit request from students. Being able to predict what the next dialogue act would be even before the student utters words can be a significant advantage for understanding students.  A notable limitation of the current approach is that collec- tion of posture and gesture data is not yet a fully scalable approach. However, given the continued decrease in the cost  Student Utterances Correctly Classified with the Help of Multimodal Features  ACK utterances misclassified as Q with multimodal features ok so I just ask please give me your name ok I get it now I understand now think I got it I know I was trying to figure out what line it comes from oh i see  S utterances misclassified as Q with multimodal features just pops up in blue closest experience I have to java is playing runexcape sorry to take too long time, I am usually not creative at all so it usually takes long time to think about something but I think I got it totally guessed what I needed to do I am not understanding well that didnt turn out right and the name of the variable is the only other thing I could think of  Table 4: Sample utterances that were incorrectly classified when multimodal features were used but were correctly classified by the model that did not use posture and gesture features.  of high-resolution video equipment, these approaches are expected to become more scalable. Additionally, work in building affect detectors suggest that it might be possible to infer these multimodal events based on streams that do not require expensive sensors [31].  8. CONCLUSION AND FUTURE WORK Understanding and modeling students in learning environ- ments is a crucial step to better support learning. To this end, learning analytics approaches that mine student interactions within learning environments hold great promise. Textual an- alytics, a branch of learning analytics, has been well studied in the literature; however, multimodal features are only just beginning to be explored for developing rich understanding of students within tutorial dialogue. This paper has focused on investigating the extent to which multimodal features of posture and gesture during computer-mediated tutoring improve unsupervised classification of student dialogue acts. The experiments showed that incorporating multimodal fea- tures regarding posture and gesture improved the accuracy of dialogue act models significantly and that it is possible to predict the dialogue act of an upcoming utterance better than majority baseline chance before the utterance is ob- served. Furthermore, detailed inspection of clusters revealed information about which dialogue acts benefit more from the multimodal features. We found that some patterns in fea- tures such as higher lower and mid torso distance indicating students moving farther from the computer can confuse the  286    Sample Student Utterances From Clusters  ACK utterances misclassified as S without multimodal features ok i am getting it that makes sense awesome thank you ok got it got it! I just thought it is an example  Q utterances misclassified as S without multimodal features why is not it prompting me to enter my name are we going to learn how the player enters their name next there is no box to type it in do I have to just means to assign the name write so what happens if I do not put what the java is expecting just comments are just a way to write notes to others to help them understand right how do you run it the run button is not available to press so the computer reads it from right to left name a variable first and then store the value for what I want to call it so the contents that come after string are what exactly declaring a variable so anything that someone types in the comments box that will be used with the scanner would you still put the prompt after those lines of codes or should you move it up to prompt the user right after you print the game name  S utterances misclassified as ACK without multimodal features not exactly sure how to go about this one beautiful!  S utterances misclassified as Q without multimodal features I guess I am not sure what the codes are currently displayed under the java code section does not have a problem quitting  Q utterances misclassified as ACK without multimodal features so you want me to redo number but change the adreamgame name to something else could I type in string the adventure quest or would I need to put in quotes or something so I could have put the system.out.println command on line number and the input statement on line number and it would still work should I do another player input code spaces or no spaces oh so I need to insert it before the scanner player input line  Table 3: Sample utterances that were correctly classified with the help of multimodal features and their incorrect classifications by the model that did not utilize posture and gesture.  dialogue act classifier to classify questions although manual tags indicate statements. In addition, experiments showed that multimodal features are especially helpful for distin- guishing questions that are very similar to statements in structure. These findings are important for understanding student utterances without needing manual annotations.  As the field moves toward richer automatic understanding of student utterances, these models will find broad appli- cation in contexts such as MOOCs and ITSs. Because the unsupervised model does not require manual labeling and the multimodal features are automatically extracted, the approach presented in this paper can be used across massive student data to understand more about whether and how students learn.  In future work, it will be important to continue to build and enrich unsupervised dialogue act classification models in order to understand better how students interact in learning environments. As multimodal data streams from learning  interactions become more common, it will be important to utilize as many information sources as possible, including multimodal features to better understand students, the dy- namics of learning, and therefore to provide more effective learning environments.  9. ACKNOWLEDGMENTS The authors wish to thank the members of the Center for Educational Informatics at North Carolina State Univer- sity for their helpful input. This work is supported in part by the National Science Foundation through Grant DRL- 1007962, IIS-1409639 and the STARS Alliance, CNS-1042468. Any opinions, findings, conclusions, or recommendations ex- pressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the National Science Foundation.  287    10. REFERENCES [1] K. E. Arnold and M. D. Pistilli. Course signals at  Purdue: Using learning analytics to increase student success. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 267270, 2012.  [2] P. Blikstein. Using learning analytics to assess students behavior in open-ended programming tasks. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 110116, 2011.  [3] P. Blikstein. Multimodal learning analytics. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 102106, 2013.  [4] D. Bohus and E. Horvitz. Facilitating multiparty dialog with gaze, gesture, and speech. In Proceedings of the International Conference on Multimodal Interfaces and the Workshop on Machine Learning for Multimodal Interaction, page 5, 2010.  [5] K. E. Boyer, J. Grafsgaard, E. Y. Ha, R. Phillips, and J. C. Lester. An affect-enriched dialogue act classification model for task-oriented dialogue. In Proceedings of the International Conference of the Association for Computational Linguistics, pages 11901199, 2011.  [6] K. E. Boyer, E. Y. Ha, R. Phillips, M. D. Wallis, M. A. Vouk, and J. C. Lester. Dialogue act modeling in a complex task-oriented domain. In Proceedings of the Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 297305. Association for Computational Linguistics, 2010.  [7] L. Chen and B. D. Eugenio. Multimodality and dialogue act classification in the RoboHelper project. In Proceedings of the Annual Meeting of Special Interest Group on Discourse and Dialogue, pages 183192, 2013.  [8] M. M. Chiu and B. Hall. Statistical discourse analysis of online discussions: Informal cognition, social metacognition and knowledge creation. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 217225, 2014.  [9] C. Coffrin, L. Corrin, P. de Barba, and G. Kennedy. Visualizing patterns of student engagement and performance in MOOCs. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 8392, 2014.  [10] E. Coopey, R. B. Shapiro, and E. Danahy. Collaborative spatial classification. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 138142, 2014.  [11] N. Crook, R. Granell, and S. Pulman. Unsupervised classification of dialogue acts using a Dirichlet process mixture model. In Proceedings of the Annual SIGDIAL Meeting on Discourse and Dialogue, pages 341348, 2009.  [12] S. DMello, R. Dale, and A. Graesser. Disequilibrium in the mind, disharmony in the body. Cognition & Emotion, 26(2):362374, 2012.  [13] A. Ezen-Can and K. E. Boyer. Unsupervised classification of student dialogue acts with query-likelihood clustering. In Proceedings of the International Conference on Educational Data Mining,  pages 2027, 2013.  [14] A. Ezen-Can and K. E. Boyer. Combining task and dialogue streams in unsupervised dialogue act models. In Proceedings of the Annual SIGDIAL Meeting on Discourse and Dialogue, pages 113122, 2014.  [15] A. Ezen-Can and K. E. Boyer. Toward adaptive unsupervised dialogue act classification in tutoring by gender and self-efficacy. In Extended Proceedings of the International Conference on Educational Data Mining (EDM), pages 94100, 2014.  [16] R. Ferguson, Z. Wei, Y. He, and S. B. Shum. An evaluation of learning analytics to identify exploratory dialogue in online discussions. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 8593, 2013.  [17] A. C. Graesser, N. K. Person, and J. P. Magliano. Collaborative dialogue patterns in naturalistic one-to-one tutoring. Applied Cognitive Psychology, 9(6):495522, 1995.  [18] J. F. Grafsgaard, K. E. Boyer, R. Phillips, and J. C. Lester. Modeling confusion: facial expression, task, and discourse in task-oriented tutorial dialogue. In Proceedings of the Conference on Artificial Intelligence in Education, pages 98105, 2011.  [19] J. F. Grafsgaard, R. M. Fulton, K. E. Boyer, E. N. Wiebe, and J. C. Lester. Multimodal analysis of the implicit affective channel in computer-mediated textual communication. In Proceedings of the ACM International Conference on Multimodal Interaction, pages 145152, 2012.  [20] J. F. Grafsgaard, J. B. Wiggins, K. E. Boyer, E. N. Wiebe, and J. C. Lester. Embodied affect in tutorial dialogue: Student gesture and posture. In Proceedings of the International Conference on Artificial Intelligence in Education, pages 110, 2013.  [21] E. Y. Ha, J. F. Grafsgaard, C. M. Mitchell, K. E. Boyer, and J. C. Lester. Combining verbal and nonverbal features to overcome the information gap in task-oriented dialogue. In Proceedings of the Annual SIGDIAL Meeting on Discourse and Dialogue, pages 247256, 2012.  [22] D. S. Hirschberg. A linear space algorithm for computing maximal common subsequences. Communications of the ACM, 18(6):341343, 1975.  [23] D. B. Jayagopi and D. Gatica-Perez. Discovering group nonverbal conversational patterns with topics. In Proceedings of the 2009 International Conference on Multimodal Interfaces, pages 36, 2009.  [24] I. Jraidi, M. Chaouachi, and C. Frasson. A dynamic multimodal approach for assessing learners interaction experience. In Proceedings of the International Conference on Multimodal Interaction, pages 271278, 2013.  [25] D. Jurafsky, E. Shriberg, B. Fox, and T. Curl. Lexical, prosodic, and syntactic cues for dialog acts. In Proceedings of the ACL/COLING-98 Workshop on Discourse Relations and Discourse Markers, pages 114120, 1998.  [26] S. P. Leeman-munk, E. N. Wiebe, and J. C. Lester. Assessing elementary students science competency with text analytics. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK),  288    pages 143147, 2014.  [27] D. J. Litman and S. Pan. Designing and evaluating an adaptive spoken dialogue system. User Modeling and User-Adapted Interaction, 12(2-3):111137, 2002.  [28] D. McNeill. Gesture and thought. University of Chicago Press, 2008.  [29] C. M. Mitchell, E. Y. Ha, K. E. Boyer, and J. C. Lester. Learner characteristics and dialogue: Recognizing effective and student-adaptive tutorial strategies. International Journal of Learning Technology (IJLT), 8(4):382403, 2013.  [30] R. T. Ng and J. Han. Efficient and effective clustering methods for spatial data mining. In Proceedings of the International Conference on Very Large Data Bases, pages 144155, 1994.  [31] L. Paquette, R. Baker, M. Sao Pedro, J. Gobert, L. Rossi, A. Nakama, and Z. Kauffman-Rogoff. Sensor-free affect detection for a simulation-based science inquiry learning environment. In Proceedings of the International Conference on Intelligent Tutoring Systems, volume 8474, pages 110, 2014.  [32] V. K. Rangarajan Sridhar, S. Bangalore, and S. Narayanan. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4):407422, 2009.  [33] M. Rodrigo and R. Baker. Comparing learners affect while using an intelligent tutor and an educational game. Research and Practice in Technology Enhanced Learning, 6(1):4366, 2011.  [34] V. Rus, C. Moldovan, N. Niraula, and A. C. Graesser. Automated discovery of speech act categories in educational games. In Proceedings of the International Conference on Educational Data Mining, pages 2532, 2012.  [35] B. Samei, H. Li, F. Keshtkar, V. Rus, and A. C. Graesser. Context-based speech act classification in intelligent tutoring systems. In Proceedings of the International Conference on Intelligent Tutoring Systems, pages 236241, 2014.  [36] J. Sanghvi, G. Castellano, I. Leite, A. Pereira, P. W. McOwan, and A. Paiva. Automatic analysis of affective postures and body motion to detect engagement with a game companion. In Proceedings of the International Conference on Human-Robot Interaction, pages 305311, 2011.  [37] E. Shriberg, A. Stolcke, D. Jurafsky, N. Coccaro, M. Meteer, R. Bates, P. Taylor, K. Ries, R. Martin, and C. Van Ess-Dykema. Can prosody aid the automatic classification of dialog acts in conversational speech Language and Speech, 41(3-4):443492, 1998.  [38] V. Southavilay, K. Yacef, P. Reimann, and R. A. Calvo. Analysis of collaborative writing processes using revision maps and probabilistic topic models. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 3847,  2013.  [39] A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, C. Van Ess-Dykema, and M. Meteer. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339373, 2000.  [40] D. R. Traum. Speech acts for dialogue agents. In Foundations of Rational Agency, pages 169201. Springer, 1999.  [41] A. K. Vail and K. E. Boyer. Adapting to personality over time: Examining the effectiveness of dialogue policy progressions in task-oriented interaction. In Proceedings of the Annual SIGDIAL Meeting on Discourse and Dialogue, pages 4150, 2014.  [42] A. K. Vail and K. E. Boyer. Identifying effective moves in tutoring: On the refinement of dialogue act annotation schemes. In Proceedings of the International Conference on Intelligent Tutoring Systems (ITS), 2014.  [43] K. Van Turnhout, J. Terken, I. Bakx, and B. Eggen. Identifying the intended addressee in mixed human-human and human-computer interaction from non-verbal features. In Proceedings of the International Conference on Multimodal Interfaces, pages 175182, 2005.  [44] K. VanLehn, A. C. Graesser, G. T. Jackson, P. Jordan, A. Olney, and C. P. Rose. When are tutorial dialogues more effective than reading Cognitive Science, 31(1):362, 2007.  [45] J. B. Wiggins, J. F. Grafsgaard, C. M. Mitchell, K. E. Boyer, E. N. Wiebe, and J. C. Lester. Exploring the relationship between self-efficacy and the effectiveness of tutorial interactions. In Proceedings of the 2nd Workshop on AI-supported Education for Computer Science (AIEDCS), pages 3140, 2014.  [46] A. Wolff, Z. Zdrahal, A. Nikolov, and M. Pantucek. Improving retention: Predicting at-risk students by analysing clicking behaviour in a virtual learning environment. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 145149, 2013.  [47] B. Woolf, W. Burleson, I. Arroyo, T. Dragon, D. Cooper, and R. Picard. Affect-aware tutors: recognising and responding to student affect. International Journal of Learning Technology, 4(3):129164, 2009.  [48] M. Worsley. Multimodal learning analytics: enabling the future of learning through multimodal data analysis and interfaces. In Proceedings of the 14th ACM International Conference on Multimodal Interaction, pages 353356, 2012.  [49] M. Worsley and P. Blikstein. Towards the development of multimodal action based assessment. In Proceedings of the International Conference on Learning Analytics Knowledge (LAK), pages 94101, 2013.  289      "}
{"index":{"_id":"46"}}
{"datatype":"inproceedings","key":"Miller:2015:ADP:2723576.2723607","author":"Miller, William L. and Baker, Ryan S. and Labrum, Matthew J. and Petsche, Karen and Liu, Yu-Han and Wagner, Angela Z.","title":"Automated Detection of Proactive Remediation by Teachers in Reasoning Mind Classrooms","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"290--294","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723607","doi":"10.1145/2723576.2723607","acmid":"2723607","publisher":"ACM","address":"New York, NY, USA","abstract":"Among the most important tasks of the teacher in a classroom using the Reasoning Mind blended learning system is proactive remediation: dynamically planned interventions conducted by the teacher with one or more students. While there are several examples of detectors of student behavior within an online learning environment, most have focused on behaviors occurring fully within the context of the system, and on student behaviors. In contrast, proactive remediation is a teacher-driven activity that occurs outside of the system, and its occurrence is not necessarily related to the student's current task within the Reasoning Mind system. We present a sensor-free detector of proactive remediation, which is able to distinguish these activities from other behaviors involving idle time, such as on-task conversation related to immediate learning activities and off-task behavior.","pdf":"      Automated Detection of Proactive Remediation by   Teachers in Reasoning Mind Classrooms     William L. Miller  Reasoning Mind,   Houston, TX  wlmiller@gmail.com          Ryan S. Baker  Teachers College,   Columbia University,  New York, NY   baker2@  exchange.tc.  columbia.edu   Matthew J. Labrum,  Karen Petsche,    Yu-Han Liu  Reasoning Mind,   Houston, TX     Angela Z. Wagner  Human-Computer   Interaction Institute,  Carnegie Mellon   University, Pittsburgh, PA  awagner@cmu.edu   ABSTRACT  Among the most important tasks of the teacher in a classroom using  the Reasoning Mind blended learning system is proactive remediation:  dynamically planned interventions conducted by the teacher with one  or more students. While there are several examples of detectors of  student behavior within an online learning environment, most have  focused on behaviors occurring fully within the context of the system,  and on student behaviors. In contrast, proactive remediation is a  teacher-driven activity that occurs outside of the system, and its  occurrence is not necessarily related to the students current task  within the Reasoning Mind system. We present a sensor-free detector  of proactive remediation, which is able to distinguish these activities  from other behaviors involving idle time, such as on-task conversation  related to immediate learning activities and off-task behavior.    1. INTRODUCTION  In recent years, researchers in learning analytics and educational data  mining have been successful at detecting a range of student behaviors  during the use of online and blended learning systems, including  whether the student is gaming the system [1], engaging in behaviors  not related to the learning task [2], exploring the learning environment  [3], or avoiding help [4]. These detectors in turn have supported both  automated intervention [5,6] and discovery with models analyses  [1,7,8,9]. As these behaviors are manifested entirely within student  interaction with the learning system, it is feasible to detect these  behaviors solely from logs of student interaction with the system.      In recent years, this work has been extended to also include detection  of behaviors not entirely occurring within the system, such as off-task  behavior outside the learning system [10], and students affective  states [9,11]. These results indicate that log files contain a great deal of  information that can be used for inference about constructs and  behaviors beyond just student behaviors within the learning system.    In this paper, we demonstrate that interaction log files can also be used  to make inference about on-task, education-related interactions  between a student and an instructor, completely outside of the learning  environment. In this paper, we focus on proactive remediation,  dynamically planned intervention by the teacher with one or more  students. In a proactive remediation, the teacher decides to provide  help to one or more students on a topic that they are not currently  struggling with, based on evidence that the student(s) need to learn that  topic. The teacher plans such interventions using formative assessment  data provided by the tutoring system. Proactive remediation is  different from the traditional view of on-task conversations in blended  learning [10], where the student discusses the current material being  presented in the tutor, with another student or the teacher.    In this paper, we describe the construction of a detector of proactive  remediation for the Reasoning Mind Genie 2 system [12]. The  Reasoning Mind Genie 2 system is a blended learning mathematics  curriculum for elementary and middle school students (current  offerings cover the second through the sixth grades), which is  implemented within classrooms with a teacher present. Reasoning  Mind combines extensive professional development, a rigorous  curriculum drawing from successful curricular design in Russia, and a  game-like, internet-based interface. Student learning in Reasoning  Mind takes place in RM City, a virtual city where students engage in  learning activities in different buildings. The primary mode of study  for students is Guided Study, wherein they are guided by a  pedagogical agent named Genie through a series of learning  objectives. It is used by approximately 100,000 students a year,  primarily in the Southern United States. The fifth and sixth grade  curricula are core curricula; they replace the traditional mathematics  class and are generally used for the students entire scheduled  mathematics instruction time, usually 3-5 days per week for 45-90  minutes each day.    The teachers role within the Reasoning Mind classroom is a crucial  one; he or she provides vital support to students beyond what is  provided by the online system. Reasoning Mind provides extensive  professional development both in mathematical content knowledge      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies  are not made or distributed for profit or commercial advantage and  that copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than ACM  must be honored. Abstracting with credit is permitted. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee. Request permissions  from Permissions@acm.org.   LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA   Copyright is held by the owner/author(s). Publication rights licensed  to ACM.  ACM 978-1-4503-3417-4/15/03$15.00  http://dx.doi.org/10.1145/2723576.2723607   290          and in effectively running a Reasoning Mind classroom. Within the  professional development materials, teachers are taught that one of the  most important activities of a teacher during a Reasoning Mind  classroom session is proactive remediation. The Genie 2 system  provides rich and detailed student metrics to the teacher, distilled using  learning analytics; the teacher is trained to use these data to plan one- on-one and small group interventions with their students. In order to  further support the teachers in these activities, teachers are assigned  implementation coordinators, who answer any questions the teacher  may have as well as helping them develop classroom strategies.  Implementation coordinators also visit teacher classrooms throughout  the year to observe and give feedback to the teacher on how to teach  with Reasoning Mind more effectively. These implementation  coordinators have been able to help teachers develop proactive  remediation strategies and other strategies for supporting students, but  their visits and services are resource-intensive, and difficult to scale.  By automatically detecting proactive remediation, it may be feasible to  determine how much and when teachers engage in this behavior,  towards giving a greater degree of feedback to teachers without having  to send an implementation coordinator to the school for a day.    At a basic level, one would expect proactive remediation to look  similar to on-task conversation and off-task behavior in the student log  files; all three are likely to involve extended periods of student  inactivity within the system. However, the log activities leading up to  a proactive remediation are likely to be quite different than those  leading to off-task behavior and on-task conversation (as when  comparing off-task behavior and on-task conversation to each other  [cf. 10]), enabling us to distinguish proactive remediation from other  events and behaviors. It would be ideal to use a combination of log  data on both student and teacher interactions in developing these  detectors; however, the teacher interactions are not yet fully  instrumented. In this paper, therefore, we study proactive remediation  working solely from student log files.     2 METHODS  2.1 Data Set  A detector of proactive remediation by teachers was constructed based  on field observations of students in Reasoning Mind and log data from  the Reasoning Mind system which was synchronized to the field  observations.     A recent study using the BROMP (Baker-Rodrigo-Ocumpaugh  Monitoring Protocol [14]) protocol for quantitative field observations  found evidence that students find Reasoning Mind highly engaging;  specifically, this study found high rates of on-task behavior and  engaged concentration among students working in the Reasoning  Mind system [13]. The same method was used in this study to observe  students in a total of six schools for a different purpose, to develop an  automated detector of proactive remediation. The BROMP protocol  has been used in a variety of contexts; as of this writing, there are 129  BROMP-certified coders.   Expert field observers coded student affect and engaged/disengaged  behaviors as students used the learning software, using the BROMP  protocol. The coders used the HART app on a Google Android  handheld computer [11], which enforced the BROMP protocol [14],  an observation protocol developed specifically for the process of  coding behavior and affect during use of educational software.    Observations were conducted during the students regular math class,  where students typically use the Reasoning Mind software. Students  were coded in a pre-chosen order, with each observation focusing on a  specific student, in order to obtain the most representative indication  of student behavior possible. At the beginning of each class, an  ordering of observation was chosen based on the computer   laboratorys layout, and was enforced using the handheld observation  software. Setting up observations took a few minutes at the beginning  of each class.    Each observation lasted up to twenty seconds, with observation time  automatically coded by the handheld observation software. If behavior  was determined before twenty seconds elapsed, the coder moved to the  next observation.   Each observation was conducted using peripheral vision or side- glances to reduce disruption. That is, the observers stood diagonally  behind the student being observed and avoided looking at the student  directly [15,18], in order to make it less clear when an observation was  occurring. This method of observing using peripheral vision was  previously found to be successful for assessing student behavior and  affect, achieving good inter-rater reliability [15,18]. To increase  tractability of both coding and eventual analysis, if two distinct  behaviors were seen during a single observation, only the first  behavior observed was coded. Any behavior involving a student other  than the student currently being observed was not coded.   The observers based their judgment of a students state or behavior on  the student and teachers work context, actions, utterances, facial  expressions, body language, and interactions with others in the room.  These are, broadly, the same types of information used in previous  methods for coding affect [16], and in line with Planalp et al.s [19]  descriptive research on how humans generally identify affect using  multiple cues in concert for maximum accuracy rather than attempting  to select individual cues. Within an observation, each observer coded  behavior with reference to five categories:    On-Task   Off-Task   Proactive Remediation   On-Task Conversation    (which refers to any behavior outside of the coding scheme   or any case where it was impossible to code student behavior)     All coding was conducted by the third, fourth, and fifth authors. These  three coders were previously trained in coding behavior and affect  using the BROMP protocol, and achieved inter-rater reliability with  the trainer of 0.66, 0.72, and 0.83, during training, on par with past  projects [15,16,17,18].    To increase the probability of model generalizability, data was  collected from a diverse sample of students, representative of the  population currently using Reasoning Mind. Five of the six schools  were in the Texas Gulf Coast region. Three of these Texas schools  were in urban locations and served economically disadvantaged  populations (defined as a high proportion of students receiving free or  reduced lunch); of these three, two served predominantly African- American student populations, and one served a predominantly  Hispanic student population. The other two schools in this region were  in suburban locations, one serving mostly White students, and the  other with a mix of student ethnicities; both of these schools had a  lower proportion of economically disadvantaged students. The sixth  school was a rural school in West Virginia, with an economically  disadvantaged, majority White population. See Table 1 for more  detailed information about the observed schools.                   291          Table 1. Regions and demographic information for schools  included in this study.    Region Free/Re duced  Price   Lunch   White African-  American   Hispanic   1 Texas  (Urban)   85% 1% 84% 13%   2 Texas  (Urban)   79% 3% 32% 63%   3 Texas  (Urban)   96% 1% 10% 88%   4 Texas  (Suburban)   48% 24% 50% 17%   5 Texas  (Suburban)   33% 52% 24% 16%   6 West  Virginia  (Rural)   51% 80% 16% 1%     These observations were synchronized with the system logs of the  students working through the Reasoning Mind system, by  synchronizing both the HART application and the Reasoning Mind  system to the same internet time server, leading to synchronization  error of under 1 second. The resulting data set consisted of 4891  distinct observations of student behavior for 408 students, coded by  three observers across six separate days.   After construction of the detectors, they were applied to the log data  for observed classes for the entire 2012-2013 academic year; this data  set was comprised of 2,974,944 actions by 462 students, including 54  students who were not present when the classes were observed, either  because they were absent or because they transferred into the class  after the observations were performed.   2.2 Feature Distillation  For each observation, a clip was computed from the log data which  matched as closely as possible to the observation (20 seconds before  observation entry time to observation entry time) [cf. 11, 9], facilitated  by the log synchronization procedure discussed above. Using the  students activities both within the twenty-second window and  preceding it (but not using the future), 93 features were developed.  Some features  for example, whether an action was correct or not, or  how long the action took  were computed for each action in the clip  and then aggregated across the clip (see next paragraph for details).  Others  for example, the fraction of previous attempts on the current  skill the student has gotten correct  are based on the students  complete activity from the beginning of the school year. A third  category involves the results of other models applied to the student log  (also called discovery with models [cf. 7]). For example, the  probability that the student knows the current skill (from Bayesian  Knowledge Tracing [20]), student carelessness [21], and features of  the students moment-by-moment learning graph [22,8] were all  included as features.   These 93 features were then aggregated across actions in the clip by a  variety of methods, depending on the nature of the feature: mean, min,  max, standard deviation, sum, presence (for example, 1 if there was  any problem item type in the clip), count, and proportion (by count  or by time; for example, what proportion of the actions in the clip   were problem item types, and what proportion of the time within the  clip was spent on problems). The result was a total of 278 features  used to develop a detector of proactive remediation; examples are  given in Table 2.   2.3  Machine Learning Algorithms  Detectors were built for PROACTIVE REMEDIATION, described  above. Detector evaluation was by ten-fold student-level evaluation,  whereby students were randomly split into ten groups and a detector  was developed using data from nine of the groups and then tested on  the remaining group, for each possible combination. Cross validation  at this level reduces concerns about over-fitting to specific students,  and ensures the generalizability of detectors to new students.   Because proactive remediation is a relatively rare occurrence  (proactive remediation represented about 0.8% of all observations),  data were re-sampled (e.g. cloning data within the minority class) to  have more equal class frequencies before machine learning techniques  were applied. However, all calculations of model goodness were  performed on the original data set.   Four algorithms were tried: JRip, J48 decision trees, step regression,  and Nave Bayes. We found that step regression  linear regression  turned into a binary classifier with a step function applied at a pre- chosen threshold  was most successful.  Feature selection was via forward selection. In this selection scheme,  features are added one at a time (starting from the empty set), each  time selecting the feature that most improves cross-validated detector  goodness. For the purposes of feature selection, detector goodness was  defined as the value of A [23] (see description below) as measured on  the original data set. Features are added in the way until no single  feature can be added to further improve the goodness of the detector.  To reduce the potential for over-fitting, a first pass was performed in  which any feature that yielded A below 0.5 in a single-feature model  were removed from the set of possible features.   A and Cohens Kappa [24] were used to assess detector goodness. A  is the probability that, given one example from each class (i.e.  PROACTIVE REMEDIATION and NOT PROACTIVE  REMEDIATION), the model can correctly identify which is which. It  is mathematically equivalent to the area under the ROC curve (AUC)  used in signal detection and to W, the Wilcoxon statistic [23]. A value  of 0.5 for A indicates performance exactly at chance, and a value of 1  indicates perfect performance. In these analyses, A was calculated at  the level of clips, rather than students. A was calculated using Baker  et al. s Simple A calculation code [25], available from  http://www.columbia.edu/~rsb2162/edmtools.html. Cohens kappa is a  measure of the degree to which the detector is better than chance at  identifying which clips involve the behavior of interest. A Kappa of 0  indicates performance at chance (according to the base rate), and a  Kappa of 1 is perfect performance; intermediate values indicate how  much better (as a percentage) the detector is than chance.     3. RESULTS  The detector for proactive remediation appeared successful in terms of  A, with an overall cross validated A of 0.90. When Kappa was  calculated, the results initially appeared poor -- Kappa was 0.06. This  difference was sufficiently surprising that we re-checked A by hand  and found that it had been computed correctly.    Upon further examination, we noted that the average confidence for  proactive remediation was 2.1%, while the average confidence for the  other examples was 0.6%. As such, almost all clips were assessed by  the detector as having confidence below 50%. Confidences that are  systematically too low or high can be adjusted post-hoc by rescaling or  by allowing the threshold to vary (these are mathematically   292          equivalent). In this case, if we choose an optimal threshold of  0.013947 (using the evolutionary equation solver in Microsoft Excel),  a much better Kappa of 0.65 is achieved.    As such, the model is successful at distinguishing proactive  remediation, and can be used for binary decision-making, but cannot  be used with a threshold of 0.5. It must be used with a custom  threshold.   When this detector was applied to the labels for on-task conversation  and off-task behavior, it yielded (non-cross-validated) A values of  0.66 and 0.59, respectively, indicating that the detector is less  successful at distinguishing proactive remediation from those specific  behaviors than it is in general, but that it is still better than chance at  distinguishing proactive remediation from these other two behaviors  that involve pauses in the system.   Table 2 shows the model for proactive remediation. Two of the three  features in this model involve actions on which students take longer  than other students (possibly indicating that they are not at the  computer while those actions wait for input  also seen in off-task  detection [cf. 10]. The other feature involves student performance on  the items in the clip (students who see an item template multiple times  in a row are receiving that item template multiple times because they  are answering incorrectly: in particular, current poor student  performance is a negative predictor of proactive remediation, perhaps  differentiating proactive remediation (which is not based on the  activity immediately preceding the action) from on-task conversation  (which likely is in many cases).      Table 2. The final proactive remediation detector.   Coefficient Feature   +0.007 The maximum value across the clip of the difference  between the normalized action duration for the given  action and the average normalized action duration for  the previous two hours for the current student.   -0.003 The maximum number of times an item template in the  clip has been seen consecutively (up to and including  the clip). Students are often given new variants of the  same item template if they are incorrect on their first  attempt.   +0.002 The maximum value across the clip of the difference  between consecutive changes in the normalized action  duration for strings of three consecutive actions with the  clip.    +0.007 (constant value)       4. ANALYSIS  The detector of proactive remediation can be used for both  intervention and discovery with models analyses [7]. As an example of  a potential use of these detectors, we consider the degree to which  teacher attendance in Reasoning Mind professional development  sessions (which inform teachers on how to effectively use the  Reasoning Mind software to decide when to perform proactive  remediation and effective methods for carrying it out), is correlated  with how often the teacher conducts proactive remediation. To  perform this analysis, proactive remediation model raw predictions   were computed for all clips in the data set, and then averaged for each  teacher. The data contained actions for 462 students across nine  teachers. Attendance data were compiled for RMs Best Practices  Workshops (BPWs). BPWs are workshops covering various areas of  teacher professional development in the context of the Reasoning  Mind classroom, including a significant focus on proactive  remediation. For each of the nine teachers represented in the data set,  BPW attendance for the 2011-2012 and 2012-2013 academic years  was tallied and correlated to the proactive remediation average.   Fig. 1 shows this relationship; the two quantities are positively  correlated, with R2 = 0.50. The three teachers with the lowest numbers  of BPWs attended were first year teachers (and thus could not have  attended BPWs in 2011-2012). If those three teachers are excluded,  the relationship is stronger, with a steeper slope and R2  = 0.73. This  analysis indicates that the occurrence of proactive remediation is  indeed increased by participation in BPWs, a goal of that program.        Figure. 1. Average proactive remediation rate (per teacher) vs.  number of Best Practices Workshops attended in academic years  2011-2012 and 2012-2013. First-year teachers are indicated by an  open square.   5. CONCLUSION  In this paper, we have constructed an automated, sensor-free detector  of proactive remediation within the Reasoning Mind mathematics  curriculum. This model achieves detector goodness of A = 0.90, and  also achieves Kappa of 0.65 after post-hoc threshold adjustment. This  detector represents another demonstration of the power of interaction  log files from online and blended learning systems to support a wide  range of inferences about learning. Past work has demonstrated that  student behaviors that occur outside of the learning system can be  detected [e.g. 10], and that student affect can be inferred [9, 11]. The  work presented here indicates that student log files can even be used to  distinguish which types of student-teacher interactions are occurring.  It is likely that the detector would be even more successful if it  incorporated log files from teacher behaviors; teacher data use is not  currently instrumented, but could be. It would be interesting to study  how much this type of additional instrumentation could contribute to  inferring this behavior.    In this paper, we used the proactive remediation detector in a simple  discovery with models analysis, which showed that attending  Reasoning Minds teacher professional development is associated with  an increase in the occurrence of proactive remediation in teachers  classes, which is one of the goals of the professional development  sessions.   0.76%!  0.80%!  0.84%!  0.88%!  0.92%!  0! 2! 4! 6! 8! 10!  Pr oa  ct iv  e  re  m ed  ia tio  n  ra  te !  Number of BPW Sessions Attended!  Proactive Remediation Rate  vs. BPW Attendance!  293          Future goals for this work involve bringing these detectors on-line for  real-time detection of this behavior; in particular, this detector has the  potential to eventually be part of a system of comprehensive,  automated detectors of teacher efficacy. As a result, we will be able to  create automated interventions for teachers that encourage effective  classroom practices, as well as providing this information to  implementation coordinators, supporting better teacher practice at  lower cost than is currently feasible.   7. ACKNOWLEDGMENTS  We would like to thank George Khachatryan for helpful discussions  and suggestions, and Mark Riedl for suggesting the term proactive  remediation. We would also like to thank Maria Nosovskaya and  Caitlin Watts for their support in data collection, and the Bill and  Melinda Gates Foundation for their generous support for this  collaboration.   8.REFERENCES  1. Baker, R. S., Corbett, A. T., Koedinger, K. R. 2004. Detecting   Student Misuse of Intelligent Tutoring Systems. In Proceedings of  the 7th International Conference on Intelligent Tutoring Systems,  pp.531-540.   2. Rowe, J., McQuiggan, S., Robison, J., Lester, J. 2009. Off-Task  Behavior in Narrative Centered Learning Environments. In  Proceedings of the Fourteenth International Conference on  Artificial Intelligence in Education (AIED-09), pp.99-106.   3. Amershi, S., Conati, C. 2009. Combining Unsupervised and  Supervised Machine Learning to Build User Models for  Exploratory Learning Environments. Journal of Educational Data  Mining 1(1), 18-71.   4. Aleven, V., McLaren, B., Roll, I., Koedinger, K. 2006. Toward  meta-cognitive tutoring: A model of help seeking with a Cognitive  Tutor. International Journal of Artificial Intelligence and  Education (16), 101-128.   5. Arroyo, I., Ferguson, K., Johns, J., Dragon, T., Meheranian, H.,  Fisher, D., Barto, A., Mahadevan, S., Woolf, B. P. 2007.  Repairing Disengagement With Non-Invasive Interventions. In  Proceedings of the 2007 conference on Artificial Intelligence in  Education: Building Technology Rich Learning Contexts That  Work, pp.195-202.   6. Roll, I., Aleven, V., McLaren, B. M., Koedinger, K. R. 2011.  Improving students help-seeking skills using metacognitive  feedback in an intelligent tutoring system. Learning and  Instruction. 21(2), 267-280.   7. Hershkovitz, A., Baker, R. S. J. d., Gobert, J., Wixon, M., Sao  Pedro, M. 2013. Discovery with Models: A Case Study on  Carelessness in Computer-based Science Inquiry. American  Behavioral Scientist. 57(10), 1479-1498.   8. Hershkovitz, A., Baker, R. S. J. d., Gowda, S. M., Corbett, A. T.  2013. Predicting Future Learning Better Using Quantitative  Analysis of Moment-by-Moment Learning. In Proceedings of the  6th International Conference on Educational Data Mining, pp.74-  81.   9. Pardos, Z. A., Baker, R. S. J. d., San Pedro, M. O. C. Z., Gowda,  S. M., Gowda, S. M. 2013. Affective states and state tests:  Investigating how affect throughout the school year predicts end  of year learning outcomes. In Proceedings of the 3rd International  Conference on Learning Analytics and Knowledge, pp.117-124.   10. Baker, R. S. J. d. 2007. Modeling and Understanding Students'  Off-Task Behavior in Intelligent Tutoring Systems. In  Proceedings of ACM CHI 2007: Computer-Human Interaction,  pp.1059-1068.   11. Baker, R. S. J. d., Gowda, S. M., Wixon, M., Kalka, J., Wagner,  A. Z., Salvi, A., Aleven, V., Kusbit, G., Ocumpaugh, J., Rossi, L.  2012. Towards Sensor-free Affect Detection in Cognitive Tutor  Algebra. In Proceedings of the 5th International Conference on  Educational Data Mining,  pp.126-133.   12. Khachatryan, G., Romashov, A., Khachatryan, A., Gaudino, S.,  Khachatryan, J., Guarian, K., Yufa, N. 2014. Reasoning Mind  Genie 2: An Intelligent Learning System as a Vehicle for  International Transfer of Instructional Methods in Mathematics.  International Journal of Artificial Intelligence in Education, 24   (3), 333-382.   13. Ocumpaugh, J., Baker, R. S. J. d., Gaudino, S., Labrum, M.  J.,Dezendorf, T. 2013. Field Observations of Engagement in  Reasoning Mind. In Proceedings of the 16th International  Conference on Artificial Intelligence and Education, pp.624-627.   14. Ocumpaugh, J., Baker, R. S. J. d., Rodrigo, M. M. T. 2012. Baker- Rodrigo Observation Method Protocol (BROMP) 1.0. Training  Manual version 1.0., Technical Report. New York, NY: EdLab.  Manila, Philippines: Ateneo Laboratory for the Learning Sciences.   15. Baker, R. S. J. d., D'Mello, S. K., Rodrigo, M. M. T., Graesser, A.  C. 2010. Better to Be Frustrated than Bored. The. International  Journal of Human-Computer Studies. 68(4), 223-241.   16. Bartel, C. A., Saavedra, R. 2009. The collective construction of  work group models. Administrative Science Quarterly 1(1), 3-17.   17. Litman, D. J., Forbes-Riley, K. 2006. Recognizing Student  Emotions on the Basis of Utterances in Spoken Tutoring  Dialogues with both Human and Computer Tutors. Speech  Communication, 48(5), 559-590.   18. Rodrigo, M. M. T., Baker, R. S. J. d., D'Mello, S., Gonzalez, M.  C. T., Lagud, M. C. V., Lim, S. A. L., Macapanpan, A. F., Pascua,  S. A. M. S., Santillano, J. Q., Sugay, J. O., Tep, S., Viehland, N. J.  B. 2008.  Comparing Learners' Affect While Using an Intelligent  Tutoring Systems and a Simulation Problem Solving Game. In  Proceedings of the 9th International Conference on Intelligent  Tutoring Systems, pp.40-49.   19. Planalp, S., DeFrancisco, V. L., Rutherford, D. 1996.Varieties of  cues to emotion in naturally occurring situations. Cognition and  Emotion, 10(2), 137-153.   20. Corbett, T., A., Anderson, J. 1995. Knowledge Tracing: Modeling  the Acquisition of Procedural Knowledge. User Modeling and  User-Adapted Interaction, 4, 253-278.   21. San Pedro, M., Baker, R., Rodrigo, M. 2011.  Detecting  Carelessness through Contextual Estimation. In Proceedings of  15th International Conference on Artificial Intelligence in  Education, pp.304-311.   22. Baker, R. S. J. d., Goldstein, A. B., Heffernan, N. T. 2011.  Detecting Learning Moment-by-Moment. International Journal of  Artificial Intelligence in Education, 21(1-2), 5-25.   23. Hanley, J., McNeil, B. 1982. The Meaning and Use of the Area  under a Receiver Operating Characteristic (ROC) Curve.  Radiology, 143, 29-36.   24. Cohen, J. 1960.  A coefficient of agreement for nominal scales.  Educational and Psychological Measurement, 20(1), 37-46.    25. Baker, R. S. J. d., Corbett, A. T., Aleven, V. (2008)x . More  Accurate Student Modeling Through Contextual Estimation of  Slip and Guess Probabilities in Bayesian Knowledge Tracing. In  Proceedings of the 9th International Conference on Intelligent  Tutoring Systems, pp.406-415.   294      "}
{"index":{"_id":"47"}}
{"datatype":"inproceedings","key":"Brooks:2015:RSB:2723576.2723614","author":"Brooks, Christopher and Chavez, Omar and Tritz, Jared and Teasley, Stephanie","title":"Reducing Selection Bias in Quasi-experimental Educational Studies","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"295--299","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723614","doi":"10.1145/2723576.2723614","acmid":"2723614","publisher":"ACM","address":"New York, NY, USA","abstract":"In this paper we examine the issue of selection bias in quasi-experimental (non-randomly controlled) educational studies. We provide background about common sources of selection bias and the issues involved in evaluating the outcomes of quasi-experimental studies. We describe two methods, matched sampling and propensity score matching, that can be used to overcome this bias. Using these methods, we describe their application through one case study that leverages large educational datasets drawn from higher education institutional data warehouses. The contribution of this work is the recommendation of a methodology and case study that educational researchers can use to understand, measure, and reduce selection bias in real-world educational interventions.","pdf":"Reducing Selection Bias in Quasi-Experimental Educational Studies  Christopher Brooks School of Information University of Michigan  brooksch@umich.edu  Omar Chavez Department of Statistics and  Data Sciences University of Texas at Austin ochavez@utexas.edu  Jared Tritz School of Information University of Michigan jtritz@umich.edu  Stephanie Teasley School of Information University of Michigan  steasley@umich.edu  ABSTRACT In this paper we examine the issue of selection bias in quasi- experimental (non-randomly controlled) educational stud- ies. We provide background about common sources of se- lection bias and the issues involved in evaluating the out- comes of quasi-experimental studies. We describe two meth- ods, matched sampling and propensity score matching, that can be used to overcome this bias. Using these methods, we describe their application through one case study that leverages large educational datasets drawn from higher ed- ucation institutional data warehouses. The contribution of this work is the recommendation of a methodology and case study that educational researchers can use to understand, measure, and reduce selection bias in real-world educational interventions.  1. INTRODUCTION Evaluating the impact of novel educational pedagogies,  strategies, programs, and interventions in quasi-experimental studies can be highly error-prone due to selection biases. The effect of these errors can be significant, and can lead to harm being done to learners, instructors, and institu- tions through misinformed decision-making. Further, the lack of confidence researchers have in their analyses of real- world deployments can lead to a decrease in situated ex- perimentation. In this paper we describe a methodology to understand and correct for selection bias, restoring the con- fidence researchers and policy-makers can have in the results of quasi-experimental studies.  A quasi-experimental study is one in which there is no ran- domized control population. This design contains the poten- tial for selection bias of learners and is a principal challenge for measuring the effectiveness of the intervention delivered.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 15 March 16 - 20, 2015, Poughkeepsie, NY, USA ACM 978-1-4503-3417-4/15/03 ...$15.00. http://dx.doi.org/10.1145/2723576.2723614.  In educational studies selection bias is often very difficult to eliminate: there are ethical considerations around the equal access to new learning technologies and programs, as well as pragmatic considerations such as dealing with open recruit- ment of learners where the bias is the result of self-selection. For instance, one would expect a series of workshops aimed at helping non-traditional learners excel in first year uni- versity would increase the grades of these students. But students may not elect to attend workshops randomly  stu- dents with existing strong study skills may be more predis- posed to attending the workshops, and this latent variable may be more explanatory of the outcome than the workshop itself.  The big data culture that has permeated academic (as well as other) institutions offers a solution to issues of se- lection bias in quasi-experimental interventions. Instead of limiting learner access to a technology or program to form a control group a priori, a subset of the overall population of learners is selected post hoc such that it best matches the group of learners who received the intervention. This cre- ates a matched sample, and allows for an apples-to-apples comparison of outcomes between the two groups of learners while contextualizing how those groups might differ with re- spect to selection bias.  The work presented here describes a process for identify- ing a matched sample of learners and contextualizing how the matched sample differs from those learners who have received educational interventions. This technique is espe- cially important when communicating research results to de- cision makers within the higher education institution. By comparing the results of a treatment (a learning technology or program) effect on a group of learners against a similarly- matched sample, researchers can control for selection bias and make a more compelling argument about the impact (or lack there of) of their intervention.  The contributions of this work are three fold:  1. A process for evaluating educational programs and in- terventions using subset matching, including an under- standing of the important statistical tests that must be considered when contextualizing how good (unbiased with respect to some attributes) of a match is achieved.  2. A case study demonstrating how this method can be applied to reduce selection bias.  295    3. A free and open source software toolkit1 that allows ed- ucational researchers to execute this process directly, complete with the reporting of contextual statistics about the matched populations.  2. ADDRESSING SELECTION BIAS  2.1 Selection Bias To identify methods to deal with the problem of selection  bias, we first describe what causes an educational interven- tion (treatment effect) to become biased. This bias is typ- ically due to members of the treatment group voluntarily selecting themselves to participate in a given intervention. Other exogenous factors may exist, such as access to technol- ogy to engage in the treatment (e.g. Internet access, access to a smart phone) or selection that is based in part on de- mographic features. Consequently one might ask whether there is some sort of important difference in the individuals who elect to participate in a particular treatment or whether this a completely random phenomenon that is unrelated to the particular individuals. Regardless, we can mitigate the effects that various factors (the observed covariates) might have on the outcome of interest. Specifically, we want to es- timate the extent to which the measured covariates influence our estimate of the average treatment effect. In general, our estimates of the average treatment effect involve matching members of the treatment condition to members of the com- parison group (control) and relies on the strong ignorability assumption [2]. To state plainly: If we observe two individ- uals with the same base set of covariates or same propensity score, then the likelihood that either one would participate in the intervention is the same for both. Thus one electing to use the intervention and the second not is purely coinci- dental, hence comparing the two students outcomes is valid approach. Without this assumption it is impossible to infer all the selection bias has been removed from the estimated treatment effect [5, 2].  This inference in practice is limited to the covariates we are able to measure which inevitably have limitations either due to resources, time or ethical constraints limiting our ability to develop a complete set of potentially relevant factors to control. A natural consequence of this is explained in [3]:  It is important to realize, however, that whether treatments are randomly assigned or not, and no matter how large a sample size one has, a skep- tical observer could always eventually find some variable that systematically differs in the E tri- als and C trials (e.g., length of longest hair on the child) and claim the average difference esti- mates the effect of this variable rather than the causal effect of Treatment. Within the experi- ment there can be no refutation of this claim; only a logical argument explaining that the vari- able cannot causally affect the dependent vari- able or additional data outside the study can be used to counter it.  This statement again points to the need for a researcher, when attempting to establish a causal relationship between  1Available at https://github.com/usaskulc/population_ matching  the application of a treatment and some sort of measured outcome, to use as complete a data set as possible. Covari- ates that are both related to an outcome of interest (such as test scores) as well as the covariates that effect the likeli- hood of an individual opting to participate in the treatment or intervention are relevant and discussed in [1]. Thus we would say selection bias due to some collection of variables X, is the bias that is introduced into our estimate of the average treatment effect, when we fail to account or control for X.  Mathematically we can state this in the following way: Suppose the true treatment effect is Ttrue. Let T(X) be the estimated treatment effect when we do not use X and T(+X) be the estimated treatment effect when we do use X, then:  bias X removed = T(X)  T(+X) (1)  This equation allows us to calculate how much selection bias is introduced as the result of failing to account for a particular covariate or set of covariates X from the data we have available. For example, suppose we were inter- ested in measuring the selection bias a variable such as Math SAT2 would introduce into our estimated treatment effect. We would first take our two groups, treatment and control, and match them based on some list of covariates (e.g. gen- der, socio-economic status, residency status) and not include Math SAT scores. Our estimated Treatment effect would be T1. We then would repeat the analysis but this time include Math SAT scores to get a second estimate of the treatment effect T2. The difference between T1 and T2 is our point estimate of the bias introduced by failing to include Math SAT as a control variable.  For a full discussion of how various types of coavariates in education can account for selection bias see [5]. To sum- marize their findings: When it comes to which variables to use in ones analysis, it is important to select data that was known before the treatment was administered, or at least could have been known before the treatment was adminis- tered. Using data collected after treatment assignment that can be influenced (changed in value) by the treatment it- self is not useful since they can introduce bias either by causing an over or underestimate of the treatment effect. In educational research, variables relating to demograph- ics, pretest information, prior academic achievement, topic and subject matter preferences, as well as psychological and personality predispositions, have been shown to affect ei- ther observed performance or propensity to participate in interventions. For instance, of these variables Shadish et al. [4] have found when it comes to interventions variables on proxy-pretests and topic preference together with demo- graphics reduce nearly all bias for language-related outcomes and variables related to demographics, pretests. Further, prior academic achievement reduced about 75% of selection bias in a mathematics related intervention. It is worth not- ing however, the actual reductions in bias could also be due to the specific context of the intervention. However, the findings do provide supporting evidence that estimating treatment effects with observational data is an appropriate approach.  2The Math SAT is a standardized test measuring the math- ematics ability of entry level college students in the United States.  296    2.2 Methods for Subset Matching The question then arises as to which matching method  best deals with the problem of selection bias. Should we match equally across all of the covariate measures that we have available, or should we use a univariate statistic that describes the propensity by which an individual relates to the treatment group Rosenbaum and Rubin [2] provide advice on this issue, and suggest that using the covariates directly or propensity scores are both sufficient and neither is clearly better than the other on this matter. Instead, it is how related the set of all covariates which are used for matching is to the treatment assignment or outcome of interest which is important.  With this caveat in mind, we outline two popular methods for finding a matched population for a quasi-experimental study. The first is a simple matching strategy, whereby scores are calculated for each covariate and each pair of subjects in the treatment and condition groups. A vector of scores for a particular pair of individuals represents the difference between subjects, and various differencing meth- ods (e.g. Euclidean distance, Mahalanobis distance) may be used depending upon the form and distribution of the data.  The second method is to collapse covariates for each in- dividual into a propensity score using a regression approach such as linear or logistic regression. The result is then a single value that describes the likelihood an individual would receive some treatment condition. Care must be taken when forming propensity scores, especially in large matching datasets where the number and diversity of non-treatment individuals outweighs that of the treatment individuals. The difference between two individuals scores forms a metric by which individuals can be matched.  Regardless of the method used, both approaches form a matrix of treatment versus non-treatment individuals where intersection elements hold the similarity two individuals have to one another. This matrix can be solved as a linear as- signment problem with the result being globally minimal (most similar) pairwise matches between the treatment and non-treatment populations.  2.3 Reporting on the Effects of Selection Bias While the subset matching technique attempts to mini-  mize the overall difference between the treatment group and a matched sample, such an approach does not guarantee that suitable matches for a given analysis can be found. It is thus important to verify how well matched the treatment group is to the non-treatment group when presenting results on the effect of the treatment. This can be done by considering the similarity of each of the covariate distributions between the treatment and non-treatment groups.  While there are several methods that might be used, a practical approach for continuous data is to compare distri- butions using a two-sampled Kolmogrov-Smirnov test, which is sensitive to both the shape and location of the distribu- tions being compared. A second useful approach is to use the Mann-Whitney test. It has greater efficiency than the t-test on data not sampled from a normal distribution, and it is nearly as efficient as the t-test on normally distributed data. Both are conservative tests that will provide a compre- hensive comparison of the distribution of two populations. In our experience, achieving a significant (e.g. p  0.05) confidence value using the Kolmogrov-Smirnov is difficult unless the non-treatment group is quite large and diverse,  leading to excellent matches. Less robust tests of the qual- ity of matches include means-test methods such the students paired t-test.  3. CASE STUDY: LEARNING COMMUNITIES  The purpose of this section of the paper is not to outline a particular case study result per se, but to demonstrate how the techniques described can be used by educational researchers to come to conclusions about the effect of their interventions, by reducing the possible selection bias of the participatory sample.  Learning communities programs3 at our institution group some students into residences based upon students interest in pursuing a particular domain or discipline. The goal of the learning communities programs is to provide students with a peer group for support, as well as provide opportu- nities for academic development. These programs have ex- isted for more than a decade in various forms, and there is a strong interest in understanding the effect these programs have on student success and achievement. Current learning communities include women students in science and engi- neering programs; students who are interested in the health sciences; students pursuing the visual arts; students who are interested in social justice and community; and students who are interested in research.  Students are not chosen at random for participation in learning communities programs. There is both self-selection bias (e.g. students who are interested in being in the learning community) as well as a formal selection phase (e.g. appli- cation forms included essays which are judged). Students may apply to many learning communities, but can only be accepted into one. Learning communities programs are only available for freshman (first year) university students.  One common question for program evaluators of learning communities is whether participation in the program raises the overall academic achievement of students. A naive ap- proach to answering this issue would be to conduct a t-test between students who are in a particular learning commu- nity and those who were not in any learning community along a particular outcome variable such as overall grade point average. Using one year of such data, the means dif- ference is 0.12 (on a four point scale, see Table 1) suggest- ing the learning community students actually perform worse than non-learning community students; a t-test confirms sig- nificance at p  0.01.  Student Group N average GPA Learning Community 103 3.13 Non-Learning Community 6,090 3.25  Table 1: Comparison of treatment (Learning Com- munity) and non-treatment (Non-Learning Commu- nity) groups using a naive analysis.  In determining how well matched the comparison groups are, a first step is to consider the list of variables being considered and similarity of the distribution of those vari- ables within each group. This can be done with a two-tailed Kolmogrov-Smirnov test, and Table 2 shows the results be- tween the two learning communities groups for a variety of  3See http://www.lsa.umich.edu/mlc  297    variables that are hypothesized as interacting with cumula- tive GPA. For the variables that are statistically significant (e.g. p < 0.01) we cannot reject the null hypothesis that the two samples come from different distributions. In this example, we see that only gender meets this criteria, sug- gesting that the distribution of gender in the two groups is different.  Variables KS Confidence (p) Sex p < 0.001   Ethnic Group p = 0.720 Citizenship Status p = 1.000 Standardized Entrance Test p = 0.987 Credits at Entry p = 0.164 Parental Education p = 0.953 Household Income p = 0.661  Table 2: Comparison of the treatement and non- treatment groups across seven demographic and performance features before matching. In this case it was the treatment group that had a higher num- ber of women than the non-treatment group.  To reduce this bias a matched set can be created. Using the equal covariate matching method described at the begin- ning of Section 2.2, it is possible to minimize the bias that may exist. Balancing across the variables listed, a paired treatmentnon-treatment dataset of 206 individuals can be created. Application of a two-tailed Kolmogrov-Smirnov test shows no significance at p = 0.01 level, though one variable (Household Income) is significant at the p = 0.05 level. The high confidence of all other p-values suggests this dataset is well balanced, except perhaps with respect to parental income.  Variables KS Confidence (p) Sex p = 1.000 Ethnic Group p = 1.000 Citizenship Status p = 1.000 Standardized Entrance Test p = 0.996 Credits at Entry p = 1.000 Parental Education p = 1.000 Household Income p = 0.036  Table 3: Comparison of the treatment and non- treatment groups across seven demographic and performance features after matching.  The result of the matching process are two populations of the same size with individuals in the first directly matched to individuals in the second. Thus, a paired t-test for statisti- cal significance can be used on outcome variables. Consider- ing GPA, the paired t-test returns a statistically significant difference (p = 0.003), with the means difference between the groups being 0.18 points in favor of the non-treatment group. In short, the researcher can now say with greater cer- tainty that there is a difference between the treatment and non-treatment students and that bias introduced because of an observed variables (except perhaps Household Income, at  the p=0.036 level) has been eliminated.4  4. CONCLUSIONS Selection bias in quasi-experimental studies can under-  mine the confidence decision-makers have in the results of analyses, and lead to possible misunderstandings and poor policy decisions. Yet institutions, researchers, and practi- tioners, are often unable to run randomized controlled ex- periments of learning innovations based on pragmatic or eth- ical concerns. This paper has introduced a methodology by which researchers can use contextualize the results of their analysis and reduce selection biases.  Leveraging big educational datasets and institutional data warehouses, researchers can often mitigate selection bias by finding a comparison group of learners who did not undergo a particular treatment. Learners can be compared equally across all covariates (e.g. demographics, previous perfor- mances, or preferences), or covariates can be collapsed into a single propensity score which can be used as the basis for matching. The end result of the matching process is a paired dataset of learners who have undergone a treatment and similar learners who did not receive the treatment. The researcher can then apply post-hoc analysis as appropriate.  In this work we have included an example of this method applied to a case study educational program which is partic- ularly affected by selection bias: university learning commu- nities. These learning communities are heavily biased based on the sex of participants (Table 2). After controlling for this bias, an increase of 66% is seen in the means differences between the treatment and control groups (from 0.12 to 0.18 in GPA units). Whether this is significant enough to change policy or deployment of the program depends on how deci- sion makers weight this particular outcome. There may be alternative student outcomes such as satisfaction, time to degree completion, or co-curricular achievements that influ- ence policy in this area. What is important here is that the researcher can feel confident that these results more accu- rately reflect the effects on the treatment population given the kinds of learners who would opt-in to the treatment.  In our experience, however, creating matched pairs of learners rarely results in perfect results, thus contextual- izing the goodness of fit between the two groups of learners is important. This can be done both before the matching as well as afterwards, using the KolmogrovSmirnov statis- tic. This technique can describe which covariates may not be possible to match on; an insight which is essential when forming educational policy.  4As the level of significance goes up (a declining p value), the more likely it is that variability (noise) in the data will cause for a rejection of a particular hypothesis. As more variables are considered, the chance of spurious correlation at a well-accepted level such as p = 0.05 or p = 0.01 for one variable increases. A more conservative value for a given confidence level can be achieved by dividing the alpha (e.g. 0.05) necessary for statistical significance by the number of variables being considered (7), a Bonferroni correction which controls the family-wise error levels. In this example, at p = 0.05, one would then expect only values of p  0.0083 to be consider statistically significant. Thus the appearance of Household Income being statistically significantly different between the two distributions should be questioned as to whether it is a spurious result.  298    5. ACKNOWLEDGEMENTS Thanks to Dr. Jim Greer from the University of  Saskatchewan for motivating earlier work in this area. Also, thanks to Dr. Ben Hansen at the University of Michigan for insights on using propensity and prognostic scores and their application to matching problems. Finally, thanks to Dr. Brenda Gunderson from the University of Michigan for her support in investigating these issues in the E2Coach frame- work.  6. REFERENCES [1] B. Hansen. The prognostic analogue of the propensity  score. Biometrika, pages 117, 2008.  [2] P. Rosenbaum and D. Rubin. The central role of the propensity score in observational studies for causal  effects. Biometrika, 70(1):4155, 1983.  [3] D. Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies. Journal of educational Psychology, 1974.  [4] W. R. Shadish, M. H. Clark, and P. M. Steiner. Can Nonrandomized Experiments Yield Accurate Answers A Randomized Experiment Comparing Random and Nonrandom Assignments. Journal of the American Statistical Association, 103(484):13341344, Dec. 2008.  [5] P. M. Steiner, T. D. Cook, W. R. Shadish, and M. H. Clark. The importance of covariate selection in controlling for selection bias in observational studies. Psychological methods, 15(3):25067, Sept. 2010.  299    p1-martinez-maldonado  1. INTRODUCTION  2. RELATED WORK  2.1 Learning Analytics Methods for Pedagogical Intervention  2.2 Designing User Interfaces    3. The LATUX Workflow  3.1 LATUX Principles  3.2 LATUX Stages   3.2.1 Stage 1  Problem definition  3.2.2 Stage 2  Low-fidelity prototyping  3.2.3 Stage 3  Higher-fidelity prototyping   3.2.4 Stage 4  Pilot studies - Classroom use  3.2.5 Stage 5  Validation in-the-wild - Classroom use  3.2.6 Iterative Design-Deployment-Evaluation    4. Case Study   4.1 Stage 1 - Problem identification   4.2 Series of studies   4.2.1 Stage 2  Validation of prototypes   4.2.2 Stage 3  Simulations with real instructors   4.2.3 Stage 4  Pilot studies  4.2.4 Stage 5  Classroom use   4.2.5 Iterative Design     5. Conclusions   6. ACKNOWLEDGMENTS  7. REFERENCES   p11-kitto  p16-scheffel  p21-eagle  p31-wang  p36-sanPedro  1. INTRODUCTION  2. METHODS  2.1 Data Source: The ASSISTments System  2.2 Data  2.2.1 College Major Choice Data  2.2.2 Middle School Data from ASSISTments   2.3 Computing Student Knowledge, Engagement, and Affect  2.3.1 Student Knowledge  2.3.2 Affect and Disengaged Behaviors  2.3.3 Carelessness   2.4 Statistical Tests and Modeling   3. RESULTS  3.1 Kruskal-Wallis Test   4. DISCUSSION AND CONCLUSION  5. ACKNOWLEDGMENTS  6. REFERENCES   p41-vogelsang  p51-ferguson  p59-hansen  p64-joksimovic  p69-ferguson  p73-vahdati  p83-prinsloo  p93-aguiar  p103-elbadrawy  p108-asif  p113-davies  1. INTRODUCTION  2. BACKGROUND  2.1 Learning Analytics in Assessment  2.2 Knowledge Components  2.3 Data Logging   3. METHODS  3.1 Participants and Data Collection  3.2 Problem Selection  3.3 Data Cleaning and Coding  3.4 Data Analysis   4. RESULTS AND DISCUSSION  4.1 Knowledge Gap Analysis  4.2 Knowledge Gap Persistency Analysis   5. CONCLUSIONS  5.1 Challenges to Overcome  5.2 Future Research   6. REFERENCES   p118-bergner  p126-brooks  p136-kennedy  p141-harrison  p146-ezen-Can  p151-milligan  p156-joksimovic  p166-gweon  p171-lee  p176-xing  p184-kovanovic  p194-snow  p203-crossley  p208-whitelock  p213-mandran  p218-gross  1. INTRODUCTION  2. EDUCATION ENVIRONMENT  3. RECOGNITION SYSTEM  3.1 Preprocessing  3.2 Image Segmentation  3.3 Character Recognizer  3.4 Word Recognition  3.5 Prior Knowledge   4. DATASETS and RESULTS  5. FUTURE WORK  6. REFERENCES   p223-rogers  1. INTRODUCTION  2. EMPIRICISM  CRITICAL REALISM  CRITICAL NATURALISM  LEARNING ANALYTICS AS A LEARNING SCIENCE   3. IMPLICATIONS FOR LEARNING ANALYTICS  4. CONCLUSION  5. REFERENCES   p231-hsiao  p236-molenaar  p241-knight  p246-allen  p255-pardo  p260-holman  p265-beheshita  p270-mostafavi  p275-gibson  p280-ezen-Can  p290-miller  p295-brooks  <<   /ASCII85EncodePages false   /AllowTransparency false   /AutoPositionEPSFiles true   /AutoRotatePages /None   /Binding /Left   /CalGrayProfile (Gray Gamma 2.2)   /CalRGBProfile (sRGB IEC61966-2.1)   /CalCMYKProfile (U.S. Web Coated 050SWOP051 v2)   /sRGBProfile (sRGB IEC61966-2.1)   /CannotEmbedFontPolicy /Error   /CompatibilityLevel 1.5   /CompressObjects /Off   /CompressPages true   /ConvertImagesToIndexed true   /PassThroughJPEGImages true   /CreateJobTicket true   /DefaultRenderingIntent /Default   /DetectBlends true   /DetectCurves 0.1000   /ColorConversionStrategy /sRGB   /DoThumbnails true   /EmbedAllFonts true   /EmbedOpenType false   /ParseICCProfilesInComments true   /EmbedJobOptions true   /DSCReportingLevel 0   /EmitDSCWarnings false   /EndPage -1   /ImageMemory 1048576   /LockDistillerParams true   /MaxSubsetPct 100   /Optimize true   /OPM 1   /ParseDSCComments true   /ParseDSCCommentsForDocInfo true   /PreserveCopyPage true   /PreserveDICMYKValues true   /PreserveEPSInfo false   /PreserveFlatness true   /PreserveHalftoneInfo false   /PreserveOPIComments false   /PreserveOverprintSettings false   /StartPage 1   /SubsetFonts true   /TransferFunctionInfo /Apply   /UCRandBGInfo /Remove   /UsePrologue false   /ColorSettingsFile ()   /AlwaysEmbed [ true     /AgencyFB-Bold     /AgencyFB-Reg     /Aharoni-Bold     /Algerian     /Andalus     /AngsanaNew     /AngsanaNew-Bold     /AngsanaNew-BoldItalic     /AngsanaNew-Italic     /AngsanaUPC     /AngsanaUPC-Bold     /AngsanaUPC-BoldItalic     /AngsanaUPC-Italic     /Aparajita     /Aparajita-Bold     /Aparajita-BoldItalic     /Aparajita-Italic     /ArabicTypesetting     /Arial-Black     /Arial-BlackItalic     /Arial-BoldItalicMT     /Arial-BoldMT     /Arial-ItalicMT     /ArialMT     /ArialNarrow     /ArialNarrow-Bold     /ArialNarrow-BoldItalic     /ArialNarrow-Italic     /ArialRoundedMTBold     /ArialUnicodeMS     /BaskOldFace     /Batang     /BatangChe     /Bauhaus93     /BellMT     /BellMTBold     /BellMTItalic     /BerlinSansFB-Bold     /BerlinSansFBDemi-Bold     /BerlinSansFB-Reg     /BernardMT-Condensed     /BlackadderITC-Regular     /BodoniMT     /BodoniMTBlack     /BodoniMTBlack-Italic     /BodoniMT-Bold     /BodoniMT-BoldItalic     /BodoniMTCondensed     /BodoniMTCondensed-Bold     /BodoniMTCondensed-BoldItalic     /BodoniMTCondensed-Italic     /BodoniMT-Italic     /BodoniMTPosterCompressed     /BookAntiqua     /BookAntiqua-Bold     /BookAntiqua-BoldItalic     /BookAntiqua-Italic     /BookmanOldStyle     /BookmanOldStyle-Bold     /BookmanOldStyle-BoldItalic     /BookmanOldStyle-Italic     /BookshelfSymbolSeven     /BradleyHandITC     /BritannicBold     /Broadway     /BrowalliaNew     /BrowalliaNew-Bold     /BrowalliaNew-BoldItalic     /BrowalliaNew-Italic     /BrowalliaUPC     /BrowalliaUPC-Bold     /BrowalliaUPC-BoldItalic     /BrowalliaUPC-Italic     /BrushScriptMT     /Calibri     /Calibri-Bold     /Calibri-BoldItalic     /Calibri-Italic     /Calibri-Light     /Calibri-LightItalic     /CalifornianFB-Bold     /CalifornianFB-Italic     /CalifornianFB-Reg     /CalisMTBol     /CalistoMT     /CalistoMT-BoldItalic     /CalistoMT-Italic     /Cambria     /Cambria-Bold     /Cambria-BoldItalic     /Cambria-Italic     /CambriaMath     /Candara     /Candara-Bold     /Candara-BoldItalic     /Candara-Italic     /Castellar     /Centaur     /Century     /CenturyGothic     /CenturyGothic-Bold     /CenturyGothic-BoldItalic     /CenturyGothic-Italic     /CenturySchoolbook     /CenturySchoolbook-Bold     /CenturySchoolbook-BoldItalic     /CenturySchoolbook-Italic     /Chiller-Regular     /ColonnaMT     /ComicSansMS     /ComicSansMS-Bold     /Consolas     /Consolas-Bold     /Consolas-BoldItalic     /Consolas-Italic     /Constantia     /Constantia-Bold     /Constantia-BoldItalic     /Constantia-Italic     /CooperBlack     /CopperplateGothic-Bold     /CopperplateGothic-Light     /Corbel     /Corbel-Bold     /Corbel-BoldItalic     /Corbel-Italic     /CordiaNew     /CordiaNew-Bold     /CordiaNew-BoldItalic     /CordiaNew-Italic     /CordiaUPC     /CordiaUPC-Bold     /CordiaUPC-BoldItalic     /CordiaUPC-Italic     /CourierNewPS-BoldItalicMT     /CourierNewPS-BoldMT     /CourierNewPS-ItalicMT     /CourierNewPSMT     /CurlzMT     /DaunPenh     /David     /David-Bold     /DFKaiShu-SB-Estd-BF     /DilleniaUPC     /DilleniaUPCBold     /DilleniaUPCBoldItalic     /DilleniaUPCItalic     /DokChampa     /Dotum     /DotumChe     /Ebrima     /Ebrima-Bold     /EdwardianScriptITC     /Elephant-Italic     /Elephant-Regular     /EngraversMT     /ErasITC-Bold     /ErasITC-Demi     /ErasITC-Light     /ErasITC-Medium     /EstrangeloEdessa     /EucrosiaUPC     /EucrosiaUPCBold     /EucrosiaUPCBoldItalic     /EucrosiaUPCItalic     /EuphemiaCAS     /FangSong     /FelixTitlingMT     /FootlightMTLight     /ForteMT     /FranklinGothic-Book     /FranklinGothic-BookItalic     /FranklinGothic-Demi     /FranklinGothic-DemiCond     /FranklinGothic-DemiItalic     /FranklinGothic-Heavy     /FranklinGothic-HeavyItalic     /FranklinGothic-Medium     /FranklinGothic-MediumCond     /FranklinGothic-MediumItalic     /FrankRuehl     /FreesiaUPC     /FreesiaUPCBold     /FreesiaUPCBoldItalic     /FreesiaUPCItalic     /FreestyleScript-Regular     /FrenchScriptMT     /Gabriola     /Garamond     /Garamond-Bold     /Garamond-Italic     /Gautami     /Gautami-Bold     /Georgia     /Georgia-Bold     /Georgia-BoldItalic     /Georgia-Italic     /Gigi-Regular     /GillSansMT     /GillSansMT-Bold     /GillSansMT-BoldItalic     /GillSansMT-Condensed     /GillSansMT-ExtraCondensedBold     /GillSansMT-Italic     /GillSans-UltraBold     /GillSans-UltraBoldCondensed     /Gisha     /Gisha-Bold     /GloucesterMT-ExtraCondensed     /GoudyOldStyleT-Bold     /GoudyOldStyleT-Italic     /GoudyOldStyleT-Regular     /GoudyStout     /Gulim     /GulimChe     /Gungsuh     /GungsuhChe     /Haettenschweiler     /HarlowSolid     /Harrington     /HighTowerText-Italic     /HighTowerText-Reg     /Impact     /ImprintMT-Shadow     /InformalRoman-Regular     /IrisUPC     /IrisUPCBold     /IrisUPCBoldItalic     /IrisUPCItalic     /IskoolaPota     /IskoolaPota-Bold     /JasmineUPC     /JasmineUPCBold     /JasmineUPCBoldItalic     /JasmineUPCItalic     /Jokerman-Regular     /JuiceITC-Regular     /KaiTi     /Kalinga     /Kalinga-Bold     /Kartika     /Kartika-Bold     /KhmerUI     /KhmerUI-Bold     /KodchiangUPC     /KodchiangUPCBold     /KodchiangUPCBoldItalic     /KodchiangUPCItalic     /Kokila     /Kokila-Bold     /Kokila-BoldItalic     /Kokila-Italic     /KristenITC-Regular     /KunstlerScript     /LaoUI     /LaoUI-Bold     /Latha     /Latha-Bold     /LatinWide     /Leelawadee     /Leelawadee-Bold     /LevenimMT     /LevenimMT-Bold     /LilyUPC     /LilyUPCBold     /LilyUPCBoldItalic     /LilyUPCItalic     /LucidaBright     /LucidaBright-Demi     /LucidaBright-DemiItalic     /LucidaBright-Italic     /LucidaCalligraphy-Italic     /LucidaConsole     /LucidaFax     /LucidaFax-Demi     /LucidaFax-DemiItalic     /LucidaFax-Italic     /LucidaHandwriting-Italic     /LucidaSans     /LucidaSans-Demi     /LucidaSans-DemiItalic     /LucidaSans-Italic     /LucidaSans-Typewriter     /LucidaSans-TypewriterBold     /LucidaSans-TypewriterBoldOblique     /LucidaSans-TypewriterOblique     /LucidaSansUnicode     /Magneto-Bold     /MaiandraGD-Regular     /MalgunGothic     /MalgunGothicBold     /MalgunGothicRegular     /Mangal     /Mangal-Bold     /Marlett     /MaturaMTScriptCapitals     /Meiryo     /Meiryo-Bold     /Meiryo-BoldItalic     /Meiryo-Italic     /MeiryoUI     /MeiryoUI-Bold     /MeiryoUI-BoldItalic     /MeiryoUI-Italic     /MicrosoftHimalaya     /MicrosoftJhengHeiBold     /MicrosoftJhengHeiRegular     /MicrosoftNewTaiLue     /MicrosoftNewTaiLue-Bold     /MicrosoftPhagsPa     /MicrosoftPhagsPa-Bold     /MicrosoftSansSerif     /MicrosoftTaiLe     /MicrosoftTaiLe-Bold     /MicrosoftUighur     /MicrosoftYaHei     /MicrosoftYaHei-Bold     /Microsoft-Yi-Baiti     /MingLiU     /MingLiU-ExtB     /Ming-Lt-HKSCS-ExtB     /Ming-Lt-HKSCS-UNI-H     /Miriam     /MiriamFixed     /Mistral     /Modern-Regular     /MongolianBaiti     /MonotypeCorsiva     /MoolBoran     /MS-Gothic     /MS-Mincho     /MSOutlook     /MS-PGothic     /MS-PMincho     /MSReferenceSansSerif     /MSReferenceSpecialty     /MS-UIGothic     /MVBoli     /Narkisim     /NiagaraEngraved-Reg     /NiagaraSolid-Reg     /NSimSun     /Nyala-Regular     /OCRAExtended     /OldEnglishTextMT     /Onyx     /PalaceScriptMT     /PalatinoLinotype-Bold     /PalatinoLinotype-BoldItalic     /PalatinoLinotype-Italic     /PalatinoLinotype-Roman     /Papyrus-Regular     /Parchment-Regular     /Perpetua     /Perpetua-Bold     /Perpetua-BoldItalic     /Perpetua-Italic     /PerpetuaTitlingMT-Bold     /PerpetuaTitlingMT-Light     /PlantagenetCherokee     /Playbill     /PMingLiU     /PMingLiU-ExtB     /PoorRichard-Regular     /Pristina-Regular     /Raavi     /RageItalic     /Ravie     /Rockwell     /Rockwell-Bold     /Rockwell-BoldItalic     /Rockwell-Condensed     /Rockwell-CondensedBold     /Rockwell-ExtraBold     /Rockwell-Italic     /Rod     /SakkalMajalla     /SakkalMajallaBold     /ScriptMTBold     /SegoePrint     /SegoePrint-Bold     /SegoeScript     /SegoeScript-Bold     /SegoeUI     /SegoeUI-Bold     /SegoeUI-BoldItalic     /SegoeUI-Italic     /SegoeUI-Light     /SegoeUI-SemiBold     /SegoeUISymbol     /ShonarBangla     /ShonarBangla-Bold     /ShowcardGothic-Reg     /Shruti     /Shruti-Bold     /SimHei     /SimplifiedArabic     /SimplifiedArabic-Bold     /SimplifiedArabicFixed     /SimSun     /SimSun-ExtB     /SnapITC-Regular     /Stencil     /Sylfaen     /SymbolMT     /Tahoma     /Tahoma-Bold     /TempusSansITC     /TimesNewRomanMT-ExtraBold     /TimesNewRomanPS-BoldItalicMT     /TimesNewRomanPS-BoldMT     /TimesNewRomanPS-ItalicMT     /TimesNewRomanPSMT     /TraditionalArabic     /TraditionalArabic-Bold     /Trebuchet-BoldItalic     /TrebuchetMS     /TrebuchetMS-Bold     /TrebuchetMS-Italic     /Tunga     /Tunga-Bold     /TwCenMT-Bold     /TwCenMT-BoldItalic     /TwCenMT-Condensed     /TwCenMT-CondensedBold     /TwCenMT-CondensedExtraBold     /TwCenMT-Italic     /TwCenMT-Regular     /Utsaah     /Utsaah-Bold     /Utsaah-BoldItalic     /Utsaah-Italic     /Vani     /Vani-Bold     /Verdana     /Verdana-Bold     /Verdana-BoldItalic     /Verdana-Italic     /Vijaya     /Vijaya-Bold     /VinerHandITC     /Vivaldii     /VladimirScript     /Vrinda     /Vrinda-Bold     /Webdings     /Wingdings2     /Wingdings3     /Wingdings-Regular     /ZWAdobeF   ]   /NeverEmbed [ true   ]   /AntiAliasColorImages false   /CropColorImages true   /ColorImageMinResolution 150   /ColorImageMinResolutionPolicy /OK   /DownsampleColorImages false   /ColorImageDownsampleType /Bicubic   /ColorImageResolution 150   /ColorImageDepth -1   /ColorImageMinDownsampleDepth 1   /ColorImageDownsampleThreshold 1.50000   /EncodeColorImages false   /ColorImageFilter /DCTEncode   /AutoFilterColorImages true   /ColorImageAutoFilterStrategy /JPEG   /ColorACSImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /ColorImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /JPEG2000ColorACSImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /JPEG2000ColorImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /AntiAliasGrayImages false   /CropGrayImages true   /GrayImageMinResolution 150   /GrayImageMinResolutionPolicy /OK   /DownsampleGrayImages false   /GrayImageDownsampleType /Bicubic   /GrayImageResolution 150   /GrayImageDepth -1   /GrayImageMinDownsampleDepth 2   /GrayImageDownsampleThreshold 1.50000   /EncodeGrayImages true   /GrayImageFilter /DCTEncode   /AutoFilterGrayImages true   /GrayImageAutoFilterStrategy /JPEG   /GrayACSImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /GrayImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /JPEG2000GrayACSImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /JPEG2000GrayImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /AntiAliasMonoImages false   /CropMonoImages true   /MonoImageMinResolution 1200   /MonoImageMinResolutionPolicy /OK   /DownsampleMonoImages false   /MonoImageDownsampleType /Bicubic   /MonoImageResolution 1200   /MonoImageDepth -1   /MonoImageDownsampleThreshold 1.50000   /EncodeMonoImages true   /MonoImageFilter /CCITTFaxEncode   /MonoImageDict <<     /K -1   >>   /AllowPSXObjects true   /CheckCompliance [     /None   ]   /PDFX1aCheck false   /PDFX3Check false   /PDFXCompliantPDFOnly false   /PDFXNoTrimBoxError true   /PDFXTrimBoxToMediaBoxOffset [     0.00000     0.00000     0.00000     0.00000   ]   /PDFXSetBleedBoxToMediaBox true   /PDFXBleedBoxToTrimBoxOffset [     0.00000     0.00000     0.00000     0.00000   ]   /PDFXOutputIntentProfile (None)   /PDFXOutputConditionIdentifier ()   /PDFXOutputCondition ()   /PDFXRegistryName ()   /PDFXTrapped /False    /CreateJDFFile false   /Description <<     /ARA <FEFF0633062A062E062F0645002006470630064700200627064406250639062F0627062F0627062A002006440625064606340627062100200648062B062706260642002000410064006F006200650020005000440046002006450646062706330628062900200644063906310636002006480637062806270639062900200648062B06270626064200200627064406230639064506270644002E00200020064A06450643064600200641062A062D00200648062B0627062606420020005000440046002006270644062A064A0020062A0645002006250646063406270626064706270020062806270633062A062E062F062706450020004100630072006F00620061007400200648002000410064006F00620065002000520065006100640065007200200036002E00300020064806450627002006280639062F0647002E>     /BGR <FEFF04180437043F043E043B043704320430043904420435002004420435043704380020043D0430044104420440043E0439043A0438002C00200437043000200434043000200441044A0437043404300432043004420435002000410064006F00620065002000500044004600200434043E043A0443043C0435043D04420438002C0020043F043E04340445043E0434044F044904380020043704300020043D04300434043504360434043D043E00200440043004370433043B0435043604340430043D0435002004380020043F04350447043004420430043D04350020043D04300020043104380437043D0435044100200434043E043A0443043C0435043D04420438002E00200421044A04370434043004340435043D043804420435002000500044004600200434043E043A0443043C0435043D044204380020043C043E0433043004420020043404300020044104350020043E0442043204300440044F0442002004410020004100630072006F00620061007400200438002000410064006F00620065002000520065006100640065007200200036002E0030002004380020043F043E002D043D043E043204380020043204350440044104380438002E>     /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200036002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>     /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200036002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>     /CZE <FEFF0054006f0074006f0020006e006100730074006100760065006e00ed00200070006f0075017e0069006a007400650020006b0020007600790074007600e101590065006e00ed00200064006f006b0075006d0065006e0074016f002000410064006f006200650020005000440046002000760068006f0064006e00fd006300680020006b0065002000730070006f006c00650068006c0069007600e9006d0075002000700072006f0068006c00ed017e0065006e00ed002000610020007400690073006b00750020006f006200630068006f0064006e00ed0063006800200064006f006b0075006d0065006e0074016f002e002000200056007900740076006f01590065006e00e900200064006f006b0075006d0065006e0074007900200050004400460020006c007a00650020006f007400650076015900ed007400200076002000610070006c0069006b0061006300ed006300680020004100630072006f006200610074002000610020004100630072006f006200610074002000520065006100640065007200200036002e0030002000610020006e006f0076011b006a016100ed00630068002e>     /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200036002e00300020006f00670020006e0079006500720065002e>     /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200036002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>     /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200036002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>     /ETI <FEFF004b00610073007500740061006700650020006e0065006900640020007300e400740074006500690064002c0020006500740020006c0075007500610020005000440046002d0064006f006b0075006d0065006e00740065002c0020006d0069007300200073006f00620069007600610064002000e4007200690064006f006b0075006d0065006e00740069006400650020007500730061006c006400750073007600e400e4007200730065006b0073002000760061006100740061006d006900730065006b00730020006a00610020007000720069006e00740069006d006900730065006b0073002e00200020004c006f006f0064007500640020005000440046002d0064006f006b0075006d0065006e0074006500200073006100610062002000610076006100640061002000760061006900640020004100630072006f0062006100740020006a0061002000410064006f00620065002000520065006100640065007200200036002e00300020006a00610020007500750065006d006100740065002000760065007200730069006f006f006e00690064006500670061002e>     /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200036002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e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a0065006b006b0065006c0020006100200062006500e1006c006c00ed007400e10073006f006b006b0061006c002000fc007a006c00650074006900200064006f006b0075006d0065006e00740075006d006f006b0020006d00650067006200ed007a00680061007400f30020006d00650067006a0065006c0065006e00ed007400e9007300e900720065002000e900730020006e0079006f006d00740061007400e1007300e10072006100200061006c006b0061006c006d00610073002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740075006d006f006b006100740020006b00e90073007a00ed0074006800650074002e002000200041007a002000ed006700790020006c00e90074007200650068006f007a006f007400740020005000440046002d0064006f006b0075006d0065006e00740075006d006f006b00200061007a0020004100630072006f006200610074002000e9007300200061007a002000410064006f00620065002000520065006100640065007200200036002c0030002d0073002000e900730020006b00e9007301510062006200690020007600650072007a006900f3006900760061006c0020006e00790069007400680061007400f3006b0020006d00650067002e>     /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 6.0 e versioni successive.)     /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200036002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>     /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200036002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>     /LTH <FEFF004e006100750064006f006b0069007400650020016100690075006f007300200070006100720061006d006500740072007500730020006e006f0072011700640061006d0069002000730075006b0075007200740069002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c002000740069006e006b0061006d0075007300200076006500720073006c006f00200064006f006b0075006d0065006e00740061006d00730020006b006f006b0079006200690161006b006100690020007000650072017e0069016b007201170074006900200069007200200073007000610075007300640069006e00740069002e002000530075006b00750072007400750073002000500044004600200064006f006b0075006d0065006e007400750073002000670061006c0069006d006100200061007400690064006100720079007400690020007300750020004100630072006f006200610074002000690072002000410064006f00620065002000520065006100640065007200200036002e00300020006200650069002000760117006c00650073006e0117006d00690073002000760065007200730069006a006f006d00690073002e>     /LVI <FEFF004c006900650074006f006a00690065007400200161006f00730020006900650073007400610074012b006a0075006d00750073002c0020006c0061006900200069007a0076006500690064006f00740075002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006100730020007000690065006d01130072006f00740069002000640072006f01610061006900200075007a01460113006d0075006d006100200064006f006b0075006d0065006e0074007500200073006b00610074012b01610061006e0061006900200075006e0020006400720075006b010101610061006e00610069002e00200049007a0076006500690064006f0074006f0073002000500044004600200064006f006b0075006d0065006e00740075007300200076006100720020006100740076011300720074002c00200069007a006d0061006e0074006f006a006f0074002000700072006f006700720061006d006d00750020004100630072006f00620061007400200075006e002000410064006f00620065002000520065006100640065007200200036002e003000200076006100690020006a00610075006e0101006b0075002000760065007200730069006a0075002e>     /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 6.0 en hoger.)     /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200036002e003000200065006c006c00650072002e>     /POL <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>     /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200036002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>     /RUM <FEFF005500740069006C0069007A00610163006900200061006300650073007400650020007300650074010300720069002000700065006E007400720075002000610020006300720065006100200064006F00630075006D0065006E00740065002000410064006F006200650020005000440046002000610064006500630076006100740065002000700065006E007400720075002000760069007A00750061006C0069007A006100720065002000640065002000EE006E00630072006500640065007200650020015F0069002000700065006E00740072007500200069006D007000720069006D006100720065006100200064006F00630075006D0065006E00740065006C006F007200200064006500200061006600610063006500720069002E00200044006F00630075006D0065006E00740065006C00650020005000440046002000630072006500610074006500200070006F00740020006600690020006400650073006300680069007300650020006300750020004100630072006F0062006100740020015F0069002000410064006F00620065002000520065006100640065007200200036002E003000200073006100750020007600650072007300690075006E006900200075006C0074006500720069006F006100720065002E>     /RUS <FEFF04180441043F043E043B044C043704430439044204350020044D044204380020043F043004400430043C043504420440044B0020043F0440043800200441043E043704340430043D0438043800200434043E043A0443043C0435043D0442043E0432002000410064006F006200650020005000440046002C0020043F043E04340445043E0434044F04490438044500200434043B044F0020043D0430043404350436043D043E0433043E0020043F0440043E0441043C043E044204400430002004380020043F043504470430044204380020043104380437043D04350441002D0434043E043A0443043C0435043D0442043E0432002E00200421043E043704340430043D043D044B043500200434043E043A0443043C0435043D0442044B00200050004400460020043C043E0436043D043E0020043E0442043A0440044B0442044C002C002004380441043F043E043B044C04370443044F0020004100630072006F00620061007400200438002000410064006F00620065002000520065006100640065007200200036002E00300020043B04380431043E00200438044500200431043E043B043504350020043F043E04370434043D043804350020043204350440044104380438002E>     /SKY <FEFF0054006900650074006f0020006e006100730074006100760065006e0069006100200073006c00fa017e006900610020006e00610020007600790074007600e100720061006e0069006500200064006f006b0075006d0065006e0074006f007600200076006f00200066006f0072006d00e100740065002000410064006f006200650020005000440046002c0020006b0074006f007200e90020007300fa002000760068006f0064006e00e90020006e0061002000730070006f013e00610068006c0069007600e90020007a006f006200720061007a006f00760061006e006900650020006100200074006c0061010d0020006f006200630068006f0064006e00fd0063006800200064006f006b0075006d0065006e0074006f0076002e002000200056007900740076006f00720065006e00e900200064006f006b0075006d0065006e0074007900200076006f00200066006f0072006d00e10074006500200050004400460020006a00650020006d006f017e006e00e90020006f00740076006f00720069016500200076002000700072006f006700720061006d00650020004100630072006f0062006100740020006100200076002000700072006f006700720061006d0065002000410064006f006200650020005200650061006400650072002c0020007600650072007a0069006900200036002e003000200061006c00650062006f0020006e006f007601610065006a002e>     /SLV <FEFF005400650020006E006100730074006100760069007400760065002000750070006F0072006100620069007400650020007A00610020007500730074007600610072006A0061006E006A006500200064006F006B0075006D0065006E0074006F0076002000410064006F006200650020005000440046002C0020007000720069006D00650072006E006900680020007A00610020007A0061006E00650073006C006A006900760020006F0067006C0065006400200069006E0020007400690073006B0061006E006A006500200070006F0073006C006F0076006E0069006800200064006F006B0075006D0065006E0074006F0076002E0020005500730074007600610072006A0065006E006500200064006F006B0075006D0065006E0074006500200050004400460020006A00650020006D006F0067006F010D00650020006F00640070007200650074006900200073002000700072006F006700720061006D006F006D00610020004100630072006F00620061007400200069006E002000410064006F00620065002000520065006100640065007200200036002E003000200074006500720020006E006F00760065006A01610069006D0069002E>     /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200036002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>     /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200036002e00300020006f00630068002000730065006e006100720065002e>     /TUR <FEFF0130015f006c006500200069006c00670069006c0069002000620065006c00670065006c006500720069006e0020006700fc00760065006e0069006c0069007200200062006900e70069006d006400650020006700f6007200fc006e007400fc006c0065006e006d006500730069006e0065002000760065002000790061007a0064013100720131006c006d006100730131006e006100200075007900670075006e002000410064006f006200650020005000440046002000620065006c00670065006c0065007200690020006f006c0075015f007400750072006d0061006b0020006900e70069006e00200062007500200061007900610072006c0061007201310020006b0075006c006c0061006e0131006e002e0020004f006c0075015f0074007500720075006c0061006e002000500044004600200064006f007300790061006c0061007201310020004100630072006f006200610074002000760065002000410064006f00620065002000520065006100640065007200200036002e003000200076006500200073006f006e00720061006b00690020007300fc007200fc006d006c0065007200690079006c00650020006100e70131006c006100620069006c00690072002e>     /UKR <FEFF04120438043A043E0440043804410442043E043204430439044204350020044604560020043F043004400430043C043504420440043800200434043B044F0020044104420432043E04400435043D043D044F00200434043E043A0443043C0435043D044204560432002000410064006F006200650020005000440046002C0020043F044004380437043D043004470435043D0438044500200434043B044F0020043D0430043404560439043D043E0433043E0020043F0435044004350433043B044F04340443002004560020043404400443043A0443002004340456043B043E04320438044500200434043E043A0443043C0435043D044204560432002E0020042104420432043E04400435043D04560020005000440046002D0434043E043A0443043C0435043D044204380020043C043E0436043D04300020043204560434043A04400438043204300442043800200437043000200434043E043F043E043C043E0433043E044E0020043F0440043E043304400430043C04380020004100630072006F00620061007400200456002000410064006F00620065002000520065006100640065007200200036002E00300020044204300020043F04560437043D04560448043804450020043204350440044104560439002E>     /ENU (Use these settings to create Adobe PDF documents suitable for reliable viewing and printing of business documents.  Created PDF documents can be opened with Acrobat and Adobe Reader 6.0 and later.)   >> >> setdistillerparams <<   /HWResolution [600 600]   /PageSize [612.000 792.000] >> setpagedevice       "}
{"index":{"_id":"48"}}
{"datatype":"inproceedings","key":"Jimenez-Gomez:2015:DCA:2723576.2723597","author":"Jim'enez-G'omez, Manuel 'Angel and Luna, Jos'e Mar'ia and Romero, Crist'obal and Ventura, Sebasti'an","title":"Discovering Clues to Avoid Middle School Failure at Early Stages","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"300--304","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723597","doi":"10.1145/2723576.2723597","acmid":"2723597","publisher":"ACM","address":"New York, NY, USA","keywords":"drop-out, early prediction, educational data mining, school failure","abstract":"The use of data mining techniques in educational domains helps to find new knowledge about how students learn and how to improve the resources management. Using these techniques for predicting school failure is very useful in order to carry out actions to avoid drop out. With this purpose, we try to determine the earliest stage when the quality of the results allows for clarifying the possibility of school failure. We process real information from a Spanish high school by structuring the whole data in incremental datasets, which represent how students' academic records grow. Our study reveals an early and robust detection of the risky cases of school failure at the end of the first out of four courses.","pdf":"Discovering Clues to Avoid Middle School Failure at Early Stages  Manuel ngel Jimnez-Gmez  Department of Computer Science and Numerical Analysis, University of  Cordoba, 14071 Cordoba, Spain  mjimenez@uco.es  Jos Mara Luna Department of Computer Science and Numerical Analysis, University of  Cordoba, 14071 Cordoba, Spain  jmluna@uco.es  Cristbal Romero Department of Computer Science and Numerical Analysis, University of  Cordoba, 14071 Cordoba, Spain  cromero@uco.es  Sebastin Ventura   Department of Computer Science and Numerical Analysis, University of  Cordoba, 14071 Cordoba, Spain  sventura@uco.es  ABSTRACT The use of data mining techniques in educational domains helps to find new knowledge about how students learn and how to improve the resources management. Using these techniques for predicting school failure is very useful in order to carry out actions to avoid drop out. With this purpose, we try to determine the earliest stage when the quality of the results allows for clarifying the possibility of school failure. We process real information from a Spanish high school by structuring the whole data in incremental datasets, which represent how students academic records grow. Our study reveals an early and robust detection of the risky cases of school failure at the end of the first out of four courses.  Keywords Educational data mining, early prediction, school failure, drop-out  1. INTRODUCTION The application of data mining (DM) techniques with infor- mation produced in educational contexts has originated an emerging discipline called Educational Data Mining (EDM).  Dr. S. Ventura also belongs to Department of Computer Science, King Abdulaziz University, Saudi Arabia Kingdom.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. LAK 15, March 16 - 20, 2015, Poughkeepsie, NY, USA Copyright 2015 ACM 978-1-4503-3417-4/15/03 $15.00 http://dx.doi.org/10.1145/2723576.2723597  This new perspective consists in the development and appli- cation of methods to explore educational data, and the use of techniques that brings a better understanding of students behaviour and learning environments [8].  Improving academic performance is among the main tasks undertaken by the EDM. This task may be separated into simpler ones like trying to find the keys of dropping-out [1] or the prediction of academic performance [2]. In turn, these subtasks are solved with standard, descriptive and predic- tive, DM techniques at a lower level: association rule mining, classification, regression or clustering [6].  Regardless of the methods used, most existing EDMworks in the literature are based on datasets generated in e-learning environments as LMS (Learning Management System) or adaptive systems [5], and almost always, related with uni- versity courses [8]. There are also few studies on the antic- ipation, or the amount of information needed for a robust prediction of academic performance. Studies often supple- ment their data with results of surveys, questionnaires or information obtained from other sources [2].  With these precedents, the aim of this work is the detection of school failure in a pre-university stage in order to carry out actions to avoid drop out. Thus, the degree of anticipa- tion plays a role of great relevance to face up school failure. The information considered in this work is exclusively in- stitutional, courtesy of a Spanish high school, among which include personal data of students, their academic records (grades of every subject at every assessment), as well as their absences.  Given the quarterly frequency with which new data are obtained, this study aims to resolve how far in advance you can determine whether a student will pass the course and, likewise. With this purpose, different datasets have  300    been obtained, representing in an incremental way over time, each academic record. On these datasets a series of classi- fication techniques have been applied to determine when it has reached the trade-off between both a good predictor of school failure and enough quantum of time to implement corrective measures.  The results obtained in the experimental study revealed an early and robust detection of school failure. During the first year out of four in this educational stage, interesting re- sults to determine the future students success are obtained and, therefore, there is enough time to take corrective ac- tion. Thus, it is possible to know what to change and with who do it. Useful information is provided to improve the teaching-and-learning process and to optimize human and material resources. This paper is structured as follows: Sec- tion 2 presents a description of the problem, which considers the nature of the information processed, the data selection and the objectives of the study, Section 3 describes clean- ing, preparation and anonymization data. The experimental study is presented in Section 4. Finally, the results obtained are analysed together with concluding remarks.  2. PROBLEM DESCRIPTION The Andalusia government has stored, for more than a decade, information generated in schools via Seneca 1 web applica- tion. Since then, this application has been gradually ex- tended by including new features, so it makes usual the finding of many missing values in previous academic years to 2005. For this reason, only the classes that began the stud- ies in the courses 2005/2006 and 2006/2007 are really inter- esting. These groups are composed of students who over- whelmingly have completed or left this educational stage. Knowing the final results in advance allows the task of pre- dicting school failure by supervised learning techniques to be addressed, such as classification [8], with which it is possible to obtain interpretable predictive models.  The school under study is located in the province of Cor- doba. This is a typical school in terms of number of stu- dents and teachers, educational offer, and socio-economic context. The scores obtained under different statistical in- dicators place it in the Andalusia average.  Management and educational organization may have changed in some shades during the considered time slot, so this study makes sense in those robust and general conclusions are not affected by these changes.  The main factors [4] composing the reality of each student can be very diverse (see Figure 1). The behaviour of in- dividual students toward their course and subjects will be given by factors intrinsic to their person, such as age, sex or nationality, previous knowledge, experience or background, skills and handicaps towards a specific field: attention span, motivation [2] or conveniences to find understanding and in- terest. However, for all students at the school under study, only the following data is stored:  1https://www.juntadeandalucia.es/educacion/ portalseneca/  Figure 1: Incidence factors in academic success   Personal information: name and surname, ID, birth- date, address, nationality, gender and legal guardians.   SNE: Special Needs Education. If a student can present learning difficulties (retardations, disabilities, etc) ed- ucational psychology and medical reports are stored. Just as individual significant adjustments in the cur- riculum.   Contrary to coexistence behaviour. These represent incidents committed by students and that should be considered. Each record contains a description of the incident, negative behaviour, the applied corrections, the teacher who admonishes, date and time, and whether it is an individual or collective warning.   For each academic year: absences, grades (in the inte- ger range 1-10), class group, and number of registra- tions, which indicates whether the student is repeating academic year.   Figure 2 shows the flow of the experiment, and specifies the phases in which the problem has decomposed. The following sections discuss in detail each of these phases.  3. PREPROCESSING Preprocessing phase, independent of the task to do a posteri- ori, includes cleaning and data preparation. Understanding cleaning as removing noise and inconsistencies; and prepara- tion as anonymization of available information [6] and treat- ment of orphan data and missing values.  Usually, data gathered in educational contexts present dis- crete values, either numerical or categorical. The numerical values that we have in this case are integer. Meanwhile, anonymization is essential for achieving a legitimate use of sensitive information. In Spain, the use of these sources of information are limited by the necessary protection of pri- vacy and confidentiality imposed by Law 2. The personal data have been eliminated and identifiers completely disso- ciated.  2Spanish Organic Law 15/1999 of December the 13th of Pro- tection of Personal Data  301    Figure 2: Flow of the experiment  4. EXPERIMENTAL DESIGN The knowledge discovery for improving academic perfor- mance [7] can be decomposed into sub-tasks or questions to answer through the information enclosed in the input data. The entire dataset can be, a priori, interesting for solving the problem, and specific preprocessing will depend on the used technique. In order to get interpretable predictive models, we have considered the use of classification models to pro- mote the prediction in both academic year and obtaining degree at the end of the educational stage. For feature se- lection, we have used ranking given by Relief method [5], an instance-based evaluator.  The experimental design has been based on an incremen- tal dataset (see Figure 3), including, at first step, previous and timeless data such as gender, age, nationality, etc. This dataset will grow term after term, stopping when the accu-  Figure 3: Incremental dataset by terms  racy of classifiers obtained does not significantly improved, achieving enough time for applying corrective actions.  In this way, we have generated seven different datasets cor- responding with information previously available: Dataset #1; at first term of first year: Dataset #2; at second term of first year: Dataset #3; at the end of the first year: Dataset #4; at first term of second year: Dataset #5; at second term of second year: Dataset #6; and finally, at the end of the second year: Dataset #7.  We have chosen WEKA [5] as environment for classifica- tion task. Ten classic techniques, which address the prob- lem from different perspectives, have been used and com- pared. We have selected a Bayesian probabilistic classifier (NaiveBayes), a support-vector machine (SMO), a neural network of radial basis function (RBFNetwork), the Multi- layer Perceptron (MLP), the algorithm of K-nearest neigh- bours (IBk), techniques based on rules and decision trees (JRip, OneR, J48 and PART) and a nominal class classifier based on boosting (AdaBoostM1).  As for the quality measures, we have chosen the percentage of patterns correctly classified (accuracy) and the Cohens kappa, since they are statistics that evaluate the goodness of the classifiers and the differences between all datasets for each algorithm.  5. RESULTS Tables 1 and 2 present the goodness of the obtained classi- fiers to solve the task of predicting whether a student will be graduated. We have conducted several statistical tests [3] in order to determine, from a statistical point of view, whether there are significant differences in predicting school failure in the different specific moments.  Focusing on the results for the accuracy measure (see Ta- ble 1), the Friedman test gets a value of 7.452, which does not belong to the critical interval [0, 2.508] for  = 0.05. Therefore, it could be determined that, with 95% confidence,  302    Table 1: Accuracy (%) of each classifier with every datasets Dataset  Algorithms #1 #2 #3 #4 #5 #6 #7  NaiveBayes 79.28 88.17 88.59 90.69 90.68 89.85 88.18 SMO 79.29 88.64 86.09 89.44 88.11 87.27 89.77  MLP 80.13 84.42 85.67 89.91 88.03 88.03 84.70 RBFNetwork 79.71 87.30 87.28 91.11 87.20 89.02 87.35  IBk 75.53 84.35 86.94 87.77 83.03 85.53 87.12 JRip 79.31 87.74 83.12 85.69 85.61 88.86 84.70 OneR 80.14 81.87 86.96 86.09 90.53 88.03 90.53  J48 81.41 88.55 86.07 86.52 88.11 83.94 85.53 PART 80.58 82.25 84.35 86.94 87.20 85.53 88.11  AdaBoostM1 80.14 88.15 89.93 90.29 88.11 85.68 87.20  Ranking 7.00 4.20 4.30 2.10 3.40 3.65 3.35  Table 2: Cohens Kappa of each classifier with every datasets Dataset  Algorithms #1 #2 #3 #4 #5 #6 #7  NaiveBayes 0.59 0.76 0.77 0.81 0.81 0.80 0.77  SMO 0.59 0.77 0.72 0.79 0.76 0.75 0.80 MLP 0.61 0.69 0.71 0.80 0.76 0.76 0.70 RBFNetwork 0.60 0.75 0.75 0.82 0.75 0.78 0.75  IBk 0.51 0.69 0.74 0.76 0.67 0.72 0.75 JRip 0.59 0.76 0.66 0.71 0.72 0.78 0.70  OneR 0.61 0.64 0.74 0.72 0.81 0.76 0.81 J48 0.63 0.77 0.72 0.73 0.76 0.68 0.71 PART 0.61 0.64 0.69 0.74 0.75 0.71 0.76  AdaBoostM1 0.61 0.76 0.80 0.81 0.76 0.71 0.75  Ranking 7.00 4.30 4.30 2.25 3.05 3.65 3.45  Table 3: Results of Wilcoxon test for accuracy and kappa measurements Accuracy Kappa  Data W+ W- p-value Data W+ W- p-value 4 vs 2 50 5 0.011 4 vs 2 49 6 0.014 4 vs 3 51 4 0.008 4 vs 3 51.5 3.5 0.007  4 vs 5 38 17 0.858 4 vs 5 29.5 15.5 0.203 4 vs 6 42 13 0.070 4 vs 6 40.5 14.5 0.091  4 vs 7 40 15 0.101 4 vs 7 39.5 15.5 0.110  there are significant differences in predicting school failure in different datasets.  Analysing the rankings obtained for the accuracy measure, it is verified that the best results were obtained at the end of first academic year. We have carried out a post-hoc anal- ysis by using the Wilcoxon test to determine if significant differences exist between this stage and the remainder. The results are shown in Table 3. The test revealed that results obtained at the end of first academic year are statistically better than those obtained in previous stages, with  = 0.05. However, we can not demonstrate statistically, that from this point on there are differences in the prediction of school failure. Therefore, the best time to predict school failure is the last assessment of first academic year.  Focusing on Kappa measurement (Table 2), the Friedman statistic determines, with 95% confidence, that can not be said the results of the all terms behave similarly. This statis- tic obtains a value 7,789, not belonging to critical range [0, 2.508] for  = 0.05.  Similarly to analysis of results for the accuracy measure, the best ranking is obtained at the end of first academic year. We have used Wilcoxon test to establish whether there are  significant differences between this moment and the remain- der (Table 3). This test shows, with 95% confidence, the results obtained at the end of first academic year are sta- tistically better than those obtained at any previous time. Furthermore, results of Wilcoxon test determined that, from then on, there were no significant differences in the predic- tion. Thus, we demonstrate the earliest stage to predict school failure is the last assessment of first academic year, and subsequent predictions, in detriment of anticipation, not improve the goodness of classification (see Figure 5).  The required amount of data to solve promoting prediction, of each academic year individually, does not allow for the same statistical study, since only two datasets are considered valid by academic year: the initial data and the results of the first term assessment. After this moment, the lack of anticipation makes irrelevant conclusions from a pedagogical point of view. However, we can obtain valuable rules by some techniques, and we can discover important information published and discussed in this section.  Additionally, we can verify that some algorithms reveal in- terpretable rules (see Figure 4) in which lies the greater rel- evance of some subjects and their link with the final results. Communication skills take the most important role, and it  303    Figure 4: Decision tree and rules produced by J48 and JRip algorithms  Figure 5: Prediction goodness versus anticipation  is logical, since the lack of reading comprehension or expres- sion are an important burden when acquiring other knowl- edge. The fact of students were in the proper grade with their age is also binding, the students who have repeated an academic year usually present bigger difficulties when learn- ing. Early detection of these difficulties, together with the consequent early action, may be key factors that help to im- prove academic performance and optimize the management of available resources.  6. CONCLUDING REMARKS In this work we have carried out a study about the detection of the earliest moment when a robust prediction of school failure is feasible. This prediction is aimed at early inter- vention, in a corrective way, over riskiest students. Thereby it is possible to make better use of available resources, such as tutoring, splitting-groups or reinforcements. It is about closing the cycle, where the new knowledge discovered will be again reversed on improving the educational system and will produce data that will be again studied.  The present work is the first experimental study about a high school in Spain, so it is susceptible to many extensions, such as the volume of data treated, considering other secondary schools, or making wider the set of techniques used on these  data. In subsequent work, we intend to apply descriptive techniques for a deeper analysis with the overall purpose of bringing better understand students and their learning environments.  7. ACKNOWLEDGMENTS Work supported by the Ministry of Science and Technol- ogy project TIN-2011-22408, FEDER funds and the Spanish Ministry of Education under the FPU grant AP2010-0041.  8. REFERENCES [1] P. Cortez and A. Silva. Using data mining to predict  secondary school student performance. In Proceeding of the 15th European Concurrent Engineering Conference/5th Future Business Technology Conference, pages 512, Porto, Portugal, 2008.  [2] J. D. Finn and D. A. Rock. Academic Success Among Students at Risk for School Failure. Journal of Applied Psychology, 82(2):221234, 1997.  [3] S. Garca, D. Molina, M. Lozano, and F. Herrera. A study on the use of non-parametric tests for analyzing the evolutionary algorithms behaviour: A case study on the cec2005 special session on real parameter optimization. Journal of Heuristics, 15(6):617644, Dec. 2009.  [4] H. Gardner. Intelligence Reframed: Multiple Intelligences for the 21st Century. Basic Books, September 2000.  [5] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten. The weka data mining software: An update. SIGKDD Explor. Newsl., 11(1):1018, Nov. 2009.  [6] D. Pyle. Data Preparation for Data Mining. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1999.  [7] C. Romero and S. Ventura. Educational data mining: A review of the state of the art. Transactions on System Man Cybernetics Part C, 40(6):601618, 2010.  [8] C. Romero and S. Ventura. Data mining in education. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 3(1):1227, 2013.  304      "}
{"index":{"_id":"49"}}
{"datatype":"inproceedings","key":"Pardo:2015:COE:2723576.2723625","author":"Pardo, Abelardo and Ellis, Robert A. and Calvo, Rafael A.","title":"Combining Observational and Experiential Data to Inform the Redesign of Learning Activities","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"305--309","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723625","doi":"10.1145/2723576.2723625","acmid":"2723625","publisher":"ACM","address":"New York, NY, USA","keywords":"active learning, approaches to learning, interventions, learning analytics, mixed methods analysis","abstract":"A main goal for learning analytics is to inform the design of a learning experience to improve its quality. The increasing presence of solutions based on big data has even questioned the validity of current scientific methods. Is this going to happen in the area of learning analytics In this paper we postulate that if changes are driven solely by a digital footprint, there is a risk of focusing only on factors that are directly connected to numeric methods. However, if the changes are complemented with an understanding about how students approach their learning, the quality of the evidence used in the redesign is significantly increased. This reasoning is illustrated with a case study in which an initial set of activities for a first year engineering course were shaped based only on the student's digital footprint. These activities were significantly modified after collecting qualitative data about the students approach to learning. We conclude the paper arguing that the interpretation of the meaning of learning analytics is improved when combined with qualitative data which reveals how and why students engaged with the learning tasks in qualitatively different ways, which together provide a more informed basis for designing learning activities.","pdf":"Combining Observational and Experiential Data   to Inform the Redesign of Learning Activities   Abelardo Pardo1  abelardo.pardo@sydney.edu.au    Robert A. Ellis2  robert.ellis@sydney.edu.au   Rafael A. Calvo1  rafael.calvo@sydney.edu.au     1School of Electrical and Information Engineering   2Institute for Teaching and Learning  The University of Sydney NSW 2006 Australia      ABSTRACT   A main goal for learning analytics is to inform the design of a  learning experience to improve its quality. The increasing  presence of solutions based on big data has even questioned the  validity of current scientific methods. Is this going to happen in  the area of learning analytics In this paper we postulate that if  changes are driven solely by a digital footprint, there is a risk of  focusing only on factors that are directly connected to numeric  methods. However, if the changes are complemented with an  understanding about how students approach their learning, the  quality of the evidence used in the redesign is significantly  increased. This reasoning is illustrated with a case study in which  an initial set of activities for a first year engineering course were  shaped based only on the students digital footprint. These  activities were significantly modified after collecting qualitative  data about the students approach to learning. We conclude the  paper arguing that the interpretation of the meaning of learning  analytics is improved when combined with qualitative data which  reveals how and why students engaged with the learning tasks in  qualitatively different ways, which together provide a more  informed basis for designing learning activities..    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Computer-assisted  instruction   General Terms  Algorithms, Design, Human Factors, Measurement, Performance.   Keywords  Learning analytics, active learning, mixed methods analysis,  approaches to learning, interventions.         1. INTRODUCTION  The combined areas of Learning Analytics and Educational Data  Mining have emerged with the potential to improve university  student learning. In contexts other than education, some authors  claim that using massive amounts of data should stop the search  for models and rely solely on correlation [1]. Does this apply to  learning To explore this question, a key objective guiding this  study is whether insights from the digital footprint left by students  engaging in a learning experience can be meaningfully elaborated  by understanding qualitative differences in how students  approach their learning in the same experience, enabling a more  nuanced interpretation of the digital footprint. We postulate that  learning contexts are highly complex ecosystems in which a large  number of factors are intertwined and shape how different  stakeholders perceive their effectiveness. Although technology  now offers the possibility of capturing massive amounts of data  about student events, the quality of data interpretation and the  effectiveness of the derived interventions are still areas subject to  debate. This paper investigates a strategy to address this challenge  through exploring observational data collected while students  interact in their technology-enabled environment, and inferring  the meaning of patterns within this data set informed by smaller  scale phenomenographic data. We argue that the interpretation of  the observational data can be improved by complementing its  analysis using data which describes the student experience of  learning, how, what and why they learn. Finding the right  framework to combine these two data sources will translate into  deeper insight and understanding of the student experience and  help to improve the design of effective interventions. The  objective is to increase the understanding of the digital footprint  in the learning environment by complementing the observational  data with a description of how students approach their learning,  the strategies they adopt and why they adopt these strategies. The  rest of this paper is organized as follows. Section 2 describes the  work related to the space explored in the document. Section 3  describes the proposed approach to study the combination of the  two data sources. Section 4 includes the description of the case  study used to validate the proposed framework. Section 5  summarizes the paper and offers lines for future research.   2. RELATED WORK  Learning analytics and educational data mining have emerged as  research areas that focus on the use of recorded interactions  occurring in a learning experience to enhance the understanding  and improve learning and the environment in which it occurs [4].  In the last years numerous products, applications and research  contributions have appeared to tackled issues such as retention  [3], student success [6, 7], career recommendations [5], etc. The  Educational Data Mining space, although closely related to   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK15, March 16 - 20, 2015, Poughkeepsie, NY, USA  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-3417-4/15/03$15.00   http://dx.doi.org/10.1145/2723576.2723625   305    learning analytics, has been focusing primarily on the algorithms  to be applied to large data quantities to automatically produce  models to predict academic achievement [2, 14].    In parallel with these contributions, several researchers have  identified a so-called middle space where the concerns derived  from a focus on aspects of learning and aspects of analytic  techniques meet [16]. Knight and Buckingham-Shum [8] provided  a more pragmatic view and suggested to take into account  literature in the fields of epistemology, pedagogy and assessment  to help delimit this middle space. Their argument derives from the  recognition of assessment as an essential part of learning, but at  the same time, one of the most controversial. The abundance of  data provided by technology about how students behave in a  learning experience has logically prompted the question of how  can this improve student achievement in general (not simply  assessment results). But there are other stakeholders that need to  be brought to the forefront of the discussion about how this  middle space is shaped: the instructor, the learning designer and  the students. Lockyer et al. [9] argued that in order for learning  analytics to gain widespread adoption, there is a need to establish  a framework connecting the data collected in a learning  environment and the design of learning experiences. Learning  designs can clearly benefit from big data collected while being  enacted so that they can be refined in a continuous improvement  cycle To do this, some research has suggested combining  methodologies informed by the social sciences in order to  understand the students perspective on the digital footprint they  leave behind [19].   In this study, we have integrated learning analytics evidence with  evidence from a social science orientation known as the Student  Approaches to Learning (SAL) framework (e.g. [1113]). At a  high level of description, the SAL framework has identified a  number of variables which account for qualitative variation in the  student experience of learning at university; student approaches to  learning, student conceptions of learning and student perceptions  of the their learning environment. Qualitative variation in any of  these variables has been shown to be positively and logically  related to qualitative variation in the other variables. For example,  deep approaches to learning have been shown to be positively and  significantly associated with coherent conceptions of learning,  positive perceptions of aspects of the learning environment such  as assessment and workload, and relatively higher levels of  academic achievement (e.g. [12]). Similarly, surface approaches  to learning have been shown to be positively associated with  fragmented conceptions of learning and negative perceptions of  the learning environment (e.g. [15]).   By combining evidence from both learning analytics and the SAL  framework, we hope to add some depth to key issues that exist in  the middle space between the two areas that starts to explain  why some students experience learning relatively more  successfully than others.    3. DATA TO SUPPORT ACTIVITY  DESIGN  To date, collected data has mostly been used to create reports and  predictive models. However, the interpretation leading to changes  in a design  has received much less attention from researchers [17,  18]. Thus, one of the most challenging areas to capitalize the  potential of big data in learning environments is the connection  between data and actions to modify those environments. In this   paper we argue that successful changes in a learning design  require a thorough understanding not only of the footprint, but  also of the approaches that students experience in their learning.    3.1  Design informed by digital footprints  Current approaches to learning analytics collect extensive data  about events occurring while learning and apply various  algorithms to produce a model for such learning. For example,  Romero et al. [15] collected a set of student attributes recorded in  blended course hosted in the Moodle Learning Management  System. A clustering algorithm was then used to produce a set of  rules used to classify future students. These rules can be used to  propose different tasks to the students depending on the category  estimated by the model.   This example shows how data provides previously unknown  knowledge (the categories in which students can be divided). If  we use a big data approach alone, the instructor does not  understand why the students adopted the strategies they did with  the technologies, why some were more successful than others and  what strategies they used.    3.2 Design informed by qualitative data  A strength of the SAL approach is the detailed investigation of the  student perspective about what they think they are learning (their  conceptions of learning), how they go about their learning and  why, (their approaches to learning), and how qualitative variation  in their conceptions and approaches are related to qualitative  variation in academic achievement as measured by variables such  as task marks and course marks.   Challenges for the SAL approach in digital learning environments  involving large numbers of students is getting a sense of, and  evidence to confirm, variation in the quality of learning across  multiple tasks and hundreds if not thousands of students. We  postulate that this shortcoming can be complemented successfully  with analytics and educational mining methodologies.   3.3 Combining both approaches to inform the  design of learning activities  The premise used in this study is that a mixed-methods approach  using both big data and SAL methodologies will enable  instructors to design better informed and student-centered  activities than if they consider the two sources of information  separately.   4. CASE STUDY  The aim of the study was to investigate why some students were  more successful than others in a first year electrical engineering  course focusing on computer engineering. The number of students  enrolled is slightly over 300. Students attend one weekly 2 hour  lecture, one weekly 2 hour tutorial, and one weekly 3 hour lab  session. The design of the course is based on active learning  principles. For the lectures, students have to prepare the session  interacting with a number of activities that include visualizing  videos, answer multiple choice questions, and solve several  problems. The following course events (and their identifier for the  analysis) are recorded and included in the digital footprint dataset  for every student:    Visualization of any page of the course notes (RV)   Access to specific sections of the course notes (ACE)   Multiple choice questions answered correctly (EQC)   306     Multiple choice questions answered incorrectly (EQI)   Video visualization events (EV).   Incorrect answers to problems sequence (EA)   Dashboard showing student participation (DV)     The course included a mid-semester exam consisting of 20  multiple choice questions and counting 20% of the final course  score. The score for this exam (MID) is considered as the  dependent variable in the analysis described below.   The methodology in this study consists of three stages. The first  considers only the digital data set, a model derived from it, and  the derived changes in the learning experience. The second stage  is an analysis of student self-reported data about the experience of  engaging with the course activities. The third phase consists on  revisiting and reassessing the changes when combining the two  data sets. The steps followed for this analysis are:   4.1 Model derived from the digital footprint  The model derived from the digital footprint data set has been  obtained using all-subsets statistical regression. The descriptive  statistics of the previously described factors are shown in the  Table 1:   Table 1 Summary of variables used in the regression   Var Min 1 st   Quart Median Mean  3rd   Quart Max   ACE 16 143 214 252.6 329 935  RV 29 238 387 417.6 549.0 1315  DV 0 0 8 21.96 28.0 210   EQC 0 28 70 95.39 138.0 390  EQI 0 18 45 60.66 83 368  EA 0 331 437 430 541 1385  EV 0 44 113 184 269 1925   MID 3 11 14 13.48 16 20    The statistical analysis was carried out with the R software  platform. Given the small number of factors, all-subsets linear  regression procedure was applied. The model with the highest R- squared value included only three of them: incorrect answers to  problems (EA), number of multiple choice questions answered  correctly (EQC), and number of multiple choice questions  answered incorrectly (EQI). The coefficients for the model are  shown in Table 2. Normality was established for the dependent  variable with the Shapiro-Wilk test (W = 0.9703, p<1e-5). The  residuals of the linear model reported a value of W = 0.9838  (p<0.005) for the same test.   Table 2 Coefficients for the Derived Linear Model  Variable Coeff Std error t value p  Intercept 10.9312 0.4089 26.732 <1e-15   EA 0.012 0.0033 3.687 <1e-3  EQC 0.0036 0.0054 6.607 <1e-10  EQI -0.0397 0.0074 -5.389 <1e-6   R-squared = 0.2598, F(3, 297) = 34.76, p-value < 1e-15     4.2 Actions derived from the Digital Model  The interpretation of the linear model is that the mid-term score is  more likely to be higher for those students with a higher  difference between correct and incorrect answers to the sequence  of problems required to prepare the lecture. Additionally, the   second and third coefficients suggests that those students that  answer the embedded questions successfully more often are more  likely to get a higher score in the mid-term. This result points to  the fact that both the formative and summative assessment  included in the course to prepare the lecture are having the  expected effect in students. The robustness of the model is  validated by the high value of R-squared. More than 25% of the  variation of the score in the mid-term is explained by the linear  combination of the three factors. After reviewing the model, the  following actions were considered:   1. Use the linear equation to estimate the midterm score and  suggest changes in course engagement.   2. Increase the number of questions in the course notes.  3. Improve the problems by providing feedback when an   incorrect answer is introduced.   4.3 Student qualitative data set  To probe the students experience of engaging in activities related  to the linear model, a survey containing the following questions  was administered to the students shortly after the midterm exam.  140 students answered the survey, with an average age of 19.5  years.   1. What was the purpose of the pre-lecture course materials  (course notes, multiple choice questions, videos, sequence  problems and participation dashboard)   2. How did you approach using the videos What did you do  and why   3. How did you approach using the problem sequence What  did you do and why   4. How did you approach using the dashboard What did you  do and why   The first question investigated the students conceptions of the  course materials used for the flipped learning design. Questions  two to four investigated how the students approached the use of  the videos, the problem sequences and the dashboard. The follow  up questions in each of these asked them about their strategies  (what they did) and their intention (why they did those things).  Student responses to the questions went through an iterative  classification process amongst the researchers following a  phenomenographic methodology (Prosser and Trigwell, 1999).  The process involved classifying student responses into  qualitatively different categories that offered some explanation as  to differences in how students reported understanding about the  purpose of the course materials, how they approached the flipped  design in the course.    4.3.1 Conceptions of course materials  Not all students held the same conceptions of the course  materials. Some students (n=72) reported conceptions which  revealed an awareness of how the materials were meant to prepare  them for the lecture in ways that would improve their  understanding. These were categorized as coherent conceptions of  the course materials.   The online course materials were to help us learn material  before the lecture, which helps us understand the lecture more.  The videos were an interactive way of engaging with students and  it could help students understand some of the more difficult parts.  Participation dashboard was to help us increase the amount  weve done by comparing us to the rest of the class.   307    In contrast, another group of students (n= 68) thought that the  online course materials just repeated the same information in the  lecture and were more focused on getting higher marks by using  the materials than preparing and understanding. These were  categorized as fragmented conceptions. For example;  The course materials are used to just reinforce information  taught in videos and placed in lecture notes. Prep-questions help  improve final semester marks and also make sure we keep up to  date with course material.   4.3.2 Approaches to videos  One group of students (n=96) reviewed the videos, taking notes  and engaging in a studious way. These were categorized as deep  approaches to engaging with the videos.   I watched the videos and took notes. I tried to understand the  notes. If I was confused, I would refer to course material notes.  Tested understanding with multiple choice questions. Very helpful  because I can constantly watch it until I understand.  A second group of students (n= 44) did not seem to engage with  the videos in ways intended by the instructor. They did not report  an intention to engage or test understanding, tending to adopt a  disengaged approach. These were labelled as surface approaches  to engaging with the videos, such as;  I watched the video whilst doing other stuff. Mainly because the  video are lengthy and my attention span is short.   4.3.3 Approaches to problem sequences  The idea behind the problem sequences was to help the students  go through a process of problem solving that would be useful in  the context of computer engineering. One group of students  (n=64) saw connections between the other parts of the course and  problem sequence. Such approaches were categorized as a deep  approach to the problem sequences.   I worked the problem sequences and then answered the  questions accordingly. If I got a question wrong, I liked how I was  able to rework it and see what I did that was wrong. Being able to  do this helped me understand what I had misconceptions about.  Also, the fact that you got the mark regardless of the attempts was  helpful as I was more open to going ahead and trying the  questions and learning as I go, rather than worrying about losing  marks because of misunderstandings in the content I just  learned.    Another group of students (n=76) did not display a similar  awareness of how to engage meaningfully with the problem  sequences. They seemed to drill the questions in the problem  sequence with little reflection, mostly focusing on getting the  right answer. These were categorized as surface approaches.   I answered each question of the problem sequences one by one,  to get the participation mark... I repeated all the questions until I  reached 100% correct.   4.3.4 Quantitative analysis of the qualitative  categories  A SAL approach to the student experience of learning looks at the  relatedness amongst the reported conceptions and approaches to  see if there are any associations amongst the categories that  warrant further research in more empirically designed studies  (Prosser and Trigwell, 1999). In the following analyses, cross- tabulations were used to investigate if there were associations   between the conceptions held, and approaches adopted. A link  between the different aspects of the experience could be helpful in  understanding how to adjust misconceptions or relatively poor  approaches. Associations were identified between conceptions of  course materials and approaches to video, approaches to problem  sequences and approaches to video and approaches to problem  sequences and the mid-term mark.  Table 3 Associations between conceptions of course materials   and approaches to videos  Conception of   course materials Approach to videos Total Deep surface   Coherent 56 16 72  Fragmented 40 28 68   Total 96 44 140   =5.83, phi = .20, p<0.05  Table 3 shows a statistically significant association between  conceptions of course materials and approaches to videos,  suggesting that a coherent conception of the course materials is  related to a deep approach to videos and a fragmented conception  of course materials is related to surface approaches to videos  (phi=.20, p<.05).  Table 4 shows a statistically significant association between  approaches to problem sequences and approaches to videos,  suggesting that a deep approach to problem sequences is related  to a deep approach to videos and a surface approach to problem  sequences is related to surface approaches to videos (phi=.20,  p<.05)   Table 4 Associations between approaches to problem  sequences and approaches to videos   Approach to video Approach to problem   sequence Total  Deep Surface   Deep 51 45 96  Surface 14 30 44  Total 65 75 140    =5.50, phi = .20, p<0.05  Table 5 shows a statistically significant association between  approaches to problem sequences and the mid-term mark,  indicating that deep approach to the problem sequences is related  to a relatively higher mark in the mid-term than surface  approaches to the problem sequences and a fragmented  conception of course materials is related to surface approaches to  videos (t=4.1, p<.001, es = 0.72)   Table 5 Associations between approaches to problem  sequences and mid-term mark   Approaches to  problem   sequences   Final Mark   Mean Standard Deviation Effect size   Deep 15.98 2.6  0.72   Surface 13.88 3.2   T test: T= 4.1**  N=140, **p<.001, #mark out of 20     308    4.4 Actions derived from the combined data  The authors recognized that the design changes derived from the  digital footprint did not reveal how we should advise students to  engage more with the material, nor did it suggest changes to helps  students adopt more effective approaches to learning. The  changes would have been based on the assumption that by  increasing the factors in the model, students improve their  learning experience.   The SAL analysis adds another dimension to the investigation of  the student experience which can improve the interpretation of the  digital footprint. Firstly, just engaging more with any of the  events does not seem to be a guarantee that a student will improve  their experience. In each of the activities, there appeared to be a  qualitative difference in how the students approached their  learning. For some students, answering the multiple choice  questions in the videos was part of a deeper engagement which  also involved taking notes, going over the videos to improve  understanding and referring to course notes as a type of cross- reference. For other students, viewing the videos was a passive  experience, not really paying attention and often doing other  things.   When advising students about their levels of participation, not  only can the instructor suggest ideas about increasing the quantity  of their participation, the advice can also model and provide  suggestions on how to improve the quality of their engagement  based on the experience of past successful students. The advice  can take the form of explicit strategies for more meaningfully  engaging with the videos, problem sequences and other events  designed to link with these activities.    For the course materials, the instructor can use the students  description of their experience to inform their redesign. The  instructions surrounding the videos can reinforce the types of  ideas sent to students not sufficiently engaging with them.  Rubrics and other learning materials can scaffold the type of note- taking and reflective strategies that the more successful students  reported adopting. Similarly, the problem sequences can be  redesigned to emphasize that repeating the sequences is useful,  but only if the students are doing in in ways to rectify  misconceptions or confirm accurate ones.   5. CONCLUSIONS  The paper has explored the benefits of designing activities for a  learning environment by considering the digital footprint of  students and contextualizing interpretations from that data with  Student Approaches to Learning framework. Using only data  from their digital footprint, the information for the types of  interventions focused on are limited and may misdirect the efforts  of teachers. Integrating an understanding of the students  approaches using the SAL framework exposes a more nuanced  reality of why students interact with the activities and the type of  strategies that they use. We argue that the combination of these  two paradigms provide a richer evidence-base for instructors  seeking to design interventions which address the needs of the  students. The question as to whether an elaborated intervention  based on both data sets is more effective for the quality of the  student experience is beyond the scope of this study and provides  motivation for further studies. Here we offer the arguments and  evidence as a basis for a richer evidence-base from which to  design interventions which are likely to help.      6. REFERENCES  [1] Anderson, C. 2008. The End of Theory: The Data Deluge   Makes the Scientific Method Obsolete. Wired Magazine.  [2] Antunes, C. 2010. Anticipating students failure as soon as   possible. Handbook of Educational Data Mining. C. Romero,  S. Ventura, M. Pechenizkiy, and R.S.J. d. Baker, eds. CRC  Press. 353.   [3] Arnold, K.E., Hall, Y., Street, S.G., Lafayette, W. and Pistilli,  M.D. 2012. Course Signals at Purdue: Using Learning  Analytics to Increase Student Success. International  Conference on Learning Analytics and Knowledge (2012),  267270.   [4] Baker, R. and Siemens, G. 2014. Educational data mining and  learning analytics. The Cambridge Handbook of the Learning  Sciences. R.K. Sawyer, ed. Cambridge University Press.   [5] Bramucci, R. and Gaston, J. 2012. Sherpa: increasing student  success with a recommendation engine. International  Conference on Learning Analytics and Knowledge (2012),  8283.   [6] Essa, A. and Ayad, H. 2012. Improving student success using  predictive models and data visualisations. Research in  Learning Technology. 5, (2012), 5870.   [7] Essa, A. and Ayad, H. 2012. Student success system: Risk  Analytics and Data Visualization using Ensembles of  Predictive Models. Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge - LAK 12  (New York, New York, USA, Apr. 2012), 158161.   [8] Knight, S., Shum, S.B. and Littleton, K. 2014. Epistemology ,  assessment , pedagogy: where learning meets analytics in the  middle space. Journal of Learning Analytics. 1, 1 (2014), 23 47.   [9] Lockyer, L., Heathcote, E. and Dawson, S. 2013. Informing  Pedagogical Action: Aligning Learning Analytics With  Learning Design. American Behavioral Scientist. 57, 10 (Mar.  2013), 14391459.   [10] McAfee, A. and Brynjolfsson, E. 2012. Big Data. Harvard  Business Review. October (2012), 6069.   [11] Pintrich, P.R. 2004. A Conceptual Framework for Assessing  Motivation and Self-Regulated Learning in College Students.  Educational Psychology Review. 16, 4 (Dec. 2004), 385407.   [12] Prosser, M. and Trigwell, K. 1999. Understanding Learning  and Teaching. The Experience in Higher Education. Society  for Research in Higher Education & Open University Press.   [13] Ramsden, P. 2003. Learning to Teach in Higher Education.  Routledge.   [14] Romero, C., Lpez, M.-I., Luna, J.-M. and Ventura, S. 2013.  Predicting students final performance from participation in  on-line discussion forums. Computers & Education. 68, (Oct.  2013), 458472.   [15] Romero, C., Ventura, S. and Garcia, E. 2008. Data mining in  course management systems: Moodle case study and tutorial.  Computers & Education. 51, 1 (Aug. 2008), 368384.   [16] Suthers, D. and Road, E.W. 2013. Learning Analytics as a  Middle Space. Proceedings of the International Conference  on Learning Analytics and Knowledge (2013), 25.   [17] Verbert, K. and Duval, E. 2012. Learning Analytics.  Elearning and Education. 8 (2012).   [18] Wise, A.F. 2014. Designing Pedagogical Interventions to  Support Student Use of Learning Analytics. Proceedings of  the International Conference on Learning Analytics and  Knowledge (2014).    309      "}
{"index":{"_id":"50"}}
{"datatype":"inproceedings","key":"Hickey:2015:FSA:2723576.2723634","author":"Hickey, Daniel T. and Quick, Joshua D. and Shen, Xinyi","title":"Formative and Summative Analyses of Disciplinary Engagement and Learning in a Big Open Online Course","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"310--314","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723634","doi":"10.1145/2723576.2723634","acmid":"2723634","publisher":"ACM","address":"New York, NY, USA","keywords":"analytic approaches, assessment, learning analytics, personalized learning, social learning analysis","abstract":"Situative theories of knowing and participatory approaches to learning and assessment were used to offer a big open online course on Educational Assessment using Google CourseBuilder in 2013. The course was started by 160 students and completed by 60, with relatively extensive instructor interaction with individual learners. This yielded much higher levels of engagement and learning than are typical of open or conventional online courses. The course was further refined and offered a second time in 2014, where it was started by 76 students and completed by 22, with a much lower level of support. Comparable levels of engagement and learning were obtained, suggesting that this participatory approach to learning and assessment can indeed be managed with more typical instructor support. Nonetheless, additional automation and streamlining is called for if the model is to eventually be used in massive online courses with thousands of students or as an autonomous self-paced open course.","pdf":"     Formative and Summative Analyses of Disciplinary  Engagement and Learning in a Big Open Online Course   Daniel T. Hickey  Indiana University   Bloomington IN, 47401  01-812-856-2344   dthickey@indiana.edu   Joshua D. Quick  Indiana University   Bloomington IN, 4740  251-463-6070   jdquick@indiana.edu   Xinyi Shen  Beijing Normal University   Beijing 100875, China  86-159-0100-1713   sxy0710@foxmail.com     ABSTRACT  Situative theories of knowing and participatory approaches to  learning and assessment were used to offer a big open online course  on Educational Assessment using Google CourseBuilder in 2013.  The course was started by 160 students and completed by 60, with  relatively extensive instructor interaction with individual learners.  This yielded much higher levels of engagement and learning than  are typical of open or conventional online courses. The course was  further refined and offered a second time in 2014, where it was  started by 76 students and completed by 22, with a much lower  level of support. Comparable levels of engagement and learning  were obtained, suggesting that this participatory approach to  learning and assessment can indeed be managed with more typical  instructor support. Nonetheless, additional automation and  streamlining is called for if the model is to eventually be used in  massive online courses with thousands of students or as an  autonomous self-paced open course.   Categories and Subject Descriptors  K.3.1 [Computers in Education]: Computer Uses in Education collaborative learning, distance learning   General Terms  Algorithms, Measurement, Performance, Design.   Keywords  Personalized learning, learning analytics, assessment, social  learning analysis, analytic approaches, analytic approaches.    1. THEORETICAL ORIENTATION  This research is rooted in the situative theories of cognition that  emerged from the Institute for Research on Learning in the 1990s  [3, 8]. In contrast to the individually-oriented learning principles  from human information processing  [1] and constructivism [7],  situative theories lead to learning principles that focus on social  participation:  (1) Learning is fundamentally social, (2) Knowledge  is integrated in the life of communities, (3), Learning is an act    Permission to make digital or hard copies of all or part of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on  the first page. Copyrights for components of this work owned by  others than ACM must be honored. Abstracting with credit is  permitted. To copy otherwise, or republish, to post on servers or to  redistribute to lists, requires prior specific permission and/or a fee.  Request permissions from Permissions@acm.org.   LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA Copyright  2015 ACM 978-1-4503-3417-4/15/03$15.00  http://dx.doi.org/10.1145/2723576.2723634    of membership, (4) Knowing depends on engagement in practice,  (5) Engagement is inseparable from empowerment, (6), Failure to  Learn is the normal result of exclusion from participation, and (7)  We already have a society of lifelong learners.1 These principles  formed the metatheory within which more specific principles  were used to design and analyze an open online course on the topic  of educational assessment.  This research drew specific inspiration from Engles notion of     productive disciplinary engagement [6]. The course design  framework assumes that engagement is productive when it leads  to new questions, clarifies misunderstanding, and leads to more  successful engagement by more participants; engagement is  disciplinary when it concerns both the declarative knowledge and  the social and cultural practices that disciplinary experts engage in.  In this course, the disciplinary knowledge consisted assessment  practices (e.g., guidelines for constructing various assessments)  principles (e.g., reliability and validity) and policies (e.g.,  standardized testing and teacher evaluation). This disciplinary  knowledge was provided in the course via a widely-used and well- respected textbook on the topic, as supplemented with various  online resources associated with each of the weekly assignments.  In particular this work represents an effort to foster interactive  forms of online engagement with course content, instructors, and  peers that is exceedingly productive and disciplinary. The design  of the course was directly shaped by Engles four design principles  for fostering productive disciplinary engagement: (1) Problematize  disciplinary content from the perspective of each learner, (2) Give  students authority and position them as stakeholders and producers  of disciplinary knowledge, (3) Establish disciplinary accountability  and require students to defend their positions, and (4) Provide  ready access to disciplinary resources.   This work drew more general inspiration from studies of online  participatory culture [11] and connectivism [12]. The specific  objective of this research is using all of these perspective to foster  productive forms of networked disciplinary engagement in online  courses, while also addressing a range of widely-shared concerns.  These concerns include (a) prevailing expectations for content  coverage typical of formal secondary and post-secondary courses,  (b) enduring expectations of accountability & achievement, (c)  conventional goals of instructional manageability, and (d) new  goals of instructional scalability. In short, this research aims to  support the forms of participatory learning needed to foster 21st  Century networked learning while still addressing a wide range of  other concerns.   2. PRIOR RESEARCH  Starting in 2009, a program of design research in the first authors  own online graduate education courses was used to organize the  various situative practices from prior research into a coherent  learning design framework that others might readily employ [10].   This framework is now called Participatory Learning and   310         Assessment and consists of the following five course design  principles: (1) Let public contexts give meaning to disciplinary  knowledge, (2) Recognize and reward disciplinary engagement, (3)  Grade disciplinary artifacts through local reflections, (4) Assess  disciplinary knowledge privately, and (5) Measure disciplinary  achievement discreetly.   These principles were then transformed into more specific features  in a big open online course (BOOC) on Educational  Assessment. The Assessment BOOC was offered to up to 500  students in fall 2013 using the Google Course Builder Platform,  with the support of a grant from Google. As summarized in a paper  presented at Learning Analytics and Knowledge 2014 [9], thirteen  participatory  learning features were scaled up and streamlined in  this new course: (1) personalized learning contexts, (2)  professional networking groups, (3) secondary emergent  networking groups, (4) publish course artifacts publically, (5)  ranking  relative relevance of ideas, (6) personalized external  content, (7) public individualized feedback, (8) peer commenting,  (9), peer endorsement, (10) peer promotion, (11) participatory  analytics & feedback, (12), appropriate accountability, and (13)  web-enabled digital badges. Extensive refinements to the existing  Google Course Builder LMS (approximately 3000 additional lines  of code to the existing 5000) were necessary to implement these  features. They were sufficiently streamlined so that the instructor  and two teaching assistants could manage the course. Further  refinements were carried out across the semester to further  automate these features and refine them in order to support even  higher levels of disciplinary engagement and learning.  Ultimately, 460 people registered for the first course, 160  completed the first assignment, and 60 completed the course.  Summative analyses of the weekly wikifolios, peer endorsements  and promotions, discussion comments, and achievement tests  revealed levels of disciplinary engagement and learning that appear  to greatly exceed those obtained in other open online courses and  many conventional online courses [9].   3. NEW COURSE FEATURES  The Assessment BOOC was further refined in anticipation of being  offered a second time. This was motivated by the goal of making  the course more manageable with typical levels of instructional  support, and making progress towards an autonomous self-paced  version of the course that might still support this sort of  participatory social learning.   3.1  Online Instructor Videos  The prior course included just two introductory online videos. The  design team debated the possibility of including online videos for  each weekly assignment. On one hand, students like online videos  because they can watch them while commuting or exercising and  because they provide a more personal connection to the instructor.  On the other hand, there was a concern that weekly videos might  lead some students to not purchase or engage with the textbook or  with their peers. Additionally, high quality videos can also be quite  time consuming to create. More fundamentally, a participatory  perspective raises the concern that the reified and decontextualized  delivery of course knowledge within typical lecture videos may  discourage students from personalizing the disciplinary knowledge  of the course. The specific concern for the Assessment BOOC was  that the static presentation of declarative knowledge in a video  might encourage students to focus on that knowledge in the  abstract, rather than problematizing that that knowledge within the  context of a self-selected curricular aim that embodies their own  experience, interests, and aspirations.   A decision in favor of videos came with the realization that videos  might provide a salient way to demonstrate the personalized  engagement expected in each weekly assignment and an efficient  way for the instructor to personalize course content beyond the  textbook. Most of the new weekly videos that were ultimately  created featured the instructor considering the relative relevance of  the various big ideas each week in light of his own delivery of both  online and conventional course.  Many of the new videos also  feature the instructor taking positions that diverged from those in  the textbook.   3.2  Drag and Rank Wikifolios  One of the central innovations in this broader program of research  is a simple scalable routine for problematizing disciplinary course  knowledge. This is accomplished by having students consider and  discuss the relative relevance of elements of course knowledge  from their own personal context. Reflecting a situative emphasis on  the context in which new knowledge is learned and used, learners  are asked to define and then continually refine their understanding  of a specific course goal and their own experiences, interests, and  aspirations. In this way, their disciplinary knowledge and their  personal knowledge are presumed to unfold together and to  reinforce each other.  In the Assessment BOOC, this personalization was organized  around (a) the intuitional context (school level and role) selected  during registration and (b) a curricular aim that was drafted  initially when registering and further refined in the first assignment.  This accomplishes the first design principle of the PLA framework.  The first course confirmed that the process of ranking relevance and  justifying those rankings (typically for the most relevant and  least relevant) is a simple way of engaging students with  otherwise-abstract course contexts. Doing so publically fostered a  remarkable level of disciplinary engagement as students compared  their rankings with each other. Nonetheless, there was room for  improvement with the routine: some students in the prior course  only partly completed the assignment (such as only providing a  rationale for the first entry). A problem for the instructor was that  this information was very difficult to summarize and analyze for  the participatory analytics and feedback (described next).  In response, a new wikifolio routine was created whereby the edit  window for each wikifolio presented the student with the to-be- ranked ideas, with an edit box directly below each set where they  could draft personalized summaries of each idea and a rationale for  the ranking. Because students could not save their edits without  rearranging the boxes, student were forced to engage in the ranking.  It is assumed, but not known, whether this also compelled them to  provide more complete rationale for the ranking.  Each wikifolio assignment include 2-4 of these routines, generally  embedded with a larger set of activities (including extended  activities that were required for the for-credit students but optional  for the non-credit students. As described next, in addition to  streamlining the activities for the students, this feature streamlined  the participatory learning analytics for the instructor   3.3 Streamlined Participatory Analytics &  Feedback  Central to this approach is the desire to analyze patterns of  disciplinary participation across different types of learners. Seeing  how particular types of classmates found particular ideas more or  less relevant reveals otherwise nuanced and abstract disciplinary  knowledge. Because students are participating in the generation of   311         this new disciplinary knowledge, they are well-positioned to  appreciate the nuances that it contains.  Consider, for example, the chapter on validity where students rank  the relevance of different forms of validity evidence. In the  previous BOOC, students in the Educator groups overwhelmingly  selected content-related evidence as most relevant while most of  the students in the Administrator groups selected criterion-related  evidence as most relevant; just a handful of students found  construct-related evidence most relevant, mostly researchers or  doctoral students who were interested in things like self-efficacy.  This kind of information is summarized each week and provided in  a weekly review along with links to exemplary wikifolios whose  posts articulate why their group tended to respond the way they did  The problem was that this process was quite laborious and provides  the participatory feedback after many students had moved on to the  next assignment. In the first BOOC, the teaching assistant manually  reviewed all of the completed wikifolios and then summarized that  data in a table. This was one of the most laborious and time- sensitive aspects of the course. The drag and rank routine  generated the table automatically and all was required was the  addition of exemplary posts. This allowed the information to be  assembled with minimal time and presented just hours after the  posting deadline, when students were still interacting with each  other.   3.4  Required Questions to Peers  Peer discussion is central to the underlying course design. The  course design framework assumes that student and instructor  discussion directly on student-generated artifacts is likely to be  much more productive and much more disciplinary than typical  discussion forums. [3] Particularly in open courses, discussion  forums have a tendency to go off in many different directions that  are only loosely related to the assignment or even the course.  Nonetheless, students need to read each others work and post  comments for this disciplinary engagement to occur. To this end,  the wikifolio assignments instructed students to post at least one  question to their classmates on their own wikifolio and to review  and post comments and questions on at least three of their peers  wikifolios. Reflecting the participatory nature of the model, the  number and nature of comments were not graded or evaluated in  any way and there was no accountability for peer interaction. In  particular, the model aims to minimize obligatory interactions that  are unproductive and non-disciplinary while maximizing more  productive and disciplinary interactions.  The average number of comments per wikifolio in the first BOOC  was 5.6 for the students who enrolled in the course for credit and  3.0 for the open students who completed the course. Unfortunately,  participation in peer commenting was quite uneven across students.  In fact, across the 841 wikifolios posted in the first BOOC, 32%  had no peer questions or comments, while another 9% included a  question but no peer responses (though nearly all had comments  and questions from the teaching assistants and the instructor).   In response, to uneven commenting, a new feature was added to the  Assessment BOOC that essentially required students to post at least  one question to their peers in order to post a completed wikifolio.  This question was prominently featured at the bottom of the  wikifolio in order to draw attention to it. The research team debated  the value and function of this feature. One of the concerns that was  raised was that peers and instructors might go directly to the  question without looking at the peer work at all; this might draw  attention away from potentially more productive interaction around  other wikifolio features such as the ranking    3.5  Embedded Formative Assessments  The fourth principle in the Participatory Learning and Assessment  design framework is Assess disciplinary understanding privately.  This reflects the assumption that instruction should not focus too  directly on static representations of disciplinary knowledge in  classroom assessments. Well-designed curriculum-oriented  assessments are proximal in that they focus on the disciplinary  knowledge that the course focused on. This makes them  particularly useful for helping students and teachers evaluate the  effectiveness of the activities that led up to them by ensuring that  the knowledge gained can be used in a somewhat different context.  The earlier online version of the Educational Assessment course  had included timed open-ended essay items as part of the midterm  and final examinations for this purpose. These items were scored  individually by the instructor. This was manageable with a small  course but was still quite time consuming; this was prohibitive  when attempting to scale up that course to the BOOC. No such  assessment were included in the first Assessment BOOC. For the  second version, each wikifolio include a practice assessment with  4-6 open ended items. Students had to enter a response to each item  in order to see the scoring key for the item. The formative  assessments were entirely voluntary. While the system retained  student responses, they were not formally evaluated as part of the  instruction.   4. CURRENT RESEARCH  Because the original grant funds were exhausted at this time, there  was only minimal support for instruction and research beyond  typical course delivery associated with graduate students who opted  to pay regular tuition and enroll in a credit bearing section of the  course. While this certainly tempers any claims that might be made  about the scalability of the new features, it certainly allowed us to  examine their feasibility of the new features, examine their  effectiveness, and compare engagement, learning, and retention  with the previous the semester. While teaching assistants were used  again to help manage the course load, they devoted significantly  less time, and much of the instructors time during the actual course  was committed to creating slides and recording weekly videos from  one week to the next. As such the course was not widely promoted  and dozens rather than hundreds of students were expected.    4.1 Enrollment in the Current Course  A total of 187 students registered for the second Assessment  BOOC. The 187 initial registrants included 12 tuition-paying  students who had enrolled in the course for graduate credit towards  an MA or PhD. Of the initial registrants, 76 (41%) completed the  first assignment. As is typical with open courses, students gradually  dropped out. Eventually, 22 students completed the course. Thus,  11% of the registrants and 29% of the students who completed the  first assignment ultimately completed the course.    4.1 Flow of the Current Course   The flow of the current course was quite similar to the previous  course. Each of the 11 weekly wikifolios first asked students to  restate and reframe their personal context and goal. They then  completed one or more applications of course concepts, one or  more relevance rankings, and a summary of the big ideas in the  chapter and related online educational resources. Each wikifolio  also included several optional elements that were required for the  for-credit students. These included additional activities, responses  to the self-check and discussion questions in the chapter, and a  set of three carefully-structured reflections (elaborated below).    312         Each week, students were instructed (but not required) to endorse  at least three peer wikifolios as being complete by clicking on a  corresponding button, which would then show the peers name as  having provided the endorsement. Reflecting the second principle  in the design framework (Reward disciplinary engagement)  students were also instructed (but not required) to highlight one  (and only one) of their classmates work for being exemplary by  clicking on the corresponding button and entering a warrant for  what was exemplary about the particular artifact.   The course was divided into three units: Practices, Principles, and  Policies. Posting a wikifolio that was endorsed by at least one peer  as being complete and completing a time-limited multiple choice  achievement test automatically generated a digital badge which was  compliant with Mozillas Open Badges Infrastructure. The earner  could choose to share that badge out over various social networking  sites or email, and could choose to include links to their actual  completed assignments and the number of peer endorsements and  comments, and/or their exam scores. Students in each networking  group who earned the most peer promotions earned a version of the  badge whose image said Leader. The criteria section of the Leader  badge indicated that the earners work had been deemed exemplary  by peers. The earner of the Leader badges had the option of  including the warrants for the peer promotions in their badges as  well. Students who earned all three badges and completed the final  exam earned an Assessment Expert Badge that contained the three  badges and all of the evidence therein.   Reflecting the third course design principle (Grade disciplinary  artifacts through reflections), the content of the wikifolios and the  comments were not directly graded for the 12 for-credit students.  Rather their reflections were graded for evidence of consequential,  critical, and collaborative engagement. This practice is intended to  sidestep the formal evaluation content of artifacts and comments as  evidence of enduring knowledge. Doing so is presumed to (a)  undermine participation in disciplinary discourse around those  artifacts, (b) result in dubious evidence of knowledge, (c) lead to  unsustainable individualized formative feedback on artifacts, and  (d) lead to unsustainable summative grading demands on  instructors. Essentially the model relies instead on conventional  assessments and tests to evaluate disciplinary knowledge and  achievement; in practice, students who post a complete draft by the  deadline and post coherent reflections receive full points for their  11 wikifolios, which count towards 55% of the final course grade.    5. CURRENT COURSE RESULTS  The aforementioned resource limitations precluded some of the  more laborious analyses carried out with the first BOOC. Particular  attention was directed towards aspects of engagement and learning  that could be automatically analyzed.   5.1. Raw Public Individual Engagement  Given that there are no requirements of length of responses to the  weekly wikifolios, the sheer number of words written in each  weekly wikifolio is one indicator of student engagement. Not  surprisingly, the credential students averaged significantly more  words per wikifolio (2820) than the open students who completed  the course (1377) and the open students who did not complete the  course (1081). A clear pattern emerged whereby the length of  student wikifolios rises and falls with the competing demand of the  module exams. The average number of words from these three  groups in the first BOOC were 1398, 1207, and 1137, respectively.  Particularly for the for-credit students, this represents substantially  more raw individual engagement.   Given the design of the wikifolio assignments, nearly all of this  engagement is presumed to be disciplinary. There are really very  few opportunities for student wikifolios to stray from the topic of  the corresponding chapter, much less stray from the topic of  assessment.    5.2 Raw Local Engagement  Another relevant indicator of engagement concerns social  interaction via comments posted on student wikifolios. These  interactions are local in that they are public but directed to  specific individuals. The average number of comments per  wikifolio for the for-credit students was 4.2 across units and 3.4  across units for the open students who completed the course.  These  are to the number of comments posted in the previous BOOC (5.6  and 3.0, respectively).   5.3. Disciplinary Engagement   Of crucial concern is the extent to which the individual and social  engagement represented disciplinary engagement. All of the  comments posted by the students who completed the course were  coded by the third author as to the extent they addressed the topic  of the particular chapter (3 points), assessment in general (2 points),  or education in general (1 point), or something else (0 points).  Twenty percent of the comments were coded by the second author,  yielding an inter-rater reliability of .85. This revealed high levels of  disciplinarity for the for-credit students (2.91) and the open  students who completed the course (2.80). The same analysis with  a representative subsample of students from the first BOOC found  nearly identical levels (2.9 and 2.8).    5.4 Contextual Engagement  As argued above, participatory perspectives argue that anchoring  disciplinary course knowledge to personally meaningful contexts is  crucial for enduring understanding and knowledge transfer. To this  end, all wikifolio questions and comments were coded as to  whether they referenced a specific educational context. This  typically referred to the personal context of either the student who  posted the wikifolio, or the personal context of the student who  posted the comment. Comments that referenced a specific context  were coded as 1, while comments that did not reference any context  of practice were coded as 0. Twenty percent of the comments were  coded by a second scorer, yielding an inter-rater reliability of .83.   The levels of contexuality varied somewhat across the six  networking groups. Looking across groups showed that just .22 of  the comments from the for-credit students and .28 of the comments  from the open completers referenced specific contexts. These levels  are substantially lower than the .51 and the .49 found with a  representative sample of students in the first BOOC. This seems  likely to have been a consequence of the required questions,  because the proportion of those required questions that referenced  specific contexts was .25, and because it seems likely that a  question that is posed without referencing a specific context would  be likely to earn a similarly decontextualized reply.   5.5. Disciplinary Achievement  Both BOOCs included three 20-item unit exams and a 30-item  comprehensive final. These exams were entirely selected response  and consisted entirely of selected-response items from the textbook  item bank. Small changes were made from the first to the second  BOOC consisting of the replacement of a few items that had  misbehaved. While students were given their exam score  immediately upon completion, item-level feedback was not  provided in order to help secure the exam content, During the  several-day exam window, students were allowed to take each   313         exam twice in three hours and take the final twice in four hours.   Participants were required to complete the exams and final to earn  digital badges, but the original 80% criteria was relaxed.  Participants were able to choose whether they included their exam  performance on their digital badges, and exam scores were factored  into final course grades for the for-credit students.    The achievement scores for each of the three midterms and the  final exam for the second BOOC are similar to the scores for  student in the previous course (88%, 83%, 78%, and 82% for the  for-credit students and 84%, 76%, 78%, and 75% for the non-credit  completers. The one notable difference is the lower performance of  the open completers on the final exam.  6. SUMMARY  In summary, most of the refinements to the Assessment BOOC  appeared to facilitate engagement and learning, and the second  BOOC was successfully taught with substantially less instructional  support. While there were fewer students in the second BOOC, the  instructor and the teaching assistants devoted most of their time to  preparing videos and feedback and had limited interaction with  individual students. The one notable difference in learning  outcomes between the first and second course is that the proportion  of comments that referenced specific learning context dropped by  half in the second course. This seems quite likely to have been the  result of the requirement that wikifolio authors post a question to  their classmates.   7. CONCLUSIONS  This round of revisions to the Assessment BOOC made some useful  progress towards further streamlining the course in ways that would  allow a truly massive course to still foster the kinds of participatory  engagement. In particular the participatory learning analytics and  feedback will need to be further automated, ideally presenting the  rankings of different groups in a graph in real time as students  complete the assignments. Additional refinements are needed to  streamline the registration system as well as the formative and  summative assessments. In the meantime, it appears that the  Participatory Learning and Assessment design framework is indeed  a promising framework for efficiently fostering relatively high  levels of disciplinary engagement and learning.  This work laid the groundwork for a self-paced version of the  Assessment BOOC to be offered in summer 2015 that features all  of the same public peer interaction and participatory feedback. All  course features are now being fully automated and new features are  being added that let learners locate locating active peers working  on particular assignments and archive completed wikifolios.  Assuming that the course attracts a few hundred learners at any  given time, it should support a level of social engagement that is  both disciplinary and productive that is unprecedented for a fully  self-paced course.     8. ACKNOWLEDGEMENTS   1  http://en.wikipedia.org/wiki/Institute_for_Research_on_Learning   Initial research was supported by a gift from Google to Indiana  University. Additional support was provided by the Indiana  University Office of the Vice President of Instructional Technology  and the MacArthur Foundation. Garrett Poortinga and Thomas  Smith contributed directly to many of the features and learning  analytics described in this paper. Rebecca Itow contributed to key  aspects of the design research and the writing of this manuscript.  Tara Kelly and Retno Hendryanti supported the instruction  described here.      9. REFERENCES  [1] Anderson, J. R., 1990. Cognitive psychology and its   implications . WH Freeman/Times Books/Henry Holt &  Co    [2]  Brinton, G. C., Chiang, M., Jain, S., Lam, H, Liu, Z., &  Wong, F. M. F., 2013. Learning about social learning  in MOOCs: From a statistical analysis to a generative  model. Cornell. http://arxiv.org/abs/1312.2159.   [3] Brown, J. S., Collins, A., & Duguid, P., 1989. Situated  cognition and the culture of learning. Educational  Researcher, 18(1), 32-42.   [4] Cobb, P., Confrey, J., Lehrer, R., & Schauble, L.,  2003.  Design experiments in educational research.  Educational researcher, 32, 1, 9-13.   [5] Cook, T. D., Campbell, D. T., & Day, A.,1979. Quasi- experimentation: Design & analysis issues for field  settings. Boston: Houghton Mifflin.   [6] Engle, R. A., & Conant, F. R..2002. Guiding principles  for fostering productive disciplinary engagement:  Explaining an emergent argument in a community of  learners classroom. Cognition and Instruction, 20 3,  399-483.   [7] Glaser, R., 1984. Education and thinking: The role of  knowledge. American Psychologist, 39(2), 93.   [8]  Greeno, J. G., 1998 The situativity of knowing,  learning, and research.  American Psychologist 53, 1, 5- 26    [9] Hickey, D. T., Kelly, T. A., & Shen, X. 2014. From small  to big to massive: Scaling up participatory learning  analytics. In Proceeding of LAK 14, Indianapolis IN.    [10] Hickey, D. T., & Rehak, A., 2013. Wikifolios and  participatory assessment for engagement,  understanding, and achievement in online courses.  Journal of Educational Media and Hypermedia, 22, 4,  229-263.    [11] Jenkins, H., 2009. Confronting the challenges of  participatory culture: Media education for the 21st  century. Cambridge MA: The MIT Press.    [12] Siemens, G., 2005. Connectivism: A learning theory for  the digital age. International Journal of Instructional  Technology and Distance Learning, 2, 1, 3-10.                                                                      314      "}
{"index":{"_id":"51"}}
{"datatype":"inproceedings","key":"Rienties:2015:SUL:2723576.2723600","author":"Rienties, Bart and Toetenel, Lisette and Bryan, Annie","title":"Scaling Up Learning Design: Impact of Learning Design Activities on LMS Behavior and Performance","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"315--319","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723600","doi":"10.1145/2723576.2723600","acmid":"2723600","publisher":"ACM","address":"New York, NY, USA","keywords":"academic retention, learning analytics, learning design","abstract":"While substantial progress has been made in terms of predictive modeling in the Learning Analytics Knowledge (LAK) community, one element that is often ignored is the role of learning design. Learning design establishes the objectives and pedagogical plans which can be evaluated against the outcomes captured through learning analytics. However, no empirical study is available linking learning designs of a substantial number of courses with usage of Learning Management Systems (LMS) and learning performance. Using cluster- and correlation analyses, in this study we compared how 87 modules were designed, and how this impacted (static and dynamic) LMS behavior and learning performance. Our findings indicate that academics seem to design modules with an invisible blueprint in their mind. Our cluster analyses yielded four distinctive learning design patterns: constructivist, assessment-driven, balanced-variety and social constructivist modules. More importantly, learning design activities strongly influenced how students were engaging online. Finally, learning design activities seem to have an impact on learning performance, in particular when modules rely on assimilative activities. Our findings indicate that learning analytics researchers need to be aware of the impact of learning design on LMS data over time, and subsequent academic performance.","pdf":"Scaling up learning design: impact of learning design  activities on LMS behavior and performance   Bart Rienties  Open University,    Institute of Educational Technology,  Milton Keynes, UK.  00-44-1908332671   Bart.Rienties@open.ac.uk   Lisette Toetenel  Open University,    Institute of Educational Technology,  Milton Keynes, UK.  00-44-1908332809   Lisette.Toetenel@open.ac.uk   Annie Bryan  Open University,    Institute of Educational Technology,  Milton Keynes, UK.  00-44-1908332696   Annie.Bryan@open.ac.uk   ABSTRACT   While substantial progress has been made in terms of predictive   modeling in the Learning Analytics Knowledge (LAK)   community, one element that is often ignored is the role of   learning design. Learning design establishes the objectives and   pedagogical plans which can be evaluated against the outcomes   captured through learning analytics. However, no empirical study   is available linking learning designs of a substantial number of   courses with usage of Learning Management Systems (LMS) and   learning performance. Using cluster- and correlation analyses, in   this study we compared how 87 modules were designed, and how   this impacted (static and dynamic) LMS behavior and learning   performance. Our findings indicate that academics seem to design   modules with an invisible blueprint in their mind. Our cluster   analyses yielded four distinctive learning design patterns:   constructivist, assessment-driven, balanced-variety and social   constructivist modules. More importantly, learning design   activities strongly influenced how students were engaging online.   Finally, learning design activities seem to have an impact on   learning performance, in particular when modules rely on   assimilative activities. Our findings indicate that learning   analytics researchers need to be aware of the impact of learning   design on LMS data over time, and subsequent academic   performance.   General Terms  Measurement, Performance, Design.   Keywords            Learning design, Learning analytics, Academic retention.   1. INTRODUCTION  Learning analytics provide institutions with opportunities to   support student progression and to enable personalized, rich   learning [1-3]. With the increased availability of large datasets,   powerful analytics engines [2], and skillfully designed   visualizations of analytics results [4], institutions may be able to   draw on past experience to create supportive, insightful models of   primary (and perhaps real-time) learning processes [5].       While substantial progress has been made in terms of predictive   modeling in the Learning Analytics Knowledge (LAK)   community over the last three to four years [2], one element that   seems to be ignored is learning design.    Conole [6, p121] describes learning design as a methodology for   enabling teachers/designers to make more informed decisions in   how they go about designing learning activities and interventions,   which is pedagogically informed and makes effective use of   appropriate resources and technologies. Learning design is   widely studied in the Higher Education sector, but no study has   yet empirically connected learning designs of a substantial   number of courses with learning behavior in Learning   Management Systems (LMSs) and learning performance. This   study will begin to overcome this gap in learning analytics   research by combining three different sources of data (i.e., data   pertaining to learning design, LMS, and learning performance)   from 40 blended and online modules involving a total of 21,803   learners. In so doing, it will enable LAK researchers to better   understand which learning design elements may be important for   enhancing learning processes and performance.   1.1 Learning design taxonomy  The learning design taxonomy used for this process was   developed as a result of the Jisc-sponsored Open University   Learning Design Initiative (OULDI) [7], and was developed over   five years in consultation with eight Higher Education institutions.   Learning design as described by Conole [6] is process based:   practitioners make informed design decisions with a pedagogical   focus and communicate these to their colleagues and learners.   This is especially relevant for institutions which deliver distance   learning, such as the Open University UK (OU). At the OU,   modules are designed by teams of academics based in a central   location, but delivered to learners by different tutors in a wide   range of locations.    Table 1 shows the learning design taxonomy, which identifies   seven types of learning activity. Assimilative activities relate to   tasks in which learners attend to discipline specific information.   These include reading text (online or offline), watching videos, or   listening to an audio file. By finding and handling information, for   example on the internet or in a spreadsheet, learners take   responsibility for extending their learning, and are therefore   engaged in active learning [8]. Communicative activities refer to   any activities in which students communicate with another person   about module content. Productive activities draw upon   constructionist models of learning, whereby recent research has   indicated that learners who build [9] and co-construct new   artefacts learn effectively [10]. Experimental activities develop   students' intrinsic motivation and industry-relevant skill transfer  [11, p211] by providing learners with the opportunity to apply   their learning in a real life setting. Interactive activities endeavor   to do the same, but in some fields this is not possible: for   example, in medicine, such activities would have health and   Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights   for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or   republish, to post on servers or to redistribute to lists, requires prior   specific permission and/or a fee. Request permissions from  Permissions@acm.org.    LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA    Copyright 2015 ACM 978-1-4503-3417-4/15/03$15.00    http://dx.doi.org/10.1145/2723576.2723600   315    safety implications for either the learner or the person that they   interact with. In these situations, a simulated environment might   be appropriate so that learners can apply their learning to a   realistic setting [12]. Finally, assessment activities encompass all   learning materials focused on assessment, whether enabling   teaching staff to monitor progress (formative); traditional   assessment for measurement purposes [13, p182] (summative); or   activities that allow learners to benchmark against their own or   fellow learners performance (ipsative).    Table 1. Learning design activities    Type of activity Example   Assimilative Attending to   information   Read, Watch, Listen,   Think about, Access.   Finding and   handling   information   Searching for and   processing information   List, Analyse, Collate,   Plot, Find, Discover,   Access, Use, Gather.    Communication Discussing module   related content with at   least one other person   (student or tutor)   Communicate, Debate,   Discuss, Argue, Share,   Report, Collaborate,   Present, Describe.   Productive Actively constructing   an artefact   Create, Build, Make,   Design, Construct,  Contribute, Complete,.    Experiential Applying learning in a   real-world setting    Practice, Apply, Mimic,   Experience, Explore,   Investigate,.   Interactive   /adaptive   Applying learning in a   simulated setting    Explore, Experiment,   Trial, Improve, Model,  Simulate.    Assessment All forms of   assessment   (summarive, formative  and self assessment)    Write, Present, Report,   Demonstrate, Critique.   2. Method   2.1 Setting  This study took place at the OU, the largest higher education   provider of online distance education in Europe. A process of   module mapping (i.e. analyzing and providing visualizations of   the learning activities and resources involved in a module) was   introduced as part of a university-wide learning initiative [14]   which aims to use learning design data for quality enhancement.   The mapping process is an intensive one, typically taking between   one and three days for a single module, depending on the   modules number of credits, structure, and quantity of learning   resources. A team of learning design specialists reviewed all the   available learning materials, classifies the types of activity, and   quantifies the time that students are expected to spend on each   activity.    Classifying learner activity can be subjective, and consistency is   important when using the data to compare module designs across   the univeristy. Therefore, the learning design team held regular   meetings to improve consistency across team members in the   mapping process. Once the mapping process was complete, the   learning design team manager reviewed the module before the   findings were sent to the faculty. Academics had the opportunity   to comment on the data before the status of the design was   finalised. In other words, each mapping was at least reviewed by   three people, which enhanced the reliability and robustness of the   data relating to each learning design.   2.2 Instruments   2.2.1 Learning Design mapping  The learning design tool at the Open Universtiy is a combination   of graphical, text-based tools that are used in conjunction with   learning design activities, which were mandated at particular   times in the design process. In total 87 modules were mapped by   the learning design team in the period January-August 2014. For   each module, the learning outcomes specified by the module team   (pertaining to knowledge and understanding; cognitive skills; key   skills; practical and/of professional skills) were captured by the   learning design specialist. Each activity within the modules   weeks, topics, or blocks was categorised according to the learning   design taxonomy (see Table 1). These categorisations were   captured in an activity planner (or blueprint).    2.2.2 LMS Data  In line with Tempelaar et al. [3], two different types of LMS data   in Moodle were gathered per module in a static and dynamic   manner: total number of visits to the LMS; and average time spent   on LMS. Subsequent derivatives of these two types of data per   week were recorded for week -2 until week 40 (data streams starts   two weeks before the actual start of the module). Although more   fine-grained learning analytics tracking data were available on   types of content, materials and ICT tools (e.g., wikis,   videoconference, discussion forums), given the diversity in usage   and the fact that not all modules used all the ICT tools we   measured, we focused on aggregate user statistics per week across   the LMS., Such data was available for 32 modules at the time this   study was conducted.    2.2.3 Learning performance  Learning performance was calculated by the number of learners   who completed and passed the module relative to the number of   learners who registered for each module. The academic retention   ranged between 28.57% and 100%. These figures do need to be   read in the context of the OUs mission to provide education for   all, regardless of entrance requirements [15].    2.2.4 Data analysis  As a first step, we analyzed the underlying structures and   collective patterns of the seven learning design activities by using   cluster analysis of the 87 modules. In line with the   recommendations of Hair, Tatham, Anderson and Black [16], we   conducted three different types of cluster analyses (K-means,   hierarchical ward, hierarchical furthest distance). Given that the   results were similar in terms of assigned clusters, in the results   section we will report cluster results using K-means analysis, as   this method is most commonly used. We then tested solutions for   2-5 clusters using K-means cluster analyses. The results seemed to   indicate that four clusters fitted the data best (at 5 clusters, too few   meaningful clusters were left). We labelled the clusters using   theoretical concepts [6, 7, 17-19] and in-depth experience of   learning design team.   As a second step, we merged the learning design data with the   LMS and learner retention data based upon module ID and year of   implementation. A mix of 15, 30 and 60 credit modules was   present. 32 modules could be linked with LMS learning behavior,   and 40 with learning performance data. Such data were not yet   available for 23 running modules, as these are currently being   undertaken by learners, while 8 modules (primarily MOOCs)   were not included in standard OU registration processes. For 8   modules the LMS data was not released yet. Finally, 16 module   learning designs referred to future designs for the academic year   316    2015-2016. Follow-up ANOVA and correlation analyses were   conducted using SPSS 21.    3. Results   3.1 Cluster analysis of learning designs  We conducted a K-means cluster analysis to identify common   patterns in how the 87 module teams designed and balanced the   seven learning design activities.       Figure 1 Cluster analysis of learning design   As illustrated in Figure 1, Cluster 1 modules seemed to have a   strong emphasis on assimilative activities, as 58% (SD = 11%) of   learning activities fell into this category, Students undertook   assimilative activities such as reading module materials, watching   videos and YouTube materials, reviewing core concepts and   approaches. In comparison to other modules, Cluster 1 modules   had a lower focus on the other six learning design activities. 24   (28%) modules were assigned to Cluster 1, which we label as   constructivist modules. For example, a first-year science   introductory module focused on understanding principles and   concepts in a range of topics, with several online assessments to   test (individual) learners understanding. Please note that not all   modules in Cluster 1 fit this description, but in comparison to   other clusters modules in Cluster 1 tended to have a relatively   stronger focus on cognition and understanding (in terms of the   taxonomy proposed by [20]). 22 (25%) modules were positioned   in Cluster 2, with a strong focus on assessment (M = 44.54, SD =   12.05), such as formative assessment for learning (e.g. write,   present, report, demonstrate) and summative assessment of   learning [3, 21]. In comparison to other modules, those in Cluster   2 had a relatively limited focus on assimilative, communication,   and interactive learning design activities. For example, an   introductory history course focused on providing a historical   perspective of a particular region in the UK, whereby a range of   assessment tasks were provided focused on culture, society and   nationhood. We label Cluster 2 as assessment-driven.   The 24 (28%) modules in Cluster 3 had a more or less equal   balance between assimilative and assessment learning design   activities, with a relatively high focus on experiential activities.   For example, the health and social care module used a mix of   understanding basic concepts as well as applying these concepts   using case-studies, self-reflections and collaborative approaches.   Therefore, we label Cluster 3 modules as balanced-variety,   whereby a range of different activities were expected from   learners.    Finally, the 16 (18%) modules in Cluster 4 seemed to use more a   learner-centered learning design approach, whereby relatively   more time was devoted towards communication, productive and   interactive activities. For example, in a foreign language module,   a range cognitive, skills-based, reflective and application tasks are   assessed using a mix of technology tools and blended tuition.   Therefore, we label these Cluster 4 modules as social   constructivist.   3.2 Linking learning design activities  Separate Pearson correlation analyses between the seven learning   design activities, total workload, and level of study indicated that   several groups of learning design activities were interrelated.   Workload is the number of hours that students objectively spend   on studying [22, p684]. In this study, workload was calculated by   the learning design team as part of the module mapping process.   Workload has been recognized as a major factor in the teaching   and learning environment[22, p684] and is of particular   importance at the OU.    We found that assimilative activities were negatively related to all   of the other six learning design activities, indicating that focusing   more on cognition and content reduces the focus on other   activities. No other statistically significant correlations were   found. Similar to assimilative activities, assessment was   negatively related to five of six learning design activities, which   may indicate that module teams implicitly or explicitly make a   trade-off between these learning design activities. Total workload   was not significantly related to any of the learning design   activities, indicating that teachers did not reserve any specific   extra time for a particular learning activity. Finally, the level of   the module (year 1, 2, 3, post-graduate) was positively correlated   with communication and total workload. Using ANOVA, no   significant differences were found with respect to disciplines. In   other words, most disciplines used a range of learning design   approaches, which seems contrast with previous findings of   studies [23] highlighting that disciplinary context strongly   influences the learning design.    3.3 Relating learning design with LMS  behavior  We linked the learning designs of 32 modules followed by 19,322   learners with their LMS data. On a total of 2,186,246 occasions,   the LMS was visited by 19,322 students. On average, students   spent 122.71 minutes per week (SD=92.47, range 14.39-167.94)   online during each of the first 10 weeks of the module. This wide   range highlights strong underlying differences in the way modules   were designed. Some modules primarily relied on traditional   methods of distance learning and course delivery via books and   readers, with limited interactions in the LMS [24]. Other modules   provided most or all of their course materials, tasks and learning   activities exclusively online and expected students to engage   actively in the LMS during the week. As a result, LMS activity   should only be regarded as a proxy for student engagement in   formal online activities, as at this point in time the OU does not   systematically collect data about formal or informal offline   activities.   We visually analyzed whether the four clusters lead to different   LMS usage over time. As illustrated in Figure 2, in particular in   the first ten weeks significant differences (using ANOVAs, not   illustrated) are present in terms of average time spent per week   between Cluster 4 social constructivist modules and the other   modules. Please note that for Cluster 3 LMS data for was only   available for one module.    317       Figure 2 Average LMS usage across four clusters   LMS visits were positively related to communication activities   and total (planned) workload, and negatively related to assessment   activities. Average time spent in the LMS correlated positively   with finding and handling information activities, communication   activities, and total workload, whilst, again, a negative relation   was found with assessment activities. Finally, if we split the   average time spent in the LMS for week -2 till week 10, only   communicative activities at week 0 and workload were   significantly correlated. Average time spent during weeks 1-10   were strongly related to modules with substantial learning   activities in terms of finding and handling information, and   communication. In other words, the learning design decisions of   teachers seemed to strongly influence how students were engaging   with the LMS, in particular when more inquiry- or social   constructivist learning activities were included in the learning   design.   3.4 Relating learning design with  learning performance  As a final step, we linked the learning design metrics with   learning performance, as illustrated in Table 2. The only   significant (negative) correlations between the seven learning   design activities and learning performance were with assimilative   activities. Modules with a relatively high proportion of   assimilative learning activities had significantly lower completion   and pass rates than other modules. Furthermore, positive   correlations were found between productive and assessment   activities and pass rates, although these were not   statistically significant.      Table 2 Linking learning design with learning performance      No significant correlations were found between our LMS   indicators and learning performance (not illustrated). Follow-up   ANOVA analyses indicated no significant differences in learning   performance between the four clusters. In other words, although   there are substantial variations in the 40 module designs, our   findings indicated that applying one of the four design templates   did not necessarily disadvantage learners in terms of retention.   However, extensive reliance on assimilative activities did seem to   have a negative influence on learning performance, although   given the relatively small number of modules within the sample   we caution readers against overgeneralization.   4. Discussion  Pedagogy and learning design have played a key role in computer-  assisted learning in the last two decades [6, 19, 25], but research   has not extensively linked learning design to learning behavior   and learner performance [23, 26]. Progress has recently been   made in how (combinations of) individual learning design   elements (e.g., task design, feedback, scaffolding, structure)   influence learning processes and success in experimental and   natural settings within single modules. However, this study was   the first to link a large range of learning designs from multiple   blended and online modules with learning behavior in a Learning   Management System (LMS) and learning performance data.    The studys first important finding is that academics seem to   design modules with an invisible blueprint in their mind. Our   cluster analyses yielded four distinctive learning design patterns   as shown in Figure 2, namely constructivist, assessment-driven,   balanced-variety, and social constructivist modules. This means   that whilst the learning design process intends to stimulate   creativity, upon analysis these unique designs neatly fitted into   four broad theoretical perspectives. This finding suggests that   although creativity is still present in the process (as none of the   designs are identical), academic staff do employ similar   combinations of pedagogical underpinnings into their learning   designs [23, 26].    Our second and perhaps most important finding is that learning   design and learning design activities in particular strongly   influence how students are engaging in our LMS. Particularly in   social constructivist environments, students actively engage with   the LMS, while in constructivist and assessment-based modules,   online activity seems substantially lower. While a vast body of   research in computer supported collaborative learning (CSCL)   [25] has found that learning design influences how people learn,   in learning analytics research learning design seems to be mostly   ignored, or overlooked. Without mapping and linking the planned   learning design activities (e.g., assessments, interactive, or   communication activities) with LMS usage, learning analytics   researchers might find it difficult to explain why certain peaks and   troughs occur over time.    Our third finding is that learning design seems to have an impact   on learning performance. In particular, modules with a heavy   reliance on content and cognition (assimilative activities) seemed   to lead to lower completion and pass rates. The availability of   learning analytics data means that management and course teams   often review courses whilst they are still in progress. If this data   suggests that learners do not perform according to the initial   learning design, it is tempting to take action. Often this includes   providing additional material to learners in the form of reading   lists or additional handouts. As this study found that modules with   a strong reliance on assimilative activities did seem to have a   negative influence on learning performance, it suggests that such   interventions might make matters worse.   5. Conclusions and future work  A substantial limitation of this study is the relatively small linked   sample size. Although the OU learning design team mapped a   substantial amount of 87 modules, only 32 modules containing   LMS data and 40 modules containing learning performance data   could currently be linked due to module completion timescales.   As a result, more advanced regression or structural equation   modeling were not feasible to determine the direct and indirect   relations in our three datasets. In the near future, we would be able   to extend the sample when more data becomes available in order   -400600  -2 1 4 7 10 13 16 19 22 25 28 31 34 37 40 cluster 1 constructivist  cluster 2 assessment driven  cluster 3 balanced-variety  318    to better understand the complex (inter)relations of learning   design on learning processes and outcomes.    Combining this analysis with the learning outcomes data allows   sharing of good practice based upon robust analysis.   Furthermore, a particularly useful feature would be to integrate   this with demographic, individual and socio-cultural data about   students, which may influence whether a learning design is   suitable for a range of learners. In terms of practical implications   for LAK, researchers, teachers and policy makers need to be   aware of how learning design choices made by teachers influence   subsequent learning processes and learning performance over   time. Following Arbaugh [17], there is an urgent need for   researchers and managers to combine research data and   institutional data and work together in order to unpack how   context, learner characteristics, modular and institutional learning   design activities impact the learning journeys of our students.   References   [1] Siemens, G., Dawson, S. and Lynch, G. Improving the quality   of productivity of the higher education sector: Policy and strategy   for systems-level deployment of learning analytics. Solarresearch,   2013.   [2] Tobarra, L., Robles-Gmez, A., Ros, S., Hernndez, R. and   Caminero, A. C. Analyzing the students behavior and relevant   topics in virtual learning communities. Computers in Human   Behavior, 31, 0 (2// 2014), 659-669.   [3] Tempelaar, D. T., Rienties, B. and Giesbers, B. In search for   the most informative data for feedback generation: Learning   Analytics in a data-rich context. Computers in Human   Behavior2014).   [4] Gonzlez-Torres, A., Garca-Pealvo, F. J. and Thern, R.   Humancomputer interaction in evolutionary visual software   analytics. Computers in Human Behavior, 29, 2 (3// 2013), 486-  495.   [5] Verbert, K., Duval, E., Klerkx, J., Govaerts, S. and Santos, J.   L. Learning Analytics Dashboard Applications. American   Behavioral Scientist, 57, 10 (October 1, 2013 2013), 1500-1509.   [6] Conole, G. Designing for Learning in an Open World.   Springer, Dordrecht, 2012.   [7] Cross, S., Galley, R., Brasher, A. and Weller, M. Final Project   Report of the OULDI-JISC Project: Practice, Challenge and   Change in Curriculum Design Process, Communities,   Visualisation and Practice City, 2012.   [8] Michel, N., Cater Iii, J. J. and Varela, O. Active versus passive   teaching styles: An empirical study of student learning outcomes.   Human Resource Development Quarterly, 20, 4 (Winter2009   2009), 397-418.   [9] Simpson, G., Hoyles, C. and Noss, R. Exploring the   mathematics of motion through construction and collaboration.   Journal of Computer Assisted Learning, 22, 2 2006), 114-136.   [10] Noteborn, G., Bohle Carbonell, K., Dailey-Hebert, A. and   Gijselaers, W. The role of emotions and task significance in   Virtual Education. The Internet and Higher Education, 15, 3   2012), 176-183.   [11] Dreher, C., Reiners, T., Dreher, N. and Dreher, H. Virtual   Worlds as a Context Suited for Information Systems Education:   Discussion of Pedagogical Experience and Curriculum Design   with Reference to Second Life. Journal of Information Systems   Education, 20, 2 (Summer2009 2009), 211-224.   [12] Okuda, Y., Bryson, E. O., DeMaria Jr, S., Jacobson, L., Shen,   B., Levine, A. I. and Quinones, J. The Utility of Simulation in   Medical Education: What Is the Evidence Mount Sinai Journal   of Medicine, 76, 4 2009), 330-343.   [13] Miller, T. Formative computer-based assessment in higher   education: the effectiveness of feedback in supporting student   learning. Assessment & Evaluation in Higher Education, 34, 2   2009), 181-192.   [14] Galley, R. and Toetenel, L. Learning design: supporting the   qualification & module design process. The Open University UK,   City, 2014.   [15] Richardson, J. T. E. Approaches to studying across the adult   life span: Evidence from distance education. Learning and   Individual Differences, 26, 0 (8// 2013), 74-80.   [16] Hair, J. F., Tatham, R. L., Anderson, R. E. and Black, W.   Multivariate data analysis. Pearson Prentice Hall Upper Saddle   River, NJ, 2006.   [17] Arbaugh, J. B. Is there an optimal design for on-line MBA   courses Acad. Manag. Learn. Educ., 4, 2 (Jun 2005), 135-149.   [18] Vygotsky, L. S. Mind in society. Harvard University Press,   Cambridge, MA, 1978.   [19] Sweller, J., van Merrinboer, J. J. G. and Paas, F. G. W. C.   Cognitive Architecture and Instructional Design. Educational   Psychology Review, 10, 3 (1998/09/01 1998), 251-296.   [20] Bloom, B. S., Englehart, M. D., Furst, E. J., Hill, W. H. and   Krathwohl, D. R. Taxonomy of educational objectives: Handbook   I: Cognitive domain. David McKay, New York, 1956.   [21] Boud, D. and Falchikov, N. Aligning assessment with   longterm learning. Assessment & Evaluation in Higher  Education, 31, 4 (2006/08/01 2006), 399-413.   [22] Kyndt, E., Berghmans, I., Dochy, F. and Bulckens, L. Time   is not enough. Workload in higher education: a student   perspective. Higher Education Research & Development, 33, 4   2014), 684-698.   [23] Rienties, B., Kaper, W., Struyven, K., Tempelaar, D. T., Van   Gastel, L., Vrancken, S., Jasinska, M. and Virgailaite-  Meckauskaite, E. A review of the role of Information   Communication Technology and course design in transitional   education practices. Interactive Learning Environments, 20, 6   2012), 563-581.   [24] Rumble, G. Open learning,distance learning, and the   misuse of language. Open learning, 4, 2 1989), 28-36.   [25] Eysink, T. H. S., de Jong, T., Berthold, K., Kolloffel, B.,   Opfermann, M. and Wouters, P. Learner Performance in   Multimedia Learning Arrangements: An Analysis Across   Instructional Approaches. American Educational Research   Journal, 46, 4 (August 20, 2009 2009), 1107-1149.   [26] Kirschner, P. A., Sweller, J. and Clark, R. E. Why Minimal   Guidance During Instruction Does Not Work: An Analysis of the   Failure of Constructivist, Discovery, Problem-Based,   Experiential, and Inquiry-Based Teaching. Educational   Psychologist, 41, 2 (2006/06/01 2006), 75-86.     319      "}
{"index":{"_id":"52"}}
{"datatype":"inproceedings","key":"VanInwegen:2015:AIA:2723576.2723616","author":"Van Inwegen, Eric and Adjei, Seth and Wang, Yan and Heffernan, Neil","title":"An Analysis of the Impact of Action Order on Future Performance: The Fine-grain Action Model","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"320--324","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723616","doi":"10.1145/2723576.2723616","acmid":"2723616","publisher":"ACM","address":"New York, NY, USA","keywords":"action order, binning, data mining, hint use, prediction of future success, tabling","abstract":"To better model students' learning, user modelling should be able to use the detailed sequence of student actions to model student knowledge, not just their right/wrong scores. Our goal is to analyze the question: Does it matter when a hint is used. We look at students who use identical attempt counts to get the right answer and look for the impact of help use and action order on future performance. We conclude that students who use hints too early do worse than students who use hints later. However, students who use hints, at times, may perform as well as students who do not use hints. This paper makes a novel contribution showing for the first time that paying attention to the precise sequence of hints and attempts allows better prediction of students' performance, as well as to definitively show that, when we control for the number of attempts and hints, students that attempt problems before asking for hints show higher performance on the next question. This analysis shows that the pattern of hints and attempts, not just their numbers, is important.","pdf":"An Analysis of the Impact of Action Order on Future  Performance: the Fine-Grain Action Model    Eric Van Inwegen Seth Adjei Yan Wang Neil Heffernan  Worcester Polytechnic Institute   100 Institute Rd  Worcester, MA, 01609-2280   +1-508-831-5569  {egvaninwegen, saadjei, ywang14, nth} @wpi.edu      ABSTRACT  To better model students learning, user modelling should be able  to use the detailed sequence of student actions to model student  knowledge, not just their right/wrong scores. Our goal is to  analyze the question: Does it matter when a hint is used. We  look at students who use identical attempt counts to get the right  answer and look for the impact of help use and action order on  future performance. We conclude that students who use hints too  early do worse than students who use hints later. However,  students who use hints, at times, may perform as well as students  who do not use hints. This paper makes a novel contribution  showing for the first time that paying attention to the precise  sequence of hints and attempts allows better prediction of  students performance, as well as to definitively show that, when  we control for the number of attempts and hints, students that  attempt problems before asking for hints show higher  performance on the next question. This analysis shows that the  pattern of hints and attempts, not just their numbers, is important.    General Terms  Algorithms, Measurement, Performance, Reliability   Keywords  Action order, Hint use, Tabling, Binning, Prediction of future  success, Data Mining   1. INTRODUCTION  Intelligent Tutoring Systems usually offer help in the form of  messages, scaffolding, etc. to students who cannot (or believe  they cannot) solve a problem on their own. Previous work has  analyzed the apparent effect of using hints on learning. Beck,  et.al. [2] have studied the question of the assumption that help is,  in fact, helpful. Hawkins et. al. [6] and Wang and Heffernan [13]  have studied the likelihood of future success of students using  particular combinations of hits and attempts (in the Assistance  Model - AM). Zhu et. al. [14] and Duong et. al. [4] have looked at  clickstream data to be able to make predictions (in the Sequence  of Actions model  SOA); they were able to show that students  who use hints first do not perform as well in the future when   compared to students who use hints later in their action order.  What was insufficient about these studies was that the bins were  not well controlled. In AM, action order is ignored; in SOA,  temporal spacing and whether the hint gives the answer were not  taken into account.  User modelling algorithms, such as  Knowledge Tracing (KT) [3] and Performance Factor Analysis  (PFA) [9], dont use the order of actions in their computations; at  least intuitively, this seems to be an omission.    This paper seeks to better understand the effects of action order  and help requests. Our basic answer to Does it matter when a  hint is used is: Yes; students who make at least one attempt  before using a hint outperform their peers who use a hint right  away, even controlling for the number of attempts made.   1.1 Background  The role of predicting future success, based on answers given in  Intelligent Tutoring Systems (ITSs) is useful both to the system  (e.g. attempting to modify work stream based on students needs  of extra work), and has been used to predict future outcomes in  high-stakes paper-based testing [5]. Well-known examples  include Knowledge Tracing (KT) and Performance Factors  Analysis. Plenty of work has been done to try to improve KT by  using additional information. A quick look at the literature gives  some examples of trying to improve KT by: adding  individualizations [8], using parameter learning [10] incorporating  time between instances to model forgetting [11], and examining  the role of first response time [12].  Although these methods have  had some success, neither do they use the order of actions, nor do  they examine hint use.   Beck et. al. [2] used KT to explore the impact of help on student  performance. Through the use of three distinct methods, they  show that help has an impact on learning, but do not address the  question when is help helpful  In this present study we present an  investigation that shows that there are indeed times when help  improves future success.   Problems analyzed had between one and seven hints. When a  problem has multiple hints, successive hints usually give more  information to solve the problem. See Figure 1 for an example.  In  Figure 1, you can see that the first hint gives the appropriate  equations to use in this problem. The second hint shows a student  exactly how to plug the values into the equations. The third hint   the bottom-out-hint simply gives the problems answer. While  not all problems in the ASSISTments system use this exact  pattern, the general trends are that more hints give successively  more help and that the last hint gives the answer.   Duong et. al. in the Sequence of Action (SOA) model [4] binned  students into only five categories: one (correct) attempt, only  attempts (student used multiple attempts, and no hints), all hints  (only hints before a single attempt), Alternative Attempt First (a   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org.                            LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA  Copyright 2015 ACM 978-1-4503-3417-4/15/03$15.00         http://dx.doi.org/10.1145/2723576.2723616   320    mix of hints and attempts, starting with an attempt), and  Alternative Hint First (a mix of hints and attempts, starting with a  hint).   One complaint against Duong et. al.s method is that they do not  take into account the distinctly different kinds of hints in the  ASSISTments system; these differences have an impact on  learning that is not accounted for in their analysis. Almost  universally, the final hint in a series (called the bottom out)  simply gives the answer to that problem; this makes the use of  that hint significantly different than the use of any other hint. (In  the ASSISTments system, students cannot proceed to the next  question until they have entered the correct answer; a question  that does not have the bottom out hint will trap students and not  allow them to finish an assignment.) In Duongs method, all hints  were treated identically, even though some are instructional aides,   while others simply give the answer with little to no instruction.   In the Assistance Model (AM), Wang studied groups of students  binned into 12 categories created from the concatenation of  divisions along two axes (attempts and hint use). Hint use was  divided into four categories: 0, (0-50%], (50%-100%), and 100%.  Attempts were divided into three categories: 1, 2-5, 6-infinity.  The Hawkins et al study takes into account the bottom-out hint,  but fails to look at the action order. Also, any number of attempts  between 2 and 5 inclusive are treated identically. This means that  a student who makes four mistakes before getting the right answer  is treated the same as a student who makes only one mistake. We  test this assumption in our analysis.   In order to explore the effect of requesting help on future  performance, ideally, we would want to know that students are  comparable. In a randomized, controlled trial setting, we would  want children that are as similar as possible to be given hints  randomly throughout their attempts. Clearly, this is nearly  impossible to create. Some notable attempts have been made.    One example of a non-data-mining case would be Attali and  Powers [1]. In their tests, they used randomized controlled trials  before GREs to test the effects of help. Although RCTs would  be perhaps the most ideal way to test the effects of help, they may  also be the least practical, especially when trying to determine the  effects of help when used throughout attempts. The number of  students that would be needed quickly becomes prohibitive.   Lastly, user modelling algorithms such as KT and PFA attempt to  learn four or three parameters per skill. Using a model similar to  Duong et. al.s [4], we attempt to learn one parameter per skill (in  a logistic regression) and subdivide students into bins based on  attempts and hint use. This gives us a single parameter per skill  and a small number (~20) to learn based on our bins.    1.2 Questions we are seeking to answer:  1. Does help correlate to an improvement in future   performance, and does it matter how many attempts are  made compared to help requested (and vice versa)     2. Does the order of requesting help (interspersed within  attempts) make a difference to learning   3. Can action order be used to predict future performance   2. METHODS  The dataset we use1 has click-stream data; by which we mean  the order of attempts and hints used to complete a problem.  Having this additional data allows for new analyses of student  performance. Using this data enables the use of fewer parameters  in a user model than KT and PFA.   The original goal of our analysis was to determine the impact of  when hints are used.  We came to the conclusion that this analysis  could be used to create a model that predicts future performance.   We explore the impact of the different factors in our final model                                                    1 The dataset analyzed comes from ASSISTments (an online  learning system developed and maintained at Worcester  Polytechnic Institute). In this system, hints are available on  demand, but must be requested, while scaffolding is automatically  given to students when they submit an incorrect answer.   Scaffolded questions (<10%) were ignored in this analysis.  The  dataset included ~ 400K problem-instances, ~14K students, and  165 skills.  Multiple choice questions were ignored; all questions  have fill-in style answers.  Questions covered middle and high  school mathematics topics.     Figure 1: A sample problem from ASSISTments showing  three hints giving varying degrees of help   321    individually to explain their importance; we then explain how the  final model was created.  In short, we use these factors to  differentiate students into bins, calculate the probability of next  problem correctness (NPC) for 80% of students in each bin, and  test our model on the remaining 20%.    2.1 Attempts and help  2.1.1 Using multiple attempts  This feature in our model hardly needs explaining (and barely  deserves a section).  The more attempts a student uses, the worse  his or her chance of success on the next problem.  The only  question we need to answer is how to use attempt count to  categorize students.   Similar to the work done by Hawkins et. al. [6] and Duong et. al.  [4], we grid students by attempts and hints.  However, instead of  grouping attempts as [1, (2-5) and 6+], we leave attempts as  individual bins for attempts of 1-4, but only for instances where  no hints are used. Other instances are regrouped (in an effort to  keep the number of divisions within our model reasonable).    2.1.2 Using multiple hints  Much as attempt use, we would expect that students who use  more hints have a lower chance of success on the next question.   However, based on the notion that these students are receiving  help (additional information), this might not be true.  Well-written  hints could give a barely struggling student information that helps  them in the future.   In our analysis, we initially start by treating different numbers of  hint use individually.  When a student uses only one attempt, there  are differences between the outcomes for different hint uses.   However, as students use more attempts, the impact of hint use  becomes smaller; attempt/hint use combinations are regrouped.   2.2 Bottom-out hint use  In ASSISTments, in order to advance to the next problem, a  student must type in the correct answer.  In the vast majority of  cases, this means that the last hint gives the final answer.  (See  Figure 1 above for an example.)  This means that this hint does  something different than give instruction.  A logical question  would be to examine the difference between using the bottom out  hint and compare the outcome to using the penultimate hint.   2.3 Action order  In order to evaluate the effect of action order on student  knowledge, a few combinations of attempts (a), hints (h) and  correct attempt (A) are subdivided according to action order.  By  comparing the results of groups where the only difference is the  order of actions, we hope to tease out the effect that action order  has on future performance.  E.g. instances of two attempts and  one hint can either occur as a-h-A (wrong attempt, hint, right  attempt) or h-a-A (hint, wrong attempt, right attempt).  If action  order does not have an effect on future performance, there should  be no difference between the groups.  It is also useful to compare  slightly different groups.  E.g. comparing a-A with a-h-A and h-a- A lets us compare the effect of hints use to no hint use with a  constant number of attempts.  Due to sample sizes, only a few  combinations are explored.   3. RESULTS AND ANALYSIS  In the analyses that follow, we primarily focus on the probability  of the student getting the next problem correct (NPC) within the  same skill as the measure of future success.     3.1 Attempts and help  There is an interplay between the effect of attempts used and hints  used on probability of next problem correctness.  When there are  large numbers of both hints and attempts used, keeping individual  combinations no longer makes sense.  Our model regroups  predictions based on similarity of outcomes, and similarity of  combinations.    3.1.1 Using multiple attempts  With attempt count, values range from one to nearly 1000.   However, the use of one to four attempts accounts for well over  90% of all problems.  For this reason, the domain of attempt uses  is: [1, 2, 3, 4, 5+].  However, when more hints are used, there is  less of an impact of the number of attempts used.  Thus, when  many attempts and hints are used, the granularity of attempts used  is reduced.   3.1.2 Using multiple hints  If we look at how NPC changes with respect to hint use (leaving  out bottom-out hint use), we must decide how fine-grain our bins  should be.  Non-bottom-out-hint use ranges from zero uses to six.   However, the number of problems that use more than three hints  is relatively small.  The impact of using multiple hints changes  based on how many attempts are made.  Especially after the  action order analysis, it makes sense to regroup hints based on the  number of attempts made.     3.2 Bottom-out hint use  To analyze the impact of using the last hint, we compare it to  students who use the penultimate hint.  We find that, across three  student knowledge groups (low, medium and high2), there is a  reliable difference in future performance between penultimate hint  use and bottom-out hint use.     Table 1: The difference between using the penultimate hint  and bottom-out-hint.   Penultimate hint avg NPC std NPC n  Low knowledge students 0.4089 0.4917 3,061  Medium knowledge students 0.6543 0.4756 6,143  High knowledge students 0.8430 0.3640 1,594  Bottom-out-hint  avg NPC std NPC n  Low knowledge students 0.2939 0.4556 30,757  Medium knowledge students 0.5439 0.4981 42,658  High knowledge students 0.7616 0.4262 4,654   Table 1 suggests that making use of the bottom hint (compared to  using the penultimate hint) has roughly 10% (absolute) difference  in future performance.  This is almost the same effect as dropping  a student by one knowledge category.  From this analysis, it is  clear that bottom-out-hints must be treated differently from any  other hint.     As for grouping students who use the bottom-out-hint, they can be  broken into two distinct groups: those who go through the bottom  out hint and then type in the correct answer, and those who try at  least once before they type in the answer.  The former category  had a future success rate of only 32%, while the latter had a rate  of nearly 50% (fairly consistently across all attempts counts).   This gives us 22 bins; this initial version (not shown) differs from                                                    2 Knowledge groups are: [0 to -/2), [-/2 to +/2), [+/2 to  1], where  = mean knowledge score and  = standard deviation.   322    the Assistance Model only slightly.  This 22-bin version will be  modified by action order after analysis of the impact action-order.   3.3 Action order  When examining the impact of action order on future  performance, there are only a few combinations of action and hint  use that have large enough numbers of instances that subdivision  is warranted.  For this dataset, the groupings worth investigating  are (2a,0h) & (2a,1h) and (3a, 0h) & (3a,1h).  The zero hint  groups are a useful comparison to the hint-used groups, and, this  comparison gives surprising results.   The slight difference between a-A and a-h-A may not be  surprising; however, the large difference between a-h-A and h-a- A demonstrates the impact that action order has on future success.   The p-value for comparing a-h-A and h-a-A is 0.0002.  (P-values  for a-A to a-h-A and a-A to h-a-A are 0.01 and <0.0001,  respectively.)  A model that uses only action combination to  generate its prediction array is leaving out information that can  improve a prediction.   (Key: a = wrong attempt; h = hint used; A = right answer.)   Table 2a: 2 attempts, 0 or 1 hint, no bottom out hints   order avg NPC std NPC n  a-A 0.732 0.443 30,402  a-h-A 0.713 0.452 3,123  h-a-A 0.624 0.485 414     Table 2b: 3 attempts, 0, 1 hint, no bottom out hint   order avg NPC std NPC n  a-a-A 0.670 0.470 7,810  a-a-h-A 0.676 0.468 1,001  a-h-a-A 0.687 0.464 500  h-a-a-A 0.583 0.495 153     The implication here is that the order of action makes a  difference.3  If the first action taken is a hint, the likelihood of  future success is reduced.  If we are to use action combinations to  predict student outcome, the first action becomes important.   3.4 Using actions to predict future success  The bins described in 3.2 can be subdivided by action order, much  as we did in section 3.3.  However, even if we merely subdivide  all possible bins (nine have only one order) by first action, we  arrive at 35 different bins.  Intuitively, this seems like too many.   Upon examination of the numbers of questions that fall into the  bins, it quickly becomes apparent that conclusions made on 35  bins will be statistically unreliable due to small differences in  predictions and small numbers of instances in each bin.  With that,  we regroup bins based on similarity of prediction and actions.  We  call this the Fine-Grain-Action model (FGA).     Table 3 gives the values based on student actions in the FGA.   (Number in each bin is given below each bins value.)  To  differentiate between the two possible first actions, the table has  been sub-divided into a (attempt) and b (hint).  It is impossible to  have only one attempt come before a hint use.  Likewise, it is                                                    3 P-values for a-a-A to h-a-a-A and (a-a-h-A & a-h-a-A) to h-a-a- A are 0.016 and 0.011, respectively   impossible to have the first action be a hint if no hints are used.  It  may help to think of Table 3 a / b as a third dimension and  overlap the two sub-tables.   Table 3a: The Fine-Grain-Action model   1st action = attempt    1 att. 2 att. 3 att. 4 att. 5 + att.   0 hint 0.8156 215,870  0.7380  22,229   0.6771  5,616   0.6380  2,326   0.6211  2,518   1 hint ----- 0.7012 3,414  0.6321  1,408   2 hint     -----   0.5812   4,011 3+  hint      -----        BOH 0.5099 40,652     Table 3b: The Fine-Grain-Action model   1st action = hint    1 att. 2 att. 3 att. 4 att. 5 + att.  0 hint ----- ----- ----- ----- -----   1 hint 0.7083 1,958  0.6192   407  0.5702   114   2 hint 0.5250 541 0.4688  465 3+   hint  0.4118   289     BOH 0.3396 13,989    Once the bins were created, predicting future success rate was as  simple as using 80% of students to create a training subset, and  testing the results on the other 20%.  Bin values were calculated;  in addition, a multivariate logistic regression was run for each  skill (much like a very simple PFA).   This gives us a chance to  compare the results of KT (fit in MATLAB using [7]), as well as  two earlier attempts at binning students based on attempts and  hint use (SOA by Duong et al and AM by Wang et al.).  The  comparisons of the methods are found in Table 4.     The RMSEs presented in Table 4 represent RMSE by student.  We felt that this is a better comparison between the tabling  models as it removes the effect that a student who has a larger  number of instances in the set would thus have a larger weight in  the final calculation of the error.  R-squared values were found by  using the Excel (2010) function applied to all data points; AUC  was calculated using SPSS.   Table 2: RMSE results comparing Knowledge Tracing (KT),  Sequence of Action model (SOA), Assistance Model (AM), and   the Fine Grain Action Model (FGA)   Model RMSE R2 AUC  KT 0.4069 0.1147 0.704   SOA 0.4036 0.1155 0.708   AM 0.4002 0.1268 0.714   FGA 0.3996 0.1282 0.715   Although the differences in RMSE between the tabling models are  slight, the analyses from section 3 demonstrate that incorporating  action order into bins derived from action combinations is helpful  in making more robust predictions for students who have not   323    gotten a question right on the first attempt.  (Its pretty easy to  predict the future success of students who are getting questions  right.)  However, examining the RMSEs of most of the  individual bins in FGA demonstrates that this model has room for  improvement; bins that predict close to 0.500 have RMSEs close  to 0.500 (which is essentially the predictive ability of a coin toss).   Two bins even have RMSEs larger than 0.5!  Needless to say, a  prediction worse than a coin toss needs improvement.   Even though some bins are clearly using the wrong algorithm, the  basic premise that we should use both action combination and  action order to improve the prediction of future success is valid.     4. CONCLUSION  From these analyses, we can conclude that help does, in certain  circumstances, help.  We feel that we can answer our questions as:   1. Can generalizations of help use be made  We can conclude that there are times when help use leads to a  better chance of future success.  However, help use must be  combined with student attempts.  Students who need too much  help or too many attempts are at a disadvantage for their future  performance; students who use all available help (especially when  they only make one attempt) are among the least likely to succeed  in the future.   2. Does the order of hint use matter  It is better for a student to try a problem before seeking help.   However, after a couple of attempts, students may be more likely  to do better with help (or are at least not at a disadvantage).    3. Can action order be used to predict student performance  This knowledge can be used to create a better prediction of future  student success.  However, when analyzing the error of the  prediction, we find that there is still plenty of room for  improvement in categories of high hint use and large numbers of  attempts.  Even with a data set of ~400K attempts, we are  encountering small bin sizes and thus statistical insignificance  when trying to examine the effects of action order with larger  combinations.  In addition, there are some circumstances where  our methods clearly fail and a new algorithm is needed.   4.1 Contributions  Although other authors have examined the role of hints and  attempt numbers [5, 7, 13, 14], no one has been able to examine  the action combination and order in the detail presented herein.   What we find is that the order of hint use and attempts (and not  just the combinations) plays a role in the future performance of  that student.  This analysis also gives further insight into  understanding the circumstances under which hints enable  learning.  The detail of analysis within the action shows that this  method gives new insight to user modelling.  To our knowledge,  no author has shown the detail of analysis to describe how action  order impacts student performance.   4.2 Future work  The Fine-Grain Action model demonstrates that action-order  analysis enhances prediction algorithms on a dataset from  ASSISTments; the obvious next question is the applicability to  other datasets.  Additional future work could include re-analyzing  the bins to simplify the model (without loss of predictive power)  and using a students history to see if they improve over time.   5. ACKNOWLEDGEMENTS  We acknowledge funding from NSF (# 1440753, 1316736,  1252297, 1109483, 1031398, 0742503), ONR's 'STEM Grand   Challenges' and IES (# R305A120125 & R305C100024) grant for  ASSISTments and support of the author.   6. CITATIONS  [1] Attali, Y., & Powers, D. (2010). Immediate feedback and   opportunity to revise answers to open-ended questions.  Educational and Psychological Measurement, 70(1), 22- 35.   [2] Beck, J. E., Chang, K., Mostow, J., & Corbett, A. (2008).  Does help help Introducing the Bayesian Evaluation and  Assessment methodology. Intelligent Tutoring Systems.  Springer Berlin Heidelberg.   [3] Corbett, A. T., & Anderson, J. R. (1994). Knowledge  tracing: Modeling the acquisition of procedural  knowledge. User modeling and user-adapted interaction,  4(4), 253-278.   [4] Duong, H. D., Zhu, L., Wang, Y., & Heffernan, N. T.  (2013). A Prediction Model Uses the Sequence of  Attempts and Hints to Better Predict Knowledge: Better to  Attempt the Problem First, Rather Than Ask for a Hint.  Paper Submitted to EDM.   [5] Feng, M., Heffernan, N. T., & Koedinger, K. R. (2006).  Predicting state test scores better with intelligent tutoring  systems: developing metrics to measure assistance  required. Intelligent Tutoring Systems. Springer Berlin  Heidelberg.   [6] Hawkins, W., Heffernan, N., Wang, Y., & Baker, R. S.  Extending the Assistance Model: Analyzing the Use of  Assistance over Time.   [7] Murphy, K.: Bayes Net Toolbox for Matlab. <  https://code.google.com/p/bnt/ > Accessed 4 September,  2014   [8] Pardos, Z. A., & Heffernan, N. T. (2010). Modeling  individualization in a bayesian networks implementation  of knowledge tracing. User Modeling, Adaptation, and  Personalization, 255-266.   [9] Pavlik Jr, P. I., Cen, H., & Koedinger, K. R. (2009).  Performance Factors Analysis--A New Alternative to  Knowledge Tracing. Online Submission.   [10] Qiu, Y., Pardos, Z. A., & Heffernan, N. T. (2012).  Towards Data Driven Model Improvement. FLAIRS  Conference.   [11] Qiu, Y., Qi, Y., Lu, H., Pardos, Z. A., & Heffernan, N. T.  (2011). Does Time Matter Modeling the Effect of Time  with Bayesian Knowledge Tracing. EDM.   [12] Wang, Y., & Heffernan, N. T. (2012). Leveraging First  Response Time into the Knowledge Tracing Model.  International Educational Data Mining Society.   [13] Wang, Y., & Heffernan, N. T. (2011). The  Assistance   Model: Leveraging How Many Hints and Attempts a  Student Needs. FLAIRS Conference.   [14] Zhu, L., Wang, Y., & Heffernan, N. T. The Sequence of  Action Model: Leveraging the Sequence of Attempts and  Hints.   324    p300-jimenez-Gomez  p305-pardo  p310-hickey  p315-rienties  p320-vanInwegen  <<   /ASCII85EncodePages false   /AllowTransparency false   /AutoPositionEPSFiles true   /AutoRotatePages /None   /Binding /Left   /CalGrayProfile (Gray Gamma 2.2)   /CalRGBProfile (sRGB IEC61966-2.1)   /CalCMYKProfile (U.S. Web Coated 050SWOP051 v2)   /sRGBProfile (sRGB IEC61966-2.1)   /CannotEmbedFontPolicy /Error   /CompatibilityLevel 1.5   /CompressObjects /Off   /CompressPages true   /ConvertImagesToIndexed true   /PassThroughJPEGImages true   /CreateJobTicket true   /DefaultRenderingIntent /Default   /DetectBlends true   /DetectCurves 0.1000   /ColorConversionStrategy /sRGB   /DoThumbnails true   /EmbedAllFonts true   /EmbedOpenType false   /ParseICCProfilesInComments true   /EmbedJobOptions true   /DSCReportingLevel 0   /EmitDSCWarnings false   /EndPage -1   /ImageMemory 1048576   /LockDistillerParams true   /MaxSubsetPct 100   /Optimize true   /OPM 1   /ParseDSCComments true   /ParseDSCCommentsForDocInfo true   /PreserveCopyPage true   /PreserveDICMYKValues true   /PreserveEPSInfo false   /PreserveFlatness true   /PreserveHalftoneInfo false   /PreserveOPIComments false   /PreserveOverprintSettings false   /StartPage 1   /SubsetFonts true   /TransferFunctionInfo /Apply   /UCRandBGInfo /Remove   /UsePrologue false   /ColorSettingsFile ()   /AlwaysEmbed [ true     /AgencyFB-Bold     /AgencyFB-Reg     /Aharoni-Bold     /Algerian     /Andalus     /AngsanaNew     /AngsanaNew-Bold     /AngsanaNew-BoldItalic     /AngsanaNew-Italic     /AngsanaUPC     /AngsanaUPC-Bold     /AngsanaUPC-BoldItalic     /AngsanaUPC-Italic     /Aparajita     /Aparajita-Bold     /Aparajita-BoldItalic     /Aparajita-Italic     /ArabicTypesetting     /Arial-Black     /Arial-BlackItalic     /Arial-BoldItalicMT     /Arial-BoldMT     /Arial-ItalicMT     /ArialMT     /ArialNarrow     /ArialNarrow-Bold     /ArialNarrow-BoldItalic     /ArialNarrow-Italic     /ArialRoundedMTBold     /ArialUnicodeMS     /BaskOldFace     /Batang     /BatangChe     /Bauhaus93     /BellMT     /BellMTBold     /BellMTItalic     /BerlinSansFB-Bold     /BerlinSansFBDemi-Bold     /BerlinSansFB-Reg     /BernardMT-Condensed     /BlackadderITC-Regular     /BodoniMT     /BodoniMTBlack     /BodoniMTBlack-Italic     /BodoniMT-Bold     /BodoniMT-BoldItalic     /BodoniMTCondensed     /BodoniMTCondensed-Bold     /BodoniMTCondensed-BoldItalic     /BodoniMTCondensed-Italic     /BodoniMT-Italic     /BodoniMTPosterCompressed     /BookAntiqua     /BookAntiqua-Bold     /BookAntiqua-BoldItalic     /BookAntiqua-Italic     /BookmanOldStyle     /BookmanOldStyle-Bold     /BookmanOldStyle-BoldItalic     /BookmanOldStyle-Italic     /BookshelfSymbolSeven     /BradleyHandITC     /BritannicBold     /Broadway     /BrowalliaNew     /BrowalliaNew-Bold     /BrowalliaNew-BoldItalic     /BrowalliaNew-Italic     /BrowalliaUPC     /BrowalliaUPC-Bold     /BrowalliaUPC-BoldItalic     /BrowalliaUPC-Italic     /BrushScriptMT     /Calibri     /Calibri-Bold     /Calibri-BoldItalic     /Calibri-Italic     /Calibri-Light     /Calibri-LightItalic     /CalifornianFB-Bold     /CalifornianFB-Italic     /CalifornianFB-Reg     /CalisMTBol     /CalistoMT     /CalistoMT-BoldItalic     /CalistoMT-Italic     /Cambria     /Cambria-Bold     /Cambria-BoldItalic     /Cambria-Italic     /CambriaMath     /Candara     /Candara-Bold     /Candara-BoldItalic     /Candara-Italic     /Castellar     /Centaur     /Century     /CenturyGothic     /CenturyGothic-Bold     /CenturyGothic-BoldItalic     /CenturyGothic-Italic     /CenturySchoolbook     /CenturySchoolbook-Bold     /CenturySchoolbook-BoldItalic     /CenturySchoolbook-Italic     /Chiller-Regular     /ColonnaMT     /ComicSansMS     /ComicSansMS-Bold     /Consolas     /Consolas-Bold     /Consolas-BoldItalic     /Consolas-Italic     /Constantia     /Constantia-Bold     /Constantia-BoldItalic     /Constantia-Italic     /CooperBlack     /CopperplateGothic-Bold     /CopperplateGothic-Light     /Corbel     /Corbel-Bold     /Corbel-BoldItalic     /Corbel-Italic     /CordiaNew     /CordiaNew-Bold     /CordiaNew-BoldItalic     /CordiaNew-Italic     /CordiaUPC     /CordiaUPC-Bold     /CordiaUPC-BoldItalic     /CordiaUPC-Italic     /CourierNewPS-BoldItalicMT     /CourierNewPS-BoldMT     /CourierNewPS-ItalicMT     /CourierNewPSMT     /CurlzMT     /DaunPenh     /David     /David-Bold     /DFKaiShu-SB-Estd-BF     /DilleniaUPC     /DilleniaUPCBold     /DilleniaUPCBoldItalic     /DilleniaUPCItalic     /DokChampa     /Dotum     /DotumChe     /Ebrima     /Ebrima-Bold     /EdwardianScriptITC     /Elephant-Italic     /Elephant-Regular     /EngraversMT     /ErasITC-Bold     /ErasITC-Demi     /ErasITC-Light     /ErasITC-Medium     /EstrangeloEdessa     /EucrosiaUPC     /EucrosiaUPCBold     /EucrosiaUPCBoldItalic     /EucrosiaUPCItalic     /EuphemiaCAS     /FangSong     /FelixTitlingMT     /FootlightMTLight     /ForteMT     /FranklinGothic-Book     /FranklinGothic-BookItalic     /FranklinGothic-Demi     /FranklinGothic-DemiCond     /FranklinGothic-DemiItalic     /FranklinGothic-Heavy     /FranklinGothic-HeavyItalic     /FranklinGothic-Medium     /FranklinGothic-MediumCond     /FranklinGothic-MediumItalic     /FrankRuehl     /FreesiaUPC     /FreesiaUPCBold     /FreesiaUPCBoldItalic     /FreesiaUPCItalic     /FreestyleScript-Regular     /FrenchScriptMT     /Gabriola     /Garamond     /Garamond-Bold     /Garamond-Italic     /Gautami     /Gautami-Bold     /Georgia     /Georgia-Bold     /Georgia-BoldItalic     /Georgia-Italic     /Gigi-Regular     /GillSansMT     /GillSansMT-Bold     /GillSansMT-BoldItalic     /GillSansMT-Condensed     /GillSansMT-ExtraCondensedBold     /GillSansMT-Italic     /GillSans-UltraBold     /GillSans-UltraBoldCondensed     /Gisha     /Gisha-Bold     /GloucesterMT-ExtraCondensed     /GoudyOldStyleT-Bold     /GoudyOldStyleT-Italic     /GoudyOldStyleT-Regular     /GoudyStout     /Gulim     /GulimChe     /Gungsuh     /GungsuhChe     /Haettenschweiler     /HarlowSolid     /Harrington     /HighTowerText-Italic     /HighTowerText-Reg     /Impact     /ImprintMT-Shadow     /InformalRoman-Regular     /IrisUPC     /IrisUPCBold     /IrisUPCBoldItalic     /IrisUPCItalic     /IskoolaPota     /IskoolaPota-Bold     /JasmineUPC     /JasmineUPCBold     /JasmineUPCBoldItalic     /JasmineUPCItalic     /Jokerman-Regular     /JuiceITC-Regular     /KaiTi     /Kalinga     /Kalinga-Bold     /Kartika     /Kartika-Bold     /KhmerUI     /KhmerUI-Bold     /KodchiangUPC     /KodchiangUPCBold     /KodchiangUPCBoldItalic     /KodchiangUPCItalic     /Kokila     /Kokila-Bold     /Kokila-BoldItalic     /Kokila-Italic     /KristenITC-Regular     /KunstlerScript     /LaoUI     /LaoUI-Bold     /Latha     /Latha-Bold     /LatinWide     /Leelawadee     /Leelawadee-Bold     /LevenimMT     /LevenimMT-Bold     /LilyUPC     /LilyUPCBold     /LilyUPCBoldItalic     /LilyUPCItalic     /LucidaBright     /LucidaBright-Demi     /LucidaBright-DemiItalic     /LucidaBright-Italic     /LucidaCalligraphy-Italic     /LucidaConsole     /LucidaFax     /LucidaFax-Demi     /LucidaFax-DemiItalic     /LucidaFax-Italic     /LucidaHandwriting-Italic     /LucidaSans     /LucidaSans-Demi     /LucidaSans-DemiItalic     /LucidaSans-Italic     /LucidaSans-Typewriter     /LucidaSans-TypewriterBold     /LucidaSans-TypewriterBoldOblique     /LucidaSans-TypewriterOblique     /LucidaSansUnicode     /Magneto-Bold     /MaiandraGD-Regular     /MalgunGothic     /MalgunGothicBold     /MalgunGothicRegular     /Mangal     /Mangal-Bold     /Marlett     /MaturaMTScriptCapitals     /Meiryo     /Meiryo-Bold     /Meiryo-BoldItalic     /Meiryo-Italic     /MeiryoUI     /MeiryoUI-Bold     /MeiryoUI-BoldItalic     /MeiryoUI-Italic     /MicrosoftHimalaya     /MicrosoftJhengHeiBold     /MicrosoftJhengHeiRegular     /MicrosoftNewTaiLue     /MicrosoftNewTaiLue-Bold     /MicrosoftPhagsPa     /MicrosoftPhagsPa-Bold     /MicrosoftSansSerif     /MicrosoftTaiLe     /MicrosoftTaiLe-Bold     /MicrosoftUighur     /MicrosoftYaHei     /MicrosoftYaHei-Bold     /Microsoft-Yi-Baiti     /MingLiU     /MingLiU-ExtB     /Ming-Lt-HKSCS-ExtB     /Ming-Lt-HKSCS-UNI-H     /Miriam     /MiriamFixed     /Mistral     /Modern-Regular     /MongolianBaiti     /MonotypeCorsiva     /MoolBoran     /MS-Gothic     /MS-Mincho     /MSOutlook     /MS-PGothic     /MS-PMincho     /MSReferenceSansSerif     /MSReferenceSpecialty     /MS-UIGothic     /MVBoli     /Narkisim     /NiagaraEngraved-Reg     /NiagaraSolid-Reg     /NSimSun     /Nyala-Regular     /OCRAExtended     /OldEnglishTextMT     /Onyx     /PalaceScriptMT     /PalatinoLinotype-Bold     /PalatinoLinotype-BoldItalic     /PalatinoLinotype-Italic     /PalatinoLinotype-Roman     /Papyrus-Regular     /Parchment-Regular     /Perpetua     /Perpetua-Bold     /Perpetua-BoldItalic     /Perpetua-Italic     /PerpetuaTitlingMT-Bold     /PerpetuaTitlingMT-Light     /PlantagenetCherokee     /Playbill     /PMingLiU     /PMingLiU-ExtB     /PoorRichard-Regular     /Pristina-Regular     /Raavi     /RageItalic     /Ravie     /Rockwell     /Rockwell-Bold     /Rockwell-BoldItalic     /Rockwell-Condensed     /Rockwell-CondensedBold     /Rockwell-ExtraBold     /Rockwell-Italic     /Rod     /SakkalMajalla     /SakkalMajallaBold     /ScriptMTBold     /SegoePrint     /SegoePrint-Bold     /SegoeScript     /SegoeScript-Bold     /SegoeUI     /SegoeUI-Bold     /SegoeUI-BoldItalic     /SegoeUI-Italic     /SegoeUI-Light     /SegoeUI-SemiBold     /SegoeUISymbol     /ShonarBangla     /ShonarBangla-Bold     /ShowcardGothic-Reg     /Shruti     /Shruti-Bold     /SimHei     /SimplifiedArabic     /SimplifiedArabic-Bold     /SimplifiedArabicFixed     /SimSun     /SimSun-ExtB     /SnapITC-Regular     /Stencil     /Sylfaen     /SymbolMT     /Tahoma     /Tahoma-Bold     /TempusSansITC     /TimesNewRomanMT-ExtraBold     /TimesNewRomanPS-BoldItalicMT     /TimesNewRomanPS-BoldMT     /TimesNewRomanPS-ItalicMT     /TimesNewRomanPSMT     /TraditionalArabic     /TraditionalArabic-Bold     /Trebuchet-BoldItalic     /TrebuchetMS     /TrebuchetMS-Bold     /TrebuchetMS-Italic     /Tunga     /Tunga-Bold     /TwCenMT-Bold     /TwCenMT-BoldItalic     /TwCenMT-Condensed     /TwCenMT-CondensedBold     /TwCenMT-CondensedExtraBold     /TwCenMT-Italic     /TwCenMT-Regular     /Utsaah     /Utsaah-Bold     /Utsaah-BoldItalic     /Utsaah-Italic     /Vani     /Vani-Bold     /Verdana     /Verdana-Bold     /Verdana-BoldItalic     /Verdana-Italic     /Vijaya     /Vijaya-Bold     /VinerHandITC     /Vivaldii     /VladimirScript     /Vrinda     /Vrinda-Bold     /Webdings     /Wingdings2     /Wingdings3     /Wingdings-Regular     /ZWAdobeF   ]   /NeverEmbed [ true   ]   /AntiAliasColorImages false   /CropColorImages true   /ColorImageMinResolution 150   /ColorImageMinResolutionPolicy /OK   /DownsampleColorImages false   /ColorImageDownsampleType /Bicubic   /ColorImageResolution 150   /ColorImageDepth -1   /ColorImageMinDownsampleDepth 1   /ColorImageDownsampleThreshold 1.50000   /EncodeColorImages false   /ColorImageFilter /DCTEncode   /AutoFilterColorImages true   /ColorImageAutoFilterStrategy /JPEG   /ColorACSImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /ColorImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /JPEG2000ColorACSImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /JPEG2000ColorImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /AntiAliasGrayImages false   /CropGrayImages true   /GrayImageMinResolution 150   /GrayImageMinResolutionPolicy /OK   /DownsampleGrayImages false   /GrayImageDownsampleType /Bicubic   /GrayImageResolution 150   /GrayImageDepth -1   /GrayImageMinDownsampleDepth 2   /GrayImageDownsampleThreshold 1.50000   /EncodeGrayImages true   /GrayImageFilter /DCTEncode   /AutoFilterGrayImages true   /GrayImageAutoFilterStrategy /JPEG   /GrayACSImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /GrayImageDict <<     /QFactor 0.76     /HSamples [2 1 1 2] /VSamples [2 1 1 2]   >>   /JPEG2000GrayACSImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /JPEG2000GrayImageDict <<     /TileWidth 256     /TileHeight 256     /Quality 15   >>   /AntiAliasMonoImages false   /CropMonoImages true   /MonoImageMinResolution 1200   /MonoImageMinResolutionPolicy /OK   /DownsampleMonoImages false   /MonoImageDownsampleType /Bicubic   /MonoImageResolution 1200   /MonoImageDepth -1   /MonoImageDownsampleThreshold 1.50000   /EncodeMonoImages true   /MonoImageFilter /CCITTFaxEncode   /MonoImageDict <<     /K -1   >>   /AllowPSXObjects true   /CheckCompliance [     /None   ]   /PDFX1aCheck false   /PDFX3Check false   /PDFXCompliantPDFOnly false   /PDFXNoTrimBoxError true   /PDFXTrimBoxToMediaBoxOffset [     0.00000     0.00000     0.00000     0.00000   ]   /PDFXSetBleedBoxToMediaBox true   /PDFXBleedBoxToTrimBoxOffset [     0.00000     0.00000     0.00000     0.00000   ]   /PDFXOutputIntentProfile (None)   /PDFXOutputConditionIdentifier ()   /PDFXOutputCondition ()   /PDFXRegistryName ()   /PDFXTrapped /False    /CreateJDFFile false   /Description <<     /ARA <FEFF0633062A062E062F0645002006470630064700200627064406250639062F0627062F0627062A002006440625064606340627062100200648062B062706260642002000410064006F006200650020005000440046002006450646062706330628062900200644063906310636002006480637062806270639062900200648062B06270626064200200627064406230639064506270644002E00200020064A06450643064600200641062A062D00200648062B0627062606420020005000440046002006270644062A064A0020062A0645002006250646063406270626064706270020062806270633062A062E062F062706450020004100630072006F00620061007400200648002000410064006F00620065002000520065006100640065007200200036002E00300020064806450627002006280639062F0647002E>     /BGR <FEFF04180437043F043E043B043704320430043904420435002004420435043704380020043D0430044104420440043E0439043A0438002C00200437043000200434043000200441044A0437043404300432043004420435002000410064006F00620065002000500044004600200434043E043A0443043C0435043D04420438002C0020043F043E04340445043E0434044F044904380020043704300020043D04300434043504360434043D043E00200440043004370433043B0435043604340430043D0435002004380020043F04350447043004420430043D04350020043D04300020043104380437043D0435044100200434043E043A0443043C0435043D04420438002E00200421044A04370434043004340435043D043804420435002000500044004600200434043E043A0443043C0435043D044204380020043C043E0433043004420020043404300020044104350020043E0442043204300440044F0442002004410020004100630072006F00620061007400200438002000410064006F00620065002000520065006100640065007200200036002E0030002004380020043F043E002D043D043E043204380020043204350440044104380438002E>     /CHS <FEFF4f7f75288fd94e9b8bbe5b9a521b5efa7684002000410064006f006200650020005000440046002065876863900275284e8e55464e1a65876863768467e5770b548c62535370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200036002e003000204ee553ca66f49ad87248672c676562535f00521b5efa768400200050004400460020658768633002>     /CHT <FEFF4f7f752890194e9b8a2d7f6e5efa7acb7684002000410064006f006200650020005000440046002065874ef69069752865bc666e901a554652d965874ef6768467e5770b548c52175370300260a853ef4ee54f7f75280020004100630072006f0062006100740020548c002000410064006f00620065002000520065006100640065007200200036002e003000204ee553ca66f49ad87248672c4f86958b555f5df25efa7acb76840020005000440046002065874ef63002>     /CZE <FEFF0054006f0074006f0020006e006100730074006100760065006e00ed00200070006f0075017e0069006a007400650020006b0020007600790074007600e101590065006e00ed00200064006f006b0075006d0065006e0074016f002000410064006f006200650020005000440046002000760068006f0064006e00fd006300680020006b0065002000730070006f006c00650068006c0069007600e9006d0075002000700072006f0068006c00ed017e0065006e00ed002000610020007400690073006b00750020006f006200630068006f0064006e00ed0063006800200064006f006b0075006d0065006e0074016f002e002000200056007900740076006f01590065006e00e900200064006f006b0075006d0065006e0074007900200050004400460020006c007a00650020006f007400650076015900ed007400200076002000610070006c0069006b0061006300ed006300680020004100630072006f006200610074002000610020004100630072006f006200610074002000520065006100640065007200200036002e0030002000610020006e006f0076011b006a016100ed00630068002e>     /DAN <FEFF004200720075006700200069006e0064007300740069006c006c0069006e006700650072006e0065002000740069006c0020006100740020006f007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400650072002c0020006400650072002000650067006e006500720020007300690067002000740069006c00200064006500740061006c006a006500720065007400200073006b00e60072006d007600690073006e0069006e00670020006f00670020007500640073006b007200690076006e0069006e006700200061006600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020004400650020006f007000720065007400740065006400650020005000440046002d0064006f006b0075006d0065006e0074006500720020006b0061006e002000e50062006e00650073002000690020004100630072006f00620061007400200065006c006c006500720020004100630072006f006200610074002000520065006100640065007200200036002e00300020006f00670020006e0079006500720065002e>     /DEU <FEFF00560065007200770065006e00640065006e0020005300690065002000640069006500730065002000450069006e007300740065006c006c0075006e00670065006e0020007a0075006d002000450072007300740065006c006c0065006e00200076006f006e002000410064006f006200650020005000440046002d0044006f006b0075006d0065006e00740065006e002c00200075006d002000650069006e00650020007a0075007600650072006c00e40073007300690067006500200041006e007a006500690067006500200075006e00640020004100750073006700610062006500200076006f006e00200047006500730063006800e40066007400730064006f006b0075006d0065006e00740065006e0020007a0075002000650072007a00690065006c0065006e002e00200044006900650020005000440046002d0044006f006b0075006d0065006e007400650020006b00f6006e006e0065006e0020006d006900740020004100630072006f00620061007400200075006e0064002000520065006100640065007200200036002e003000200075006e00640020006800f600680065007200200067006500f600660066006e00650074002000770065007200640065006e002e>     /ESP <FEFF005500740069006c0069006300650020006500730074006100200063006f006e0066006900670075007200610063006900f3006e0020007000610072006100200063007200650061007200200064006f00630075006d0065006e0074006f0073002000640065002000410064006f00620065002000500044004600200061006400650063007500610064006f007300200070006100720061002000760069007300750061006c0069007a00610063006900f3006e0020006500200069006d0070007200650073006900f3006e00200064006500200063006f006e006600690061006e007a006100200064006500200064006f00630075006d0065006e0074006f007300200063006f006d00650072006300690061006c00650073002e002000530065002000700075006500640065006e00200061006200720069007200200064006f00630075006d0065006e0074006f00730020005000440046002000630072006500610064006f007300200063006f006e0020004100630072006f006200610074002c002000410064006f00620065002000520065006100640065007200200036002e003000200079002000760065007200730069006f006e0065007300200070006f00730074006500720069006f007200650073002e>     /ETI <FEFF004b00610073007500740061006700650020006e0065006900640020007300e400740074006500690064002c0020006500740020006c0075007500610020005000440046002d0064006f006b0075006d0065006e00740065002c0020006d0069007300200073006f00620069007600610064002000e4007200690064006f006b0075006d0065006e00740069006400650020007500730061006c006400750073007600e400e4007200730065006b0073002000760061006100740061006d006900730065006b00730020006a00610020007000720069006e00740069006d006900730065006b0073002e00200020004c006f006f0064007500640020005000440046002d0064006f006b0075006d0065006e0074006500200073006100610062002000610076006100640061002000760061006900640020004100630072006f0062006100740020006a0061002000410064006f00620065002000520065006100640065007200200036002e00300020006a00610020007500750065006d006100740065002000760065007200730069006f006f006e00690064006500670061002e>     /FRA <FEFF005500740069006c006900730065007a00200063006500730020006f007000740069006f006e00730020006100660069006e00200064006500200063007200e900650072002000640065007300200064006f00630075006d0065006e00740073002000410064006f006200650020005000440046002000700072006f00660065007300730069006f006e006e0065006c007300200066006900610062006c0065007300200070006f007500720020006c0061002000760069007300750061006c00690073006100740069006f006e0020006500740020006c00270069006d007000720065007300730069006f006e002e0020004c0065007300200064006f00630075006d0065006e00740073002000500044004600200063007200e900e90073002000700065007500760065006e0074002000ea0074007200650020006f007500760065007200740073002000640061006e00730020004100630072006f006200610074002c002000610069006e00730069002000710075002700410064006f00620065002000520065006100640065007200200036002e0030002000650074002000760065007200730069006f006e007300200075006c007400e90072006900650075007200650073002e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a0065006b006b0065006c0020006100200062006500e1006c006c00ed007400e10073006f006b006b0061006c002000fc007a006c00650074006900200064006f006b0075006d0065006e00740075006d006f006b0020006d00650067006200ed007a00680061007400f30020006d00650067006a0065006c0065006e00ed007400e9007300e900720065002000e900730020006e0079006f006d00740061007400e1007300e10072006100200061006c006b0061006c006d00610073002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740075006d006f006b006100740020006b00e90073007a00ed0074006800650074002e002000200041007a002000ed006700790020006c00e90074007200650068006f007a006f007400740020005000440046002d0064006f006b0075006d0065006e00740075006d006f006b00200061007a0020004100630072006f006200610074002000e9007300200061007a002000410064006f00620065002000520065006100640065007200200036002c0030002d0073002000e900730020006b00e9007301510062006200690020007600650072007a006900f3006900760061006c0020006e00790069007400680061007400f3006b0020006d00650067002e>     /ITA (Utilizzare queste impostazioni per creare documenti Adobe PDF adatti per visualizzare e stampare documenti aziendali in modo affidabile. I documenti PDF creati possono essere aperti con Acrobat e Adobe Reader 6.0 e versioni successive.)     /JPN <FEFF30d330b830cd30b9658766f8306e8868793a304a3088307353705237306b90693057305f002000410064006f0062006500200050004400460020658766f8306e4f5c6210306b4f7f75283057307e305930023053306e8a2d5b9a30674f5c62103055308c305f0020005000440046002030d530a130a430eb306f3001004100630072006f0062006100740020304a30883073002000410064006f00620065002000520065006100640065007200200036002e003000204ee5964d3067958b304f30533068304c3067304d307e305930023053306e8a2d5b9a3067306f30d530a930f330c8306e57cb30818fbc307f3092884c3044307e30593002>     /KOR <FEFFc7740020c124c815c7440020c0acc6a9d558c5ec0020be44c988b2c8c2a40020bb38c11cb97c0020c548c815c801c73cb85c0020bcf4ace00020c778c1c4d558b2940020b3700020ac00c7a50020c801d569d55c002000410064006f0062006500200050004400460020bb38c11cb97c0020c791c131d569b2c8b2e4002e0020c774b807ac8c0020c791c131b41c00200050004400460020bb38c11cb2940020004100630072006f0062006100740020bc0f002000410064006f00620065002000520065006100640065007200200036002e00300020c774c0c1c5d0c11c0020c5f40020c2180020c788c2b5b2c8b2e4002e>     /LTH <FEFF004e006100750064006f006b0069007400650020016100690075006f007300200070006100720061006d006500740072007500730020006e006f0072011700640061006d0069002000730075006b0075007200740069002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c002000740069006e006b0061006d0075007300200076006500720073006c006f00200064006f006b0075006d0065006e00740061006d00730020006b006f006b0079006200690161006b006100690020007000650072017e0069016b007201170074006900200069007200200073007000610075007300640069006e00740069002e002000530075006b00750072007400750073002000500044004600200064006f006b0075006d0065006e007400750073002000670061006c0069006d006100200061007400690064006100720079007400690020007300750020004100630072006f006200610074002000690072002000410064006f00620065002000520065006100640065007200200036002e00300020006200650069002000760117006c00650073006e0117006d00690073002000760065007200730069006a006f006d00690073002e>     /LVI <FEFF004c006900650074006f006a00690065007400200161006f00730020006900650073007400610074012b006a0075006d00750073002c0020006c0061006900200069007a0076006500690064006f00740075002000410064006f00620065002000500044004600200064006f006b0075006d0065006e007400750073002c0020006b006100730020007000690065006d01130072006f00740069002000640072006f01610061006900200075007a01460113006d0075006d006100200064006f006b0075006d0065006e0074007500200073006b00610074012b01610061006e0061006900200075006e0020006400720075006b010101610061006e00610069002e00200049007a0076006500690064006f0074006f0073002000500044004600200064006f006b0075006d0065006e00740075007300200076006100720020006100740076011300720074002c00200069007a006d0061006e0074006f006a006f0074002000700072006f006700720061006d006d00750020004100630072006f00620061007400200075006e002000410064006f00620065002000520065006100640065007200200036002e003000200076006100690020006a00610075006e0101006b0075002000760065007200730069006a0075002e>     /NLD (Gebruik deze instellingen om Adobe PDF-documenten te maken waarmee zakelijke documenten betrouwbaar kunnen worden weergegeven en afgedrukt. De gemaakte PDF-documenten kunnen worden geopend met Acrobat en Adobe Reader 6.0 en hoger.)     /NOR <FEFF004200720075006b00200064006900730073006500200069006e006e007300740069006c006c0069006e00670065006e0065002000740069006c002000e50020006f0070007000720065007400740065002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e00740065007200200073006f006d002000650072002000650067006e0065007400200066006f00720020007000e5006c006900740065006c006900670020007600690073006e0069006e00670020006f00670020007500740073006b007200690066007400200061007600200066006f0072007200650074006e0069006e006700730064006f006b0075006d0065006e007400650072002e0020005000440046002d0064006f006b0075006d0065006e00740065006e00650020006b0061006e002000e50070006e00650073002000690020004100630072006f00620061007400200065006c006c00650072002000410064006f00620065002000520065006100640065007200200036002e003000200065006c006c00650072002e>     /POL <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>     /PTB <FEFF005500740069006c0069007a006500200065007300730061007300200063006f006e00660069006700750072006100e700f50065007300200064006500200066006f0072006d00610020006100200063007200690061007200200064006f00630075006d0065006e0074006f0073002000410064006f00620065002000500044004600200061006400650071007500610064006f00730020007000610072006100200061002000760069007300750061006c0069007a006100e700e3006f002000650020006100200069006d0070007200650073007300e3006f00200063006f006e0066006900e1007600650069007300200064006500200064006f00630075006d0065006e0074006f007300200063006f006d0065007200630069006100690073002e0020004f007300200064006f00630075006d0065006e0074006f00730020005000440046002000630072006900610064006f007300200070006f00640065006d0020007300650072002000610062006500720074006f007300200063006f006d0020006f0020004100630072006f006200610074002000650020006f002000410064006f00620065002000520065006100640065007200200036002e0030002000650020007600650072007300f50065007300200070006f00730074006500720069006f007200650073002e>     /RUM <FEFF005500740069006C0069007A00610163006900200061006300650073007400650020007300650074010300720069002000700065006E007400720075002000610020006300720065006100200064006F00630075006D0065006E00740065002000410064006F006200650020005000440046002000610064006500630076006100740065002000700065006E007400720075002000760069007A00750061006C0069007A006100720065002000640065002000EE006E00630072006500640065007200650020015F0069002000700065006E00740072007500200069006D007000720069006D006100720065006100200064006F00630075006D0065006E00740065006C006F007200200064006500200061006600610063006500720069002E00200044006F00630075006D0065006E00740065006C00650020005000440046002000630072006500610074006500200070006F00740020006600690020006400650073006300680069007300650020006300750020004100630072006F0062006100740020015F0069002000410064006F00620065002000520065006100640065007200200036002E003000200073006100750020007600650072007300690075006E006900200075006C0074006500720069006F006100720065002E>     /RUS <FEFF04180441043F043E043B044C043704430439044204350020044D044204380020043F043004400430043C043504420440044B0020043F0440043800200441043E043704340430043D0438043800200434043E043A0443043C0435043D0442043E0432002000410064006F006200650020005000440046002C0020043F043E04340445043E0434044F04490438044500200434043B044F0020043D0430043404350436043D043E0433043E0020043F0440043E0441043C043E044204400430002004380020043F043504470430044204380020043104380437043D04350441002D0434043E043A0443043C0435043D0442043E0432002E00200421043E043704340430043D043D044B043500200434043E043A0443043C0435043D0442044B00200050004400460020043C043E0436043D043E0020043E0442043A0440044B0442044C002C002004380441043F043E043B044C04370443044F0020004100630072006F00620061007400200438002000410064006F00620065002000520065006100640065007200200036002E00300020043B04380431043E00200438044500200431043E043B043504350020043F043E04370434043D043804350020043204350440044104380438002E>     /SKY <FEFF0054006900650074006f0020006e006100730074006100760065006e0069006100200073006c00fa017e006900610020006e00610020007600790074007600e100720061006e0069006500200064006f006b0075006d0065006e0074006f007600200076006f00200066006f0072006d00e100740065002000410064006f006200650020005000440046002c0020006b0074006f007200e90020007300fa002000760068006f0064006e00e90020006e0061002000730070006f013e00610068006c0069007600e90020007a006f006200720061007a006f00760061006e006900650020006100200074006c0061010d0020006f006200630068006f0064006e00fd0063006800200064006f006b0075006d0065006e0074006f0076002e002000200056007900740076006f00720065006e00e900200064006f006b0075006d0065006e0074007900200076006f00200066006f0072006d00e10074006500200050004400460020006a00650020006d006f017e006e00e90020006f00740076006f00720069016500200076002000700072006f006700720061006d00650020004100630072006f0062006100740020006100200076002000700072006f006700720061006d0065002000410064006f006200650020005200650061006400650072002c0020007600650072007a0069006900200036002e003000200061006c00650062006f0020006e006f007601610065006a002e>     /SLV <FEFF005400650020006E006100730074006100760069007400760065002000750070006F0072006100620069007400650020007A00610020007500730074007600610072006A0061006E006A006500200064006F006B0075006D0065006E0074006F0076002000410064006F006200650020005000440046002C0020007000720069006D00650072006E006900680020007A00610020007A0061006E00650073006C006A006900760020006F0067006C0065006400200069006E0020007400690073006B0061006E006A006500200070006F0073006C006F0076006E0069006800200064006F006B0075006D0065006E0074006F0076002E0020005500730074007600610072006A0065006E006500200064006F006B0075006D0065006E0074006500200050004400460020006A00650020006D006F0067006F010D00650020006F00640070007200650074006900200073002000700072006F006700720061006D006F006D00610020004100630072006F00620061007400200069006E002000410064006F00620065002000520065006100640065007200200036002E003000200074006500720020006E006F00760065006A01610069006D0069002E>     /SUO <FEFF004b00e40079007400e40020006e00e40069007400e4002000610073006500740075006b007300690061002c0020006b0075006e0020006c0075006f0074002000410064006f0062006500200050004400460020002d0064006f006b0075006d0065006e007400740065006a0061002c0020006a006f0074006b006100200073006f0070006900760061007400200079007200690074007900730061007300690061006b00690072006a006f006a0065006e0020006c0075006f00740065007400740061007600610061006e0020006e00e400790074007400e4006d0069007300650065006e0020006a0061002000740075006c006f007300740061006d0069007300650065006e002e0020004c0075006f0064007500740020005000440046002d0064006f006b0075006d0065006e00740069007400200076006f0069006400610061006e0020006100760061007400610020004100630072006f0062006100740069006c006c00610020006a0061002000410064006f00620065002000520065006100640065007200200036002e0030003a006c006c00610020006a006100200075007500640065006d006d0069006c006c0061002e>     /SVE <FEFF0041006e007600e4006e00640020006400650020006800e4007200200069006e0073007400e4006c006c006e0069006e006700610072006e00610020006f006d002000640075002000760069006c006c00200073006b006100700061002000410064006f006200650020005000440046002d0064006f006b0075006d0065006e007400200073006f006d00200070006100730073006100720020006600f60072002000740069006c006c006600f60072006c00690074006c006900670020007600690073006e0069006e00670020006f006300680020007500740073006b007200690066007400650072002000610076002000610066006600e4007200730064006f006b0075006d0065006e0074002e002000200053006b006100700061006400650020005000440046002d0064006f006b0075006d0065006e00740020006b0061006e002000f600700070006e00610073002000690020004100630072006f0062006100740020006f00630068002000410064006f00620065002000520065006100640065007200200036002e00300020006f00630068002000730065006e006100720065002e>     /TUR <FEFF0130015f006c006500200069006c00670069006c0069002000620065006c00670065006c006500720069006e0020006700fc00760065006e0069006c0069007200200062006900e70069006d006400650020006700f6007200fc006e007400fc006c0065006e006d006500730069006e0065002000760065002000790061007a0064013100720131006c006d006100730131006e006100200075007900670075006e002000410064006f006200650020005000440046002000620065006c00670065006c0065007200690020006f006c0075015f007400750072006d0061006b0020006900e70069006e00200062007500200061007900610072006c0061007201310020006b0075006c006c0061006e0131006e002e0020004f006c0075015f0074007500720075006c0061006e002000500044004600200064006f007300790061006c0061007201310020004100630072006f006200610074002000760065002000410064006f00620065002000520065006100640065007200200036002e003000200076006500200073006f006e00720061006b00690020007300fc007200fc006d006c0065007200690079006c00650020006100e70131006c006100620069006c00690072002e>     /UKR <FEFF04120438043A043E0440043804410442043E043204430439044204350020044604560020043F043004400430043C043504420440043800200434043B044F0020044104420432043E04400435043D043D044F00200434043E043A0443043C0435043D044204560432002000410064006F006200650020005000440046002C0020043F044004380437043D043004470435043D0438044500200434043B044F0020043D0430043404560439043D043E0433043E0020043F0435044004350433043B044F04340443002004560020043404400443043A0443002004340456043B043E04320438044500200434043E043A0443043C0435043D044204560432002E0020042104420432043E04400435043D04560020005000440046002D0434043E043A0443043C0435043D044204380020043C043E0436043D04300020043204560434043A04400438043204300442043800200437043000200434043E043F043E043C043E0433043E044E0020043F0440043E043304400430043C04380020004100630072006F00620061007400200456002000410064006F00620065002000520065006100640065007200200036002E00300020044204300020043F04560437043D04560448043804450020043204350440044104560439002E>     /ENU (Use these settings to create Adobe PDF documents suitable for reliable viewing and printing of business documents.  Created PDF documents can be opened with Acrobat and Adobe Reader 6.0 and later.)   >> >> setdistillerparams <<   /HWResolution [600 600]   /PageSize [612.000 792.000] >> setpagedevice       "}
{"index":{"_id":"53"}}
{"datatype":"inproceedings","key":"Xiong:2015:ISL:2723576.2723636","author":"Xiong, Xiaolu and Wang, Yan and Beck, Joseph Barbosa","title":"Improving Students' Long-term Retention Performance: A Study on Personalized Retention Schedules","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"325--329","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723636","doi":"10.1145/2723576.2723636","acmid":"2723636","publisher":"ACM","address":"New York, NY, USA","keywords":"intelligent tutoring system, knowledge retention, personalization, retrieval practice, spacing effect","abstract":"Traditional practices of spacing and expanding retrieval practices have typically fixed their spacing intervals to one or few predefined schedules [5, 7]. Few have explored the advantages of using personalized expanding intervals and scheduling systems to adapt to the knowledge levels and learning patterns of individual students. In this work, we are concerned with estimating the effects of personalized expanding intervals on improving students' long-term mastery level of skills. We developed a Personalized Adaptive Scheduling System (PASS) in ASSISTments' retention and relearning workflow. After implementing the PASS, we conducted a study to investigate the impact of personalized scheduling on long-term retention by comparing results from 97 classes in the summer of 2013 and 2014. We observed that students in PASS outperformed students in traditional scheduling systems on long-term retention performance (p ","pdf":"Improving Students Long-Term Retention Performance: A   Study on Personalized Retention Schedules  Xiaolu Xiong   Worcester Polytechnic Institute  100 Institute Rd   Worcester, MA 01609  (508) 831-5000   xxiong@wpi.edu   Yan Wang  Worcester Polytechnic Institute   100 Institute Rd  Worcester, MA 01609   (508) 831-5000   ywang14@wpi.edu   Joseph Barbosa Beck  Worcester Polytechnic Institute   100 Institute Rd  Worcester, MA 01609   (508) 831-5000   josephbeck@wpi.edu         ABSTRACT  Traditional practices of spacing and expanding retrieval practices  have typically fixed their spacing intervals to one or few   predefined schedules [5, 7]. Few have explored the advantages of   using personalized expanding intervals and scheduling systems to  adapt to the knowledge levels and learning patterns of individual   students. In this work, we are concerned with estimating the   effects of personalized expanding intervals on improving  students long-term mastery level of skills. We developed a   Personalized Adaptive Scheduling System (PASS) in   ASSISTments retention and relearning workflow. After  implementing the PASS, we conducted a study to investigate the   impact of personalized scheduling on long-term retention by  comparing results from 97 classes in the summer of 2013 and   2014. We observed that students in PASS outperformed students   in traditional scheduling systems on long-term retention  performance (p = 0.0002), and that in particular, students with   medium level of knowledge demonstrated reliable improvement   (p = 0.0209) with an effect size of 0.27. In addition, the data we  gathered from this study also helped to expose a few issues we   have with the new system. These results suggest personalized   knowledge retrieval schedules are more effective than fixed  schedules and we should continue our future work on examining   approaches to optimize PASS.     Categories and Subject Descriptors  H.4 Information Systems Applications; K.3.1 Computer Uses in   Education; J.4 Social and Behavioral Sciences   General Terms  Algorithms, Measurement, Performance, Design, Theory.   Keywords  Knowledge retention, retrieval practice, spacing effect, intelligent   tutoring system, personalization   1. INTRODUCTION   1.1 Automatic Reassessment and Relearning   System  Based on a robust memory phenomenon known as the spacing   effect [4], expanding retrieval practice is often regarded as a  superior technique for promoting long-term retention relative to   equally spaced retrieval practice [3, 8]. Expanding retrieval   practice works by, after the student learns a skill, having the  student perform the skill at gradually increasing spacing intervals   between successful retrieval attempts. Research has shown that   spacing practice has a cumulative effect so that each time an item  is practiced it receives an increment of strength [10]. This effect is   specifically crucial to subjects such as mathematics: we are more   concerned with students capability to recall the knowledge that  they acquired over a long period of time. What is more, the ability   to retain a skill long-term is one of the three indicators of robust   learning [2].      Figure 1. The enhanced ITS mastery learning cycle   Inspired by the importance of long-term retention and the design   of the enhanced ITS mastery cycle in Figure 1 proposed by Wang  and Beck [11], we developed and deployed a system called the   Automatic Reassessment and Relearning System (ARRS) [13] to   make decisions about when to review skills that students have  mastered in ASSISTments, a non-profit, web-based tutoring      Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights   for components of this work owned by others than ACM must be   honored. Abstracting with credit is permitted. To copy otherwise, or   republish, to post on servers or to redistribute to lists, requires prior   specific permission and/or a fee. Request permissions from   Permissions@acm.org.   LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA   Copyright is held by the owner/author(s). Publication rights licensed to   ACM.   ACM 978-1-4503-3417-4/15/03$15.00   http://dx.doi.org/10.1145/2723576.2723636   325    system. ARRS is an implementation of expanding retrieval in the   ITS environment. Unlike most ITS systems in which the tutoring   stops if the student masters a given skill, ARRS assumes that if a  student masters a skill with three correct responses in a row, such   mastery is not necessarily an indication of long-term retention.   Therefore, ARRS will present the student with retention tests on  the same skill at expanding intervals spread across a schedule of   at least 3 months. The default setting of the ARRS scheduling   system uses a spacing interval of 7-14-28-56, and this indicates  that each skill requires 4 level tests: the first level of retention   tests takes place 7 days after the initial mastery; the second level   of retention tests 14 days after successfully passing the first  retention test, and so on. If a student answers incorrectly in one of   these retention tests, ASSISTments will give him an opportunity  to relearn this skill before redoing the same level of test.   Table 1. Retention performance by mastery speed and   retention interval from pilot study   Retention   test delay   # tests % correctness   Mastery speed 3  4   1 day 1186 84.4%   4 days 1169 82.2%   7 days 1171 81.7%   14 days 1233 81.2%   Mastery speed 5  7   1 day 467 77.9%   4 days 432 76.2%   7 days 362 77.1%   14 days 420 73.1%   Mastery speed > 7   1 day 280 67.5%   4 days 320 62.8%   7 days 267 59.6%   14 days 243 54.8%      In our previous studies [13, 14] of modeling student retention   performance, we found that the number of problems required  achieving mastery, which we referred to as the mastery speed, is   an extremely important feature for predicting students retention   performance. We observed that, in general, the slower the mastery  speed, the lower the probability that the student can answer the   problems in the retention test correctly. Students who mastered a  skill in 3 or 4 problems had approximately an 82% chance of   responding correctly on the first retention test, while students who   took over 7 attempts to master a skill only had a 62% chance [13].  Based on these results, we conclude that students with different   mastery speeds have different retention patterns, so we began   searching for the optimal retrieval schedules for different levels of  student knowledge.    In order to find the optimal retention schedule for students and the  best way to boost their performance in long-term mathematics   learning, we conducted a pilot study by setting up four different   interval schedules (1 day, 4 days, 7 days, and 14 days) and   examined the impact on retention performance by comparing   results across different groups of students. The results are shown   in Table 1 and [12]. We saw a consistent decrease in retention  performance with the longer retention intervals across in all   students, no matter if they fell into the high mastery level, medium   mastery level or low mastery level category. The results from  Table 1 also demonstrated a main effect of mastery speed on   retention performance: students with slower mastery speed had   lower performance than students with a faster mastery speed; this  statement is true even when we compared a 1-day performance of   students with a mastery speed of over 7 (67.5% correct) speed   versus a 14-day performance of students with a mastery speed of 3  or 4 (81.2% correct). A sizeable and interesting effect is that   students with slower mastery speeds had bigger decreases in  retention performance as retention intervals lengthened. For   example, a high mastery level student had a decrease of 3.2%   between 1 day tests and 14 days tests but the retention  performance of low mastery level students dropped 12.7%. These   results suggest retention intervals probably should vary, rather   than be fixed, based on the students knowledge of the skill.   1.2 Personalized Adaptive Scheduling System  Although ARRS helps students review knowledge after a time   period, it neither knows a students knowledge level, nor does it   have the mechanism to change the retention schedule based on a  particular students performance. Here we formed a hypothesis   that we can improve students long-term retention levels by   adaptively assigning students with gradually expanding and  spacing intervals over time and we proposed to design and   develop such a system, called Personalized Adaptive Scheduling   System (PASS), as shown in Figure 2. PASS enables ARRS to  schedule retention tests for students based on their knowledge   levels. In the spring of 2014, we enhanced the traditional ARRS   with the PASS and deployed it in ASSISTments.      Figure 2. Design of Personalized Adaptive Scheduling System   (PASS)   The current workflow of PASS aims to improve students long- term retention performance by setting up personalized retention   test schedules based on their knowledge levels. Here we rely on   the mastery speed of a skill as an estimate of the students  knowledge and, consequently, predictor of retention performance.   We retained the ARRS design of 4 expanding intervals of   retention tests for each skill; however, PASS alters how the first  interval behaves.  When a student finishes initially learning a   326    skill, we use his mastery speed to decide when to assign his first   level 1 retention test. The mapping between mastery speed and   retention delay intervals of the level 1 test is shown in Table 2.  When a student passes the first test, PASS will schedule another   test with a 1-day longer delay.  Once the student passes the 7-day   test, he is promoted to level 2 with a delay of 14 days.  From that  point on the intervals are the same as in the ARRS system.  Note   that mastery speed can be extracted from both students initial   learning and relearning processes. Therefore, when a student fails  a retention test, a relearning assignment will be assigned to the   student immediately.  How quickly the student relearns this   assignment will be used to set the interval for his next test.  The  mechanism of level 2 to level 4 tests is simpler. When a student   fails a retention test, the retention delay will be reduced to the  previous level (e.g., from 56 days to 28 days).  It will be increased   to the next level if the student passes the delayed retention test.   Table 2. Mapping between mastery speed and level 1 retention   delays    Mastery Speed Retention Delay   3 7   4 6   5 5   6 4   7 3   > 7 1      Here is an example of a student working with PASS in  ASSISTments. Lets assume he needed 4 attempts to achieve three   correct responses in a row in an initial learning assignment, so his   mastery speed on this skill was 4. PASS then scheduled the first  level 1 retention test for him to complete 6 days after the initial   mastery. 6 days later, the student passed the retention test and   PASS scheduled a 7-day retention test. Then a week later, the  student passed the 7-day retention test and moved to the level 2   retention tests.    2. A STUDY ON IMPACT OF   PERSONALIZED EXPANDING   RETENTION INTERVALS   After the deployment of PASS in ASSISTments, several key   issues were revealed that needed to be explored in order to realize   the potential benefits of personalized expanding retention  intervals and scheduling for students. We first conducted a study   in ASSISTments to compare the new PASS with the traditional   ARRS without PASS. In addition, this study explored the  influence of personalized scheduling on students long-term   performance, student learning patterns and how they interact with  the ASSISTments.    There were several objectives for this study. A central goal was to  investigate potential long-term retention performance   improvement to the benefit of personalized spacing schedules. We   enabled PASS for all classes that were using ARRS on May 15,  2014; we expected students in these classes might be assigned   homework during the next few months and thereby become the   participants in the study. We ended this study on September 1,   2014 and found that 2,052 students from 40 classes were using   PASS in the summer of 2014. Teachers of these classes assigned   93 different homework assignments to their students. Since  traditional ARRS had been deployed in ASSISTments for over   two years and a lot of data have been accumulated in the system,   we extracted previous summers ARRS-enabled classes that used  the same assignments as the historical control group. 2,541   students from 57 classes in the summer of 2013 were qualified to   act as historical control group.   During these two summer periods, students consistently received   mathematics problem sets as homework assignments from their  teachers. Once they answered three consecutive questions   correctly in a problem set, students in the PASS condition would   be given retention tests based on their mastery speed. If a student  answered a retention test correctly, he was then given another   retention test with a longer delay until he passed the level 1 test  with a 7-day delay. On the other hand, students in traditional   ARRS condition got 7-day delay retention tests after the mastery   and went on with the 14-day tests if they answered the 7-day tests  correctly. In this study, we defined how students performed on the   14-day retention tests (14 days after passing the level 1 test and at   least 21 days after the initial mastery learning) as the outcome  long-term retention tests. It is important to note that students   usually receive several homework assignments and they may   perform differently in these assignments, which means a student  would have multiple tests that should be accounted for in the   long-term performance. However, it is also possible that students   do not complete assignments. Specifically, if a student has not  finished the outcome retention test of a homework assignment by   the end of this study, we cannot take this record into account.    3. RESULTS AND ANALYSIS  Retention test completion rate was calculated based on the  number of homework assignments that had outcome tests   answered divided by the total number of homework assignments.   Days spent is the time interval between the start time of level 1  retention tests and the start time of outcome tests in days. Test   count accounts for how many level 1 retention tests a student has   to answer before this student can proceed to outcome tests.  Students long-term performance was calculated as the ratio of   number of questions answered correctly in outcome tests to   number of all questions answered in outcome tests.   3.1 Retention Test Completion Rate, Day   Spent and Test Count  At the end of this study, the first result we noticed was that a lot   of homework assignments in both groups did not have the records  for associated outcome tests.  In other words, a lot of students did   not reach the 14-day retention tests. In the traditional ARRS   condition, a total of 8404 homework assignments had been  assigned to students but only 1,558 (18.5%) of these assignments   had 14-days retention tests answered. When looking at the PASS   condition, the retention test completion rate was even lower, only  1,029 (13.6%) of total 7,589 homework assignments had outcome   tests answered.  In one sense these low completion rates could   result from the fact these homework and retention tests were  assigned to students during the summer vacation so that perhaps   many students did not treat these assignments seriously.  The data   also indicated the difference in the completion rates of the two  conditions were statistically significant (p < 0.0001). We   hypothesized that this was due to the fact that students in the   327    PASS condition took more tests in order to pass the 7-day delay   tests. Remember, some medium- and low-knowledge students had   to pass a number of shorter-delay tests to even reach the 7-day and  then 14-day retention tests. To address this hypothesis, we   investigated how many days were needed to reach the 14-day test   from the beginning of level 1 retention tests. The data was  grouped by the three identified mastery speed bins to represent   high-, medium- and low-knowledge students on their homework   assignments   Table 3. Average day spent of each knowledge level by   conditions   Initial mastery   performance     ARRS PASS p-value   Mastery Speed  3 - 4   16.80 18.96 0.0002   Mastery Speed  5 - 7   17.67 33.24 0.0001   Mastery Speed  > 7   17.34 32.33 0.0001      Table 3 describes the differences in average days spent between  ARRS and PASS conditions. The minimum possible delay is 14   days, achievable for ARRS students who answer the 7-day test   correctly, and then take their ARRS test when it is immediately  available.  Students who failed the first ARRS test would have to   take one or more additional 7-day tests until they responded   correctly and could be promoted to the 14-day test.  For the PASS  condition, 14 days is a lower bound only for those students with   an initial mastery speed of 3, as slower mastery speeds would   require multiple first-level tests before being promoted to the 14- day interval.  As expected, students in the PASS condition spent   more time in the practices of level 1 retention tests; especially for   medium- and low-knowledge students who spent nearly two more  weeks in the process of passing the 7-day delay tests relative to   ARRS students. Table 4 demonstrates that students in the PASS   condition had more tests to answer by showing the average test  count of the two conditions therefore it took them more days to   reach 14-day tests.   Table 4. Average test count of each knowledge level by   conditions   Initial mastery   performance     ARRS PASS p-value   Mastery Speed   3 - 4   1.34 1.21 0.0003   Mastery Speed  5 - 7   1.44 3.25 0.0001   Mastery Speed   > 7   1.59 3.69 0.0001      3.2 Long-Term Retention Performance  After it was observed that PASS made students take more practice  in the retention tests, we became more curious about the impact of   PASS on long-term retention performance. It is important to   emphasize that students were balanced with respect to proficiency   in the ARRS and PASS conditions given their close homework   performance level: 71.0% correct versus 71.2%. An initial   analysis on long-term retention performance across all students  showed the PASS condition (83.4%) outperformed the ARRS   condition (77.2%) with a reliable but small improvement (p =   0.0002, effect size = 0.15). When considering the performance  changes in different knowledge level of students, we again   grouped the data by three identified mastery speed bins; then we   examined students long-term retention performance with p- values and effect sizes.   Table 5. Long-term (14-day) retention performance   comparison and sample size (in parenthesis)   Initial mastery   performance     ARRS PASS p-value Effect   size   Mastery   Speed 3  4   81.79%   (978)   83.91%   (889)   0.2266 0.06   Mastery   Speed 5  7   73.08%   (327)   84.53%   (97)   0.0209 0.27   Mastery  Speed > 7   64.82%   (253)   70.59%   (51)   0.4301 0.12      The comparison of long-term retention performance shows that all  three groups of students in the PASS condition outperformed   those in the ARRS condition, although the improvements were   not all statistically significant; only students with medium- knowledge on skills performed reliably better with an effect size   of 0.27. For students with high knowledge on skills, the benefit of   using PASS was limited; this suggests that solely relying on 7-day  delay tests is sufficient for this population. A previous study [12]   also suggested that high-knowledge students have high resistance   against forgetting.  On the other hand, providing low-knowledge  students with more spaced retention tests and relearning   assignments did not stop the decay of retention even after these   students had approximately 3 additional relearning assignments  on the same skill, and we only noticed a small effect size (0.12)   improvement on the retention performance. Because PASS  employs a higher stand of mastery and retention, thus few low-  knowledge students reached outcome tests; we in fact noticed that   only 51 tests had been completed, so this also prevented us from  achieving a higher effect size in PASS condition. Another notable   result was when we compared Table 5 vertically: we could see   that PASS helped to close the performance gap between different  groups of students. In fact, in the PASS condition, the long-term   performance of medium-knowledge students even outperformed   the high-knowledge students. Of course, the small sample size  tells us we need more studies to validate this result.   4. CONTRIBUTIONS, FUTURE WORK  AND CONCLUSIONS  The paper makes three contributions. First, the work behind this   paper designed and deployed a personalized expanding interval  scheduling system that utilizes spacing effect in the field. Through   the participation of thousands of students, we carried out a study   to test the idea of assigning students with different delays of  retention tests to help them better retain skills. As the first study   on this system, the paper explores the path of improving ITS to   328    help students achieve robust learning via personalized expanding   retrieval practices. The second contribution of this paper is a   validation of the hypothesis that students long-term performance  can be improved by giving them tests that are well spaced out and   scheduled appropriately, before gradually expanding the spacing   between these tests. Most importantly, this study demonstrates the  importance of individualization in scheduling retention tests, as it   shows that students with medium knowledge can match up their   long-term performance with high-knowledge students by using  PASS. The third contribution of this paper is the confirmation of   concept of finding the optimal retention interval by using mastery   speed as a measurement of students knowledge level. By using  mastery speed to group students, we can distinguish different   learning and retention patterns among students with different  knowledge levels. In the process of work, we have noticed that   there has been other work on retention, such as the personalized   spaced review system [6]; however, this work focuses on fact  retrieval and is able to make far stronger assumptions of when   students are exposed to content.  Our work examines a procedural   skill, in a classroom context where we cannot be sure what  material teachers cover in class and we are not aware of all   homework assignments, thus we cannot be sure when students last   saw a skill.   This PASS and its implementation in ASSISTments have been   introduced to the field for just a few months, so we are still at the  initial phase of study. Our goal is to find the optimal spacing   schedules for students and the best way to boost their performance   in long-term mathematics learning. There are many further  problems that we are interested in: What should we do to help   low-knowledge students, considering the improvement we saw in   the study was so small, particularly given the increased amount of  practice they received From the data we collected, it was obvious   that there were some areas that required improvement. For   example, we simulated a scenario to improve the retention  performance of low-knowledge students to match up to the   performance level of high-knowledge students (83.91%) and also   improve completion rates to the level of ARRS condition so we  could collect 228 data points. Given these optimistic assumptions,   there intervention would have an effect size of 0.45. Thus, in this  scenario, achieving a medium effect size (0.5) is not feasible.    What is the fundamental cause of mistakes Lack of effort or   interest on the students part, or a genuine lack of knowledge [1]  How can we increase the completion rate Most importantly, how   can we solve the optimization problem to balance time cost and   performance improvement [9] Is there a better way than just  assigning high-frequency retention tests to students   This paper presents the initial study of using the personalized  adaptive scheduling system to explore a solution to the optimal   spacing schedule problem. With the experiment data we collected,   we are excited to see that the PASS can help to improve long-term  retention performance across all three groups of students and   become the backbone of future development for promoting   student robust learning.     5. ACKNOWLEDGMENTS  We acknowledge funding for ASSISTments from NSF (#   1440753, 1316736, 1252297, 1109483, 1031398, and 0742503),   ONR's 'STEM Grand Challenges' and IES (# R305A120125 &  R305C100024) grants.    6. REFERENCES  [1] Anderson, J. R. (2014). Rules of the mind. Psychology Press.   [2] Baker, R. S., Gowda, S. M., Corbett, A. T., & Ocumpaugh, J.  (2012, January). Towards automatically detecting whether   student learning is shallow. In Intelligent Tutoring Systems   (pp. 444-453). Springer Berlin Heidelberg.   [3] Crowder, R. G. (1976). Principles of learning and memory.    [4] Hintzman, D. L. (1974). Theoretical implications of the   spacing effect.   [5] Logan, J. M., & Balota, D. A. (2008). Expanded vs. equal  interval spaced retrieval practice: Exploring different   schedules of spacing and retention interval in younger and   older adults. Aging, Neuropsychology, and Cognition, 15(3),  257-280.   [6] Lindsey, R. V., Shroyer, J. D., Pashler, H., & Mozer, M. C.   (2014). Improving students long-term knowledge retention  through personalized review. Psychological science, 25(3),   639-647.   [7] Karpicke, J. D., & Roediger III, H. L. (2007). Expanding  retrieval practice promotes short-term retention, but equally   spaced retrieval enhances long-term retention. Journal of   Experimental Psychology: Learning, Memory, and   Cognition, 33(4), 704.   [8] Melton, A. W. (1967). Repetition and retrieval from   memory. Science (New York, NY), 158(3800), 532-532.    [9] Pavlik, P. I., & Anderson, J. R. (2008). Using a model to  compute the optimal schedule of practice. Journal of   Experimental Psychology: Applied, 14(2), 101.   [10] Thalheimer, W. (2006). Spacing learning events over time:  What the research says.   [11] Wang, Y., & Beck, J. E. (2012). Using Student Modeling to   Estimate Student Knowledge Retention. International  Educational Data Mining Society.   [12] Xiong, X., & Beck, J. E. (2014). A Study of Exploring   Different Schedules of Spacing and Retrieval Interval on   Mathematics Skills in ITS Environment. In Intelligent  Tutoring Systems (pp. 504-509). Springer International   Publishing.   [13] Xiong, X., Li, S., & Beck, J. E. (2013). Will You Get It  Right Next Week: Predict Delayed Performance in Enhanced   ITS Mastery Cycle. In FLAIRS Conference.   [14] Xiong, X., Adjei, S. A., & Heffernan, N. T. (2014)  Improving Retention Performance Prediction with   Prerequisite Skill Features. The 7th International Conference   on Educational Data Mining     329      "}
{"index":{"_id":"54"}}
{"datatype":"inproceedings","key":"Sekiya:2015:CAC:2723576.2723594","author":"Sekiya, Takayuki and Matsuda, Yoshitatsu and Yamaguchi, Kazunori","title":"Curriculum Analysis of CS Departments Based on CS2013 by Simplified, Supervised LDA","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"330--339","numpages":"10","url":"http://doi.acm.org/10.1145/2723576.2723594","doi":"10.1145/2723576.2723594","acmid":"2723594","publisher":"ACM","address":"New York, NY, USA","keywords":"curriculum, curriculum analysis, supervised LDA, syllabus","abstract":"The curricula higher educational institutions offer is a key asset in enabling them to systematically educate their students. We have been developing a curriculum analysis method that can help to find out differences among curricula. On the basis of Computing Science Curricula CS2013, a report released by the ACM and IEEE Computer Society, we applied our method to analyzing 10 computer science (CS) related curricula offered by CS departments of universities in the United States. Using the method enables us to compare courses across universities. Through an analysis of course syllabi distribution, we found that CS2013 uniformly covered a wide area of computer science. Some universities emphasized human factors, while others attached greater importance to theoretical ones. We also found that some CS departments offered not only a CS curriculum but also an electrical engineering one, and those departments showed a tendency to have more Architecture and Organization (AR) related curricula. Furthermore, we found that even though Information Assurance and Security (IAS) has not yet become a very popular field, some universities are already offering IAS related courses.","pdf":"Curriculum Analysis of CS Departments Based on CS2013 by Simplified, Supervised LDA  Takayuki Sekiya Information Technology  Center, the University of Tokyo 3-8-1 Komaba, Meguro,  Tokyo, Japan sekiya@ecc.u-tokyo.ac.jp  Yoshitatsu Matsuda Graduate School of Arts and  Sciences, the University of Tokyo 3-8-1 Komaba, Meguro,  Tokyo, Japan matsuda@graco.c.u-  tokyo.ac.jp  Kazunori Yamaguchi Graduate School of Arts and  Sciences, the University of Tokyo 3-8-1 Komaba, Meguro,  Tokyo, Japan yamaguch@graco.c.u-  tokyo.ac.jp  ABSTRACT The curricula higher educational institutions offer is a key as- set in enabling them to systematically educate their students. We have been developing a curriculum analysis method that can help to find out differences among curricula. On the basis of Computing Science Curricula CS2013, a report re- leased by the ACM and IEEE Computer Society, we applied our method to analyzing 10 computer science (CS) related curricula offered by CS departments of universities in the United States. Using the method enables us to compare courses across universities. Through an analysis of course syllabi distribution, we found that CS2013 uniformly covered a wide area of computer science. Some universities empha- sized human factors, while others attached greater impor- tance to theoretical ones. We also found that some CS depart- ments offered not only a CS curriculum but also an electrical engineering one, and those departments showed a tendency to have more Architecture and Organization (AR) related curricula. Furthermore, we found that even though Infor- mation Assurance and Security (IAS) has not yet become a very popular field, some universities are already offering IAS related courses.  Categories and Subject Descriptors K.3.2 [Computers and Education]: Computer and Informa- tion Science Educationcurriculum  General Terms Experimentation  Keywords Syllabus, Curriculum, Curriculum Analysis, Supervised LDA  1. INTRODUCTION The curricula higher educational institutions offer is the  key asset in enabling them to systematically educate their  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK15, March 1620, 2015, Poughkeepsie, NY, USA Copyright 2015 ACM 978-1-4503-3417-4/15/03...$15.00 http://dx.doi.org/10.1145/2723576.2723594  students. A wide variety of activities and studies on curricula have been carried out, such as the development of national curricula and methodologies for faculties to design curricula [11].  For over 40 years, the ACM and IEEE Computer Society jointly have been working on curricular guidelines for un- dergraduate programs in computer science (CS) and have been releasing reports as their work results on around a ten- year cycle. The latest work is the report Computing Science Curricula CS2013 released in December 2013 [1].  We consider that curriculum guidelines like CS2013 can be the foundation for universities to analyze their own curricula and design new curricula. For analyzing curricula, we de- veloped a web-based curriculum analysis tool based on the method we reported in [13]. Using the tool, we analyzed the curricula of MIT and those of the Open University (OU) on the basis of the Body of Knowledge (BOK) of CS2008, and found the characteristics of these universities [12]. For exam- ple, the OU offered many courses in Network Computing, Computation Science and Social and Professional Issues, but few courses in Discrete Structures. This practical na- ture of courses in OU is consistent with the fact that about 70 percent of OU undergraduates are in full-time employment.  We also applied our method to analyzing CS related curric- ula of universities in Japan [14] on the basis of two computing curricula, J07-CS in the CS field [4] and J07-IS in the informa- tion system field [7], both developed by IPSJ. The obtained results showed that our analysis revealed characteristics of each department, such as emphasizing application skills of computers and researching computer and information themselves.  In the work reported in this paper, we used CS2013 as a ba- sis for applying our curriculum analysis method to analyzing CS related curricula offered by CS departments of the top 10 universities in the United States. Using the method enabled us to compare courses across universities on the basis of the common BOK of the computing curricula. We analyzed the word distribution of syllabi in a topic space and found that CS2013 uniformly covered a wide area of CS. MIT and Stan- ford emphasized human factors, while CMU attached greater importance to the theoretical ones. We also found some CS departments offered not only a CS curriculum but also an electrical engineering one, and those departments showed a tendency to have more Architecture and Organization (AR) related curricula. Furthermore, even though Infor-  330    mation Assurance and Security (IAS) has not yet become a very popular field, some universities are already offering IAS related courses.  The rest of this paper is organized as follows. In Section 2, we review related work. In Section 3, we explain the ba- sic theories of our method and show that we improved our curriculum analysis method by using a simplified version of supervised LDA (ssLDA) [15, 2]. Experiment results are de- tailed in Section 4, and Section 5 concludes the paper with a summary of key points.  2. RELATED WORK A number of studies have been made on methodologies  and tools for analyzing curricula by using statistically pro- cessed syllabus data [16, 6, 10]. The approach we use enables us to use this data to compare courses and curricula offered by different universities.  Many educators in the CS field have read the CS2013 re- port and some of them contributed their knowledge to it. For example, Marshall tried to quantify the changes in the CS structure of the ACM/IEEE curricula series [8]. He focused attention on the BOK of computing curricula and visualized the structure of the curricula. However, he modeled Knowl- edge Areas (KAs) of BOK as a network graph to represent Knowledge Units (KUs) and topics as edges of the graph in the BOK. In applying his method to comparing the computing curricula, it was necessary to identify corresponding topics among the curricula. On the other hand, our method deals with curricula merely as a set of syllabus documents, and no identification is needed.  Gluga et al. developed a web-based system called PRO- GOSS that maps curricula learning goals and mastery levels to individual assessment tasks across entire degree programs [5]. The PROGOSS system has a function for educators to map prerequisites and goals of their courses into CS2013. Our curriculum analysis method shows the differences among computing curricula series, and it will help users of PRO- GOSS to modify an old curriculum based on past computing curricula.  Mendez et al. reported that they applied several learning analytic techniques to a curriculum [9]. They used the long- standing grade data of students in their institution for the curriculum analysis, and were to inform faculty members of their research results to improve their curriculum. We believe that comparing different curricula is a useful means for achieving such curriculum improvements.  3. CURRICULUM ANALYSIS 3.1 Acquiring Course Syllabi Sets  In this research, we analyzed several curricula offered by CS departments on the basis of CS2013. We referred to an article titled Computer Rankings, Best Undergraduate En- gineering Programs1 and chose the top 10 universities in the rankings as study subjects. Table 1 lists the universities and the relevant departments of each. In the rest of this paper we use the IDs in the rightmost column of the table to specify a university.  In applying our curriculum analysis method, we regarded a curriculum as a course syllabi set. We downloaded web  1U.S.News & World Report, http://colleges.usnews. rankingsandreviews.com/best-colleges/rankings/ engineering-doctorate-computer (accessed 2014-08-25)  pages from each departments website and from them ex- tracted course descriptions. Though students cannot take the courses freely, for simplicity we did not take such details into consideration.  3.2 Method 3.2.1 Outline  Here, we will described the details of the analysis method by using a machine learning approach. First, every syllabus is regarded as a set of used words (namely, the bag of words model). Then, each syllabus is projected to a point in the topic space defined by CS2013. Latent Dirichlet allocation (LDA) [3] is employed for this projection. LDA is widely used in natural language processing and machine learning and is known to be useful for extracting the topic space from a set of reference documents and projecting other documents to the extracted space. However, the original LDA is designed to ex- tract the topics automatically. On the other hand, the topic of a reference document is given in advance in our target dataset (namely, the BOK of CS2013). Consequently, although we used some complicated modifications in our previous work, they did not guarantee that the extracted model would be consistent with the given topics. In this paper, we propose a new method we call simplified supervised LDA (ssLDA) for giving an approximate model to our target dataset. Super- vised LDA (sLDA) has been proposed in a different context [15, 2] where classification labels are given as well as doc- uments. However, the labels are not directly related to the extracted topics in LDA. Our ssLDA is a simplified version of sLDA, where each extracted topic is bound to a given topic label. Table 2 shows the correspondence of terms between the proposed ssLDA model and our curriculum analysis.  It is essential that ssLDA allows a syllabus to belong to multiple KAs because it is based on a probabilistic model. In the training data (namely, CS2013), every syllabus belongs to a single KA. Therefore, many classification methods, such as support vector machines (SVMs), would seem to be suitable for the training phase. However, the syllabus in an actual curriculum is often distributed over many KAs. Therefore, methods that focus on classification are not appropriate. On the other hand, since ssLDA learns a probabilistic model from the training data, the dominance ratios of multiple KAs can be estimated from an actual syllabus.  It is also essential that ssLDA can estimate a continuous probabilistic model even from the sparse data because of the Dirichlet prior. Simple probabilistic models, such as naive Bayes classifiers, give a discontinuous probabilistic estima- tion because almost all words occur only a few times in CS2013. Therefore, such simple models are not appropriate for syllabi belonging to multiple KAs.  3.2.2 Generative Model The probabilistic generative model of a document with a  label in ssLDA is given as follows:  1. The probability of occurrence of topics  = (i) (con- strained to   j i = 1) is generated by Dirichlet distri-  bution with a hyper-parameter . Here, i = 1, . . . , K and K is the number of topics.  2. For each word w, a topic assignment zn = (zni) is given by multinomial distribution with the parameter ( = (i)), where n = 1, . . . , N and N is the number of words in the document. Each zn is the K-dimensional vector  331    Table 1: CS related departments of universities in the United States. Rank Department, University ID  1 Electrical Engineering, Computer Science, Massachusetts Institute of Technology MIT 2 Computer Science Department, Stanford University Stanford 3 School of Computer Science, Carnegie Mellon University CMU 4 Department of Electrical Engineering and Computer Sciences, Computer Science Division,  University of California, Berkeley UCB  5 Department of Computer Science, University of Illinois at Urbana-Champaign Illinois 6 College of Computing, Georgia Institute of Technology Georgia 7 Department of Electrical Engineering and Computer Science, University of Michigan Michigan 8 Computer Science Department, the University of Texas at Austin UTAustin 9 Department of Computer Science, Cornell University Cornell  10 The Computing +Mathematical Sciences Department, California Institute of Technology Caltech  where the assigned topic is 1 and the others are 0. Then, the word is given by the multinomial distribution with the parameter ( = (i j)), where i is the assigned topic in zn and j corresponds to the actual word wn in the vocabulary (where   j i j = 1 for every i).  3. The allocated topic of the document c  {1, . . . , K} is given by the following softmax distribution with the parameter z =   n zn/N and a hyper-parameter  > 0:  P (c|z, ) = exp (zc) /Ki=1 exp (zi). Here, z = (zi) can be regarded as the empirical distribution on the topic assignments of the document.  The graphical model of the above generative model is shown in Figure 1. This is an extension of the original LDA model [3] with c and . Because each label c corresponds to an internal topic assignment z, the model is much simpler than the previous sLDA models [15, 2].  3.2.3 Inference and Parameter Estimation For estimating the variables and parameters in the model,  Table 2: Term correspondence table between ssLDA model and curriculum analysis.  ssLDA Model Curriculum Analysis document syllabus  set of documents curriculum topic KA  position of a document in topic space  dominance ratios of KAs of a syllabus  training data BOK of CS2013 allocated label in training data  KA allocated to each syllabus in CS2013   = (i) true dominance ratios of KAs zn = (zni) KA allocated to word n   = (i j) Strength of relationship  between KA i and word j c most dominant KA  z w   c     N K  D  Figure 1: Generative model of simplified supervised LDA.  such as  and , we use the maximum likelihood estima- tion with a variational EM algorithm. Though the infer- ence process can be regarded as a special case of a more general method on a more complicated model in [15], it is derived here under the simplified model. The original like- lihood of a document with a given topic label is given as log P (w, c|, , ) where w = (wn) is a document consisting of words wns. The estimation of this original likelihood is intractable. Thus, the following lower bound is derived by the variational Bayesian approach in a way similar to that in the original LDA [3]:  log P (w, c|, , ) = log   d   Z  P (w, c, , Z|, , )  = log   d   Z  P (w, c, , Z|, , )Q (, Z|, ) Q (, Z|, )     d   Z  Q (, Z|, ) log P (w, c, , Z|, , ) Q (, Z|, )  = Eq (log P (w, c, , Z|, , ))  Eq (log Q (, Z|, )) = Eq (log P (w, , Z|, )) + Eq (log P (c|Z, ))  Eq (log Q (, Z|, )) (1)  where Z = (zn), Q (, Z|, ) = q (|)n q (zn|n) is the variational distribution, and Eq() is the expectation operator over Q.  = (i) and  = (n) are the free variational pa- rameters. In addition,  and n = (ni) are the Dirichlet pa- rameter and the multinomial one (constrained to   i ni = 1),  respectively. The lower bound in Eq. (1) is the same as that in the original LDA except for the second term Eq (log P (c|Z, )) in the last form, which is derived from the softmax distribu- tion of the topic label. The lower bound of Eq (log P (c|Z, )) is given as follows in a way similar to that in [15]:  Eq (log P (c|Z, )) = Eq ( log  exp (zc) i exp (zi)  )  = Eq (   n znc)  N  Eq  log    i  exp (zi)         n nc N   Eq log   i  zi exp ()   =    n nc N   . (2) Here, Jensens inequality on the exponential function is  applied under the conditions   zi = 1 and zi  0. Then, the variational and model parameters (, , ) can be estimated  332    by the variational EM algorithm. Only the update equations are shown here.  and  are estimated by  i = +   n ni, (3)  ni  iwn exp  (i)      k  k  + icN  (4)  where  is the digamma function and ic is the Dirac delta function.  is estimated by  i j    d   n  1 [ j = wdn  ] dni (5)  where d = 1, . . . , D corresponds to each document in the datasets and 1  [ j = wdn  ] is the function taking 1 only when the  n-th word wdn in the document d is equal to the word j (or 0 otherwise). The update equations for  and  are almost exactly the same as those in the original LDA [3]. The only difference is the additional term ic/N in the update for . It strengthens ni only if i is equivalent to the given topic label c. In addition, the weight depends on  and N only. In this paper,  and  are treated as fixed hyper-parameters. The settings on them are described in Section 3.2.5. Note that  degenerates into 0 if it is optimized by the maximum likeli- hood estimation because the penalty for misclassification is overestimated. Though it is a kind of overfitting problem, it can be avoided by the cross-validation method described in Section 3.2.5.  3.2.4 Prediction After the estimation of  of the generative model, a given  document with no label is projected to a position  = ( i )  in the topic probability space with the expected topic label c. In order to estimate  robustly,  is estimated as the expectation of  over the conditional probability with no la- bel. In other words,  is the mean over P (w, |, ). This distribution is approximated as   Z Q (, Z|, ) = q (|)  by the variational approach, where  = ( i )  and  = (nc) are the parameters estimated by the original LDA with no label. In other words, the additional term ic/N is omitted in the update process of  and  in the same way as in the original LDA. Then, i =    i /   i   i because    is the Dirichlet parameter. Regarding c, it is given so that log P (c = c|) is the maximum over c. By Eq. (2), log P (c|) is approximated as   n   nc  c where constant factors are omitted. Therefore,  c is given so that c is the largest over c  3.2.5 Determination of Hyper-parameters Here, the setting of the hyper-parameters  and  is de-  scribed. Though  is originally given as a K-dimensional vector for the K-dimensional Dirichlet distribution, it is given as a single parameter by assuming that  is the same constant over all the topics. was set to 1 in the same way as in our pre- vious work [14] by using some empirical and theoretical con- siderations. To determine we use a cross-validation method that makes use the classification accuracy of the topic labels. In this paper, we utilize the leave-one-out cross-validation (LOOCV) estimation of the topic classification accuracy in the training data CS2013. In order to avoid the effects of local maxima, the optimizations of ssLDA were carried out in five trials from random initial values for each . Then, the result  Table 3: Parameter  and average accuracy.  Accuracy  5.0 0.172 10.0 0.399 20.0 0.638 50.0 0.663  100.0 0.595 200.0 0.534  Table 4: Average accuracy in other methods (naive Bayes classifier and SVMs with various kernels).  Classifier Accuracy Naive Bayes 0.656 SVM (linear) 0.264 SVM (radial) 0.325 SVM (sigmoid) 0.620  Table 5: KAs of CS2013. ID KA AL Algorithms and Complexity AR Architecture and Organization CN Computational Science DS Discrete Structures GV Graphics and Visualization HCI Human-Computer Interaction IAS Information Assurance and Security IM Information Management IS Intelligent Systems NC Networking and Communication OS Operating Systems PBD Platform-Based Development PD Parallel and Distributed Computing PL Programming Languages SDF Software Development Fundamentals SE Software Engineering SF Systems Fundamentals SP Social Issues and Professional Practice  with the largest likelihood estimation was used. The LOOCV estimations of various  are shown in Table 3. It can be seen from the table that  = 50 is the best setting.  In order to compare the proposed ssLDA model with other classification methods, the LOOCV estimations of the classi- fication accuracy were calculated in the same training data CS2013. Table 4 shows the results for naive Bayes classifier and SVMs (with the linear, radial, and sigmoid kernels). We used the R e1071 package2 which was an implementation of naive Bayes classifier and SVMs. The hyper-parameters of SVMs were tuned by the cross validation. It shows that the highest accuracy in ssLDA (0.663 in Table 3) is (more or less) superior to all the other methods. It verifies the valid- ity of ssLDA model from the viewpoint of the classification accuracy.  4. EXPERIMENT 4.1 Computing Science Curricula CS2013  The ACM and the IEEE Computer Society released the report Computing Science Curricula CS2013 in December 2013 as a reference curriculum in CS. The BOK consists of a set of 18 KAs, each of which contains about 10 KUs that cor- respond to syllabi in our curriculum analysis method. Table 5 shows the names and abbreviations of KAs. 2Package e1071, http://cran.r-project.org/web/ packages/e1071/e1071.pdf (accessed 2015-01-23).  333    Table 6: Top 10 words strongly related to each KA. KA Words AL algorithm graph tree complexity automatum solve implement algorithmic class strategy AR instruction memory architecture familiarity assembly level organization processor representation machine CN simulation modeling science information including datum model algorithm computational processing DS proof probability induction propositional relation predicate usage bayes counting theorem GV rendering visualization graphic surface image representation animation rasterization light color HCI user interface interaction design motivation HCI evaluation technology quantitative report IAS security attack secure forensic cryptographic threat cryptography familiarity policy SE IM query relational database information index datum schema transaction file mining IS search agent reasoning planning classification robot representation learning implement algorithm NC network platform social layer familiarity application allocation industrial IP describe OS system operating memory device access SF virtual OS file management PBD function programming web mobile operation class constraint variant language event PD parallel parallelism distributed shared message versus race algorithm synchronization SF PL type program language code static analysis semantic syntax memory optimization SDF design program software component principle coding programming error code structure SE software requirement team risk project process specification testing development validation SF performance logic scheduling memory machine error program simple resource figure SP social professional privacy computing ethical computer intellectual policy HCI environmental  A general LDA model can also produce a set of topics, but the set is not controllable. Using ssLDA we can explicitly make each topic of ssLDA correspond to some KAs of CS2013. Table 6 shows the sets of the top 10 words strongly related to each KA of CS2013. For example, the words related to AL (Algorithms and Complexity) include algorithm, graph, and tree, which are types of data structures. In summary, supervised learning of ssLDA enables us to get a model for the intended topics with few trials.  4.2 Syllabi Distributions in Topic Space Here, we show how we investigated the syllabi distribu-  tions in both CS2013 and the actual universities in order to verify the appropriateness of the extracted topic space. If the topic space is appropriate, the syllabi should be distributed uniformly with a low-biased center in the topic space. First, Table 7 shows the mean (namely, the center) and the stan- dard deviation of the distribution along each axis of topic for CS2013 and the actual universities. Both of the distri- bution means seem to be near to the ideal unbiased point (1/18  5.56%, 5.56%,    , 5.56%). The Euclidean distance from the unbiased point is 3.65 for CS2013 and 5.05 for the universities. This shows that the means of the actual uni- versities were slightly further from the unbiased point than those of CS2013. Nevertheless, the distances were not es- pecially large even in the actual universities. This suggests that the extracted topic space gives a low-biased center for the actual universities as well as the artificially constructed CS2013. For the standard deviation and the maximum, the values in CS2013 are relatively large. Almost all the maxima are near to the largest value 100%. This shows that the syllabi of CS2013 are distributed widely over the topic space. On the other hand, both the standard deviations and maxima for the universities were quite smaller than those on CS2013. This shows that the syllabi of the actual universities are distributed densely over the limited area of the topic space. However, the results do not clarify whether the syllabi are distributed uni- formly in the limited topic space or not. In order to investigate the uniformity in CS2013 and the actual universities, we use the eigenvalues of the correlation matrix of the distribution. Since the correlation matrix is a normalized covariance matrix  whose diagonal variances are normalized to 1, the uniformity in the limited area can be investigated. Figure 2 shows the bar graphs of the rank-ordered eigenvalues for the following four sets of syllabi: CS2013, all the actual universities, MIT only, and Stanford only. The top two universities are included only for comparison. Note that the leftmost (and the small- est) eigenvalue on each bar graph is always 0 because of the constraint that   i   i = 1 in the topic space. If the syllabi are  distributed completely uniformly in the limited space, all the eigenvalues are assumed to be the same except for the small- est one (=0). In Figure 2, the bar graph on CS2013 seems to be flat. This shows that the syllabi of CS2013 are distributed uniformly in the wide area of the topic space. Though the slope of the bar graph for all the actual universities is steeper than that for CS2013, it is relatively flatter than those for the actual single universities (MIT and Stanford). This suggests that the distribution of syllabi for the actual universities is weakly uniform. In summary, the results show that CS2013 gives a wide and uniform distribution with a low-biased cen- ter in the topic space. This verifies that the proposed method extracts an appropriate topic space from the given dataset CS2013. On the other hand, the syllabi of the actual univer- sities are distributed within a limited area of the topic space. However, its center is low-biased and the distribution is rel- atively uniform. This suggests that the proposed method is suitable as a means to analyze actual universities.  4.3 Analysis of CS Department Courses Using the model based on BOK of CS2013 with ssLDA, we  analyzed CS department curricula of the universities in Table 1 and obtained the distributions of the KAs in CS2013. If a course syllabus on GV (Graphics and Visualization) has a relatively high probability, the course is considered to offer GV related topics such as CG, Animation, and solid model- ing. Table 8 shows a part of a course syllabus titled CS418 Interactive Computer Graphics at Illinois. The course CS418 is the most strongly related course to GV.  We can compare courses offered by various departments by their distributions and find similar courses by the prox- imity of the distributions. For example, Table 9 shows the 10 courses nearest to CS418. Here, the Euclidean distance in  334    Table 7: Mean, standard deviation (SD), and maximum of the syllabi distribution along each topic for CS2013 and actual universities.  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP mean(CS) 5.6 5.1 4.1 4.4 4.5 5.6 5.6 6.3 6.4 5.3 7.1 4.4 6.1 6.9 4.7 5.9 5.7 6.3 mean(univ) 6.1 5.4 7.0 4.5 7.4 7.6 4.3 4.8 7.0 4.0 4.5 5.3 4.3 5.2 5.2 5.9 4.3 7.2 SD(CS) 16.8 15.5 14.0 13.8 15.9 17.2 16.0 18.0 18.7 12.6 15.8 12.3 16.9 18.2 13.6 17.3 15.0 18.7 SD(univ) 5.1 3.8 4.4 3.9 6.1 5.3 4.1 4.2 5.6 3.0 3.4 3.9 3.0 4.2 3.6 4.3 2.3 5.8 max(CS) 90.4 83.9 88.6 85.5 90.4 81.9 92.3 86.7 86.1 76.8 77.5 85.8 89.5 90.0 88.3 93.1 85.7 91.1 max(univ) 54.4 39.3 35.5 36.5 55.6 49.4 48.7 50.6 36.9 37.7 31.9 30.4 28.4 51.6 30.8 36.9 21.6 54.5  Table 8: Course syllabus CS418 offered by Illinois and its distribution over KAs. syllabus CS418 Interactive Computer Graphics Basic mathematical tools and computational techniques for modeling, rendering, and animating 3-D scenes... 1. Rotate, translate, scale an object represented by triangle mesh, and manage hierarchies of such transformations. (a)(b)(c)(h)(i)(j)(k)> 2. Render an image of a meshed object with lighting, texture, reflections and perspective from an arbitrary viewpoint with hidden surfaces removed and extraneous geometry clipped. (a)(b)(c)(h)(i)(j)(k)> 3. Model and render an object using parametric curves and surfaces including Hermite, Bezier, NURBS and Catmull-Clark subdivision surface presentations. (a)(b)(c)(h)(i)(j)(k)>... AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP 1.0 2.8 7.2 2.7 55.6 4.8 0.6 2.3 8.2 0.6 0.6 4.2 1.4 0.9 1.5 1.1 3.0 1.5  0 5 10 15 20 0  0.5  1  1.5  2  2.5 CS2013                                          rank  ei ge  nv al  ue  0 5 10 15 20 0  0.5  1  1.5  2  2.5 All Universities                                          rank  ei ge  nv al  ue  0 5 10 15 20 0  0.5  1  1.5  2  2.5 MIT only                                          rank  ei ge  nv al  ue  0 5 10 15 20 0  0.5  1  1.5  2  2.5 Stanford only                                          rank  ei ge  nv al  ue  Figure 2: Bar graphs of rank-ordered eigenvalues of corre- lation matrices of distributions for CS2013 (upper left), all actual universities (upper right), MIT only (lower left), and Stanford only (lower right))  the topic space is used as the distance. Thanks to ssLDA this distance is more robust to word choices than a tf-idf based distance.  4.4 Analysis of University Characteristics in Topic Space  In this section, we show how we investigate the character- istics of the actual universities in the extracted topic space. Here, each university is characterized by the mean of its syl- labi in the topic space. Table 10 shows the means of all the 10 universities. We used principal component analysis (PCA) to extract their characteristics. Table 11 shows the three prin- cipal components of the topics, which account for about 90 percent of the total variance. Each component is discussed in  Figure 3: Plot of the universities along the second principal component (horizontal) and the third one (vertical).  turn below.   The first component: The values for the topics in the first principal component were highly correlated to those for the mean of all the actual universities (the second row (mean(univ)) in Table 7). The correlation coefficient between them was 0.99. The mean of all the actual uni- versities is the averaged bias from the ideal unbiased point. Thus, this component can be interpreted as the current trends common to all the universities. In other words, the result suggests that the first component cor- responds to the degree to which each university follows the current trends.   The second component: The topics with higher values are HCI, SE, and SP, all of which include human factors.  335    Table 9: 10 nearest course to CS418. Rank Distance Course Title University  1 0.125 EECS 487. Interactive Computer Graphics Michigan 2 0.142 CS419 Production Computer Graphics Illinois 3 0.146 15-462 Computer Graphics CMU 4 0.150 184. Foundations of Computer Graphics (4) UCB 5 0.165 CS 7490 Adv Image Synthesis Georgia 6 0.187 CS 3451 Computer Graphics Georgia 7 0.221 CS/CNS 171. Introduction to Computer Graphics Laboratory Caltech 8 0.236 CS 4620: Introduction to Computer Graphics Cornell 9 0.246 CS 5620: Introduction to Computer Graphics Cornell  10 0.258 6.837 Computer Graphics MIT  On the other hand, those with lower values are IS, CN, and DS, which are related to theoretical or mathematical factors. These observations suggest that the second component could be interpreted as the weight between the human factors and the theoretical ones.   The third component: The topics with higher values are CN, SP, and SDF, which seem to be related to gen- eral problems in software development. On the other hand, those with lower values are GV and PL, which are related to elemental techniques. These observations suggest that the third component could be interpreted as the axis from the elemental techniques to the general solutions.  Figure 3 shows the plot of the universities along the second principal component (horizontal) and the third one (verti- cal). Interestingly, the top three universities (MIT, Stanford, and CMU) seem to form a triangle with the other universities placed within it. By utilizing the above possible interpreta- tions of the principal components, we can make the following speculations from the results. First, both MIT and Stanford emphasize human factors, while CMU attaches greater im- portance to theoretical ones. Second, MIT focuses on general solutions while Stanford emphasizes elemental techniques.  4.5 Analysis of CS Department Curricula If a CS department offers many courses that are related to  a number of KAs, it is considered the department focuses on the KAs. That is, we can detect the characteristics of the curriculum of each CS department from the distribution of its courses.  We can average all courses of a given department as shown in Table 10. These averaged distributions can be considered to characterize the curriculum of each department.  From the deviation of a curriculum from 1/18 of the uni- form distribution, we can ascertain whether the curriculum is strongly related to some KAs or not. For example, uni- versities with departments whose name includes Electrical Engineering (EE), such as UCB, MIT, and Michigan, have a curriculum with a higher AR (Architecture and Organi- zation) probability than the other universities, reflecting the characteristics of those CS departments offering courses to EE students.  Figures 4 and 5 show box plots of course distributions for various KAs. A short box means that only a few courses are related to the KA and a long box means that there are various courses related to the KA, some of them weakly and the others strongly. The center of an arrow is the mean, and the length of the arrow stands for 2 SDs. The courses that are particularly strongly related to some KAs are drawn as small  circles in the figures. As mentioned above, UCB, MIT, and Michigan offer courses that are strongly related to AR.  Overall there are comparatively few courses related to IAS (Information Assurance and Security), NC (Networking and Communication), PD (Parallel and Distributed Computing), and SF (Systems Fundamentals). However, even though IAS has not yet become a very popular field, some universities (e.g., Illinois, Georgia, and Caltech) offer courses strongly re- lated to it. As examples, one could cite the CS461: Computer Security I course at Illinois and the CS 6260: Applied Cryp- tography course at Georgia. Similarly, we find NC related courses such as 356R: Introduction to Wireless Networks at UTAustin and PD related courses such as ACM/CS 114: Parallel Algorithms for Scientific Applications at Caltech.  The description of SF in the CS2013 final report is as follows: The Systems Fundamentals Knowledge Area is designed to present an integrative view of these fundamental concepts in a unified albeit simplified fashion, providing a common foun- dation for the different specialized mechanisms and policies appropriate to the particular domain area. Some KUs of SF are cross-referenced by other KAs such as PD, NC, OS (Oper- ating Systems), AL (Algorithms and Complexity), and AR. In other words, the courses related to SF are also related to these KAs. In this way, using our curriculum analysis method, we were able to compare courses across universities on the basis of common computing curricula.  5. SUMMARY In this paper, we showed how we used a curriculum anal-  ysis method we have been developing to analyze computer science (CS) related curricula offered by the top 10 CS de- partments of universities in the United States. We applied the method on the basis of Computing Science Curricula CS2013, a report released by the ACM and IEEE Computer Society. To enable our method to perform this analysis, we improved it by using a simplified version of supervised la- tent Dirichlet allocation (ssLDA). We found that CS2013 uni- formly covered a wide area of CS. We found that among the universities that served as study subjects, MIT and Stanford emphasized human factors while CMU attached greater im- portance to theoretical ones. We also found that some CS departments offered not only a CS curriculum but also an electrical engineering, and those departments showed a ten- dency to have more Architecture and Organization (AR) related curricula. Furthermore, we found that even though Information Assurance and Security (IAS) has not yet be- come a very popular field, some universities are already offering IAS related courses. Similarly, courses related to Networking and Communication (NC) and Parallel and Distributed Computing (PD) are being offered. However,  336    Table 10: Distribution of each curriculum. (%) AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  MIT 5.9 6.2 7.5 4.5 6.1 9.5 3.6 3.9 6.3 3.7 4.2 4.6 3.7 3.3 5.7 7.6 4.1 9.5 Stanford 5.7 6.2 4.6 2.8 9.5 10.7 4.1 4.8 5.5 3.4 2.8 5.5 5.0 6.3 4.4 7.4 4.2 7.1 CMU 6.5 4.3 8.9 7.3 9.5 6.9 3.0 4.7 7.3 2.8 3.1 4.7 3.7 5.9 5.6 5.0 4.2 6.5 UCB 6.3 7.5 7.0 4.0 7.2 7.4 5.9 3.8 6.2 4.3 3.5 7.1 4.8 4.5 4.9 4.8 3.7 7.1 Illinois 5.7 4.3 7.4 3.5 9.9 7.8 3.7 5.1 7.3 3.5 4.1 4.9 4.3 7.0 4.2 5.9 4.4 6.9 Georgia 5.6 4.9 6.5 4.8 6.5 7.3 5.1 5.4 6.2 4.6 5.2 4.8 4.5 4.9 5.5 6.1 4.4 7.5 Michigan 6.3 6.2 6.1 4.4 6.8 7.0 4.0 4.5 8.9 3.8 5.2 5.4 3.6 4.9 6.3 5.5 5.0 6.1 UTAustin 5.5 5.1 8.6 4.4 6.0 7.3 5.0 4.7 5.2 5.2 4.5 6.2 4.2 5.1 5.8 5.9 4.5 6.9 Cornell 6.5 5.6 7.5 4.4 7.7 6.8 4.0 4.6 9.0 3.6 4.9 6.2 4.2 5.5 4.6 4.7 3.8 6.5 Caltech 8.7 5.3 7.0 4.4 8.0 6.1 3.9 4.9 7.5 3.5 4.9 4.6 4.2 4.9 5.5 6.0 4.3 6.1  Table 11: Three principal components in the topic space: Each component (with the dominance of the variance) is normalized by letting its Euclidean norm be 10.  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP 1st (69%) 1.3 -0.1 2.7 -1.9 4.2 4.1 -2.6 -1.6 2.5 -3.3 -2.6 -0.4 -2.4 -0.4 -0.7 0.7 -2.3 2.7 2nd (11%) -2.1 2.4 -3.2 -3.2 -1.7 4.9 1.6 -0.5 -4.1 1.2 -1.1 0.7 1.2 -1.1 -0.6 3.1 -0.1 2.6 3rd (7%) 0.6 1.5 3.7 1.3 -5.4 -0.5 0.1 -1.9 1.0 0.5 1.2 0.3 -2.1 -5.0 2.4 0.1 -0.9 3.2  courses related to Systems Fundamentals (SF) are also re- lated to other Knowledge Areas (KAs) because some Knowl- edge Units (KUs) of SF are cross-referenced by other KAs. These findings confirmed that the improvement by ssLDA brought to our method is effective.  In future work, we plan to use our method to analyze a larger number of curricula. We also want to use it to search various educational areas that offer curricular guidelines such as the computing curricula and analyze curricula in those areas.  6. REFERENCES [1] ACM/IEEE-CS Joint Task Force on Computing  Curricula. Computer science curricula 2013. Technical report, ACM Press and IEEE Computer Society Press, December 2013.  [2] D. M. Blei and J. D. McAuliffe. Supervised topic models. In NIPS, volume 7, pages 121128, 2007.  [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. Journal of Machine Learning Research, 3:9931022, 2003.  [4] Committee of Education of Computer Science, Information Processing Society of Japan. Computing curriculum standard in computer science J07-CS report (january 20, 2009), 2009. http: //www.ipsj.or.jp/12kyoiku/J07/20090407/J07_ Report-200902/4/J07-CS_report-20090120.pdf.  [5] R. Gluga, J. Kay, and R. Lister. PROGOSS: Mastering the curriculum. In Proceedings of The Australian Conference on Science and Mathematics Education (formerly UniServe Science Conference), 2012.  [6] M. Ida. Textual information and correspondence analysis in curriculum analysis. In Proceedings of the 18th international conference on Fuzzy Systems, pages 666669. IEEE Press, 2009.  [7] Y. Kaminuma. Summary of J07-IS curriculum, 2008. http: //www.ipsj.or.jp/12kyoiku/J07/20090407/J07_ Report-200902/5/J07-IS_curriculum-200803.pdf (in Japanese).  [8] L. Marshall. A comparison of the core aspects of the ACM/IEEE computer science curriculum 2013  strawman report with the specified core of CC2001 and CS2008 review. In Proceedings of Second Computer Science Education Research Conference, CSERC 12, pages 2934, New York, NY, USA, 2012. ACM.  [9] G. Mndez, X. Ochoa, and K. Chiluiza. Techniques for data-driven curriculum analysis. In Proceedings of the Fourth International Conference on Learning Analytics And Knowledge, LAK 14, pages 148157, New York, NY, USA, 2014. ACM.  [10] S. Ota and H. Mima. Machine learning-based syllabus classification toward automatic organization of issue-oriented interdisciplinary curricula. Procedia - Social and Behavioral Sciences, 27:241247, 2011.  [11] W. F. Pinar, W. M. Reynolds, P. Slattery, and P. M. Taubman. Understanding Curriculum: An Introduction to the Study of Historical and Contemporary Curriculum Discourses. Peter Lang Pub Inc., 1995.  [12] T. Sekiya, Y. Matsuda, and K. Yamaguchi. Analysis of computer science related curriculum on LDA and isomap. In ITiCSE10, Proceedings of the 15th Annual SIGCSE Conference on Innovation and Technology in Computer Science Education, pages 4852, 2010.  [13] T. Sekiya, Y. Matsuda, and K. yamaguchi. Development of a curriculum analysis tool. In ITHET 2010, 9th International Conference on Information Technology Based Higher Education and Training, pages 413418, 2010.  [14] T. Sekiya, Y. Matsuda, and K. Yamaguchi. Analysis of computer science related curriculum. In Summer Symposium in Shizukuishi 2013, SSS2013, pages 3340, 2013. (in Japanese).  [15] C. Wang, D. Blei, and F.-F. Li. Simultaneous image classification and annotation. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 19031910. IEEE, 2009.  [16] X. Yu, M. Tungare, W. Fan, Y. Yuan, M. Prez-Quinones, E. Fox, W. Cameron, and L. Cassel. Automatic syllabus classification using support vector machines. Handbook of Research on Text and Web Mining Technologies. Information Science Reference, 2008.  337    AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  MIT  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  Stanford  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  CMU  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  UCB  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  Illinois  Figure 4: Distribution of courses offered by MIT, Stanford, CMU, UCB, and Illinois.338    AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  Georgia  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  Michigan  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  UTAustin  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  Cornell  AL AR CN DS GV HCI IAS IM IS NC OS PBD PD PL SDF SE SF SP  0 10  30 50  Caltech  Figure 5: Distribution of courses offered by Georgia, Michigan, UTAustin, Cornell, and Caltech.339    "}
{"index":{"_id":"55"}}
{"datatype":"inproceedings","key":"Chen:2015:TAL:2723576.2723584","author":"Chen, Bodong and Chen, Xin and Xing, Wanli","title":"Twitter Archeology of Learning Analytics and Knowledge Conferences","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"340--349","numpages":"10","url":"http://doi.acm.org/10.1145/2723576.2723584","doi":"10.1145/2723576.2723584","acmid":"2723584","publisher":"ACM","address":"New York, NY, USA","keywords":"Twitter, Twitter analytics, hashtag analysis, learning analytics, social network, topic modeling","abstract":"The goal of the present study was to uncover new insights about the learning analytics community by analyzing Twitter archives from the past four Learning Analytics and Knowledge (LAK) conferences. Through descriptive analysis, interaction network analysis, hashtag analysis, and topic modeling, we found: extended coverage of the community over the years; increasing interactions among its members regardless of peripheral and in-persistent participation; increasingly dense, connected and balanced social networks; and more and more diverse research topics. Detailed inspection of semantic topics uncovered insights complementary to the analysis of LAK publications in previous research.","pdf":"Twitter Archeology of Learning Analytics and Knowledge Conferences  Bodong Chen University of Minnesota Minneapolis, MN, USA chenbd@umn.edu  Xin Chen Purdue University  West Lafayette, IN, US chen654@purdue.edu  Wanli Xing University of Missouri  Columbia, MO, US wxdg5@mail.missouri.edu.edu  ABSTRACT The goal of the present study was to uncover new insights about the learning analytics community by analyzing Twit- ter archives from the past four Learning Analytics and Knowl- edge (LAK) conferences. Through descriptive analysis, in- teraction network analysis, hashtag analysis, and topic mod- eling, we found: extended coverage of the community over the years; increasing interactions among its members regard- less of peripheral and in-persistent participation; increas- ingly dense, connected and balanced social networks; and more and more diverse research topics. Detailed inspection of semantic topics uncovered insights complementary to the analysis of LAK publications in previous research.  Categories and Subject Descriptors K.3.1 [Content Analysis and Indexing]: Linguistic pro- cessing; H.3.5 [Online Information Services]: Pattern analysis; I.2.7 [Natural Language Processing]: Text anal- ysis; K.4.3 [Organizational Impacts]: Computer-supported collaborative work  General Terms Algorithms, Human Factors, Measurement  Keywords Learning Analytics, Twitter, Twitter Analytics, Social Net- work, Hashtag Analysis, Topic Modeling  1. INTRODUCTION Learning analytics as a nascent field of scholarship is evolv-  ing rapidly and garnering broad interest in both educational research and practice [23]. Since the inaugural Learning An- alytics and Knowledge (LAK) conference in 2011, exciting moves have been made during the past four years. The Soci- ety of Learning Analytics Research (SoLAR) was launched in 2012; the Learning Analytics Summer Institute (LASI)  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. LAK 15, March 16 - 20 2015, Poughkeepsie, New York, USA Copyright 2015 ACM 978-1-4503-3417-4/15/03$15.00. http://dx.doi.org/10.1145/2723576.2723584  was first held in 2013; the first issue of the Journal of Learn- ing Analytics was published in 2014. Together these events indicated the establishment of learning analytics as an inde- pendent field of research and practice.  Since learning analytics as a field is still in its early stage, efforts have been made to understand the evolution of the field as well as its linkages with others. For instance, re- searchers have attempted to understand the similarities and distinctions between learning analytics and Educational Data Mining (EDM)two communities that share similar inter- ests but grew separately in their early years [37]. Colleagues have also attempted to study the roots of learning analyt- ics and its relations with fields such as learning sciences, machine learning, and data-driven analytics [16, 3]. More recently, empirical studies were conducted to understand the field based on the LAK Open Dataseta dataset which contains structured metadata from research publications in the field of learning analytics and EDM [43, 14]; to this end, topic models [36], ontology [46], visualizations [35], and knowledge systems [30, 20] were built and contributed to the efforts of uncovering key themes of the field and identifying major challenges faced by the community [38].  The present study contributes to the ongoing reflection upon learning analytics by analyzing Twitter archives of the past four LAK conferences from 2011 to 2014. Using tweets posted by the conference participantswho attended LAK either in person or remotelywe are hoping to uncover new insights about the evolution of the field. The significance of this work is two-fold. First, because learning analytics is a relatively new field attracting participation of both aca- demics and practitioners, many community members have not published in conference proceedings or relevant academic journals, or are not intended to publish at all; as a result, the analysis of data from academic publications, as those in- cluded in the LAK Open Dataset, falls short in revealing the reach and development of the community. In contrast, Twit- ter as an information sharing and social networking platform broadly used at LAK conferences affords us with authentic, multimodal data from a richer pool of participants besides those who have published in the learning analytics literature. Second, Twitter supports rich, real-time social interactions among conference participants, in the form of retweeting, mentioning, and replying, which could facilitate meaningful exchanges not supported by traditional academic publish- ing venues. By analyzing social interactions on Twitter by identifying leaders, characterizing information diffusion patterns, and detecting sub-communities, for instancewe could obtain a more vivid picture of community dynam-  340  http://dx.doi.org/10.1145/2723576.2723584   ics of learning analytics. Therefore, the analysis of Twit- ter archives, or in our words, Twitter archeology of LAK conferences could potentially reveal new insights about the learning analytics community.  In this article, we start with a brief introduction to Twit- ter and Twitter analytics. Then we introduce the LAK Twitter dataset and the analytic approaches applied in the study. After that, we present and discuss results from our analysis and conclude by discussing limitations and future directions of this study.  2. TWITTER Twitter is, as we write, an online social networking ser-  vice that enables users to share short messages known as tweets. While Twitter is normally conceptualized as a social network, or a microblogging service, it has essentially grown into an information or news network [27]. Because of the ag- ileness offered by its 140-character limit, it has emerged to become a personal news-wire, in Twitters own words [41], on which all types of world events are posted and further spread through retweeting. These user behaviors collectively give rise to trending topics and facilitate large-scale social phenomena, such as Haiti earthquake relief efforts [40] and Arab Spring [31]. Thanks to Twitters nature as a per- sonal news platform, it has also contributed to transforming journalism, by changing how people become aware of news [18] or how journalists engage with their profession [28].  Twitter has been widely used at conferences, from its im- pressive debut at the 2007 South by Southwest Interactive (SXSWi) conference to almost every academic conference the authors have attended in recent years. At conferences, Twitter could be used to establish a backchannel to enable richer communication among attendees and extend conver- sations beyond the conference venue [2]. Within a tradi- tional academic conference setting, the space is normally di- vided into a front stage for the speaker and a large back areafor the audience [34]. In this context, attention is solely focused to the front and interactions are usually limited to Q&A periods. Because of the constrains posed by time and space, opportunities provided for the audience to interact with each other and to collectively construct understanding of a given speech are usually rare. As a result, the traditional conference model could cause feedback lags, stress for ask- ing questions, and decreasing participation [1, 15]. Twitter could help close the gap of social interactions at conferences, thanks to its simplicity and the ubiquity of Internet connec- tion. Conversationality and collaboration afforded by Twit- ter, largely through mentioning (@) [19] and hashtagging (#) [21], could help mitigate the disconnect among conference participants. Not surprisingly, Twitter has become widely used at academic conferences.  3. TWITTER ANALYTICS Because the extensive use of Twitter in various social  sectors, the analysis of Twitter data carries the potential to offer actionable knowledge for stakeholders, or to help us discover information diffusion paradigms on social me- dia. For example, sentiment analysis of tweets has also been broadly applied to understand customer perception of cer- tain products or brands [8, 11], or to characterize presiden- tial debates by combining tweets with live television pro- grams [12]. New information diffusion mechanisms could  also be discovered from various aspects of Twitter usage such as social linkages [27] and retweeting behaviors [42]. In education, Twitter and other social media platforms are increasingly used in classrooms to facilitate communication between teachers and students, as well as among students [17, 25, 26]. Combining data-driven approaches and ethno- graphic approaches, researchers use Twitter data to investi- gate the unique online culture among the digital natives [22, 7], students identity performance on social media [24], and college students learning experiences [10].  3.1 Twitter Analytics in the Academic Con- ference Context  Twitter usage at academic conferences has also attracted some research attention. Previous studies have mainly fo- cused on three aspects. The first aspect centers on users and usage of Twitter at academic conferences. For exam- ple, some studies seek to understand who use Twitter at conferences, why they use it, and how. Using a survey, col- leagues identify attendees, online attendees, speakers, and organizers of conferences as the main user groups of Twitter at a conference [15]. Through content analysis of tweets, researchers also distinguish seven main purposes of using Twitter during conferences, including: comments on pre- sentations, sharing resources, discussions and conversations, jotting down notes, establishing an online presence, and ask- ing organizational questions [34].  The second category of research focuses on interaction on Twitter during conferences. For instance, Social Network Analysis of online interactions of Twitter users identified different types of users, characterized by different levels of participation and influence [9]. Visual analytics platforms are designed to facilitate Twitter users interaction during conferences [13].  The last cluster of research centers on the effect of using Twitter for academic conferences. For example, one study explores whether the use of Twitter enhances conference ex- perience, collaboration, and collective construction of knowl- edge [34]. Others incorporate timeline analysis and Social Network Analysis together to study whether the use of Twit- ter could help reach a broader audience [29].  The present study has a unique agenda different from all the three categories. It is the very first study, as far as we know, attempting to track the evolution of an academic field by mining Twitter data from its annual conferences. To achieve this goal, we attempt to answer the following major research questions through in-depth analysis of the LAK Twitter archive:   To which extent did Twitter enable participation and conversation in each years LAK conference, charac- terized by the occurrences of tweets, retweets, and replies   Supposing Twitter participants could approximately represent the learning analytics community, how did the composition of Twitter participants change over the years   What does the social networks of LAK Twitter par- ticipants look like Who are the influential figures in the community To which extent did the community dynamics change over the years  341     What are the underlying topics in LAK Twitter par- ticipation To which extent did the topics evolve over the years  4. METHODS  4.1 Dataset The dataset was aggregated through the official LAK con-  ference hashtags, i.e., #LAK11, #LAK12, #LAK13, and #LAK14, and archived using the Twitter Archiving Google Spread- sheet (TAGS).1 A total of 10,736 tweets by 1,217 unique Twitter users were archived in this dataset. An overview of Twitter participants2 and tweets is provided in Table 1.  Conference Participants Tweets LAK11 215 1358 LAK12 606 4050 LAK13 280 2223 LAK14 362 3105  Table 1: Overview of Dataset  4.2 Preprocessing Before any actual data analysis, substantial efforts were  put to clean the dataset. Because TAGS and the Twitter API have been evolving over the years, inconsistencies were evident in the multi-year dataset. For example, it was until 2012 when the Twitter API would return an entities_str that encapsulates all information related to a tweet; in the 2011 dataset, to whom a tweet was addressed was not pro- vided, whereas such information was stored in later archives in a to_user field. More importantly, the biggest challenge we faced when cleaning the data was a systematic mistake of user ids in the 2011 archive. To fix this issue, we re- placed ones user id in 2011 if the user could be found in later archives; otherwise, we used the current Twitter API to retrieve the user id. In addition, we paid special atten- tion to track users who changed their screen names over the years, by tracking their user ids and replacing the obsolete names with the newest ones.  After data cleaning, further parsing was conducted at the tweet level. Specifically, if a tweet was identified as a retweet (i.e., starting withRT @user: ...), the user from whom this tweet was retweeted was extracted; if a tweet was identified as a reply, the user(s) to whom this tweet was addressed to were also parsed.  The cleaned data was saved into a set of comma-separated values (CSV) files each containing tweets, users, retweets, replies, mentions, and hashtags for later analysis. Other formats such as .Rdata and JSON were also created to meet needs within our team.  4.3 Data Analysis To answer these research questions, we conducted a range  of analysis on the LAK Twitter dataset.  1Martin Hawkseys Twitter Archiving Google Spreadsheet (TAGS), version 3 and 5. 2In the following sections of this paper, we are using partic- ipants to denote Twitter participants of LAK conferences, unless it is specified otherwise.  4.3.1 Desriptive Analysis To get a basic understanding of Twitter participation at  LAK, we first conducted descriptive analysis on the dataset. In addition to the summary presented in Table 1, we pro- duced summarizing statistics with regards to retweets and replies in each years conference. We also conducted de- scriptive analysis at the user level, computing the means of tweets, retweets and replies sent by each user, as well as the average numbers of times each user got retweeted or was replied to. Comparisons were made across years to uncover possible changes.  4.3.2 The Flow of Twitter Participants at LAK To understand the composition of Twitter participants at  LAK, we tracked new and returning participants across four conferences. For each participant, we identified the year(s) he or she participated. A Sankey diagram, which is com- monly used to visualize energy or material flow [33], is pro- duced to visualize the flow of Twitter participants at LAK. Further descriptive statistics were conducted to understand different types of participants defined by their appearance over the years.  4.3.3 Interaction Social Networks Interaction network graphs were generated based on retweet,  reply, and mention interactions. These network graphs were directional. For example, if user A retweeted user B, a di- rectional edge would be drawn from user A to user B. We initially focused on reciprocate rate, an important network measure for a directional network. This concept is defined as the percentage of the pairs that have edges pointing to each other among all connected pairs of nodes. A higher re- ciprocate rate would imply a more egalitarian network. We also analyzed other characteristics of each years interaction network, including average degree, network diameter, aver- age path length, and the proportion of the largest connected component.  Because retweeting is an essential mechanism for informa- tion propagation in Twitter [42], we further studied retweets to determine who is influential in the network or how many people a piece of information reaches [45]. In particular, community detection was performed to identify the influen- tial figures and explore the online community development over the four years. This analysis was composed of the fol- lowing steps: (1) Because clustering algorithm is sensitive to outliers, extreme outliers have to be removed [44]. In this study, giant component filter algorithm is applied to the data in each year and expected to eliminate the outliers vertices (users); (2) after getting rid of the outliers, we performed the community detection (clustering) analysis using the fast unfolding algorithm [6]. According to [6], this algorithm starts with assigning each node into a different community in the network. Then the algorithm evaluates the gain of modularity by placing a node into another community. The node will stays in the original community if no positive mod- ularity is obtained. The previous process is repeated until no further improvement of modularity is possible. On the other hand, to affirm that consistent clusters (communities) across different runs, the fast unfolding community detec- tion algorithm is executed 20 times on each years network and the highest frequency number of communities appearing in those networks are chosen as the resulting communities.  342  http://mashe.hawksey.info/2012/01/twitter-archive-tagsv3/ http://mashe.hawksey.info/2013/02/twitter-archive-tagsv5/   4.3.4 Evolution of Topics in the Community In addition to analyzing participants and social interac-  tions among them, another important aspect was to under- stand the topics discussed on Twitter and their changes over time. We first did a hashtag analysis. Hashtag is an impor- tant Twitter mechanism that users use to signal the central topics expressed in their tweets. We took the occurrences of hashtags and generated a hashtag cloud for each year. The hashtags #LAK11 to #LAK14 were removed because they appeared in all tweets from respective years. Then we in- spected the hashtag clouds for popular topics in each year and their changes over the years.  Second, topic modeling was applied to uncover underlying topics in the tweets. In particular, Latent Dirichlet Alloca- tion (LDA) [5] was used. The analytical process included the following steps:  (1) Text sanitizing. First of all, text that was semanti- cally less meaningful or irrelevant was removed. Such text included Twitter users screen names, links, Twitter-specific syntax (e.g., RT, via), and special character encodings (e.g., &amp;). We also decided to remove hashtags because they tended to distort the semantic space because of their high frequencies.  (2) LDA and visual exploration. Second, we adopted the topicmodels R package to model the topics on the sani- tized text. To identify the optimal number of topics for topic modeling, we tested with a sequence of numbers from 1 to 100, which are suitable for the size of dataset based on our experience. The model selection was made based on the harmonic mean of the estimated log-likelihood of each model [32]. After choosing the optimal topic model, we then used LDAvis to assist visual exploration and interpretation of extracted topics. Compared to the turbo topics [4], which has been applied on LAK literature data [36], LDAvis en- abled interactive exploration and clustering of topics. Using LDAvis, we were able to interactively make sense of the top- ics, assign names to meaningful topics, and cluster them into several categories.  (3) Tracking selected topics. Finally, we chose to track the development of certain research topics of learning analytics over the four years. Note that LDA would assign a most probable topic to each tweet [5]. We were then able to count how many times each topic has appeared in each conference. Data were further visualized for interpretation.  5. RESULTS AND DISCUSSION  5.1 Descriptive Analysis of Twitter Participa- tion and Conversation  Table 2 presents descriptive statistics of each years tweets. With the exception of LAK12, the numbers of participants and tweets have been increasing over the years. The counts and percentages of retweets and replies also increased over- all, with the exception of LAK12 again.  To understand the reason why LAK12 had more partic- ipants and tweets, we specially consulted with the confer- ence organizers. The explanation was that a substantial amount of tweets during LAK12 was about the technologies adopted for live video streaming. Tweets were posted to illustrate how the streaming technologies could be used in such a context. As a result, folks who may not be interested in Learning Analytics per se but more in video streaming  and recording were attracted to the conversation.  Conference Tweets Retweets Replies lak11 1358 450 (33.1%) 230 (16.9%) lak12 4050 1207 (29.8%) 430 (10.6%) lak13 2223 570 (25.6%) 363 (16.3%) lak14 3105 1255 (40.4%) 570 (18.4%)  Table 2: Descriptive Analysis of Tweets at LAK  Descriptive analysis of Twitter participants is presented in Table 3. Overall, the number of tweets, retweets, and replies have been improving over the years, except for LAK12 prob- ably because of its broader participation. This result in- dicated growing participation and interactivity within the learning analytics community. The standard deviation of each measure also increased over the years, showing increas- ing disparity of participation. This could be partially ac- counted by the increase of participants over the years (see Table 1), most of whom were peripheral on Twitter discus- sion. Even though further tests failed to confirm a power law distribution on these measures, the distribution of them was extremely positively skewed.  Overall, descriptive analysis at the conference and partic- ipant levels indicated extending reach of the LAK commu- nity and increasing interactions among its members over the years.  Conf Tweets RTs Replies RT-ed Replied lak11 6.3 (14.1) 2.1 (4.7) 1.1 (3.5) 2.0 (7.5) 0.9 (3.4) lak12 6.7 (23.8) 2.0 (5.0) 0.7 (2.9) 1.9 (13.4) 0.6 (3.7) lak13 7.9 (31.5) 2.0 (4.5) 1.3 (7.4) 2.0 (7.7) 1.1 (4.2) lak14 8.6 (30.9) 3.5 (10.4) 1.6 (7.5) 3.5 (13.5) 1.5 (6.0)  Table 3: Means and Standard Deviations of Tweets, Retweets (RTs), Replies, Times Being Retweeted (RT-ed), and Times Being Replied for Participants  5.2 The Flow of the LAK Community To understand the composition of the LAK community as  reflected by Twitter participation, we tracked participation of all Twitter participants over the years, focusing on new and returning participants each year. The results are visu- alized as a Sankey diagram presented in Figure 1. In this diagram, the horizontal axis represents the time dimension, with each column representing one conference. The fifth (or last) column represents participants who stopped participat- ing at some point. Within each column, new and returning participants are presented separately. All lines are drawn from left to right, representing the flow of participants from one section to another between two columns; the width of a line denotes the volume of its flow. For example, for par- ticipants who participated in LAK11 and never returned, the line will be drawn from LAK11 directly to Leave in the last column; for those who participated in both LAK11 and LAK12, they are represented by the line from LAK11- New to LAK12-Back. Using this visualization, we can easily inspect the flow of Twitter participants across the years.  To our surprise, only a small fraction of participants have been returning to LAK conferences Twitter discussion, in- dicated by the thinner lines towards the Back sections of LAK12 to LAK14. By looking at the returning participants  343    Figure 1: Flow of participants from LAK11 to LAK14.  Participants LAK11 LAK12 LAK13 LAK14 sheilmcn 1 224 202 19 gsiemens 68 142 33 61 houshuang 106 1 10 115 sbskmi 52 51 19 58 dan suthers 2 56 99 6 dougclow 35 43 46 29 bodong c 6 2 12 90 dgasevic 7 66 9 9 R3beccaF 2 23 11 51 cab938 18 47 14 3 shaned07 5 40 19 13 mhawksey 5 23 26 13 abelardopardo 16 27 8 7 ErikDuval 8 24 21 1 aneesha 19 1 1 1 helinur 4 9 1 2 cteplovs 4 1 5 2 georgekroner 1 8 2 1  Table 4: Twitter Users Participating in All LAK Confer- ences, Sorted by the Total of Tweets  more closely, we found that only 18 colleagues (among all 1,217 unique participants) participated in Twitter discus- sion during all LAK conferences (see Table 4). This number only increased to 55 when we counted users participating at least three conferences. Overall, as indicated in Figure 2, the majority of participants only participated in one conference and never returned. Thus, although the learning analytics community is having a broader reach shown in the previous section, many participants remain peripheral.  In particular, the action of leaving was especially popular for new participants each year. As shown by the outbound- ing lines from New participants each year, most of them direct to the Leave category, indicating these new partic- ipants never participated again. In contrast, returning par- ticipants were much more likely to return, indicated by the more balanced divide between leaving and returning within participants in the Back category in each column.  Based on these observations, the LAK community seems to be fluid reflected by Twitter participation during its annual conferences. While it keeps attracting interested col- leagues from various domains, the community is still more  Figure 2: Twitter participants by the count of conferences.  or less unstable as an emerging field of research and practice.  5.3 Interaction Network Analysis Interaction networks were generated based on retweet, re-  ply, and mention actions. As shown in Table 5, the recip- rocate rates were increasing (except LAK12), implying in- creased interactions among the participants. The increased average degree, decreased average path length and network diameter, and overall increased percentage of nodes con- tained by the largest connected network component indi- cated that the network was becoming denser and more con- nected.  In the network graphs in Figure 3, the node size and color are based on betweenness centrality. Betweenness central- ity is a centrality measure of a node within a network. It denotes a nodes position within a network in terms of its ability to bridge the connection between other node pairs or groups in the network. Hence, the nodes with larger betweenness centrality are more influential and function as bridges connecting the community. As shown in the network visualizations, from LAK11 to LAK12, the network became much larger; however, due to the reasons explained earlier, a large portion of nodes in the LAK12 network are in periph- eral positions, only loosely connected through one influential figure. There are also less nodes in the center, implying the discussion was led by only a few key figures. In LAK13 and LAK14, an increasing number of nodes gain higher between- ness centrality and emerged to connect the community more tightly. Hence, when the LAK Twitter community had an increasing reach and interaction, a larger group of leaders (at least in terms of Twitter participation) have also been emerging.  5.4 Community Detection in the Retweet Net- works  Community detection in the retweet networks generated 3, 6, 5, 6 communities respectively for LAK to LAK14. The retweeting network graphs are visualized in Figure 4. In these graphs, different communities are coded in different colors and the different levels of influence of participants within a network, measured by betweenness centrality, is scaled to the node size.  Overall, except for LAK 12, the numbers of communi- ties increase steadily from 3 to 6 in LAK14, along with the  344    Conf Nodes Edges Reciprocate Rate % Avg. Degree Diameter Avg. Path Length Largest Component % lak11 215 569 13.1 2.65 7 2.95 90.70 lak12 606 1521 12.8 2.51 9 3.10 83.33 lak13 280 736 14.9 2.63 8 2.99 87.50 lak14 362 1369 19.5 3.78 6 2.73 91.71  Table 5: Interaction Network Characteristics  Figure 3: The largest connected interaction network com- ponents from LAK11 to LAK14. Note: Node size and color are based on betweenness centrality.  increasing numbers retweets from 450 to 1255. The com- plexity of the network structure was also enhanced. In par- ticular, because every detected community in the networks was usually dominated or centered on an influential figure, the increased numbers of communities means the emergence of new hubs in Twitter retweeting networks. This finding is consistent with the observations of emerging leaders in the previous section.  Second, we also found the connections among members inter or intra communities were not that extensive, except for more dominant participants. Given a community is nor- mally centered around a leading figure, the following situa- tion might often take place: a community leader generates a tweet, and then a leader from another community retweet it and further diffuse the idea to his or her entire community.  Furthermore, the sizes of various communities are more balanced in LAK13 and LAK14, reflecting more participants from various disciplines contributing to the LAK confer- ences. The connections within and between communities in LAK13 and LAK14 were significantly improved as well. More exchanges among different scholars can contribute to the development for long-term viability for the learning an- alytics community. These findings are also consistent with the interaction network analysis in the previous section.  5.5 Hashtag Analysis In the hashtag analysis, the hashtags #LAK11  #LAK14 and  #LearningAnalytics took dominant proportions and were therefore removed from the hashtag cloud visualizations. However, we also noticed that the dominance of #LAK1* be- came weaker over the years, implying the emergence of more  Conf Hashtags Freq #lak1* % #LearningAnalytics % lak11 113 1817 74.7 0 lak12 281 5820 69.5 24.0 lak13 177 2900 76.7 13.9 lak14 197 3890 71.0 19.6  Table 6: Descriptive Statistics of Hashtags  hashtags. This finding indicated that the research topics in the community have become more diverse, connected with an increasingly deepening inquiry in the community.  Interesting trends could be observed from the hashtag clouds in Figure 5. Some hashtags were popular in a specific year but eventually faded away later. For example, #edchat, #edtech20, and #edtools were very dominant in LAK11, but did not appear in the successive years. The fade of these more general hashtags and the rise of #LearningAnalytics (see Table 6) implied the formation of a collective commu- nity identity.  In addition, the popularity of some hashtags in a spe- cific year was related to promotion efforts of certain work- shops, projects or technologies. For instance, the hash- tags #elifocus and #EduLive relevant to the video stream- ing technologies were popular in LAK12, but did not ap- pear again in LAK13 and LAK14. #Linkedupproject and #plasma, which were related to two learning analytics projects, had some momentum in LAK13 and LAK14 respectively. #DCLA13 and #lakdata14 were respectively related to the Discourse-Centric Learning Analytics workshop in 2013 and the LAK data challenge in 2014.  Other than these promotion hashtags, a few hashtags emerged and persisted over the years. For example, data-related hashtags and #MOOC(s) appeared from the first year and represented long-standing interests within the community. #EDM, #DataMining, and #BigData started to become more pervasive in 2012, indicating the bridging between the learn- ing analytics and EDM communities. Though these terms may be overshadowed by some of the most promoted hash- tags, they were the most persistent topics in the community.  5.6 Topics Modeling of Twitter Discussion Going beyond hashtag analysis, we applied LDA to un-  cover underlying topics in Twitter discussion during LAK conferences. The harmonic mean of the log-likelihood per number of topics is plotted in Figure 6. The maximum is reached when the LDA model was trained with 34 topics. The 34-topic model that best accounted for the corpus was chosen accordingly.  LDAvis [39] was then used to help interpreting the topic model. Figure 7 to 9 illustrate three screenshots of LDAvis during our exploration of the topic space. To enable visual- izations of topics, LDAvis first projected the 34-dimensional space to 2D using multidimensional scaling. Each topic  345    (a) LAK11 (b) LAK12  (c) LAK13 (d) LAK14  Figure 4: Largest connected components in retweet networks.  (a) LAK11 (b) LAK12  (c) LAK13 (d) LAK14  Figure 5: Hashtag clouds. Note: Hashtags #LAK1* and #LearningAnalytics were removed for each year.  346    Figure 6: The harmonic mean of estimated log-likelihoods per number of topics. Note: The optimal number of topics is 34 when the maximum log-likelihoods is observed.  Figure 7: Red cluster is chosen, and corresponding top terms are displayed.  could then be represented by a circle in the 2D space. Us- ing certain distance calculation algorithms, LDAvis could further cluster topics based on their distance among each other. After some exploration, 5 clusters appeared to be the most interpretable choice. By clicking on each cluster in the left panel, the top terms corresponding to a cluster would be updated in the right panel (Figure 7). The red clus- ter corresponded to terms such as {conference, good, notes, work, slides, session} and appeared to be related to meta- informationof conferences; the green cluster featured terms including {like, best, great, interested, cool} and was related to positive sentiments about the conferences; the blue and yellow clusters, being associated with {data, big, mining, ed- ucational, challenge} and {student(s), social, research, use, course} respectively, were related to specific research topics; the violet cluster was linked to terms like {learning, ana- lytics, knowledge, paper(s), conf, journal} that were more general in the LAK context.  We further explored each individual topic, by clicking on its corresponding circle (Figure 8). The distribution of one specific term among topics could also be inspected, by hov- ering over the term in the right panel (Figure 9). Results indicated that the trained LDA model was meaningful, in that most topics were interpretable based on their terms and topics under a same cluster were semantically closer. Simi- lar to the findings from topic clusters, we found tweet topics generally fell into a few distinguishable categories, includ- ing (1) information-sharing related to conferences and the community, (2) experience-sharing and comments, and (3) more specific research topics (such as MOOC, assessment,  Figure 8: Topic 9 is chosen, top terms displayed.  Figure 9: Termlearningis chosen, relevant topics enlarged.  students, course design). Based on the exploration, we tagged each topic with its  pertinent terms and then focused on topics most relevant to learning analytics research and tracked their evolution over the years. In Figure 10, the change of eleven topics is illus- trated. The y-axis represents the percentage of a topic in a years tweets. As a validity check, two topics popular in LAK14 are included: Topic 10 {social use media win net- works share survey ipad} related to a promoted project and Topic 11 {graesser systems predictive agents model tutors} related to a keynote speech. These two topics both peeked in LAK14, showing a certain level of validity of LDA.  Further inspection of the topics identified an increasing emphasis on students, need, assessment, and feedback in the community, indicated by the growing popularity of Top- ics 1-4. In addition, each years conference presented unique hot topics: topics relevant to ethics and social media were relatively popular at LAK11; big data, linked data, and ed- ucational data mining were trending at LAK12; course de- sign, ethical issues, discourse, and measurement were pop- ular during LAK13; topics involving assessment, student needs, intelligent tutors, and educational data mining were rising at LAK14. Comparing with previous analysis of learn- ing analytics publications, the popularity of student-related topics appeared to be shared by LAK academic literature and tweets [46]; however, the evolution of topics in tweets tended to not agree with the analysis of publications [36].  6. CONCLUSIONS The present study built on previous research that set to  understand the field of learning analytics from a variety of angles. Using a unique set of Twitter data from previous LAK conferences, we aimed to uncover new insights about  347    Figure 10: Tracking changes of selected research topics.  the community combining multiple analysis attending to dif- ferent aspects of Twitter participation. Through descriptive analysis, interaction network analysis, hashtag analysis, and topic modeling, we found an extended reach of the com- munity and increasing interactions among its members; in- creasingly dense, connected, and balanced social networks; peripheral and in-persistent participations; and more and more diverse research topics. In particular, detailed inspec- tion of semantic topics identified a rising emphasis on stu- dents over the years as well as distinctive hot topics in each years conference.  We would like to mention a few limitations or potential risks of the present study. First, we were not able to test the comprehensiveness of the Twitter archive. Based on our experience with TAGS, some tweets could have got lost for various reasons (e.g., the time an archive was created). Sec- ond, because of the 140-character limit, tweets might be less suitable for in-depth semantic analysis. While most topics we identified were meaningful, some topics were hard to in- terpret. Third, since not all conference participants or com- munity members use Twitter, the analysis of tweets could only reconstruct parts of the dialogues and would run the risk of missing important messages, events, or figures. For future directions, we would like to connect tweets and aca- demic publications, to further construct a more integrated picture of the learning analytics community.  7. REFERENCES [1] C. Anderson. The long tail: Why the future of business  is selling less of more. Hyperion, 2008.  [2] C. Atkinson. The backchannel: how audiences are using Twitter and social media and changing presentations forever. New Riders, 2009.  [3] N. Balacheff and K. Lund. Multidisciplinarity vs. Multivocality, the Case of Learning Analytics. In Proceedings of the Third International Conference on Learning Analytics and Knowledge, LAK 13, pages 513, New York, NY, USA, 2013. ACM.  [4] D. M. Blei and J. D. Lafferty. Visualizing topics with multi-word expressions. arXiv preprint  arXiv:0907.1013, 2009.  [5] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allocation. The Journal of Machine Learning Research, 3:9931022, Mar. 2003.  [6] V. D. Blondel, J.-L. Guillaume, R. Lambiotte, and E. Lefebvre. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment, 2008(10):P10008, 2008.  [7] D. Boyd. Its Complicated: the social lives of networked teens. Yale University Press, 2014.  [8] W. Chamlertwat, P. Bhattarakosol, T. Rungkasiri, and C. Haruechaiyasak. Discovering Consumer Insight from Twitter via Sentiment Analysis. Journal of Universal Computer Science, 18(8):973992, 2012.  [9] B. Chen. Is the Backchannel Enabled Using Twitter at Academic Conferences. 2011 Annual Meeting of American Educational Research Association, 2011.  [10] X. Chen, M. Vorvoreanu, and K. Madhavan. Mining Social Media Data for Understanding Students Learning Experiences. IEEE Transactions On Learning Technologies, 7(3):246259, 2014.  [11] D. Davidov, O. Tsur, and A. Rappoport. Enhanced sentiment learning using twitter hashtags and smileys. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, pages 241249. Association for Computational Linguistics, 2010.  [12] N. A. Diakopoulos and D. A. Shamma. Characterizing debate performance via aggregated twitter sentiment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 11951198. ACM, 2010.  [13] M. Dork, D. Gruen, C. Williamson, and S. Carpendale. A visual backchannel for large-scale events. Visualization and Computer Graphics, IEEE Transactions on, 16(6):11291138, 2010.  [14] H. Drachsler, S. Dietze, E. Herder, M. dAquin, and D. Taibi, editors. Proceedings of the LAK Data Challenge 2014, held at LAK 2014, the 4th Conference on Learning Analytics and Knowledge (LAK2014),  348    CEUR Workshop Proceedings, Vol. 1137, 2014.  [15] M. Ebner, G. Beham, C. Costa, and W. Reinhardt. How people are using Twitter during conferences. Creativity and innovation Competencies on the Web, page 145, 2009.  [16] R. Ferguson. Learning analytics: drivers, developments and challenges. International Journal of Technology Enhanced Learning, 4(5):304317, 2012.  [17] G. Grosseck and C. Holotescu. Can we use Twitter for educational activities. In 4th international scientific conference, eLearning and software for education, Bucharest, Romania, 2008.  [18] A. Hermida. Twittering the news: The emergence of ambient journalism. Journalism Practice, 4(3):297308, 2010.  [19] C. Honey and S. C. Herring. Beyond microblogging: Conversation and collaboration via Twitter. In System Sciences, 2009. HICSS09. 42nd Hawaii International Conference on, pages 110. IEEE, 2009.  [20] Y. Hu, G. McKenzie, J.-A. Yang, S. Gao, A. Abdalla, and K. Janowicz. A Linked-Data-Driven Web Portal for Learning Analytics: Data Enrichment, Interactive Visualization, and Knowledge Discovery. In LAK Workshops, 2014.  [21] J. Huang, K. M. Thornton, and E. N. Efthimiadis. Conversational tagging in twitter. In Proceedings of the 21st ACM conference on Hypertext and hypermedia, pages 173178. ACM, 2010.  [22] M. Ito, S. Baumer, M. Bittanti, D. Boyd, R. Cody, B. Herr, H. Horst, P. Lange, D. Mahendran, K. Martinez, et al. Hanging out, messing around, geeking out: Living and learning with new media, 2009.  [23] L. Johnson, R. Smith, H. Willis, A. Levine, and K. Haywood. The 2011 horizon report. The New Media Consortium, Austin, Texas, 2011.  [24] R. Junco. Engaging Students through Social Media: Evidence-Based Practices for Use in Student Affairs. John Wiley & Sons, 2014.  [25] R. Junco, C. M. Elavsky, and G. Heiberger. Putting twitter to the test: Assessing outcomes for student collaboration, engagement and success. British Journal of Educational Technology, 44(2):273287, 2013.  [26] R. Junco, G. Heiberger, and E. Loken. The effect of Twitter on college student engagement and grades. Journal of Computer Assisted Learning, 27(2):119132, 2011.  [27] H. Kwak, C. Lee, H. Park, and S. Moon. What is Twitter, a social network or a news media In Proceedings of the 19th international conference on World wide web, pages 591600. ACM, 2010.  [28] D. L. Lasorsa, S. C. Lewis, and A. E. Holton. Normalizing Twitter: Journalism practice in an emerging communication space. Journalism Studies, 13(1):1936, 2012.  [29] J. Letierce, A. Passant, J. G. Breslin, and S. Decker. Using Twitter During an Academic Conference: The# iswc2009 Use-Case. In ICWSM, 2010.  [30] G. R. Lopes, L. A. P. P. Leme, B. P. Nunes, and M. A. Casanova. RecLAK: Analysis and Recommendation of Interlinking Datasets. In LAK Workshops, 2014.  [31] G. Lotan, E. Graeff, M. Ananny, D. Gaffney, I. Pearce, et al. The revolutions were tweeted: Information flows during the 2011 Tunisian and Egyptian revolutions. International Journal of Communication, 5:31, 2011.  [32] M. Ponweiser. Latent Dirichlet Allocation in R, 2012.  [33] P. Riehmann, M. Hanfler, and B. Froehlich. Interactive sankey diagrams. In Information Visualization, 2005. INFOVIS 2005. IEEE Symposium on, pages 233240. IEEE, 2005.  [34] C. Ross, M. Terras, C. Warwick, and A. Welsh. Pointless babble or enabled backchannel: conference use of twitter by digital humanists. Digital Humanities, 2010.  [35] M. Scheffel, K. Niemann, S. L. Rojas, H. Drachsler, and M. Specht. Spiral me to the core: Getting a visual grasp on text corpora through clusters and keywords. In LAK Workshops, 2014.  [36] M. Sharkey and M. Ansari. Deconstruct and Reconstruct: Using Topic Modeling on an Analytics Corpus. In LAK Workshops, 2014.  [37] G. Siemens. Learning analytics: envisioning a research discipline and a domain of practice. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge, pages 48. ACM, 2012.  [38] G. Siemens and R. S. d Baker. Learning analytics and educational data mining: towards communication and collaboration. In Proceedings of the 2nd international conference on learning analytics and knowledge, pages 252254. ACM, 2012.  [39] C. Sievert and K. Shirley. Ldavis: A method for visualizing and interpreting topics. In 2014 ACL Workshop on Interactive Language Learning, Visualization, and Interfaces, Baltimore, June 2014.  [40] B. G. Smith. Socially distributing public relations: Twitter Haiti, and interactivity in social media. Public Relations Review, 36(4):329335, nov 2010.  [41] B. Stone. Twitter As News-wire, July 2008.  [42] B. Suh, L. Hong, P. Pirolli, and E. H. Chi. Want to be Retweeted Large Scale Analytics on Factors Impacting Retweet in Twitter Network. In Social Computing (SocialCom), 2010 IEEE Second International Conference on, pages 177184, Aug 2010.  [43] D. Taibi and S. Dietze. Fostering analytics on learning analytics research: the LAK dataset. In CEUR WS Proceedings Vol. 974, Proceedings of the LAK Data Challenge, 2013.  [44] W. Xing, B. Wadholm, and S. Goggins. Learning analytics in CSCL with a focus on assessment: an exploratory study of activity theory-informed cluster analysis. In Proceedins of the Fourth International Conference on Learning Analytics And Knowledge, pages 5967. ACM, 2014.  [45] T. R. Zaman, R. Herbrich, J. Van Gael, and D. Stern. Predicting information spreading in twitter. In Workshop on Computational Social Science and the Wisdom of Crowds, NIPS, volume 104, pages 17599601. Citeseer, 2010.  [46] A. Zouaq, S. Joksimovic, and D. Gasevic. Ontology Learning to Analyze Research Trends in Learning Analytics Publications. In LAK (Data Challenge), 2013.  349      "}
{"index":{"_id":"56"}}
{"datatype":"inproceedings","key":"Dascalu:2015:DCS:2723576.2723578","author":"Dascalu, Mihai and Trausan-Matu, Stefan and Dessus, Philippe and McNamara, Danielle S.","title":"Discourse Cohesion: A Signature of Collaboration","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"350--354","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723578","doi":"10.1145/2723576.2723578","acmid":"2723578","publisher":"ACM","address":"New York, NY, USA","keywords":"cohesion, collaboration assessment, computer supported collaborative learning, discourse analysis, learning analytics","abstract":"As Computer Supported Collaborative Learning (CSCL) becomes increasingly adopted as an alternative to classic educational scenarios, we face an increasing need for automatic tools designed to support tutors in the time consuming process of analyzing conversations and interactions among students. Therefore, building upon a cohesion-based model of the discourse, we have validated ReaderBench, a system capable of evaluating collaboration based on a social knowledge-building perspective. Through the inter-twining of different participants' points of view, collaboration emerges and this process is reflected in the identified cohesive links between different speakers. Overall, the current experiments indicate that textual cohesion successfully detects collaboration between participants as ideas are shared and exchanged within an ongoing conversation.","pdf":"Discourse cohesion: A signature of collaboration  Mihai Dascalu,   Stefan Trausan-Matu  University Politehnica of Bucharest   313 Splaiul Indepententei  Bucharest, Romania   mihai.dascalu@cs.pub.ro  stefan.trausan@cs.pub.ro   Philippe Dessus     Univ. Grenoble Alpes, LSE,  F38000 Grenoble, France   philippe.dessus@upmf-grenoble.fr   Danielle S. McNamara     Arizona State University  Tempe, Arizona 85287-2111   dsmcnama@asu.edu         ABSTRACT  As Computer Supported Collaborative Learning (CSCL) becomes  increasingly adopted as an alternative to classic educational  scenarios, we face an increasing need for automatic tools designed  to support tutors in the time consuming process of analyzing  conversations and interactions among students. Therefore,  building upon a cohesion-based model of the discourse, we have  validated ReaderBench, a system capable of evaluating  collaboration based on a social knowledge-building perspective.  Through the inter-twining of different participants points of view,  collaboration emerges and this process is reflected in the  identified cohesive links between different speakers. Overall, the  current experiments indicate that textual cohesion successfully  detects collaboration between participants as ideas are shared and  exchanged within an ongoing conversation.   Categories and Subject Descriptors  I.2.7 [Natural Language Processing], K.3.1 [Computer Uses in  Education]   General Terms  Algorithms, Measurement   Keywords  Computer Supported Collaborative Learning, Cohesion,  Discourse Analysis, Learning Analytics, Collaboration  Assessment.   1. INTRODUCTION  Computer Supported Collaborative Learning (CSCL) has gained a  broader usage in multiple educational scenarios and has become a  viable alternative to classic learning environments and settings as  it can be employed in a multitude of activities, such as MOOCs or  collaborative serious games. At the same time, with its wider   adoption, the need for automated tools capable of supporting and  evaluating the corresponding actors has becomes more stringent  due to the fact that the analysis of conversations involving  multiple participants is a time consuming process. Trausan-Matu  [23], for example, reported that the time required for a thorough  analysis of a chat session greatly exceeds the actual duration of  the online conversation, rendering the manual evaluation process  virtually impossible for large corpora of conversations.   In a nutshell, collaboration can be perceived as a measure of  interaction among participants centered on sharing ideas, fostering  creativity for working in groups [24] and influencing others  points of view during the discussion. Therefore, our interest  consists of automatically assessing collaboration from CSCL text- based interactions among multiple participants performed during  specific educational scenarios.  From a more pragmatic perspective, our aim is to validate a  computational model for evaluating collaboration based on a  cohesion-based model of the discourse [6; 25]. This study  represents an extension of the initial model [6], which has now  been further validated within an educational setting. Therefore,  besides fine-tuning the automatic assessment process, validation  has now been performed on a larger corpus manually annotated by  considerably more evaluators. As an overview, we perform a  longitudinal analysis of an ongoing conversation, following its  timeline and relying on cohesion to model the knowledge  transferred or constructed among participants. In other words,  CSCL outlines a socio-cultural paradigm focused on the idea that  new knowledge is created collaboratively through the process of  social knowledge building [2; 20; 22].   In contrast to the most simplistic models of assessing  collaboration which rely on counting the number of utterances  exchanged between or addressed to different speakers, our model  supports the notion that cohesion is a salient predictor of  collaboration. Therefore, by modeling the interactions between  participants through textual cohesion, signatures of collaboration  emerge. It is common for tutors to attempt to detect breaks in  conversations that have limited or no collaboration or intense  collaboration zones in learners productions. Automatic methods,  such as ReaderBench [5], will provide crucial support to tutors in  extracting such zones.   The following section is centered on the concept of cohesion, as  well as underlying computational approaches for analyzing  students productions. The third section briefly introduces the  mechanism of scoring utterances or contributions, while the fourth  section presents the collaboration assessment model. Then we  shift the point of interest towards the validation of the proposed  model, followed by discussions and conclusions.      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA Copyright 2015  ACM 978-1-4503-3417-4/15/03...$15.00  http://dx.doi.org/10.1145/2723576.2723578   350    2. DISCOURSE COHESION  The concept of cohesion was introduced by Halliday and Hasan  [9] in terms of the cohesive ties perceived as relations of  meaning that exist within the text, and that define it as a text. [9].  Cohesion provides overall unity and is used for establishing the  underlying structure of meaning. Therefore, cohesion addresses  the connections in a text based on features that highlight relations  between constituent elements (words, sentences or blocks of text).  In other words, text cohesion can be perceived as the sum of  lexical, grammatical, and semantic relations that link together  textual units.  From a computational viewpoint, cohesion is reflected in the  linguistic form of discourse [16] and is often regarded as an  indicator of its structure. More specifically, cohesion can derive  from: a) discourse connectedness reflected as relations between  sentences (e.g., explanation, contrast) through cue words or  phrases (e.g., but, because) [15]; b) referencing expressions that  reflect the status of an entity in the discourse and can be identified  through co-reference resolution [11; 15; 18]; c) lexical or  semantic similarity of words obtained from semantic distances in  ontologies [4], cosine similarity applied on vector spaces from  Latent Semantic Analysis [13], or topic relatedness measured as  Jensen-Shannon divergence in Latent Dirichlet Allocation [3].   Within our implemented model, cohesion is determined as the  product between the inverse normalized distance between textual  elements and an average semantic similarity measure composed  of: a) lexical proximity that is easily identifiable through identical  lemmas and semantic distances [4] within ontologies (WordNet  [17] or, for French language, a transposed and serialized version  of Wordnet Libre du Franais (WOLF) [19]); and b) semantic  similarity measured through Latent Semantic Analysis (LSA) [13]  and Latent Dirichlet Allocation (LDA) [3]. Additionally, specific  natural language processing techniques [14] are applied to reduce  noise and to improve the systems accuracy: spell-checking,  tokenization, splitting, part of speech tagging, parsing, stop words  elimination, dictionary-only words selection, stemming,  lemmatization, named entity recognition, and co-reference  resolution [18].   LSA and LDA models were trained using three specific corpora:  TextEnfants [7] (approx. 4.2M words), Le Monde (French  newspaper, approx. 24M words) for French, and Touchstone  Applied Science Associates (TASA) corpus (approx. 13M words)  for English. Moreover, improvements have been enforced on the  initial models: the reduction of inflected forms to their lemmas,  the annotation of each word with its corresponding part of speech  through a NLP processing pipe, and the adjustment of occurrences  through the use of term frequency-inverse document frequency  (Tf-Idf) [14].  Our previous experiments [5] have shown that Wu-Palmer  ontology-based semantic similarity [27] combined with LSA and  LDA models can be used to complement each other, in the sense  that underlying semantic relationships are more likely to be  identified if multiple complementary approaches are combined  after normalization, reducing the errors that can be induced by  using a single method. Overall, in order to better evaluate  cohesion between textual fragments, we have combined  information retrieval specific techniques, mostly reflected in word  repetitions and normalized number of occurrences, with semantic  distances extracted from ontologies or from LSA-based or LDA- based semantic models. Based on the previous cohesion function,  we define a cohesive link as a connection between two textual  elements that has a high value for cohesion. In the actual   implementation, the mean value of all semantic similarities is  considered the threshold for detecting cohesive links.   In the end, in order to have a better representation of discourse in  terms of underlying cohesive links, we represent these links as a  cohesion graph [6; 25], which can be perceived as a generalization  of the previously proposed utterance graph [26].   3. UTTERANCE SCORING  Assessing collaboration proves to be challenging, particularly  when our aim consists of finding an approximation that best  reflects the importance or the value of a collaborative act.  Nevertheless, the evaluation process must be performed in two  steps. First, the local importance of each utterance must be  determined with regards to the entire conversation. Second,  collaboration is approximated as the impact of each utterance on  another participants discourse through cohesive links. This  section is centered on evaluating each utterance and assigning it  an importance score, whereas the next section details the process  of automatically assessing collaboration.   In order to evaluate the importance of each utterance, we must  first determine the value of its constituents or, more specifically,  the relevance of each contained word. With regards to the process  of evaluating each words relevance in relation to its  corresponding textual fragment (e.g., sentence, utterance, or entire  conversation), there are several classes of factors that play an  important role in the final analysis (see Table 1).   Table 1. Factors used to measure a words relevance   Class Descriptors  Statistical presence Normalized term frequency used to   reflect the specificity of each single text    Semantic relatedness Semantic similarity to the analysis  element (sentence, utterance, entire  conversation)   Semantic coverage The importance of the semantic chain  containing a particular word and its span  throughout the entire conversation      Out of the three classes, the most straightforward factor consists  of computing the statistical presence of each word. The next class  is focused on determining the semantic relatedness between a  word and its corresponding textual fragment, whereas the last  class evaluates the semantic coverage of each concept.  Semantically related words are grouped together in lexical chains  by using an adaptations of the algorithm proposed by Galley and  McKeown [8] that relies solely on semantic distances from  WordNet [4]. The previous lexical chains are then merged into  semantic chains by using the semantic similarities between the  chains expressed in terms of LSA and LDA models [5]. Semantic  coverage is reflected in the length and the span of these semantic  chains as this provides a reliable global estimator for the  importance of each concept with regards to the entire  conversation. Based on the previous classes of factors, the  keywords of the conversation are determined as the words with  the highest cumulative relevance based on their individual  occurrences.   In terms of the scoring model, each utterance is initially assigned  an individual score equal to the normalized term frequency of  each word multiplied by its previously determined relevance [5].  Putting it differently, we measure to what extent each utterance   351    conveys the main concepts of the overall conversation, as an  estimation of on-topic relevance. Afterwards, these individual  scores are augmented through cohesive links to other inter-linked  textual elements by using the previously defined cohesion values  as weights. Overall, we can state that keywords are used to reflect  the local importance of each word, whereas cohesive links are  used to transpose the local relevance upon other inter-linked  elements.   4. AUTOMATIC COLLABORATION  ASSESSMENT  The actual information transfer through cohesive links from the  cohesion graph can be split into a personal and a social  knowledge-building (KB) process [2; 20; 22] taking place within  each participants utterances. First, a personal dimension emerges  by considering utterances from the same speaker as a continuation  of ones discourse. Second, utterances exchanged with different  speakers encompass collaboration and sustain social interaction.  Therefore, each utterance now has its previously defined  importance score and a knowledge-building effect, both personal  and social. By considering all cohesive links, the values for  personal and social knowledge building are correspondingly  increased: if the link is between utterances having the same  speaker, the previous KB index (sum of personal and social KB)  is transferred to the personal dimension of the current utterance;  otherwise, if the pair of utterances is between different  participants, the social knowledge-building dimension of the  currently analyzed utterance is increased with the same amount of  information (KB index multiplied by the cohesion function). In  other words, continuation of ideas or explicitly referencing  utterances of the same speaker builds a personal knowledge- building effect, whereas the social perspective measures the  interaction with other participants.  Within the conducted experiments, collaboration emerges from: a)  the active participation of all members, b) the sharing of ideas and  points of view, c) the dense inter-twining of utterances whose  reference IDs are manually annotated by the speakers through an  explicit referencing facility available from the conversation  environment  ConcertChat [10]   Our automated text analysis tool, ReaderBench [5], highlights the  contribution of utterances through new layers of granularity added  to the overall collaboration analysis. By new layers, we refer to  the estimation of both personal and social knowledge-building  effects, as well as the identification of intense collaboration zones   intervals of utterances in which participants are actively  involved, collaborate, and generate new ideas related to the  ongoing context of the discussion. From a computational  perspective, we used a greedy algorithm [6] in order to build up  intense collaboration zones by expanding maximum local values  or social knowledge-building peaks. Each local maximum is  expanded sideways and, in the end, only zones above a minimum  spread of 5 utterances are selected as intense collaboration zones.   5. VALIDATION OF COLLABORATION  ASSESSMENT  The validation experiments focused on the assessment of 10 chat  conversations that took place in an academic environment in  which Computer Science students from the 4th year undergoing a  Human-Computer Interaction course in Romania debated on the  advantages and disadvantages of CSCL technologies. According  to the script presented in the previous section, each conversation  involved 4 to 5 participants who each had to argue for a given  technology (e.g., chat, blog, wiki, forum, or Google Wave) in   specific usage scenarios during the first phase of the discussion,  and then subsequently propose an integrated alternative that  encompassed the previously presented advantages. The 10 chats  were manually selected from a larger corpus of over 100  conversations. The chat conversations were manually annotated  by 76 4th year undergraduate students following the same course,  but from a different class, and 34 freshman master students  attending the Adaptive and Collaborative Systems course. Each  student annotated 3 chat conversations while addressing the  following tasks: a) grading each speaker on a 1 to10 scale in terms  of collaboration or exchange of ideas with other participants, and  b) identifying intense collaboration zones as segments of the  conversations in which multiple participants contribute and  actively participate in the ongoing discourse with on-topic and  relevant utterances.  A significant amount of time is necessary for a rater to glean a  deep understanding of a conversation (1.5 to 4 hours on average).  Hence, we opted to distribute the evaluation of each conversation  to multiple raters [23]. This approach resulted in an average of 33  annotations per conversation.   First, we validated the machine versus human agreement by  computing intra-class correlations between raters for each chat  and, second, as these correlations were all very high indicating  very few disagreements between raters, non-parametric  correlations (Spearmans Rho) were calculated between machine  and human mean ratings for each chat (see Table 2). The low  number of participants per chat was also a determinant factor for  the previous high values, as the comparisons between shorter  numerical series tend to have more extreme values, resulting in  either highly positive or negative correlations.   As an interpretation of the results presented in Table 2, we can  observe that the non-parametric correlations were high except for  chats 3 and 8. In the latter conversations, we were able to identify  atypical behaviors that justify these discrepancies: a) dominance  of the conversation by certain participants at given moments  throughout the conversation; b) existence of wide-spread  segments in which multiple participants seemed to get involved in  a similar degree, rendering the differentiation among them more  difficult; c) disequilibrium of the conversation due to the focus on  only one technology (blog in both conversations) which shifted  the overall balance with regards to the other technologies that  should have been debated.   With regards to the identification of intense collaboration zones,  all manual annotations were cumulated within a histogram, which  presented for each utterance the number of raters that considered  it to be part of an intense collaboration. Afterwards, the same  greedy algorithm was applied on this histogram in order to obtain  an aggregated version that reflected the intense collaboration  zones emerging as an overlap of all annotations (see Table 2).   Moreover, as presented in Table 2, there is a good overlap in  terms of accuracy (measured as precision, recall and F1 score)  between the annotated collaboration zones and those that were  automatically identified. This indicates that the model is a good  estimator of the annotated zones.   However, the rather low correlations with an average of .34 in  terms of collaboration evolution are justifiable as the scales are  completely different. On the one hand, we have the number of  inclusions of each utterance within manually identified intense  collaboration zones. This was a subjective and bias-prone task as  there were no constraints imposed in terms of the overall coverage  of these zones and the raters perception of interaction among  multiple participants. Additionally, this is a quantitatively   352    cumulative score obtained from the overlap of zones from  multiple raters which, in essence, is a transversal sum of  occurrences. On the other hand, the system provides an estimation  of social knowledge building based on the social interaction   modeled through cohesive links. The latter estimation reflects a  qualitative process following the timeline spanning the  conversation and therefore a longitudinal effect in contrast to the  previous transversal sum.     Table 2. Intense collaboration zones extracted from manual versus automatically identified annotations   Conver- sation   No.  utter- ances   ICC Rho Zones extracted from manual annotations Zones automatically identified P R  F1   score r   Chat 1 339 .97 1.0** [37;128], [167;298] [16;130], [154;228], [238;279],  [291;298], [307;321], [334;345]   .76 .91 .83 .27   Chat 2 283 .82 .80 [13;49], [68;131], [144;169],  [193;207], [229;236], [245;266]   [10;47], [55;79], [88;112],  [128;237], [246;275]    .64 .85 .73 .31   Chat 3 405 .73 .30 [36;315] [15;353], [397;401], [364;373] .79 1.0 .89 .43   Chat 4 251 .91 .70 [19;148], [184;194], [203;212] [27;131], [188;209], [220;259]  .73 .79 .76 .44   Chat 5 416 .96 .90* [28;98], [120;265], [280;287],  [346;362]   [16;285], [308;314], [331;407] .68 .99 .81 .23   Chat 6 378 .96 .98** [64;227], [248;308], [321;359] [47;331], [344;371] .80 .95 .87 .46   Chat 7 270 .91 .70 [40;97], [108;128], [145;220],  [232;257]   [14;126], [137;213], [221;236],  [255;265]   .73 .85 .78 .21   Chat 8 389 .92 .40 [30;127], [140;154], [189;208],  [235;285], [299;356]   [14;43], [59;172], [193;208],  [238;250], [260;384]   .71 .87 .78 .31   Chat 9 190 .97 .80 [51;65], [80;190] [48;67], [84;121], [127;184],  [190;195]   .91 .86 .89 .56   Chat 10 297 .86 .80 [27;75], [89;104], [139;287] [18;152], [167;182], [190;221],  [234;243], [253;257], [267;305]   .69 .76 .72 .21   Average 321.8 .90 .74  .74 .88 .81 .34   Note: *p < .05; **p < .01   6. Discussion  Based on the results presented in Table 2 and highly related to the  process of modeling social knowledge building, we can consider  cohesion as a binder between the utterances within an intense  collaboration zone. Cohesion measures the topic relatedness  between the utterances, whereas social interaction in a cohesive  context determines collaboration. In other words, cohesion among  utterances of different speakers becomes a signature of  collaboration.  Nevertheless, we must highlight certain limitations of our model.  Foremost, the model addresses only specific educational situations  in which participants share, continue, debate, or argue certain  topics or key concepts of the conversation. In other words,  collaboration is particularly derived from idea sharing between  participants who exchange cohesive utterances. It becomes  obvious that specific discourse markers or speech acts (e.g.,  confirmations or negations) [1; 21] should be also considered for  modeling collaboration, but for our specific educational scenario  presented in detail in section five, cohesion by itself proved to be  a reliable predictor. As the students debated on specific topics,  textual cohesion highlighting the exchange or continuation of  ideas represented a reliable estimator of the generated  collaborative effect.   Overall, the presented model should not be perceived as a rigid  structure, but as an adaptable one that evolves based on the  cohesion to other participants utterances. Nevertheless, we must  highlight additional limitations in terms of personal knowledge   building, social knowledge transfer, actual noise of the  experiment, and underlying cognitive processes. As an initial  assumption, we consider personal knowledge building as the  reflection of ones thoughts continued into subsequent utterances  through cohesive links. This is only partially true because the  underlying cognitive processes can be more elaborate than the  written form expressed within the conversation.   7. CONCLUSIONS  Starting from a dialogic model of discourse represented through  cohesive links, we validated our system in terms of analyzing  collaboration by employing an assessment model based on social  knowledge building. This demonstrated that the microstructure  level connectedness reflected in cohesion is a building block for  achieving a truly collaborative discourse.   Overall, collaboration was assessed using a bottom-up approach.  Initially, the importance of an utterance was measured with  regards to the overall discourse, it was assigned a corresponding  score, and afterwards the impact on other utterances of different  speakers was determined. In other words, within each intense  collaboration zone, there are multiple utterances that are cohesive  one to another and whose local importance scores are used to the  approximate the collaborative effect. This approach involves a  twofold estimation that uses an approximation of each utterances  importance and considers that the transferred information between  different participants as a measure for collaboration.   353    Based on this analysis, we can extend the perspective of  collaboration in terms of achieving a coherent representation of  the discourse through the inter-animation of participants points of  view. Therefore, starting from dialogism as a framework of CSCL  [12], our aim is to employ methods specific to computational  linguistics in order to model the exchange and sharing of ideas  among participants in a conversation.   Importantly, our analyses have a broad spectrum of applications,  extending from the initial evaluation based on cohesion between  utterances towards group cohesion achieved through  collaboration.   8. ACKNOWLEDGEMENTS  We would like to thank the students of University Politehnica of  Bucharest who participated in our experiments. This research was  partially supported by the LTfLL FP7 project, by the Sectoral  Operational Programme Human Resources Development 2007- 2013 of the Ministry of European Funds through the Financial  Agreement POSDRU/159/1.5/S/134398 and by NSF grants  1417997 and 1418378 to Arizona State University.   9. REFERENCES  [1] Austin, J.L., 1962. How to Do Things With Words. Harvard   University Press, Cambridge, MA.   [2] Bereiter, C., 2002. Education and mind in the knowledge  age. Lawrence Erlbaum Associates, Mahwah, NJ.   [3] Blei, D.M., Ng, A.Y., and Jordan, M.I., 2003. Latent  Dirichlet Allocation. Journal of Machine Learning Research  3, 4-5, 9931022.   [4] Budanitsky, A. and Hirst, G., 2006. Evaluating WordNet- based Measures of Lexical Semantic Relatedness.  Computational Linguistics 32, 1, 1347.   [5] Dascalu, M., 2014. Analyzing discourse and text complexity  for learning and collaborating, Studies in Computational  Intelligence. Springer, Switzerland.   [6] Dascalu, M., Trausan-Matu, S., and Dessus, P., 2013.  Cohesion-based analysis of CSCL conversations: Holistic  and individual perspectives. In CSCL 2013, N. Rummel, M.  Kapur, M. Nathan and S. Puntambekar Eds. ISLS, Madison,  USA, 145152.   [7] Denhire, G., Lemaire, B., Bellissens, C., and Jhean-Larose,  S., 2007. A semantic space for modeling children's semantic  memory. In Handbook of Latent Semantic Analysis, T.K.  Landauer, D.S. McNamara, S. Dennis and W. Kintsch Eds.  Erlbaum, Mahwah, 143165.   [8] Galley, M. and Mckeown, K., 2003. Improving word sense  disambiguation in lexical chaining. In IJCAI03, G. Gottlob  and T. Walsh Eds. Morgan Kaufmann Publishers, Inc.,  Acapulco, Mexico, 14861488.   [9] Halliday, M.a.K. and Hasan, R., 1976. Cohesion In English.  Longman, London.   [10] Holmer, T., Kienle, A., and Wessner, M., 2006. Explicit  Referencing in Learning Chats: Needs and Acceptance. In  EC-TEL 2006, W. Nejdl and K. Tochtermann Eds. Springer,  Crete, Greece, 170 184.   [11] Jurafsky, D. and Martin, J.H., 2009. An introduction to  Natural Language Processing. Computational linguistics,  and speech recognition. Pearson Prentice Hall, London.   [12] Koschmann, T., 1999. Toward a dialogic theory of learning:  Bakhtin's contribution to understanding learning in settings  of collaboration. In CSCL'99, C.M. Hoadley and J. Roschelle  Eds. ISLS, Palo Alto, 308313.   [13] Landauer, T.K. and Dumais, S.T., 1997. A solution to Plato's  problem: the Latent Semantic Analysis theory of acquisition,  induction and representation of knowledge. Psychological  Review 104, 2, 211240.   [14] Manning, C.D. and Schtze, H., 1999. Foundations of  statistical Natural Language Processing. MIT Press,  Cambridge, MA.   [15] McNamara, D.S., Graesser, A.C., Mccarthy, P., and Cai, Z.,  2014. Automated evaluation of text and discourse with Coh- Metrix. Cambridge University Press, Cambridge.   [16] McNamara, D.S., Louwerse, M.M., Mccarthy, P.M., and  Graesser, A.C., 2010. Coh-Metrix: Capturing linguistic  features of cohesion. Discourse Processes 47, 4, 292330.   [17] Miller, G.A., 1995. WordNet: A lexical database for English.  Communications of the ACM 38, 11, 3941.   [18] Raghunathan, K., Lee, H., Rangarajan, S., Chambers, N.,  Surdeanu, M., Jurafsky, D., and Manning, C.D., 2010. A  Multi-Pass Sieve for Coreference Resolution. In EMNLP '10  ACL, Cambridge, MA, 492501.   [19] Sagot, B. and Darja, F., 2008. Building a free French  wordnet from multilingual resources. In Ontolex 2008,  Marrakech, Maroc, 6.   [20] Scardamalia, M., 2002. Collective cognitive responsibility  for the advancement of knowledge. In Liberal Education in a  Knowledge Society, B. Smith and C. Bereiter Eds. Open  Court Publishing, Chicago, 6798.   [21] Searle, J., 1969. Speech Acts: An Essay in the Philosophy of  Language. Cambridge University Press, Cambridge, UK.   [22] Stahl, G., 2006. Group cognition. Computer support for  building collaborative knowledge. MIT Press, Cambridge,  MA.   [23] Trausan-Matu, S., 2010. Automatic Support for the Analysis  of Online Collaborative Learning Chat Conversations. In 3rd  Int. Conf. on Hybrid Learning, P.M. Tsang, S.K.S. Cheung,  V.S.K. Lee and R. Huang Eds. Springer, Beijing, China,  383394.   [24] Trausan-Matu, S., 2010. Computer support for creativity in  small groups using chats. Annals of the Academy of  Romanian Scientists, Series on Science and Technology of  Information 3, 2, 8190.   [25] Trausan-Matu, S., Dascalu, M., and Dessus, P., 2012.  Textual complexity and discourse structure in Computer- Supported Collaborative Learning. In ITS 2012, S.A. Cerri,  W.J. Clancey, G. Papadourakis and K. Panourgia Eds.  Springer, Chania, Grece, 352357.   [26] Trausan-Matu, S., Stahl, G., and Sarmiento, J., 2007.  Supporting polyphonic collaborative learning. Indiana  University Press, E-service Journal 6, 1, 5874.   [27] Wu, Z. and Palmer, M., 1994. Verb semantics and lexical  selection. In ACL '94 ACL, New Mexico, USA, 133138.   354      "}
{"index":{"_id":"57"}}
{"datatype":"inproceedings","key":"Simsek:2015:CAR:2723576.2723603","author":"Simsek, Duygu and S'andor, 'Agnes and Shum, Simon Buckingham and Ferguson, Rebecca and De Liddo, Anna and Whitelock, Denise","title":"Correlations Between Automated Rhetorical Analysis and Tutors' Grades on Student Essays","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"355--359","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723603","doi":"10.1145/2723576.2723603","acmid":"2723603","publisher":"ACM","address":"New York, NY, USA","keywords":"academic writing, academic writing analytics, argumentation, learning analytics, metadiscourse, natural language processing, rhetorical parsing, writing analytics","abstract":"When assessing student essays, educators look for the students' ability to present and pursue well-reasoned and strong arguments. Such scholarly argumentation is often articulated by rhetorical metadiscourse. Educators will be necessarily examining metadiscourse in students' writing as signals of the intellectual moves that make their reasoning visible. Therefore students and educators could benefit from available powerful automated textual analysis that is able to detect rhetorical metadiscourse. However, there is a need to validate such technologies in higher education contexts, since they were originally developed in non-educational applications. This paper describes an evaluation study of a particular language analysis tool, the Xerox Incremental Parser (XIP), on undergraduate social science student essays, using the mark awarded as a measure of the quality of the writing. As part of this exploration, the study presented in this paper seeks to assess the quality of the XIP through correlational studies and multiple regression analysis.","pdf":"Correlations between Automated Rhetorical Analysis  and Tutors Grades on Student Essays   Duygu Simsek1, gnes Sndor2, Simon Buckingham Shum3,   Rebecca Ferguson4, Anna De Liddo1, Denise  Whitelock4   1Knowledge Media Institute  4Institute of Educational Technology  The Open University, Walton Hall,   Milton Keynes, MK7 6AA, UK     {firstname.lastname}  @open.ac.uk   2Parsing & Semantics Group Xerox  Research Centre Europe 6 chemin   Maupertuis, F-38240 Meylan, France     agnes.sandor  @xrce.xerox.com   3Connected Intelligence Centre  University of Technology, Sydney,  Broadway NSW 2007, Australia     Simon.BuckinghamShum   @uts.edu.au     ABSTRACT  When assessing student essays, educators look for the students  ability to present and pursue well-reasoned and strong arguments.  Such scholarly argumentation is often articulated by rhetorical  metadiscourse. Educators will be necessarily examining  metadiscourse in students writing as signals of the intellectual  moves that make their reasoning visible. Therefore students and  educators could benefit from available powerful automated  textual analysis that is able to detect rhetorical metadiscourse.  However, there is a need to validate such technologies in higher  education contexts, since they were originally developed in non- educational applications. This paper describes an evaluation study  of a particular language analysis tool, the Xerox Incremental  Parser (XIP), on undergraduate social science student essays,  using the mark awarded as a measure of the quality of the writing.  As part of this exploration, the study presented in this paper seeks  to assess the quality of the XIP through correlational studies and  multiple regression analysis.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in  Education   General Terms  Measurement, Design, Human Factors, Theory.   Keywords  learning analytics, writing analytics, academic writing, academic  writing analytics, natural language processing, rhetorical  parsing, metadiscourse, argumentation.   1. INTRODUCTION  In order to be regarded as academically literate, students in higher  education need to engage with the ideas in the literature by   recognising when significant claims are being made in articles,  and by demonstrating the ability to examine them critically. One  of the key requirements of academic writing in higher education  is that students must develop a critical mind, make their thinking  visible, and learn to construct sound arguments in their discipline  [1].   Educators expect their students to learn to write in an  academically sound way; specifically, learn to make knowledge-  level moves and claims in their essays by recognising, and  deploying, scholarly rhetoric, which is often articulated by  metadiscourse. This term refers to the features of text that convey  the authors intended meaning and intention [2]. Metadiscourse  provides linguistic cues that explicitly organise the discourse,  express a viewpoint, argument and claim, engage the readers, and  signal the writer's stance [2]. For example, in Figure 1 the  italicised words are the elements of metadiscourse used by the  author to signal the rhetorical function of the sentences as  summary.     Figure 1. Metadiscourse that conveys summary statement   When assessing their students writing therefore, educators will,  among other features, be looking for scholarly metadiscourse as  an indicator of argumentation. From a discourse-centric learning  analytics perspective, a significant development in Natural  Language Processing (NLP) is the automatic recognition of the  rhetorical signals that authors use in research articles when  making a significant scholarly move. Such powerful  computational language technologies for extracting metadiscourse  are becoming available; but since they are originally developed in  non-educational contexts, there is a need to validate them in a  higher education framework.  This paper describes the first study undertaken to assess the  validity of a language technology, the Xerox Incremental Parser  (XIP), on undergraduate social science student writing.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org. LAK15, March 1620, 2015, Poughkeepsie, NY,  USA Copyright 2015 ACM 978-1-4503-3417-4/15/03$15.00  http://dx.doi.org/10.1145/2723576.2723603   355  mailto:Permissions@acm.org http://dx.doi.org/10.1145/2723576.2723603   2. RELATED WORK  This section provides an overview of the relevant aspects of the  three research areas related to our study: teaching academic  writing, automated rhetorical analysis and learning analytics.   2.1 Teaching Academic Writing  The main purpose of an academic author is to convince readers of  the validity of the claims put forward [3]. Consequently, as  philosophers of science and learning science researchers have  argued, rhetoric serves important functions within an argument,  by both engaging readers with the claims that are being made and  signaling their epistemic status [4].   However, educational research literature shows that students and  educators have contrasting views regarding the expectations and  interpretations of written assignments [5-8]. There is a general  consensus among educators in all disciplines that the key element  of student writing is argument [5, 8], whereas students think  that the presentation of content and knowledge is the most  important element, and that argumentation is one of the least  important assessment factors for their educators [8, 9]. Research  studies also show that high-scoring essays tend to be richer in  argumentation, whereas low-scoring essays are more focused on  factual descriptive information, and are poor in argumentation  [8,10]. This indicates that educators value rhetoric and  argumentation within student essays and look for their students  ability to present and pursue well-reasoned, and strong arguments.   Textbooks and guides for scholarly style and argumentation are  abundant; yet the acquisition of the skills remains a difficult task.  New experimental methods could benefit from language  technology tools that have been developed for the analysis of  scholarly language.   2.2 Automated Rhetorical Analysis  There exist today some natural language processing systems that  detect authors rhetorical moves in scholarly texts. One approach  is argumentative zoning [11], which assigns a rhetorical move  label to every sentence of the article. Another approach is concept  matching [12], which assigns rhetorical move labels to  rhetorically salient sentences only. Automated rhetorical analysis  could be used both in evaluation, and as a self-teaching tool   for    students,   who   could   inspect   the   rhetoric   and  argumentation of their own writing through the use of a rhetorical  analyser.   In our experiment we used the concept matching framework,  since it has the potential to focus students and educators  attention to salient sentences conveying specific rhetorical moves  related to argumentation around research problems. The  framework is implemented as the rhetorical module of the Xerox  Incremental Parser (XIP) [13]. It detects and labels rhetorically  salient sentences in scholarly writing based on the identification  of metadiscourse conveying the authors rhetorical strategy. The  labels are the following: SUMMARY (summarising the goals or  results of the article), EMPHASIS (emphasising the importance of  ideas), BACKGROUND (describing background knowledge  necessary for understanding the articles contribution),  CONTRAST (describing tensions, contrasts between ideas,  models or research directions), NOVELTY (conveying that an  idea is new), TENDENCY (describing emerging research  directions), and OPEN QUESTIONS (describing problems that  have not been solved) [14].       XIP is a candidate parser for evaluating and teaching academic  metadiscourse, if it can be embedded in a more complete  learning analytics approach.   2.3 Learning Analytics  Analysing written texts manually is a labour-intensive process,  which is an increasing problem as massive-scale learning takes  place online. Therefore, academic writing analytics research is  burgeoning especially in the field of automated analysis of  student writing [15].   Learning analytics offer the potential for automated, timely, and  formative feedback. But although computational rhetorical  parsing technology has been developed to analyse academic  writing, it is barely deployed in educational contexts. Since this  study will be the first step towards validating rhetorical parsing in  an educational context, it is too early to propose employing this  technology for summative assessment.   3. STUDY  We currently undertake a broad research project that seeks to  explore the possibilities of applying the XIP rhetorical parser in  an educational tool [16-18]. As part of this exploration this study  seeks to investigate to what extent XIP is accurate and sufficient  for detecting good academic writing in students essays given the  teachers grade as an evaluation measure. We ask the following  research questions: Is there a correlation between the salient  sentences extracted by XIP and final grades What are the  rhetorical markers out of the salient sentences detected by XIP  that are most promising as indicators of good academic writing in  students essay How accurate is the XIP output   3.1 Dataset  The student writing used in this study was from one of the final  year undergraduate education and arts modules of the UK based  distance education university, The Open University (OU). In the  EA300 Childrens Literature module, students studied key  examples of novels, picture books, poems and creative  performance produced for children aged between 3 and 18.  Students read a selection of related critical material and consider  major themes, issues and debates in the field. At the end of the  module, students wrote 3000 word long essays for an assignment  that required them to engage in depth with texts and approaches  explored within the module.    1307 students submitted an essay, which were marked out of 100  by an Associate Lecturer (tutor). All tutors used the same  marking grid with guidelines provided by the module team.   Students were assessed, in part, on their ability to think through  the strengths and limitations of the materials they used, and to  express this critical thinking clearly in their writing.   Figure 2. A sample XIP analysis   356    The marking grid prompted the tutors to consider six aspects two  of which are especially in line with the metadiscourse XIP can  identify: approach to alternative explanations and arguments and  construction of academic argument. These aspects evaluate to  what extent students are engaged critically with ideas coming  from different sources, discuss alternative explanations, and  produce well structured, coherent and persuasive arguments.   3.2 Methodology  XIP was used to analyse the 1307 student essays. The XIP  analysis results were quantified by calculating the total number of  salient sentences extracted by the parser and the numbers of each  rhetorical sentence type. First, a correlational study was  conducted with these analysis results based on the essays marks.  This was followed by a multiple regression analysis in order to  understand the effect, if any, of each rhetorical sentence type on  essay marks.   3.3 Correlational Study Results  The correlational study was conducted to understand whether  there is any relation between the number of results from the XIP  output and the marks of the essays.   A Pearson [19] product-moment correlation coefficient was  computed to assess the relationship between the total number of  salient sentences found by XIP in student essays and the mark of  these essays. Correlation was computed as 0.190, which means a  weak, positive correlation between the essay marks and the total  number of salient sentences extracted by XIP. Thus increases in  the total number of salient sentences are weakly correlated with  increases in mark. Table 1 below shows the coefficient results for  each rhetorical sentence type and the mark.   Table 1. Correlational Study Results for each Rhetorical  sentence type   Rhetorical  Sentence Type   Value of the  Correlation  Coefficient   Strength of the  Correlation   CONTRAST 0.151 Weak   BACKGROUND 0.109 Weak   TENDENCY 0.025 No meaningful  correlation   EMPHASIS 0.076 No meaningful  correlation   NOVELTY 0.097 No meaningful  correlation     Overall, no negative and no meaningful correlation was found  with the sentence types TENDENCY, EMPHASIS, NOVELTY,  SUMMARY and OPEN QUESTION. There was a weak, positive  correlation between the essay mark and the total number of  CONTRAST and BACKGROUND sentences.    Although these results give some insights into the correlations  between XIP findings and marks, they do not tell the whole story.  Besides the total number of salient sentences, the rhetorical type  distribution is known; so that it can be used to interpret how  strongly each sentence type affects the final mark. This was done  through multiple regression analysis using all 1307 essays of the  dataset.   3.4 Multiple Regression Analysis Results  In the multiple linear regression model, the mark of the essays  was taken as the dependent variable and the number of salient  sentences for each XIP category (TENDENCY, EMPHASIS,  NOVELTY, SUMMARY, OPEN QUESTION, CONTRAST and  BACKGROUND) marked up in the essays as independent  variables.   The regression model proved to be highly significant. Following  normal convention, p0.05 signifies a statistically significant  result, and p0.001 is regarded as highly significant. The p value  for this model was less than 0.001, which indicates that the model  is statistically highly significant. It means that this is a strong  evidence to further interpret how strongly independent variables  help to explain the essay mark with the model.   Adjusted R2 measures the proportions of the total variability in  the dependent variable, which is explained by the independent  variables of the model. For this model the adjusted R2 was 0.048,  which means that 4.8% of the total variability in mark was  explained by the independent variables.   When each independent variable was analysed, we found that the  two of the independent variables: CONTRAST and  BACKGROUND are statistically highly significant and have  explanatory power for the dependent variable essay mark  (CONTRAST, p0.001; BACKGROUND, p 0.001).   When unstandardized coefficients were examined for these two  independent variables, the following interpretations were made:    for a one unit increase in the number of CONTRAST  sentences within essays, the model predicts that the  dependent variable, essay mark, will increase between  0.498 and 1.078 points (calculated as B2*Std.Error),  holding all other independent variables fixed/constant.    for a one unit increase in the number of  BACKGROUND sentences within essays, the model  predicts that the dependent variable, essay mark, will  increase between 1.075 and 3.431 points, holding all  other independent variables fixed/constant.   For the rest of the independent variables the p value was not  significant, therefore they cannot be interpreted in the same way  as CONTRAST and BACKGROUND.   We carried out an internal validation using a randomly selected  subset of the overall data. We set IBMs SPSS statistical software  package to randomly select half of the data, and ran the regression  analysis on this. This produced exactly the same results. We are  currently repeating the study with similar datasets to see whether  we get the same results for external validation.   4. DISCUSSION  4.1 The performance of XIP on the student  essays  The scope of this study did not allow us to carry out a large-scale  evaluation of the performance of XIP. One of the authors has  evaluated 225 automatically detected salient sentences, and found  that 49 (22%) of them did not play the role of the scholarly  argumentation in the essay. We could not measure the coverage,  i.e. the percentage of actual salient sentences that are missed.   357    An important source of errors is related to the specificity of  literary essays that the current version of XIP does not account  for:  Since these essays involve the analysis of literary work, the  rhetorically salient expressions may also be parts of that analysis  and not of the scholarly argumentation. The following sentence,  which refers to the childrens story, Peter Pan, illustrates such a  non-rhetorical expression detected by XIP (underlined):   Wendy is not seen to challenge this role even when she  is out of her comfort zone and enters Never land.   The performance of XIP could be improved by adapting the  system to the domain in future work. However, the noise in the  literary essays in our study does not amount to a proportion that  would undermine the validity of the statistical correlations.   4.2 Relationship between the tutors marking  grid and the salient sentences  As we previously pointed out, the teachers marking grid contains  criteria for evaluating the essays according to six aspects, two of  which are particularly in line with our framework: Approach to  alternative explanations and Construction of Academic  argument. Thus it is probably these two aspects that underlie the  correlations between the tutors grades and XIP results on  sentences labeled as CONTRAST and BACKGROUND. In this  section we briefly describe the underlying relationship between  the two aspects and the semantics of the two sentence types, and  we attempt to show why the other sentence types have not shown  significant correlation with the tutors grades.   Sentences labelled CONTRAST capture the expression of  tensions, contrasts between ideas, models or research directions,  and the sentences labelled BACKGROUND make reference to  relevant other work. Thus these two sentence types in XIP do  indeed perform discourse functions that convey alternative  explanations, which in turn are organic parts of academic  argument [20]. The following example illustrates this overlap:   Trites' analysis of young adult literature disputes Hunt's  assertion by arguing that children's literature often  affirms the child's sense of Self and personal power  (Hewings, 2009, p.287).   The XIP analysis captures that there is tension within ideas by  matching the expressions analysis disputes and assertion by  arguing, which suggest that the student is aware of alternative  analyses of young adult literature. At the same time these  expressions position the student's statement within the framework  of her/his academic argumentation over approaches to the child's  characteristics in children's literature.   The quantitative study did not show any statistically significant  correlations between the marks and the other salient sentence  types detected by XIP: SUMMARY, EMPHASIS, NOVELTY,  TENDENCY and OPEN QUESTIONS. Taking into account the  evaluation aspects and the object of the essays, we propose the  following explanation.   The SUMMARY sentences merely convey the idea that the  author summarizes her essay. Thus these sentences do not  contribute to any of the evaluation aspects. Referring to new  research directions, raising open questions, emphasising ideas as  surprising, or important, and describing research tendencies are  not usual discourse moves in literature analysis; these are  elements of argumentation schemes in mainly empirical research.   While salient sentences do indicate the authors awareness of  alternative analyses, and show efforts to develop scholarly  argumentation, their mere presence does not imply that the  alternative analyses are discussed at a sufficient level, or that the  argumentation is sound, well-structured or coherent. It simply  signals that the writer does convey some content on alternative  analyses, and that this argumentation does treat the topic in a  scholarly style. Still the fact that the number of salient sentences  shows a correlation with the grades indicates that the more  scholarly metadiscourse is present in a student essay the more it is  likely that it gets a better grade in the evaluation.   4.3 Some Outliers  Whereas in the great majority of the essays the grade was  correlated with the number of salient sentences detected by XIP,  in some rare cases high grades were given by the tutors to essays  with very few salient sentences, and conversely,  low  grades  were given to essays with a relatively great number of salient  sentences. A close look at some of these essays allows us to  provide some insight into these cases.   We observed that the high graded essays with few salient  sentences that we examined have a strikingly vivid and literary  style, which does not strictly follow the patterns of concise  scholarly communication. These essays convey a personal  approach, show deep knowledge, and use unconventional  expressions. Alternative explanations required by the marking  grid are provided, however they are embedded into a particular  narrative flow, in which the expression of contrast is distributed  along several sentences (underlined). Consider the following  extract:   As Hunt states sameness and difference is the essence  of childrens books; they have recurrent ideas  (2009a,p. 71). He goes on to cite  [Here comes a list  of examples.] But is this the only tradition the book  breaks Based upon the themes detailed above this  essay will look at what similarities and differences A  Monster Calls has to childrens literature from the last  250 years, focusing particularly on Toms Midnight  Garden.   Instead of referring to the alternative arguments through  expressions like as contrasting analyses or critical debates,  the author of this essay lays them out in several steps.    What we have observed in the case of low-graded essays  containing a relatively high number of salient sentences is that on  the contrary, their style is simple and schematic, and sometimes  their syntactic structure is not clear:     I do not think any of the themes I have mentioned were  written about to change or challenge aspects of the  community, I believe these issues were just to define  the culture of society as it was in the Victorian era and  to reinforce the roles subliminally.   In order to discover these outliers, and categorize them correctly,  we would need to supplement rhetorical analysis with features  that take into account style.   5. CONCLUSION  This study is an example learning analytics approach that can be  followed by the wider LA community who might want to evaluate  the potential use of analytics products within learning contexts,   358    for which there is a growing interest. The purpose of our study  has been the evaluation of the rhetorical parser, XIP, in  correlation with tutors essay grades as a measure of quality.   We have made a significant advance toward understanding the  power and effectiveness of XIP in educational contexts. The  results show that the output of XIP is strongly related to teachers  expectations in student essays: we have found statistically  significant correlations between two of the XIP rhetorical move  labels, CONTRAST and BACKGROUND, and the final grade of  the essays. These two labels convey rhetorical moves that are  particularly in line with two aspects of the tutors marking grid.   Following the quantitative study, we had a closer look at some  student essays, which provided some insights regarding the  parsers performance. We have found that the quality of the parser  is reasonably good, considering that it has not been customized  for the particular domain. We have also analyzed some essays,  which got high grades and contain few salient sentences, as well  as ones that got low grades and contain many salient sentences.  Such essays show special writing style, and further work is  needed to recognise them, and integrate their features into an  automatic analysis tool.   Overall, the focus of this research is not on grading student  writing automatically, but on the potential to automatically  identify attributes of good academic writing, so that we can  design computer-aided support for both educators and students in  monitoring students progress and in displaying the rhetorical  analysis of the essays as formative feedback. Social science  student essays have constituted the material of our first study,  which we plan to follow up with student essays from various  other disciplines. This will then allow us to create a framework  that will be the middle ground between learning and  computation, helping members of both communities articulate, in  precise terms, the opportunities for pedagogically sound learning  analytics.   6. REFERENCES  [1] S. M. Glynn and K. D. Muth,  Reading and writing to learn   science: Achieving scientific literacy,  Journal of Research  in Science Teaching, vol. 31, pp. 1057-1073, 1994.   [2] K. Hyland, Metadiscourse: Exploring interaction in writing:  Continuum International Publishing, 2005.   [3] A. de Waard, S. Buckingham Shum, A. Carusi, J. Park, M.  Samwald, and . Sndor,  Hypotheses, evidence and  relationships: The HypER approach for representing  scientific knowledge claims,  2009.   [4] W. A. Sandoval and K. A. Millwood,  The quality of  students' use of evidence in written scientific explanations,   Cognition and Instruction, vol. 23, pp. 23-55, 2005.   [5] M. Lea and B. V. Street,  Student writing in higher  education: An academic literacies approach,  Studies in  higher education, vol. 23, pp. 157-172, 1998.   [6] R. Andrews, Argumentation in Higher Education: Improving  practice through theory and research: Routledge, 2010.   [7] T. Lillis and J. Turner,  Student writing in higher education:  contemporary confusion, traditional concerns,  Teaching in  Higher Education, vol. 6, pp. 57-68, 2001.   [8] L. S. Norton,  Essay-writing: what really counts,  Higher  Education, vol. 20, pp. 411-442, 199    [9] D. Hounsell,  Essay planning and essay writing,  Higher  Education Research and Development, vol. 3, pp. 13-31,  1984.   [10] C. Coffin, M. J. Curry, S. Goodman, A. Hewings, T. Lillis,  and J. Swann, Teaching academic writing: A toolkit for  higher education: Routledge, 2002.   [11] Teufel, S., Kan, M-J. (2009). Robust argumentative zoning  for sensemaking in scholarly documents. In Proceedings of  the 2009 international conference on Advanced language  technologies for digital libraries (NLP4DL'09/AT4DL'09).   [12] Sndor,  .  (2007). Using  the  author's  comments  for  knowledge  discovery.    Semaine  de  la  Connaissance:   Atelier Texte et Connaissance. Nantes. June  29.http://citeseerx.ist.psu.edu/viewdoc/downloaddoi=10.1.1 .73.19 38&rep=rep1&type=pdf   [13] S. At-Mokhtar, J.-P. Chanod, and C. Roux,  Robustness  beyond shallowness: incremental deep parsing,  Natural  Language Engineering, vol. 8, pp. 121-144, 2002.   [14] Sndor, . and Vorndran, A. (2010). The detection of salient  messages from social science research papers and its  application in document search. Workshop on Natural  Language Processing Tools Applied to Discourse Analysis in  Psychology, Buenos Aires, Argentina, May 10-14. 2010.   [15] White, B., & Larusson, J. A. (2015). Chapter 8: Identifying  Points for Pedagogical Intervention Based on Student  Writing: Two Case Studies for the Points of Originality. In  J. A. Larusson & B. White (Eds.), Learning Analytics: From  Research to Practice (156-190). Springer.   [16] Simsek, D., Buckingham Shum, S., De  Liddo,  A.,  Ferguson, R. and Sndor, . (2014) Visual Analytics of  Academic Writing, Demo at The 4th International Learning  Analytics and Knowledge, Indianapolis, IN, USA, pp. 265- 266, ACM New York, NY, USA 2014   [17] Simsek, D., Buckingham Shum, S., Sndor, ., De Liddo, A.  and Ferguson, R. (2013). XIP Dashboard: Visual Analytics  from Automated Rhetorical Parsing of Scientific  Metadiscourse. 1st International Workshop on Discourse- Centric Learning Analytics. (3rd International Conference on  Learning Analytics & Knowledge, 8 April 2013, Leuven,  Belgium). Open Access Eprint: (http://oro.open.ac.uk/37391)   [18] Taibi, D., Sndor, ., Simsek, D., Buckingham Shum, S., De  Liddo, A. and Ferguson, R. (2013) Visualizing the  LAK/EDM Literature Using Combined Concept and  Rhetorical Sentence Extraction, 1st Learning Analytics and  Knowledge Data Challenge at Learning Analytics and  Knowledge (LAK 13), Leuven, Belgium   [19] Pearson, K. (1895), Royal Society Proceedings, 58, 241  [20] Swales, J.M., Feak, C. (1994). Academic Writing for   Graduate Students. Ann Arbor, the University of Michigan  Press Suthers D. and Verbert, K. Learning analytics as a   middle space . Proc. 3rd Int. Conf. on Learning Analytics  and Knowledge (LAK '13), Leuven, BE, pp.1-4, ACM: New  York, 2013.         359      "}
{"index":{"_id":"58"}}
{"datatype":"inproceedings","key":"Worsley:2015:LML:2723576.2723624","author":"Worsley, Marcelo and Blikstein, Paulo","title":"Leveraging Multimodal Learning Analytics to Differentiate Student Learning Strategies","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"360--367","numpages":"8","url":"http://doi.acm.org/10.1145/2723576.2723624","doi":"10.1145/2723576.2723624","acmid":"2723624","publisher":"ACM","address":"New York, NY, USA","keywords":"computational, constructionist, data mining, learning sciences","abstract":"Multimodal analysis has had demonstrated effectiveness in studying and modeling several human-human and human-computer interactions. In this paper, we explore the role of multimodal analysis in the service of studying complex learning environments. We compare uni-modal and multimodal; manual and semi-automated methods for examining how students learn in a hands-on, engineering design context. Specifically, we compare human annotations, speech, gesture and electro-dermal activation data from a study (N","pdf":"Leveraging Multimodal Learning Analytics to Differentiate  Student Learning Strategies   Marc Worsley  Marcelo Worsley  Stanford University   520 Galvez Mall, CERAS 217  Stanford, CA 94305   mworsley@stanford.edu     Paulo Blikstein  Stanford University   520 Galvez Mall, CERAS 232  Stanford, CA 94305   paulob@stanford.edu rsley@stanford.edu paulob@stanford.edu  ABSTRACT  Multimodal analysis has had demonstrated effectiveness in  studying and modeling several human-human and human-computer  interactions. In this paper, we explore the role of multimodal  analysis in the service of studying complex learning environments.  We compare uni-modal and multimodal; manual and semi- automated methods for examining how students learn in a hands- on, engineering design context. Specifically, we compare human  annotations, speech, gesture and electro-dermal activation data  from a study (N=20) where student participating in two different  experimental conditions. The experimental conditions have already  been shown to be associated with differences in learning gains and  design quality. Hence, one objective of this paper is to identify the  behavioral practices that differed between the two experimental  conditions, as this may help us better understand how the learning  interventions work. An additional objective is to provide examples  of how to conduct learning analytics research in complex  environments and compare how the same algorithm, when used  with different forms of data can provide complementary results.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   collaborative learning, computer assisted instruction, computer  managed instruction.    General Terms  Algorithms, Human Factors.   Keywords  Learning Sciences; Computational; Constructionist; Data Mining   1. INTRODUCTION  Multimodal analysis has provided a powerful tool for studying  complex human-human and human-computer interactions across a  variety of domains. Furthermore, these technologies have had a  strong impact on the development of multimodal interfaces that  create more naturalistic, engaging and authentic environments. The  development of multimodal interfaces has started to make its way  into the education domain in the form of intelligent tutoring  systems, but automated multimodal analyses have been scarcely  explored in non-computer mediated environments. In this paper, we  follow in the paradigm of multimodal learning analytics [1 - 3] in   order to expand multimodal analysis to complex, hands-on learning  environments. While multimodal analysis has long been a staple of  education research, the introduction of computational multimodal  techniques has received some resistance. This hesitation is  understandable given the infancy of the field. In an effort to  contribute to this discussion and propel the advancement of the  multimodal learning analytics paradigm, we report on the  affordances of employing a multimodal analysis in a complex  learning environment and show how multimodal learning analytic  techniques can be relevant for improving the field.  This study builds on our prior work [4], where we present two  different approaches that students use in engineering design:  example-based reasoning  using examples from the real-world as  an entry point into solving a task; and principle-based reasoning   using engineering fundamentals as the basis for ones design. These  two reasoning strategies complement prior work on analogical  problem solving [5], case-based reasoning [6], mechanistic  reasoning [7, 8] and expertise [9 - 11]. In [4] we described example- based reasoning and principle-based reasoning in qualitative terms,  and then proceeded to use these two approaches in a controlled  study (N=20) that compares how each approach impacts learning  gains and performance during a collaborative hands-on activity. In  that study we found that students in the principle-based reasoning  condition engineered higher quality structures and also had higher  learning gains from pre-test to post-test. The goal of this paper is to  discover the multimodal practices that can help explain the  observed differences in the two experimental conditions, learning  and success. Furthermore, as a primary objective we want to show  that the two experimental conditions are associated with markedly  different processes. Finally, we want to compare the results that we  obtain from the analysis of hand-annotated data, to the results  achieved from multimodal sensor data.  In what follows we briefly present some pertinent prior literature;  describe the experiment from which the data was derived; delineate  the basic algorithm used to analyze the data; summarize important  results; and discuss the implications of this work.   2. PRIOR LITERATURE  This paper builds on a rich body of research in educational data  mining and learning analytics [12]. Through these disciplines  researchers have demonstrated the ability to analyze data from a  wide range of modalities. Previous work includes examples from  intelligent tutoring systems that leverage: discourse analysis (e.g.  [13]), content word extraction (e.g. [14]), uncertainty detection  (e.g. [15]), sentiment analysis (e.g. [16 - 18]), linguistic analysis,  prosodic and spectral analysis, and multi-modal analysis (e.g. [13,  19]). Additionally we leverage approaches from the multimodal  interfaces community: technological tools and research for  integrating data streams [1, 20]; extracting rich context from audio- video data [21]; and behavior [22], for example. We also build on   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee. Request permissions from Permissions@acm.org.    LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03$15.00  http://dx.doi.org/10.1145/2723576.2723624   360    work from engineering education that has focused on studying  design patterns among novices and experts (e.g. [23]).   Through the computer science and learning analytics communities  we are continually seeing new technologies and techniques for  analyzing data. The challenge, however, is to leverage those  technologies in a way that aligns with learning theory, and that  allows us to answer important questions about learning. In terms of  learning theory, this work is informed by that of [24] which  qualitatively identified different multimodal learning and/or  epistemic states. These states were characterized by body pose,  amount of dialogue and student gaze as they worked in groups to  complete a worksheet. Because of differences in context, we are not  examining the same modalities, nor do we expect to see the same  states. Nonetheless, the underlying assumptions around the  connection between behavior, epistemology and learning remain  central to the approach that we employ.  Additionally, the analytic tools used to identify student behavioral  states borrows strategies from our prior work in learning analytics  [25,26]. In these papers we analyzed high frequency data by  reducing the dataset down to a set of representative states that  generalized to all of the participants. This is the same approach that  we will employ in this paper, but differ from the prior work in that  we construct states that include multimodal data, as opposed to  constructing states from log-files, computer program snapshots or  click-streams.  Finally, readers may note an alignment between this research and  work on productive multivocality [27]. In particular, the  multivocality project involved a diverse group of researchers  engaging in conversations about the different approaches, analytics  techniques and pivotal moments that occurred in a variety of  datasets. Many of these researchers used a similar construct of  interaction analysis to examine how groups of students engage in a  given learning experience. As an outcome of their work, the authors  highlight the ways that diverse opinions and approaches improved  the quality of the findings, assuming some amount of shared  understanding and loosely similar goals. The research described in  this paper differs from that of the multivocality community in a  number of ways. First, in using a traditional two condition  experimental design, a primary objective of our work was to  compare two learning strategies for the multimodal behaviors that  distinguish one group from the other. Secondly, while the  multivocality project did contain a Group Scribbles dataset that  included both face-to-face interactions and hands-on manipulation  of objects, the majority of the datasets involved computer-mediated  environments. The present dataset does not involve any computer- mediated interactions. A final point of distinction is that the  multivocality project reported having trouble comparing across  analyses because of different units of analysis. In the present paper,  we are able to use the same analytic technique at the same unit of  analysis, with two different data streams. In this way the results  have less to do with the analytic technique or the grain size of the  analysis, and have more to do with the data being analyzed. There  are other notable differences that we will exclude for the sake of  brevity.   3. METHODS  To provide the reader with additional context, we briefly describe  the research participants and the task that they completed before  entering into a discussion of the analyses and results.  Students used common household materials: one paper plate, 4 ft.  of garden wire, four drinking straws and five wooden Popsicle  sticks. The objective was to use the materials provided to create a  structure that could support a weight of approximately half a pound.   Participants were also asked to support the weight as high off the  table as possible.   Our population of students consisted of twelve 9th- through 12th- grade students and eight undergraduate students. Pairs of students  were randomly assigned to either use example- or principle-based  reasoning, after controlling for prior education experience. Thus,  each condition had six high school students and four undergraduate  students. In the example-based condition, students generated three  example structures from their home, community or school in order  to motivate their design. In the principle-based condition, students  identified three engineering principles that conferred strength and  stability to a ladder, an igloo and a bridge before embarking on the  building task.  The data capture environment included: a Kinect sensor  for  capturing audio, gesture and video; a high resolution web camera -  to record how students moved the different materials; and an  electro-dermal activation sensor  for measuring stress and/arousal.  All sensor data was synchronized through the data collection  software, and also verified by a research assistant.   3.1 Activity Sequence  The overall flow of activities that students completed included: a  pre-test; an intervention, i.e. one of the two conditions; a  preliminary design drawing; a hands-on, paired, building activity;  post-test; and reflection (Figure 1).     Figure 1. Overall study design   3.2 Data  The two analyses included in this paper are drawn from two  different sets of modalities.   Hand-Annotated Data. The first consists of manually annotated  object manipulation classes (Table 1), and are based on our prior  work [28, 29].   Table 1. Object Manipulation Classes  Class Codes   REALIZE Actively constructing the final  product   PLAN Prototyping ideas or inspecting the  materials   EVALUATE Testing a mechanism or testing the  system   Pre-Test  Intervention  Design Sketch  Activity  Post-Test  Reflection  361    MODIFY Making changing to an existing  design   REVERT Undoing one of more parts of a  previous design   NOTHING Engaging in little to no physical or  verbal activity     The second set of modalities included audio data, hand/wrist  movement data and electro-dermal activation data.  Audio data. Data was derived from a combination of audio channels  from an overheard web camera, and from the Xbox Kinect sensor.  A custom piece of software was developed based on the Carnegie  Mellon University (CMU) Sphinx Speech Recognition Toolkit.  Specifically, the source code was modified to leverage the  programs voice activity detection feature. Voice activity detection  is an automated means for determining when voice-based audio is  being generated. Several speech recognition software solutions  contain some variant of voice activity detection. The custom  software provided voice detection start and stop times for all of the  audio channels. Audio was considered to be present if either of the  audio sources detected a voice, within a given second of time. Thus  the final format of this data is a binary representation. Every second  of the activity is labeled with a zero or one, for the absence or  presence of audio at that time stamp. Because the audio channel  captured sound from both participants this piece of data is the same  for each person in a dyad.  Hand/wrist movement. Hand/wrist movement data was also  generated from the Xbox Kinect sensor. Once again a custom built  application was used to store three dimensional data for twelve  upper body joints. The application uses native features available  from the Kinect for Windows SDK, specifically, the ability to  conduct skeletal tracking in the seated position. The custom  application stores the data at 10 Hz. From the file generated, we  utilize only the left and right wrist, hand and elbow data points. For  each successive pair of data points we compute the angular  displacement for the vectors that connect: left wrist and left hand;  left wrist and left elbow, right wrist and right hand; right wrist and  right elbow. The eventual angular displacement that is recorded is  an average of the four angular displacements. Using angle as the  means for comparison reduces biases introduced by participants  having different sized bodies and limbs. Accordingly, for each  tenth of a second in time we have stored the total angular hand/wrist  displacement.  Electro-dermal Activation. Electro-dermal activation (also referred  to galvanic skin response and/or skin conductance) readings were  captured at 8 Hz. Processing electro-dermal activation data  involved controlling for individual differences in variance, as well  as individual differences in stress response. In practice, this was  achieved by collecting baseline data as students completed the task  of counting down by 7.We will refer to this as the math stress  test. As additional baseline data, students also completed a Stroop  test, and had their electro-dermal activation recorded during non- task related activities. As before, each data point was time-stamped  with the local date and time. Each data point was then transformed  into an index value by subtracting the mean from the math stress  test, and then dividing by the standard deviation of the math  stress test data for that student. When we compared electro-dermal  activation index values across the different activities, there were no  statistically significant differences between experimental  conditions for the baseline data, the Stroop test, or the math test.  However, across the intervention, design phase and the building   activity differences were statistically significant. This provided  validation that this normalization was effective.   3.3 Algorithm  The approach follows our previous work [28, 29] on analyzing  design strategies and success on hands-on engineering tasks. Here  we significantly extend that work by incorporating multimodal  data, as opposed to simply using hand-coded data. A visualization  of the general algorithm is provided in Figure 2.     Figure 2. General algorithm  Time-stamp. The first step of extracting process data is to ensure  that all data is properly time-stamped. This provides a means for  synchronizing across the different modalities.  Segment. The time-stamped data is then segmented. Data is  segmented every time a pairs structure is tested. We interpret  testing as representing an instance in which at least one person in  the pair is eliciting feedback that will update the students on the  current stability of their structure. Testing usually takes the form of  a team member placing the weight on the structure. We will refer  to individual segments as test segments.  Segmentation always resulted in a single value for each data stream.  For the audio data the value is the proportion of the test segment  during which voice activity was detected. For the hand/wrist  movement data, the value is the average total angular displacement  during that test segment. Finally, the electro-dermal activation  value is the average index value during that particular test  segment.   Timestamp  Segment  Cluster  Re-label  Normalize  Compare  Behavior   Frequency  Compute  Distances  Group  Participants  Compare  Clusters  362    As a whole, the segmentation process serves to smooth the data.  Instead of having to take into account each of the spikes and troughs  that may emerge from any of the data streams, segmentation allows  us to look at broader trends. Noise reduction is also achieved during  the following step.  Cluster. After the segmentation process, there are hundreds of  unique test segments. Some of these will be very similar to one  another, only differing by an infinitesimal amount, while others  will vary quite extensively from one another. The goal of clustering  is to identify natural groupings among the various test segments  and ultimately provide a common set of states, or behaviors, by  which to compare individual user sequences. However, before  proceeding with clustering, we first do data standardization.  Namely, we adjust each value, such that all of the data in a given  column has a mean of zero and a standard deviation of one. This  process eliminates bias in clustering, by ensuring that each column  contributes equally to the distance metric, which in the case was  Euclidean distance. After standardizing the data, we used X-Means  clustering to group the data points into a set of clusters that place  each test segment with the other test segments that it is most  similar to. Once each test segment has been grouped with similar  test segments, each cluster, or group, can be described based on  the average values of all of its members. These values provide the  basis for determining common behavioral practices in later  sections.  Re-label. All test segments that are put into the same cluster are  given the same name and value. Accordingly, each students  sequence of test segments can now be represented as a list of  clusters.  Normalize. In the normalization step, each students re-labeled  sequence is lengthened to permit direct comparisons between each  pair of participants. The two forms of normalization used are L-1  normalization and dynamic time warping [30]. In the case of L-1  normalization, each sequence is lengthened so that all participants  sequences are of equal length. In dynamic time-warping, a  modification of Levenshtein distance [31] is used to find the best  match between pairs of sequences.  Compare Behavior Frequency. After L-1 normalization, the next  step is to compare behavior frequency data across the three metrics  of interest: success, experimental condition; and learning. The  comparisons are based on Mood Median Tests along each of the  individual clusters of test segments.  However, instead of the  traditional Mood Median Test, which computes statistical  significance based on a Chi-Square distribution, we use a binomial  test. These two tests were used because the data did not meet the  requirements for other traditional statistical tests. This step  represents the conclusion of one branch of the analysis tree (left  hand branch of Figure 2).  Compute Distances. Following dynamic time warping, a distance  is computed between each pair of participants.  Group Participants. Based on the pair-wise distances, similar  participants are forced into one of two groups using K-Means  clustering. In each case a student is put into the group that contains  other students whose process was most similar to their own.  Forcing the students into one of two groups was done to align with  the two experimental conditions.  Compare Participant Groups. Finally, the groups are compared  using a binomial test to determine the probability that individuals  were randomly assigned to their specific group. Specifically, it is  here that we examine the hypothesis that different groups, as  partitioned by experimental condition, success on the activity, or   based on post-test score, used markedly different processes from  one another.   4. Results  We present the results from the two analyses separately. First, we  consider the results from the hand-annotated analysis and then the  multimodal sensor data.   4.1 Hand-Annotated Data Analysis  The first entry point for analysis is the generalized test segment  cluster centroids that are derived from the clustering step for each  analysis. From Figure 3 we see that hand-coded analysis results in  four clusters that we have entitled: PREPARE  which primarily  consists of PLAN and NOTHING actions; IMPLEMENT  which  primarily consists of REALIZE and REVERT actions; ADJUST   which primarily consists of MODIFY actions; and  SIGNIFICANTLY ADJUST  which is represented by extensive  MODIFY actions. This provides a canonical set of actions that  interestingly, distinguishes between two variants of modifying  (Figure 3). Additionally, Figure 4 provides the distribution of the  four test segment behaviors derived from the hand-annotated  data.     Figure 3. Test segment cluster centroids for hand-coded data       Figure 4. Relative frequency of common behaviors for hand- annotated data    When we use the cluster centroids to re-label each students test  segments and compare sequence similarity, we find a weak   -1.5  -1  -0.5  0  0.5  1  1.5  2  2.5  3  PREPARE IMPLEMENT MODIFY SIGNIFICANTLY MODIFY  St an  da rd  iz ed   T im  e  plan nothing build adjust undo  PREPARE 37%  IMPLEMENT 39%  ADJUST 17%  SIGNIFICANT LY ADJUST  7%  363    correlation between experimental condition and overall process  similarity (p < 0.07).     Figure 5. Participant Cluster Assignment based on Process  Similarity  Finally, when we compare the two experimental conditions in terms  of how frequently they use the different test segments (PREPARE,  IMPLEMENT, ADJUJST, SIGNIFICANLY ADJUST) we find  that usage of IMPLEMENT is positively correlated with principle- based reasoning (p = 0.011) (Figure 6), design quality (p = 0.011),  and learning (p < 0.001). Recall that these differences are based on  Mood Median Tests where we used a binomial distribution as  opposed to a Chi-Squared distribution. Accordingly, all p-values  are based on an exact binomial test.     Figure 6. Median common behavior usage by condition  From this analysis, then, it appears as though the IMPLEMENT  behavior is beneficial and correlates with experimental conditions,  success and learning.   4.2 Multimodal Sensor Data Analysis  Again, the first entry point for analysis is the generalized test  segment cluster centroids that are derived from the X-Means  clustering step. In particular, we found four multimodal test  segment types (Figure 7). As before, for each cluster, we created  a name that captures the primary multimodal behaviors associated  with that cluster of test segments.     Figure 7. Test Segment cluster centroids for multimodal data  The most common segment, FLOW, is characterized by near or  below average behavior across all three variables: audio, hand/wrist  movement and electro-dermal activation; and is 56% of all test  segments (Figure 8). Furthermore, the FLOW test segment  appears to bear some similarity to flow as described in [32].       Figure 8. Distribution of Multimodal Test Segments  The second most frequently occurring test segment is ACTION,  which represents 18% of all test segments. This behavior  primarily consists of students who are currently engaging in above  average hand/wrist movement and nothing more.  The third most frequently occurring state is TALK. The amount of  audio in this cluster is approximately two standard deviations above  the mean. Hand/wrist data is just above the mean, and electro- dermal activation is nearly half a standard deviation below average.   The final cluster is STRESS. This behavior is characterized by  extremely large values of electro-dermal activation, as well as  above average hand/wrist movement  When we re-label each students sequence with the cluster  centroids, and compare process similarity, we find that there is a   0  2  4  6  8  10  12  Group A Group B  N um  be r o  f S tu  de nt  s  Example-Based  Principle-Based  0 2 4 6 8  10 12  PREPARE IMPLEMENT ADJUST SIGNIFICANTLY ADJUST  N um  be r o  f I ns  ta nc  es  Example-Based Principle-Based  -1.5  -1  -0.5  0  0.5  1  1.5  2  2.5  3  3.5  TALK FLOW ACTION STRESS  St an  da rd   D ev  ia tio  ns  electro-dermal activation hand/wrist movement  audio  TALK 18%  FLOW 56%  ACTION 18%  STRESS 8%  364    very high correlation between the results of the unsupervised  clustering and the initial experimental conditions. Specifically,  seven of the eight students assigned to Group A are from the  principle-based condition. The inverse pattern is observed for  Group B, with seven of the eight individuals in that group coming  from the example-based reasoning condition (Figure 9).       Figure 9. Composition of groups based on experimental  condition as derived from process similarity   The likelihood of this happening randomly is less than 0.002,  suggesting that the two conditions did, in fact, utilize markedly  different processes. Having observed that there are salient  differences between the processes that the two conditions use, as  determined through multimodal data, we now consider the nature  of those differences.  Comparing the frequency with which students use the different  multimodal test segments we find that students in the principle- based reasoning condition were more frequently in the multimodal  behavior FLOW (p = .007) (Figure 10). We also observed that the  multimodal behavior FLOW is positively correlated with student  learning (p < 0.001).     Figure 10. Median common behavior usage by condition   5. DISCUSSION  The two analyses included in this paper pinpointed different  behaviors (or test segments) that were associated with principle- based reasoning, success and learning. Furthermore, the results  from both analyses help us answer questions about how principle- based reasoning and example-based reasoning are manifested  differently. The results from the hand-annotated data, which all  correlated with IMPLEMENT provided a consistent picture of how  principle-based reasoning may be related to success and learning,   namely that IMPLEMENT is mediating all three variables.  However, where the hand-annotated data provided a consistent  result, the multimodal sensor data provided a much more definitive  delineation between the two experimental conditions. Specifically,  the multimodal analysis resulted in participant clusters that almost  perfectly aligned to the two experimental conditions. This is  particularly significant because this result was achieved without  any algorithmic supervision and was based only on multimodal  sensory data. Additionally, the differences between the two  experimental conditions was also more pronounced in terms of  differential usage of the multimodal behavior FLOW. Recall that  students in the principle-based reasoning condition were much  more likely to use FLOW (p = 0.007) than their peers in the  example-based reasoning condition. In this way, then, the two  analyses provided complementary results that may not have been  easily garnered from either analysis by itself. Hence it may be  instructive to continue using hand-annotated data, as it still has  great utility. Furthermore, though, this study suggests that an  integrated multimodal analysis can significantly enhance the fields  ability to detect and model student learning.  The process of clustering the data into different test segments was  also constructive in that it enabled us to eliminate some of the noise  in the data and reduce each students time series into a set of  common behaviors. This was particularly useful when dealing with  the continuous data provided from both sets of modalities. In the  case of the hand-annotated data, clustering grouped actions that  were similarly performed in conjunction with one another, and  created higher level groups of actions. For the multimodal sensor  data, three of the four test segments can easily be explained as  being unimodal in nature. However, FLOW which was the most  prevalent only becomes distinct from the other categories when  considered as multimodal.   6. FUTURE WORK  In future work we will endeavor to automatically draw additional  semantics from the multimodal sensor data. For example, we will  more closely examine prosodic, spectral and voice quality features  of the speech being generated, as well as detect moments of  laughter, for example. Additionally, we will use the video data to  more closely examine the nature of the collaboration. For example,  head and body position information may be indicative of pair of  students that is working collaboratively by co-constructing objects,  as compared to a pair of students that are working individually and  only occasional consulting one another. Identifying these patterns  of interactions may be an additional asset to studying the impact  that a given learning strategy has on students behavioral practices.   7. CONCLUSION  In this paper, we embarked upon presenting a comparative analysis  of learning analytics on hand-annotated and multimodal sensory  data. For both analyses we used the same analytic technique, and  examined how these approaches can be used in the service of  studying the differences between two experimental conditions.  Ultimately, we found that both have significant utility in helping us  better understand student learning. Additionally, these techniques  provide a reasonable means for studying complex, open-ended,  hands-on learning environments. Our hope is that through this  paper, researchers will develop additional insights and motivation  for leveraging human-annotated data and multimodal learning  analytics to study complex learning environments in ways that can  clearly connect the learning sciences with learning practices.   0  2  4  6  8  10  Group A Group B  N um  be r o  f S tu  de nt  s  Principle-Based Example-Based  0  5  10  15  20  25  30  TALK FLOW ACTION STRESS  Example-based reasoning Principle-based reasoning  365    8. REFERENCES  [1] Worsley, M. 2012. Multimodal Learning Analytics: Enabling   the Future of Learning through Multimodal Data Analysis  and Interfaces. In Proceedings of the 14th ACM international  conference on Multimodal interaction (ICMI '12). ACM,  New York, NY, USA. 353-356.   [2] Blikstein, P. 2013. Multimodal Learning Analytics. In  Proceedings of the 3rd Annual Learning Analytics and  Knowledge Conference. Leuven, Belgium.    [3] Scherer,S., Worsley,M. and Morency, L. 2012. 1st  international workshop on multimodal learning analytics:  extended abstract. In Proceedings of the 14th ACM  international conference on Multimodal interaction (ICMI  '12). ACM, New York, NY, USA, 609-610.   [4] Worsley, M., & Blikstein, P. (2014). Assessing the Makers:  The Impact of Principle-Based Reasoning on Hands-on,  Project-Based Learning. Proceedings of the 2014  International Conference of the Learning Sciences (ICLS), 3,  11471151.   [5] Gick, M., & Holyoak, K. (1980). Analogical problem  solving. Cognitive Psychology, 355, 306355   [6] Kolodner, J. L. 1997. Educational implications of analogy: A  view from case-based reasoning. American psychologist,  52(1)   [7] Russ, R. S., Coffey, J. E., Hammer, D., & Hutchison, P.  2009. Making classroom assessment more accountable to  scientific reasoning: A case for attending to mechanistic  thinking. Science Education, 93(5), 875891.    [8] Lehrer, R., & Schauble, L. (1998). Reasoning about structure  and function: Childrens conceptions of gears. Journal of  Research in Science Teaching, 35(1), 325.   [9] Chi, M. Glaser, Rees. 1981. Expertise in problem solving.   [10] Nokes T J, Schunn C D and Chi M. 2010, Problem Solving   and Human Expertise. In: Penelope Peterson, Eva Baker,  Barry McGaw, (Editors), International Encyclopedia of  Education. volume 5, pp. 265-272. Oxford: Elsevier.   [11] Ahmed, S., & Wallace, K. M. 2003. Understanding the  differences between how novice and experienced designers  approach design tasks, 14, 111. doi:10.1007/s00163-002- 0023-z   [12] Baker, R.S.J.d., Yacef, K. 2009. The State of Educational  Data Mining in 2009: A Review and Future Visions. Journal  of Educational Data Mining, 1 (1), 3-17.   [13] Litman, D., Moore, J., Dzikoska, M., and Farrow. E. 2009.  Using Natural Language Processing to Analyze Tutorial  Dialogue Corpora Across Domains and Modalities.  Proceedings 14th International Conference on Artificial  Intelligence in Education (AIED), Brighton, UK, July.   [14] Chi, M., Van Lehn, K., Litman, D., and  Jordan, P. 2010.  Inducing Effective Pedagogical Strategies  Using Learning  Context Features. In: Proc. of the 18th Int. Conference on  User Modeling.   [15] Liscombe, J., Hisrchberg, J., and Venditti, J. 2005.  Detecting  Certainness in Spoken Tutorial Dialogues.  In Proceedings of  Interspeech 2005Eurospeech, Lisbon, Portugal.   [16] Craig, S. D., D'Mello,S., Witherspoon, A. and Graesser, A.  2008. 'Emote aloud during learning with AutoTutor:  Applying the Facial Action Coding System to cognitive-  affective states during learning', Cognition & Emotion, 22: 5,  777  788.   [17] DMello, S. K., Craig, S. D., Witherspoon, A., McDaniel, B.,  and Graesser, A. 2008. Automatic detection of learner's  affect from conversational cues. User Modeling and User- Adapted Interaction 18, 1-2 (Feb. 2008), 45-80.   [18] Conati, C. and Maclaren, H. 2009. Empirically building and  evaluating a probabilistic model of user affect. User  Modeling and User-Adapted Interaction. August, 2009 267- 303.   [19] Worsley, M. & Blikstein P. (2010). Towards the  development of learning analytics: student speech as an  automatic and natural form of assessment. Paper Presented  at the Annual Meeting of the American Education Research  Association (AERA).   [20] Fouse, A., Weibel, N., Hitchins, E. & H, J. 2011. Chronoviz  A System for Supporting Navigating Time-Coded Data. In  Proceedings of CHI 2011, SIGCHI/ACM. PP. 299-304.   [21] Schick, A., Morlock, D. Amma, C., Schultz, T. and  Stiefelhagen, R. 2012. Vision-Based Handwriting  Recognition for Unrestricted Text Input in Mid-air. In  Proceedings of the 14th ACM International on Multimodal  Interaction (ICMI '12). ACM, New York, NY, USA, 217- 220   [22] Song, Y., Morency, L. and Davis, R. 2012. Multimodal  Human Behavior Analysis: Learning Correlation and  Interaction Across Modalities. In Proceedings of the 14th  ACM International on Multimodal Interaction (ICMI '12).    [23] Atman, C. J., Cardella, M. E., Turns, J., & Adams, R. 2005.  Comparing freshman and senior engineering design  processes: an in-depth follow-up study. Design studies,  26(4), 325-357.   [24] Scherr, R. E., & Hammer, D. 2009. Student Behavior and  Epistemological Framing: Examples from Collaborative  Active-Learning Activities in Physics. Cognition and  Instruction, 27(2), 147174.  doi:10.1080/07370000902797379    [25] Piech, C., Sahami, M., Koller, D., Cooper, S., & Blikstein, P.  (2012, February). Modeling how students learn to program.  In Proceedings of the 43rd ACM technical symposium on  Computer Science Education (pp. 153-160). ACM.   [26] Blikstein, P., Worsley, M., Piech, C., Sahami, M., Cooper,  S., & Koller, D. 2014. Programming pluralism: Using  learning analytics to detect patterns in the learning of  computer programming. Journal of the Learning Sciences,  23(4), 561-599.   [27] Suthers, D. D., Lund, K., Ros, C. P., Teplovs, C., Law, N.,  & others. (2013). Productive multivocality in the analysis of  group interactions. Springer    [28] Worsley, M. and Blikstein, P. (2013). Toward the  Development of Mulitmodal Action Based Assessment. In  Proceedings of the Third International Conference on  Learning Analytics and Knowledge (LAK '13). ACM, New  York, NY, USA, 94-101.   [29] Worsley, M., & Blikstein, P. (2014). Analyzing Engineering  Design through the Lens of Computation. Journal of  Learning Analytics, 1(2), 151-186.   366    [30] Rabiner, L. R., Rosenberg, A. E., & Levinson, S. E. 1978.  Considerations in dynamic time warping algorithms for  discrete word recognition. The Journal of the Acoustical  Society of America, 63(S1), S79S79.   [31] Levenshtein, V. I. 1966. Binary Codes Capable of Correcting  Deletions, Insertions and Reversals. Soviet Physics Doklady,  10(8), 707710.   [32] Csikszentmihalyi, M. 1992. Flow: the psychology of  happiness. London: Rider.             367      "}
{"index":{"_id":"59"}}
{"datatype":"inproceedings","key":"Suthers:2015:CNP:2723576.2723626","author":"Suthers, Dan","title":"From Contingencies to Network-level Phenomena: Multilevel Analysis of Activity and Actors in Heterogeneous Networked Learning Environments","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"368--377","numpages":"10","url":"http://doi.acm.org/10.1145/2723576.2723626","doi":"10.1145/2723576.2723626","acmid":"2723626","publisher":"ACM","address":"New York, NY, USA","keywords":"interaction analysis, learning analytics, networked learning environments, social network analysis","abstract":"Learning in social settings is a complex phenomenon that involves multiple processes at individual and collective levels of agency. Thus, a richer understanding of learning in socio-technical networks will be furthered by analytic methods that can move between and coordinate analyses of individual, small group and network level phenomena. This paper outlines Traces, an analytic framework designed to address these and other needs, and gives examples of the framework's practical utility using data from the Tapped In educator professional network. The Traces framework identifies observable contingencies between events and uses these to build more abstract models of interaction and ties represented as graphs. Applications are illustrated to identification of sessions and key participants in the sessions, relations between sessions as mediated by participants, and longer-term participant roles.","pdf":"From Contingencies to Network-level Phenomena:  Multilevel Analysis of Activity and Actors in   Heterogeneous Networked Learning Environments  Dan Suthers   Dept. of ICS, University of Hawaii  1680 East West Road, POST 317   Honolulu, HI 96822  1.808.956.3890   suthers@hawaii.edu     ABSTRACT  Learning in social settings is a complex phenomenon that involves  multiple processes at individual and collective levels of agency.  Thus, a richer understanding of learning in socio-technical  networks will be furthered by analytic methods that can move  between and coordinate analyses of individual, small group and  network level phenomena. This paper outlines Traces, an analytic  framework designed to address these and other needs, and gives  examples of the frameworks practical utility using data from the  Tapped In educator professional network. The Traces framework  identifies observable contingencies between events and uses these  to build more abstract models of interaction and ties represented  as graphs. Applications are illustrated to identification of sessions  and key participants in the sessions, relations between sessions as  mediated by participants, and longer-term participant roles.     Categories and Subject Descriptors  Collaborative and social computing systems and tools, Social  networks, Collaborative learning, Graph algorithms     Keywords  Interaction analysis, Social network analysis, Learning analytics,  Networked learning environments    1. Introduction  As formal and informal education increasingly takes place in  online interaction spaces, tools are needed to help those who  manage such spaces identify and understand activity and key  actors. This paper outlines one possible approach for building  such tools, based on a framework for analysis of learning  interactions that has broader ambitions. The purpose of the paper  is twofold: to summarize the current version of this framework,  called Traces, and to illustrate its relevance to practitioners via an  envisioned activity reporter.    The framework is motivated by a view of learning as a complex  and multilevel phenomenon. Theories of how learning takes place   in social settings vary in the agent of learning (e.g., individual,  small group, network or community), and in the process of  learning [34] (e.g., information transfer or knowledge  communication [6], intersubjective meaning-making such as  argumentation and co-construction [2, 33], shifts in participation  and identity [17, 27], and accretion of cultural capital [30]). We1  claim that learning takes place simultaneously at all of these levels  of agency and with all of these processes, potentially at multiple  time scales [18]. Thus, understanding learning in its full richness  requires data that reveal the relationships between individual and  collective levels of agency and potentially coordinating multiple  theories and methods of analysis [8, 21, 39]. A multi-level  approach is also motivated by our theoretical stance that social  regularities arise from how myriad individual acts are aggregated  and influence each other, possibly mediated by artifacts [16, 15],  and the methodological implication that to understand phenomena  such as actor relationships or community structures, we need to  also look at the stream of individual acts out of which these  phenomena are constructed.    The Traces analytic framework (described shortly) was designed  to meaningfully connect multiple levels of analysis. Matching  their complementary strengths and weaknesses, we combine  interaction analysis, which enables us to see what groups of  individuals are doing and how they are doing it (but at a level of  detail that obscures larger scale patterns), with social network  analysis, which provides summaries of ties and affiliations in a  form amenable to drawing conclusions about network or  community patterns (but loses the details of how people actually  interact). We also work with intermediate levels of analysis  between these extremes.    The practical relevance of our approach to learning analytics  derives from phenomena such as the emergence of Web 2.0 [23]  and its adoption by the educational communities, including more  recent interest in MOOCS (Massive Open Online Courses) [1]. In  these environments, learning is distributed across time and virtual  place (media), and learners may participate in multiple settings.  We focus on networked learning environments (NLE)2, which we  define to include any socio-technical network that involves                                                                        1 Although this paper is single-author, it is based on an ongoing   line of work reaching back over many years: see  acknowledgments for colleagues. Because it is a collective  accomplishment, this paper will use we throughout, although  I am responsible for its content.   2 Also called NLC, but whether or not they are communities is  an empirical and indeed theoretical question.    Permission to make digital or hard copies of all or part of this work for personal  or classroom use is granted without fee provided that copies are not made or  distributed for profit or commercial advantage and that copies bear this notice  and the full citation on the first page. Copyrights for components of this work  owned by others than ACM must be honored. Abstracting with credit is  permitted. To copy otherwise, or republish, to post on servers or to redistribute  to lists, requires prior specific permission and/or a fee. Request permissions  from Permissions@acm.org.     LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA  Copyright is held by the owner/author(s). Publication rights licensed to ACM.  ACM 978-1-4503-3417-4/15/03$15.00   http://dx.doi.org/10.1145/2723576.2723626   368    mediated interaction between participants (hence networked) in  which learning might take place, including for example online  communities [3, 26] and cMOOCs (connectionist MOOCs, [32]).  Our work will not be as applicable to isolated activity by  individuals, no matter how many, such as xMOOCS in which  large numbers of individuals interact primarily with courseware.  We are developing our framework using a data corpus from  Tapped In, an online network of education professionals,  including but not limited to preservice and inservice teachers. This  NLE displayed many of the characteristics of current  heterogeneous distributed environments. It offered asynchronous  threaded discussions, quasi-synchronous chat, file sharing, and  other media for interaction. Participants included members of  organizations and others in scheduled events and persons who  came to Tapped In of their own accord; and all participants were  free to wander between specific settings and events.  Consequently, the activity of a given participant resulted in  different kinds of traces in the log files associated with these  media, at different times and different virtual spaces. The trace of  what for a given participant was a unitary experience is  fragmented across these logs. The need to reassemble fragmented  traces into a single analytic artifact was another motivation for  developing Traces.    To preview the Traces approach, logs of events are abstracted and  merged into a single abstract transcript of events, and this is used  to derive a series of representations that support levels of analysis  of interaction and of ties. Three kinds of graphs model interaction:  Contingency graphs record how events such as chatting or posting  a message are observably related to prior events by temporal and  spatial proximity and by content.  Uptake graphs aggregate the  multiple contingencies between each pair of events to model how  each given act may be taking up prior acts.  Session graphs are  abstractions of uptake graphs: they cluster events into spatio- temporal sessions with uptake relationships between sessions.  Relationships between actors and artifacts are abstracted from  interaction graphs to obtain associograms [38, 40], which can be  folded into traditional sociograms.   Other publications have detailed some of the theory [15, 34] and  analytic representations [37, 40] behind this work. This paper  reports on how these ideas have been implemented to transform  events into interaction and session graphs and sociograms, and in  particular will focus on the utility of the derived sociograms and  how they retain information from the interaction analysis.   2. Tapped In  In developing and testing our framework, we use a data corpus  from SRI Internationals Tapped In (tappedin.org), an  international online network of educators engaged in diverse  forms of informal and formal professional development and peer  support [10, 31]. Tapped In was motivated by the desire to  understand how to initiate and manage large heterogeneous  communities of educators, how such communities evolve, and the  benefits that participants derive from their involvement (Mark  Schlager, personal communication). This network included  activities that were sponsored by formal organizations (e.g.,  universities, school districts, and nonprofits) mixed with volunteer  driven and other unsponsored activities, in both synchronous and  asynchronous media, with participants from across all career  stages and diverse occupations related to education. Thus, Tapped  In provides an opportunity to develop and test hypotheses, tools,  and techniques for understanding heterogeneous networks.  Cumulatively, Tapped In hosted the content and activities of more   than 150,000 education professionals (over 20,000 per year in our  study period) in thousands of user-created spaces that contain  threaded discussions, shared files and URLs, text chats, an event  calendar, and other tools to support collaborative work. Over its  16-year history, more than 50 organizations, including education  agencies and institutions of higher education became tenants in  the system with online courses, workshops, seminars, mentoring  programs, and other collaborative activities. There were also  approximately 40-60 public activities per month designed by  Tapped In members and open to anyone in the community.  Volunteers drove the majority of Tapped In community-wide  activity [10]. Extensive data collection capabilities captured the  activity of all members and groups, including chat data,  discussion board interactions, and file sharing. The trend in  current social media is for such data to be treated as a proprietary  asset, and some are even discouraging big data research outside of  industry (as reported by [5]), a dangerous trend. The Tapped In  data corpus offers a valuable opportunity for study of how large  heterogeneous socio-technical networks evolve.    We selected a period from September 2005 through May 2007 for  our research, and used smaller samples within this period to  develop and test our methods. This period was chosen because  graphs of activity showed peak usage during this time, and we  wanted to study a successful network.3 (Usage tapered off after  2010, and Tapped In was shut down in 2013.) In the remainder of  this paper, we illustrate the analytic capabilities of the Traces  framework with several granularities of analysis centered around  one day of data in April 2006. This day includes a Teaching  Teachers session on mentoring that we had previously selected  for micro-analysis due to its engaged and thoughtful discussion,  and other related sessions. (A sample of this session that will be  used for examples is shown later in Table 1.) By analyzing data  from the days and weeks surrounding this session, we can assess  how the methods described in this paper identify this session and  its actors, and how it was embedded in its surrounding context.    3. Traces Analytic Framework  This section briefly summarizes the framework, updating previous  descriptions [36, 40] to reflect the current state of design and  implementation. The representations used at various levels of  analysis are shown schematically in Figure 1 (next page):  micro- relations between situated acts by participants are identified and  then aggregated into interactional relations, and further aggregated  into network level phenomena.   At the bottom, we exemplify various traces of activity (such as log  files of events) that provide the source data. (Our actual data  includes database logs and textual transcripts of chats.) These are  parsed using methods that are necessarily system-specific into an  event stream, shown in the second level (boxes in Figure 1b). The  event representation includes their time, location, event type,  actor, and content or media object where relevant.    3.1 Contingency Graph  At this level of abstraction (Figure 1b), we compute contingencies  between events, to produce a model of how acts are mutually  contextualized (shown as arrows). Human action is contingent  upon its setting in diverse ways: our computational methods                                                                        3 After years of study of this corpus I am impressed with how   successful it was. The interaction was rich and diverse, a credit  to Mark Schlagers design vision and what he, Judi Fusco and  Patti Schank accomplished in carrying out this vision.    369    capture some of these contingencies that are amenable to  automated detection. A contingency called proximal event (PE)  reflects the likelihood that events occurring close together in time  and space are related. In analyzing quasi-synchronous chat,  contingencies are installed to prior contributions in the same room  that occur within an adjustable time window but not too recently.  A Keystroke Level Model [25] of how long a typist would have  taken to type the contribution is used to ensure that the prior  contribution was already visible when typing began by closing the  window this amount of time before the contribution (see [36] for  details). For example, referring to Table 1 (next page), APs  contribution at 23:42 is PE contingent on MTs question at 23:35,  but APs contribution at 23:38 is not, having occurred too soon  after. Address (ADR) and reply (RPL) contingencies are installed  between an utterance mentioning a user by name and the last  contribution (ADR) and next contribution (RPL) by that  participant within a time window, using a parser/matcher of user  IDs to first names. For example, an Address Contingency is  placed between DAs 24:27 That is an interesting question M  and MTs prior question at 23:35; while a Reply Contingency is  placed between MTs  thought about often D at 24:42 and  Ds 24:27. Same actor contingencies (SA) are installed to prior  acts of a participant over a larger time window to reflect the  continuity of an agents purpose. All of MTs later contributions  are contingent on the earlier ones by Same Actor in this short  excerpt. Overlap in content as represented by sets of lexical stems  is used to produce a lexical contingency (LEX) weighted by the  number of overlapping stems. For example, the lexical stems for   APs contribution at 23:38 and for ASs contribution at 24:09  overlap on peopl, using the NLTK Lancaster Stemmer, [24].  We do not claim that this specific set of contingencies is sufficient  for analysis, and others are under development, though we are  finding them to be a good starting point to explore the potential of  our framework. Our research and development strategy is to see  how far we can get with these simpler contingencies before  adding computational complexity.    The resulting contingency graph (e.g., Figure 1b) is represented  as the first layer of abstraction in what we call an Entity-Event- Contingency graph or EEC [37]. In the EEC graph, vertices are  Events. These can be of various types (e.g., enter chat, exit chat,  chat contribution), and are annotated with time stamps, actors,  content (e.g., chat content), and locations (e.g., chat rooms)  involved in the event. Contingencies are typed edges between  vertices (the types were described in the previous paragraph), and  there may be multiple edges between any two vertices (e.g., two  proximal events by the same actor with lexical overlap will have  at least three contingencies between them).   3.2 Uptake Graph  It is necessary to collapse the multiple edges between vertices into  single edges for two reasons. First, most graph algorithms assume  that there is at most only one edge between two vertices. Second,  we are interested in uptake, the relationship between events in  which a human action takes up some aspects of prior events as  being significant in some manner [37]. Replying to prior  contributions in chats and discussions are examples of uptake, but     Figure 1. Levels of Analysis and their Representations      370    uptake is not limited to replies: one can appropriate a prior actors  contribution in other ways. Uptake is not specific to a medium: it  can occur in different media, and cross media [37]. Contingencies  are of interest only as collective evidence for uptake, so we  abstract the contingency graph to an uptake graph.    As shown in Figure 1c, uptake graphs are similar to contingency  graphs in that they also relate events, but they collect together  bundles of the various types of contingencies between a given pair  of vertices into a single graph edge, weighted by a combination of  the strength of evidence in the contingencies and optionally  filtering out low-weighted bundles. Different weights can be used  for different purposes (e.g., finding sessions; analyzing the  interactional structure of sessions, constructing sociograms).  Importantly, we do not throw away the contingency weights: these  are retained in a vector to summarize the nature of the uptake  relation (and, once aggregated into sociograms, of the tie between  actors, to be discussed shortly).   An example weighting that has been used for uptake graphs is SA  2, LEX 1, ADR 3, RPL 3, PE 3. The lexical weight (LEX) is  multiplied by the number of lexical stems in common between the  two contributions, and proximal event (PE) decays according to  temporal distance between events within a time window of 120  seconds (e.g., it could be 3, 2 or 1). A permissive threshold of 1  allows creation of an arc in the uptake graph if there is just one  lexical overlap or to any event in the prior 120 seconds; a  threshold of 2 would require at least one other contingency for  older contributions before the possibility of uptake is asserted.    For example, consider DAs contribution at 24:27 and MTs  contribution at 24:42 (Table 1). The second event is contingent on  the first by lexical overlap (I), address, reply, and proximal  event (in the same room and 15 seconds apart). Applying the  uptake graph weighting, we get an uptake relation of weight 10,  annotated with a vector of weighted constituent contingencies that  we can write as: <SA 0, LEX 1, ADR 3, RPL 3, PE 3>.    3.3 Sessions  Clusters of events in spatio-temporal proximity are computed to  identify  sessions (indicated by rounded containers in Figure  1c). We can do several interesting things with uptake graphs.    For inter-session analysis, we collapse each session into a single  vertex representing the session, but retain the inter-session uptake  links. (For example, there are four sessions in Figure 1c and two  inter-session uptakes.) These inter-session links indicate potential  influences across time and space from one session to another. An  example will be given shortly in conjunction with Figure 2.    For intra-session analysis, the uptake graph for a session is  isolated. Several paths are possible from here. For example, the  sequential structure of the interaction can be micro-analyzed to  understand the development of group accomplishments: this part  is not automated. Methods for graph structure analysis can be  applied, such as cluster detection, analysis of graph motifs [20]  that represent uptake patterns of interest, or tracing out thematic  threads [41].   3.4 Sociograms  Either within or across sessions, we can fold uptake graphs into  actor-actor sociograms (directed weighted graphs, Figure 1d).   The tie strength between actors is the sum of the strength of  uptake between their contributions. These sociograms can be  analyzed using conventional social network analysis methods  such as degree or eigenvector centrality to identify key actors, etc.  [22, 42]. With sociograms we want to be stricter about the   evidence for relations between the two actors, so we use a  different weighting that downplays proximity and emphasizes  direct evidence of orientation to the prior actor. In the examples of  this paper we use SA 0 (no self-loops), LEX 3, ADR 5, RPL 5, PE  1. The threshold is set at 4, meaning the sum of weights must be at  least 4 to count towards a tie between actors in the sociogram.  (We are not claiming that these weights are optimal.)    Continuing our example, the weighing of uptake of DAs  contribution at 24:27 by MTs contribution at 24:42 becomes <SA  0, LEX 3, ADR 5, RPL 5, PE 1>. To illustrate how weights from  multiple uptakes are aggregated into a tie we need one more  uptake relation. MTs contribution at 25:18 is contingent on the  same contribution by DA (24:27) by <SA 0, LEX 0, ADR 0, RPL  5, PE 1>. If we folded only these two uptake relations by actor,  wed get a directed actor-actor tie from MT to DA weighted by a  vector of <SA 0, LEX 3, ADR 5, RPL 10, PE 2>, for a total of 20.  Of course, there are many other pairs of events that evidence  uptake of DA by MT, so the sums in any nontrival sociogram will  be larger.    3.5 Summary and Implementation  In summary, our framework provides multiple pathways for  analysis. Contingencies are applied to events in the EEC abstract  transcript to produce a contingency graph. Contingencies are then  aggregated into uptake between the same events. A single  aggregation can be used, or optionally different weightings can be  used for identifying sessions, doing detailed interaction analysis,  or constructing sociograms. In either case, uptake that crosses  partitions can be used to identify influences across space and time,  and uptake within partitions can be analyzed to study the  interactional structure of a session. Uptake graphs can be folded                                                                        4 By the Tapped In user agreement, transcripts of sessions such as   this one were public, and many were posted on a public web site  with full user login names. However, to conform to my own  institutions requirement, initials are used in this paper. The  exception will be two participants whose volunteer activities  (among that of other participants) were extremely important to  Tapped In, and who want to be identified: BjB and JeffC.    Table 1. Sample chat from Tapped In  Full names have been abbreviated to initials.4   23:35 MT: are all good teachers good mentors  23:38 AP: some people will take a while to get to that point  23:42 AP: No..not all  23:51 EB: definitely not  23:55 LH: Training can help, but I think some is personality  24:09 AS: some people are excellent teachers but are horrible   mentors  24:09 EB: some great teachers can not hold a decent   conversation with an adult  24:11 AP: i had to co-ops who would be awful mentors  24:24 LH: Nods  24:27 DA: That is an interesting question M,  ... I would   probably say yes first off, and then wonder some more  24:42 MT: it is something I have thought about often D  24:47 AP: I think its alot of personality  25:17 DA: one thing a mentor has to know is how to operate   with a peer, and ow to be intentional about handing over,  or encouraging greater independence   25:18 MT: observation has made me think that it takes an extra  special ingredient to tip the scales      371    into networks where nodes are actors rather than events, to which  sociometrics are applied. The present paper will illustrate  applications of session identification and analysis of sociograms.   Another line of analysis not discussed in this paper is to fold  events into actor-artifact networks, or bipartite weighted directed  graphs that we call associograms for short, because they capture  how actors are associated with each other via mutual read and  write of media objects. In that line of work, we have undertaken  community analysis of associograms to detect not only human  participants in communities, but also the artifacts that reflect their  mediated nature (e.g., synchronous or asynchronous) [38].    The framework is presently implemented in Java, using the  Hibernate object/relational model and persistence engine  (hibernate.org/orm), with callouts to the NLTK library (nltk.org)  and iGraph (igraph.sourceforge.net). Visualizations and  sociometrics for this paper were computed in Gephi (gephi.org).  See [36] for further discussion of the implementation.     4. Examples: Periodic Activity/Actor Reports  Facilitators (e.g., educators and learning network managers) may  find it useful, for varying reasons, to know where activity has  taken place in their NLE and the roles of participating actors.  There may be multiple virtual settings for participation (e.g., chat  rooms and discussion forums), and many potential participants.  As in our Tapped In environment, interaction events may be  scheduled formally or informally or occur spontaneously. A  facilitator might like to know: When and in what settings did  substantive interaction take place Who were the central actors  (including educators and students), and who appears disengaged  How were sessions related to each other, e.g., by central actors It  would be useful to have answers to these questions on a regular  recurring basis for timely intervention, e.g., daily or weekly, and  also over longer spans for research purposes.    This section provides examples of how our framework can  provide information relevant to these questions. We first briefly  show how sessions are detected, and then show analyses at  various granularities of analysis to illustrate the utility of the  Traces framework. At the granularity of a single session, we  identify main actors, those who may be disengaged, and   coherence of uptake. At granularity of one day, we visualize the  relations between sessions, and identify persons who are social  bridges mediating these relationships. At the granularity of one  week (where we can begin to discern enduring behaviors), we  characterize mediated relationships between actors. At the  granularity of multiple weeks, we plot the role development of  actors according to centrality metrics. These analyses are  presented in a different order than just listed, to follow a plausible  scenario of investigation. We begin with example reports for one  days activity (April 6, 2006), chosen because it includes some  interesting sessions we have studied, enabling us to validate the  results.    4.1 What happened today   Different options exist for detection of sessions in interaction  graphs. If interaction is not clearly demarcated by periods of non- interaction and one wishes to discover clusters of high activity, we  have found that cohesive subgraph detection or community  detection algorithms [11] such as modularity partitioning [4]  applied to uptake graphs are useful [36]. However, in the present  corpus, although there is ongoing activity everywhere, activity is   distributed across rooms, and the activity within a room almost  always has periods of non-activity between sessions. For present  purposes of analysis, identifying weakly connected components  on the proximal event contingency is sufficient to identify  sessions. This can be done efficiently without needing to construct  a contingency graph (it will be constructed later for other  purposes). Activity is tracked in each room, and a new session ID  is assigned to the room every time there is a gap of S seconds of  no activity. S is a tunable parameter: here we use 240 seconds.     4.1.1 What sessions of activity took place   Table 2 shows sessions comprising at least 100 events, including  chat entry/exit, chat messages, and emotes. (There were many  more small sessions. One could just as easily select by number of  participants.) We see that there is some activity in the morning  before school, and it resumes after school through the evening.   Several sessions run in parallel, and some rooms are reused.   4.1.2 How are the sessions related  Beyond size, we might be interested in how sessions relate to each  other. Figure 2 visualizes a portion of the inter-session uptake  graph for the day, including several of the largest sessions from  Table 1. Uptake relations between events in different sessions  have been aggregated into weighted uptake relations between  sessions. We see that some sessions have more heavily weighted  links between them. Reading the edges in reverse order (uptake  points backwards in time), we see that session 737 in  TI_Reception influenced 755 in Teachteach_Grp, which in turn  influenced 848 NTraining_Group. These are the three largest  sessions in Table 1. Sessions 908 (also in TI_Reception) and 828  (not in Table 1, but having the second highest number of  participants) were also related to 848. A facilitator of Tapped In  would know that TI_Reception often shows up in this manner, as  it the entry point for persons who just logged in. Clearly many  participants then went to 755 Teachteach_Grp, as indicated by the  thick 755737 link. But why is 755 so influential on 848, which  took place immediately after in NTraining_Grp A closer look at  these sessions will answer this and other questions.    Table 2. Sessions on April 6 containing at least 100 events  #E = event count, #P = participant count. Personal office names are  anonymized using initials. Participants are in multiple time zones.    ID #E #P Room Start End   81 193 7 SharedSpace  05:35:38  06:29:24   98 229 3 DS_Ofc  06:05:23  07:00:26   176 104 11 EdTech_Grp  07:44:38  08:26:54   737 224 34 TI_Reception  16:26:18  17:31:20   749 235 5 KC_Ofc  16:35:34  17:26:20   755 412 20 Teachteach_Grp  16:45:14  18:04:44   767 361 14 KS_Ofc  16:53:50  18:01:57   768 140 5 MH_Ofc  16:54:44  18:15:55   845 335 8 ArtsSites_Grp  17:51:24  19:07:40   848 512 19 NTraining_Grp  17:54:07  19:10:45   854 244 5 Mriker_Grp  17:56:43  18:41:54   908 438 18 TI_Reception  18:36:37  20:18:39   928 283 7 DS_Ofc  18:56:24  19:49:16   934 384 4 CommunityRoom  19:00:20  19:51:41     372    4.2 Who was important    It would be of interest to educators or NLE facilitators to know  who the key participants are in their online learning communities,  whether for assessment in formal educational settings, to  encourage volunteers in participant driven settings, or for research  purposes such as to study what drives key participants. It is also  important to know who is disengaged.    Some of these needs can be met through social network analysis.  As summarized in section 3, we can generate sociograms for any  granularity of the uptake graph (e.g., within a session, or across  sessions over a time period) by folding uptake relations between  events into ties between their actors. For example, a facilitator  might want to see a sociogram summarizing actor activity in  session 755, the Teachteach_Grp. The sociogram is shown in  Figure 3. Node size is weighted in-degree, discussed below.      4.2.1 Sociometrics  Sociograms add information over mere counts of number of  contributions (Contr in Table 3), because some sociometrics are  sensitive to the network context of nodes representing actors. For  example, weighted in-degree (WInDeg) indicates the extent to  which other acts have contingencies to and hence potentially took  up a given act: aggregating these for an actor is an estimate of  how much an actors contributions are taken up by others. This  metric is sensitive to both the level of activity of the actor and that  activitys relation to others activity. Weighted out-degree  (WOutDeg) is an estimate of how much an actor takes up others  contributions. Eigenvector centrality (Eigen) is a nonlocal metric  that takes into account the centrality of ones neighbors [22, p.  169], indicating the extent to which an actor is connected to others  who are themselves central. Betweeness centrality is an indicator  of actors who potentially play brokerage roles in the network:  high betweeness centrality means that that node representing an  actor is on relatively more shortest paths between other actors [22,  p. 185], so potentially controls information flow or mediates  contact between these actors. Betweeness will be of particular  interest when examining activity across sessions: different  sessions generally have different actors, so an actor attending  multiple sessions will have high betweeness.    4.2.2 Central actors in sessions  To continue our example, metrics for session 755  (Teachteach_Grp) are shown in Table 3. Our interest here is not  the specific actors of this historical Tapped In session, but rather  in what the analysis shows about this familiar data that it could  also show about novel data from current online interactions. We   can see that MT is the most central actor in this session by all  metrics. She has the most contributions (Contr); they are taken up  the most by others (WinDeg); and she takes up others  contributions the most (WOutDeg). As the leader, she mediates  between other actors in the session (Between). Eigenvector  centrality (Eigen) shows that other actors who are not as highly  ranked by Contr are central via their association with MT and  each other (see edge thickness in Figure 3). The sociometrics rank  actors distinctly from counts of contributions, for example, AP  has as many contributions as MT but is not as central on other  metrics. Actors WS, BJB, BJB2, and KLC have notably low  activity, and there are 6 actors not connected to any others (shown  only in Figure 3) because they did not make any contributions.  These are potentially disengaged participants, but one should  check their activity in other sessions before drawing this  conclusion. Actors BjB and BjB2 are the same person: she was  one of the most active volunteers, and often helped other  facilitators run their sessions, so she was given two accounts.  BjBs 3 contributions are all broadcast announcements of events  starting in various rooms.    A similar table for session 848 NTraining_Grp (omitted for space)  shows that MT is again the most central actor by all four metrics  shown. Examination of the transcripts for the two sessions show      Figure 2. Closeup of session graph with inter-session  uptake. Node and edge size is weighted degree.         Figure 3. Sociogram for session 755,   Teachteach_Grp      Table 3. Sociometrics for actors in 755 (Teachteach_Grp)  Sorted by WInDeg. Six actors with all values of 0.0 are omitted.   Actor Contr WInDeg WOutDeg Eigen Between   MT 74 1086.0 983.0 1.000 0.034   LH 38 663.0 535.0 0.915 0.006   EB 47 581.0 535.0 0.915 0.006   AS 41 571.0 628.0 0.915 0.009   DA 39 553.0 606.0 1.000 0.023   AP 72 462.0 661.0 0.847 0.012   EK 14 237.0 235.0 0.847 0.006   KA 8 174.0 171.0 0.780 0.001   BE 7 131.0 194.0 0.679 0.034   SR 7 128.0 124.0 0.686 0.001   WS 2 65.0 48.0 0.696 0.000   BJB2 3 65.0 16.0 0.865 0.007   BjB 3 16.0 0.0 0.296 0.000   KLC 1 4.0 0.0 0.074 0.000     373    that 755 ends with MT saying ...I have another discussion  scheduled in the Teachers in Training room and saying farewell;  she then shows up in the NTraining_Grp room and leads session  848, an online class. The sociometric table (and the sociogram)  for 848 shows that LH and AP are also present there: the chat  transcript shows that they followed MT to her next session and  asked permission to observe the class. This overlap in actors is the  primary basis for the weight on the combined 848755 uptake  relation, shown as line thickness in Figure 2. One apparent lurker  from 755, BJB2, is also in 848 with low levels of activity: we find  out more about her below.    4.2.3 Central actors across sessions  Some actor roles may not be apparent from a single session or  even day. Facilitators may be interested in those who are active  consistently over time. Betweeness is of particular interest here,  because sessions usually have little overlap in participants, but  some persons play key roles in Tapped In by participating in  multiple sessions, thereby potentially facilitating flow of  information and contacts across sessions. Aggregating into one  sociogram for the week from Sunday to Saturday that contains  April 6, 2006, we obtain the metrics in Table 4. We were familiar  with JeffC as the volunteer greeting participants in TI_Reception  (session 737) and MT as the facilitator of 755 and 848, but this  table draws attention to other actors. DW merits investigation as  the most active person this week. Although BjB was lurking in  the two sessions we examined, it turns out she facilitated sessions  of her own this week (including 845 ArtSites_Grp in Table 1, at  the same time as she observed 848 as BJB2). Three of the actors  are notable for their high betweeness: DW, BjB and JeffC,  meaning that they are potential brokers between the groups found  in various sessions.  Traces has detected an important role: these  three actors were volunteer facilitators who staffed TI_Reception  and helped others in their sessions (as well as their own).    Superficially, these analyses appear methodologically similar to  the many sociometric analyses found in the literature, so we  should highlight what the Traces framework has added. The  Traces framework derived these latent ties from automated  interaction analysis of streams of events, by identifying and then  aggregating multiple contingencies between events, and then  folding the resulting uptake relations between events into an  actor-actor graph. This has significant advantages over (for  example) manual content analysis or the use of surveys to derive  tie data, which are labor intensive, or reliance on explicit  friending relations: both surveys and friend links may not  reflect the ties that are latent in actual interaction between the  persons in question. Another advantage is described below.    4.3 How are actors related to each other   To recapitulate, the above analysis derived ties between actors by  aggregating multiple contingencies between their contributions.  The contingencies indicate the qualitative nature of the  relationship between these contributions, e.g., being close in time  and space, using the same words, and addressing another actor by  name. When contingencies are aggregated into uptake relations,  we keep track of what each type of contingency contributed to the  uptake relation. This record keeping is continued when folding  uptakes into ties, so that for any given pair of actors we have a  vector of weights that provides information about the nature of the  relationship in terms of the underlying contingencies.  For example, consider actors MT and DA, whom we saw  conversing in Table 1. The vectors underlying their tie for the full  week of April 2-8, 2006 are shown below. Contingency weights   have been divided by weights so that the following may be read as  counts of actual events. PE can be read as the denominator for  ADR and RPL. WS is the weighted sum based on the unadjusted  weighting scheme:    From To SA LEX ADR RPL PE WS  DA MT 0 29 16 4 34 350  MT DA 0 24 9 8 28 305   We can see that they frequently call each other by name. Sorting  all ties by weighted tie strength (WS), we find that the two closest  actors for this week were SLC and HB:    From To SA LEX ADR RPL PE WS  SLC HB 0.0 389 58 52 444 2161  HB SLC 0.0 359 40 69 414 2036   Relational information might be of interest to educators or  researchers who are managing collaborative learning activities  amongst students, or even to examine ones own relations to  students. Traces makes this possible by retaining information  about the interactional origins of ties.    4.4 How coherent are the sessions    Automating sequential analysis of uptake graphs requires further  work to bring the strength of uptake relations into closer  agreement with human interpretation. Ongoing work is addressing  this problem with further contingencies and experimenting with  weighting. However, we have found that folding our uptake  graphs to sociograms smooths out the noise and provides  sensible results [35]. Also, the uptake graphs we are presently  producing can expose some the structure of sessions through  graph clustering analysis. Specifically (and as discussed further in  [36]), applying a modularity partitioning or community  detection algorithm to the uptake graph for an entire day, we can  see whether modularity partitioning treats each session as a single  coherent graph cluster. Modularity partitioning tries to maximize  the links within a cluster as compared to what is expected at  random [22, p. 224]. Thus, if a session corresponds to a  modularity partition it has high internal linkage by our estimated  uptake relations, and lower linkage to other sessions. On the other  hand, if a session as defined by spatio-temporal proximity of  events was not coherent interactionally, then there would be more  than one modularity partition of the uptake graph for the session.  Here, coherence simply means continuity of uptake: it is not a  value judgment on the quality of interaction, as there may have  been several distinct yet high quality conversations in a space- time region.    Table 4. Top 10 actors by eigenvector centrality, April 2-8    Actor WInDeg WOutDeg Eigen Between   DW 11878.0 11514.0 1.000 0.037   BjB 5196.0 4781.0 0.912 0.033   JeffC 8851.0 9168.0 0.645 0.024   SR 1579.0 2057.0 0.566 0.002   M 2577.0 2462.0 0.515 0.008   MT 3746.0 3241.0 0.485 0.002   MD 837.0 1036.0 0.424 0.001   BJB2 642.0 647.0 0.401 0.006   EW 734.0 864.0 0.378 0.001   PJ 3900.0 3650.0 0.373 0.002     374    To make these ideas more concrete, we ran modularity  partitioning using Blondels algorithm [4] on the uptake graph of  April 6th, which includes all of the sessions of Table 2.  Figure 4  shows (a) the full uptake graph colored by modularity class,  followed by excerpts for three sessions: (b) 755 Teachteach, (c)  848 Ntraining, and (d) 908 TI_Reception. As can be seen, the  topic-oriented sessions 755 and 848 are classified as mostly in one  modularity class. The second modularity class visible in both  cases is when participants were saying goodbye. On the other  hand, session 908 in TI_Reception has two large modularity  classes visible. This room is where most Tapped In users (those  who have not set their entry point to be their offices) will first  appear when they log in. Volunteers here direct newcomers to the  room holding the session they may be interested in. Various  spontaneous conversations also arise. Thus it is not surprising that  a session in TI_Reception has multiple modularity partitions. In  this session, there was an initial conversation, and then the  participants present changed around the time another session  ended at the breakpoint shown by the modularity partitioning.   All of the other sessions in Table 2 except two consisted primarily  of one modularity partition. The two exceptions were both where  chat was interrupted by other activity: in one case, an instructor  asks the students to do some work; in the other, the participants  briefly go into a Skype session. It should be noted that modularity  partitioning is sensitive to the graph over which it works. If one  gives any one of these sessions only the uptake graph for that  session, the algorithm will find more partitions. Ongoing work is  examining community detection on interaction graphs as a way of  analyzing sessions.    4.5 How does participation change over time  Analyses on longer time scales may be of interest to researchers  as well as practicing educators. Here we exemplify tracing the  development of actors roles over time in terms of changes in their  sociometrics. Using the same process as described above,  sociograms were automatically generated from over 700,000 chat  events for actors at one-week intervals over a 14-week period.  Weighted in-degree, betweeness and eigenvector centralities were  charted; due to space constraints we only show the latter in Figure  5. To illustrate different actor trajectories, we show two top actors  from Table 4 and three other actors of varying levels of  participation previously discussed.      Here, eigenvector centrality indicates the extent to which ones  contributions are taken up, weighted by the centrality of those  doing the uptaking. DW and JC are consistently highly central,  reflecting their high level of activity. MT, the facilitator of the  April 6 sessions, has varying levels of participation, sometimes as  high as the top facilitators. Her primary sessions run once a  month, as indicated by spikes every 4th week. A typical participant  is DA, who periodically comes in for moderate levels of activity,  but her eigenvector centrality is elevated in comparison to her  activity due to interacting with central actors such as MT (notice  that she has two spikes on the same weeks as MT). Finally, AP  only appeared on one day in this range of data, but achieved  noticeable centrality due to her high interaction with MT (Figure  3). The point is to illustrate that the Traces framework can  automatically generate sociometric and other data on participation  from large event logs to trace participant roles over time.    5. Contributions and Related Work   This paper introduced the Traces analytic framework, which  integrates traces of activity that are distributed across media,  places and time into an abstract transcript, and then provides a  linked abstraction hierarchy that uses observable contingencies  between events to build models of interaction and ties. It  addresses the need to understand aggregate phenomenon (e.g.,  ties, roles, and communities) as both produced by and  providing the setting of specific interactional events. The  framework has been implemented in software that enables us to  automatically analyze sequences of hundreds to millions of user  acts to derive models of interaction and social ties. This software  was developed using as a test case a rich historical data corpus  where diverse participants interacted in an environment that  exhibits many features of todays distributed interaction. The  paper illustrated the potential functionality of Traces through  examples of analyses that a facilitator of a networked learning  environment may want to undertake, including identification of  sessions and key participants in the sessions, relations between  sessions as mediated by participants, and participant roles as they  developed over time.      (a) Modularity partitioning of full uptake graph for 4/6/06                 (b) 755 Teachteach   (c) 848 NTraining     (d) 908 TI_Reception   Figure 4. Modularity partitioning of uptake graph   Nodes are chat contributions. Layout is by OpenOrd in Gephi. Colors are  modularity partitions by Blondels algorithm.      375    Other authors have noted the need to combine multiple forms of  analysis, including specifically social network analysis in  networked learning environments. For example, de Laat and  colleagues [9] and Martnez and colleagues [19] showed the  utility of combining social network analysis with various  qualitative and quantitative methods in the study of participation  networks. Others have constructed and folded interaction graphs  into sociograms of ties between actors. For example, Rosen &  Corbit [29] constructed graphs based on temporal proximity, and  Haythornthwaite and Gruzd [13] describe preliminary work in  extracting interaction relations from references and names. The  Traces project is in the same spirit, but is arguably more mature.  We consider multiple kinds of relations between events to provide  a richer basis for session identification and subsequent analysis of  activity and actors within sessions, and have automated these  analyses. Work by Trausan-Matu on polyphonic analysis [41]  has affinities to our use of multiple contingencies, but has only  recently been abstracted to higher levels of analysis. A thesis by  Charles [7] has provided an alternative implementation of our  approach and extended the set of contingencies. This and other  work by Ulrich Hoppe and colleagues is closest to ours in  computational aspirations, e.g., their work on detecting  collaboration patterns [12]. Our approach dovetails with work that  applies natural language processing methods for analysis of  interactional structure [28], and indeed rules for generating  additional contingencies could be derived from such research.    6. Future Work   The present implementation is research software, written to  explore the range of analytic functionality enabled by the Traces  framework. Hence it is flexible and reconfigurable, but the level  of power and flexibility is overkill for some applications, and  reconfiguration is accomplished through a technical interface of  XML scripts and exported graph files rather than an end user GUI.  Practical implementations for specific analytic applications could  be optimized (made faster and simpler), at the cost of flexibility,  and interfaces more suitable for educators and NLE managers  would of course be needed.    The framework could be re-implemented to use current  technologies and APIs. For example, the data import layer could  utilize the TinCan API (tincanapi.com), which would make our  framework applicable to a variety of platforms adhering to this  standard and make it easier to write importers for other platforms  in a less ad-hoc manner. We might replace our custom Hibernate  graph persistence engine with Neo4j (neo4j.com), which has  matured since we started this project. An analytic workbench such  as [14] would increase usability for researchers.   Our ongoing research includes implementation and testing other  promising contingencies (e.g., n-gram echoing), and tuning  weights on contingencies for capturing the interactional structure  of sessions. Once these are well grounded, we can study how  patterns of uptake and metrics on the derived sociograms relate to  session quality, and use these to automatically identify sessions of  interest. Two other objectives include using inter-session  relationships to trace the spread of ideas, and incorporating  analysis of asynchronous discussions for analysis of cross-media  influences. Automating the generation of interaction and social  network graphs opens up further approaches for relating fine- grained interaction to more aggregated levels of analysis. For  example, multiple sociograms can be generated over time to track  significant changes to the group structure or individual role  emergence, or over larger time scales to identify critical points in  the formation of healthy or unhealthy communities. These   analytic options are enabled by our frameworks abstraction away  from media-specific concepts and representations, and the  automation of mapping between levels of analysis that this  abstraction enables.   7. ACKNOWLEDGMENTS  Many thanks to Nathan Dwyer for co-developing the ideas behind  this work and writing the core software implementation; Mark  Schlager, Patti Schank and Judi Fusco of SRI for sharing their  data and expertise; and Kar-Hai Chu and Devan Rosen for their  collaboration on this project. We are indebted to the multitude of  Tapped In participants, particularly highly committed volunteers  such as BJ Berquist (BjB) and Jeff Cooper (JeffC). This work was  partially supported by NSF Award 0943147. The views expressed  herein do not necessarily represent the views of NSF.   8. REFERENCES  [1] I. E. Allen and J. Seaman, Changing Course: Ten Years of   Tracking Online Education in the United States, 2013.  [2] J. Andriessen, M. Baker and D. D. Suthers, eds., Arguing to   Learn: Confronting Cognitions in Computer-Supported  Collaborative Learning Environments., Kluwer, Dordrecht,  2003.   [3] S. A. Barab, R. Kling and J. H. Gray, Designing for Virtual  Communities in the Service of Learning, Cambridge  University Press, New York, 2004.   [4] V. D. Blondel, J.-L. Guillaume, R. Lambiotte and E.  Lefebvre, Fast unfolding of communities in large networks,  Journal of Statistical Mechanics: Theory and Experiment,  http://dx.doi.org/10.1088/1742-5468/2008/10/P10008  (2008).   [5] D. Boyd and K. Crawford, Critical questions for big data:  Provocations for a cultural, technological, and scholarly  phenomenon, nformation, Communication, & Society, 15  (2012), pp. 662-679.   [6] R. Bromme, F. W. Hesse and H. Spada, eds., Barriers and  Biases in Computer-Mediated Knowledge Communication  And How They May Be Overcome, Springer, New York,  2005.   [7] C. Charles, Analysis of Communication Flow in Online  Chats, Department of Computer Science and Applied  Cognitive Science, Unpublished Master's Thesis, University  of Duisburg-Essen, Duisburg, Germany, 2013, pp. 90.   [8] M. De Laat, Networked Learning, Politie Academie,  Apeldoorn, 2006.   [9] M. De Laat, V. Lally, L. Lipponen and R.-J. Simons,  Investigating patterns of interaction in networked learning  and computer-supported collaborative learning: A role for  Social Network Analysis, International Journal of Computer  Supported Collaborative Learning, 2 (2007), pp. 87-103.   [10] U. Farooq, P. Schank, A. Harris, J. Fusco and M. Schlager,  Sustaining a community computing infrastructure for online  teacher professional development: A Case Study of  Designing Tapped In, Computer Supported Cooperative  Work, 16 (2007), pp. 397-429.   [11] S. Fortunato, Community detection in graphs, Physics  Reports, 486 (2010), pp. 75-174.   [12] I. Halatchliyski, T. Hecking, T. Ghnert and H. U. Hoppe,  Analyzing the path of ideas and activity of contributors in an  open learning community, Journal of Learning Analytics, 1  (2014), pp. 72-93.   [13] C. Haythornthwaite and A. Gruzd, Analyzing networked  learning texts, in V. Hodgson, C. Jones, T. Kargidis, D.   376    McConnell, S. Retalis, D. Stamatis and M. Zenios, eds.,  Proc. 6th International Conference on Networked Learning,  Lancaster University, Halkidiki, Greece, 2008.   [14] T. Hecking, S. Manske, L. Bollen, S. Govaerts, A. Vozniuk  and H. U. Hoppe, A Flexible and Extendable Learning  Analytics Infrastructure, in E. Popescu, R. H. Lau, K. Pata,  H. Leung and M. Laanpere, eds., Advances in Web-Based  Learning  ICWL 2014, Springer International Publishing,  2014, pp. 123-132.   [15] S. Joseph, V. Lid and D. D. Suthers, Transcendent  Communities, in C. Chinn, G. Erkens and S. Puntambekar,  eds., The Computer Supported Collaborative Learning  (CSCL) Conference 2007, International Society of the  Learning Sciences, New Brunswick, 2007, pp. 317-319.   [16] B. Latour, Reassembing the Social: An Introduction to Actor- Network-Theory, Oxford University Press, New York, 2005.   [17] J. Lave and E. Wenger, Situated Learning: Legitimate  Peripheral Participation, Cambridge University Press,  Cambridge, 1991.   [18] J. L. Lemke, Across the scales of time: Artifacts, activities,  and meanings in ecosocial systems, Mind, Culture &  Activity, 7 (2000), pp. 273-290.   [19] A. Martnez, Y. Dimitriadis, E. Gmez-Snchez, B. Rubia- Avi, I. Jorrn-Abelln and J. A. Marcos, Studying  participation networks in collaboration using mixed  methods, International Journal of Computer-Supported  Collaborative Learning, 1 (2006), pp. 383-408.   [20] R. Milo, S. S. Shen-Orr, S. Itzkovitz, N. Kashtan, D.  Chklovskii and U. Alon, Network Motifs: Simple building  blocks of complex networks, Science, 298 (2002), pp. 824- 827.   [21] P. R. Monge and N. S. Contractor, Theories of  Communication Networks, Oxford University Press, Oxford,  2003.   [22] M. Newman, Networks: An Introduction, Oxford University  Press, 2010.   [23] T. O'reilly, What is Web 2.0 - Design patterns and business  models for the next generation of software,  http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/3 0/what-is-web-20.html, 2005.   [24] J. Perkins, Python Text Processing with NLTK 2.0 Cookbook,  Packt Publishing, Birmingham, UK, 2010.   [25] J. Raskin, The Humane Interface: New Directions for  Designing Interactive Systems, Addison Wesley, Reading,  Mass., 2000.   [26] K. A. Renninger and W. Shumar, Building Virtual  Communities: Learning and Change in Cyberspace,  Cambridge University Press, Cambridge, 2002.   [27] B. Rogoff, Observing sociocultural activity on three planes:  Participatory appropriation, guided participation, and  apprenticeship., in J. V. Wertsch, P. D. Rio and A. Alvarez,  eds., Sociocultural Studies of Mind, Cambridge University  Press, New York, 1995, pp. 139-164.   [28] C. P. Ros, Y.-C. Wang, Y. Cui, J. Arguello, K. Stegmann,  A. Weinberger and F. Fischer, Analyzing collaborative  learning processes automatically: Exploiting the advances of  computational linguistics in computer-supported  collaborative learning, International Journal of Computer- Supported Collaborative Learning, 3 (2008), pp. 237-271.   [29] D. Rosen and M. Corbit, Social network analysis in virtual  environments, Proc. 20th ACM conference on Hypertext and  hypermedia (HT '09), ACM, New York, NY, 2009, pp. 317- 322.   [30] M. Scardamalia and C. Bereiter, Higher Levels of Agency for  Children in Knowledge Building: A Challenge for the Design  of New Knowledge Media, The Journal of the Learning  Sciences, 1 (1991), pp. 37-68.   [31] M. Schlager, J. Fusco and P. Schank, Evolution of an Online  Education Community of Practice, in K. Renninger and W.  Shumar, eds., Building Virtual Communities, Cambridge  University Press, Cambridge, 2002, pp. 129-158.   [32] G. Siemens, Massive Open Online Courses: Innovation in  Education, in R. McGreal, W. Kinuthia, S. Marshall and T.  McNamara, eds., Open Educational Resources: Innovation,  Research and Practice, Commonwealth of Learning and  Athabasca University, Vancouver, 2013, pp. 5-15.   [33] G. Stahl, Group Cognition: Computer Support for  Collaborative Knowledge Building, MIT Press, Cambridge,  MA, 2006.   [34] D. D. Suthers, Technology affordances for intersubjective  meaning-making: A research agenda for CSCL, International  Journal of Computer Supported Collaborative Learning, 1  (2006), pp. 315-337.   [35] D. D. Suthers and C. Desiato, Exposing chat features  through analysis of uptake between contributions, Proc.  Hawaii International Conference on the System Sciences  (HICSS-45), January 4-7, 2012, Grand Wailea, Maui,  Hawaii (CD-ROM), Institute of Electrical and Electronics  Engineers, Inc. (IEEE), New Brunswick, 2012.   [36] D. D. Suthers and N. Dwyer, Identifying uptake, sessions,  and key actors in a socio-technical network, Proc. Hawaii  International Conference on the System Sciences (HICSS- 44), January 5-8, 2011, Kauai, Hawaii (CD-ROM), Institute  of Electrical and Electronics Engineers, Inc. (IEEE), New  Brunswick, 2015, pp. CD-Rom.   [37] D. D. Suthers, N. Dwyer, R. Medina and R. Vatrapu, A  framework for conceptualizing, representing, and analyzing  distributed interaction, International Journal of Computer  Supported Collaborative Learning, 5 (2010), pp. 5-42.   [38] D. D. Suthers, J. Fusco, P. Schank, K.-H. Chu and M.  Schlager, Discovery of community structures in a  heterogeneous professional online network, Proc. Hawaii  International Conference on the System Sciences (HICSS- 46), January 7-10, 2013, Grand Wailea, Maui, Hawaii (CD- ROM), Institute of Electrical and Electronics Engineers, Inc.  (IEEE), New Brunswick, 2013.   [39] D. D. Suthers, K. Lund, C. P. Ros, C. Teplovs and N. Law,  Productive Multivocality in the Analysis of Group  Interactions, Springer, New York, 2013.   [40] D. D. Suthers and D. Rosen, A unified framework for multi- level analysis of distributed learning in G. Conole, D.  Gaevi, P. Long and G. Siemens, eds., Proc. First  International Conference on Learning Analytics &  Knowledge, Banff, Alberta, February 27-March 1, 2011,  ACM, New York, NY, 2011, pp. 64-74.   [41] S. Trausan-Matu and T. Rebedea, A polyphonic model and  system for inter-animation analysis in chat conversations  with multiple participants, in A. Gelbukh, ed.,  Computational Linguistics and Intelligent Text Processing,  Springer, Berlin, 2010, pp. 354-363.   [42] S. Wasserman and K. Faust, Social Network Analysis:  Methods and Applications, Cambridge University Press, New  York, 1994.      377      "}
{"index":{"_id":"60"}}
{"datatype":"inproceedings","key":"Mouri:2015:ULA:2723576.2723598","author":"Mouri, Kousuke and Ogata, Hiroaki and Uosaki, Noriko","title":"Ubiquitous Learning Analytics in the Context of Real-world Language Learning","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"378--382","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723598","doi":"10.1145/2723576.2723598","acmid":"2723598","publisher":"ACM","address":"New York, NY, USA","keywords":"network analysis, network graph, time map, ubiquitous learning log","abstract":"This paper describes a method of the visualization and analysis for mining useful learning logs from numerous learning experiences that learners have accumulated in the real world as the ubiquitous learning logs. Ubiquitous Learning Log (ULL) is defined as a digital record of what learners have learned in the daily life using ubiquitous technologies. It allows learners to log their learning experiences with photos, audios, videos, location, RFID tag and sensor data, and to share and reuse ULL with others. By constructing real-world corpora which comprise of accumulated ULLs with information such as what, when, where, and how learners have learned in the real world and by analyzing them, we can support learners to learn more effectively. The proposed system will predict their future learning opportunities including their learning patterns and trends by analyzing their past ULLs. The prediction is made possible both by network analysis based on ULL information such as learners, knowledge, place and time and by learners' self-analysis using time-map. By predicting what they tend to learn next in their learning paths, it provides them with more learning opportunities. Accumulated data are so big and the relationships among the data are so complicated that it is difficult to grasp how closely the ULLs are related each other. Therefore, this paper proposes a system to help learners to grasp relationships among learners, knowledge, place and time, using network graphs and network analysis.","pdf":"Ubiquitous Learning Analytics in the Context of Real- world Language Learning  Kousuke Mouri  Kyushu University   744 Motooka, Nishi-Ku  Fukuoka, 819-0395, Japan   +81-92-802-5875  mourikousuke@gmail.com   Hiroaki Ogata  Kyushu University   744 Motooka, Nishi-Ku  Fukuoka, 819-0395, Japan   +81-92-802-5875  hiroaki.ogata@gmail.com   Noriko Uosaki  Osaka University   1-1 Yamadaoka    Suita, 565-0871, Japan  +81-6-6879-4013   n.uosaki@gmail.com       ABSTRACT  This paper describes a method of the visualization and analysis for  mining useful learning logs from numerous learning experiences  that learners have accumulated in the real world as the ubiquitous  learning logs. Ubiquitous Learning Log (ULL) is defined as a  digital record of what learners have learned in the daily life using  ubiquitous technologies. It allows learners to log their learning  experiences with photos, audios, videos, location, RFID tag and  sensor data, and to share and reuse ULL with others. By  constructing real-world corpora which comprise of accumulated  ULLs with information such as what, when, where, and how  learners have learned in the real world and by analyzing them, we  can support learners to learn more effectively. The proposed  system will predict their future learning opportunities including  their learning patterns and trends by analyzing their past ULLs.  The prediction is made possible both by network analysis based  on ULL information such as learners, knowledge, place and time  and by learners' self-analysis using time-map. By predicting what  they tend to learn next in their learning paths, it provides them  with more learning opportunities. Accumulated data are so big  and the relationships among the data are so complicated that it is  difficult to grasp how closely the ULLs are related each other.  Therefore, this paper proposes a system to help learners to grasp  relationships among learners, knowledge, place and time, using  network graphs and network analysis.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Computer Supported  Ubiquitous Learning   General Terms  Measurement, Performance, Design   Keywords  Ubiquitous learning log, network analysis, time map, network   graph   1. INTRODUCTION  In recent years, many researchers in the educational engineering  area have been studying focusing on ubiquitous theme. For  example, CSUL (Computer Supported Ubiquitous Learning) or  context-aware ubiquitous learning (u-learning) is defined as a  technology-enhanced learning environment supported by  ubiquitous computing technologies such as mobile devices, QR- code, RFID tag and wireless sensor networks [1], [2]. These  learning take place not only in-class learning but also in a variety  of out-class learning space such as home, library and museum.    For example, Hwang et al. reported about their ubiquitous  learning system that targeted personal learners using PDA  (Personal Digital Assist) at an elementary school nature science  course in Taiwan [3]. In their developed system, the learners can  develop a concept map [4] based on what they have learned from  text book. Also, the learners can revise and review their concept  map using PDA in the field. Similarly, Ogata et al. reported about  their ubiquitous learning system called SCROLL [5]. The system  will allow them to share with others by recording what they have  learned on web browser and mobile device.   These learning dataset in the ubiquitous learning system include  spatiotemporal data. Spatiotemporal data usually contain the states  of an object, an event or a position in space over a period of time.  These datasets might be collected at different locations, various  time points in different formats. It poses many challenges in  representing, processing, analysis and mining of dataset due to  complex structure of spatiotemporal objects and the relationships  among them in both spatial and temporal dimensions [6], [7].  Similarly, it poses many issues about relationship between the  learners and the ubiquitous learning logs due to complex structure  of the ubiquitous learning logs in SCROLL. In addition, it is  important for learners to recognize what and how they have  learned by analyzing and visualizing the past ULLs, so that they  can improve what and how to learn in future [2]. To tackle these  issues, it is necessary to reveal relationships between the learners  and the ubiquitous learning logs.   Therefore, this paper proposes a system to analyze and visualize  relationships between the learners and the ubiquitous learning logs  in SCROLL using Time-map and network graph. The objective of  this study will reveal what and how they have learned in their  daily lives, so that the system will recommend the prediction such  as knowledge, place and time in the next learning step to the  learners by analyzing them. By realizing these challenges, the  significances and contributions of this paper is to allow the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA  Copyright 2015 ACM 978-1-4503-3417-4/15/03$15.00   http://dx.doi.org/10.1145/2723576.2723598   378    learners to predict the method of learning new knowledge in the  next step at the real time in the real world. This research is a  necessary precursor in order to realize an innovative learning  method in the next-generation education which integrates  Augmented Reality (AR) and ubiquitous technologies.   2. Related Works   2.1 Time Map  Time-map is a library of javascript, which collaborated with  Google map and SIMILE (Semantic Interoperability of Metadata  and Information in unLike Environments) TimeLine (SIMILE  project) [8]. SIMILE focuses on developing robust, open source  tools that empower users to access, manage, visualize and reuse  digital assets. The time-map function means that the user can  scroll the timeline and then the Google map will display the  learning logs recorded during learners selected period. It is  designed to help learners to reflect what they have learned. For  example, if a learner clicks his learning logs on timeline, Google  map will display their positions as shown in Figure 1. After  visualizing log information, Time-map will facilitate learners to  reflect on their logs with spatio and temporal information. They  are able to grasp their learning context and time zone. Also, it is a  possibility that the geographic information is a clue of recalling  what they have learned.       Figure 1. Time Map   2.2 Network Analysis  Network analysis is a method of effective analysis to reveal  relevance of the data and educational improvement in the future.  For example, Shane et al. analyzed to reveal relationships between  the citation and all paper that were published in the first three  editions of the international conference on LAK (LAK 2011, 2012  and 2013) [9]. They have reported that for the further  development of this field it is important to reflect on the diversity  of disciplines and methodologies that have contributed to the  current state of learning analytics research.    This paper was analyzed using the following well-established  measures in social network analysis, in order to reveal  relationships between the learners and the ubiquitous learning logs  [10], [11].  1. Degree: the number of edges a node has in a network  2. Closeness: the distance of a node to all other nodes in the   network   3. Betweenness: the number of shortest paths between any two  nodes that pass via a given node   This system was constructed to automatically visualize and  analyze relationships between the learners and the ubiquitous  learning logs in accordance with each learner. Also, all social  networking variables were computed using Gephi open source  software for social network analysis [12].    3. Design of the system  3.1 SCROLL  With the evolution of the mobile device, people prefer to record  learning contents using mobile devices instead of taking memos  on paper. SCROLL allows the learners to record learning logs  using mobile device and personal computer. One of the objectives  of SCROLL is to support international students in Japan to learn  Japanese language from what they have learned formal and  informal setting. It adopts an approach of sharing user created  contents among users and is constructed based on a LORE (Log- Organize-Recall-Evaluate) model which is shown in Figure 2 [5].       Figure 2. LORE model   3.2 How to collect the ubiquitous learning log  As shown in Figure 3, the learners can record the ubiquitous  learning log with a photo using mobile device and SCROLL. The  learning log includes meta-data such as author, language, created  time, location (latitude and longitude) and tag, and the learners  can search target ubiquitous learning log object (ULLO) by them.         Figure 3. An example of adding a ULLO   3.3 Three-Layer structures in SCROLL  To visualize and analyze several relationships between the  learners and the ubiquitous learning logs, we have uniquely  defined them as three-layers structures as shown in Figure 4.   379    The upper layer contains each author in order to confirm position  of own or other learners. For example, if a learner learned various  ubiquitous learning logs on SCROLL, there is a possibility that  other learners had already learned it. Therefore, when the learner  learned them, they can grasp other learners status with past  learning experiences.   The intermediate layer contains the knowledge that learners  learned. Also, some fields of learning tasks can be included in this  layer. For example, some task-based learning in ubiquitous  learning environment can be carried out using knowledge and  event [14]. The scalability of the layers can be enhanced and the  field of visualization can be widened by linking ones own  learning logs to the knowledge learned by doing tasks.   The lowest layer contains data such as location and time. The  layer allows the learners to grasp when and where they have  learned by revealing place and time.   Analysis by categorizing three-layers has following advantages:   1. Places with a large number of links to the related knowledge  are the places where they can learn a lot of knowledge. For  example, if a certain supermarket or convenience is related  with a lot of knowledge such as natto (a traditional Japanese  food made from fermented soybeans), green soy beans, tofu,  miso soup, and cup noodle, by analyzing relationships  between the knowledge and the location. The System can  provide learners with a valuable learning information.   2. Knowledge which is related to many places is the  knowledge which we can learn in various places. For  example, if a learner experience tea ceremony of a  traditional Japanese culture at the university in Japan, a set  of tea ceremony related knowledge (e.g. tea, seize: to sit in  the correct manner on a Japanese tatami mat) can be learned  in other various places. The tea can be learned by  purchasing at the supermarket and the seiza can be learned  at the martial art gym.     Figure 4. Three-layer structure in SCROLL   4. Implementation  4.1 The layout types of the network graph  In this paper, we have implemented the network layout as shown  in Figure 5. The network layout consists of using three basic  layouts and an original layout we developed.   The first layout consists of using the random network as shown in  Figure 5 (1). It is simple algorism generating them randomly on  the graph after filtering some nodes, and then the system will link  related node and node.   The second layout consists of using force-directed layout as  shown in Figure 5 (2). It is a force vector algorithm proposed in  the Gephi software, appreciated for its simplicity and for the   Figure 5. The layouts of network graph   380    readability of the network it helps to visualize [15].   The third layout consists of using Yifan Hu multilevel layout as  shown in Figure 5 (3) [16]. It is a very fast algorithm to reduce the  complexity. The repulsive forces on one node from a cluster of  distant nodes are approximated by a Barnes-Hut calculation,  which treats them as one super-node [17].   The final layout consists of using the original layout we have  developed as shown in Figure 5 (4a). The original layout will be  categorized four areas as shown in Figure 5 (4b). The word- centered on collocational network is shown in time-series order  what they have learned. Similarly, the place-centered on  collocational network is visualized the place linking each other,  and the time-centered on collocational network is visualized the  time linking each other.   4.2 Color of visualized nodes  The learners might get confused when they recognize past  learning logs because there might be too many of visualized nodes.  Therefore, it is definitely necessary to establish some criteria for  distinction of each node. To effectively distinguish kind of each  node, we defined as below using node color.     Pink color node shows the learners own name on the upper  layer. If connecting the pink node to yellow node on the  intermediate layer, edge color will be decided as pink so that  they can be easily recognized as the learners own logs    Blue color node shows the names of other learners on the  upper layer. If connecting the blue node to yellow color  node on the intermediate layer, edge color is decided blue  color.    Green color node shows the names of veteran or famous  learner on the upper layer. Ig connecting the yellow node to  the green node on the intermediate layer, edge color will be  decided as green color.    Yellow color node shows both the learner own knowledge  and the knowledge of other learners. For example, the   learner can recognize his own knowledge because edge  between the learner own name on the upper layer and the  knowledge on the intermediate layer is pink color. In  addition, the learner might discover knowledge of other  learners related to own knowledge.    White color node shows the location of the learners on the  lowest layer. The node includes latitude, longitude, building  names and the attributes.    Brown color node shows created time of the knowledge on  the lowest layer. The created time is categorized to attributes  such as  morning ,  midday ,  evening ,  Spring ,   Summer ,  fall  and  winter .   4.3 System interface for visualizing and  analyzing  The interface combining network graph and time-map for  visualizing relationships between the learners and ubiquitous  learning logs is shown Figure 8. It consists of the following  components:    1. Search form: This input form is used to search target word  (e.g. natto and tofu) on the all networks of SCROLL.   2. Layout form: The learners will choose one layout in this  select form (e.g. Random layout, Force-directed layout,  Yifan multilevel layout and original layout).   3. Network graph: The network graph shows the layout  calculated by the system, and the layout in Figure 9 shows a  sample of the original layout. Also, the network graph and  time map function are linked each other. For example, if a  learner clicked a certain node on the network graph, the time  map will show the location and time corresponding to it.  Therefore, learners can obtain its location and time  information.   4. Time map: Time map function consists of the timeline and  Google map. It represents the shift of learning history in   Figure 6. Interface of this system   381    accordance with lapse of time. Learners might forget their  learning logs when and where they have learned before.  Therefore, the system will remind them of their learning  logs recorded during the specified period of time by  showing them on the timeline (default: two month before  and after the setting time). Besides, the system will lead  them to be aware of knowledge recorded right before or  after the knowledge of their interest which was recorded by  other learners. Therefore, it will give them a clue on what to  learn in the next learning step.   5. Analysis results (Knowledge): The analysis by knowledge is  shown as a trend ranking in order to expand their learning  opportunity. By arranging ULLs in the in-degree centrality  order, they will know ULLs that they are likely to study in  the next step. That way they are able to have more learning  opportunity.   6. Analysis results (Place): Place analysis is based on locations  where they have learned. The place of high importance  means the location where there are a lot of opportunities to  learn if visiting them. It is analyzed in the same way as in- degree centrality analysis, and the system shows the results  to the learners.   5. Conclusion and Future Work  This paper describes the system for analyzing and visualizing  relationships between the learners and the ubiquitous learning logs.  International students can add their knowledge as the ubiquitous  learning log in SCROLL, and then SCROLL can provide learning  contents to recall what they learned based on their learning  contexts.    As future works, it is necessary to recommend and present past  learning logs on the system in accordance with each learner's  condition detected from some results such as social analysis,  association analysis and decision tree. In addition, it is also  necessary to evaluate whether detected analysis results are  appropriate or not.   6. ACKNOWLEDGMENTS  The part of this research work was supported by the Grant-in-Aid  for Scientific Research No.25282059, No.26560122, No.  25540091 and No.26350319 from the Ministry of Education,  Culture, Sports, Science and Technology (MEXT) in Japan. The  research results have been partly achieved by Research and  Development on Fundamental and Utilization Technologies for  Social Big Data (178A03), the Commissioned Research of  National Institute of Information and Communications  Technology (NICT), Japan.   7. REFERENCES    [1] Ogata, H., and Yano, Y. (2004). Context-aware support for   computer-supported ubiquitous learning. Proc. of IEEE  International Workshop on Wireless and Mobile  Technologies in Education, 27-34.    [2] Hwang, G. J., Tsai, C. C. and Yang, S. J. H. (2008). Criteria,  strategies and research issues of context-aware ubiquitous  learning. Educational Technology and Society, 11(2), 81-91.   [3] Hwang, G. J., Wu, P.H. and Ke, H.R. (2011). An interactive  concept map approach to supporting mobile learning  activities for natural science courses, Computers and  Education, vol.57, no.4, pp.2272-2280.   [4] Joseph, D.N. and Alberto, J.C. (2006). The Origins of the  concept mapping tool and the continuing evolution of the  tool, Information Visualization, no.5, pp.175-184.   [5] Ogata, H., Li, M., Bin, H., Uosaki, N., El-Bishoutly, M., &  Yano, Y. (2011). SCROLL: Supporting to share and reuse  ubiquitous learning logs in the context of language learning.  Research and Practice on Technology Enhanced Learning, 6  (3), pp.69-82.   [6] K.Venkateswara R, A.Govardhan and Dr.K.V.Chalapati R.  (2011). Discovering Spatiotemporal Topological  Relationships, The second international workshop on  Database Management Systems, DMS-2011,Springer  Proceedings LNCS-CCIS 198.   [7]  K.Venkateswara R & A.Govardhan. (2012). Spatiotemporal  data mining: Issues, Task and Applications, International  Journal of Computer Science and Engineering Survey, Vol.3,  No.1, Feb2012, pp.39-52.   [8] SIMILE project: http://www.simile-widgets.org/timeline/  [9] Shane, D., Dragon, G., George, S. and Srecko, J. (2014).   Current State and Future Trends: A Citation Network  Analysis of the Learning Analytics Field, International  conference on Learning Analytics and Knowledge (LAK  2014) ACM, pp.231-240.    [10] Freeman, L.C. (1978). Centrality in social networks  conceptual clarification, Social Networks, vol.1, no.3,  pp.215-239.   [11] Watts, D.J and Strogatz, S.H. (1998). Collective dynamics of  small world networks, Nature 393, 6684, pp.440-442.   [12] Bastian, M., Heymans, S and Jacomy, M. (2009). Gephi: An  open source Software for Exploring and Manipulating  Networks, In Proceedings of the Third International AAAI  Press.    [13] Li, M., Ogata, H., Hou, B., Uosaki, N., & Yano, Y. (2012).  Personalization in Context-aware Ubiquitous Learning-Log  System,  Mobile and Ubiquitous Technology in Education,  pp.41-48.   [14] Mouri,K., Ogata, H., Li, M., Hou, B., Uosaki, N., & Liu, S.  (2013). Learning Log Navigator: Supporting Task-based  Learning Using Ubiquitous Learning Logs, Journal of  Research and Practice on Technology Enhanced Learning  (RPTEL), 8(1), pp.117-128.   [15] Mathieu, J., Tommaso, V., Sebastien, H and Mathieu, B.  (2014). ForeceAtlas2, a Continuous Graph Layout Algorithm  for Handy Network Visualization Designed for the Gephi  Software, in PLOS One 9(6):e98679. doi: 10.1371/ journal.  pone. 0098679, Jun 10.   [16] Y. F. Hu. (2005). Efficient and high quality force-directed  graph drawing, The Mathematica Journal, vol. 10, pp.3771.   [17] J. Barnes and P. Hut. (1986). A Hierarchical O (n log n)  Force-Calculation Algorithm, Nature, 324(4), pp.446-449.     382      "}
{"index":{"_id":"61"}}
{"datatype":"inproceedings","key":"Zhu:2015:ESU:2723576.2723591","author":"Zhu, Mengxiao and Feng, Gary","title":"An Exploratory Study Using Social Network Analysis to Model Eye Movements in Mathematics Problem Solving","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"383--387","numpages":"5","url":"http://doi.acm.org/10.1145/2723576.2723591","doi":"10.1145/2723576.2723591","acmid":"2723591","publisher":"ACM","address":"New York, NY, USA","keywords":"eye tracking, network analysis, problem solving","abstract":"Eye tracking is a useful tool to understand students' cognitive process during problem solving. This paper offers a unique perspective by applying techniques from social network analysis to eye movement patterns in mathematics problem solving. We construct and visualize transition networks using eye-tracking data collected from 37 8th grade students while solving linear function problems. By applying network analysis on the constructed transition networks, we find general transition patterns between areas of interest (AOIs) for all students, and we also compare patterns for high- and low-performing students. Our results show that even though students share general transition patterns during problem solving, high-performing students made more strategic transitions among AOI triples than low-performing students.","pdf":"      An Exploratory Study Using Social Network Analysis to Model  Eye Movements in Mathematics Problem Solving    Mengxiao Zhu  Educational Testing Service   660 Rosedale Rd  Princeton, NJ   mzhu@ets.org   Gary Feng  Educational Testing Service   660 Rosedale Rd  Princeton, NJ   gfeng@ets.org   ABSTRACT  Eye tracking is a useful tool to understand students cognitive  process during problem solving. This paper offers a unique  perspective by applying techniques from social network analysis  to eye movement patterns in mathematics problem solving. We  construct and visualize transition networks using eye-tracking  data collected from 37 8th grade students while solving linear  function problems. By applying network analysis on the  constructed transition networks, we find general transition  patterns between areas of interest (AOIs) for all students, and we  also compare patterns for high- and low-performing students. Our  results show that even though students share general transition  patterns during problem solving, high-performing students made  more strategic transitions among AOI triples than low-performing  students.   Categories and Subject Descriptors  H.5.m. [Information interfaces and presentation (e.g., HCI)]:  Miscellaneous; E.1 [Data Structures]: Graphs and networks.   General Terms  Measurement, Experimentation, Performance   Keywords  Eye tracking, network analysis, problem solving   1. INTRODUCTION  One way to study why some students perform better than others  while solving problems is to describe how they solve problems  differently. However, mental processes during problem-solving  are usually not directly observable. Researchers have used various  methodologies to measure cognitive processes using techniques  such as eye tracking (e.g., [24]), EEG (e.g., [15]), and action logs  (e.g., [14]) or keystroke data (e.g., [1]) automatically recorded by  the servers for computerized tasks.   In this paper, we explore the use of one of these techniques,  eyetracking, to study students eye-movement patterns during  mathematics problem solving. While most previous studies  focused on analyzing the length of time that students spent on   different areas [17, 18] on the paper or the screen, this paper takes  a different approach by investigating the dynamic process and the  transitions among different areas that students look at during  problem solving. While transitions among AOIs have long been  studied in eye-tracking research [12], the methods are often task- specific and difficult to apply in the analysis of complex tasks.  Here we introduce techniques from the field of social network  analysis (SNA) [26], adapting them for analyzing transitions of  eye movements. Our empirical data come from a mathematics  task that assesses middle school students understanding of linear  functions. In our dataset, eye movement data were collected from  37 8th grade students. We analyzed the transition patterns of all  students and also compared the differences between high- performing and low-performing students.  This paper is organized as follows. Section 2 reviews related  studies and applications of eyetracking, especially in an education  setting. Section 3 describes the dataset and the Moving Sidewalks  task used in this study, as well as the general background of the  CBAL initiative (Cognitively Based Assessment of, for, and as  Learning; [4]) and learning progression. Section 4 introduces the  network measures applied for eye-tracking analysis, network  visualization, and related results. Section 5 extends the network  analysis to compare students from different performing groups.  Section 6 includes discussion and future directions for additional  studies. In all analyses and comparisons, we focus on general  applications of the SNA techniques and give only high-level  interpretation of results. Substantive interpretation in terms of the  meaning of results for solving the linear functions task is left for  future studies.   2. RELATED WORK  Eye movements provide moment-by-moment information on  visual attention, which is a close indicator of the information  being processed [6, 22]. Many studies have used eye tracking to  study human reading behavior. One of the first models of eye  movements in reading developed by Just and Carpenter [13]  proposed the eye-mind principle, that is, wherever the mind  wants to see, the eye moves to. It is not surprising that much of  their model was a production system simulating cognitive  processes in reading. The eye-mind assumption was soon found  overly simplistic (see [22]) and more sophisticated models  emerged to account for oculomotor factors in eye movements  (e.g., [6]). But this does not change the larger picture, that is, that  the eye gaze is a proxy of the attention spotlight [7, 21]. This  explains why eyetracking has attracted increasing attention among  assessment researchers [8, 9].   In educational research, eye movements have been used to study  students problem solving processes and performance in various  domains. For instance, the study by Tai, Loehr, & Brigham [24]  collected eye-tracking data on six preservice science teachers  while they solved an 18-item multiple-choice science assessment.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.   LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA  Copyright is held by the owner/author(s). Publication rights licensed to  ACM.  ACM 978-1-4503-3417-4/15/03$15.00  http://dx.doi.org/10.1145/2723576.2723591   383          T p d  I e m f c s d t q s r i s s R p  B p d d t t i d A c a t t s t b s  3 T i c b l c o I t v m O s t O t T  p b s s  The results sho participants wit different discipl  In several r experiments an movement patt format of rearr choice items [1 shared goal of  differences in e the questions  questions corre students who a relevant areas m incorrectly tend studies, variou such as ANOV Random Forest patterns [13] on  Besides analyz problems, anot data is to loo different AOIs transition matri time and numb in the tracking density, numbe AOIs [12], and can then be a analysis of tra tracking studies the education f spent looking  transitions amo by focusing o students proble  3. THE DA The data in this initiative at ET completing the based mathem linear functions construct analy of how students In this study, w task (as shown  visual layout an more in-depth a On this page, s section of the s the left section. Our dataset inc the Moving Si Tobii T120 eye 120Hz and has  performance in based on a lear student spent 2 solve all subque  owed that the e th similar scienc lines.    recent studies  nd analyses t terns for studen ranging simple  17, 25], and a  these studies w  eye-movement  correctly and   ectly. An impo answered the qu more, while stu ded to focus mo us analysis met VA [18], and m  t [3], were ado n certain areas fr  zing where stu ther potential w ok at the trans s. These trans ices [12], whic er of transitions  g range. Basic  er and proportio d advanced mo applied to anal ansition matrice s. Most studies  field, focused   at individual  ong these areas.  on understandin em solving proc  ATASET  s study were col TS. Eighth-grad e Moving Sidew  matics assessme s [10]. The ass sis of the doma s understanding we focus on Qu in Figure 1) be  nd the cognitiv analyses, especi tudents need to  screen using inf     cludes 37 eighth idewalks task.  e-tracker, which a typical accur  n the assessmen rning progressio 92.23 seconds  estions in Quest  eye-gaze pattern ce backgrounds  researchers  to study the  nts while solvi algebraic equa more complex   was to assess w patterns of stu those who d  ortant finding [ uestions correct udents who ans ore on irrelevan thods including achine learning opted to comp  from different gr  udents focused way to analyze sitions that stu sitions can be ch can be used  s between any t statistical analy  on of specific s dels, such as M lyze these mat es is not yet w reviewed above on the direct c  AOIs, instea In this paper, w  ng the transit cesses.    llected as part o de students wer walks One-Rid  ent of students essment task w  ain and a learnin g of linear funct uestion 4 in the ecause of the co ve processes inv ially the analysi o answer four q formation provi  h grade particip Eye tracking  h samples the  racy of 0.5 visu nt was rated on  on framework [ (with SD = 149 tion 4.   ns are similar a s and different a  conducted si difference of   ing problems i ations [23], mul  format [3, 18] whether there ar udents who answ did not answe [3, 18, 25] wa tly tended to lo swered the que nt areas. In the a g statistical m g techniques, su are the eye fix roups.    d on while so e the eye-move udents made a e represented   to record the  two predefined ysis, such as m subscans, and u Markov models trices. However widely used in e, especially tho comparisons of ad of studying we try to fill thi tion patterns d  of the CBAL res re eye-tracked  der task, a scen s understandin  was integrated w ng progression m tions develops [ e Moving Side omplexity in bo volved, which a is of AOI transi  questions on the ided in the figu  pants who comp was done usin eye gaze locati  ual degrees. Stud a 4-level scale 2]. On average 9.94) on this pa  among  across   imilar  eye-  in the  ltiple- . One   re any  wered   er the  s that  ook at  stions  above  odels,   uch as  xation   olving  ement   among  using  dwell  areas   matrix  unique   [16],  r, the   n eye- ose in  f time  g the  is gap  during   search  while  nario- ng of  with a  model  [2].    walks   oth the  allows  itions.  e right  ure on   pleted  ng the  ion at  dents   (0-3)  , each  age to   4. NE In  mo sta and  4.1 On sho sho on  sam wh of  occ tho  It  stu as ima ans be  fol exc in  rev Qu bee com Wh ind sta poi  4.2 AO We cor rev (i.e res in  item ref equ the are  EYE MO ETWORK  this section, we  ovement pattern arted with the m d then introduce  1 Visualizi ne way to visual ows the location ow the scan pat the image of th  mple recorded  hich ranges from the sample in t curred before th ose in blue or bl  Figure 1 can be seen f  udent paid a lot  useful informat age on the up swering the que seen from this  llowed the orde ception of a cer the midst of gr  visited Question uestion D). The en less confide me back to do hile scan path dividual studen atistical analyses ints and study tr  2 Transitio OIs  e first define  rresponding to  viewed above i e., the screen) th search hypothes how students s m  for examp fer to, and other uation problem e image, graph,  eas, as shown   VEMENT  ANALYSI  e explore potent ns using the da most basic anal ed AOI-based tr  ing the Sca lize eye movem n of the line of th of one of the he screen that th by the eye-tra  m dark red to b the recording; f hose in yellow, lack.    1. A sample sca from this visua of attention to   tion provided in pper left provi estions and thus s visualization t er of the questi rtain part in Que reen and cyan  n C at the ver e pattern sugge ent with the an ouble-check afte h plots can r nts problem-so s are needed. A ransition pattern  on Matrix a  AOIs and the the eye movem  in Section 2, A hat are a meani  sis or design [12 witch their atte  ple, the questio r part of the scr . We defined 1 questions, and  in Figure 2. T  PATTERN IS  tial ways to ana  ataset described  lysis/visualizatio ransition pattern  an Paths  ments is the scan f gaze over tim e students in ou he student saw.  acker. The colo black, indicates  for example, ga , which in turn  an path from a alization that,  the questions t  n the graph at th ides only little  was visited les that this specifi ons in answeri estion C (shown ones, indicatin  ry end, after h ests that the st swer for Quest er finishing the  reveal much i olving processe  An alternative is  ns between area  and Netwo  en discuss the  ment patterns b AOIs are region ingful unit of an 2]. In this study ention among va ons, the graph  reen  in order  1 AOIs on this corresponding   To minimize t  Time   NS AND   alyze students e in Section 3. W  on using raw d n analysis.   n path [12], wh me. In Figure 1,   ur sample overl  Each dot show  or of the sampl  the relative or aze samples in  n were earlier th  a student  as expected, t  themselves as w he bottom left. T e information  ss often. It can a fic student roug ng them, with  n by the black d ng that the stud having worked  tudent might ha tion C and had e other questio nformation ab  es, more rigoro to aggregate th  as in the space.  rk between  transition mat between AOIs.  ns in the stimu nalysis for a giv  y, we are interes arious parts of  that the questio to solve the lin  s screen, includ question respon  the effect of e  eye  We   data   hich  we   laid  ws a  les,  rder  red  han      this  well  The  for   also  ghly   the  dots  dent   on  ave   d to  ons.  bout  ous   hese   n   trix  As   ulus  ven  sted  the  ons   near  ding  nse   eye-  384          t w T s r i c d m t  F n t t A t w t a N a  4 N  l t n t  S u  tracking noise  which the AOI  The next step  straightforward represent the A indicate the nu column AOI. T dwell time on  matrices, if we then we can eas  Figure  Figure 3 shows network as in F time spent in  transitions. If th AOIs, the links two extra nodes whole AOI tran transition matri across time su Next, we introd analyze the con  4.3 Netwo Networks   Technique limited in eye- this paper is to network analys tracking studies  SNA studies w understand the   on AOI trans dwell time was  of creating t d. The columns a AOIs in the re umber of trans he diagonal of t the correspon  e consider AOIs sily visualize th  Figure 2. AO  3. AOI Transit s the visualizati Figure 1. The si  that AOI and here are a larger  are darker and s to indicate the nsition sequenc ix and network  uch that the tem duce related net nstructed transiti  rk Measure  es for modeling  -tracking resear o introduce som sis (SNA) [26]  s in the form of   were initially c relationship am  sitions, we rem s less than 0.1 se the transition m and the rows of egion and the  sitions from th this matrix can   nding AOIs. In s as nodes and e transition mat  OIs for Questio  tion Network f ion of the same ize of the node d the directed  r number of tran  d wider. In this g e start point and ce. It is worth n k represent agg mporal order o twork measures ion matrix and n  es for AOI   the AOI transit rch [12, 16]. A  me of the existin to analyze sim transition matri  conducted by s mong human be  moved transitio econd.   matrix [12] is f the transition m  cells in the m he row AOI t be used to reco  n the AOI tran d transitions as  trix using a netw    on 4     for Student 2  e students tran s represents the links represen  nsitions betwee graph, we also a  d the end point f noticing that the gregated inform of transitions is s that can be us network.   Transition  tion matrix are  An important go ng methods of  milar data from ices.   sociologists to  ings. The most   ons in    then  matrix  matrix  to the  ord the  nsition   links,  work.    nsition  e total  nt the  en two  added   for the  e AOI  mation  s lost.  sed to   n   rather  oal of  social   m eye-  better  well-  kno wh con exp bey and me wh SN int sim  4.3 Th net cal the wit nod nod cal the g(g of  len sug eye Re ref per rec B a def net mu dya no  div nul ma two con tem  4.3 To Le pro the pat Ho num ind cha the and tria Th lett T Th stru det For  own early exa hich revealed th nnected to each panded to stud yond human ne d Internet rou ethods are usual hich record the  NA can be appl  erested in stud milar data struct  3.1 Network he two most of twork density  lculated by sum en dividing by t th g nodes and de A to node B  de B to node A) lculate the dens e matrix except g-1). The densi transitions betw  ngth of an AO ggests that the  e gaze between   eciprocity in soc flects the extent rson as their  ciprocity means as well as from fined by Holla twork can be ca utual dyads with ads with one-w links in betwee  viding the numb ll dyads. For A ay occur if a stu o AOIs. The  nsecutively, th mporal order of  3.2 Triad Ce  study pattern inhardt [5] in ovides informat em. Among any tterns as shown olland and Lein mbers and let dicates the num aracter indicate e third character d a potential ex ad for the case  here are four po ter U means   means transiti  he triad census uctures in the  tailed structures r instance, man  ample was Mil he surprising f h other than we  dy the structure etworks, such a uter networks  lly conducted o connections am  lied in eye-trac dying the AO tures to SNA ne  k Density and ften used mea  and reciproc mming the value the total number d if links are di  is considered d ), the total num sity of an AOI  t the diagonal a ty provides inf ween any two   OI transition se student has loo more AOIs.   cial science data t to which one friend. In th  s whether we ob m B to A. Acco and and Leinha ategorized into  h two-way links ay links betwee  en. Reciprocity  ber of mutual d  AOI transition n udents attention  back and fo hough, because the transitions.   ensus  ns extending b  ntroduced the  tion on structure y three nodes, t n in Figure 4. A nhardt [11] and tters. In the n mber of mutual es the number o r indicates the n xtra letter at th  es when the firs ossible letters:  up, the letter   ive.    system provid graph and ena  s between differ ny 030C structu  grams small w fact that people e expected. SN  es of  various t as protein netwo  [20]. Related on data in the fo mong nodes. Ex king studies, e  OI transition pa etworks.   d Reciprocity sures in the S  city [26]. Net es of all links in r of possible lin irected (i.e., a l different than a   mber of possible  transition netw  are summed an formation on th AOIs and is a quence. A hig  oked at more A  a, such as in a fr s friends also   he context of  bserve transitio ording to the dy ardt [11], all li one of the follo  s between two n en two nodes, an [26] of a netwo  dyads by the tot networks, a high n switches back  orth does not  e the recipro    beyond two n system of tria es of three node there are 16 po  A naming conven d Davis and Le name string, th l dyads in the  of asymmetric d number of null  he end indicates st three charact the letter D  C means cyc  des statistics o ables the resear rent students in  ures being obser  world study [1 e are more tigh NA has since be types of netwo orks, power gri  d techniques a format of matric xisting methods specially if one atterns that sh  y  SNA literature  twork density  n the network a nks. For a netw link pointing fr link pointing fr links is g(g-1).   work, all values nd then divided  he average num an indicator of  gher density va OIs and/or shif  riendship netwo consider the fo AOI transitio  ns from AOI A yad census syst inks in a direc owing three typ nodes, asymmet nd null dyads w  ork is calculated tal number of n h reciprocity va and forth betwe have to happ  ocity ignores   nodes, Davis a ad census, wh es and links amo ossible isomorp ntion is defined einhardt [5] us he first charac triad, the seco  dyads in the tri dyads in the tri s the shape of  ters are the sam means down,  clic, and the le  on the three-no rchers to comp eye-tracking da  rved indicates t  19],  htly  een   orks  ids,  and  ces,  s in  e is  hare   are  is   and  ork   rom  rom   To  s in   by  mber   the  alue  fted   ork,  ocal  ons,  A to  tem  cted  pes:  tric   with  d by  on-  alue  een  pen  the   and  hich  ong  phic  d in  sing  cter  ond  iad,  iad,  the  me.  the  tter   ode  pare  ata.  that   385          t f e v  T s a s i a t l r s t o t  F 0 s v A h w s A t d i i s t  the student pre fashion. Since  exclusive, the  visited AOIs.   To get an overv students, we co and calculated  shown in Tab indicates that t any two AOIs i that this student lot of transition relatively high v student moved  the first one ov over 80% of th the time.   Table 1. Net    Density  Reciprocity  003  012  102  021D  021U  021C  111D  111U  030T  030C  201  120D  120U  120C  300   For triad struct 012, and 102. O show that a fai visited together AOIs in our sa have transition words, the un students are no AOIs during pr the triad struc density and re indicates that th in the network suggest that the the missing l  efers to move  all 16 patterns 030C also ind  Figure 4. view of the netw onstructed the t  the above int ble 1). For net there is, on ave n the space. Th t only briefly lo  ns before movin value of mean r from one AOI   ver half of the  he time and eve  twork Statistics  Mean  1.38  0.61   58.32  44.08  62.89   2.95  2.51  6.19   17.73  17.84   1.35  0.78   17.84  2.60  2.95  7.24   21.16   tures, the three On the one han ir number of c r. This makes   ample. For insta ns among only  neven number  t making rando roblem solving tures also sup eciprocity. The here are relative ks. And the hig e densities of th links in these  across differen s in the triad   dicates less revi  . Triad Census  work structures transition netw troduced netwo twork density,  erage, at least   he very small mi ooked at the spa ng to another pa reciprocity of 0 to the next, tha time. The max  n the minimum  s for all 37 Tra  SD  0.79  0.11   42.02  13.49  14.81   1.87  2.02  3.91  5.59  5.70  1.34  1.00  8.35  2.23  2.22  4.91   10.62    most observed nd, high number ombinations of sense given t  ance, it is less l the five respo of triad struc  om decisions on g. On the other  pport the obser e high number ely high numbe gh numbers of  he networks are  e structures.   nt AOIs in a c census are mu isiting of previ     s in our dataset  works for all of  ork statistics (r  the mean of  1 transition bet inimum (0.03) s ace without mak age of questions .61 shows that  at student will r ximum reciproc  m is over one th  ansition Netwo  Min Ma 0.03 4.0 0.35 0.8  22 24 20 6 9 9 0  0  0 1 1 2 1 2 0  0  0 3 0  0  0 2 0 4  d structures are rs of these struc f three AOIs ar the definition o likely for stude onse areas. In  ctures indicates n transitions bet hand, the coun  rvations on ne r of 102 struc  ers of reciprocal f all three struc  not very high,  Another inter  cyclic  utually  iously   of 37  f them  results  f 1.38  tween  shows  king a  s. The  if one  revisit  city is  hird of   rks    ax  03  81  46  69  98  8  9   16  29  26  5  4   37  8  9   20  40   e 003,  ctures  re not  of the  ents to   other  s that  tween  nts on  twork  ctures  l links  ctures  given  esting   obs wh clo lea doe dif eith of   5. LE Be as  com per tran sta lev tas the fun pag Am sco stu (LP per me dif the two  To N stu Lo hav = 2 per 11 SD thr ind Ch be  two the num com to b con num add  servation on tr hich can be ob osed loop witho ast during the p es not require fferent AOIs. It  her revisit prev AOIs.    EYE MO EARNING esides providing  in Section 4.3 mpare students rforming versus nsition pattern  atistics for stude vels. For each q k, students in th  e levels of the nctions. For Qu ge based learnin mong the total o ores 0 to 3, resp udents who got  P04 > 0) and l rformed indepe easures. The r fference on netw e triad structures o groups, includ  Figure  be specific, low = 29) have m  udents (M = 40. w-performing s ve more 021D t 2.13, SD = .99, rforming studen 1U triad structu  D = 3.45, N = 8 ree triad struct dicating standar hecking these th  seen that high- o AOIs and mo e third one from mber of 111U mparison, low-p branch out from nnected in the mber of 021D dition, low-perf  iad structures i bserved when t out reciprocal li problem solving  a long chain  t is highly possi viously visited A  VEMENT  G PROGRE  g overall descri 3, the network s belonging to  s low-performin  ns. In this sec ents based on th question page i he sample were e learning prog uestion 4, we u ng progression l of 37 students,  pectively, for LP scores higher th low- (LP04 =  endent sample  results indicate work density or s, three are foun ding 003, 021D  e 5. Differences w-performing s  more 003 triad  63, SD = 20.25 students (M =  triad structures t , N = 8), t(24)  nts (M = 16.90, ures than high-p 8), t(20) = -2.6 tures are plotte d errors.   hree triad struct performing stud ove back and fo m one of these   U structures fo performing stud  m one AOI to tw ir gazing patte  D structures fo forming student  is the least obs the visits to th inks. This resul g in this curren  n of sequential ible that studen AOIs or move o  PATTERN SSION LE iptive informati k statistics can  different grou ng, to identify d ction, we comp heir different lea in the CBAL M  e evaluated by a gression in und use variable LP level.   29, 0, 5, and 3 P04. Due to the han 0, we split s 0) performing  t tests on all  e that there  r reciprocity. A nd significantly , and 111U.    s on Triad Stru students (M = 6  structures than 5, N = 8), t(27) 3.17, SD = 2.  than high-perfo = 2.05, p < 0.0 , SD = 5.88, N  performing stud 7, p < 0.05. Th ed in Figure 5  tures as shown  dents tend to fo  forth between th two (as indica  r high-perform dents have less  wo AOIs, which ern (as indicat or low-perform ts tend to find   served one, 030 hree AOIs form lt indicates that nt case, it usua l single visits  nts will be need on to other tripl  NS AND  VELS   ion for the sam n also be used  ups, that is, hi differences in th pare the netw arning progress Moving Sidewa a content expert derstanding lin  P04 to indicate    students receiv e small number students into hi groups. Next,  available netw is no signific t the .05 level,   y different betwe  uctures  63.21, SD = 45. n high-perform  = 2.04, p < 0. .00, N = 29) a  orming students 05. However, lo  N = 29) have few dents (M = 21. he means for th 5 with error b  in Figure 4, it c ocus on a subset hese two and v ated by the hig  ming students).   strategy and te  h are never direc ted by the hig  ming students).  fewer connectio  0C,  m a  t, at  ally   of  d to  lets   mple  d to  gh- heir  ork   sion  alks  t on  near   the   ved  r of  gh-  we  ork   cant  for  een      31,  ming   05.  also  (M   ow- wer  25,   hese  bars   can  t of   visit  gher   In  end  ctly   gher  In   ons   386          between triples of different AOIs (as indicated by the higher  number of 003 structures for low-performing students).    6. DISCUSSION AND FUTURE WORK  In this paper, we constructed eye-movement transition networks  and conducted related analysis on these networks. The results  show that, in general, students did not make random decisions on  transition among AOIs and they tend to revisit previously visited  ones. Comparing high- and low-performing students in terms of  their mastery of linear functions, it was found that low-performing  students are more likely to consider information in isolation,  whereas high-performing students tend to connect multiple  sources of information in solving complex math problems.   As in any other methods using transition matrices, the network  analyses in this paper share the limitation of partially ignoring the  sequential information in the eye-tracking data. For instance, in a  triadic structure, the information of which link happened first was  not recorded. Despite this limitation, the analysis still provides us  useful information on students cognitive process during problem  solving.   The current work can be extended in several directions. First,  numerous other network measures can be applied to the AOI  network analysis. For example, the network centrality measure  [26] can provide information on the preferred AOIs by students.  Second, cluster analysis can be conducted on the transition  networks and related measures used to discover latent subgroups  of students, some of which may shed more light on student  cognition than their test scores. Last, the current analysis focuses  only on transition structures and not the content of the AOIs.  Further analysis with AOI information can provide more detailed  information on the transition patterns.   7. ACKNOWLEDGMENTS  Our thanks to our ETS colleagues Dr. James Carlson, Dr. Lei  Chen, and Dr. Paul Deane for their suggestions, and the  anonymous reviewers for their comments.   8. REFERENCES  [1] Almond, R. et al. 2012. A preliminary analysis of keystroke   log data from a timed writing task. ETS Reseach Report RR- 12-23. (2012).   [2] Arieli-Attali, M. et al. 2012. The use of three learning  progressions in supporting formative assessment in middle  school mathematics. The annual meeting of the American  Educational Research Association (Vancouver, Canada,  2012).   [3] Bayazit, A. et al. 2014. Predicting learner answers  correctness through eye movements with random forest.  Educational Data Mining: Applications and Trends. A.  Pea-Ayala, ed. Springer International Publishing. 203226.   [4] Bennett, R.E. 2010. Cognitively based assessment of, for,  and as learning (CBAL): A preliminary theory of action for  summative and formative assessment. Measurement:  Interdisciplinary Research & Perspectives. 8, 2 (2010), 70 91.   [5] Davis, J.A. and Leinhardt, S. 1972. The structure of positive  interpersonal relations in small groups. Sociological Theories  in Progress. J. Berger, ed. Houghton Mifflin. 218251.   [6] Feng, G. 2006. Eye movements as time-series random  variables: A stochastic model of eye movement control in  reading. Cognitive Systems Research. 7, 1 (2006), 7095.   [7] Findlay, J.M. and Walker, R. 1999. A model of saccade  generation based on parallel processing and competitive   inhibition. Behavioral & Brain Sciences. 22, 4 (1999), 661 721.   [8] Gorin, J.S. 2006. Test design with cognition in mind.  Educational Measurement: Issues and Practice. 25, 4 (2006),  2135.   [9] Gorin, J.S. and Embretson, S.E. 2012. Using cognitive  psychology to generate items and predict item characteristics.  Automatic item generation: Theory and practice. M.J. Gierl  and T.M. Haladyna, eds. New York: Routledge. 136  156.   [10] Graf, E.A. et al. 2010. Highlights from the Cognitively Based  Assessment of, for, and as Learning (CBAL) Project in  Mathematics. ETS Research Spotlight. 3, (2010), 1930.   [11] Holland, P.W. and Leinhardt, S. 1970. A method for  detecting structure in sociometric data. American Journal of  Sociology. 76, 3 (1970), 492513.   [12] Holmqvist, K. et al. 2011. Eye tracking: A comprehensive  guide to methods and measures. Oxford University Press.   [13] Just, M.A. and Carpenter, P.A. 1980. A theory of reading:  From eye fixations to comprehension. Psychological Review.  87, 4 (1980), 329354.   [14] Kerr, D. et al. 2011. The feasibility of using cluster analysis  to examine log data from educational video games. CRESST  Report 790. National Center for Research on Evaluation,  Standards, and Student Testing (CRESST). (2011).   [15] Kolm, J. et al. 2013. How long is the coastline of teamwork  A neurodynamic model for group and team operation and  evolution. 15th International Conference on Human- Computer Interaction - Augmented Cognition (2013).   [16] Van Der Lans, R. et al. 2008. Eye-movement analysis of  search effectiveness. Journal of the American Statistical  Association. 103, 482 (2008), 452461.   [17] Lindner, M.A. et al. 2014. Tracking the decision-making  process in multiple-choice assessment: evidence from eye  movements. Applied Cognitive Psychology. 28, 5 (2014),  738752.   [18] Madsen, A.M. et al. 2012. Differences in visual attention  between those who correctly and incorrectly answer physics  problems. Physical Review Special Topics - Physics  Education Research. 8, 1 (2012), 010122.   [19] Milgram, S. 1967. The small world problem. Psychology  today. 2, 1 (1967), 6067.   [20] Newman, M.E.J. 2003. The structure and function of  complex networks. SIAM Review. 45, (2003), 167256.   [21] Posner, M.I. et al. 1980. Attention and the detection of  signals. Journal of Experimental Psychology: General. 109,  2 (1980), 160174.   [22] Rayner, K. 1998. Eye movements in reading and information  processing: 20 years of research. Psychological Bulletin. 124,  3 (1998), 372422.   [23] Susac, A.N.A. et al. 2014. Eye movements reveal students  strategies in simple equation solving. International Journal  of Science and Mathematics Education. 12, 3 (2014), 555 577.   [24] Tai, R.H. et al. 2006. An exploration of the use of eyegaze  tracking to study problemsolving on standardized science  assessments. International Journal of Research & Method in  Education. 29, 2 (2006), 185208.   [25] Tsai, M.-J. et al. 2012. Visual attention for solving multiple- choice science problem: An eye-tracking analysis.  Computers & Education. 58, 1 (2012), 375385.   [26] Wasserman, S. and Faust, K. 1994. Social network analysis:  Methods and applications. Cambridge University Press,  Cambridge.    387      "}
{"index":{"_id":"62"}}
{"datatype":"inproceedings","key":"Knight:2015:TIW:2723576.2723638","author":"Knight, Simon and Wise, Alyssa F. and Chen, Bodong and Cheng, Britte Haugan","title":"It's About Time: 4th International Workshop on Temporal Analyses of Learning Data","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"388--389","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723638","doi":"10.1145/2723576.2723638","acmid":"2723638","publisher":"ACM","address":"New York, NY, USA","keywords":"CSCL, discourse analytics, knowledge building, learning analytics, sequence mining, temporality","abstract":"Interest in analyses that probe the temporal aspects of learning continues to grow. The study of common and consequential sequences of events (such as learners accessing resources, interacting with other learners and engaging in self-regulatory activities) and how these are associated with learning outcomes, as well as the ways in which knowledge and skills grow or evolve over time are both core areas of interest. Learning analytics datasets are replete with fine-grained temporal data: click streams; chat logs; document edit histories (e.g. wikis, etherpads); motion tracking (e.g. eye-tracking, Microsoft Kinect), and so on. However, the emerging area of temporal analysis presents both technical and theoretical challenges in appropriating suitable techniques and interpreting results in the context of learning. The learning analytics community offers a productive focal ground for exploring and furthering efforts to address these challenges. This workshop, the fourth in a series on temporal analysis of learning, provides a focal point for analytics researchers to consider issues around and approaches to temporality in learning analytics.","pdf":"Its About Time: 4th International Workshop on   Temporal Analyses of Learning Data   Simon Knight   Knowledge Media Institute   Open University, UK    sjgknight@gmail.com        Alyssa F. Wise  Faculty of Education   Simon Fraser University   alyssa_wise@sfu.ca      Britte Haugan Cheng   Center for Technology in Learning  SRI International    britte.cheng@sri.com    Bodong Chen  College of Education & Human   Development, University of Minnesota   bodong.chen@gmail.com                   ABSTRACT  Interest in analyses that probe the temporal aspects of learning   continues to grow. The study of common and consequential   sequences of events (such as learners accessing resources,   interacting with other learners and engaging in self-regulatory   activities) and how these are associated with learning outcomes,   as well as the ways in which knowledge and skills grow or evolve   over time are both core areas of interest. Learning analytics   datasets are replete with fine-grained temporal data: click streams;   chat logs; document edit histories (e.g. wikis, etherpads); motion   tracking (e.g. eye-tracking, Microsoft Kinect), and so on.   However, the emerging area of temporal analysis presents both   technical and theoretical challenges in appropriating suitable   techniques and interpreting results in the context of learning. The   learning analytics community offers a productive focal ground for   exploring and furthering efforts to address these challenges. This   workshop, the fourth in a series on temporal analysis of learning,   provides a focal point for analytics researchers to consider issues   around and approaches to temporality in learning analytics.   Categories and Subject Descriptors   K.3.1 [Computers and Education]: Computer Uses in   Education  collaborative learning.   General Terms  Measurement, Design, Human Factors,    Keywords  Learning analytics, temporality, discourse analytics,   knowledge building, sequence mining, CSCL   1. WORKSHOP BACKGROUND  The temporal component of learning has typically been   underexplored in both applied and research contexts [15, 18, 19].   This is a complex issue; temporality involves consideration of   duration, sequence, pace, and salience of target events [21, 27], in   addition to consideration of accretion over time [10, 15, 18]. For   example, while many discussions around MOOCs emphasize   student retention rates by simply counting students online   actions, the analysis of temporal patterns in the clickstream data   tracking student actions has the potential to uncover deeper   insights and provide greater predictive power [4, 13].    Measures and methods for characterizing and analyzing the   temporal evolution of dynamics of group interactions are needed   and emerging [1, 3, 7, 24]. Despite the relative ease of access   learning analytics researchers have to process data (through log   files for example), relatively little research has made use of this   temporal information [24], with most research opting for a   coding and count strategy [as discussed in 25, 28]. With the rise   of online learning and available trace data, the availability of data   for analysis is growing [9], but we should be mindful that bigger   is not necessarily richer; methodological and conceptual work is   needed to develop analytic approaches that leverage the temporal   features of these data sets to make increasingly sophisticated   knowledge claims and diagnostic assessments about learning [23].   In addition new approaches are needed to integrate analysis of   data streams, thereby revealing how phenomena (e.g., mouse   clicks, utterances, gazes, gestures, persistent representations such   as diagrams) co-occur, interact, and facilitate learning, and   furthermore, show how they dynamically affect one another over   time. Such analyses can help reveal dynamic relationships and   support the development of theory and design principles [2].   We are not only interested in how sequences of click-stream data   are related to learning outcomes, but why. Moreover, the   separation of data within clickstreams  which clicks are   associated, how they are chunked into meaningful sequences, and   what objects are available to click  are related to a theorized   account of data representation and segmentation. Greater   understanding of temporality is key here; the very understanding   of an episode or event is tied to temporal notions around the   demarcation of meaningful segments. Issues are more complex   yet, in addition to temporal analyses which consider the   arrangement of events within sequences and the organization of   multiple events over time, there are those which explore time as a   continuous flow of events, examining their positioning, rates, and   duration [20]. Both approaches raise complex questions around   operationalization and data collection [26].    Much recent work (for example the use of use of lag sequential   analysis [8, 22] in [used in 5], t-patterns [16, 17] in [14], pattern-  analyses [e.g. used in 12], and Markov models [see recent   inclusion in the analytic techniques of, 6]) has focused on analysis   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other   uses, contact the Owner/Author.    Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA   ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723638       388  mailto:sjgknight@gmail.com mailto:alyssa_wise@sfu.ca mailto:britte.cheng@sri.com mailto:bodong.chen@gmail.com http://dx.doi.org/10.1145/2723576.2723638   of recurring sequences and their association with learning. While   t-pattern analysis can be used to explore longer, more temporally   separated sequences than LSA and Markov models, all these   techniques are best suited to relatively short recurring sequences   and analysis of event transitions [24]. Therefore, other approaches   will be needed for temporal analysis of accretion and flow or   development over time. For example, in analysis of the unstable   and evolving nature of topics in dialogue, Introne and Dreschler   take as their unit of analysis a sequence of replies, seek[ing] to   understand how clusters of words in these reply sequences   change, merge, and split [11]. Here their interest is in modelling   the statistical properties of the co-occurrence of words over time,   as opposed to modelling probabilities based on dictionary entries   or other corpora. Regardless of focus, fundamental to these   examples is the bringing together of both analytic and theoretical   accounts. The learning analytics community offers a productive   focal ground for exploring and furthering such efforts through its   positioning at the nexus of learning and analytic concerns.   2. REFERENCES  [1] Akhras, F.N. and Self, J.A. 1999. Modeling the process,   not the product, of learning. Computers as cognitive tools.   S. Lajoie P... and S. Derry J..., eds. Lawrence Erlbaum   Associates. 328.   [2] Azevedo, R. and Witherspoon, A.M. 2008. Detecting,   Tracking, and Modeling Self-Regulatory Processes during   Complex Learning with Hypermedia. AAAI Fall   Symposium: Biologically Inspired Cognitive Architectures   (2008), 1626.   [3] Barab, S.A. et al. 2001. Constructing networks of action-  relevant episodes: An in situ research methodology. The   Journal of the Learning Sciences. 10, 1-2 (2001), 63112.   [4] Chen, B. et al. 2015. How do mooc learners intentions   relate to their behaviors and overall outcome Proceedings   of the AERA Annual Meeting (Chicago, Illinois, 2015).   [5] Chen, B. and Resendes, M. 2014. Uncovering what   matters: Analyzing transitional relations among   contribution types in knowledge-building discourse.   Proceedings of the Fourth ACM International Learning   Analytics and Knowledge Conference (Indianapolis, US,   2014).   [6] Chiu, M.M. and Fujita, N. 2014. Statistical Discourse   Analysis of Online Discussions: Informal Cognition, Social   Metacognition and Knowledge Creation. Proceedings of   the Fourth International Conference on Learning Analytics   And Knowledge (New York, NY, USA, 2014), 217225.   [7] Collazos, C.A. et al. 2002. Evaluating collaborative   learning processes. Groupware: Design, Implementation,   and Use. Springer. 203221.   [8] Faraone, S.V. and Dorfman, D.D. 1987. Lag sequential   analysis: Robust statistical methods. Psychological   Bulletin. 101, 2 (Mar. 1987), 312323.   [9] Ferguson, R. 2012. The State of Learning Analytics in   2012: A Review and Future Challenges. Technical Report   #kmi-12-01. The Open University, UK.   [10] Furberg, A. and Ludvigsen, S. 2008. Students Meaning making of Socio scientific Issues in Computer Mediated  Settings: Exploring learning through interaction   trajectories. International Journal of Science Education.   30, 13 (2008), 17751799.   [11] Introne, J.E. and Drescher, M. 2013. Analyzing the flow of   knowledge in computer mediated teams. Proceedings of   the 2013 conference on Computer supported cooperative   work (New York, NY, USA, 2013), 341356.   [12] Kinnebrew, J.S. et al. 2014. Analyzing the temporal   evolution of students behaviors in open-ended learning   environments. Metacognition and Learning. (2014), 129.   [13] Kloft, M. et al. 2014. Predicting MOOC Dropout over   Weeks Using Machine Learning Methods. Modeling Large   Scale Social Interaction in Massively Open Online Courses   Workshop at Empirical Methods in Natural Language   Processing (Doha, Qatar, 2014).   [14] Kuvalja, M. et al. 2014. Patterns of co-occurring non-  verbal behaviour and self-directed speech; a comparison of   three methodological approaches. Metacognition and   Learning. (2014), 125.   [15] Littleton, K. 1999. Productivity through interaction.   Learning with computers: Analysing productive   interaction. K. Littleton and P. Light, eds. 179.   [16] Magnusson, M.S. 2000. Discovering hidden time patterns   in behavior: T-patterns and their detection. Behavior   Research Methods, Instruments, & Computers. 32, 1 (Mar.   2000), 93110.   [17] Magnusson, M.S. 1996. Hidden Real-Time Patterns in   Intra- and Inter-Individual Behavior: Description and   Detection. European Journal of Psychological Assessment.   12, 2 (Jan. 1996), 112123.   [18] Mercer, N. 2008. The seeds of time: why classroom   dialogue needs a temporal analysis. Journal of the   Learning Sciences. 17, 1 (2008), 3359.   [19] Mercer, N. and Littleton, K. 2007. Dialogue and the   Development of Childrens Thinking: A Sociocultural   Approach. Routledge.   [20] Molenaar, I. and Wise, A.F. in preparation. Concepts of   time: A framework for thinking about the temporal aspects   of learning. (in preparation).   [21] Perera, D. et al. 2009. Clustering and sequential pattern   mining of online collaborative learning data. Knowledge   and Data Engineering, IEEE Transactions on. 21, 6   (2009), 759772.   [22] Putnam, L.L. 1983. Small Group Work Climates A Lag-  Sequential Analysis of Group Interaction. Small Group   Research. 14, 4 (1983), 465494.   [23] Reimann, P. et al. 2014. e-Research and learning theory:   What do sequence and process mining methods contribute   British Journal of Educational Technology. 45, 3 (2014),   528540.   [24] Reimann, P. 2009. Time is precious: Variable- and event-  centred approaches to process analysis in CSCL research.   International Journal of Computer-Supported   Collaborative Learning. 4, 3 (Sep. 2009), 239257.   [25] Suthers, D.D. 2006. Technology affordances for   intersubjective meaning making: A research agenda for   CSCL. International Journal of Computer-Supported   Collaborative Learning. 1, (Aug. 2006), 315337.   [26] Winne, P.H. 2014. Issues in researching self-regulated   learning as patterns of events. Metacognition and   Learning. (2014), 19.   [27] Wise, A.F. et al. 2013. Temporal Considerations in   Analyzing and Designing Online Discussions in Education:   Examining Duration, Sequence, Pace, and Salience.   Assessment and Evaluation of Time Factors in Online   Teaching and Learning:. E. Barbera and P. Reimann, eds.   IGI Global.   [28] Wise, A.F. and Chiu, M.M. 2011. Analyzing temporal   patterns of knowledge construction in a role-based online   discussion. International Journal of Computer-Supported   Collaborative Learning. 6, 3 (Sep. 2011), 445470.   389      "}
{"index":{"_id":"63"}}
{"datatype":"inproceedings","key":"Drachsler:2015:EPI:2723576.2723642","author":"Drachsler, Hendrik and Hoel, Tore and Scheffel, Maren and Kismih'ok, G'abor and Berg, Alan and Ferguson, Rebecca and Chen, Weiqin and Cooper, Adam and Manderveld, Jocelyn","title":"Ethical and Privacy Issues in the Application of Learning Analytics","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"390--391","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723642","doi":"10.1145/2723576.2723642","acmid":"2723642","publisher":"ACM","address":"New York, NY, USA","keywords":"data ownership, ethics, learning analytics, legal rights, privacy, surveillance","abstract":"The large-scale production, collection, aggregation, and processing of information from various learning platforms and online environments have led to ethical and privacy concerns regarding potential harm to individuals and society. In the past, these types of concern have impacted on areas as diverse as computer science, legal studies and surveillance studies. Within a European consortium that brings together the EU project LACE, the SURF SIG Learning Analytics, the Apereo Foundation and the EATEL SIG dataTEL, we aim to understand the issues with greater clarity, and to find ways of overcoming the issues and research challenges related to ethical and privacy aspects of learning analytics practice. This interactive workshop aims to raise awareness of major ethics and privacy issues. It will also be used to develop practical solutions to advance the application of learning analytics technologies.","pdf":"Ethical and Privacy Issues   in the Application of Learning Analytics    Hendrik Drachsler  Open University of the Netherlands   Welten Institute  hendrik.drachsler@ou.nl     Gbor Kismihk   University of Amsterdam  Center of Job Knowledge Research   Amsterdam Business School  G.Kismihok@uva.nl     Weiqin Chen   Oslo and Akershus University  College of Applied Sciences   Weiqin.Chen@hioa.no     Tore Hoel  Oslo and Akershus University  College of Applied Sciences   tore.hoel@hioa.no     Alan Berg  University of Amsterdam   ICT Services  a.m.berg@uva.nl        Adam Cooper  University of Bolton   CETIS  a.r.cooper@bolton.ac.uk      Maren Scheffel  Open University of the Netherlands   Welten Institute  Maren.Scheffel@ou.nl     Rebecca Ferguson  The Open University   Institute of Educational Technology  rebecca.ferguson@open.ac.uk        Jocelyn Manderveld   SURF   Jocelyn.Manderveld@surfnet.nl   Abstract  The large-scale production, collection, aggregation, and  processing of information from various learning platforms and  online environments have led to ethical and privacy concerns  regarding potential harm to individuals and society. In the past,  these types of concern have impacted on areas as diverse as  computer science, legal studies and surveillance studies. Within a  European consortium that brings together the EU project LACE,  the SURF SIG Learning Analytics, the Apereo Foundation and the  EATEL SIG dataTEL, we aim to understand the issues with  greater clarity, and to find ways of overcoming the issues and  research challenges related to ethical and privacy aspects of  learning analytics practice. This interactive workshop aims to  raise awareness of major ethics and privacy issues. It will also be  used to develop practical solutions to advance the application of  learning analytics technologies.   General Terms  Algorithms, Measurement, Design, Human Factors, Legal Aspects   Keywords  Learning analytics, ethics, privacy, legal rights, data ownership,  surveillance   1. Introduction  Until now, there have been few papers published relating to ethics  and privacy in the research field known as learning analytics  [1]2,[3][4] and even fewer policies or guidelines regarding  privacy, legal protection rights or other ethical implications that  address Big Data in Education. One exception is a recent policy   published by the Open University UK1. Whereas the law relating  to personally identifiable information (PII) is widely understood,  there has been insufficient attention to privacy from a user- centered perspective, and there are no clearly defined best  practices for scenarios such as anonymisation of educational data.  One of the particular challenges of the context of education is that  it requires a degree of openness on the part of the learner.  Learners perform learning tasks within a learning environment in  order to increase their knowledge and develop competences, and  they expect to receive support to overcome gaps in knowledge.  They also expect to be in a safe environment where they can make  mistakes without fearing any serious consequences.   Using learners behavior and performance data in the context of  learning analytics enables others to determine, visualize, and  report strengths and weaknesses of individual learners and larger  groups. In principle, this has always been the case in education;  however, learning analytics enable the provision of this  information in real time and on demand. Furthermore, learning  analytics can take much more information into account than  classroom assessment procedures, and it may not be clear to the  learner which data is being used. Learning analytics can be used  to compute the relationships between learners based on their  interactions, or to compare the investment of a learner in a course  based on time spent on the learning material, or to compare text  written by students with pre-existing corpora. Thus, learning  analytics go far beyond traditional assessment procedures and  affect the privacy rights of learners in a new manner, necessitating  a clarification of the concept. In this workshop, we will  investigate different privacy notions and solutions, and propose  ways of reconciling these in the field of learning analytics.   2. Ethics & Privacy in Learning Analytics  An analysis of papers presented at the LAK14 conference reveals  that privacy is recognised as an important issue; however, privacy  is not dealt with in any depth. Twelve of the 57 papers presented                                                                        1http://www.open.ac.uk/students/charter/essential-documents/ethical-use-  student-data-learning-analytics-policy    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author. Copyright is held by the authors.   LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA   ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723642      390    at the conference mentioned privacy, three of them describing  how data had been anonymised to protect privacy. The rest of the  papers that mentioned this subject were concerned with privacy  for example as a barrier [5], as a restriction on data tracking, and a  property of a cluster of stakeholder concerns revolving around  risks [6].  The challenges posed by privacy are clearly obstacles that need to  be overcome in order to reap the benefits of learning analytics.  Arnold et al. noted that many myths surrounding the use of data,  privacy infringement and ownership of data need to be dispelled  and can be properly modulated once the values of learning  analytics are realized [7]. This theme was also taken up by [5],  Learners need to be convinced that [these analytics] are reliable  and will improve their learning without intruding into their  privacy. Swenson [8] called for ethical literacy among learning  analytics practitioners, maintaining an ethical viewpoint and  fully incorporating ethics into theory, research, and practice of the  LAK discipline.  Aguilar [5] noted that it is important to be mindful of privacy  when designing user interfaces. However, Piety [10] observed that  approaches to privacy are likely to depend on context. While  ethics and privacy are features of educational data sciences in  many arenas, there are often distinctions between the approaches  to these issues from private companies and public entities. In  many countries, public bodies must adhere to regulations and  standards when dealing with data. For example, those in the USA  are required to adhere to the Family Educational Rights and  Privacy Act (FERPA) and other regulations. However,  in the  private sector there are fewer restrictions and less regulations  regarding data collection and use [10].   Privacy is typically not defined in the LAK community papers we  have analyzed. In order to make issues related to privacy more  central to learning analytics application design, there is a need to  unpack this concept in terms of its sociocultural context. Privacy  is related to how data are used in learning analytics. When data  contain information that can be linked to a specific individual, we  refer to them as personal data. We also talk about private data,  data that individuals want to keep to themselves and share only on  their own terms. The boundaries between personal and private  data are social agreements that depend on who the person is and in  what social setting the data are created. A key question to be  addressed is Who owns the data The answer certainly involves  the individual associated with these data, but assigning ownership  of the data to this person is often too simple a solution. Data are  often generated in a social context  they may be seen as a  resource that is shared within a community or a network. Some  rights may also rest with the individual or organization that is  responsible for generating and storing these data, or for arranging  and visualizing sets of data in ways that generate actionable  insights.   3. Workshop Organization  3.1 Workshop Facilitators   The workshop will be organized jointly by the FP7 EU LACE  project (http://www.laceproject.eu), the Apereo Foundation  (https://www.apereo.org), the SURF SIG Learning Analytics  (https://www.surfspace.nl/sig/18-learning-analytics/) and the  European Association for Technology Enhanced Learning  (EATEL) SIG dataTEL (http://ea-tel.eu/sig-datatel/). All partners  aim at advancing the learning analytics field by coming up with  practical solutions and guidelines for ethical & privacy issues that   are a critical part of most data-driven research in Education. The  main goals are to increase the knowledge and awareness about  ethical and privacy boundaries of learning analytics research and  practice, to identify existing theories of trust and privacy, to  promote the re-use of best practice solutions on privacy and  ethics, to foster the cooperation between different learning  analytics research units, and to develop a kind of code of honor  for learning analytics research supported by IT-based legal tools.    3.2 Expected outcomes of the workshop  We aim to publish the submitted topics and the practical solutions  gained from the workshop in proceedings that will be  disseminated by the LACE project after the workshop.   4. ACKNOWLEDGMENT  The LACE project is funded by the European Commission  Seventh Framework Programme, grant number 619424.   References  [1] Greller, W., & Drachsler, H. (2012). Translating learning   into numbers: a generic framework for learning analytics.  Educational Technology & Society, 15 (3), 4257.   [2] Prinsloo, P, & Slade, Sharon. (2013, 8-12 April). An  evaluation of policy frameworks for addressing ethical  considerations in learning analytics. Paper presented LAK13,  Leuven, Belgium.   [3] Slade, Sharon, & Prinsloo, Paul. (2013). Learning analytics:  ethical issues and dilemmas. American Behavioral Scientist,  57 (10), pp. 1510-1529   [4] Pardo, A., & Siemens, G. (2014). Ethical and privacy  principles for learning analytics. British Journal of  Educational Technology, 45(3), 438-450.   [5] Ferguson, R., De Liddo, A., Whitelock, D., de Laat, M., &  Buckingham Shum, S. (2014). DCLA14: Second  International Workshop on Discourse-Centric Learning  Analytics. (pp. 283-284). In Proceedings of the LAK14, New  York, USA: ACM Press. doi:10.1145/2567574.2567631   [6] Drachsler, H., Stoyanov, S., & Specht, M. (2014). The  impact of learning analytics on the Dutch education system  (pp. 158162). In Proceedings of the LAK14, New York,  USA: ACM Press. doi:10.1145/2567574.2567617   [7] Arnold, K. E., Lynch, G., & Huston, D. (2014). Building  Institutional Capacities and Competencies for Systemic  Learning Analytics Initiatives, (pp. 257-260). In Proceedings  of the LAK14, New York, USA: ACM  Press.doi:10.1145/2567574.2567592   [8] Swenson, J. (2014). Establishing an ethical literacy for  learning analytics, (pp. 246-250). In Proceedings of LAK14,  New York, USA: ACM Press.doi:10.1145/2567574.2567631   [9] Aguilar, S. (2014). Perceptions and Use of an Early Warning  System During a Higher Education Transition Program, (pp.  113-117). In Proceedings of LAK14,  New York, USA: ACM  Press.doi:10.1145/2567574.2567592   [10] Piety, P. J., Hickey, D. T., & Bishop, M. J. (2014).  Educational Data Sciences  Framing Emergent Practices for  Analytics of Learning, Organizations, and Systems, (pp. 193- 202). In Proceedings of LAK14,  New York, USA: ACM  Press. doi:10.1145/2567574.2567631      391      "}
{"index":{"_id":"64"}}
{"datatype":"inproceedings","key":"Hickey:2015:IWO:2723576.2723639","author":"Hickey, Daniel and Jovanovic, Jelena and Lonn, Steve and Willis, James E.","title":"2Nd Int'L Workshop on Open Badges in Education (OBIE 2015): From Learning Evidence to Learning Analytics","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"392--393","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723639","doi":"10.1145/2723576.2723639","acmid":"2723639","publisher":"ACM","address":"New York, NY, USA","keywords":"education, learning analytics, online learning, open badges","abstract":"Open digital badges are Web-enabled tokens of learning and accomplishment. Unlike traditional grades, certificates, and transcripts, badges include specific claims about learning accomplishments and detailed evidence in support of those claims. Considering the richness of data associated with Open Badges, it is reasonable to expect a very powerful predictive element at the intersection of Open Badges and Learning Analytics. This could have substantial implications for recommending and exposing students to a variety of curricular and co-curricular pathways utilizing data sources far more nuanced than grades and achievement tests. Therefore, this workshop was aimed at: i) examining the potentials of Open Badges (including the associated data and resources) to provide new and potentially unprecedented data for analysis; ii) examining the kinds of Learning Analytics methods and techniques that could be suitable for gaining valuable insights from and/or making predictions based on the evidence (data and resources) associated with badges, and iii) connecting Open Badges communities, aiming to allow for the exchange of experiences and learning from different cultures and communities.","pdf":"2nd Intl Workshop on Open Badges in Education   (OBIE 2015): From Learning Evidence to Learning Analytics   Daniel Hickey  Center for Research on Learning and   Technology, Indiana University  504 Eigenmann Hall,    Bloomington, IN 47406, USA  dthickey@indiana.edu      James E. Willis    Center for Research on Learning and  Technology, Indiana University   504 Eigenmann Hall,   Bloomington, IN 47406, USA   jaedwill@indiana.edu   Jelena Jovanovic  Department of Software Engineering,   University of Belgrade  Jove Ilica 154,    11000 Belgrade, Serbia  jeljov@gmail.com                   Steve Lonn  University of Michigan   1401B Duderstadt Center,   2281 Bonisteel, Ann Arbor,    MI 48109-2094, USA  slonn@umich.edu              ABSTRACT  Open digital badges are Web-enabled tokens of learning and  accomplishment. Unlike traditional grades, certificates, and  transcripts, badges include specific claims about learning  accomplishments and detailed evidence in support of those  claims. Considering the richness of data associated with Open  Badges, it is reasonable to expect a very powerful predictive  element at the intersection of Open Badges and Learning  Analytics. This could have substantial implications for  recommending and exposing students to a variety of curricular  and co-curricular pathways utilizing data sources far more  nuanced than grades and achievement tests. Therefore, this  workshop was aimed at: i) examining the potentials of Open  Badges (including the associated data and resources) to provide  new and potentially unprecedented data for analysis; ii) examining  the kinds of Learning Analytics methods and techniques that  could be suitable for gaining valuable insights from and/or  making predictions based on the evidence (data and resources)  associated with badges, and iii) connecting Open Badges  communities, aiming to allow for the exchange of experiences and  learning from different cultures and communities.     Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education.   General Terms  Measurement, Design, Human Factors.   Keywords  Open Badges, Online Learning, Education, Learning Analytics.   1. MOTIVATION  Open digital badges are Web-enabled tokens of learning and  accomplishment. Unlike traditional grades, certificates, and  transcripts, badges include specific claims about learning  accomplishments and detailed evidence in support of those  claims. A systematic examination of the adoption of Open Badges  in numerous educational settings by Indiana Universitys Design  Principles Documentation Project (http://dpdproject.info/)  revealed that the introduction of badges prompted useful  deliberations about (a) the learning to be recognized, (b) the  claims to be made about that learning, and (c) the assessment  needed to support those claims. Such deliberations tend to  enhance the validity of the evidence associated with a badge, thus  properly supporting the claims stated in the badge. Therefore, the  evidence contained in badges offers unprecedented opportunity  for analyzing learning. Furthermore, the thinking and negotiations  that go into deciding what claims and evidence to include in  digital badges often result in highly credible and valuable  information about the accomplished learning.   Considering the richness of data associated with Open Badges -  (meta)data stored in badges themselves, primarily requirements,  achievement evidence and timestamps, as well as data about  learning pathways followed by students (i.e., learning traces) - it  is reasonable to expect a very powerful predictive element at the  intersection of Open Badges and Learning Analytics. The data  associated with badges might offer highly valuable input not only  to predict what may be a good fit for the next learning activity, but  perhaps even what a learner is capable of in a certain timeframe.  This fertile ground could have substantial implications for  recommending and exposing students to a variety of curricular  and co-curricular pathways utilizing data sources far more  nuanced than grades and achievement tests.   Open Badges are rapidly gaining traction among educational  practitioners, education-oriented companies and non-profit  organizations. They have already been adopted by over 2000  organizations1, including well-known learning management  platforms such as Moodle, Blackboard and Canvas. The                                                                       1 http://goo.gl/qH5BZW   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723639    392    organizers are currently involved in efforts to track badges  functionality across these platforms, and introduce this  functionality into other platforms including Open edX and Google  Coursebuilder. The newly formed Badge Alliance  (http://www.badgealliance.org/) already includes 700 members  and a dozen working groups, and the Clinton Global Initiative  estimates that over 4M badges have been issued to date2. This  indicates the relevancy and potential that the concept and  technology of Open Badges could unlock for education domain in  general, and, for the above stated reasons, for Learning Analytics  field, in particular.   2. OBJECTIVES  The objectives of the workshop were threefold:    Examine the potentials of Open Badges (including the  associated data and resources) to provide new and potentially  unprecedented data for analysis.    Examine the kinds of Learning Analytics methods and  techniques that could be suitable for gaining valuable  insights from and/or make predictions based on the evidence  (data and resources) associated with badges.     Build a bridge across Open Badges communities, aiming to  allow for the exchange of experiences and learning from  different cultures and communities.   3. TOPICS  The workshop welcomed submissions on some of the topics from  the following (though not restrictive) list:    Challenges and opportunities related to the use of Learning  Analytic methods and techniques in the domain of Open  Badges.     Different kinds of feedback for teachers and/or students that  could be generated based on the data and resources  associated with badges.    Analysis and visualization of learners badge-earning  pathways as means of i) assisting badge issuers in improving  their instructional design, and ii) scaffolding learners  reflection over the learning process.    How different kinds of badges  e.g., participatory badges vs.  assessment-based badges  impact learners engagement in  disciplinary discourse    Intersection of Open Badges and the Open Analytics  initiative and other recent efforts to open and share analytics  data  how open access and visibility affect the functionality  of badges as a motivator of behaviour    Combined use of Open Badges Specification3 with other  education-oriented specifications and vocabularies (e.g.,  TinCan, LRMI), to allow for a greater insight into to  evidence associated with badges.    Using semantic technologies to i) analyze evidence contained  in digital badges, ii) allow for a comparison and alignment of  badges issued by different organizations, and iii) interpret the  meaning of a collection of heterogeneous badges  what do  they say about a learner and his/her competencies    Examining learning pathways that emerge in large scale  badging efforts like the seven-city Cities of Learning4 in  2014.                                                                        2 http://10mbetterfutures.org/  3 https://github.com/openbadges/openbadges-specification   4 http://goo.gl/YNKsU8    4. AUDIENCE  Open Badges connect educational providers and practitioners,  entrepreneurs, and researchers in discourses on teaching, learning,  assessment, digital credentials, and digital education in general.  While preserving its general focus on opportunities and challenges  associated with Open Badges, this 2nd installment of the OBIE  workshop was primarily intended for those interested in the  intersection of Open Badges and Learning Analytics. This  intersection includes gathering, integration, and analysis of data  and resources associated with Open Badges, with the ultimate aim  of providing teachers, learners and other stakeholders in the ever- increasing Open Badges ecosystem with informative and relevant  feedback, and predictive functions. Participants in the workshop  were exposed to presentations and discussions on the position and  role of digital badges in instructional design, on compelling data  and analytics challenges and opportunities that the Open Badges  infrastructures open for the Learning Analytics field, and on  different types and designs of badge systems and their potential to  impact the future of education.   5. PREVIOUS EDITION OF THE  WORKSHOP  This is the second edition of Open Badges in Education (OBIE)  workshop. The first edition (Open Badges in Education: Novel  Motivational, Scaffolding and Recognition Mechanism for Web- based Learning5) was held together with the 13th International  Conference on Web-based Learning (ICWL2014) in Tallinn,  Estonia, in August 2014. Even though the workshop was expected  to be a small event, it attracted over 30 people interested in badges  and related educational approaches and practices. During the  workshop, the participants engaged in lively discussions on many  important topics related to Open Badges, and to keep the  discussion live and to evolve a community around the concept and  technology of Open Badges, we set up OBIE Google Group6 as a  communication channel for the workshop participants and  contributors, as well as all researchers, educators, Open Badges  enthusiasts, and everyone interested in, willing to learn about,  and/or work/experiment with badges in educational settings. It  was this group that initially proposed and supported organization  of the 2nd OBIE workshop at LAK15.   6. ACKNOWLEDGMENTS  Significant parts of the effort in organizing this meeting were  supported by grants from the MacArthur Foundations Digital  Media and Learning Initiative. It was also partially supported by  the GRASS project (ref. num.: 543029-LLP-1-2013-1-RS-KA3- KA3MP) funded by the European Commission.  Our thanks to the  LAK15 Chairs and Program Committee for hosting this event.  Last but not the least important, we thank the members of the  workshop Program Committee, as well as all the members of the  Open Badges community who supported this event.                                                                            5 https://sites.google.com/site/obie2014ws/   6 http://goo.gl/biLPd6    393      "}
{"index":{"_id":"65"}}
{"datatype":"inproceedings","key":"Duval:2015:VVA:2723576.2723643","author":"Duval, Erik and Verbert, Katrien and Klerkx, Joris and Wolpers, Martin and Pardo, Abelardo and Govaerts, Sten and Gillet, Denis and Ochoa, Xavier and Parra, Denis","title":"VISLA: Visual Aspects of Learning Analytics","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"394--395","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723643","doi":"10.1145/2723576.2723643","acmid":"2723643","publisher":"ACM","address":"New York, NY, USA","keywords":"information visualisation, learning analytics, visual analytics","abstract":"In this paper, we briefly describe the goal and activities of the LAK15 workshop on Visual Aspects of Learning analytics.","pdf":"VISLA: Visual Aspects of Learning Analytics  Erik Duval Dept. Comp. Sc., KU Leuven  Celestijnenlaan 200A B3000 Leuven, B  erik.duval@cs.kuleuven.be  Katrien Verbert Dept. Comp. Sc., KU Leuven  Celestijnenlaan 200A B3000 Leuven, B  katrien.verbert@cs.kuleuven.be  Joris Klerkx Dept. Comp. Sc., KU Leuven  Celestijnenlaan 200A B3000 Leuven, B  joris.klerkx@cs.kuleuven.be Martin Wolpers  Fraunhofer FIT Schloss Birlinghoven  53754 Sank Augustin, D martin.wolpers@fit.fraunhofer.de  Abelardo Pardo El & Inf Eng, Univ. Sydney  NSW 2006, A  abelardo.pardo@sydney.edu.au  Sten Govaerts School of Eng, EPFL  Station 9 CH-1015 Lausanne, CH  sten.govaerts@epfl.ch Denis Gillet  School of Eng, EPFL Station 9  CH-1015 Lausanne, CH denis.gillet@epfl.ch  Xavier Ochoa ESPOL, Gustavo Galindo KM 30.5 Via Perimetral  Guayaquil, EC xavier@cti.espol.edu  Denis Parra Dept. Computer Science  Pontificia Univ. Catolica Chile Santiago, CL  dparra@ing.puc.cl  ABSTRACT In this paper, we briefly describe the goal and activities of the LAK15 workshop on Visual Aspects of Learning analyt- ics.  Categories and Subject Descriptors H.5 [Information Interfaces and Presentation]: Mul- timedia Information Systems, User Interfaces, Group and Organization Interfaces  General Terms Design, Experimentation, Human Factors  Keywords learning analytics, visual analytics, information visualisa- tion  1. WORKSHOP GOALS The use of visualisation techniques for learning is not new.  In a learning analytics context, the application of infor- mation visualisation techniques can help both teachers and learners to explore and understand relevant user traces that are collected in various (online) environments. Typical ap- plications include dashboards [] or tabletop visualisations that support collaborative learning []. Thus, such tech- niques can improve (human) learning.  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author.  LAK 15 Mar 16 - 20 2015, Poughkeepsie, NY, USA Copyright is held by the owner/author(s). ACM 978-1-4503-3417-4/15/03. http://dx.doi.org/10.1145/2723576.2723643.  The goal of this workshop is to build a strong research ca- pacity around visual approaches to learning analytics. The longer term goal is to improve the quality of learning ana- lytics research that relies on information visualisation tech- niques.  2. ACTIVITIES Authors were invited to submit original unpublished work.  The following types of contributions were possible:   Short papers (3-5 pages) that state the position of the authors within the scope of the workshop and describe solution concepts, prototypes and work in progress.   Full papers: (8-12 pages) that describe mature work, including evaluation.  Each contribution to the workshop was asked to further- more explicitly address the ollowing items:  1. What kind of data is being visualised What tools were used to clean up the data (if any)  2. For whom is it intended This could be the learner, teacher, manager, researcher, etc.  3. How is data visualised Which interaction techniques are applied What tools, libraries, data formats, etc. are used Describe the workflows and recipes used to develop the visualisation  4. Why are the visual approaches applied (i.e. rationale behind the application of a visualisation)  5. How has the approach been evaluated or how could it be evaluated  6. What were the encountered problems and pitfalls dur- ing the visualisation process  394    We received 7 short papers and 2 long papers. At the time of writing, the review process is proceeding.  Topics covered in the submissions illustrate the variety of approaches in the domain: tablet visualisations for self- regulated learning, tabletop visualisations for enquiry based learning, visualisations of student responses to short answer questions, visualisations of tag clusters for learning reposi- tories, visualisations of competence graphs, the visualisation of uncertainty in predictions of course failure, the visualisa- tion of Learning Management System logs, visualisation of collaborative writing in education, and visual analytics for discussion fora on learning how to program...  During our 1-day workshop, we aim to facilitate an inter- active and engaging event where we want to avoid death by powerpoint by promoting discussion activities over presenta- tions. In the first half of the workshop, we will therefore ask participants to shortly present the work of another submis- sion and to relate it back to their own work. Facilitators may allocate challengers per presentation to move the discussion around common themes and differences in approaches.  During the second half of the workshop, we will invite the participants to share their tools, workflows and recipes in a hands on discussion session so that they can benefit from each others knowledge, apply their visual approaches on either their own dataset or on the dataset that we provide.  Finally, we will move the discussion to the final topic of the workshop, which is the development of the equivalent of the VAST challenge for learning [], which will be linked back with the LAK14 and LAK15 data challenge []. To get this discussion going, we will invite a keynote speaker with direct experience in VAST.  As [] mentions:  The annual Visual Analytics Science and Tech- nology (VAST) challenge provides Visual Ana- lytics researchers, developers, and designers an opportunity to apply their best tools and tech- niques against invented problems that include a realistic scenario, data, tasks, and questions to be answered. Submissions are processed much like conference papers, contestants are provided reviewer feedback, and excellence is recognized with awards. A day-long VAST Challenge work- shop takes place each year at the IEEE VAST conference to share results and recognize out- standing submissions.  3. RELATED WORKSHOPS Three related editions of our workshop around the theme  of applying visual approaches in TEL and in learning ana- lytics were held at EC-TEL 2011 (https://sites.google. com/site/advtel2011/), at Learning Analytics Summer In- stitute (LASI14) in Cambridge, MA, USA (http://solaresearch. org/conferences/lasi/lasi2014/) and at the JTEL sum- mer school 2013 (http://www.prolearn-academy.org/Events/ summer-school-2013).  4. CONCLUSION We believe that an initiative similar to VAST for learning  analytics data sets would help to build up research capacity around visual aspects of learning analytics.  5. ACKNOWLEDGMENTS We gratefully acknowledge the review efforts of our Pro-  gram Committee: Michael Derntl (D), Pedro J. Munoz Merino (E), Alexandra Cristea (UK), Ulrich Hoppe (D), Eelco Herder (D), Patrick Jermann (CH), Ralf Klamma (D), Till Nagel (D), Lars Bollen (NL), Dragan Gasevic (CA), Sharon Hsiao (USA), Judy Kay (AU), Dietrich Albert (A), Maria Jesus Rodriguez Triana (CH).  6. REFERENCES [1] K. Cook, G. Grinstein, and M. Whiting. The VAST  challenge: history, scope, and outcomes: An introduction to the special issue. Information Visualization, 13(4):301312, 2014.  [2] H. Drachsler, S. Dietze, E. Herder, M. dAquin, and D. Taibi. The learning analytics & knowledge (LAK) data challenge 2014. In Proceedings of the Fourth International Conference on Learning Analytics And Knowledge, LAK 14, pages 289290, New York, NY, USA, 2014. ACM.  [3] R. Martinez-Maldonado, Y. Dimitriadis, A. Martinez-Mones, J. Kay, and K. Yacef. Capturing and analyzing verbal and physical collaborative learning interactions at an enriched interactive tabletop. International Journal of Computer-Supported Collaborative Learning, 8(4):455485, 2013.  [4] K. Verbert, S. Govaerts, E. Duval, J. Santos, F. Assche, G. Parra, and J. Klerkx. Learning dashboards: an overview and future research opportunities. Personal and Ubiquitous Computing, 18(6):14991514, 2014.  395  https://sites.google.com/site/advtel2011/ https://sites.google.com/site/advtel2011/ http://solaresearch.org/conferences/lasi/lasi2014/ http://solaresearch.org/conferences/lasi/lasi2014/ http://www.prolearn- academy.org/Events/summer-school-2013 http://www.prolearn- academy.org/Events/summer-school-2013     "}
{"index":{"_id":"66"}}
{"datatype":"inproceedings","key":"Drachsler:2015:RLD:2723576.2723641","author":"Drachsler, Hendrik and Dietze, Stefan and Herder, Eelco and d'Aquin, Mathieu and Taibi, Davide and Scheffel, Maren","title":"The 3rd LAK Data Competition","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"396--397","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723641","doi":"10.1145/2723576.2723641","acmid":"2723641","publisher":"ACM","address":"New York, NY, USA","keywords":"data mining, learning analytics, linked data, visualization","abstract":"The LAK Data Challenge 2015 continues the research efforts of the previous data competitions in 2013 and 2014 by stimulating research on the evolving fields Learning Analytics (LA) and Educational Data Mining (EDM). Building on a series of activities of the LinkedUp project, the challenge aims to generate new insights and analysis on the LA  EDM disciplines and is supported through the LAK Dataset - a unique corpus of LA  EDM literature, exposed in structured and machine-readable formats.","pdf":"The 3rd LAK data competition    Hendrik Drachsler    Welten Institute, Open University of  the Netherlands   hendrik.drachsler@ou.nl       Mathieu d'Aquin  Knowledge Media Institute,   The Open University, UK   m.daquin@open.ac.uk      Stefan Dietze  L3S Research Center   Leibniz University, Hannover,  Germany   dietze@l3s.de     Davide Taibi   Institute for Educational    Technologies, National Research  Council of Italy   davide.taibi@itd.cnr.it     Eelco Herder   L3S, Research Center    Leibniz University, Hannover,  Germany   herder@l3s.de     Maren Scheffel  Welten Institute, Open University of   the Netherlands   maren.scheffel@ou.nl  Abstract  The LAK Data Challenge 2015 continues the research efforts of  the previous data competitions in 2013 and 2014 by stimulating  research on the evolving fields Learning Analytics (LA) and  Educational Data Mining (EDM). Building on a series of activities  of the LinkedUp project, the challenge aims to generate new  insights and analysis on the LA & EDM disciplines and is  supported through the LAK Dataset - a unique corpus of LA &  EDM literature, exposed in structured and machine-readable  formats.    Categories and Subject Descriptors  E.1 [Data Structures] Distributed data structures; E.2 [Data  Storage Representations] Linked representations; J.1  [Administrative Data Processing] Education; H.1.1  [Information Systems] Models and principles, Systems and  Information Theory; H.3.1 [Information Storage and Retrieval]:  Content Analysis and Indexing   General Terms  Algorithms, Measurement, Design, Standardization,  Experimentation, Human Factors, Theory.   Keywords  Learning analytics, data mining, linked data, visualization.   1. Introduction  A variety of datasets is used in the Learning Analytics field for  research on teaching and learning. The available datasets can be  roughly distinguished between (a) tracking data that comes from  different learning environments [1] and (b) Linked data from the  web [2].   Tracking data from different learning environments involves  interactions of learners with different tools and resources. The  main driver for analyzing these data is the vision of increased   awareness of the learning progress, self-regulation support, and  personalized learning that offers potential to create more effective  learning experiences through new possibilities for the prediction  and reflection of individual learning processes. Most of the time  the tracking data underlies legal and privacy restrictions that make  it difficult to share the data or make it accessible to third parties.   Next to the large amount of tracking data, there is an increasing  amount of Linked Data on the Web that covers educational data  published by institutions about their courses and learning  resources. The Linked Data approach enables the enrichment of  learning content and the learning experience by making use of  various connected data sources. Through reusing schemas and  vocabularies as well as by relying on persistent URIs for data  referencing, a higher level of interoperability is being provided.  This makes it more convenient to use Linked Data for research  purposes and makes the outcomes of the research more  comparable.    The LinkedUp project has created a dataset catalogue1 of  educationally relevant (linked) datasets that is freely accessible  [4]. The main aim of LinkedUp was to identify and promote  innovative success stories that exploit Linked Data in educational  scenarios. Under this objective, it contributed a linked dataset for  the Learning Analytics (LA) and Educational Data Mining (EDM)  communities to facilitate research, analysis, and smart explorative  applications to gain new insights into the research papers  published in this domain [5].   In 2014, the LinkedUp project2 organized the 2nd LAK Data  Challenge based on the LAK13 dataset [3]. A range of interesting  applications and analytical research has been contributed to the  previous challenges that provided new insights into the  development of the emerging research field3. The submissions  provide evidence for the main differences as well as the  relationships between the EDM and the LAK research  community. As the LinkedUp project has been successfully  completed in October 2014, we are glad to announce that the  LACE project4 was interested to host the LAK data challenge in  2015 in cooperation with SoLAR.                                                                         1http://data.linkededucation.org/ and   http://data.linkededucation.org/linkedup/catalog/   2 http://www.linkedup-project.eu   3 http://linkedu.eu/event/lak2013-linkeddata-tutorial/page_id=58  4 http://laceproject.eu    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author. Copyright is held by the  owner/author(s).    LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723641        396    The challenge submissions should exploit the extended LAK 2014  dataset for meaningful purposes. This includes submissions which  cover one or more of the following, non-exclusive list of topics.   - What are the main differences between the EDM and  LAK community over time   - What are new and emerging topics  - Do different visualisations of the data lead to different   insights   - How can the LAK dataset be used to point to related   research efforts in other scientific disciplines such as  educational science, ethics and legal aspects but also  new technologies    - Which of the articles in the data provide evidence for  positive or negative effects of learning analytics  research on the educational practices   2. The LAK Dataset   The LAK Dataset has been extracted to create a structured corpus  including full text, references, and metadata including authors,  affiliations, titles, keywords and abstracts. The schema used to  describe the papers in the dataset is based on two established  schemas: the Semantic Web Conference Ontology (already used  to describe metadata about publications from the Semantic Web  conferences and related events) and the Linked Education schema.  The data is accessible under http://lak.linkededucation.org.    Throughout the past year and following on the LAK Data  Challenge 2014, the dataset has been improved and expanded as a  joint effort by SoLAR, ITD-CNR5, and the LinkedUp project.  Latest publications from the LAK2014 and EDM2014  conferences were added, enriching content and keywords with  references to DBpedia and including the actual references of each  publication. The current version of the LAK datasets consists of:    Proceedings of the ACM International Conference on  Learning Analytics and Knowledge (LAK) (2011-14)    Proceedings of the previous editions of the LAK Data  Challenge (2013-14)    Proceedings of the International Conference on Educational  Data Mining (2008-14)    The open access journal Educational Technology & Society  recently published a 2012 special issue on Learning and  Knowledge Analytics: Educational Technology & Society -  Special Issue on Learning & Knowledge Analytics.    Journal of Educational Data Mining (2009-14)   Journal of Learning Analytics (2014)   3. Workshop Organization  3.1 Workshop Facilitators   The workshop is organized jointly by SoLAR, the LACE project  and associated partner CNR-ITD. In addition the special interest  groups Linked Education (http://linkededucation.org) and SIG  dataTEL (http://ea-tel.eu/sig-datatel/) of EATEL will support the  competition. All partners aim at advancing data-driven research in  education. The main goals are to foster the cooperation between  different Learning Analytics research units and to offer reference  datasets for data-driven research.   The partners can look back on an annual workshop series at  different conferences, including:     Linked Learning 2014  4th International Workshop on                                                                        5 www.itd.cnr.it/     Learning and Education with the Web of Data (LILE2014) at  ISWC2014, Riva del Garda, Italy.    RecSysTEL 2014, 5th International Workshop on  Recommender Systems for Technology Enhanced Learning  (RecSysTEL14) at ICALT 2014, Athens, Greece.    Linked Learning 2013  3rd International Workshop on  Learning and Education with the Web of Data (LILE2013) at  WWW 2013, Rio de Janeiro, Brazil.    Learning Analytics and Linked Data (LADA12) at 2nd at  LAK12, Vancouver, Canada.     Linked Learning 2012 - 2nd International Workshop on  Learning and Education with the Web of Data (LILE2012) at  WWW2012, Lyon, France.   3.2 Evaluation of submissions   The submissions will be reviewed by members of the challenge  committee to pre-select submissions for presentation. The  accepted submissions will be published in online proceedings and  presented during an interactive session at the LAK 2015  conference. During the LAK conference and based on the  presentations, the challenge winners are identified according to  the LinkedUp Evaluation Framework [9]. Finally, the best three  submissions will be presented in a panel at the main stage of the  conference and receive an award by SoLAR.   References  [1] Verbert, K., Manouselis, N., Drachsler, H., & Duval, E.   (2012). Dataset-Driven Research to Support Learning and  Knowledge Analytics. Educational Technology & Society,  15 (3), 133148.   [2] T. Heath and C. Bizer (2011). Linked Data: Evolving the  Web into a Global Data Space. Synthesis Lectures on the  Semantic Web: Theory and Technology. Morgan &  Claypool.   [3] Taibi, D. and Dietze, S. (2013). Fostering Analytics on  Learning Analytics Research: the LAK Dataset. CEUR WS  Proceedings Vol. 974, Proceedings of the LAK Data  Challenge.   [4] dAquin, M., Adamou, A., Dietze, S., Assessing the  Educational Linked Data Landscape, ACM Web Science  2013 (WebSci2013), Paris, France, May 2013.   [5] dAquin, M., Dietze, S., Drachsler, H., Taibi, D. (2014).  Using linked data in learning analytics. eLearning Papers, Nr.  36,  ISSN: 1887-1542.   [6] Nunes, B. P., Fetahu, B., Dietze, S., Casanova, M. A.,  Cite4Me: A Semantic Search and Retrieval Web Application  for Scientific Publications, 12th International Semantic Web  Conference (ISWC2013), Sydney, Australia, (2013).   [7] Maturana, R.A., Alvarado, M. E. Lpez-Sola, S., Ibez,  M.J., & Ruiz Elsegui, L., (2013). Linked Data based  applications for Learning Analytics Research. CEUR WS  Proceedings of the LAK Data Challenge, Vol. 974.   [8] Zouaq, Amal, Sreko Joksimovi, and Dragan Gaevi.  (2013). Ontology Learning to Analyze Research Trends in  Learning Analytics Publications. CEUR WS Proceedings of  the LAK Data Challenge Vol. 974.   [9] Drachsler, H., Stoyanov, S., d'Aquin, M., Herder, E., Dietze,  S., & Guy, M. (2014, 16-19 September). An Evaluation  Framework for Data Competitions in TEL. Paper presented  at the 9th European Conference on Technology-Enhanced  Learning (EC-TEL 2014), Graz, Austria.   397      "}
{"index":{"_id":"67"}}
{"datatype":"inproceedings","key":"Monroy:2015:LAA:2723576.2723658","author":"Monroy, Carlos and Rangel, Virginia Snodgrass and Bell, Elizabeth R. and Whitaker, Reid","title":"A Learning Analytics Approach to Characterize and Analyze Inquiry-based Pedagogical Processes","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"398--399","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723658","doi":"10.1145/2723576.2723658","acmid":"2723658","publisher":"ACM","address":"New York, NY, USA","keywords":"inquiry-based pedagogy, learninformatics, process mining","abstract":"Here we describe the use of learning analytics (LA) for investigating inquiry-based science instruction. We define several variables that quantify curriculum usage and leverage tools from process mining to examine inquiry-based pedagogical processes. These are initial steps toward measuring and modeling fidelity of implementation of a science curriculum. We use data from one school district's use of an online science curriculum (N","pdf":"A Learning Analytics Approach to Characterize and  Analyze Inquiry-based Pedagogical Processes  Carlos Monroy, Virginia Snodgrass Rangel, Elizabeth R. Bell, Reid Whitaker   Rice University Center for Digital Learning and Scholarship  6100 Main St., MS 112   Houston, TX 77005  United States   +1 713 348 5433  {carlos.monroy, vsr, erb10, reid}@rice.edu    ABSTRACT  Here we describe the use of learning analytics (LA) for  investigating inquiry-based science instruction. We define several  variables that quantify curriculum usage and leverage tools from  process mining to examine inquiry-based pedagogical processes.  These are initial steps toward measuring and modeling fidelity of  implementation of a science curriculum. We use data from one  school districts use of an online science curriculum (N=1,021  teachers and nearly 330,000 page views).    Categories and Subject Descriptors  K.3.0 [Computers and Education]: General; H.4 [Information  Systems Applications]: Miscellaneous.   General Terms  Management, Design, Human Factors.   Keywords  Process mining, inquiry-based pedagogy, learninformatics.   1. INTRODUCTION  In this paper we build on the Learning Analytics infrastructure  developed for STEMscopes, an online, inquiry-based science  curriculum for grades K-12 in the U.S., and use data from one  school district to explain how we currently are using learning  analytics data to understand how the curriculum is used. Our goals  here are: 1) to define a set of variables for measuring curriculum  use, and 2) to briefly describe the application of process-mining  methods, using ProM [3], an open source process modeling and  mining software to explore patterns of use. We harness ongoing  research in Learning Analytics such as the one described in [4].   2. MEASURING INQUIRY PROCESSES   Inquiry-based science education has its roots in constructivist  pedagogies [1]. The basic premise of constructivism is that  students learn by constructing knowledge gained from hands-on  experiences and social interaction. STEMScopes [2] is based on  the 5E instructional model and builds on previous lesson cycles   meant to help teachers implement inquiry-based instruction in the  classroom. The 5E refers to five steps within the cycle:  engagement, exploration, explanation, elaboration, and evaluation.  We used anonymized event log data from teacher interactions  with the STEMscopes website for a mid-size school district for  the 2012-2013 school year. Here we adopt a broader notion of  curriculum use, that is, a visit to a curriculum resource  constitutes use. STEMscopes is divided into science standards  (called scope). Each scope is divided into steps (from the 5E+I/A  model). Steps, in turn contain different elements such as activities  designed to fulfill the pedagogical purpose of each step. In the  next section, we describe several variables we have created in an  effort to characterize teaching practices (see formulas 1 to 4).   2.1 Activity Level Instruction (ALI)  This variable tells us the concentration of use on a pedagogical  step, by measuring the contribution of visits to elements within  that step in relation to the total number of visits to all steps (V).  Formula 1 describes the ALI for the Engage step (En); this  formula is replicated for each one of the seven steps. In the test  case we present here, mean ALI values for Engage, Explore, and  Explain (the steps related to the inquiry process) range from 0.21  to 0.26, with Explore having the highest value (0.26), revealing  comparable proportions of use among the three (approximately  25% of visits to each one of these steps). Average use for  Elaborate and Evaluate drops to 0.15 and 0.08, this is expected  because they include fewer activities. Finally, mean values for  Acceleration and Intervention are the lowest ones.   2.2 Inquiry Instruction Contribution (IIC)  This index measures the proportion of combined visits to elements  in Engage, Explore, and Explain in relation to the total number of  visits (V) to the seven 5E+I/A steps (see formula 2). It represents  the ratio of inquiry instruction to total visits. The average IIC  value was 0.70, which indicates a high concentration of access to  the inquiry-related steps in relation to total access of the  curriculum. Although the IIC index depicted in figure 1 does not  indicate a clear trend, it does suggest a large concentration of  teachers with IIC values between 0.5 and 0.85 (X-axis) for  teachers with mid- and high-levels of visits (Y-axis). Conversely,  fewer teachers are found on the lower range (between 0.0 and  0.5), suggesting that students likely have more opportunities to  explore new concepts and then articulate their new knowledge.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.     Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723658      (1) (2)   (4) (3)   398    2.3 Next Step Index (NSI)  Activities in the Elaborate step include students creating their  own questions and cross-curricular activities addressing areas  such as math or reading. The NSI variable (formula 3) therefore  measures the proportion of Elaborate usage compared to the  average of combined visits to the Engage, Explore, and Explain  steps (m=3), which allows us to compare the degree of  broadening instruction in relation to the inquiry-related steps.  The NSI index shows a concentration of teachers in a narrow  range (scatterplot labeled NSI in figure 1). This index generally  has a low value since it compares activities on one step Elaboratein relation to usage on three steps.   2.4 Evaluation Index (EVI)  The Evaluation Index variable (formula 4) measures the degree to  which teachers access the evaluations embedded in the curriculum  in relation to the combined average of the inquiry-related steps,  Engage, Explore, and Explain steps (m=3). Similar to the NSI  index, the Evaluation index tends to have lower values with a  concentration of use between 0.0 and 0.1 (scatterplot labeled EVI  in figure 1) and a handful of outliers (teachers with EVI values of  0.3 and greater on the X-axis), suggesting a moderate level of  access of the assessments, relative to use of the three inquiry- related steps (similar to NSI). Although the short assessments are  critical for measuring students progress, the curriculum offers  other more formative ways to assess students [5].   2.5 Process Mining  Indices described earlier give a glimpse of curriculum use,  however, they do not show sequences and completeness of steps.  A Petri Net generated by ProM for a random (but non- representative) sample of three teachers showed that: 1) Explore  and Engage were the steps that teachers tended to access first, 2)  Explain step tends to follow Engage, and that 3) Acceleration  following Explore was an unexpected sequence. Figure 2 depicts  the steps preceding and following Engage activities (red box). For  approximately 57% of the cases, the preceding activities are from  the same Engage step, 35% from the Evaluate step, and 8% from  Intervention. On the other hand, subsequent activities visited after  Engage include a different activity in the same step 66% of the  time, and activities in Explain 34% of the time. This case  illustrates one of the strengths of process modeling, namely to  better understand inquiry processes beyond the proportions of  usage revealed by the indices defined in section two.    3. CONCLUSIONS AND FUTURE WORK  In general, results discussed in section 2 shed light on ways to  measure inquiry instruction. As we refine our analysis with ProM,  we expect to conduct fidelity of implementation studies, which  can offer insights about path variations for diverse science topics,  alignment to best practices or canonical inquiry-paths, and  timespan for covering materials. The indices offer a high level  perspective of usage, depicting the influence of the 5E pedagogy  based on proportions of use. However, they fall short in terms of  the details offered by process modeling. We expect to combine  these methods to help us better understand classroom instruction.   4. REFERENCES  [1] Minner, D., Levy, A., and  Century, J. 2010. Inquiry-based   science instructionwhat is it and does it matter Results  from a research synthesis years 1984 to 2002. Journal of  Research in Science Teaching. 47(4), 474-496.   [2] Monroy, C, Snodgrass-Rangel, V. and Whitaker, R. 2013.  STEMscopes: Contextualizing Learning Analytics in a K-12  Science Curriculum. Proceedings of the 3rd. LAK Conference.  210-219.    [3] Van der Aalst, W. 2010. Process Mining: Discovery,  Conformance and Enhancement of Business Processes.  Springer, Germany.   [4] Wise, A., Zhao, Y., and Hausknecht, S. 2013. Learning  analytics for online discussions: a pedagogical model for  intervention with embedded and extracted analytics.  Proceedings of the 3rd. LAK Conference. 48 -56.   [5] Zuiker, S. and Whitaker, R. 2013. Refining inquiry with  multi-form assessment: Formative and summative  assessment functions for flexible inquiry. Int. Journal of  Science Education. DOI: 10.1080/09500693.2013.83448  Figure 2. Inputs and Outputs for the Engage step (red box),  and the rest of the 5E+A/I steps.   Figure 1. Comparing visits (Y-axis) and indices from section two (X-axis ranging from 0 to 1). NSI and EVI ranges (X-axis)  have been normalized between 0 and 1 for displaying purposes. For all graphs, N=143,663 visits and N=1021 teachers.   399      "}
{"index":{"_id":"68"}}
{"datatype":"inproceedings","key":"Mor:2015:PPR:2723576.2723656","author":"Mor, Dalit and Laks, Hagar and Hershkovitz, Arnon","title":"Predicting Post-training Readiness to Work with Computers: The Predominance of Log-based Variables","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"400--401","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723656","doi":"10.1145/2723576.2723656","acmid":"2723656","publisher":"ACM","address":"New York, NY, USA","keywords":"decision tree, work readiness, working with computers","abstract":"In today's job market, computer skills are part of the prerequisites for many jobs. In this paper, we report on a study of readiness to work with computers (the dependent variable) among unemployed women (N","pdf":"Predicting Post-training Readiness to Work with  Computers: The Predominance of Log-based Variables  Dalit Mor  Tel Aviv University   P.O. Box 39040  Tel Aviv 6997801, Israel   dalitmor@mail.tau.ac.il   Hagar Laks  Tel Aviv University   P.O. Box 39040  Tel Aviv 6997801, Israel   ehrenfeldh@mail.tau.ac.il   Arnon Hershkovitz  Tel Aviv University   P.O. Box 39040  Tel Aviv 6997801, Israel   arnonhe@tauex.tau.ac.il     ABSTRACT  In today's job market, computer skills are part of the  prerequisites for many jobs. In this paper, we report on a study  of readiness to work with computers (the dependent variable)  among unemployed women (N=54) after participating in a  unique training focused on computer skills and empowerment.  Associations were explored between this variable and 17  variables from four categories: log-based, computer literacy and  experience, job-seeking motivation and practice, and training  satisfaction. Only two variables were associated with the  dependent variable: Knowledge post-test duration and  satisfaction with content. Building a prediction model of the  dependent variable, another feature was highlighted: Total  number of actions in the course website along the course. Our  analyses highlight the predominance of the log-based variables  over the variables from the other categories, and we thoroughly  discuss this finding.   Categories and Subject Descriptors  K.3.2 [Computers and Education]: Computer and Information  Science Education  literacy. K.4.2 [Computers and Society]:  Social Issues  employment.    General Terms  Human Factors.   Keywords  Work readiness, working with computers, decision tree.   1. INTRODUCTION  Information and communication technology (ICT) is part of  everyday life in the 21st century, and the rapid development of  ICT requires a completely new set of skills related to  technological literacy. As new technology is constantly being  developed, fluency with information technology and computer  self-efficacy are gaining more importance regarding the profiles  of employees in today's job market.     Permission to make digital or hard copies of part or all of this work for personal or  classroom use is granted without fee provided that copies are not made or distributed  for profit or commercial advantage and that copies bear this notice and the full citation  on the first page. Copyrights for third-party components of this work must be honored.  For all other uses, contact the Owner/Author.    Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723656   Computer self-efficacy is defined as an individuals perceptions  of his or her ability to use computers in the accomplishment of a  task rather than reflecting simple component skills [1].  Computer self-efficacy was found to be positively related to  computer usage, previous experience, and  directly related to  our study - computer training and computer self-efficacy (or  similar constructs) were found (e.g., [3,4]). There is no wonder,  therefore, that unemployed older workers express a desire to  receive additional training on technology, preferable in a hands- on fashion [2].  Therefore, training programs for employees often suggest both  computer-related content and modules which aim on improving  the participants' self-efficacy concerning working in today's job  market; such a program is at the heart of the current study.  While many previous studies have examined the efficiency of  such programs in improving measures related to computer self- efficacy, the novelty of the current study is in examining the  relationships between of log-based (training-related) variables  and a variable similar to computer self-efficacy.  Though very simple, these log-based measures outperform other  variablesincluding such variables that were found in previous  studies as good indicators to computer self-efficacyin  predicting the dependent variable, which measures perceptions  of unemployed women's readiness to work with computers.   2. METHODOLOGY  2.1 Research Field  Data analyzed for this study was drawn from Appleseeds  Academy's  Technological Empowerment for Unemployed  Women  course (TEUW). During this course, unemployed  women are taught basic computer applications and best job- seeking practices. In addition, the participants take part in an  empowerment workshop, in order to enhance their chances of  finding a job. A typical course includes 16 meetings (4.5-hour  long each) taken within one month. The course is accompanied  by a Moodle website, holding all of the materials used during  the meetings and extra materials for self-learning.    2.2 Participants, Data, Research Variables  Overall, we collected data of 54 participants, all women of ages  25-65, who took the TEUW course during February-March  2014. Participants were drawn from groups located in different  areas of Israel, including both big cities and small towns. Data  were collected via knowledge pre/post-tests, pre/post surveys of  computer use and attitudes towards computers, and log files.   The dependent variable is Post-training Employment  Readiness High/Low. It is binary (1/0 for high/low readiness),   400    based on a median split of the average of five employment- efficacy items in the post-survey; each of which was scored on a  5-point Likert scale (e.g.,  I feel confident to present my skills  and strengths ,  I feel confident to go to job interviews ).   The 17 independent variables are divided into four categories:   2.2.1 Log-based Variables  For each participant, Total Number of Actions and Knowledge  Pre/Post-Test Length (time-difference between entrance to the  test and hitting the  Finish  button [seconds]) were computed.   2.2.2 Computer Literacy, Experience  Knowledge Pre/Post-Test Score (percentage of correct answers  from the pre/post knowledge test) and Computer Pre/Post-Use  (average of pre/post survey items referring to computer  applications use, originally rated on a 5-point Likert scale).   2.2.3 Job-seeking Motivation and Practice  Based on relevant items (originally scored on a 5-point Likert  scale) from the pre/post survey, we computed: Pre-training  Employment Readiness (based on the pre survey, calculated  similarly to the dependent variable, without a median split),  Motivation towards the Training (1 item), Pre/Post Job-seeking  Activeness (6 items), and Pre/Post Beliefs in Finding Suitable  Job (1 item).   2.2.4 Training Satisfaction  These variables are based on the post survey (originally scored  on a 5-point Likert scale): Satisfaction with Content (5 items),  Satisfaction with Instructor (7 items), Satisfaction with  Empowerment Workshop (9 items), Satisfaction with Final  Project (3 items).   3. SUMMARY OF RESULTS  3.1 Pre/Post-Training Employment Readiness  As a first step, we explored some statistics of the Pre/Post- training Employment Readiness variables (see Table 1). Overall,  the mean Post (M=4.45, SD=0.61) is meaningfully and  statistically significantly higher than the mean c:tempPre  (M=3.66, SD=0.88), with t(53)=5.33, at p<0.001 (a paired- sample t-test was used). Overall, it seems that the course had  dramatically increased the level of participants' readiness to  work with computers, and that the Post-training Employment  Readiness might even demonstrate a ceiling effect.   3.2 Direct Relationship between the  Dependent and the Independent Variables  While exploring direct relationships between the dependent  variable and the independent variables  using independent- sample t-tests  we have found only two significant  relationships. These relationships were found to log-based and  satisfaction-related variables:    Participants with high Post-training Employment  Readiness values took the knowledge post-test (Knowledge  Post-Test Length) much quicker than those with low  values (~23 minutes, compared with ~37 minutes);    Participants with high Post-training Employment  Readiness values were satisfied with the course content   (Satisfaction with Content) more than those with low  values (4.59 on a 5-point Likert scale, compared with  4.23).   3.3 Predicting the Dependent Variable  We developed a decision tree model, using RapidMiner Studio,  to predict the dependent variable, using a manual forward  feature selection. Of the single-feature models, only one  performed better than chance. The final model has a LOOCV  kappa of 0.524. The tree size is 16, its height is 7, and it has 9  leaves (Figure 1, confusion matrix in Table 1). Overall, three  variables entered the best decision tree in the following order:  Knowledge Post-Test Length, Satisfaction with Content, Total  Number of Actions.   Post Test Length > 2911: L {L=7, H=0}  Post Test Length  2911  | Satisfaction with Content > 3.7  | | Num. of Actions > 14  | | | Num. of Actions > 19  | | | | Post Test Length > 1008  | | | | | Post Test Length > 1299  | | | | | | Post Test Length > 1596  | | | | | | | Num. of Actions > 61: H {L=1, H=7}  | | | | | | | Num. of Actions  61: L {L=2, H=1}  | | | | | | Post Test Length  1596: H {L=0, H=8}  | | | | | Post Test Length  1299: L {L=7, H=1}  | | | | Post Test Length  1008: H {L=0, H=6}  | | | Num. of Actions  19: L {L=2, H=0}  | | Num. of Actions  14: H {L=0, H=8}  | Satisfaction with Content  3.7: L {L=4, H=0}     Figure 1. Best decision tree prediction model for Post- training Employment Readiness (L=Low, H=High).      Table 1. Confusion matrix for the prediction model    Actual  Precision Low High   Prediction Low 13 2 86.7%  High 10 29 74.4%  Recall 56.5% 93.5%       4. REFERENCES  [1] Compeau, D.R. & Higgins, C.A. 1995. Computer self-  efficacy: development of a measure and initial test. MIS  Quarterly, 19(2), 189-211.   [2] Lee, C.C., Czarja, S.J., & Sharit, J. 2008. Training older  workers for technology-based employment. Educational  Gerontology, 35(1), 15-31.   [3] Salanova, M., Grau, R.M., Cifre, E., & Llorens, S. (2000).  Computer training, frequency of usage and burnout: the  moderating role of computer self-efficacy. 2000. Computers  in Human Behavior, 16(6), 575-590.   [4] Torkzadeh, R, Pflughoeft, K., & Hall, L. 1999. Computer  self-efficacy, training effectiveness and user attitudes: An  empirical study. Behaviour & Information Technology,  18(4), 299-309.  401      "}
{"index":{"_id":"69"}}
{"datatype":"inproceedings","key":"Xu:2015:IIN:2723576.2723663","author":"Xu, Zhenhua and Makos, Alexandra","title":"Investigating the Impact of a Notification System on Student Behaviors in a Discourse-intensive Hybrid Course: A Case Study","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"402--403","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723663","doi":"10.1145/2723576.2723663","acmid":"2723663","publisher":"ACM","address":"New York, NY, USA","keywords":"notification system, self-expectancy, student online behavior","abstract":"This study investigated the effects of students' opting to use notification tools in a collaborative discourse-intensive online graduate course. Social constructivism and self-expectancy theory were applied to frame our understanding of the interactive relationship between the use of the notification tools, student's online contribution behavior and student's self-expectancy. Log-data from a 12-week hybrid (online and face-to-face) graduate course at a Canadian faculty of education was analyzed. Findings from the correlation, mediation and ANOVA analyses suggested that activation of the notification tool system positively affected students' contribution behavior and that the influence of the use of notification tools on student contribution behavior was partially mediated by student's self-expectancy.","pdf":"         Investigating the Impact of a Notification System on  Student Behaviors in a Discourse-Intensive    Hybrid Course: A Case Study  Zhenhua Xu   University of Toronto  252 Bloor St W, Toronto, ON M5S 1V6   zhenh.xu@mail.utoronto.ca     Alexandra Makos    University of Toronto   252 Bloor St W, Toronto, ON M5S 1V6  alexandra.makos@mail.utoronto.ca  ABSTRACT  This study investigated the effects of students opting to use  notification tools in a collaborative discourse-intensive online  graduate course. Social constructivism and self-expectancy theory  were applied to frame our understanding of the interactive  relationship between the use of the notification tools, students  online contribution behavior and students self-expectancy. Log- data from a 12-week hybrid (online and face-to-face) graduate  course at a Canadian faculty of education was analyzed. Findings  from the correlation, mediation and ANOVA analyses suggested  that activation of the notification tool system positively affected  students contribution behavior and that the influence of the use of  notification tools on student contribution behavior was partially  mediated by students self-expectancy.   Categories and Subject Descriptors  H.5.3 [Group and Organization Interfaces]: Synchronous  interaction, Web-based interaction   General Terms  Design, Human Factors, Theory   Keywords  Student Online Behavior, Self-expectancy, Notification System     1. INTRODUCTION  The design of online learning environments influences student   behavior [2, 3]. To understand how the design influences  behavior, we need to understand the types of tracked information  and the ways information is presented to the learner [4]. A review  of the literature finds little empirical evidence for the importance  of using tracked information to guide learners self-regulated  behavior because this type of information is typically only  available to instructors or system administrators. This study was  intended to fill the gap by exploring how the use of personalized  notification tools increases user contribution behavior in a  collaborative online learning environment, Pepper, where online  discussions are a significant portion of students final grades.         Through the lenses of social constructivism and self-expectancy  theory, we examined the interactive relationship between  students self-expectancy (in this study described as students  perceived value for academic tasks and their prior professional  experience with the tasks), the use of notification tools embedded  within the online learning environment and their online  contribution behavior. Based on Atkinsons self-expectancy  model [1] achievement behavior is predicted by value, that is, a  students responses to the task. Responses would include those  with goals, values and interests. According to Weiner [6],  students prior experience with tasks is a useful predictor of  academic performance. In this case study, we treated the use of  notification tools as a type of strategy to guide learning and  examined its relationship with self-expectancy. Specifically, we  investigated the difference between its direct and indirect  relationship with student contribution behavior. We hypothesized:  1) that the use of the notification tools would be influenced by  students self-expectancy; 2) that the use of the notification tools  would cause a behavior change in students contributions in  Pepper  students would be more likely to contribute to Pepper if  they had the notification system activated; 3) that students self- expectancy would influence their contribution behavior.      2. SUMMARY OF DATA SOURCE  Data were drawn from the log files of a hybrid graduate   course using Pepper at a leading Canadian faculty of education.  The primary artifact students generated in the environment was a  note that can contain text, attachments, embedded videos and  images, hyperlinks to the Internet, and links to other students  notes within the environment. Student productivity metrics  include the following: Notes-written, Notes-read, Private- messages sent, replies made to peer notes, Replies-received from  peers, Likes- given, Likes-received, Links-created to peers notes  and Links- received to student notes. Pepper has six notification  tools: 1) Replies to one of your notes, 2) sends you a private  message, 3) posts a message to the class wall, 4) edits a note that  is co-authored by you, 5) links to one of your notes and, 6) adds a  new note to one of the various folders in the course community.  Each notification was coded as a dichotomous variable. The  number of notifications received for each tool was extracted and  treated as a continuous variable. A total of 73 students enrolled in  this 12-week course. Weekly online discussions were valued at  50% of the students final grade. In this course, students were  recommended to activate their notification system to keep them  aware of online discussion activity. 80% of students activated  notifications in the course.               Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.  Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723663      402            3. SUMMARY OF PRELIMARY RESULTS    Correlation analyses suggested the use of notification tools is  significantly associated with students online contribution and  their interactivity behaviors. Replies-received were significantly  associated with the number of notes written, the number of  messages written and the number of Likes-given. It suggests that  students who received more replies from peers or instructors  tended to write more notes (r = .88, p < .001) and more messages  (r = .28, p < .05). They also tended to give more Likes to peers  notes (r = .35, p < .05). The correlations between Likes-received,  the number of note written and the number of Likes-given was  significantly positive, which suggests receiving Likes motivates  desirable discussion behavior. Students who received more Likes  tend to write more notes (r = .71, p < .001) and give more Likes to  others (r = .51, p < .001). In addition, students who wrote more  notes tended to be linked more often by others (r = .43, p < .01).  Students who created more links to others notes also tended to be  linked more by their peers (r = .67, p < .001). Students perceived  task value was positively correlated with Link-notification tool (r  = .24, p < .05). That is, students who expected to expand their  knowledge and to apply it to their professional practice tended to  use the Link-notification tool more frequently. Students prior  professional experience with the academic tasks was also  significantly associated with the Link-notification tool (r = .23, p  < .05).    Further, mediation analysis indicated a mediated relationship  among students self-expectancy, the use of notifications and  student contribution behavior. Although, the results did not show  a direct causal relationship between student self-expectancy and  the use of notification tools, p > .05, given that the null hypothesis  was violated (H0: ab  0), the coefficient value from  unstandardized regression was significant from zero, (.33 .44) =  .15, suggesting a significant indirect effect of self-expectancy on  contribution behavior. To further test this indirect effect,  bootstrapping procedures were conducted. Unstandardized,  indirect effects were computed for each of 10,000 bootstrapped  samples, and the 95% confidence interval was computed by  determining the indirect effects at the 2.5th and 97.5th percentiles.  The bootstrapped unstandardized indirect effect was 1.46, and the  95% confidence interval ranged from .21 to 1.28. Thus, the  indirect effect was statistically significant, which suggested a  mediated path exists. Following the significant mediated path, we  saw a significant direct path from the used of the notification tools  and student online contribution behavior, c = 21.51, p = .02.  Given that c is significant and large, we could assume that the  influence of the use of notification tools on student contribution  behavior was partially mediated by student self-expectancy.   Finally, we ran comparison tests to explore the difference in  student online contribution behavior when they chose to use/not  use a notification system. Results suggested student contribution  behavior increased when the Response-notification tool was  turned on (MON = 1291.34 vs. MOFF= 1182.53); similarly, there  was an increase in contribution behavior when the Message- notification tool was turned on (MON = 1278.11 vs. MOFF=  1102.06). Further, when both the Co-author and Classwall- notification tools were turned on, student contribution behavior  tended to increase (MCoauthorON = 1352.78 vs. MCoauthorOFF =  1218.09; MClasswallON = 1288.31 vs. MClasswallOFF = 1205.04).  However, the use of the Link-notification tool did not make a  difference in student contribution behavior (MLinkON = 1179.67 vs.  MLinkOFF = 1245.52). In addition, results from ANOVA analysis  indicated a statistically significant relationship between students   prior experience with academic tasks and the number of notes  written, F(4, 65) = 3.87, p < .05, along with the number of replies  written, F(4, 65) = 3.56, p < .05. There was a significant  relationship between students perceived task value and the  number of links created, F(2, 65) = 6.47, p < .05. The use of the  Co-author-notification tool was significantly associated with the  number of notes written, F(1, 65) = 6.04, p < .05, and the number  of replies written, F(1, 65) = 5.34, p < .05. The use of the  Classwall-notification tool was significantly associated with the  number of notes written, F(1,65) = 3.68, p = .05. The relationship  between the Classwall-notification tool and the number of replies  written was also statistically significant, F(1, 65) = 5.34, p < .05.          4. CONCLUSIONS  In this case study, we investigated the effects of students   opting to use notification tools in a collaborative discourse- intensive online course. Data analysis supported our hypothesis of  active notifications positively influencing student contribution  behavior. Of the six notification types, the Private-message  notification tool was the most popular. Additionally, the Classwall  and Co-author- notification tools were positively associated with  note production in the learning community. Combined, these three  notification tools represent an awareness of opportunities for  students to engage in collaborative interactions in the  environment. The use of these notification tools may be an  indicator of the developed sense of collective cognitive  responsibility in the community [5]. The positive correlation  between notification use and student activity illustrates the need  for students to generate a sense of affirmation and connectedness  to their community. Findings from our data analysis also  suggested student self-expectancy partially mediated the impact of  the use of notification tools on student contribution behavior.  Given that students self-expectancy is the only mediator in this  study, further exploration is needed to help us understand what  factors could significantly influence this relationship.       5. ACKNOWLEDGEMENTS  Our thanks to Professors Jim Hewitt and Clare Brett and the  Pepper team for their dedication in designing an online learning  environment that supports discourse-intensive courses.          6. REFERENCES  [1] Atkinson J. W. (1964). An Introduction to Motivation. NJ:   Van Nostrand.   [2] Boettcher, J. V. (2007). Ten core principles for designing  effective learning environments: Insights from brain research  and pedagogical theory. Innovate: Journal of Online  Education, 3(3).    [3] Lulee, S. T. (2010). Basic Principles of Interaction for  Learning in Web-Based Environment.   [4] Verpoorten, D., Glahn, C., Kravcik, M., Ternier, S., &  Specht, M. (2009). Personalization of learning in virtual  learning environments. In Learning in the Synergy of  Multiple Disciplines (pp. 52-66). Springer Berlin Heidelberg.   [5] Scardamalia, M. (2002). Collective cognitive responsibility  for the advancement of knowledge. In B. Smith (Ed.) Liberal  Education in a Knowledge Society (pp. 67-98). Chicago:  Open Court.   [6] Weiner B. (1992). Human Motivation: Metaphors, Theories,  and Research. CA: Sage.   403      "}
{"index":{"_id":"70"}}
{"datatype":"inproceedings","key":"Ye:2015:MIE:2723576.2723653","author":"Ye, Shiwei and Sun, Yuan and Wang, Haobo and Sun, Yi","title":"Minimum Information Entropy Based Q-matrix Learning in DINA Model","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"404--405","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723653","doi":"10.1145/2723576.2723653","acmid":"2723653","publisher":"ACM","address":"New York, NY, USA","keywords":"Boolean matrix factorization, approximation algorithm, q-matrix, rule space","abstract":"Cognitive diagnosis models (CDMs) are of growing interest in test development and measurement of learners' performance. The DINA (deterministic input, noisy, and gate) model is one of the most widely used models in CDM. In this paper, we propose a new method and present an alternating recursive algorithm to learn Q-matrix and uncertainty variables, slip and guessing parameters, based on Boolean Matrix Factorization (BMF) and Minimized Information Entropy (MIE) respectively for the DINA model. Simulation results show that our algorithm for Q-matrix learning has fast convergence to the local optimal solutions for Q-matrix and students' knowledge states A matrix. This is especially important and applicable when the method is extended to big data.","pdf":"Minimum Information Entropy Based Q-matrix Learning in DINA Model  Shiwei Ye University of China Academy  of Science Beijing, China  shiwye@ucas.ac.cn  Yuan Sun National Institute of  Informatics Tokyo, Japan  yuan@nii.ac.jp  Haobo Wang University of China Academy  of Science Beijing, China  wanghaobo@ucas.ac.cn Yi Sun  University of China Academy of Science  Beijing, China sunyi@ucas.ac.cn  ABSTRACT Cognitive diagnosis models (CDMs) are of growing interest in test development and measurement of learners perfor- mance. The DINA (deterministic input, noisy, and gate) model is one of the most widely used models in CDM. In this paper, we propose a new method and present an alternating recursive algorithm to learn Q-matrix and uncertainty vari- ables, slip and guessing parameters, based on Boolean Ma- trix Factorization (BMF) and Minimized Information En- tropy (MIE) respectively for the DINA model. Simulation results show that our algorithm for Q-matrix learning has fast convergence to the local optimal solutions for Q-matrix and students knowledge states A matrix. This is especially important and applicable when the method is extended to big data.  Categories and Subject Descriptors D.2.8 [Metrics]: Performance measures; D.4.8 [Performance]: Measurements; F.2.1 [Numerical Algorithms and Prob- lems]: Computations on matrices; I.2.6 [Learning]: Con- cept learning, Parameter learning  General Terms Algorithms, Measurement, Performance, Theory  Keywords Rule Space,Q-matrix, Boolean Matrix Factorization, approx- imation algorithm  1. INTRODUCTION  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact the Owner/Author. LAK15 Mar 16-20, 2015, Poughkeepsie, NY, USA ACM 978-1-4503-3417-4/15/03. http://dx.doi.org/10.1145/2723576.2723653.  Cognitive diagnosis has recently gained prominence in e- ducational assessment, and other disciplines [1] [2] [3] [4]. Central to cognitive diagnostic models (CDMs) is the well- known Q-matrix, which specifies the item-attribute relation- ships. How to specify Q-matrix is without a doubt the most important issue to put diagnostic assessment into practice. In addition to typical Q-matrix specification by domain ex- perts, data-driven Q-matrix learning is attracting more at- tentions recently.  Among the CDMs, the DINA (deterministic input, noisy, and gate) model is one of the most widely used models and considered as the foundation [5] [6]. In this paper we propose a new framework for DINA model-based Q-matrix learn- ing under the assumption that attribute dimensions are not available in real situations. In Section 2, we spontaneously transfer the Q-matrix learning problem into Boolean Ma- trix Factorization (BMF) problem [7] to obtain attribute dimensions. Because BMF is an NP-hard problem [9][10], we develop an alternating recursive method to find approx- imate solution by adding one dimension in attribute latent space in each step. Accordingly we describe that uncertainty variables slip and guessing parameters in the DINA model can be analytically estimated through the Maximum Infor- mation Entropy(MIE). The simulation results are shown in Section 3 and finally the conclusion is given in Section 4.  2. CONJUNCTIVE MODEL, BMF AND DI- NA MODEL  For conjunctive model, based on the students knowledge states andQ-matrix for items, an ideal item response matrix, denotedR, can be generated , whose elementRij is typically represented in the following form.  Rij(A,Q) = Ki=1 qjk ik =  { 1 ik  qjk, k = 1, . . . ,K 0 ik < qjk),k  {1, . . . ,K}  (1)  Here Rij is the latent response variable of the ith student with the latent knowledge states i to the jth item, while qj indicating whether the student i has all the attributes k required for item j. It represents a deterministic prediction of item response from each students knowledge state.  Based on BMF [8], we verified the equation (1) can be  404    expressed in a Boolean relations of the knowledge states A matrix and the Q-matrix as follows (see [7] for details).  R = AQT = ( ks=1uisvsj  )mn (2)  where  represents the Boolean matrix product and  de- notes the maximum (truth function of logical disjunction).  In the DINA model, probability of a students correct re- sponse to an test item is determined by two error probabil- ities, and one latent response variable. The slip probability (sj) represents the probability of getting a wrong response on the jth item when all required attributes are present, while the guessing probability (gj) represents the probabil- ity of getting a correct response on the j th item when at least one required attribute is lacking. The students at- tribute patterns i = (i1, i2, ..., iK), called knowledge states, indicate the i th students mastery status in terms of the K attributes, and A represents students knowledge states matrix here.  For simplification, we denote B = A,C(i, j) = R(i, j), then the DINA model can be defined as the following.  P (R(i, j)|B,Q) = m i=1  n j=1  s R(i,j)C(i,j) j g  R(i,j)C(i,j) j (3)   (1 sj)R(i,j)C(i,j)(1 gj)R(i,j)C(i,j)  where C = B  QT . The optimization problem can be ex- pressed to find the maximum likelihood of conditional prob- ability on matrices B and Q  E(B,Q) =  m i=1  n j=1  lnP (R(i, j)|B,Q) (4)  The optimization process for the above equation can be done by our iterative algorithm on every column of matri- ces B and Q based on equation equation (2) and equation (4). After obtaining the matrix B and Q, the slip and guess- ing parameters can be obtained analytically. Due to space limitations, proofs and details of algorithms are omitted.  3. SIMULATION RESULTS The data we used in this study is from the paper [11],  where EM algorithm approach for DINA model-based Q- matrix learning have been proposed. Their research mainly focuses on statistical validation and hypothesis testing for the Q-matrix, and it also indicates sufficient and necessary conditions under which the model parameters are identifi- able from the response data. However, in their algorithms the number of attributes in the to-be-estimated Q-matrix is assumed to be known because true and designed Q-matrices are needed to be compared. Under the same settings as in the paper [11], the estimated Q-matrix and the true Q- matrix were compared based on our proposed algorithm. Due to lack of space, the detailed descriptions on our simu- lation and results cannot be shown here.  As a result, it was shown that our iteration algorithm had fast convergent speed. In the simulation, we repeated the experiment 100 times, and we also found that if the number of students M  1000, in every round the Q-matrix can be completely restored.  Based on our simulation results, it was revealed that our algorithm was immune to initial values of slip and guessing parameters as well. Both of them were always convergent to  the values between 0.1 and 0.2 which are normal values in general testing environment.  4. CONCLUSIONS In this paper, we proposed a new method and present-  ed an alternating recursive algorithm to learn Q-matrix and uncertainty variables slip and guessing based on Boolean matrix factorization (BMF) and maximum information en- tropy (MIE) for the DINA model of CDMs. Simulation re- sults show that our algorithm has fast convergence to the local optimal solutions for matrices B and Q. Moreover, our algorithm is also immune to initial values of slip and guessing parameters. The proposed method could be espe- cially important and effective when it is applied to big data.  5. ACKNOWLEDGMENTS This work was supported by JSPS KAKENHI Grant Num-  bers 26560134 and 25280121.  6. REFERENCES [1] Tatsuoka, K. K. 1983. Rule space: an approch for  dealing with misconceptions based on item response theory. Journal of Educational Measurement, 20, 345-354.  [2] Rupp, A., Templin, J. and Henson, R. A. 2010. Diagnostic Measurement: Theory, Methods, and Applications. Guilford Press.  [3] Barnes, T. 2010. Novel derivation and application of skill matrices: The Q-matrix method. Handbook on Educational Data Mining. 159-172.  [4] Koedinger, K.R., McLaughlin, E.A., Stamper, J.C. 2012. Automated student model improvement. In Proceedings of the 5th International Conference on Educational Data Mining.  [5] de la Torre, J. 2008. An empirically-based method of Q-matrix validation for the DINA model: Development and applications. Journal of Educational Measurement,45, 343-362.  [6] DeCarlo, L. T. 2012. Recognizing uncertainty in the Q-matrix via a Bayesian extension of the DINA model. Applied Psychological Measurement, 36, 447-468.  [7] Sun, Y., Ye, S.W., Inoue, S., Sun, Y. 2014. Alternating Recursive Method for Q-matrix Learning. Proceedings of the 7th International Conference on Educational Data Mining (EDM 2014), page 14-20.  [8] Vaidya, J. 2012. Boolean Matrix Decomposition Problem: Theory, Variations and Applications to Data Engineering. IEEE 28th International Conference on Data Engineering  [9] Belohlavek, R., Vychodi, V. 2010. Discovery of optimal factors in binary data via a novel method of matrix decomposition, Journal of Computer and System Sciences 76:3-20  [10] Miettinen, P., Mielikainen, T., Gionis, A., Das, G., Mannila, H. 2008. The Discrete Basis Problem. IEEE Transactions on Knowledge and Data Engineering, 20(10), 1348-1362.  [11] Liu, J., Xu, G., Ying, Z. 2012. Data-driven learning of q-matrix. Applied Psychological Measurement. 36(7), 548-564.  405      "}
{"index":{"_id":"71"}}
{"datatype":"inproceedings","key":"Kang:2015:FSI:2723576.2723644","author":"Kang, Raymond and Radinsky, Josh and Lyons, Leilah","title":"Frequent Sequential Interactions As Opportunities to Engage in Temporal Reasoning with an Online GIS","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"408--409","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723644","doi":"10.1145/2723576.2723644","acmid":"2723644","publisher":"ACM","address":"New York, NY, USA","keywords":"GIS, opportunities to learn, temporal reasoning","abstract":"Temporal reasoning (i.e., reasoning about relationships across time) is complex and difficult, particularly when engaged through complex media such as online Geographic Information System (GIS) applications. Partnering with Social Explorer (SE), a Web-based GIS application that allows users to create interactive visualizations of large sociological datasets, we engaged in frequent sequential pattern mining of a database of users' interactions with SE. The resulting frequent sequences provide initial descriptions of how SE affords opportunities to engage in temporal reasoning.","pdf":"Frequent Sequential Interactions as Opportunities to  Engage in Temporal Reasoning with an Online GIS   Raymond Kang1, Josh Radinsky1, Leilah Lyons1,2  1Learning Sciences Research Institute 2Computer Science   University of Illinois at Chicago  { rkang2, joshuar, llyons } @ uic.edu     ABSTRACT  Temporal reasoning (i.e., reasoning about relationships across  time) is complex and difficult, particularly when engaged through  complex media such as online Geographic Information System  (GIS) applications. Partnering with Social Explorer (SE), a Web- based GIS application that allows users to create interactive  visualizations of large sociological datasets, we engaged in  frequent sequential pattern mining of a database of users  interactions with SE. The resulting frequent sequences provide  initial descriptions of how SE affords opportunities to engage in  temporal reasoning.   Categories and Subject Descriptors  H.2.8 [Database Management]: Database Applications  data  mining, spatial databases and GIS; I.5.2 [Pattern Recognition]:  Design Methodology  pattern analysis.   General Terms  Experimentation, Human Factors   Keywords  GIS, temporal reasoning, opportunities to learn   1. INTRODUCTION  The increasing availability of very large data sets has captured the  imagination of researchers, educators, as well as the general  public, and there seems to be a consensus that developing the  skills and dispositions associated with making sense of such data  sets will be a focal point of future education. Access to large data  sets through sophisticated online tools has proliferated, yet little is  known about how these tools impact users interpretation of and  ability to manipulate these data. This is particularly true for the  use of interactive data visualization tools such as Social Explorer  (SE), a Web application that leverages Geographic Information  Systems (GIS) to create visualizations of U.S. census data. As  individuals interpret and manipulate data with SE, they encounter  a range of opportunities to explore and make sense of data sets,  but this diversity of opportunities can also lead to becoming  lost, unable to make sense of the perceived visualizations.    Temporal reasoning provides a source of difficulty for people  making sense of data visualizations such as those afforded by SE.  For instance, population change can be described via multiple  metrics that are often conflated and confused (e.g., net versus  percent population growth), especially when considering different  scales of time and historical contexts [3]. Here, we characterize  users sequential manipulations that change the data visualization  state as interactions with data (IWD), and attempt to identify  sequential patterns that are useful for modeling learners activity  with these tools. We start from the premise that users would  engage in at least two types of temporal IWDs with SE: (1)  toggling, where the user repeatedly queries data from two or more  specific time periods (e.g., 2000 Census, then 2010 Census, then  back to 2000 Census); and (2) stepping, where the user queries  data from consecutive time periods, either forward or backward in  time (e.g., from the 2011 American Community Survey [ACS] to  2012 ACS). Accordingly, this report focuses on analyzing users  sessions with SE, specifically by examining frequent sequential  patterns [4] that occurred during their interactions with SE and  using these frequent sequences as descriptors for IWD.    Since this study of SE Web-log files does not attempt to assess  users reasoning, we view these IWDs as opportunities to engage  in temporal reasoning, used here to better understand the range of  contexts in which users might learn to engage in temporal  reasoning. Toggling can provide opportunities for users to make  comparisons between data from two or more periods of time,  while stepping can provide opportunities for users to interpret  trends that emerge over time. By identifying these opportunities to  engage, in future work we will examine other logged interactions  surrounding these sequences. Ultimately, this work will allow us  to further characterize IWDs as opportunities to engage in  geospatial or quantitative reasoning, allowing descriptions for the  relationships within and between these IWDs as well as types of  reasoning. In the work presented here, we ask:    1. What frequent sequences emerge from the dataset when we  focus on temporal reasoning   2. How do these frequent sequences help describe the IWD  learners exhibit with SE, particularly toggling and stepping    2. METHODOLOGY  This initial analysis of temporal IWD relies on the data-selection  interaction with SE, which logs when the user changes the dataset  they wish to visualize. This comprises changes to any  combination of the year, survey (e.g., ACS 5-year estimates),  category (e.g., Race), and variable (e.g., African American).  These are the four elements of the data-tuple: <y, s, c, v> for year,  survey, category, and variable. As an initial exploratory study, the  data-tuple captured in the logs was used to generate an 11-tuple,  our metadata-tuple, <S, cy, cs, cc, cv, ny, ns, nc, nv, em, ts>. A  description of these metadata is provided in Table 1.   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.     Copyright is held by the owner/author(s).   LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723644     408    Table 1. Descriptions of metadata used in sequence analysis.   Label Description Database  Support (%)   S ID: Label for unique sessions N/A  cy, cs,   cc, cv   Boolean: True if elements of  the data-tuple were changed   y s c v  42 66 77 91   ny, ns,   nc, nv   Boolean: True if elements of  the data-tuple were not seen yet   y s c v  56 89 95 98   em Boolean: True if the exact data- tuple was seen earlier    48   ts Categorical: Whether the  change in year was non-  /consecutive   non- consecutive  32 19   Boolean values for changes to the data-tuple were generated to  help discover sequences that explored within as opposed to across  year-values. Boolean values for new elements of the data-tuple  allowed discovering sequences that may suggest more exploratory  IWDs than the two we hypothesized (toggling and stepping). The  em, or exact-match, Boolean value identified sequences that  indicated toggling behavior while the ts, or time-step, value  helped identify sequences of consecutive or non-consecutive  changes in time (the year). Metadata-tuples were created for each  session entry within a session that contained at least 10 entries and  at most 1000 entries, exclusion criteria that helped avoid bots and  trivial usage of SE. This resulted in a database of 5,856 sessions  recorded from the SE log system between the dates January 22,  2014 and February 22, 2014. These sessions had an average of  9.22 data- selection interactions (s.d. = 13.8).   For sequence mining, we used an open source, Java-based library  called SPMF (v. 0.96q) [1]. We applied the CM-SPADE [2]  algorithm, which uses co-occurrence information to support  pruning of sequences. Due to the enormous number of potential  sequences that could be discovered as well as the performance  information provided by the implementers, we believe this was  the best decision for our purposes. A minimum support (support is  the number of sessions that contain a variable of interest)  threshold of 0.19 (19%) was chosen using a guideline for setting  such thresholds [4], where the minimum support is set to the level  of frequency (here, of consecutive time-steps) within the database.   3. RESULTS AND DISCUSSION  With such a low threshold for minimum support, we generated  3,008,063 frequent sequences. Table 2 provides a brief overview  of these sequences with respect to our metadata-tuple, specifically  the time-step and exact-match elements. Other than the raw  number of sequences for each metadata element, the rest of the  values are means with parenthetical standard deviations.   Table 2. Summary statistics of frequent sequences based on  time-step and exact-match metadata.   Metadata # of  Seq.   Support  (%)   # of  Elements   # of Data  Selection  Events    ts non 504 21.0 (0.02) 4.26 (1.09) 1.91 (0.47)   cons. 4 19.2 (0.00) 2.00 (0.82) 1.00 (0.00)   em 7809 21.0 (0.02) 6.28 (1.45) 4.10 (0.84)   Even from the gross summary statistics, we see that taking  sequential, consecutive time-steps as an IWD is extremely rare  behavior for users of SE. Moreover, these stepping sequences  only contained one data-selection interaction, suggesting that  users did not have opportunities to engage in more complex   reasoning about trends (e.g., sequentially comparing trends of one  or more variables over time).   It is also of note that non-consecutive time-steps were rare in our  corpus of frequent sequences. Take, for example, the sequence (  denotes separate data-selection interactions): nv  non-ts  cv.  This sequence begins with a new variable, takes a non- consecutive time-step, and then changes the variable. We interpret  this sequence as an example of another IWDthat of leaping  between time points. Leaping can provide opportunities to engage  in comparison between variables in two or more different time  periods, but it also affords opportunities to explore (and  potentially get lost in) the dataset.    However, leaping IWD are an order of magnitude less represented  than toggling sequences containing exact-matches to previously  seen data-tuples. As a discussion point, let us use the following  sequence as an example: ns  em  em. This sequence contains  two data-selection interactions that led to an exact-match,  suggesting a common IWD is to explore elements of the data- tuple (here, the survey) and then toggle, one or more times, back  to previously seen data. This suggests that toggling is, in fact, a  frequent mode of interaction with the data maps in SE, one that  affords making multiple comparisons between a set of two or  more data visualizations. As we examine the temporal reasoning  users do when toggling, we will better understand how toggling  mediates inspecting temporal change.    4. CONCLUSION AND FUTURE WORK  This work elucidates the potential for using frequent sequences as  a means for detecting and characterizing IWDs and the  opportunities to learn afforded in those interactions. In so doing,  we provide a method to study how opportunities to learn are  mediated by tools such as SE. We hope to use a similar approach  to characterize IWDs relevant to other forms of reasoning  afforded by GIS (e.g., quantitative, geospatial, correlational).   Leveraging frequent sequences in order to identify IWDs as  opportunities to engage in reasoning moves toward two goals: (1)  furthering our understanding of IWDs as a useful unit of  interaction for studying opportunities to learn with tools such as  SE; and (2) surfacing design implications for map-interface query  tools to afford engagement with multiple types of complex  reasoning, including historical reasoning with census data.   5. ACKNOWLEDGMENTS  This material is based upon work supported by the National  Science Foundation under NSF INSPIRE Grant No. 1248052.   6. REFERENCES  [1] Fournier-Viger, P., Gomariz, A., Soltani, A., Lam, H., &   Gueniche, T. 2014. SPMF: Open-Source Data Mining  Platform. Retrieved from: http://www.philippe-fournier- viger.com/spmf/   [2] Fournier-Viger, P., Gomariz, A., Campos, M., & Thomas, R.  2014. Fast vertical sequential pattern mining using co- occurrence information. Proc. 18th Pacific-Asia Conference  on Knowledge Discovery and Data Mining (PAKDD 2014),  12 pages (to appear).   [3] Lemke, J. L. 2000. Across the scales of time: Artifacts,  activities, and meanings in ecosocial systems. Mind, Culture,  and Activity 7, 4, 273-290.   [4] Liu, B. 2011. Web Data Mining: Exploring Hyperlinks,  Contents, and Usage Data (2nd ed.). New York: Springer.   409      "}
{"index":{"_id":"72"}}
{"datatype":"inproceedings","key":"Hawn:2015:BRB:2723576.2723652","author":"Hawn, Aaron","title":"The Bridge Report: Bringing Learning Analytics to Low-income, Urban Schools","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"410--411","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723652","doi":"10.1145/2723576.2723652","acmid":"2723652","publisher":"ACM","address":"New York, NY, USA","keywords":"instructor support, learning analytics, predictive analytics, risk prediction","abstract":"Widespread adoption of learning analytics for risk prediction faces different challenges at low-income secondary schools than at post-secondary institutions, where such methods have been more widely adopted. To leverage the benefits of learning analytics for under-resourced communities, educators must overcome the barriers to adoption faced by local schools: internet access, data integration, data interpretation, and local alignment. We present the case study of an enhanced reporting tool for parents and teachers, the Bridge Report, locally designed to meet the needs of a low-income secondary school in New York City. Parent and Teacher focus groups suggest that addressing local obstacles to learning analytics can create conditions for enthusiastic adoption by parents and teachers.","pdf":"The Bridge Report: Bringing Learning Analytics   to Low-Income, Urban Schools    Aaron Hawn  Opportunity Charter School   240 W. 113th Street  New York, NY 10026   aaronhawn@opportunitycharter.org   ABSTRACT  Widespread adoption of learning analytics for risk prediction  faces different challenges at low-income secondary schools than  at post-secondary institutions, where such methods have been  more widely adopted. To leverage the benefits of learning  analytics for under-resourced communities, educators must  overcome the barriers to adoption faced by local schools: internet  access, data integration, data interpretation, and local alignment.  We present the case study of an enhanced reporting tool for  parents and teachers, the Bridge Report, locally designed to meet  the needs of a low-income secondary school in New York City.  Parent and Teacher focus groups suggest that addressing local  obstacles to learning analytics can create conditions for  enthusiastic adoption by parents and teachers.    Categories and Subject Descriptors  H.1.2 [User/Machine Systems]: Human factors, Human  information processing; K.3.1 [Computers and Education]:  Computer Uses in Education  Computer-assisted instruction   General Terms  Design, Human Factors   Keywords  Instructor Support, Predictive Analytics, Learning Analytics, Risk  Prediction   1. INTRODUCTION & CHALLENGES  As learning analytics interventions expand from higher education  to K-12 contexts, established post-secondary methods will not  simply trickle down to local schools. Predictive analytics and  reporting methods that work for undergraduates and university  administrations must adapt to low-income elementary and  secondary contexts to ensure widespread adoption. With low- income students making up almost half of the nations public  school population, closing the learning analytics cycle [3] with  effective interventions for these students offers large opportunities  for positive academic impact. We study methods to enhance K-12  learning analytics adoption by collaboratively designing solutions  to the challenges faced by one New York City charter school,  serving high-need secondary school students: 70% African- American, 30% Hispanic, and 70% qualifying for free/reduced  lunch. The case study school faces challenges to learning analytics  adoption similar to those faced by low-income schools across the   United States: lack of internet access [7], data fragmentation [2],  limited data interpretation skills [5], and the neglect of local  context.   2. THE BRIDGE REPORT  In an effort to overcome these obstacles to learning analytics  adoption, the school created a new reporting intervention, the  Bridge Report, to serve as a bridge to effective academic  intervention and to communication with families.    Initial report drafts were used as the basis for feedback in  interviews with school administrators, learning specialists, social  workers, and teachers. After each round of feedback, the reports  design was edited in an iterative process guided by the schools  Director of Analytics, who served as data guide or wrangler for  the larger instructional process [4]. Extensive feedback was used  to establish intervention thresholds for attendance, GPA,  discipline, and other measures. Figure 1 provides an example of  one students data presented in the Teacher View.   The current design and data elements of the Bridge report are  created in response to the particular challenges of low-income  secondary schools. In order to overcome families lack of internet  access, for example, the Bridge Report is distributed on a  quarterly basis in hard copy, as a supplement to the traditional  report card. To overcome a lack of data integration for both  teachers and families, developers combine data from several  online systems and data silos within the SIS. Figure 1 illustrates  the integration of several important indicators from distinct data  sources: quarterly GPA, absences, tardiness, discipline incidents,  state testing scores, Grade Level Equivalents in reading and math,  and Lexile scores. Combined in one report, these data provide  more holistic and efficient grounds for academic decision-making.    To address obstacles to local school alignment, the color of most  Bridge Report elements (red, yellow, or green) is determined by a  set of school-defined thresholds for intervention. Use of   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723652   410    red/yellow/green follows Course Signals successful traffic light  motif, which was found informative for students [1]. Color  thresholds represent both local and state criteria and provide  essential context for stakeholders involved in academic decisions.   The most controversial feature included in the report, for both  teachers and parents, is a predictive risk indicator for Level 1  performance on New York State ELA and Mathematics Tests.  Level 1 is the lowest of four proficiency levels, and indicates  student performance well-below proficiency. Interviews with  school staff indicated concern that negative predictions might de- motivate students. These concerns about student motivation stand  in contrast to survey feedback received on Purdues Course  Signals intervention system, where 74% of students reported that  Course Signals improved their motivation [1]. College students,  however, differ significantly from middle school students, and  further study is needed to gauge the impact of predictive reporting  directed toward this younger population.  Models used to generate the Risk of Level 1 Performance have  relied on linear regression, within a standard cross-validation  paradigm and have included features such as Computer Adaptive  Testing Pre-test scores, Days Present/Tardy, IEP Status, Grade  Level, and Office Discipline Referral Count. The most recent  models have achieved a cross-validated correlation to end-of-year  state test scores of r=0.748 for ELA and r=0.669 for Math.    3. FEEDBACK  A small number of parent interviews were conducted individually  during parent/teacher conferences (n=6). Parents were given a  copy of the Bridge Report for their own child, and interviewers  recorded their verbal response to survey questions about the  report. Focus group sessions with most of the schools middle  school teachers (n=22) were conducted as well. Grade-level sets  of reports were distributed to teachers and, after an introductory  session, teachers gave feedback through an anonymous survey.   3.1 Parent Feedback  Overall, the small number of parents interviewed responded very  positively to the report, especially to the inclusion of red, yellow,  green color cues and to the variety of information presented. Over  80% of parents Agreed or Strongly Agreed with the following  statements: (1) This report will help me make decisions about my  childs education, (2) This report will help me have effective  conversations with teachers and administrators, and (3) This  report matches my own understanding of my child in school.   In agreement with many teacher comments, parents indicated a  high-level of engagement with risk predictions about their childs  state test performance. 100% agreed or strongly agreed that (1)  the student risk predictions seemed accurate, that (2) as parents,  they took the predictions seriously, and that (3) these risks would  impact their academic decisions.    Parents were also asked to use the report to talk about their  childs strengths and weaknesses in school. The interviewer then  judged the accuracy of the parents understanding in relation to  the information contained in the report. Overall, 69% of parent  interpretation was Very Accurate or Mainly Accurate,  without any previous instruction in interpreting the reports.   3.2 Teacher Feedback  Overall, teachers were enthusiastic about the Bridge Report.  Comments included: This is great! I cant wait to use this and the  ease of understanding is very helpful. We have all the  information for each of our students in one place. Teachers were   able to suggest many possible uses for the Bridge Reports: parent  and student conferences, lesson planning, and determining  academic interventions.   Teacher positive comments about utility and ease of use suggest  that the project goals of data integration, ease of interpretation,  and local alignment may have been achieved. More than 80% of  teachers agreed or strongly agreed that (1) These reports will be  useful to me and (2) These reports will help me make more  effective academic decisions. Only 18% of teachers indicated  some concern that the reports might bias their behavior towards  students. When asked, How could these reports be more useful  teachers often responded with comments about increased local  alignment, requesting more specific information on students  current participation and performance in intervention services.    When asked about including risk predictions on reports, teachers  were positive about parents and fellow teachers receiving  predictions but conflicted about students receiving the same  predictions. For example, one teacher wrote: Predictions are  definitely beneficial for both the teachers and the parents, as it  gives both the time to plan interventions accordingly. Responses  to including predictions for students, however, were mainly  negative: I dont think it would be necessary to have it on the  copy for students. I feel like that would cause a lot of anxiety for  our students, who already have high enough anxiety about the  state tests. Or, Students can change anytime, so I do not think I  would include predictions. That is just making an assumption  based on current performance and students need a chance to  redeem themselves.    4. FUTURE WORK  Continuing work during the 2014-15 school year will explore the  motivational impact of student-facing reporting in an attempt to  position the Bridge Report as one piece of a comprehensive and  impactful learning analytics intervention [cf.6].   5. REFERENCES  [1] Arnold, K E. and Pistilli, M. D. 2012. Course Signals at   Purdue: Using Learning Analytics to Increase Student  Success. In Proceedings of the 2nd International Conference  on Learning Analytics and Knowledge. 267-270.   [2] Benjamin, H. 2014. Tearing Down the Walls Between  Software Silos. Education Week. 34.6 (Oct. 2014) s6-s7.    [3] Clow, D. 2012. The Learning Analytics Cycle: Closing the  Loop Effectively. In Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge. 134-138.   [4] Clow, D. 2014. Data Wranglers: Human interpreters to help  close the feedback loop. In Proceedings of the 4th  International Conference on Learning Analytics and  Knowledge. 49-53.   [5] Goodman, D. P. and Hambleton, R. K. 2004. Student Test  Score Reports and Interpretive Guides: Review of Current  Practices and Suggestions for Future Research. Applied  Measurement in Education 17.2 (2004): 145-220.   [6] Wise, A. F. 2014. Designing Pedagogical Interventions to  Support Student Use of Learning Analytics. In Proceedings  of the 4th International Conference on Learning Analytics  and Knowledge. 203-211.   [7] Zickuhr, R and Smith, A. 2013. Home Broadband 2013. Pew  Research Center. Washington, D.C.  411      "}
{"index":{"_id":"73"}}
{"datatype":"inproceedings","key":"Dodge:2015:IUS:2723576.2723657","author":"Dodge, Bernie and Whitmer, John and Frazee, James P.","title":"Improving Undergraduate Student Achievement in Large Blended Courses Through Data-driven Interventions","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"412--413","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723657","doi":"10.1145/2723576.2723657","acmid":"2723657","publisher":"ACM","address":"New York, NY, USA","keywords":"at-risk student prediction, blended learning, interventions, large enrollment courses, learning analytics, learning management systems, motivation, time logs","abstract":"This pilot study applied Learning Analytics methods to identify students at-risk of not succeeding in two high enrollment courses with historically low pass rates at San Diego State University: PSY 101 and STAT 119. With input from instructors, targeted interventions were developed and sent to participating students (n","pdf":"Improving Undergraduate Student Achievement in Large  Blended Courses Through Data-Driven Interventions   Bernie Dodge  San Diego State University   Learning Design and Technology   San Diego CA 92182-4561   +1 619 594-7401  bdodge@mail.sdsu.edu   John Whitmer  Blackboard, Inc  40 Gold Street   San Francisco, CA 94133  +1 530 554-1528   john.whitmer@blackboard.com   James P. Frazee  San Diego State University   Instructional Technology Services  San Diego CA 92182-8114   +1 619 594-2893  jfrazee@mail.sdsu.edu        ABSTRACT  This pilot study applied Learning Analytics methods to identify  students at-risk of not succeeding in two high enrollment courses  with historically low pass rates at San Diego State University:  PSY 101 and STAT 119. With input from instructors, targeted  interventions were developed and sent to participating students  (n=882) suggesting ways to improve their performance. An  experimental design was used with half of the students randomly  assigned to receive these interventions via email and the other half  being analyzed for at-risk triggers but receiving no intervention.  Pre-course surveys on student motivation [4] and prior subject  matter knowledge were conducted, and students were asked to  maintain weekly logs of their activity online and offline connected  to the courses. Regression analyses, incorporating feature  selection methods to account for student demographic data, were  used to compare the impact of the interventions between the  control and experimental groups. Results showed that the  interventions were associated with a higher final grade in one  course, but only for a particular demographic group.    Categories and Subject Descriptors  K.3.1. Computer Uses in Education   General Terms  Performance, Experimentation.   Keywords  Learning management systems, learning analytics, time logs,  motivation, blended learning, interventions, large enrollment  courses, at-risk student prediction.                  Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.     Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723657   INTRODUCTION  The intent of this study was to identify methods and interventions  that would reduce the number of students who fail early on in  their academic career. It was also designed to discover approaches  that could be practically adopted by faculty with minimal support  and thus are scalable to a large number of courses.   1. METHOD  1.1 Courses  Faculty from two courses with high numbers of students not  passing the course volunteered to participate in the study in the  Spring 2014 semester: Introductory Psychology (PSY 101) and  Introduction to Business Statistics (STAT 119).  Two sections of  each course were used in the study. Each of these courses met  twice a week for 75 minutes. One session was held face to face in  a 500-seat lecture hall with large screens and clickers; the other  was delivered online. Both the face to face and online lectures  were archived and available for students to access later.   1.2 Learner Profile  There were approximately 1,605 students enrolled in the four  sections involved in the study. Of this total, 882 (55%) consented  to participate in the study. The learners were predominantly  female (69%), and almost all were 17-19 years old (86%). 37% of  the students from under-represented minority groups.  There was  also a large participation (38%) by students from low socio- economic statuses, as indicated by eligibility for Pell grants.    1.3 Hypotheses  The research hypotheses driving this study are:   1. Student interaction data from multiple academic  technology sources provides a better prediction of  course achievement than LMS data alone.    2. Faculty interventions with at-risk students, identified  through academic technology interaction data, has a  positive relationship with student likelihood to pass the  course.    3. There are significant differences in the relationship  between student use of faculty interventions and student  achievement based on student characteristics  (motivation toward subject matter, demographic factors,  prior educational experience, and current education  status).    412    1.4 Research Design  An experimental design with independent measures was used.  Within each section, 50% of the students within each course  section were randomly assigned to either the treatment (n = 442)  or control group (n = 440). All other aspects of the courses were  identical. This approach allowed the isolation of the intervention  from other potentially confounding factors.   1.5 Demographic Data  After the semester was completed, demographic data were  analyzed to shed light on factors related to course achievement  and possible differences in results based on these characteristics.  These variables were pre-selected by the campus Institutional  Research department as having previously demonstrated  relationships with student achievement and/or identifying  populations deemed of special priority to provide support. A total  of 43 variables was identified, including race/ethnicity, socio- economic status, grade level, first-generation college student, etc.   1.6 Treatment  The interventions in this study were developed collaboratively  with the two faculty participants rather than being imposed on  them by technologists or administrators. As a result of these  ongoing conversations, the following triggers were identified:  Logins to the Blackboard LMS, exam and quiz grades, and clicker  points (a proxy for attending the live lecture).   Interventions were developed in the form of emails sent on behalf  of the faculty to students using the internal mail application within  Blackboard. The messages included online and in-person  resources that could help them improve their course performance.   2. RESULTS  All together, a total of 21 trigger events based on use reports  were created for students in PSY 101 and 19 trigger events were  created for students in STAT 119.  The distribution of the total  number of triggers flagged for students follows in Figure 1. As  can be seen, most students in the courses received one or more  triggers (70% STAT, 86% PSY).  Most students received a few  triggers, with the large majority receiving 1-4 triggers.       Figure 1 Number of Triggers Received      2.1 Accuracy of Triggers at Predicting Grade  In both courses, the triggers were significantly (p<0.0001) related  to student final grade for students receiving at least one trigger.   Figures two and three demonstrate this relationship for the STAT  and PSY courses. The STAT course, which had a lower overall  number of triggers as a percentage of students, had a stronger   relationship between the triggers and final grade, explaining 66%  of the variation in final grade.  The PSY course, while still  significant, had a weaker relationship, explaining 48% of the  variation in final grade.     2.2 Effectiveness of Interventions  Analysis of variance (ANOVA) found no significant difference  for either course between the experimental and control groups on  course achievement,    Looking more deeply by taking demographic variables into  account, we did find a significant difference between achievement  based on one variable: Pell-eligibility, which is a proxy for low  socio-economic status.  Among PSY students, those students who  were Pell-eligible and received the interventions were less likely  to have a grade requiring them to repeat the course (e.g. < C-).  Pearson Chi2 analysis comparing the Pell and non-Pell  populations by participating in the control group resulted in  Chi2(1) = 6.4007, Pr = 0.000. Within the control group, 67 passed  the course and 20 did not. As Figure 2 illustrates, within the  experimental group 91% passed the course and 9% did not.    If all Pell eligible students had received the interventions, we can  estimate that all but 15 students would have passed the class, an  improvement over the 27 that need to take it again.     Figure 2: Relationship Between Receiving Interventions and  Passing the PSY 101 Course (Pell-Eligible vs All Students)     This differential effect of interventions on the Pell-Eligible  students was not seen in the STAT 119 class, however.     3. DISCUSSION  Faculty and researchers designing this study perceived the email  interventions to be the best treatment possible within the  constraints of the study, but likely to have a limited impact on  student achievement.  There are certainly many other  interventions that one could imagine (e.g. supplemental  instruction, one-on-one tutoring sessions, directed study  resources) with a higher potential impact.    4. CONCLUSION  The limited impact of the emailed interventions used this year  suggests the need for stronger interventions. The authors are  currently laying out a framework for describing and designing  interventions for future study.     5. ACKNOWLEDGMENTS  Thanks to San Diego State University President Elliot Hirshman  for his Building on Excellence strategic plan and to Geoffrey  Chase, Dean of Undergraduate Studies for supporting this effort  through the Learning Analytics Working Group.  413      "}
{"index":{"_id":"74"}}
{"datatype":"inproceedings","key":"Niemann:2015:IAL:2723576.2723660","author":"Niemann, Katja","title":"Increasing the Accessibility of Learning Objects by Automatic Tagging","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"414--415","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723660","doi":"10.1145/2723576.2723660","acmid":"2723660","publisher":"ACM","address":"New York, NY, USA","abstract":"Data sets coming from the educational domain often suffer from sparsity. Hence, they might comprise potentially useful learning objects that are not findable by the users. In order to address this problem, we present a new way to automatically assign tags and classifications to learning objects offered by educational web portals that is solely based on the objects' usage.","pdf":"Increasing the Accessibility of Learning Objects by Automatic Tagging  Katja Niemann Fraunhofer Institute for Applied Information Technology FIT  Schloss Birlinghoven Sankt Augustin, Germany  katja.niemann@fit.fraunhofer.de  ABSTRACT Data sets coming from the educational domain often suffer from sparsity. Hence, they might comprise potentially use- ful learning objects that are not findable by the users. In order to address this problem, we present a new way to auto- matically assign tags and classifications to learning objects offered by educational web portals that is solely based on the objects usage.  Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications data mining ; J.1.3 [Computer Applications]: Adminis- trative data processingeducation  General Terms Algorithms, Experimentation.  1. INTRODUCTION Many educational web portals allow users and domain ex- perts to manually enrich the learning resources with social metadata like free-text tags or classifications from a con- trolled vocabulary. This social metadata provides power- ful knowledge that can be used to improve the quality of searching and recommendations [2]. Nevertheless, data sets coming from the domain of technology enhanced learning often suffer from sparsity in respect to semantic and social metadata describing the learning objects which hinders their accessibility and impedes their recommendation [4].  In order to deal with this problem, we propose to automati- cally exchange tags and classifications between similar learn- ing objects. Here, the similarity of two objects is calculated by comparing their usage contexts, i.e. the objects they were most often used with. This way, our approach is different from well-known approaches for automatic tagging that base on the objects content or on tag co-occurrences [1, 2].  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author. LAK 15 Mar 16 - 20 2015, Poughkeepsie, NY, USA Copyright is held by the owner/author(s). ACM 978-1-4503-3417-4/15/03. http://dx.doi.org/10.1145/2723576.2723660  The paper is structured as follows. Chapter 2 presents our proposed approach. Chapter 3 introduces the data set that is used for the evaluation which results are discussed in chap- ter 4. Finally, chapter 5 gives a conclusion.  2. APPROACH 2.1 Usage Context-based Object Similarity We assume two learning objects to hold a semantic rela- tion if they are often used in the same usage contexts, i.e. together with the same co-occurrences. Figure 1 shows two exemplary user sessions from two different users. Both users access a text documents describing the Grand Palais and a picture showing the Centre Pompidou. Thereafter, the first user accesses a video about Renzo Piano while the second user opens a text document having Richard Rogers as topic. This means that the learning objects about Renzo Piano and Richard Rogers are not used together but they are used in similar usage contexts, i.e. together with the same learn- ing objects. Hence, the learning objects about Renzo Piano and Richard Rogers are assumed to hold a semantic rela- tion. This is in fact true, as they denote those two architects who designed the Centre Pompidou together. This example shows how the knowledge of the users is implicitly given by their activities and can be exploited without asking them to explicitly share it. A detailed description of this approach can be found in [3].  Figure 1: Exemplary user sessions  2.2 Automatic Tagging The automatic tagging is a process that is conducted in three steps. First, all pair-wise similarities between the learning objects are calculated based on their usage. Second, for each object all tags and classifications that are assigned to a sim- ilar object are selected as candidate tags. In order for two objects to be assumed to be similar their similarity must ex- ceed a pre-defined threshold. Third, the candidate tags are weight and only the most promising ones are selected. The weight of a tag or classification in respect to a learning object is calculated by accumulating and normalising the similarity values of all objects that hold this tag or classification and are similar to the selected learning object.  414    3. DATA SET The MACE (Metadata for Architectural Contents in Eu- rope) portal enables users to search for learning objects across repository boundaries and filter the results, e.g. ac- cording to their language, the original repository, and the classification terms they hold. 78.69% of the 12,442 learning resources accessed so far hold semantic metadata, i.e. 70.8% hold tags, 14.83% hold classifications, and 8.82% hold both in which each tagged learning resource holds on average 6.59 tags and each classified learning resource holds on average 2.27 classifications. As the tags are free text, about 73% of the 13,291 distinct tags are only used once and only about 4% of the tags are added to more than 10 objects. This shows the speciality of the tags which impedes the discov- ery of the objects they describe. The classification values are slightly more equally distributed with about 39% of the classifications being only used once and about 77% of the classifications being used at maximum 5 times.  4. EVALUATION In the conducted experiment we assumed two objects to be similar as soon as their usage-based similarity is higher than zero while the threshold t, i.e. the weight a tag or classifi- cation must exceed in order to be added to a new learning object, is varied from 0.1 to 1.0. This way, the number of tagged learning objects (i.e. 8,620) raises by at least 8% (9,309 tagged objects with t=1.0) up to 25% (10,783 tagged objects with t=0.1). Additionally, the number of tags an object holds on average increases drastically. When setting the threshold t to its maximum, a tagged object holds on average 34.72 tags while with a lower threshold, the number of tags per object raises up to 680.06 (with t=0.1). Further- more, the number of classified objects (i.e. 1,806) increases by 19% (2,144 classified objects with t=1.0, each holding on average 2.72 classifications) up to 245% (6,229 classified objects with t=0.1, each holding on average 7.59 classifica- tions).  The number of tags that were only assigned to one ob- ject halve through the automatic tagging process, i.e. from 73.33% (9,746) to 37.1% (4,930). In turn, the percentage of the tags that are used 2-100 times increases from 26.24% (3,487) to 56.65% (7,530). Though, the percentage of tags that are assigned to more than 100 objects only increases from 0.44% (58) to 6.25% (831). This shows that indeed the rarely used tags are assigned more often to learning objects and not (only) the already frequently used ones. The same effect can be observed for the classifications. The percent- age of classifications that are only used once decreases from 38.59% (336) to 19.06% (166) in the MACE data set. The percentage of classifications used 2-5 times only increases slightly from 38% (331) to 44.55% (368). Finally, 36.62% of the classifications are assigned to 10-100 objects after the au- tomatic tagging while the number of classifications assigned to more than 100 objects stays the same.  In order to evaluate the quality of the automatic added tags and classifications we use them to calculate rating predic- tions. Figure 2 shows the mean average error (MAE) and the coverage (i.e. number of user-item pairs in the test set for which a rating can be predicted) of the content-based recommender systems for the MACE data set and varying thresholds. It can be seen that the coverage increases with  a decreasing threshold. This is due to the fact that more tags and classifications can be assigned to the learning ob- jects which leads to more content-based object relations. The MAE increases as well with an increasing threshold. However, the MAE can even be improved compared to the baseline if the threshold is high enough. Finally, the classifi- cations neither increase the MAE nor the coverage as much as the tags. This can be motivated by the fact that less classifications than tags are added.  baseline tags + classifications added tags added classifications added  0.5  0.52  0.54  0.56  0.58  0.6  0.62  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  m e  a n   a v  e ra  g e   e rr  o r  threshold  (a) Mean average error  50  55  60  65  70  75  80  85  90  95  100  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1  co v e ra g e  threshold  (b) Coverage  Figure 2: Performance of the content-based recommender  5. CONCLUSION We proposed a new way to automatically enhance the se- mantic metadata representations of learning objects offered by educational web portals to enhance their accessibility. This is done by exchanging tags and classifications between the objects based on the objects usage context-based simi- larities. Thus, no content information about the objects is needed to detect similarities. We found that this way even learning objects that previously did not hold any tags or classifications could be automatically tagged and classified. Furthermore, we found that especially the rarely used tags and classifications were assigned to new objects.  6. ACKNOWLEDGMENTS The work presented in this paper has been partly supported by the project Open Discovery Space project that is funded by the European Commissions CIP-ICT Policy Support Program (Project Number: 297229).  7. REFERENCES [1] E. Diaz-Aviles, M. Fisichella, R. Kawase, and  W. Nejdl. Unsupervised Auto-tagging for Learning Object Enrichment. In Proc. of the 6th European Conference on Technology Enhanced Learning (EC-TEL 11), pages 8396. Springer, 2011.  [2] S. Lohmann, S. Thalmann, A. Harrer, and R. Maier. Learner-Generated Annotation of Learning Resources - Lessons from Experiments on Tagging. In Proc. of the International Conference on Knowledge Management (I-KNOW 2008), pages 304312, 2008.  [3] K. Niemann and M. Wolpers. Usage Context-Boosted Filtering for Recommender Systems in TEL. In Proc. of the 8th European Conference on Technology Enhanced Learning (EC-TEL 13), pages 246259, Berlin Heidelberg, 2013. Springer.  [4] K. Verbert, N. Manouselis, X. Ochoa, M. Wolpers, H. Drachsler, I. Bosnic, S. Member, and E. Duval. Context-aware Recommender Systems for Learning: a Survey and Future Challenges. IEEE Transactions on Learning Technologies, 5(4):318335, 2012.  415      "}
{"index":{"_id":"75"}}
{"datatype":"inproceedings","key":"Shehata:2015:MSS:2723576.2723661","author":"Shehata, Shady and Arnold, Kimberly E.","title":"Measuring Student Success Using Predictive Engine","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"416--417","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723661","doi":"10.1145/2723576.2723661","acmid":"2723661","publisher":"ACM","address":"New York, NY, USA","keywords":"algorithms, data mining, learning analytics, machine learning, predictive modeling, regression analysis, student success","abstract":"A basic challenge in delivering global education is improving student success. Institutions of education are increasingly focused on improving graduation and retention rates of their students. In this poster, we describe Student Success System (S3) that can measure student performance starting from the first weeks of the semester and the adoption process for S3 by University of Wisconsin System (UWS).","pdf":"MeasuringStudentSuccessUsingPredictiveEngine ShadyShehata D2LCorporation  151CharlesStreetWest,Suite400 Kitchener,ON,Canada,N2G1H6  15197720325ext.3257 Shady.Shehata@d2l.com  KimberlyE.Arnold UniversityofWisconsin  1305LindenDrive Madison,WI53706  T6082639443 kimberly.arnold@wisc.edu  ABSTRACT A basic  challenge  in  delivering  global  education  is  improving student success. Institutions of education are increasingly focused on improving graduation and retention rates of their students. In this  poster, we  describe Student  Success  System (S3)  that  can measure student performance starting from the first weeks of the semester  and  the  adoption  process  for  S3  by  University  of Wisconsin System (UWS).   Categories and Subject Descriptors G.3 [PROBABILITY AND STATISTICS]  General Terms Terms:  Algorithms,  Management,  Measurement,  Performance, Design, Experimentation.  Keywords Learning Analytics, Data Mining, Machine Learning, Predictive Modeling, Regression Analysis, Algorithms, Student Success.  1. Introduction  Advancements  in  data  analysis  and  predictive  modeling  have tremendous  potential  to  improve  student  success  by  enabling colleges and universities to build powerful predictive models that predict student behavior. S3 is an Early Intervention System that empowers  institutions  with  predictive  analytics  to  improve student success, retention, completion, and graduation rates.  S3 provides  educators  with  early  indicators  and  predictions  of student success and risk levels. Predictions generated by S3 are based on predictive models that are created by applying machine learning  algorithms  on  historic  course  data  (usually  prior offerings  of  the  same  course  for  which  predictions  are  to  be generated).  The  predictive  models  are  adaptable  and customizable to the instructional approach of each course, as well engagement and  ____________________________  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies  bear  this  notice  and  the  full  citation  on  the  first  page. Copyrights for third-party components of this work must be honored. For all other uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723661  achievement  expectations.  The  system  does  not  currently recommend and deliver content based on the analysis of a users performance or learning style.  Section 2 describes  the Student Success  System  predictive  modeling.  Section  3  describes  the process  of S3 adoption at  the  University of Wisconsin  System (UWS).  2. Student Success System S3 is developed by D2L Corporation and is currently available in production. The core component of S3 System is the predictive engine that is able to generate customized predictive model for an  individual  course.  The  S3  predictive  engine  generates regression models for each course. Figure 1 shows the main page of the student success system where instructors can monitor the status of each student in terms of their predicted success index.   The success index is expressed as score on a scale of 0-10, and trend indicated by the color and shape of an associated symbol: At  Risk  (red  triangle),  Potential  Risk  (yellow  diamond),  and Successful  (green  circle).  The  levels  are  determined  based  on thresholds on the predicted grade. The defaults are: 0%-60% for At-Risk,  60%-80%  for  Potential  Risk,  and  80%-100%  for successful.  The success index is broken down into five success predictors, a.k.a. Domains as shown in Figure 2 win-loss chart. The predictive model can be configured by the S3 administrator to include or exclude each of these predictors. When selected, the individual success predictors are combined to provide the overall value of the success index.  The importance (weight)  associated with  individual  predictors  is  automatically  determined  by the predictive  model.  The  success  index  that  is  generated  by regression model can be aggregated either  at the domain or the variable  levels.  The  model  aggregation  determines  the  type  of aggregation  for  calculating  the  success  index.  The  domain indicators represent the success/risk outcome based on the set of  Figure 1. Student Dashboard. .  416  http://dx.doi.org/10.1145/2723576.2723661   measurements related to each domain. An alternative aggregation level is for the success index to be determined based on variables without the pre-defined grouping of variables them into domains.  At domain  aggregation,  the  success  index  is  generated  in  two steps.  First,  each domain model generates predictions based on the  domain  variables.  Then,  the  overall  domain  generates  the overall success index based on the output of each domain model as shown in Figure 3. At variable aggregation, the success index is generated in one step where the variables of all  domains are used to generate the overall success index as show in Figure 4.  3. UWS Adoption of S3 The University of Wisconsin System (UWS) was awarded a grant to  begin  exploring  data  analytics  to  improve  student  success. UWS was looking for a comprehensive solution that would allow instructors  access  to new insights  about  how to better  support their students. Since the composition of UWS totals over 150,000 unique  individuals  each year, UWS was  looking for a solution that was robust enough to support exceptionally diverse learners. A  rich  source  of  contextualized  data  was  available  from  a decades use of the Desire2Learn learning management system.  Therefore,  UWS  partnered  with  D2L  to  begin  piloting  the Student Success System.  In the pilot period, five campuses and roughly  6,500  students  have  experienced  S3.  Since  S3  is designed  on  a  premise  of  probabilistic,  predictive  modeling, UWS  felt  strongly  that  the  information  serve  as  a  proactive catalyst  for  instructors.   For  these  reasons,  pilot  faculty were selected  based  on  evidence  of  iterative  student  intervention. Instructor  interest  was  gauged  and  if  the  instructor  chose  to participate, a stipend was awarded to ensure sufficient time was dedicated to the pilot. One of the most appealing features of S3 was  the  course-level  models  for  student  success.  However, predicting risk can lead to adverse effects on students, especially those early in their post-secondary career. For this reason, UWS dedicated  significant  resources  to  evaluate  how models  were: configured  and  selected,  behaved  over  time  (semester),  and impacted  individual  students.  For  each  pilot  course,  model criteria  and  historical  course  data  was  gathered,  domain configurations were created, and models were built in simulation mode.   A model matrix  was then created for each pilot course. Each model matrix  laid out between 18 and 25 possible  model configurations  along  with  aggregate  error  measures  (Mean Squared  Error  and  Average  Percent  Correct),  and  error  for students with unsuccessful outcome (grade of D or F). After the matrices  were  created,  each  matrix  was  presented  to  the  pilot faculty member, and up to three models were selected for more detailed  review.  Aggregate  error  measure  and  other  associated metrics  can  be  very  helpful  in  delineating  between  potential models.  However,  in  UWSs  estimation,  aggregate  measures alone were not sufficient to select a model for high-risk decision making.   Therefore, after faculty selected up to three models for more detailed review, and student level previews were generated (as shown in Figure 5).   These renderings allow for a student- level view of how a model acts across the weeks in a semester, providing an additional level of information to help the instructor and  S3  administrator  make  the  best  model  selection.  Of  key importance  at  this  juncture  is  to  evaluate  various  model configurations  for  base-level  disparate  student  impact. Additionally, the individual  student-level preview allows for an easy  visual  cue  for  many  instructors  about  when  a  model stabilizes  (week  3  versus  week3,  for  example)  to  help  them design their intervention strategy.  After the instructor evaluated  the model matrix  for her  course, followed by the student level-preview, the instructor, selected the model that she felt would best allow her to support the students in her  class.  Instructors relied on the error measures  and other metrics,  along  with  which  model  would  supplement  their intervention and support strategies.  The ultimate model selection was always left to the discretion of the course instructor. S3 then automatically generated new predictions every seven days.  Figure 5. Sample S3 Student-Level Preview. .  Figure 3. Domain Aggregation. .  Figure 4. Variable Aggregation. .  Figure 2. Win-Loss Chart. .  417      "}
{"index":{"_id":"76"}}
{"datatype":"inproceedings","key":"Maruya:2015:LSU:2723576.2723655","author":"Maruya, Kazushi and Watanabe, Junji and Takahashi, Hiroyuki and Hashiba, Shoji","title":"A Learning System Utilizing Learners' Active Tracing Behaviors","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"418--419","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723655","doi":"10.1145/2723576.2723655","acmid":"2723655","publisher":"ACM","address":"New York, NY, USA","keywords":"dynamic text display, finger tracing, interpersonal interaction","abstract":"A monitoring system that does not disturb learners' motivation and attention is important, especially in online learning with massive numbers of participants. We propose a learning system, called the finger trail learning system (FTLS), that can monitor participants' learning attitude by means of their finger movements. On the display of the FTLS, letters are presented with low contrast in the initial state, and the contrast of the letters changes to high when they are traced by learners. We implemented the FTLS as an iOS application and confirmed that the software can be utilized to monitor learners' attitudes. In addition, we compared trails of finger movements between participants with high and low performance. The results show that the trail of finger movements recorded by the FTLS can be an index of learners' attitudes.","pdf":"A learning system utilizing learners active tracing  behaviors.   Kazushi Maruya            Maruya.kazushi  @lab.ntt.co.jp   Junji Watanabe           Watanabe.junji  @lab.ntt.co.jp      Hiroyuki Takahashi           Hiroyuki-takahashi  @east.ntt.co.jp   Shoji Hashiba            shouji.hashiba  @east.ntt.co.jp     ABSTRACT  A monitoring system that does not disturb learners motivation  and attention is important, especially in online learning with  massive numbers of participants.  We propose a learning system,  called the finger trail learning system (FTLS), that can monitor  participants learning attitude by means of their finger  movements. On the display of the FTLS, letters are presented  with low contrast in the initial state, and the contrast of the  letters changes to high when they are traced by learners. We  implemented the FTLS as an iOS application and confirmed  that the software can be utilized to monitor learners attitudes.  In addition, we compared trails of finger movements between  participants with high and low performance. The results show  that the trail of finger movements recorded by the FTLS can be  an index of learners attitudes.     Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Distance learning   General Terms  Design, Human Factors    Keywords  Interpersonal interaction, Dynamic text display, Finger tracing   1. INTRODUCTION  In the current paper, we propose a system for learning text  materials, called the finger tracing learning system (FTLS). The  FTLS is developed based on an interactive dynamic text display  format using users finger tracing (Figure 1) [6]. In the initial state,  the contrast of the letter is set low and the displayed letters are  barely visible. When learners trace the display, the contrast of  letters under the touched area increases gradually and reaches the  maximum level. The contrast of touched letters stays at the  maximum level until the user reset the contrast of all letters into   the initial state by pressing a software reset button. As shown in  Figure 1, texts displayed with this dynamic method can be mixed  with texts displayed with a conventional static method. The  software can record touched positions on the display continuously.        Figure. 1. Dynamic text displays      The FTLS may appear to be ordinary, since this system is  apparently similar to systems in which the contrast of letters is  changed a button press. However, tracing requires continuous  commitment to the learning materials whereas a button press is  finished in an instant and provides little information. The trail of  the tracing would contain richer information than a simple button  press. The FTLS may also be somewhat similar to highlighting a  text using some tool. The learner, however, cannot see the text  until they touch it. A merit of this system with a dynamic text  display [2, 4, 7] is that the learner is strongly motivated to actively  trace the learning materials. Without the learners self-motivation,  a learning program using this system does not work.   Using the FTLS, The teacher can monitor learners reading  behaviors by viewing their finger movement data. Analyses of the  reading speed and acceleration at locations of texts is useful to  grasp the extent that the learner focuses on the learning materials.  The level of comprehension depends on reading speed. This  dependency was also observed in the reading from screens [1, 5].  Also, it is often reported that reading from computer screen is  skimming, i.e. the reading is fast and desultory rather than in  detail [3]. In the FTLS, the learner should read the materials by  tracing the text carefully from the beginning to the end. The  focusing of learners attention is indispensable for ensuring good  tracing behavior that can be monitored both in online and offline  manners. These characteristics prohibit learners skimming and  promote learners to read the text slowly. This slowdown in  reading rate would lead better understanding of the reading  materials.        Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.     Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723655   NTT Communication  Science Laboratories  3-1 Morinosato Wakamiya, Atsugi, Kanagawa,   Japan   NTT-ME Corporation  3-21-14, Higashi Ikebukuro, Toshima-ku, Tokyo,   Japan   418    2. EXAMINATION OF THE FTLS IN THE  CLASSROOM  In this study, we examined the effectiveness of the FTLS by  recording the students finger movements, when they learn using  it. In addition, we compared trails of finger movements between  participants with high and low performance.   2.1 Procedures  The lecture described a safety training program for laying  telephone lines and lasted about 65 minutes. The lecture targeted  young employees of Nippon Telegram and Telephone  Corporation with little knowledge about safety in the laying  operation.    In the lecture, learners were instructed to trace the text displayed  on their own iPad screen according to teachers reading. The  contrast of most of the words (without section titles and leading  texts) was set very low so that the words were barely visible. The  contrast of letters under the touched area increased gradually and  reached the maximum level in 2 sec. The key words of the  contents were highlighted in red. Short sets of a lecture and  review with the FTLS, each of which lasted 1 to 5 minutes, were  performed 11 times, and then ten minutes were provided for  learners to review all contents.    The class ended with a memory test and questionnaire. In the  memory test, learners were asked to recall the 71 key words to fill  blanks in the text displayed on the laptop PC. The tracing  behaviors of the learners were recorded throughout the lecture.  The number of learners was 47. To compare the results for the  FTLS (tracing condition), a lecture was also given to another 42  learners without using the FTLS. These learners just viewed the  same material with a static text display consisting of letters with  the highest contrast (seeing condition).   2.2 Results  We pooled the scores of the memory tests for each condition  (tracing/seeing) and averaged the scores across participants  (Figure 2a). The mean score for the tracing condition was  significantly larger that for the seeing condition (t(75)=2.57,  p=.012).    Next, we superimposed tracing data from five learners who got  higher (>85 %) scores and compared them with data from five  learners who got lower (< 50 %) scores (Figure 2b). The amount  of finger tracing was generally more in the higher-score group  than in the lower-score group. These results suggest that the  appropriate tracing behavior enhanced the learners performance.    Thus, the results show that the FTLS generally enhances the  performance of learners.  When a learner uses the FTLS, they are  required to attend to the display to perform appropriate finger  movements.  In addition, a pilot observation showed that the time  to read the text by tracing is generally longer than the time to read  the text silently. These characteristics of reading with tracing  behavior make the relationship between the learner and learning  materials deeper and more active than in silent reading.         Figure. 2. Results of examination. a: tracing data. b: leaners   performance in the tracing and seeing conditions.     3. DISCUSSION   The FTLS can monitor the tracing behavior of learners. When the  number of learners increases, monitoring by the teacher becomes  difficult. An automatic monitoring system would help in the  promotion of learning in a large-scale classroom. The finger  position data were considerably different among learners. The  extent of learners active attitudes may be reflected in some  features of the tracing data, such as the length of the trace, the  total amount of acceleration in the tracing finger movement, and  the frequency with which important words are traced. A detailed  analysis of the relationship between these features in the tracing  data and the learning performance would be the next step. The  quantitative relationship between the tracing data and the learning  performance would be a basis for automatically monitoring  learners behavior to support the teachers role in both classical  and online education.   4. REFERENCES  [1] Dyson, M.C., Haselgrove, M. 2000. The effects of reading   speed and reading patterns on the understanding of text  read from screen, Journal of Research in Reading, 23(2),  210-223   [2] Forlizzi, J., Lee, J.C. and Hudson, S.E. 2003. The Kinedit  System: Affective Messages Using Dynamic Texts,  Proceedings of CHI 03, ACM, New York, NY, 377-384.   [3] Horton, W., Taylor, L., Ignacio, A., & Hoft, N.L. 1996. The  web page design cookbook : all the ingredients you need to  create 5-star Web pages, NewYork: John Wiley.   [4] Lee, J.C., Forlizzi, J. and Hudson, S.E. 2002. The Kinetic  Typography Engine: An Extensible System for Animating  Expressive Text, In the proceedings of the UIST 2002, 81-90.   [5] Masson, M.E.J. 1982. Cognitive processes in skimming  stories. Journal of Experimental Psychology: Learning,  Memory and Cognition, 8(5), 400-417   [6] Maruya, K., Uetsuki, M., Ando, H., Watanabe, J. 2012.  Yu bi  yomu : interactive reading of dynamic text, In the  proceedings of Multimedia 2012, ACM, New York, NY, 1499- 1500   [7] Wong, Y.Y. 1996. Temporal Typography: A Proposal to  Enrich Written Expression, In the proceedings of CHI 96,  ACM, New York, NY,  408-409.   419      "}
{"index":{"_id":"77"}}
{"datatype":"inproceedings","key":"Barmaki:2015:CST:2723576.2723650","author":"Barmaki, Roghayeh and Hughes, Charles E.","title":"A Case Study to Track Teacher Gestures and Performance in a Virtual Learning Environment","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"420--421","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723650","doi":"10.1145/2723576.2723650","acmid":"2723650","publisher":"ACM","address":"New York, NY, USA","keywords":"gesture, grounding, professional development, teacher preparation, virtual learning environment","abstract":"As part of normal interpersonal communication, people send and receive messages with their body, especially with their hands. Gestures play an important role in teacher-student classroom interactions. In the domain of education, many research projects have focused on the study of such gestures either in real classrooms or in tutorial settings with experienced teachers. Novice teachers especially need to understand the messages they are sending through nonverbal communication as this can have a major effect on their ability to manage behaviors and deliver content. Such learning should optimally occur before experiencing the real classroom. To assist in this process, we have developed a virtual classroom environment- TeachLivE- and used it for teacher practice, reflection and assessment. This paper investigates the way teachers use gestures in the virtual classroom settings of TeachLivE. Biology and algebra teachers were evaluated in our study. Analysis of video recordings from real and virtual environment seems to indicate that algebra teachers gesture significantly more often than biology teachers. These results have implications for providing useful feedback to participant teachers. ","pdf":"   A Case Study to Track Teacher Gestures and Performance  in a Virtual Learning Environment   Roghayeh Barmaki, Charles E. Hughes  Department of Computer Science    University of Central Florida   {barmaki, ceh} @ eecs.ucf.edu     ABSTRACT  As part of normal interpersonal communication, people send and  receive messages with their body, especially with their hands.  Gestures play an important role in teacher-student classroom  interactions. In the domain of education, many research projects  have focused on the study of such gestures either in real classrooms  or in tutorial settings with experienced teachers. Novice teachers  especially need to understand the messages they are sending  through nonverbal communication as this can have a major effect  on their ability to manage behaviors and deliver content. Such  learning should optimally occur before experiencing the real  classroom. To assist in this process, we have developed a virtual  classroom environment- TeachLivE- and used it for teacher  practice, reflection and assessment. This paper investigates the way  teachers use gestures in the virtual classroom settings of  TeachLivE. Biology and algebra teachers were evaluated in our  study. Analysis of video recordings from real and virtual  environment seems to indicate that algebra teachers gesture  significantly more often than biology teachers. These results have  implications for providing useful feedback to participant teachers.     Keywords  teacher preparation, professional development, virtual learning  environment, gesture, grounding   1. INTRODUCTION  Gestures are a form of nonverbal communication in which visible  bodily actions are used to communicate important messages, either  in place of speech or together and in parallel with spoken words  [7]. There has been extensive research in human factors,  psychology and education on gesture within the last two decades  [7, 14], but there are still many unexplored domains. In this study,  we investigate the teachers use of gestures in an immersive  teaching learning environment-TeachLivE. Our target group  includes biology and algebra high school teachers from the U.S.  public school systems.   2. GESTURE and EDUCATION  Teachers gestures influence student comprehension and student  learning, especially in instructional discourses [10, 14]. Some  studies have shown that a speakers gestures facilitate listeners  comprehension of speech [2]. In a survey by Roth in 2001, the role  of gestures was studied in teaching and learning. This especially  addressed the role of gestures in knowing and learning scientific   and mathematical concepts in school-aged children [14].   Gesture movements are classified into four categories according to  McNeills [11] study: iconics (the form is related to the semantic  content of speech); metaphorics (linking to an abstract concept);  deictics (pointing movements); and beats (not presenting a  describable meaning). Macedonia et al. explored the impact of  iconic gestures in foreign language word learning [9]. Their  research indicated that iconic gestures in comparison to  meaningless gestures helped the memorization of foreign language  nouns in a significant fashion. In a similar work, Alibali et al. [1]  introduced gestures as a means for teachers to scaffold students  understanding. In their research, there was a hypothesis based on  the work of Lakoff [8], which states that teachers use gestures to  ground their instructional language, especially in abstract  concepts. The analysis on selected video sessions of a mathematics  lesson indicated that a teachers gesture was used most frequently  for new materials, for referents that were highly abstract, and in  response to students questions and comments [1]. Also, Pozzer- Ardenghi et al. in [13] explored the videos of science lectures  (subject: human body parts in biology high school classes). Their  research indicates multimodal resources and nonverbal aspects of  teaching may help students to be able to better articulate their  conceptions and understandings with peers.   3. STUDY  The research presented here focuses on the gestures of biology and  algebra teachers in an immersive learning environment-  TeachLivE. TeachLivE was developed at the University of Central  Florida to provide an interactive learning experience for teachers to  hone their management, communication and content delivery skills.  TeachLivE is based on digital puppetry software that controls five  student avatars in a virtual classroom setting [12]. A single human- in-the-loop (inter-actor) orchestrates the actions of the student  avatars. For practitioners, it is possible to request the level of  misbehavior and lesson plan of the classroom prior to their teaching  sessions in order to leverage their professional development [3].   In this study, we analyze the video records of participant  teachers to investigate how significantly gestures were used during  the teaching sessions. The research of Alibali et al. [1] about  algebra teaching class is used in our research as a reference for  different types of gestures (pointing and representing gestures) that  teachers may use in the classroom. One major difference between a  virtual classroom setting and a real classroom is that, within the  TeachLivE environment, there are no real students. This is an  opportunity for novice teachers to rehearse and improve their skills  without putting real children at risk [5]. For example, teachers can  easily practice their targeted lesson plan several times until they  master it. In contrast, based on specific features of this virtual  classroom, it is not possible to track and study the student learning  of virtual students in a manner that is similar to previous studies.  Most research involving a teachers gestures and students learning  have been done either in a real classroom or tutorial setting [1]. In a     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723650       420       real setting, students perceptions and their pre/post-tests in the  classroom have been used as indicators for their teachers skills,  but such an approach is not useful in our virtual setting. One  solution to resolve this is to use domain experts to review the  nonverbal behavior of teachers using the video records and scale  their harmony/grounding between speech and gestures. An  alternate method is to do the pre/post-tests with the students whose  teachers participate in the TeachLivE sessions. The first method  could be helpful for the novice teachers who do not teach in any  classes during the study, and the second way might be useful for  other practicing and experienced teachers.   In the biology study, 17 (female) biology teachers from Florida  public schools participated. They were asked to teach virtual kids  about the definition of technology and its applications in biology in  one ten-minute session per month for four consecutive months. A  reflection tool recorded the front view of the teacher while  interacting with the virtual classroom shown on a huge TV display.  In the next step, video records of teachers were analyzed by domain  experts to see how the subjects benefit from their hand movements  to explain new and abstract topics, or to respond to questions of  virtual students. The experts did annotations for specific moments  of the videos based on the given reference [1] over the 174  videos. The analysis showed that only a few teachers (2 out of 17)  used gestural representations frequently (at least 10 occurrences of  gestural employment per session). Even though the use of gestures  during the teaching is highly correlated with the concept and lesson  plan; however, producing gestures by people during their speech is  a spontaneous action, and may not be highly related to the context  and lesson plan. In the second phase of the study, we will analyze  the teaching sessions of 17 algebra teachers in TeachLivE. We  hypothesize that, since algebra links to abstract concepts, teachers  will use their gestures more fruitfully in these sessions. As a  preliminary step, we reviewed video recordings of five algebra  teachers in real classroom and noticed the impact of gesture  employment in their teaching sessions; however, one can expect  that there will be some differences with the gestures of teachers in  the simulated classroom versus the real one. It is expected that the  frequency of pointing and representing gestures will be higher in  algebra teachers than found in the similar study of biology teachers.   4. DISCUSSION  Nonverbal communication, including kinesics, facial expressions  and gestures provide huge amounts of information about the  speaker. Hand gestures help listeners to scaffold the speakers talks  specifically in instructional discourse. Teachers may benefit from  gestures in their lectures that introduce new concepts to students for  different courses, especially mathematics, sciences and linguistics.  Focusing on biology and algebra teachers, we investigated the  applications of gestures in a virtual classroom setting. Video  analysis of biology teachers did not show very significant use of  gestures during the teaching sessions except for two (out of 17  subjects); however we expect to see more meaningful gestures  from algebra teachers. It may not be reasonable to compare the  results directly, and claim that biology teachers use their hand  gestures less frequently than algebra teachers based on this pilot  study. Some factors such as lesson plan and context, the personality  of individual teachers, cultural influences and social behaviors  could be involved in these results [1, 4, 11]; however it reminds us  of the basic difference between math and the other sciences   applications of gestures by mathematics teachers significantly help  students scaffolding and understanding of new concepts. These  actions assist students to learn and remember new and abstract  topics that are linked to the gestures. Our research is going to   progress as follows: collecting data from participating algebra  teachers in the simulated classroom, reviewing them and then  reporting the final results. One promising, but very different  direction is to invite participants with special needs, especially with  autism to interact with the virtual students.   5. ACKNOWLEDGMENTS  We would like to thank Drs. Lisa Dieker and Michael Hynes,  TeachLivEs principal investigators, and Dr. Carrie Straub, the  educational research director of the project for the data collection.  We wish to acknowledge support for the TeachLivE project from  the Bill & Melinda Gates Foundation (OPP1053202) and the  National Science Foundation (CNS1051067, IIS111615). Any  opinions expressed in this material are those of the authors and do  not necessarily reflect the views of the sponsors.   6. REFERENCES  [1]   Alibali, M. W. and Nathan, M. J. Teachers gestures as a means of   scaffolding students understanding: Evidence from an early algeb-  ra lesson. Video research in the learning sciences, (2007), 349-365.   [2]    Alibali, M. W., Kita, S. and Young, A. J. Gesture and the  process of speech production: We think, therefore we gesture.  Language and cognitive processes, 15, 6 (2000), 593-613.   [3]   Barmaki, R. Nonverbal Communication and Teaching  Performance. Stamper, J., Pardos, Z., Mavrikis, M., McLaren,  B.M. (eds.) In Proceedings of the 7th International Conference on  Educational Data Mining (EDM). London, 2014, 441-443.   [4]    Beheshti, R., Ali, A. M. and Sukthankar, G. Cognitive Social  Learners: an Architecture for Modeling Normative Behavior. In  Proceedings of the 29th AAAI Conference on Artificial  Intelligence. (Jan). Austin, TX, 2015 (to appear).   [5]    Hayes, A. T., Straub, C. L., Dieker, L. A., Hughes, C. E. and  Hynes, M. C. Ludic Learning: Exploration of TLE TeachLivE  and Effective Teacher Training. International Journal of Gaming  and Computer-Mediated Simulations, 5, 2 (2013), 20-33.   [6]    Kelly, S. D., Manning, S. M. and Rodak, S. Gesture gives a hand  to language and learning: Perspectives from cognitive  neuroscience, developmental psychology and education.  Language and Linguistics Compass, 2, 4 (2008), 569-588.   [7]    Kendon, A. Gesture: Visible action as utterance. Cambridge  University Press, 2004.   [8]    Lakoff, G. and Nez, R. E. Where mathematics comes from:  How the embodied mind brings mathematics into being. Basic  books, 2000.   [9]    Macedonia, M., Mller, K. and Friederici, A. D. The impact of  iconic gestures on foreign language word learning and its neural  substrate. Human brain mapping, 32, 6 (2011), 982-998.   [10]  McNeil, N. M., Alibali, M. W. and Evans, J. L. The role of  gesture in children's comprehension of spoken language: Now  they need it, now they don't. Journal of Nonverbal Behavior. 24,  2 (2000), 131-150.   [11]  McNeill, D. Hand and mind: What gestures reveal about  thought. University of Chicago Press, 1992.   [12]  Nagendran, A., Pillat, R., Kavanaugh, A., Welch, G. and  Hughes, C. A unified framework for individualized avatar-based  interactions. Presence: Teleoperators and Virtual Environments,  23, 2 (2014), 109-132.   [13]  Pozzer-Ardenghi, L. and Roth, W. On performing concepts  during science lectures. Science Education, 91, 1 (2007), 96-114.   [14]  Roth, W. Gestures: Their Role in Teaching and Learning. Review  of Educational Research, 71, 3 (2001), 365-392.     421      "}
{"index":{"_id":"78"}}
{"datatype":"inproceedings","key":"Nwanganga:2015:QEE:2723576.2723651","author":"Nwanganga, Frederick and Aguiar, Everaldo and Ambrose, G. Alex and Goodrich, Victoria and Chawla, Nitesh V.","title":"Qualitatively Exploring Electronic Portfolios: A Text Mining Approach to Measuring Student Emotion As an Early Warning Indicator","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"422--423","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723651","doi":"10.1145/2723576.2723651","acmid":"2723651","publisher":"ACM","address":"New York, NY, USA","keywords":"affect, analytic approaches #38; methods, emotions, natural language processing, predictive analytics, quantified self, reflecting learning, text mining","abstract":"The collection and analysis of student-level data is quickly becoming the norm across school campuses. More and more institutions are starting to use this resource as a window into better understanding the needs of their student population. In previous work, we described the use of electronic portfolio data as a proxy to measuring student engagement, and showed how it can be predictive of student retention. This paper highlights our ongoing efforts to explore and measure the valence of positive and negative emotions in student reflections and how they can serve as an early warning indicator of student disengagement.","pdf":"Qualitatively Exploring Electronic Portfolios: A Text Mining Approach to Measuring Student Emotion as an Early  Warning Indicator  Frederick Nwanganga University of Notre Dame  Notre Dame, Indiana 46556 fnwangan@nd.edu  Everaldo Aguiar University of Notre Dame  Notre Dame, Indiana 46556 eaguiar@nd.edu  G. Alex Ambrose University of Notre Dame  Notre Dame, Indiana 46556 gambrose@nd.edu  Victoria Goodrich University of Notre Dame  Notre Dame, Indiana 46556 v.goodrich@nd.edu  Nitesh V. Chawla University of Notre Dame  Notre Dame, Indiana 46556 nchawla@nd.edu  ABSTRACT The collection and analysis of student-level data is quickly becoming the norm across school campuses. More and more institutions are starting to use this resource as a window into better understanding the needs of their student population. In previous work, we described the use of electronic portfo- lio data as a proxy to measuring student engagement, and showed how it can be predictive of student retention. This paper highlights our ongoing efforts to explore and measure the valence of positive and negative emotions in student re- flections and how they can serve as an early warning indica- tor of student disengagement.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.0 [Computer Uses in Education]: General  General Terms Measurement, Performance  Keywords Analytic Approaches & Methods, Natural Language Pro- cessing, Predictive Analytics, Text Mining, Emotions, Af- fect, Reflecting Learning, Quantified Self  1. INTRODUCTION & BACKGROUND In previous work [1, 2], we showed that the efficiency of re-  tention prediction systems based on academic performance  This material is based upon work supported by the Na- tional Science Foundation under Grant No. DUE 1161222  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 15, March 16 - 20, 2015, Poughkeepsie, NY, USA Copyright 2015 ACM 978-1-4503-3417-4/15/03 http://dx.doi.org/10.1145/2723576.2723651 ...$15.00.  alone can improve significantly when features that serve as a proxy to student engagement are incorporated. We used data describing student interactions with their electronic portfolios (ePortfolios) to quantify the amount of time and energy engineering students were allocating to a set of in- trospective assignments that invited them to reflect on their future careers as engineers. In [2], we paired that infor- mation with outcome data to show that the distributions of those ePortfolio feature values were statistically significantly different across students who were retained after a year, and those who did not persist. Further, in [1], we showed that these features were also quite predictive.  In this paper, we show how responses provided by stu- dents to reflective assignments given half way through, and towards the end of their first semester in college, can ex- ternalize both positive and negative sentiments, and how these sentiments could be used to predict end-of-semester outcomes.  By text mining student reflections about their interest in majoring in engineering from their ePortfolios, we seek to show that both the arousal and valence of student emotion can be an early warning indicator that a student is disengag- ing over time. In addition, we seek to continue to show the predictive potential of electronic portfolios as a rich data source for next generation learning analytics. Electronic Portfolios are a web space, story, and system that function as a workspace and showcase in which to collect, select, reflect, publish, link, archive, and demonstrate knowledge, skills, reflections, and more as multimedia evidence.  2. CONTEXT Our work in this paper is focused on the reflections of 419  first year engineering student to the following two questions asked in the middle and at the end of the semester, respec- tively: (1)Engineering is a very broad field of study. What is it about engineering that interests you (2) What does it mean to be an engineer How does engineering fit into your interests  Based on known outcomes of whether a student continued in or left the engineering program after the first semester, we classified 48 of the students as Leavers and 371 of them as Stayers. Of the students in the Leavers class, we selected 15% of them using a stratified sampling method (without  422    replacement) for our experiments. Because of the significant imbalance in the dataset, we also sampled an equal number of students from the Stayer class. These two subsets were combined and make up our sample set.  3. TEXT MINING According to [4], the words people use reveal a great deal  about them. They provide insight into the emotional, social and even physical state of a person. A persons deeper mo- tives and fears can sometimes be inferred by the words they use even if it is unknown or unacknowledged by the author. It is, therefore, within reason to suggest that when applied in a limited context, a study of the words used by an author (such as a student) while introspectively reflecting on a topic can be informative with regards to the degree to which the author is interested or disinterested (affective state) about the particular topic they are writing about. Building on the work previously done by [2, 1], we attempt to discover the degree to which first year engineering students (2012 cohort) are engaged or disengaged by analyzing the words they use.  3.1 Word Frequency Analysis After removing stop words (e.g., a, it, the, an, etc) and  excluding the words Engineer, Engineering and Engi- neers (which were disproportionately more frequent than any other words across the vast majority of the students writing), we generated a simple word frequency count of the mid-semester and end of semester reflections for each class of student (Stayers and Leavers). While the set of words used by each class varied, they were not informative in dis- criminating between members of each class.  3.2 Measuring Emotion Basic approaches to sentiment analysis use machine learn-  ing algorithms to simply identify the polarity of sentiment in text: positive, negative or neutral. They do not deal with the strength of the sentiment, account for the existence of both positive and negative emotions in the same text, or identify the discrete emotions that exist within text (fear, love, sadness) [6]. While this may be sufficient in some ap- plications, in others, it is necessary to not only identify the presence of both positive and negative sentiment, but it is also important to measure the strength of the sentiment ex- pressed. For example, programs that are designed to iden- tify at-risk users in online communications would need to be sensitive not only to the balance of sentiments expressed by a particular user, but also to the strength of the sentiments expressed [3].  Alternative approaches to sentiment analysis attempt to go beyond the single polarity classification method discussed earlier, to the identification of the existence of emotion (pos- itive and negative) in unstructured text. One such approach is to perform a word frequency analysis on text for the oc- currence of words from a dictionary of positive or negative words (such as love, hate or like). The Linguistic Inquiry and Word Count (LIWC) tool [5], makes use of this approach. It uses simple word counting and an extensive and pre-defined list of emotion-bearing words to detect positive or negative emotion in text. The list of emotion bearing words used by LIWC have been compiled and validated by panels of human judges and have undergone statistical testing. Rather than determining the overall emotion or emotional strength of a body of text, LIWC calculates the prevalence of emotion in  the text. Applying the LIWC tool to both the mid-semester and  end of semester student reflections shows a skew in positive emotions among Stayers and a skew in negative emotions among Leavers. While these results are only those of a sample set, we believe that our process when applied to the entire data set would produce very similar results. Going froward, we intend to do just that, as well as explore other methods of sentiment analysis against our data set in order to better predict student outcomes.  4. CONCLUSION & FUTURE WORK Our preliminary results show that simply using word fre-  quency counts as a predictor of outcome is ineffective or in- sufficient at best. While there seemed to be a slight variance in the distribution of words used by Leavers and Stayers, the inferred information value of word frequency alone ap- pears to be low. However, the measurement of the arousal and valence of student emotions as a predictor of outcome, as determined by the LIWC tool, shows promise.  Current and future research includes: (1) Applying the methods utilized in this research to a larger data set. (2) Deploying an early intervention plan based on student dis- engagement alerts and predictive metrics provided by the quantitative and qualitative data gathered from student ePort- folios. (3) Evaluating the predictive value of other text mining methodologies (i.e. parts of speech analysis, con- cordances, named entity extraction, summarization, classi- fication and clustering).  5. REFERENCES [1] E. Aguiar, N. V. Chawla, J. Brockman, G. A. Ambrose,  and V. Goodrich. Engagement vs performance: using electronic portfolios to predict first semester engineering student retention. In Proceedings of the Fourth International Conference on Learning Analytics And Knowledge, pages 103112. ACM, 2014.  [2] V. Goodrich, E. Aguiar, G. A. Ambrose, L. McWilliams, J. Brockman, and N. V. Chawla. Integration of eportfolios into rst-year experience engineering course for measuring student engagement. In Proceedings of the American Society for Engineering Education Conference,, 2014.  [3] Y. Huang, T. Goh, and C. L. Liew. Hunting suicide notes in web 2.0 - preliminary findings. pages 517521, Los Alamitos, 2007. IEEE.  [4] J. Pennebaker, M. Mehl, and K. Niederhoffer. Psychological aspects of natural language use: Our words, our selves. Annual Review of Psychology, 54:547577, 2003.  [5] J. W. Pennebaker, C. K. Chung, M. Ireland, A. Gonzales, and R. J. Booth. The Development and Psychometric Properties of LIWC2007. Austin, Texas, 2007.  [6] M. Thelwall, K. Buckley, G. Paltoglou, D. Cai, and A. Kappas. Sentiment strength detection in short informal text. Journal of the American Society for Information Science and Technology, 61(12):25442558, 2010.  423      "}
{"index":{"_id":"79"}}
{"datatype":"inproceedings","key":"Absar:2015:MMC:2723576.2723654","author":"Absar, Rafa and Gruzd, Anatoliy and Haythornthwaite, Caroline and Paulin, Drew","title":"Media Multiplexity in Connectivist MOOCs","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"424--425","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723654","doi":"10.1145/2723576.2723654","acmid":"2723654","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, connectivism, learning, social media, social networks","abstract":"In this poster, we present work on exploring use of multiple social media platforms for learning in two connectivist MOOCs (or cMOOCs) to develop and evaluate methods for learning analytics to detect and study collaborative learning processes.","pdf":"Media multiplexity in connectivist MOOCs  Rafa Absar   Department of Computer Science, Clarkson University  P.O. Box 5815, Potsdam, NY 13699-5815   1-315-261-4837  rafa.absar@mail.mcgill.ca   Anatoliy Gruzd  Ted Rogers School of Management, Ryerson University    350 Victoria Street, Toronto, ON, Canada, M5B 2K3  1-416-979-5000 x7937   gruzd@ryerson.ca   Caroline Haythornthwaite   The iSchool at The University of British Columbia  Suite 470-1961 East Mall, Vancouver, BC, Canada, V6T 1Z1    1-604-827-4790  c.haythorn@ubc.ca        Drew Paulin  Sauder School of Business, University of British Columbia  453C-2053 Main Mall, Vancouver, BC, Canada V6T 1Z2    1-604-822-5344   drew.paulin@sauder.ubc.ca        ABSTRACT  In this poster, we present work on exploring use of multiple social  media platforms for learning in two connectivist MOOCs (or  cMOOCs) to develop and evaluate methods for learning analytics  to detect and study collaborative learning processes.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative learning,  J.1 [Administrative Data Processing] Education;   General Terms  Measurement, Human Factors.  Keywords  Social Media, Learning, Connectivism, Social Networks,  MOOCs.   1. INTRODUCTION  Contemporary learning can leverage multiple media platforms to  create and sustain social relationships and communities in which  learners participate, discover, share, and co-construct of  knowledge. An open question is what work these media do in  support of learning, and how each medium contributes to whole of  learning Our goal is to develop methods to detect and study  learning processes across multiple social media platforms. This  poster presents preliminary work regarding a connectivist MOOC  that engages learners across multiple platforms. While connecting  across platforms provides opportunity for understanding learning  across media and communities, it presents both users and  researchers with challenges associated with using, connecting,  collecting and processing of data from multiple platforms.    2. SOCIAL MEDIA AND LEARNING  Learning in new networked, mediated spaces is socially  embedded, tied to the interests of the learner in multiple,  overlapping social spheres, and supported by a culture of freely  created and shared content. Such practices are gaining attention in  higher education [1,2,3], and align with a connectivist approach to  learning where learners negotiate and construct meaning and   knowledge across a network of learners, platforms, and sources  [4] in support of personal, work and educational outcomes.  Challenges exist for learners due to the need to bridge learning  across multiple platforms. Responses call for systems support for  data aggregation [5] and the facilitation of personal learning  environments [6,7]. Researchers face challenges of linking  identities across multiple social media [8], with potential methods  including focusing on unique behavioural patterns exhibited  across platforms, informational redundancies offered by the user  across platforms, and machine learning towards effective user  identification [9,10].   3. MULTI-PLATFORM cMOOCS  This poster presents an examination of multi-platform data from  two connectivist courses: CCK11 (Connectivism and Connective  Knowledge 2011; 12 week course; Jan. to April 2011) and  Change11 (35 weeks; Sept. 2011 to May 2012). Discussions and  learning processes in these courses were supported through the  following tasks: Aggregate across a wide variety of resources to  read, watch or play with; Remix, keeping track of in-class  activities using blogs or other types of online posts; Repurpose, go  beyond repeating to creating content; Feed Forward, by sharing  work with others in or outside the course to spread knowledge.    Course resources were provided to learners using gRSShopper  and online seminars delivered using Elluminate. The courses were  not limited to a single platform and content was distributed across  the web; participants were encouraged to create blogs using any  blogging service, discuss on Google groups, tweet, or use other  means (e.g., Flickr, Second Life, YouTube). Participants were  asked to use the #cck11 or #change11 tags in whatever content  they created and shared. Content was aggregated daily for an  online newsletter.   To examine participation, data were scraped form the archives of  the daily newsletters, including tweets, discussion threads, blog  posts, and comments on blogs (Table 1)    Table 1: Number of posts on each platform  Platform CCK11 Change11  Twitter posts (tweets) 1722 5665  Blog posts 812 2486  Blog comments 306 134  Discussion thread posts 68 87   4. DATA CHALLENGES  Several research and technical challenges come up when  collecting and processing data from cMOOCs, primarily because  they do not use a single, centralized platform to support class   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    LAK15, March 16-20, 2015, Poughkeepsie, NY, USA.  ACM 978-1-4503-3417-4/15/03.   http://dx.doi.org/10.1145/2723576.2723654   424    interactions. These include a number of data collection issues: we  developed a custom script to scrape data from a publicly available  archive of daily newsletters that included snippets of participants  blog posts (with the link to the original posts), comments left on  each blog post, threaded discussions and Twitter messages;  however this excluded non-archived data, and messages that  failed to use the class hashtags. Only snippets of blog posts were  available in the daily newsletters, so we attempted to follow  links to the original blog posts to retrieve them as well,  encountering a number of technical issues (blogs on different  servers; expired URLs; domains that had disappeared; password- protection, etc.). Issues also emerged re the encoding of  webpages, particularly for self-hosted blogs on unique domain  names.   Other issues arose from the course material itself, since this can be  presented and stored in a variety of formats. That participants  were located around the world added another challenge to the pre- processing stage (e.g., posts not in English). Data collection also  requires a significant amount of cleaning up after extraction is  complete. Some newsletter elements were redundant and had to be  detected and removed. Discussion threads as stored in the course  archive page only included replies to the original post, but  unfortunately did not include the original post itself. This was  further complicated by the fact that although all blog postings  were RSS'd to the course page (by request of the course  instructors), they were not archived, so we have to try to scrape  the original blog pages themselves (if available). Finally, we  noticed that some newsletter pages were missing from the archive.    5. Identity resolution  To be able to analyze cross-platform data, we needed an effective  way to distinguish between the same participants across different  platforms despite different usernames for each platform. This kind  of identity resolution has been addressed using computational  linguistics and machine learning techniques [11] and can be  separated into two tasks:  coreference resolution (resolving single  identities across multiple platforms) and alias resolution  (identifying two or more people with the same name or alias  across platforms). cMOOC participants could (and some did)  remain entirely anonymous when participating in the class. As a  result, we often did not have enough information to match users  automatically. To achieve the highest possible level of data, we  took a manual brute-force approach of matching usernames across  platforms in which every username from each platform was  matched and cross-referenced with usernames from other  platforms and any matching ones grouped together. Through this  process, we identified the unique users who posted in each  platform (Table 2). We were also able to identify the users that  posted in more than one platform (Table 3).  Although the number  of users who posted across three or more platforms is small, a  reasonable number of participants posted in at least two platforms.  Although this is a promising approach, it is time-consuming and  may miss identifying single identities or erroneously group two  separate identities as one.  Faster, more reliable and real-time  identity resolution methods are required and are the subject of  future work.  Table 2: Number of unique users who posted on each platform  Platform CCK11 Change11  Twitter 145 794  Blog 105 278  Blog comments 56 27  Discussion thread posts 18 17   Table 3: Number of users who posted in more than 1 platform  No. of users posting in CCK11 Change11  4 platforms 2 3  3 platforms 10 5  2 platforms 32 103  6. FUTURE DIRECTIONS  This work focused on determining technical and logistical  challenges associated with the collection of social media data, and  identity resolution across multiple platforms. We found only a  small percentage of users posted to multiple platforms during the  class; instead, among the four different ways of interacting with  others in the class, Twitter was the single, most popular platform.  And even though blogs (specifically blog posts) were the second  largest content generators after Twitter; our manual review of the  blog posts revealed that they were primarily used to take notes  and write reflection-type pieces, and they were not used to interact  with one another. We started this project with the expectation that  we would need to identify and resolve online identities across  multiple platforms; but in reality, both classes primarily relied  only on Twitter for user-to-user interaction. We conclude that the  need for identity resolution may be very low. We expect that a  similar pattern may occur in other cMOOC-type courses, i.e., that  only a few of the most active users rely on two or more media  platforms. Our future work will examine this further; if it holds,  the main implication is that there might not be a need for  resource-intensive identity resolution algorithms when studying  social media multiplexity and that we might be able to analyze  each platform independently from other platforms, at least from  the perspective of identifying overlapping communities of users  across platforms.    7. REFERENCES  [1] Dahlstrom, E., Walker, J.D., & Dziuban, C. 2013. ECAR Study of   Undergraduate Students and Information Technology, 2013.  Retrieved from: http://www.educause.edu/library/resources/ecar- study-undergraduate-students-and-information-technology-2013    [2] Smith, S.D. and Caruso, J.B. 2010. 2010 The ECAR Study of  Undergraduate Students.   [3] Moran, M., Seaman, J., & Tinti-Kane, H. 2012. How Today's Higher  Education Faculty Use Social Media. Pearson/Babson Survey  Research Group.     [4] Siemens, G. 2005. Connectivism: A learning theory for the digital  age. International Journal of Instructional Technology and Distance  Learning. 2, 1 (2005), 310.   [5] Vu, X.T. et al. 2014. Knowledge and Systems Engineering. 245,  (2014), 109119.   [6] Bogdanov, E., Limpens, F., Li, N., et al. 2012. A social media  platform in higher education. Proceedings of the 2012 IEEE Global  Engineering Ed. Conf. , 18.   [7] Gillet, D. 2013. Personal learning environments as enablers for  connectivist MOOCs. 2013 12th International Conference on  Information Technology Based Higher Education and Training  (ITHET). (Oct. 2013), 15.   [8] Gruzd, A. et al. 2014. Learning analytics for the social media age.  LAK 14. ACM (2014), 254256.   [9] Zafarani, R. and Liu, H. 2009. Connecting Corresponding Identities  across Communities. ICWSM (2009), 354357.   [10] Zafarani, R. and H. Liu. 2013. Connecting users across social media  sites: A behavioral-modeling approach. In Proceedings of the 19th  ACM SIGKDD International Conference on Knowledge Discovery  and Data Mining, KDD 13, 41-49.   [11]Gruzd, A., & Haythornthwaite, C. 2011. Networking Online: Cyber  communities. In Scott, J., & Carrington, P. (Eds.), Handbook of  Social Network Analysis. London: Sage, 449-487.     425      "}
{"index":{"_id":"80"}}
{"datatype":"inproceedings","key":"Worsley:2015:ULA:2723576.2723659","author":"Worsley, Marcelo and Blikstein, Paulo","title":"Using Learning Analytics to Study Cognitive Disequilibrium in a Complex Learning Environment","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"426--427","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723659","doi":"10.1145/2723576.2723659","acmid":"2723659","publisher":"ACM","address":"New York, NY, USA","keywords":"affect, cognition, learning sciences","abstract":"Cognitive disequilibrium has received significant attention for its role in fostering student learning in intelligent tutoring systems and in complex learning environments. In this paper, we both add to and extend this discussion by analyzing the emergence of four affective states associated with disequilibrium: joy, surprise, neutrality and confusion; in a collaborative hands-on, engineering design task. Specifically, we conduct a comparison between two learning strategies to make salient how the strategies are associated with different affective states. This comparison is grounded in the construction of a probabilistic model of student affective state as defined by the frequency of each state, and the rate of transition between affective states. Through this comparison we confirm prior research that highlights the importance of confusion as a marker of knowledge construction, but put to question the notion that surprise is a significant mediator of cognitive disequilibrium. Overall, we show how modeling learner affect is useful for understanding and improving learning in complex, hands-on learning environments.","pdf":"Using Learning Analytics to Study Cognitive  Disequilibrium in a Complex Learning Environment      Marcelo Worsley  Stanford University   520 Galvez Mall, CERAS 217  Stanford, CA 94305   mworsley@stanford.edu   Paulo Blikstein  Stanford University   520 Galvez Mall, CERAS 232  Stanford, CA 94305   paulob@stanford.edu     ABSTRACT  Cognitive disequilibrium has received significant attention for its  role in fostering student learning in intelligent tutoring systems and  in complex learning environments. In this paper, we both add to  and extend this discussion by analyzing the emergence of four  affective states associated with disequilibrium: joy, surprise,  neutrality and confusion; in a collaborative hands-on, engineering  design task. Specifically, we conduct a comparison between two  learning strategies to make salient how the strategies are associated  with different affective states. This comparison is grounded in the  construction of a probabilistic model of student affective state as  defined by the frequency of each state, and the rate of transition  between affective states. Through this comparison we confirm prior  research that highlights the importance of confusion as a marker of  knowledge construction, but put to question the notion that surprise  is a significant mediator of cognitive disequilibrium. Overall, we  show how modeling learner affect is useful for understanding and  improving learning in complex, hands-on learning environments.   Keywords  Learning Sciences, Affect, Cognition   1. INTRODUCTION  In recent years the analysis of student affect in education contexts  has gained increased attention (e.g. see [1-5]). In particular,  researchers have found ways to both extract (from video, speech,  human observation and bio-sensing) and embody (within virtual  agents) various affective states. As one can imagine, many of these  projects have jointly been in the service of studying and  understanding student learning while also developing a means to  influence student affect. However, many of these projects have  been situated in the context of individual, computer-mediated  learning environments. The goal of this paper is to extend the  analysis of student affect outside of the traditional context of  computer-mediated interactions. Instead, this paper will involve  analyzing student affect during the course of a hands-on  engineering design task where two students work together over the  course of an hour. Additionally, our analysis of affect will be  discussed in the context of comparing two different learning  strategies from our prior work [6]. In [6] we defined and situated  two engineering design strategies commonly used by students:   example-based reasoning and principle-based reasoning (see [6] for  a brief description of each reasoning strategy.)  With regard to the two experimental conditions we hypothesize that  students in the principle-based reasoning condition would spend  more time expressing disequilibrium than their peers in the  example-based condition. Similarly, we hypothesize that students  in the principle-based condition transition from equilibrium   disequilibrium with greater frequency than their peers in the  example-based reasoning condition.  In approaching this paper we were centrally interested in studying  disequilibrium as proposed by D'Mello and Graesser [5]. In [5] the  authors propose that there are four primary affects that participate  in equilibrium and disequilibrium. Their model suggests that  surprise and joy are two of the emotions that either create or correct  impasses. However, [5] did not have sufficient instances of surprise  or delight to validate the existence of those pathways. Thus, one  contribution of this work is to more closely examine those  pathways. In [5] the authors also describe two direct paths between  equilibrium and disequilibrium. Their hypothesized affect  dynamics graph is reproduced in Figure 1.     Figure 1. Hypothesized Model of Affect Dynamics by D'Mello  and Graesser   2. METHODS  Students worked in pairs to solve an engineering design task. They  used common household materials: one paper plate, 4 ft. of garden  wire, four drinking straws and five wooden Popsicle sticks. The  objective was to use the materials provided to create a structure that  could support a weight of approximately half a pound. Participants   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for third- party components of this work must be honored. For all other uses, contact  the Owner/Author.     Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723659     426    were also asked to support the weight as high off the table as  possible.   Pairs of students (N=20) were randomly assigned to either use  example- or principle-based reasoning, after controlling for prior  education experience. Both conditions were initially shown a  picture of a bridge, a ladder and an igloo. In the example-based  condition after seeing the three pictures, students generated three  example structures from their home, community or school. These  student-generated example structures were to be used as inspiration  for their eventual design. In the principle-based condition students  identified three engineering principles that conferred strength and  stability to a ladder, an igloo and a bridge before embarking on the  building task.  Frontal images were generated from a Kinect sensor and used to  create a video of each students head and face. These videos were  then processed using FACET (previously known as CERT).  FACET uses facial action units to predict the presence or absence  of seven different affective states. Since we are only interested in  the states directly related to cognitive disequilibrium, we only  included surprise, joy, neutral and confusion in our analyses.    2.1 Analyses  We conducted two analyses of the student facial expressions. In the  first analyses we looked at average evidence of each facial  expression during the experiment. This analysis allows us to test  the hypothesis that students in principle-based reasoning condition  demonstrated higher values for joy, confusion and surprise than  their peers in the example-based reasoning condition.  For the second analysis we compute transition probabilities based  on the most dominant expression at a given time step. We only  focus on time steps in which the student experienced a change in  dominant facial expression. By looking at the transitions we can  explore the hypotheses that students in the principle-based  reasoning condition had a greater number of transitions between  equilibrium and disequilibrium.    3. Results  An analysis of ordinary variance (ANOVA) was run to determine  the variance between experimental groups. Based on the results  from that analysis confusion significantly differs by condition, with  the principle-based condition expressing higher confusion evidence  (p(18) < 0.05) than the example-based condition. However, there  was not a difference in the evidence for joy, surprise or neutral.   Next we looked at the number of transitions for: Neutral   Confusion, Confusion Neutral, SurpriseConfusion and  JoyNeutral. Recall that these are the expressions that the  literature describes as being relevant. We again perform an  ANOVA, and find that the two experimental conditions  significantly differed in their transition behavior from Neutral  Confusion (p (18) <0.05), as well as from NeutralSurprise  (p(18)<0.05). As before, students in the principle-based condition  were more likely to transition from NeutralConfusion than  students in the example-based condition.    4. DISCUSSION  The reader will recall that a primary motivation for this research  was to study the model of affective dynamics in [8] as applied to a  hands-on learning environment and to also investigate the different  affective behaviors with each experimental conditional. Based on   the preliminary findings from this study we find mixed  confirmation and disconfirmation. Specifically, expressions of  confusion, and transitioning from NeutralConfusion appear to be  positively correlated with high post-test scores. This conclusion  aligns with [8] and is based on the fact that the principle-based  reasoning strategy was more likely to express confusion and more  likely to transition from NeutralConfusion. These transitions to  confusion are likely to be indicative of greater cognitive  disequilibrium. However, we do not see strong results that indicate  that surprise mediates transitions from NeutralConfusion. Put  differently, it does not appear that surprise is a primary factor in  pushing students towards cognitive disequilibrium. Furthermore,  based on the analysis of transitions, going from NeutralSurprise  was more likely within the example-based reasoning condition, and  was associated with lower post-test scores. Hence, it may be the  case the surprise may be a way to induce impasse, but that it can  also simply be an expression that is associated with a lack of  conceptual understanding of the content.   5. CONCLUSION  In closing, then, by modeling and analyzing student affective state  in a hands-on learning context, and in a comparison across two  experimental conditions, we have 1) confirmed that affect is a  relevant part of studies of human cognition; 2) contributed to the  discussion of cognitive disequilibrium; 3) taken the work of  analyzing student affect to the domain of collaborating engineering  design; 4) examined how two different learning strategies are  evidenced in student affect; and 5) motivated the incorporation of  affect analysis in complex learning environments, potentially as a  tool for teachers and practitioners to leverage in providing  appropriate feedback and assistance to students.     6. REFERENCES  [1] Picard,R. 1997. Affective Computing. MIT Press,   Cambridge, 1997  [2] Grafsgaard, J. F., Wiggins, J. B., Boyer, K. E., Wiebe, E. N.,   & Lester, J. C. 2013. Automatically Recognizing Facial  Expression: Predicting Engagement and Frustration. In  Proceedings of the 6th International Conference on  Educational Data Mining.   [3] Rodrigo, M.M.T., Baker, R.S.J.d. 2011. Comparing Learners'  Affect While Using an Intelligent Tutor and an Educational  Game. Research and Practice in Technology Enhanced  Learning, 6 (1), 43-66   [4] DMello, S., Lehman, B., & Person, N. 2010. Monitoring  affect states during effortful problem solving activities.   Journal of Artificial Intelligence in Education   [5] DMello, S., & Graesser, A. 2012. Dynamics of affective  states during complex learning. Learning and Instruction,  22(2), 145157.   [6] Worsley, M., & Blikstein, P. (2014). Assessing the Makers:  The Impact of Principle-Based Reasoning on Hands-on,  Project-Based Learning. Proceedings of the 2014  International Conference of the Learning Sciences (ICLS), 3,  11471151.         427      "}
{"index":{"_id":"81"}}
{"datatype":"inproceedings","key":"Zushi:2015:ALS:2723576.2723645","author":"Zushi, Mitsumasa and Miyazaki, Yoshinori and Norizuki, Ken","title":"Analysis of Learners' Study Logs: Mouse Trajectories to Identify the Occurrence of Hesitation in Solving Word-reordering Problems","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"428--429","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723645","doi":"10.1145/2723576.2723645","acmid":"2723645","publisher":"ACM","address":"New York, NY, USA","keywords":"e-learning, information retrieving tool, mouse trajectory, occurrence of hesitation, study logs","abstract":"In this paper, we describe a Web application we have been developing in order to help both teachers and learners notice the crucial aspects of solving word-reordering problems (WRPs). Also, we discuss ways to analyze the recorded mouse trajectories, response time, and drag and drop (DD) logs, because these records are potential indicators of the degree of learners' understanding.","pdf":"Analysis of Learners Study Logs: Mouse Trajectories to Identify       the Occurrence of Hesitation in Solving Word-Reordering   Problems  Mitsumasa Zushi      Graduate School of Informatics,  Shizuoka University   Johoku 3-5-1, Naka-ku Hamamatsu City,  432-8011, Japan   +81 (53) 478-1539  mzushi@inf.shizuoka.ac.jp   Yoshinori Miyazaki      Graduate School of Informatics,   Shizuoka University      Johoku 3-5-1, Naka-ku Hamamatsu   City, 432-8011, Japan  +81 (53) 478-1476       yoshi@inf.shizuoka.ac.jp   Ken Norizuki  School of Information Studies,   Shizuoka Sangyo University   4-1-1, Surugadai Fujieda City,   426-8668, Japan   +81 (54) 646-5460  norizuki@ssu.ac.jp        ABSTRACT  In this paper, we describe a Web application we have been  developing in order to help both teachers and learners notice the  crucial aspects of solving word-reordering problems (WRPs).  Also, we discuss ways to analyze the recorded mouse trajectories,  response time, and drag and drop (D&D) logs, because these  records are potential indicators of the degree of learners  understanding.      Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   Computer-assisted instruction (CAI).   General Terms  Languages   Keywords  E-Learning, Mouse Trajectory, Study Logs, Information  Retrieving Tool, Occurrence of Hesitation    1. INTRODUCTION  A major goal of this study is to provide learners and teachers with  a practical Web application which will sensitize them to the  crucial aspects of solving WRPs. In WRPs learners are required to  make an English sentence from given words. This type of problem  is a popular means in Japan to measure learners knowledge of  grammatical items, sentence structure, idioms, and usagesthe  command of which is essential for producing correct sentences.  Since Japanese is a synthetic language, it has quite a loose word  order, while in an analytic language like English, word ordering  has an extreme importance to decide the meaning and  grammaticality of the sentence. This is why novice or poor  Japanese learners of English have difficulty in making a correct  English sentence, and WRPs are effective in confirming the   ability needed to produce English sentences. In the process to  achieve our goal, we have been developing software that records  mouse movements when learners solve problems, so that teachers  can analyze the mouse trajectories and other study logs in order to  ascertain the understanding levels of learners. There have been  several analyses of mouse movements such as Arroyo, E. et al.  (2006) and Freeman, J. B. and Ambady, N. (2010), but their goals  were not to ascertain learners understanding levels like ours.   In this paper, we will describe our Web application briefly and  discuss the results from a recent experiment conducted with  learners at a certain Japanese university. Exploration of mouse  movements, we hypothesize, will lead to a better evaluation of  learners performance than mere scores by binary computer  marking system.   2. WEB APPLICATION   Our web application has three modules with independent, but also  interrelated functions, with each module incorporated with special  functions to facilitate both learners and teachers using of this  software. This software has been programmed using PHP,  JavaScript, and MySQL.   1) Study Module requires learners to perform word-reordering  tasks by dragging and dropping each word into the  appropriate position in a sentence, and then to rate their  confidence levels. The module records all mouse trajectories  as well as the timing of D&Ds in answering the problems.    2) Problem Constructing Module, where all teachers have to do  is just type correct sentences in English and the  corresponding meaning in Japanese, has Chunking and Word  Fixing functions. They help teachers adjust the problems  length or avoid double answers to one problem.     3) Retrieval & Analysis Module reproduces all the actions  recorded in the learners mouse trajectories, and analyzes the  study logs both from the learners and problems  perspectives, such as the percentage of correct answers  classified by grammatical items, the total number of  problems attempted, the percentage of overall correct  answers, the average time needed for answering problems,  and the number of times a particular problem attempted by  all learners.   3. EXPERIMENT AND DISCUSSION  An important consideration is how to differentiate a confident  correct answer from a lucky hunch. Both answers are correct, but  one is reached by a learner who has full control of grammatical   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.    Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723645      428    structures and lexical items to construct the right sentence, while  the other answer, though correct, may be produced by a learner  who possibly lacks that particular knowledge. The latter learner  may have managed to reach the target structure by coincidence  after a long period of fluctuating mouse movements. The fact of  being correct or wrong alone may not always be a true reflection  of the ability tested unless the process of the learners responses  are carefully considered.   The answers reached by learners fell into one of four categories:  1) confident and correct, 2) confident but incorrect, 3) not  confident but correct, and 4) not confident and incorrect.  Confident and correct answers include the most useful logs for  finding characteristic mouse trajectories of answers without  hesitation or uncertainty. The answers from category 3 need very  careful treatment because most computer marking systems may  evaluate them as passing in spite of the fact that learners are not  yet sure of the correct answer, or they might have reached the  correct answers accidentally. There is no need of analyzing the  answers of category 4 for the purpose of ascertaining learners  understanding levels because they clearly do not understand,  which means that they have to review from the beginning.   Through the comparison of confident and not confident answers  in our former experiments, it was found that important parameters  for mouse movements included time needed for answering, D-C  time (duration between a drop and the next click), the number of  D&Ds, and number of U-turns (the right-and-left or up-and-down  mouse movements). As is discussed in Zushi, M. et al. (2012) and  (2014), these parameters could serve as an index of hesitation  resulting from learners lack of confidence.    In order to analyze the parameters in more detail, we conducted  another experiment, asking 20 learners to solve 30 problems on  our system. We collected 581 sets of data, with 19 sets of data  missing owing to recording failure or clicking mistakes on the part  of learners. The 158 answers of category 1 and 26 of category 3  are totally different in mouse trajectories, and we found that time  for answering and D-C time are of great use as indicators which  separate these two types of answers.   Longer time needed for answering is, in the general assumption,  connected to difficulty for learners to solve the problem, and thus  to high rate of incorrect answers. But answer time does not clearly  separate the two groups of correct and incorrect answers.    N: # of data.   M: mean value.   SD: standard deviation.     Fig. 1. Distribution of correct and incorrect answers in terms of answer   time.   Answer time, as is illustrated in the box-whisker diagram (Fig. 1),  is not a perfect classifier of correct and incorrect answers. The  middle range of distribution is a domain for both types of answers.   Compare this with the diagram below (Fig. 2). Both types of     Fig. 2. Distribution of confident and not confident answers in terms of   answer time.   answers are correct, but they are far apart from each other in terms  of confidence levels, and the difference between confidence levels   is illustrated in the time difference needed for answering. Thus,  time for answering and learners confidence are deeply connected,  showing this times availability as clues to uncertainty or  hesitation in the process of solving problems. If an e-Learning  system detects problems whose answers are correct but their  trajectories belong to category 3, the system should not pass the  learners but demand them to review more of the same type of  problems. Another parameter for mouse trajectories that can be an  indicator for confidence is the longest D-C time as shown below  (Fig. 3).     Fig. 3. Distribution of confident and not confident answers in terms of the   longest D-C time.   The more uncertain the learner is of the correct sentence, the  longer the D-C time will be somewhere in performing a task, and  this time can be interpreted as hesitation of mouse movements.   These parameters, combined with excessive D&Ds (i.e., D&Ds  employed more than necessary) will be useful clues not only to  detect difficult problems for a particular learner but also to locate  which part of a problem is difficult for the learner. The process of  answers being produced is a more important clue than the answers  themselves, which might include lucky hunches, when we try to  measure learners understanding levels. Uncertainty and hesitation  in solving problems resulting from lack of knowledge or  inadequate understanding are revealed more clearly in the  processes for answering than in answers themselves.   ACKNOWLEDGMENTS  This work was supported by a Grant-in-Aid (KAKENHI) for  Scientific Research (C) (25370621), and we also owe special  thanks to Takashi Miyamoto (a student from Faculty of  Informatics, Shizuoka University) for his help with the data  processing indispensable in our study.   REFERENCES  [1] Arroyo, E. et al. 2006. Usability Tool for Analysis of Web   Designs Using Mouse Tracks. In Proceedings of CHI EA06,  ACM, 484-489.   [2] Freeman, J. B. and Ambady, N. 2010. MouseTracker:  Software for Studying Real-Time Mental Processing Using a  Computer Mouse-Tracking Method. Behavior Research  Methods, Vol. 42, No. 1, 226-241.   [3] Zushi, M. et al. 2012. Web Application for Recording  Learners Mouse Trajectories and Retrieving Their Study  Logs for Data Analysis. APTEL 2011 Best papers:  Knowledge Management & E-Learning: An International  Journal, Vol. 4, No. 1, 37-50.   [4] Zushi, M. et al. 2014. Development of a Web Application:  Recording Learners' Mouse Trajectories and Retrieving Their  Study Logs to Identify the Occurrence of Hesitation in  Solving Word-Reordering Problems.  Journal on Systemics,  Cybernetics and Informatics: JSCI, Vol. 12, No. 2, 86-91.    429      "}
{"index":{"_id":"82"}}
{"datatype":"inproceedings","key":"Corrin:2015:SIF:2723576.2723662","author":"Corrin, Linda and de Barba, Paula","title":"How Do Students Interpret Feedback Delivered via Dashboards","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"430--431","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723662","doi":"10.1145/2723576.2723662","acmid":"2723662","publisher":"ACM","address":"New York, NY, USA","keywords":"dashboards, feedback, learning analytics, self-regulated learning","abstract":"Providing feedback directly to students on their engagement and performance in educational activities is important to supporting students' learning. However, questions have been raised whether such data representations are adequate to inform reflection, planning and monitoring of students' learning strategies. In this poster we present an investigation of how students interpret feedback delivered via learning analytics dashboards. The findings indicated that most students were able to articulate an interpretation of the feedback presented through the dashboard to identify gaps between their expected and actual performance to inform changes to their study strategies. However, there was also evidence of uncertain interpretation both in terms of the format of the visualization of the feedback and their inability to understand the connection between the feedback and their current strategies. The findings have been used to inform recommendations for ways to enhance the effectiveness of the delivery of feedback through dashboards to provide value to students in developing effective learning strategies to meet their educational goals.","pdf":"How do students interpret feedback delivered via  dashboards      Linda Corrin   Melbourne Centre for the Study of Higher Education   The University of Melbourne  Melbourne, Victoria, Australia   lcorrin@unimelb.edu.au   Paula de Barba  Melbourne Centre for the Study of Higher Education   The University of Melbourne  Melbourne, Victoria, Australia   paula.de@unimelb.edu.au     ABSTRACT  Providing feedback directly to students on their engagement and  performance in educational activities is important to supporting  students learning. However, questions have been raised whether  such data representations are adequate to inform reflection,  planning and monitoring of students learning strategies. In this  poster we present an investigation of how students interpret  feedback delivered via learning analytics dashboards. The findings  indicated that most students were able to articulate an  interpretation of the feedback presented through the dashboard to  identify gaps between their expected and actual performance to  inform changes to their study strategies. However, there was also  evidence of uncertain interpretation both in terms of the format of  the visualization of the feedback and their inability to understand  the connection between the feedback and their current strategies.  The findings have been used to inform recommendations for ways  to enhance the effectiveness of the delivery of feedback through  dashboards to provide value to students in developing effective  learning strategies to meet their educational goals.   Categories and Subject Descriptors  K.3.1 Computer uses in education    General Terms  Design, Human Factors.   Keywords  Learning analytics, feedback, dashboards, self-regulated learning.   1. INTRODUCTION  The use of learning analytics to provide feedback directly to  students on their learning engagement and performance is a  rapidly expanding area of development. Much of the initial focus  of learning analytics tool development concentrated on delivering  data about students activities to academics and institutions,  mostly to support student retention [1]. However, new tools are  now emerging that give students direct access to data on their   assessment performance and engagement in activities delivered  via learning management systems (LMS) and other online learning  and assessment tools. A popular format for the delivery of such  feedback to students is a dashboard that visualizes multiple  sources of data about students engagement and performance in a  single consolidated view.    The purpose of the research was to develop a greater  understanding of how students interpret feedback delivered  through learning analytics dashboards and the influence this has  on their learning strategies and motivation. The research was  guided by three main research questions:    1. How do students interpret feedback delivered through learning  analytics dashboards   2. What actions do students take in response to dashboard  feedback   3. How do access to dashboard feedback influence students  motivation and performance in their course   This poster presents results for the first of these research questions  considering the ways that students interpreted the data presented  through the dashboard. By understanding more about how  students interpret the different presentations of dashboard  feedback, and areas where they struggle to interpret the data,  recommendations can be made for future dashboard design as well  as areas for student learning support.   2. METHOD  2.1 Participants  The study was conducted in two large undergraduate subjects at  the University of Melbourne in the first semester of the 2014  academic year. The subjects, a first-year biology subject and a  second-year Japanese language subject, were both delivered in a  blended learning format incorporating online activities as well as  face-to-face lectures, tutorials, labs and/or workshops. A total of  24 students were recruited to participate in this study with 14  students from Biology and 10 students from Japanese.    2.2 Dashboard Design  The dashboard design was primarily influenced by the learning  design of the subjects and the data available from the Blackboard  Learning Management System. The different learning activities in  each subject resulted in variation in the layout and included data in  the dashboard for each subject (see figures 1 and 2). The  dashboards were populated with data on students assessment  performance (both formative and summative) and their online  engagement extracted from the LMS.      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.     Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723662   430    2.3 Data Collection Procedures  A mixed-methods approach was used in this study with four  sequential phases. The first phase involved an online survey of  participants, which was administered in the third week of the  semester. The second phase took place in week six of the semester  and involved an interview where participants were presented with  a dashboard of their own data and asked to reflect on their  interpretation of the data using the think aloud interview method  [4]. The third phase of the study involved a second interview,  which took place in week 11. This interview followed a similar  structure to the first interview, however before the participants  were shown their updated dashboard, they were asked several  questions about the actions they had taken in response to the first  interview and the impact these had on their performance and  motivation towards the subject. The final phase of the study  involved a second online survey, which was distributed after the  participants had received their final marks for the subject. In this  survey the participants were asked to reflect on the usefulness of  having access to the dashboard throughout the semester in light of  their final grade and the impact that it had on their study  strategies.   3. RESULTS  A thematic analysis was conducted on the data from across the  four phases of the study to address the first research question,   which asked how students interpret feedback delivered through  learning analytics dashboards. Through the analysis of the  interview and survey data it was found that there was diversity in  participants strategies and ability to interpret the feedback. The  majority of participants (83%) were able to offer an explanation  for the feedback taking into consideration their judgment of the  value of the activity, their motivation towards the subject, and  external factors that impacted their ability to engage in the  activity. However, there were also four out of the 24 participants  who struggled to interpret the feedback in a way that would allow  them to determine appropriate strategies to address the gap in their  learning. An element of the dashboard that had a major influence  on participants interpretation of the feedback was provision of the  class average. This influence was positive to most students  motivation (88%), but also caused distraction for some students  (50%) from their overall performance goal.     4. CONCLUSION  Despite the concerns raised in the literature about students  inability to meaningfully interpret feedback in this form and to  translate this into strategies to improve their learning [2, 3], the  findings from this study have shown that the majority of students  were able to identify gaps in their performance. These students  were able to attribute intrinsic or extrinsic reasons to link their  performance with their study strategies and determine a course of  action. However, there was also evidence of uncertainty in  interpretation of feedback, both due to difficulty in interpreting the  ways the data had been visualized as well as difficulty in linking  the feedback to their learning strategies. Therefore, when  dashboards are made available to students, it is recommended that  support resources are provided and processes established.    There are still many challenges to identifying the best types of  feedback to deliver via dashboard applications. However, the  indication of the usefulness of dashboard feedback to students  found in this study indicates that further exploration of the  possibilities is worthwhile.   5. ACKNOWLEDGMENTS  Our thanks to Tania Fernando for assistance with the dashboard  development, and Dr Abi Brooker for assistance with processing  the interview data. This research was funded through an early  career academic grant from the University of Melbourne.    6. REFERENCES  [1] Bichsel, J. 2012. Analytics in Higher Education: Benefits,   Barriers, Progress, and Recommendations (Research  Report), Louisville, CO: EDUCAUSE Centre for Applied  Research. Retrieved from  http://net.educause.edu/ir/library/pdf/ERS1207/ers1207.pdf.   [2] MacNeill, S., Campbell, L., and Hawksey, M. 2014.  Analytics for Education. Journal of Interactive Media in  Education, 1-12.   [3] Corrin, L., Kennedy, G., and Mulder, R. 2013. Enhancing  learning analytics by understanding the needs of teachers. In  H. Carter, M. Gosper & J. Hedberg (Eds.), Electric Dreams.  Proceedings ascilite 2013 Sydney, 201-205.   [4] Ericsson, K.A., & Simon, H.A. (1980). Verbal reports as  data. Psychological Review, 87, 215-250.         Figure 1. Example of the student dashboard for Biology   Figure 2. Example of the student dashboard for Japanese   431      "}
{"index":{"_id":"83"}}
{"datatype":"inproceedings","key":"Rogers:2015:LAO:2723576.2723649","author":"Rogers, Tim and Colvin, Cassandra and West, Deborah and Dawson, Shane","title":"Learning Analytics in Oz: What's Happening Now, What's Planned, and Where Could It (and Should It) Go","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"432--433","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723649","doi":"10.1145/2723576.2723649","acmid":"2723649","publisher":"ACM","address":"New York, NY, USA","keywords":"institutional preparedness, learning analytics, maturity model, student retention","abstract":"This poster outlines the process and purpose of two related Australian Office for Learning and Teaching (OLT) commissioned grants to investigate the current usage and future potential of learning analytics in Australian Higher Education, with a view to developing resources to guide Australian universities in their adoption of learning analytics. The commissioned grants run from February 2014 to June 2015. Preliminary results will be available for LAK 15.","pdf":"Learning Analytics in Oz: What's happening now, what's  planned, and where could it (and should it) go   Tim Rogers  University of South   Australia  Adelaide Australia  +61-8-8302 7842   Tim.rogers  @unisa.edu.au      Cassandra Colvin  University of South   Australia  Adelaide Australia  +61-8-8302 7843   Cassandra.colvin @unisa.edu.au      Deborah West  Charles Darwin   University  Darwin Australia  +61-8-8946 6861  Deborah.west  @cdu.edu.au      Shane Dawson  University of South   Australia  Adelaide Australia  +61-8-8302 7850  Shane.Dawson  @unisa.edu.au   ABSTRACT  This poster outlines the process and purpose of two  related Australian Office for Learning and Teaching  (OLT) commissioned grants to investigate the current  usage and future potential of learning analytics in  Australian Higher Education, with a view to developing  resources to guide Australian universities in their adoption  of learning analytics. The commissioned grants run from  February 2014 to June 2015. Preliminary results will be  available for LAK 15.    General Terms  Management, Human Factors.   Keywords  Learning analytics; student retention; institutional  preparedness; maturity model   1. LEARNING ANALYTICS AS AN  INSTITUTIONAL DECISION-MAKING  ISSUE  While learning analytics is often touted as a game  changer for higher education [4] there remains  considerable doubt about how the field will develop and  so what steps universities should take to adopt data-driven  practices to improving student engagement, success and  retention. While the majority of developments in the field  have so far focused on addressing students at-risk of  failure or drop-out [5] there is considerable interest in  how data can be marshaled to improve learning and  curriculum and progressively evolve to supporting  personalised learning within mass higher education [e.g.  2].    Given the rapid emergence of the field and a  commensurate increase in budgetary constraints there has   been no shortage of advice on what an institution should  do [e.g. 3]. However, these tracts offer vey abstract and  normative descriptions and there is little sense of how an  institution can move from an early stage of digital  awareness to a more rigorous data driven culture. The aim  of the two OLT commissioned projects outlined in this  poster is to develop a detailed guide that will allow  institutions to see where they sit within the likely  potential(s) of the field and what their current choices  imply for their future capabilities (e.g. when or whether to  consider open source options, finding the appropriate role  for vendors, comparing methods for making data  accessible, considering the tradeoff between internal skills  development and outsourcing etc.).  .   2. THE PROJECT  Key research aims include the comprehension of the  current state of learning analytics in Australian higher  education and the development of a roadmap, maturity  model or similar to guide the uptake of learning analytics  tools and practices. The project was undertaken by two  separate but complementary project teams, one led by the  University of South Australia (UniSA) (partner  institutions: University of Technology Sydney (UTS)  University of Melbourne (Melb), University of the  Sunshine Coast (USC), Macquarie University (Macq),  University of New England (UNE)); the other by Charles  Darwin University (CDU) (partner institutions: Griffith  University (Griff), Batchelor Institute of Indigenous  Tertiary Education (Batchelor), Murdoch University  (Murdoch), University of Newcastle (UoN)).     The projects gleaned primary data from institutions and  key experts about their perceptions regarding learning  analytics, its potential for the higher education sector, and  possible affordances and constraints.     Data was sourced through the following:     Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on  the first page. Copyrights for third-party components of this work  must be honored. For all other uses, contact the Owner/Author.    Copyright is held by the owner/author(s). LAK '15, Mar 16-20,  2015, Poughkeepsie, NY, USA A C M  978-1-4503-3417- 4/15/03. http://dx.doi.org/10.1145/2723576.2723649   432  http://dx.doi.org/10.1145/2723576.2723649   2.1 Interviews   International learning analytics research experts   (conducted primarily at LAK 14) (n    Learning analytics vendors (n    International institutional senior managers (n    Australian institutional senior managers (n    Australian academics (n       2.2 Group concept mapping  A group concept mapping process elicited the opinions of  28 key international learning analytics figures on the  factors considered vital to the long-term impact of  learning analytics adoption. The participants were drawn  from a range of backgrounds: research, practitioner, senior  manager, vendor and not-for-profit advocacy  organisations. The group concept mapping process[1] is a  blend of qualitative and quantitative approaches  incorporating brainstorming, sorting of ideas,  multidimensional scaling and cluster analysis.     2.3 Surveys  Two surveys designed to capture current practices and  levels of awareness of learning analytics within Australian  universities at an institutional (n  (n     These multiple sources of data will allow a granular and  multidimensional insight into current and future  operationalisations and conceptualisations of learning  analytics:     Bringing together the perspectives of vendors,  researchers and practitioners will allow the  triangulation of their insights.     The temporal, processual dimensions of the  research (i.e. the future scoping in the group  concept mapping) will allow the actual and  planned practices of today to be critiqued via the  possibilities inherent in the future.    The co-participation of researchers and  practitioners will yield insight into how the  research agenda can better inform higher  education practice.     3. ACKNOWLEDGMENTS  This research is funded through the Australian  Governments Department of Education, Office of  Learning and Teaching Strategic Priority Projects  Scheme. UniSA led project reference no: SP13-3249.  CDU led project reference no: SP13-3268.     4. REFERENCES  [1] Kane, M. and Trochim, W. 2007. Concept mapping for   planning and evaluation. Sage.   [2] McLoughlin, C. and Lee, M.J. 2009. Personalised learning  spaces and self-regulated learning: Global examples of  effective pedagogy. Same places, different spaces.  Proceedings ascilite Auckland 2009. (2009).   [3] Norris, D. and Baer, L. 2013. Building organizational  capacity for analytics. Educause.   [4] Oblinger, D. 2012. Game changers: education and  information technologies. Educause.   [5] Siemens, G., Dawson, S. and Lynch, G. 2013. Improving  the quality and productivity of the higher education sector:  Policy and strategy for systems level deployment of  learning analytics. Australian Government: Office for  Learning and Teaching.         433      "}
{"index":{"_id":"84"}}
{"datatype":"inproceedings","key":"Shibani:2015:TMA:2723576.2723648","author":"Shibani, Antonette and Koh, Elizabeth and Hong, Helen","title":"Text Mining Approach to Automate Teamwork Assessment in Group Chats","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge,"series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"434--435","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723648","doi":"10.1145/2723576.2723648","acmid":"2723648","publisher":"ACM","address":"New York, NY, USA","keywords":"assessment, chat, collaboration, teamwork, text mining","abstract":"The increasing use of chat tools for learning and collaboration emphasizes the need for automating assessment. We propose a text mining approach to automate teamwork assessment in chat data. This supervised training approach can be extended to other domains for efficient assessment.","pdf":"Text mining approach to automate teamwork assessment  in group chats   Antonette Shibani  antonette.x@nie.edu.sg  Elizabeth Koh  elizabeth.koh@nie.edu.sg   Helen Hong  helen.hong@nie.edu.sg  National Institute of Education, Nanyang Technological University,   Singapore     ABSTRACT  The increasing use of chat tools for learning and collaboration  emphasizes the need for automating assessment. We propose a  text mining approach to automate teamwork assessment in chat  data. This supervised training approach can be extended to other  domains for efficient assessment.   Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Computer Uses in  Education - Collaborative Learning,   I.2.7 [Artificial Intelligence]: Natural Language Processing -  Text analysis   General Terms  Measurement, Design   Keywords  Teamwork, collaboration, chat, assessment, text mining   1. INTRODUCTION  Chat texts are complex to analyze due to their short lengths and  contextual complexities [2]. Conventional analysis of text data  will not be sufficient to capture the meaning and context of chat  data. Hence, it requires more levels of processing to automate  the analysis. The large amount of textual data in big data sets  like chats makes the process of manual assessment time- consuming. Automating this process will reduce effort since it  captures repeated words and ensures consistency.     In collaborative learning, the teamwork involved is often taken  for granted. Teamwork is also an important 21st century  competency. Therefore this study examines teamwork in chat  data. We design an approach to assess teamwork by automating  the analysis of chat data using text mining methods. The six  teamwork dimensions are from the analytic framework to  measure teamwork competency [3]. This supervised approach  will require more effort for training the classifiers as it requires  many steps of rules to be written. However, once the system is  developed, classification becomes efficient for future analysis.   2. DATA COLLECTION  Teamchats, a custom-made chat program based on the open  source Ajax chat was the chat environment used for this study  [6]. Secondary school students in Singapore were randomly  grouped into teams of three or four in the online chat.    There were a total of 272 students grouped into 76 teams. Two  tasks  an ice-breaker task and a dilemma task were assigned to  them through a chat script that was managed by a facilitator. The  duration of the two chat tasks were about 45 minutes. A total of  19672 lines were found in 76 teams with a minimum of 71 lines  and a maximum of 487 lines per team.    3. PREPARATION  Before classification and analysis, we prepared the chat data to  aid machine learning using these two steps.   3.1 Situation Coding  A single line of chat cannot be coded for a teamwork dimension  because the meaning is implied only by the context of the  previous and the next few lines. So, a set of lines grouped by a  topic was defined as a situation. The situations were derived  from emergent group development phases [7] but with non- chronological and interwoven stages in our data.  Manual  content analysis was used to code situations guided by the  definitions in Table 1. The rationale was to reduce the ambiguity  in the text by designating chat lines into these situations.   Table 1. Definitions for Situations   Situation Definition  ST1  Introduction   Students get introduced to the chat environment  and team members. E.g. Hello   ST2   Icebreaker task  preparation   Students prepare the team members to start the  ice-breaker task using action words or  suggestions. E.g. Lets do the icebreaker   ST3   Icebreaker task  discussion   Students discuss about the icebreaker task by  asking questions and contributing ideas using  content related words.   ST4 Icebreaker  task ending   Students wrap up icebreaker task coming to a  consensus by summarizing the discussion.   ST5   Dilemma task  preparation   Students prepare the team members to start the  dilemma task. This includes transitions and  question repetition E.g. Identify the problem    ST6 Dilemma  task discussion   Students discuss about the dilemma task by  contributing ideas, urls and asking questions.   ST7 Dilemma  task ending   Students wrap up dilemma task discussion by  coming up with a solution with clear consensus.   ST8        Team dismissal   Students leave the team by bidding farewell to  team members in this last situation. E.g. Bye   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for third-party components of this work must be honored. For all other  uses, contact the Owner/Author.   Copyright is held by the owner/author(s).  LAK '15, Mar 16-20, 2015, Poughkeepsie, NY, USA  ACM 978-1-4503-3417-4/15/03.  http://dx.doi.org/10.1145/2723576.2723648   434  http://dx.doi.org/10.1145/2723576.2723648   3.2 Pre-processing  The chat data in its raw form contains contents that need to be  cleaned before processing by the classification system [2]. All  words should be decapitalized and unnecessary stop words and  punctuation marks should be removed. The chat language  should be pre-processed as suggested below followed by  tokenization of text.   Acronyms  ikr, nvm  Use urban dictionary  Short forms  sry, cher Create user-defined dictionary  Misspellings  discribe, evryone Use spell checker with auto- correct dictionary  Emoticons - ;), :p Convert into indicators  Purposely misspelled words  soooooo, yayy Use regular  expression pattern matching  Regional dialect  alamak, can lah Define Singlish glossary  Words with different meanings 2, hw Find context with rules   4. CLASSIFICATION  The indicators of teamwork dimensions should be carefully  identified to form the indicative terms dictionary [2]. The  indicative terms dictionary is used for generating rules for  categorization in multiple steps to capture context and achieve  accuracy in assessment.   4.1 Rule Generation  Rules will be written iteratively making use of NLP features for  qualitative analysis of chat data [1, 5]. The following factors  will be used to automate the coding of teamwork dimensions.  Existence: The existence of indicative terms will be first  checked and coded Boolean yes if the term is present and no  if not present.  Frequency: The frequency of indicative terms could be a  possible indication of the strength of a dimension. Higher  frequency of words would mean that the situation is more  indicative of that dimension. However, the most frequent terms  should be examined with a threshold to check if they add value  or have no meaning.   Weightage: Certain indicative terms may hold higher weightage  than others since they would strongly indicate the presence of a  dimension. Terms can hold a score according to their weightage.  Proximity: The position of words in the situations and how far  they are apart will be useful in identifying certain instances of  dimensions.   Complex patterns: More complex patterns combining the  indicative terms and situations can be written to capture specific  instances of exceptions in the chat.   The workflow of automated analysis is shown in Figure 1. Each  step requires further refining after testing to achieve accuracy.      Figure 1: Automated analysis workflow   4.2 Categorization  Categorization techniques including k-NN, Naive Bayes and  SVM will be applied for topic classification using training and  test sets from our data [4] and checked for reliability by  comparing with manual coding. Scores for the categorized  teamwork dimensions can be given and then aggregated for each  student and transferred to the teamwork competency measure.   5. CONCLUSION AND FUTURE WORK  An approach to automate the analysis of chat data using text  mining is proposed.  It will improve the accuracy of automated  analysis by including more steps for identifying the context of  chat text when compared to the existing approaches. We are  now in the process of classification using R. We will also need  to test the reliability of the classifiers with other datasets.  Nevertheless, we posit that the system is transferable to other  corpora by defining a new indicative terms dictionary. The  findings of this study will also be useful in identifying  appropriate classifiers and visualization techniques for chat data  analysis. Future work could find ways to automatically identify  situations based on the time duration, number of messages or  admin message markers.   6. ACKNOWLEDGEMENTS  This paper refers to data from the research project  OER62/12EK, funded by the Education Research Funding  Programme, National Institute of Education, Nanyang  Technological University, Singapore. The views expressed in  this paper are the authors and do not necessarily represent the  views of NIE.   7. REFERENCES  [1] Crowston, K., Allen, E. E., & Heckman, R. 2012. Using   natural language processing technology for qualitative data  analysis. International Journal of Social Research  Methodology, 15(6), 523-543.   [2] Dong, H., Hui, S. C., & He, Y. 2006. Structural analysis of  chat messages for topic detection. Online Information  Review, 30(5), 496-516.   [3] Koh, E., Hong, H., & Seah, J. (2014). An Analytic Frame  and Multi-method Approach to Measure Teamwork  Competency. Proceedings of the 14th IEEE International  Conference on Advanced Learning Technologies, 7-9 July  2014 (pp. 264-266). IEEE.   [4] zyurt, ., & Kse, C. 2010. Chat mining: Automatically  determination of chat conversations topic in Turkish text  based chat mediums. Expert Systems with  Applications, 37(12), 8705-8710   [5] Ros, C., Wang, Y. C., Cui, Y., Arguello, J., Stegmann, K.,  Weinberger, A., & Fischer, F. 2008. Analyzing  collaborative learning processes automatically: Exploiting  the advances of computational linguistics in computer- supported collaborative learning. International journal of  computer-supported collaborative learning, 3(3), 237-271.   [6] Tschan, S., & Nicolcev, P. 2013. Package AJAX_Chat  (Version 0.8.7) [Software]. Available from  http://frug.github.io/AJAX-Chat/   [7] Yoon, S. W., & Johnson, S. D. 2008. Phases and patterns  of group development in virtual learning  teams. Educational Technology Research and  Development, 56(5-6), 595-618.   435    p340-chen  p350-dascalu  p355-simsek  1. INTRODUCTION  2. RELATED WORK  2.1 Teaching Academic Writing  2.2 Automated Rhetorical Analysis  2.3 Learning Analytics   3. STUDY  3.1 Dataset  3.2 Methodology  3.3 Correlational Study Results  3.4 Multiple Regression Analysis Results   4. DISCUSSION  4.1 The performance of XIP on the student essays  4.2 Relationship between the tutors marking grid and the salient sentences  4.3 Some Outliers   5. CONCLUSION  6. REFERENCES   p360-worsley  1. INTRODUCTION  2. PRIOR LITERATURE  3. METHODS  3.1 Activity Sequence  3.2 Data  3.3 Algorithm   4. Results  4.1 Hand-Annotated Data Analysis  4.2 Multimodal Sensor Data Analysis   5. DISCUSSION  6. FUTURE WORK  7. CONCLUSION  8. REFERENCES   p368-suthers  p378-mouri  p383-zhu  p388-knight  p390-drachsler  p392-hickey  p394-duval  p396-drachsler  p398-monroy  p400-mor  p402-xu  p404-ye  p406-harrer  p408-kang  p410-hawn  p412-dodge  p414-niemann  p416-shehata  1. Introduction  2. Student Success System  3. UWS Adoption of S3   p418-maruya  p420-barmaki  1. INTRODUCTION  2. GESTURE and EDUCATION  3. STUDY  4. DISCUSSION  5. ACKNOWLEDGMENTS  6. REFERENCES   p422-nwanganga  p424-absar  p426-worsley  1. INTRODUCTION  2. METHODS  2.1 Analyses   3. Results  4. DISCUSSION  5. CONCLUSION  6. REFERENCES   p428-zushi  p430-corrin  p432-rogers  1. LEARNING ANALYTICS AS AN INSTITUTIONAL DECISION-MAKING ISSUE  2. THE PROJECT  2.1 Interviews  2.2 Group concept mapping  2.3 Surveys   3. ACKNOWLEDGMENTS  4. REFERENCES   p434-shibani  1. INTRODUCTION  2. DATA COLLECTION  3. PREPARATION  3.1 Situation Coding  3.2 Pre-processing   4. CLASSIFICATION  4.1 Rule Generation  4.2 Categorization   5. CONCLUSION AND FUTURE WORK  6. ACKNOWLEDGEMENTS  7. REFERENCES      "}
{"index":{"_id":"85"}}
{"datatype":"inproceedings","key":"Dascalu:2015:RIT:2723576.2723647","author":"Dascalu, Mihai and Stavarache, Larise L. and Trausan-Matu, Stefan and Dessus, Philippe and Bianco, Maryse and McNamara, Danielle S.","title":"ReaderBench: An Integrated Tool Supporting Both Individual and Collaborative Learning","booktitle":"Proceedings of the Fifth International Conference on Learning Analytics And Knowledge","series":"LAK '15","year":"2015","isbn":"978-1-4503-3417-4","location":"Poughkeepsie, New York","pages":"436--437","numpages":"2","url":"http://doi.acm.org/10.1145/2723576.2723647","doi":"10.1145/2723576.2723647","acmid":"2723647","publisher":"ACM","address":"New York, NY, USA","keywords":"Discourse Analysis, Learning Analytics, Participation/Collaboration Assessment, Reading Strategies, Textual Complexity","abstract":"The core of our ReaderBench software framework exposes a unified vision for predicting and assessing comprehension in both individual and collaborative learning scenarios. ReaderBench aims to improve both the quality and the classification of the analyzed documents by using an expanded range of criteria such as: morphology, semantics, discourse analysis with emphasis on polyphony and dialogism, thus providing reliable support for both tutors and students across a range of educational settings. ReaderBench uses a unitary cohesion-based representation of discourse applied into three major directions, all tightly connected by the underlying model and the Natural Language Processing (NLP) computations: reading strategies, textual complexity, and collaboration evaluation in Computer Supported Collaborative Learning (CSCL) conversations.","pdf":"ReaderBench: An Integrated Tool Supporting both  Individual and Collaborative Learning   Mihai Dascalu,   Larise L. Stavarache,   Stefan Trausan-Matu   University Politehnica of Bucharest  313 Splaiul Indepententei   Bucharest, Romania  mihai.dascalu@cs.pub.ro   larise.stavarache@ro.ibm.com  stefan.trausan@cs.pub.ro   Philippe Dessus,  Maryse Bianco     Univ. Grenoble Alpes, LSE,  F38000 Grenoble, France   philippe.dessus@upmf-grenoble.fr  maryse.bianco@upmf-grenoble.fr   Danielle S. McNamara       Arizona State University  Tempe, Arizona 85287-2111   dsmcnama@asu.edu          ABSTRACT  The core of our ReaderBench software framework exposes a  unified vision for predicting and assessing comprehension in both  individual and collaborative learning scenarios. ReaderBench  aims to improve both the quality and the classification of the  analyzed documents by using an expanded range of criteria such  as: morphology, semantics, discourse analysis with emphasis on  polyphony and dialogism, thus providing reliable support for both  tutors and students across a range of educational settings.  ReaderBench uses a unitary cohesion-based representation of  discourse applied into three major directions, all tightly connected  by the underlying model and the Natural Language Processing  (NLP) computations: reading strategies, textual complexity, and  collaboration evaluation in Computer Supported Collaborative  Learning (CSCL) conversations.   Categories and Subject Descriptors  I.2.7 [Natural Language Processing], K.3.1 [Computer Uses in  Education]   General Terms  Algorithms, Measurement   Keywords  Discourse Analysis, Learning Analytics, Reading Strategies,  Textual Complexity, Participation/Collaboration Assessment.   1. INTRODUCTION  ReaderBench is an educational software that uses natural language  processing and text mining technologies to allow both tutors and  learners to regularly track learning progress, providing multi- lingual support for English, French, and partially for Italian. The  main areas covered by our system are derived from a unitary   cohesion-based representation of discourse [1] used to: a) identify  reading strategies [2], b) evaluate textual complexity [2], and  c) assess participation and collaboration [3]. Overall,  ReaderBench is a flexible and easy-to-use tool that can be  integrated with a students learning path to enable real-time  progress tracking. In a nutshell, ReaderBenchs evaluation process  starts with the setup of an initial baseline of the textual complexity  scores for the targeted educational materials, continues with the  detection of the reading strategies used by learners self-explaining  the given texts, and finally assesses participation and  collaboration in CSCL conversations that encourage creativity.   2. OVERVIEW  The benefits of using an automatic system to evaluate students  learning progress during an educational activity (e.g., courses,  evaluations, exams, research activities) adds valuable  enhancements to the tutor-learner relationship, particularly with  regards to communication, real-time feedback, and, last but not  least, aligning expectations for both sides. If we consider the  tutor-learner relationship as a whole, then we can clearly see the  importance of having immediate and relevant feedback on the  learning and understanding processes, which on the tutors side  can be both time consuming and potentially subjective.   ReaderBench is a fully functional framework which uses text  mining techniques based on advanced natural language processing  and machine learning algorithms to design and deliver summative  and formative assessments using multiple sets of data (e.g., textual  materials, behavior tracks, meta-cognitive explanations). The  cohesion-based integrated approach used within ReaderBench  addresses multiple facets of learners comprehension processes.     Figure 1 ReaderBench General processing pipeline [1]      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee. Request permissions from  Permissions@acm.org.  LAK '15, March 16 - 20, 2015, Poughkeepsie, NY, USA Copyright 2015  ACM 978-1-4503-3417-4/15/03...$15.00  http://dx.doi.org/10.1145/2723576.2723647     More specifically, in terms of educational data mining,  ReaderBench encompasses a wide variety of techniques, besides  the natural language processing pipeline applied in the pre- processing phase including [1]: semantic distances based on  ontologies, Latent Semantic Analysis (LSA) and Latent Dirichlet  Allocation (LDA) for evaluating textual cohesion, as well as  Social Network Analysis for modeling the interactions among  participants. In addition, specific internal processes deduced from  a cohesion-based underlying discourse structure are focused on  (see Figure 1)[1]: a) topic extraction for determining the relevance  of keywords, b) extractive unsupervised summarization using the  bottom-up scoring mechanism, c) identification of reading  strategies employed by learners in their verbalizations, d) textual  complexity assessment by enforcing Support Vector Machines  (SVM) classification on top a wide variety of metrics ranging  from surface factors to morphology, semantics and discourse  analysis, as well as e) collaboration evaluation derived from the  cohesion-based and voice inter-animation automatic models. In  sum, our integrated tool is applicable across multiple situations:  tutor course preparation, essay scoring, vocabulary structure  analysis, evaluation, or student assessment during a course.   Moreover, ReaderBench can be used as a Personal Learning  Environment (PLE), enabling two kinds of work-loops in terms of  individual learning: a) a reading loop, in which learners can get  information about the textual organization and structure of reading  materials from ReaderBench, and b) a writing loop that provides  learners the opportunity to develop at length what they understood  from the text (e.g., reading strategies applied on self-explanations  or summaries). In addition, the tutor can use ReaderBench to  select appropriate textual materials suited for the learners level.  From a collaborative perspective, ReaderBench can be used to  support both learners and tutors in the learning process. Within  the reading loop, learners are focused on other peers  contributions, as well as on an overview of the entire  conversation. The writing loop emphasizes the active participation  and collaboration of each member by considering learners  productions in the ongoing conversations.   3. COMPARISON TO SIMILAR SYSTEMS  The design of ReaderBench is focused on multiple views centered  on discourse cohesion used for comprehension assessments and  text analysis. In comparison to existing tools such as Coh-Metrix  [5], DMeasure [4] or iSTART [6], ReaderBench encompasses their  major functionalities and extends the individual learning  perspective with collaborative specific CSCL analyses to support  both tutors and learners.   In comparison to Coh-Metrix, ReaderBench uses different textual  analysis factors and employs SVM to enhance prediction  accuracy. ReaderBench also incorporates multi-hierarchical  classes obtained through the aggregation of factors from common  text analysis systems [7], as well as specific factors focused on  semantics and cohesion-based discourse analysis [2]. Coh-Metrix,  by contrast, provides a more detailed evaluation and a broader  range of integrated factors by also covering more usage scenarios.  If we consider automated software tools for French, DMeasure is  also a good alternative for assessing textual complexity, but it  lacks a broader set of semantic and discourse factors, as well as a  friendly interface for common usage.  In comparison to iSTART, a comprehension strategy tutoring  system, whereas iSTART is principally appropriate for high school  and adult students, ReaderBench also targets primary school  students, and provides multi-lingual support.   4. CONCLUSIONS  ReaderBench provides large range computations and data  measurements, which have been previously validated and  compared to human scores [1-3]. Moreover, ReaderBench can be  integrated across a variety of pedagogical scenarios and can be  easily extended for different purposes such as support for other  languages and vocabularies, integration of additional semantic and  cohesion-centered factors, as well as the possibility to visually  display charts and reports to facilitate feedback.   ReaderBench can be configured such that certain complexity  factors, which do not apply to all vocabularies, are excluded from  the final analysis report. On the one hand, tutors can select  learning materials by using the multi-dimensional textual  complexity model, can compare the learners productions to the  automatically extracted features, and can evaluate self- explanations while addressing the identified reading strategies. On  the other hand, learners can take advantage of the document  assessment facilities in order to better understand the structure,  difficulty level and topics of the assigned material. Moreover,  they can improve their own self-regulated processes through the  systems feedback, especially in the case of their self-explanations  in terms of the identified reading strategies. In addition,  collaboration with other peers can be an alternative for problem- solving specific tasks.   5. ACKNOWLEDGEMENTS  This research was partially supported by the ANR DEVCOMP  project, by the Sectorial Operational Program Human Resources  Development 2007-2013 of the Ministry of European Funds  through the Financial Agreements POSDRU/159/1.5/S/ 134398  and 132397, by the LTfLL FP7 project, and by NSF grants  1417997 and 1418378 to Arizona State University.   6. REFERENCES  [1] Dascalu, M., 2014. Analyzing discourse and text complexity   for learning and collaborating, Studies in Computational  Intelligence. Springer, Switzerland.   [2] Dascalu, M., Dessus, P., Trausan-Matu, S., Bianco, M., and  Nardy, A., 2013. ReaderBench, an environment for analyzing  text complexity and reading strategies. In AIED 2013, H.C.  Lane, K. Yacef, J. Mostow and P. Pavlik Eds. Springer,  Memphis, USA, 379388.   [3] Dascalu, M., Trausan-Matu, S., and Dessus, P., 2014.  Validating the Automated Assessment of Participation and of  Collaboration in Chat Conversations. In ITS 2014, S.  Trausan-Matu, K.E. Boyer, M. Crosby and K. Panourgia  Eds. Springer, Honolulu, USA, 230235.   [4] Franois, T. and Miltsakaki, E., 2012. Do NLP and machine  learning improve traditional readability formulas In  PITR2012 ACL, Montreal, Canada, 4957.   [5] McNamara, D.S., Graesser, A.C., McCarthy, P., and Cai, Z.,  2014. Automated evaluation of text and discourse with Coh- Metrix. Cambridge University Press, Cambridge.   [6] McNamara, D.S., O'Reilly, T.P., Rowe, M., Boonthum, C.,  and Levinstein, I.B., 2007. iSTART: A web-based tutor that  teaches self-explanation and metacognitive reading  strategies. In Reading comprehension strategies, D.S.  McNamara Ed. Erlbaum, Mahwah, NJ, 397420.   [7] Nelson, J., Perfetti, C., Liben, D., and Liben, M., 2012.  Measures of text difficulty: Testing their predictive value for  grade levels and student performance. Council of Chief State  School Officers.     "}
