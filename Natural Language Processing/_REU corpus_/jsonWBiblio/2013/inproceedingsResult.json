{"index":{"_id":"1"}}
{"datatype":"inproceedings","key":"Suthers:2013:LAM:2460296.2460298","author":"Suthers, Dan and Verbert, Katrien","title":"Learning Analytics As a Middle Space","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"1--4","numpages":"4","url":"http://doi.acm.org/10.1145/2460296.2460298","doi":"10.1145/2460296.2460298","acmid":"2460298","publisher":"ACM","address":"New York, NY, USA","keywords":"boundary objects, learning analytics, multidisciplinarity, productive multivocality","abstract":"Learning Analytics, an emerging field concerned with analyzing the vast data given off by learners in technology supported settings to inform educational theory and practice, has from its inception taken a multidisciplinary approach that integrates studies of learning with technological capabilities. In this introduction to the Proceedings of the Third International Learning Analytics  Knowledge Conference, we discuss how Learning Analytics must function in the middle space where learning and analytic concerns meet. Dialogue in this middle space involves diverse stakeholders from multiple disciplines with various conceptions of the agency and nature of learning. We hold that a singularly unified field is not possible nor even desirable if we are to leverage the potential of this diversity, but progress is possible if we support productive multivocality between the diverse voices involved, facilitated by appropriate use of boundary objects. We summarize the submitted papers and contents of these Proceedings to characterize the voices and topics involved in the multivocal discourse of Learning Analytics.","pdf":"Learning Analytics as a  Middle Space   Dan Suthers Dept. of Information and Computer Sciences,  University of Hawaii at Manoa 1680 East West Road, POST 309  Honolulu, HI 96822 USA suthers@hawaii.edu  Katrien Verbert Department of Computer Science, Eindhoven  University of Technology P.O. Box 513 5600 MB Eindhoven, The  Netherlands k.verbert@tue.nl  ABSTRACT Learning Analytics, an emerging field concerned with an- alyzing the vast data given off by learners in technology supported settings to inform educational theory and prac- tice, has from its inception taken a multidisciplinary ap- proach that integrates studies of learning with technological capabilities. In this introduction to the Proceedings of the Third International Learning Analytics & Knowledge Con- ference, we discuss how Learning Analytics must function in the middle space where learning and analytic concerns meet. Dialogue in this middle space involves diverse stake- holders from multiple disciplines with various conceptions of the agency and nature of learning. We hold that a sin- gularly unified field is not possible nor even desirable if we are to leverage the potential of this diversity, but progress is possible if we support productive multivocality between the diverse voices involved, facilitated by appropriate use of boundary objects. We summarize the submitted papers and contents of these Proceedings to characterize the voices and topics involved in the multivocal discourse of Learning Analytics.  Categories and Subject Descriptors K.3 [Computers and Education]: General  Keywords learning analytics, multidisciplinarity, boundary objects, pro- ductive multivocality  1. DIALECTICS IN LAK The International Learning Analytics & Knowledge (LAK)  Conference, now in its third year1, is a venue for reporting and advancing research that is motivated by the nexus of two emerging societal phenomena. First we are witnessing  1http://lakconference2013.wordpress.com/  Copyright is held by the author/owner(s). Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission. LAK 13, April 08 - 12 2013, Leuven, Belgium Copyright 2013 ACM 978-1-4503-1785-6/13/04 .  rapidly expanding use of technologies in supporting learn- ing, not only in established institutional contexts and plat- forms, but also in online settings offering free and open learn- ing opportunities. Second, the unprecedented availability of data that learners generate in the process of accessing ma- terials, interacting with educators and peers, and creating new content in these technological settings, coupled with ad- vances in analytics and data mining, knowledge modeling, and open data offer great potential for research into how learning takes place in socio-technical settings and the de- velopment of new forms of analytics that can inform learners and educators. Learning Analytics research seeks to bring these technical, pedagogical, and social domains into dia- logue with each other to ensure that interventions and orga- nizational systems serve the needs of all stakeholders [7].  The third year of a new conference series might be seen as a critical year in which the identity of an emerging field of study begins to stabilize. Learning Analytics from its in- ception has sought to take a multidisciplinary approach to integrating studies of learning with technological capabili- ties. LAK 2011 called for a focus on integrating the tech- nical and the social/pedagogical dimensions of learning an- alytics2. LAK 2012 was advertised as a multidisciplinary conference for: learning scientists; (computer) scientists and data/knowledge engineers; researchers in education, sociol- ogy, psychology, information science; educators at all levels; Institutional/Organizational data analysts; training and de- velopment professionals; educational and academic leaders; business leaders; [and] course management system develop- ers and leaders3. At that conference, papers addressed di- verse educational issues and approaches to those issues that leverage the availability of data about learning with compu- tational, representational and visualization techniques, but the first author concluded that learning and analytic con- cerns needed to be integrated more tightly.  This year, we sought to address these needs and bring these many voices into dialogue under the overarching theme of Dialectics in Learning Analytics. This theme has three facets. First, to explore the Middle Space within which Learning and Analytics intersect, we called for papers and events that explicitly connect analytic tools to theoretical and practical aspects of understanding and managing learn- ing. Second, we planned the program to foster Productive Multivocality between the diverse voices or perspectives that come from different disciplines, theories, research meth- ods, and roles within the educational enterprise. Third,  2https://tekri.athabascau.ca/analytics/ 3http://lak12.sites.olt.ubc.ca/  1    there is a dialectic between the Old and the New: we are facing the centuries-old problem of improving learning, but bringing established knowledge together with a new set of tools not available before. Also, we address these problems in the city of Leuven: centuries old, lively new. Below we elaborate on the first two themes.  2. THE MIDDLE SPACE Our advice to authors4 requested that papers address the  middle space where learning and analytics meet. Some papers may make contributions primarily to analytic tech- nologies, while others may contribute primarily to learning theory or educational practice, but learning analytics calls for work that address each in relation to the other.  Learning analytics is about learning. Learning analytics research should be explicit about the theory or conception of learning underlying the work, and manifest this conception in the work presented. We hold that no particular theory or conception of learning should be favored a-priori: individu- als, small groups, and/or larger collectives may be the agent of learning; and learning may consist of knowledge or skill acquisition, intersubjective meaning-making, or changes in identity and participation in a community, among other pro- cesses [9]. Furthermore, learning can be conceived of as simultaneously taking place at all of these granularities of agency and involving all of these epistemological processes. Research that analyzes learning processes across multiple granularities and brings multiple methodological and theo- retical orientations to bear are particularly appropriate for understanding learning as a complex phenomenon. Regard- less of how learning is conceptualized, the point is to encour- age learning analytics researchers to make their conception explicit and reflect on how their analytic approach relates to their conception of learning, so that we can be mindful about our assumptions in dialogue with each other.  Learning analytics is about analytics. Learning analyt- ics research should be explicit about how the work either offers relevant new analytic methods (e.g., computational, representational, statistical, and visualization methods) or advances our understanding of the value of existing analytic methods, in either case for understanding learning and edu- cational practices. Research on learning analytics may vary in the degree to which it makes technical contributions, but the connection to learning should be present. Research mak- ing a strong technical contribution need not also include a study in an applied setting, but should at least discuss how the properties of the technical contribution are relevant to understanding or managing learning in practice. Research with a learning theoretic or practical contribution need not advance the technical state of analytics, but should at least discuss or evaluate whether and how the affordances of the chosen analytic methods bring forth relevant aspects of the data in a manner enabling the primary contribution. Thus, we hope that the field will avoid simplistic judgments about work being merely technical vs. not innovative that are sometimes heard, but rather bring these kinds of contribu- tions into coordination.  In summary, although individual research efforts may dif- fer in their emphasis, we believe that all research in Learn- ing Analytics should address the middle space by includ- ing both learning and analytic concerns and addressing the  4http://lakconference2013.wordpress.com/for-authors/  match between technique and application. Advances in learn- ing theory and practice are welcome, provided that they are accompanied with an evaluation of how existing or new an- alytic technologies support such advances. Advances in an- alytic technologies and methods are welcome, provided that they are accompanied with an evaluation of how understand- ing of learning and educational practices may be advanced by such methods.  3. PRODUCTIVE MULTIVOCALITY Learning analytics is multidisciplinary, drawing on the-  ories and methods from diverse research traditions. Our community includes educators, learning scientists, computer scientists, administrators, and policy makers, among others (see also the LAK 2012 call quoted above). Many voices are brought together, leading to the question of how such multivocality can be productive. Is a unified field of Learn- ing Analytics possible  The situation is similar to that of the Computer Sup- ported Collaborative Learning research community, wherein efforts have been made to bridge across theoretical and method- ological traditions [5, 10], leading to the following observa- tions that apply to Learning Analytics as well. The field is too diverse (in theory, methods, and even objectives) for unification to be possible; nor would it be desirable, as this diversity is a potential strength of Learning Analyt- ics. However, this strength can only be realized if these voices are brought into dialogue (or multi-logue) with each other, avoiding balkanization into disjoint dialogues nomi- nally under learning analytics and knowledge. This ten- sion between the impossibility of unification and the need to dialogue might be served by boundary objects [8], phys- ical or conceptual entities that each tradition interprets in its own way, but that provide common referents or points of articulation to ground conversations.  Boundary objects can exist and be leveraged in multiple layers. The middle space itself serves as a topical bound- ary object, as Learning Analytics brings multidisciplinary voices into discourse around the question of how analytic tools can help our understanding of learning and design of educational practices. A focus on data grounds discussion in concrete situations where differing conceptions are uncov- ered in their implications for learning and educational prac- tice. Yet, shared data is not enough: members of different traditions may talk past each other, construing the data in entirely different ways and addressing incommensurable con- cerns. An attempt at shared analytic objectives is helpful: even if the objectives are interpreted in different ways, this provides a second dimension along which assumptions can be exposed and compared. Analytic representations such as visualizations that are meaningful for members of multiple research traditions or for both technologists and practition- ers can play important roles in this process. Bringing repre- sentations from different analytic sources into alignment into each other highlights concordances and discordances. Visual representations are accessible in some manner to technolo- gists and practitioners alike, and can be a focus for questions about whether analytic computations are exposing features that are pedagogically actionable. The middle space is where we also find boundary objects in the form of specific peda- gogical objectives or interventions, some of which are noted below.  2    4. LEARNING ANALYTICS AS REFLECTED IN THESE PROCEEDINGS  Thus far we have been speaking of ideals, while a research community is also defined by the practices of its participants. Who is participating in the Learning Analytics dialogue, and what topics do we find in this middle space These questions might best be answered on an empirical basis, as reflected in the present proceedings.  Submissions were received with author affiliations from 31 countries world-wide, and accepted papers include af- filiations from Australia, Belgium, Canada, the Czech Re- public, Denmark, Ecuador, France, Germany, Greece, Hong Kong, Italy, Malta, the Netherlands, Norway, Spain, South Africa, Switzerland, the United Kingdom, and the United States of America. There were 58 full paper submissions, 37 short paper submissions (including 2 classified as design briefings), and also various panel, workshop and tutorial pro- posals. Each paper was reviewed by at least three reviewers, and our decisions were based on the content of the reviews as well as numerical rankings. Of the full papers, 16 (27.6%) were accepted as such, and 14 (24.1%) as short papers (with one conversion to a design briefing). Of the short papers, 8 (22.9%) were accepted in that category. Some other submis- sions not accepted as papers were encouraged to resubmit as posters, and 11 were accepted as such. We also accepted or solicited 3 panels, and the Workshop and Tutorial chairs accepted 5 of 10 workshop proposals and solicited 3 excellent tutorials.  Let us examine how the topics of these papers characterize the field of Learning Analytics. The following themes are emerging in the learning analytics area (see also [7, 11]):  Reflections on Learning Analytics. This section of the proceedings includes the present extended abstract and a contribution by Balacheff and Lund that addresses the con- ference theme, going into depth on the multi-disciplinarity of LAK and its potential for productive miltivocality.  Visualizations for reflection and awareness. Several con- tributions focus on increasing reflection and awareness through the use of visualization techniques. The focus is on collect- ing traces that learners leave behind and visualizing those traces to improve learning [2]. Dashboard applications are presented as well as evaluation studies that assess the impact of such dashboards on learning. Such dashboards are focus- ing on the analysis and visualization of different learning indicators to foster awareness and reflection about learning processes [1]. These indicators include resource accesses, time allocated, and knowledge level indicators. Visualiza- tion methods appear throughout the other categories, re- flecting their relevance to multiple stakeholders.  Social network analysis and visualization. Social learning analytics [3] is the core research topic in three collections of contributions. In the first, analysis and visualization of social interactions is researched to make people aware of their social context and to enable them to explore this con- text [4]. In TEL, this is particularly, but not only, relevant for Computer Supported Collaborative Learning (CSCL), where the interactions with peer learners are a core aspect of how learning is organized.  Communication and collaboration. Papers in this second social learning area focus on both reading and writing activ- ities, including analysis of paragraph level revisions and how collaboration forms around topics in collaborative writing,  analyses of discussion forums that includes temporal pat- terns of reading as well as writing and how analytics can support learner self-regulation, and how students promo- tion of each others work in a blogging environment can be leveraged for feedback.  Discourse analytics. Papers in this section have some affinity to those in Sequence Analysis further on as well to so- cial learning analytics. Discourse is approached from diverse perspectives, including scientometric path analysis (citation analysis that includes temporality) to model knowledge evo- lution in Wikiversity; a comparative evaluation of seven au- tomatic classification approaches for identifying exploratory dialogue; and theoretical reflections on how learning analyt- ics embody assumptions about both our own and learners epistemologies, with applications to analysis of user trace data.  Behavior analysis. Papers classified here offer advances in making higher level inferences from low level actions. The papers generally start with fine-grained behavior, but are wide ranging in the nature of the data and scope of anal- ysis. Three papers reflect the importance of extending our scope of analysis to include multimodal behavior such as ges- ture and object manipulation, and using alternative sources of data such as eye tracking, in this case to analyze joint attention. A fourth addresses how to uncover higher lev- erl learning processes based on low level data in the Kahn Academy.  Affect analytics. Affect is discussed in two full paper contributions, including novel ideas to detect emotional as- pects from eye-tracking data. Eye-tracking data is also used by several other contributions to detect behavior patterns. Such patterns are used to estimate confidence or the level of expertise.  Predictive analytics. Prediction of learner performance and modeling of learners have been researched extensively by the educational data mining, educational user modeling and educational adaptive hypermedia communities. The objective is to estimate the unknown value of a variable that describes the learner, such as performance, knowledge, scores or learner grades [6]. Predictive analytics that exam- ine extraction of learning indicators are researched by two contributionsamong others to learn from Facebook data and to predict at-risk students.  Sequence analysis. The temporal sequencing of activity contains useful information about learners. While concerns with sequentiality also surface elsewhere (e.g., in communi- cation and collaboration, and discourse analytics), two pa- pers are gathered here for their explicit focus on sequential structure of behavior. They do so at two very different gran- ularities: microgenetic sequence analysis of the development of student understanding of fractions in a game environment, and sequences of course enrollment and completion as a stu- dent works through a self-designed program of study.  MOOCs. The proceedings shifts towards an applications focus, beginning with learning analytics as applied to Mas- sive Open Online Courses (MOOCs) in which large num- bers of people participate with an open enrollment model. Although enrollments can be very high, completion rates are lower than in traditional courses, giving urgency to re- search on motivations for participation and understanding how learners may obtain value from a MOOC under differ- ent participation trajectories, even if they do not formally complete the course.  3    Assessment. The objective of 4 contributions is to use data that is tracked from the actual use of learning environ- ments to support assessment processes. The papers address the generalizability of data-mining assessment models, and assessments based on rubrics in an LMS, on student inter- action with online lectures, and on models of the cognitive processes involved in a learning task.  Supporting Teachers. Assessment is a natural lead-in to work that is explicitly concerned with support for educators. Three contributions focus specifically on teacher support among others by visualizing data collected in real applica- tions to monitor and direct content development. Work con- ducted in collaboration with teachers has great potential for creative synergies in the middle space.  Challenges with respect to scalable learning analytics, ethics and institutional policies for using student data, and collec- tion and sharing of datasets for research purposes are dis- cussed by three contributions.  Analytic Architectures. Two papers offering architectures for managing learning analytics data address some of the above challenges with frameworks for organizing the ana- lytic enterprise that support analysis across micro and macro levels of activity, and across the various tools used in a col- laborative learning environment.  Design Briefings. The final set of papers explore analytics in the context of specific system designs, including how ana- lytics can support gamification of learning management sys- tems, a system that analyzes classroom video in real time to automatically assess levels of student attention, and exami- nation of three ways in which learning analytics can support orchestrate community inquiry in the SAIL environment.  Panels and Workshops. The proceedings ends with ab- stracts for panels and workshops to be held at the con- ference. Panels examine institutional and individual en- gagement with learning analytics; while workshops explore new discourse analytic approaches, and the specific analytic challenges offered by video-based learning, learning resource repositories, and teaching as objects of study.  Thus we have in this proceedings a rich collection of ana- lytic methods, and of learning settings and educational prac- tices that the methods might inform. We are well poised for discussion that keeps learning and analytics simultaneously in focus. By being explicit about our assumptions and lever- aging multiple forms of boundary objects, we can embark on productively multivocal discourse that leverages the diver- sity of the Learning Analytics and Knowledge community.  5. ACKNOWLEDGMENTS We thank Erik Duval and Xavier Ochoa for their support  and discussion of the conference theme, and the reviewers for their help in providing high quality reviews of submit- ted contributions. Discussion with Janet Kolodner also in- fluenced the middle space theme. Katrien Verbert is a post-doctoral fellow of the Research Foundation - Flanders (FWO).  6. REFERENCES [1] E. Duval, J. Klerkx, K. Verbert, T. Nagel, S. Govaerts,  G. A. Parra Chico, J. L. Santos Odriozola, and B. Vandeputte. Learning dashboards & learnscapes. In Workshop at CHI2012: ACM SIGCHI Conference on Human Factors in Computing Systems, May 2012.  [2] E. Duval and K. Verbert. Learning analytics. eleed, 8, 2012.  [3] R. Ferguson and S. B. Shum. Social learning analytics: five approaches. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge, LAK 12, pages 2333, New York, NY, USA, 2012. ACM.  [4] J. Heer and D. Boyd. Vizster: Visualizing online social networks. In Proceedings of the Proceedings of the 2005 IEEE Symposium on Information Visualization, INFOVIS 05, pages 5, Washington, DC, USA, 2005. IEEE Computer Society.  [5] T. Koschmann, editor. Theories of Learning and Studies of Instructional Practice. New York: Springer, 2011.  [6] C. Romero and S. Ventura. Educational data mining: A survey from 1995 to 2005. Expert Syst. Appl., 33(1):135146, July 2007.  [7] G. Siemens. Learning analytics: envisioning a research discipline and a domain of practice. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge, LAK 12, pages 48, New York, NY, USA, 2012. ACM.  [8] S. L. Star and J. R. Griesemer. Institutional ecology, translations and boundary objects: Amateurs and professionals in berkeleys museum of vertebrate zoology, 1907-39. Social Studies Of Science, 19(3):387420, 1989.  [9] D. D. Suthers. Technology affordances for intersubjective meaning making: A research agenda for CSCL. The International Journal of Computer-Supported Collaborative Learning, 1(3):315337, 2006.  [10] D. D. Suthers, K. Lund, C. P. Rose, G. Dyke, N. Law, C. Teplovs, W. Chen, M.M. Chiu, H. Jeong, C-K. Looi, R. Medina, J. Oshima, K. Sawyer, H. Shirouzu, J-W. Strijbos, S. Trausan-Matu, J. van Aalst. Towards productive multivocality in the analysis of collaborative learning. In Connecting Computer-Supported Collaborative Learning to Policy and Practice: Proceedings of the 9th International Conference on Computer-Supported Collaborative Learning (CSCL 2011) (Vol. III), pages 10151022. Hong Kong: International Society of the Learning Sciences, 2011.  [11] K. Verbert, N. Manouselis, H. Drachsler, and E. Duval. Dataset-driven research to support learning and knowledge analytics. Educational Technology and Society, 15(3):133148, June 2012.  4      "}
{"index":{"_id":"2"}}
{"datatype":"inproceedings","key":"Balacheff:2013:MVM:2460296.2460299","author":"Balacheff, Nicolas and Lund, Kristine","title":"Multidisciplinarity vs. Multivocality, the Case of Learning Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"5--13","numpages":"9","url":"http://doi.acm.org/10.1145/2460296.2460299","doi":"10.1145/2460296.2460299","acmid":"2460299","publisher":"ACM","address":"New York, NY, USA","keywords":"educational data mining, learning analytics, multidiscipinarity, multivocality, technology enhanced learning","abstract":"In this paper, we consider an analysis of the TeLearn archive, of the Grand Challenges from the STELLAR Network of Excellence, of two Alpine Rendez-Vous 2011 workshops and research conducted in the Productive Multivocality initiative in order to discuss the notions of multidisciplinarity, multivocality and interidisciplinarity. We use this discussion as a springboard for addressing the term Learning Analytics and its relation to Educational Data Mining. Our goal is to launch a debate pertaining to what extent the different disciplines involved in the TEL community can be integrated on methodological and theoretical levels.","pdf":"Multidisciplinarity vs. Multivocality, the case of Learning  Analytics  Nicolas Balacheff  CNRS, LIG Research Lab   University of Grenoble  46 Avenue Flix Viallet 38031   Grenoble  +33 (0) 4 76 57 50 67   Nicolas.Balacheff@imag.fr                  Kristine Lund  CNRS, ICAR Research lab   University of Lyon, ENS Lyon  15 parvis Ren Descartes 69700 Lyon   +33 (0) 4 37 37 63 16  Kristine.Lund@ens-lyon.fr   ABSTRACT  In this paper, we consider an analysis of the TeLearn archive, of  the Grand Challenges from the STELLAR Network of  Excellence, of two Alpine Rendez-Vous 2011 workshops and  research conducted in the Productive Multivocality initiative in  order to discuss the notions of multidisciplinarity, multivocality  and interidisciplinarity. We use this discussion as a springboard  for addressing the term Learning Analytics and its relation to  Educational Data Mining. Our goal is to launch a debate  pertaining to what extent the different disciplines involved in the  TEL community can be integrated on methodological and  theoretical levels.   Categories and Subject Descriptors  H.1.0 [Information Systems]: Models and Principles  General.    General Terms  Measurement, Documentation, Human Factors, Standardization,  Theory   Keywords  Technology Enhanced Learning, Learning Analytics, Educational  Data Mining, Multidiscipinarity, Multivocality   1. THE TEL DICTIONARY PERSPECTIVE  The Technology Enhanced Learning (TEL) research area is by  nature multidisciplinary, constantly importing terms, concepts and  methods from the various disciplines participating in its  development. TEL also coins terms, forges concepts and designs  new methods in order to face the original problems it encounters.  This evolution is so rapid and the motivations are so diverse that  the language used is often not well defined. As a result, it is  difficult to ensure that the wheel is not being reinvented despite  what appears to be new research. In the course of TEL  development, various terms may appear whose difference may be  more related to variations in the communities that coined them  rather than to fundamental differences in the concepts being  considered.  The term Learning Analytics is a good candidate  for exploring these issues and for understanding the conceptual,   linguistic and social stakes of the appearance of such a term and  its associated challenges.   The term Learning Analytics was first used in 2009 by  Bienkowski Mingyu & Means [3] and shortly thereafter, the first  international conference on Learning Analytics was held in May  2011 in Banff (Alberta, Canada). Such a short delay suggests that  the expression was not coined to respond to the need of a specific  research program, but rather to identify the point of convergence  of a community since holding a conference implies the existence  of enough research and results to feed its program. This remark  anticipates a question that we will raise about the nature of the  term Learning Analytics  a term that could have one of two  uses for a community.  Either the community has identified a  particular problem within its boundaries and the term aids in  conceptualizing that problem, or the term was coined and has  demonstrated its efficiency outside of the research community  (e.g. at the frontier of another research community or to a general  societal need) and is imported and plays a key role in defining a  new community around its promises; this phenomena is often  associated to a technology push (e.g. mobile learning). In the  first case, if the community is single-discipline, then it is likely  that researchers will use their tried and true methods to attempt to  conceptualize and solve the problem. If the community is  multidisciplinary, perhaps multiple frameworks and methods will  be applied in parallel. If the term is imported from another  research or practitioner community, there is an opportunity to  integrate different frameworks and methods used towards a  common goal, perhaps achieving a new vision or defining a new  set of research questions.    We argue that the term Learning Analytics is currently  mobilized within a multidisciplinary community of researchers  and in this paper, we explore its origins and the opportunities this  provides.   2. MULTIDISCIPLINARITY,  INTERDISCIPLINARITY AND  MULTIVOCALITY  Three concepts will play a key role in discussing the birth of the  term Learning Analytics and its current status:  multidisciplinarity, interdisciplinarity and multivocality. Van den  Besselaar & Heimeriks [2] argue that if a disciplinary research  field is defined as a group of researchers working on a specific  set of research questions, using the same set of methods and a  shared approach (op. cit., p. 706), then the different forms of so  called non-disciplinary research such as multidisciplinarity,  interdisciplinarity and transdisciplinarity are ways of combining  elements from various disciplines in order to get these disciplines      (c) 2013 Association for Computing Machinery. ACM acknowledges that this  contribution was authored or co-authored by an employee, contractor or affiliate of  the national government of France. As such, the government of France retains a  nonexclusive, royalty-free right to publish or reproduce this article, or to allow  others to do so, for Government purposes only.   LAK '13, April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00   5    to productively interact.  Such interactions between disciplines  can take many forms, ranging from communicating and  comparing ideas, exchanging data, methods and procedures to  mutually integrating concepts, theories, methodologies and  epistemological principles (op. cit., 2001).   The claim here is that the indicator that distinguishes between the  forms of non-disciplinary research is the level of integration of the  disciplinary approaches that they are based upon. In van den  Besselaar and Heimeriks view, neither theoretical perspectives  nor actual results from different participating disciplines are  integrated during multidisciplinarity. Rather, the subject under  study is approached from different angles, using different  disciplinary perspectives (op. cit., p. 706). Choi & Pak [5] hold a  similar view, arguing that multidisciplinarity draws on knowledge  from different disciplines, but each researcher group stays within  its own boundaries. On the other hand, interdisciplinary research  integrates contributing disciplines by creating its own theoretical,  conceptual and methodological identity [2] or in other words,  analyzes, synthesizes and harmonizes links between disciplines  into a coordinated and coherent whole ([5] p. 351).  We agree with Leeds-Hurwitz [9], that disciplines are social  constructions and that in order to carry out ones own research  lucidly, it is necessary to be aware of disciplinary history, cognate  disciplines, international variations and rival subdisciplines. The  second author of this article and colleagues took up goals similar  to those of Leeds-Hurwitz during a series of international  workshops that involved sharing five corpora of group  interactions and performing multiple analyses from different  epistemological and methodological frameworks on each of them.  We aimed for what we termed Productive Multivocality [10].  Multivocality makes reference to the different analytical voices  that gathered around a particular corpus and those voices became  productive when progress was made towards refining analytical  concepts, rendering explicit epistemological positions, and in  general characterizing under what conditions learning occurs in a  set of corpora taken as being representative of the types of corpora  studied in the Computer Supported Collaborative Learning  (CSCL) community. These workshops were carried out during the  following international conferences within a group of people  interested in CSCL: International Conference on the Learning  Sciences (ICLS 2008), Computer Supported Collaborative  Learning (CSCL 2009), the STELLAR Alpine Rendez-Vous  (ARV 2009), ICLS 2010 and ARV 2011.   We argue that multivocality is closer to interdisciplinarity than to  multidisciplinarity. In order to illustrate this, let us take a closer  look at the nature of the CSCL community in general. This closer  look is intended to set the stage for reporting on the beginnings of  a similar analysis of the communities laying stake to Learning  Analytics. Our underlying assumption is that multivocality and  interdisciplinarity are approaches that move research fields  forward. We argue that the communities researching Learning  Analytics are nicely positioned to benefit from such approaches,  much in the same way that CSCL has been.  In a study [17] that obtained answers from 15 out of 28  researchers who were representative of geographical locations and  of levels of participation in CSCL from 1995 to 2005, the authors  found that respondents worked from multiple disciplines: 15  people responded working from 21 disciplines (5 from Computer  and/or Information Science, 4 from Education, 4 from Psychology  (including 2 from Educational Psychology), 1 from Conversation  Analysis and 1 from Knowledge Building. In another study that  questioned reviewers from the 2007 CSCL conference, the   disciplines mostly cited were Computer Science, Psychology,  Educational Sciences and the Learning Sciences. However, a large  variety of other disciplines were cited and thirty four disciplines  were each cited by only one person. The granularity of these  disciplines varied greatly going from large grain (i.e. Linguistics,  Artificial Intelligence, Economics) to small grain (i.e. Curriculum  and Methods of Teaching Arabic or Educational Measurement  and Statistics). The field of CSCL is clearly multidisciplinary:  different disciplinary perspectives are brought to bear upon the  object of computer supported collaborative learning.   But is the CSCL community interdisciplinary or has a small part  of the community made progress in that direction As we have  stated elsewhere [15] it is not possible nor desirable to attempt  complete integration as the CSCL and larger Learning Sciences  community are too diverse, both theoretically and  methodologically. Indeed, diversity is one of the strengths that we  wish to maintain since dialogues about analytical constructs  between researchers that differ in their ontology and epistemology  are particularly enlightening [1]. Instead of attempting to merge  theoretical perspectives into a kind of super theory, we chose a  set of boundary objects [14] to form the bridges between  disciplines and create dialogue with the goal of better  understanding how learning takes place in different contexts. An  example of how the boundary objects we chose functioned as a  bridge between disciplines will be presented in section 4.  Indeed, as the Productive Multivocality initiative culminates in  publication of a book with Springer, expected in 2013, we claim  to have achieved a certain level of integration between disciplines.  We succeeded in that by sharing data with the supposition that  this same data will serve the purpose of each different analysts  problmatique1. If on the basis of this shared analytical object, we  can claim an ontological convergence between the researchers  who analyze the same corpus, we cannot claim a theoretical  convergence, nor a methodological one. Multivocality is quite  specific in that multiple voices are expected to identify key  phenomena, but each voice will identify potentially different  phenomena or if they are the same, they will specify them and  explain them in different ways, while relying on different  assumptions. It is only when these ways of explaining become  explicit and are compared to other ways, with the shared  analytical object serving as a boundary object, is it possible to  approach either methodological convergence or theoretical  convergence and thus interdisciplinarity.   This discussion of the extent to which multivocality may tend  toward interdisciplinarity in the context of CSCL sets the stage for  our analysis of the case of Learning Analytics, although we  have not yet the same kind of evidence.   In this communication we will first explore the lexical  environment of Learning Analytics, then we will discuss its  scientific position in relation to the effort to define it, its  disciplinary foundations and their potential interactions. Finally  we will open the discussion concerning the scientific stakes of the  emerging Learning Analytics community and its potential role in  the TEL research area, at the crossroads of multivocality and  multidisciplinarity.                                                                        1 Problmatique is a French word used to name a coherent set of   problems and assumptions. It provides a coherent framework to  express problems, why it is interesting to solve them and how  the current research described is able to do so. This term is not a  synonym for the English word problematic.   6    3. LEARNING ANALYTICS AND  RELATED TERMS AND CONCEPTS   From 2004 onwards, the TEL research community has made  an effort to create a shared thematic open archive, TeLearn2,  where one can find a large number of articles and reports. In 2010,  the first author did an analysis of the key terms and dominant  expressions in this repository and it so happens that Learning  Analytics was absent. This could mean that the community  engaged in this area had not yet committed itself to contributing to  the archive, but certainly we could at least conclude that its  potential impact was not visible at that time. This is not surprising  given its recent emergence. However, a search with the expression  Educational Data Mining provided some results, as well as one  with the expression learning trails. This suggests a potential of  interest for Learning Analytics, indeed.    A more recent exploration of the outcomes of the 2011 Stellar  Alpine Rendez-vous (ARV) and of the Grand Challenge problems  as proposed by the Stellar network of excellence is more positive.  It shows the appearance of Learning Analytics as a key word in  the TEL research community and suggests relations to some other  expressions. Figure 1 shows the clouds of keywords3, organized  according to their proximity in the texts of the Stellar Grand  Challenge problems.     Figure 1.  Keywords, organized by proximity in the texts of  the Stellar Grand Challenge problems, and their relation to  Learning Analytics (only partial links are shown to help   readability). All links are available online4. Colors represent  the clusters centered on the most important keywords.                                                                        2 http://telearn.archives-ouvertes.fr/en  3 This original map accounts for the co-occurrences present in this   corpus. To build the map, we selected words that frequently  appear in the same segment (the threshold is 50 words before -  50 words after the chosen word). If two words appear close to  one another, they are linked in the map. Generic words  (learning, technology) were deleted for this analysis as they  brought too much noise. We present here a simplified version of  the map so that it remains readable despite the limited space in  this text.    4 http://www.tel-thesaurus.net/maps/contexteGCP/GexfWalker/   Surprisingly, data and Learning Analytics have no direct  connections in this particular representation, In order to unpack  this lack of direct connections, we can look more closely at the  structure of the graph; it shows that Learning Analytics is  massively connected to analytics and engagement (cf. Figure  2). Analytics is connected to games, feedback, predictive  models, teachers, data mining and learners. The direct  neighborhood of data is made up of 15 terms and expressions  (cf. Figure 3): tools, competence, privacy, data driven  tools, data protection, datasets, educational data,  educational providers, information, infrastructure, learning   resources, learning activity, outcomes, policy,  education.    Figure 2.   A close-up of the connections with Learning  Analytics from Figure 1.     Figure 3. The terms closest to data in the Stellar Grand   Challenges  As shown in Figure 1, the shortest path from analytics to data  is via learners and tools. While this makes sense without  being very specific, it does not allow for differentiating between   7    Learning Analytics and learning trails and Educational Data  Mining, two expressions that are often associated to data in TEL  Research. Therefore, we looked for another similar corpus that  could help us differentiate between Learning Analytics,  learning trails and Educational Data Mining. We decided to  apply the same clouds of keywords methodology on all the white  papers produced by the ARV 2011 workshops. Perhaps the small  groups of close colleagues working on specific TEL issues could  help us differentiate the expressions above.   When looking at the graph constructed on these white papers with  the same methodology from the data extracted from the final texts  of the Stellar Grand Challenge problems, one can observe that  analytics has no connection to data or to dataset, arguably the  two semantically closest keywords (cf. Figure 4.).      Figure 4. The terms closest to analytics in the white papers.  It appears very likely that this lack of connections between data  and Learning Analytics or just to analytics would be due to a  lack of connections between the research communities involved.  The final work on the formulation on the Grand Challenge  Problems was done in a larger context although it began with the  white papers written by the ARV 2011 workshops so it could  perhaps be understood that connections between data and  Learning Analytics could be lost in that many researchers in the  STELLAR network of excellence from different fields  participated. But how is this also the case in the small groups of  close researchers working together on specific TEL subjects We  therefore decided to look more closely at two workshops focusing  heavily on data: (ws8: Productive Multivocality and ws6:  DataTEL). We will consider this issue more precisely in the next  paragraphs.   We might wonder if the two workshops having data at the locus  of their work are part of the same community. To what extent are  they working with data in similar ways Do they use similar  vocabulary and have similar goals If we analyze the white papers  they provided after their workshops were over, we see that data is  of different granularity and specific goals and vocabulary are  different, although both workshops are interested in the high level  goal of better understanding learning. Here, we compare the  answers given by the two workshops to two questions involving   defining Grand Challenge Problems for the future. The first  question is What problems of the European education system are  addressed, and what are the long term benefits for society and the  second is What are the main activities to address this Grand  Challenge Problem   The first workshop Productive Multivocality answered the first  question insisting on the variety of the approaches, on the nature  of corpora and on empirical evidence: In order to make use of  this variety of research data across the different research groups  these relevant data sets should be shared and made accessible.  The data sets and related analyses could serve as boundary  objects and stimulate fruitful discussion across the different  research approaches. This would not just show the multivocality  in CSCL research, but could also serve as a means for converging  evidence about the potentials and effectiveness of TEL and CSCL.  This allows not just an overview about the effectiveness of CSCL  in teaching and learning for researchers and the scientific  community, but also for stakeholders and practitioners.    The main activities needed to address this Grand Challenge  Problem include the development of a technical infrastructure for  supporting open data sharing and exchange of results and lessons  learned among researchers, practitioners and stakeholders. The  Productive Multivocality workshop also suggests the  implementation and formative evaluation of a supportive  structure for a dialogical interpretation of the data in order to  make the community and stakeholders aware what results  converge among the different data sets and different  interpretations and in order to identify open questions.  On the other hand, the DataTEL workshop answers the first  question by claiming that The research on TEL recommender  systems can contribute to decrease the drop-out rate [in  education] by disseminating its research outcomes for the  development of different support systems for teachers and students  to offer relevant information at the right time.  The main activities needed to address this Grand Challenge  Problem suggested by the DataTEL workshop are: customize  existing recommendation algorithms for learning, employ  recommender systems in real-life scenarios and develop suitable  evaluation criteria for different kinds of recommender systems.  In comparing how these two workshops chose to define their  Grand Challenge Problem, we note that although data is at the  heart of both of their proposals, there is almost no shared  vocabulary, apart from perhaps cognates of learning. In  addition, there is a great difference in terms of scope in the two  workshops objectives. The Productive Multivocality workshop  aims to first produce an infrastructure for sharing data and second  to elicit reasons for researchers from different disciplines to  analyze this data. The DataTEL workshop centers on a specific  problem that they aim to remedy  that of high drop-out rates in  on-line education.  Of course, it is hardly surprising that two  different teams focus on different aspects of learning by analyzing  data and that one focuses on solving a problem for practitioners  (DataTEL) and the other on a problem designed to integrate  research results from different communities (Productive  Multivocality). It is clearly not enough for two research  communities to hold an interest for data and to analyze data in  order to better understand learning in order for them to share  problmatiques. However, as we will show in section 4 with the  Productive Multivocality initiative, it is also not enough to share  the same data and to analyze it to look for phenomena illustrating  learning for researchers to share problmatiques.  Our intention in   8    this article is to use analyses of these other multidisciplinary  contexts (i.e. TeLearn archive, STELLAR Grand Challenges, two  Alpine Rendez-Vous 2011 workshops and Productive  Multivocality) to show the potential for developing a mindset  where theoretical and methodological approaches in Learning  Analytics and Educational Data Mining are compared and  contrasted with the goal of understanding each others  problmatiques.    A first attempt at analyzing vocabulary used in the communities  that are implicated in TEL was undertaken during the fall of 2011.  This analysis is a precursor to understanding to what extent  problmatiques are already partially shared between participating  disciplines and to pinpoint areas where perhaps existing tensions  can be productive. A discussion was launched in the TEL  Dictionary LinkedIn Group with the question of whether there  was a specific concept underpinning Learning Analytics or  whether it was a new flag in the TEL research community used as  a shared sign of recognition among those researchers interested in  exploring the benefit from importing an analytics problmatique  into TEL Research. This discussion didnt catch much attention,  but left the idea that there was no clear view of how Learning  Analytics could be differentiated from Educational Data  Mining.    In a recent effort to define Learning Analytics, a seminal paper  by Long and Siemens [10] refers to the definition proposed by the  1st LAK conference:   Learning analytics is the measurement, collection, analysis and  reporting of data about learners and their contexts, for purposes  of understanding and optimising learning and the environments in  which it occurs  (ibid. p. 34).   Let us compare it with the definition of Educational data mining  prepared by Michel C. Desmarais and Ryan S.J.D. Baker for the  TEL Dictionary:   Educational Data Mining is a term used for processes designed  for the analysis of data from educational settings to better  understand students and the settings which they learn in. 5   The distance between both definitions is not obvious for most  TEL researchers. There is a general understanding that in order to  design better adapted environments, one must be able to develop  models and techniques to gather and to analyze data in order to  make relevant pedagogical decisions and to provide feedback  fulfilling learners personal needs. Indeed, a recent US report  expresses this shared concern: Two areas that are specific to the  use of big data in education are educational data mining and  learning analytics ([3] p. 8, authors emphasis).  The report  points out an interesting difference which at first glance might  appear as a difference of strategy:  Unlike educational data  mining, learning analytics does not generally address the  development of new computational methods for data analysis but  instead addresses the application of known methods and models  to answer important questions that affect student learning and  organizational learning systems  [3].    We chose to use the classic method of exploring the origin and the  related contexts of Learning Analytics and Educational Data  Mining in order to better understand the possible difference  between these two expressions. This is the core methodology of                                                                       5 http://www.tel-thesaurus.net/wiki/index.php/Educational_data_   mining   the construction of the TEL Dictionary; we question the history of  the terms used by the involved research communities and trace the  circumstances of their origin. From this perspective the two  expressions have different profiles.   In the case of Educational Data Mining, the birth of the  expression and the respective community has a rather long  history. The first workshops on data mining and learning [3] were  held in the context of classic TEL conferences, namely AIED  (2007, 2005), EC-TEL (2007), ICALT (2007), UM (2007), AAAI  (2006, 2005), and ITS (2006, 2004, 2000). In 2008, the first  conference specifically on Educational Data Mining was held in  in Montral. One will notice that Educational Data Mining has  naturally taken its place within the flow of the history of data use  in TEL research.     In the case of Learning analytics, the origin is clearly  exogenous. It has been inspired by the universe of business,  characterized by data-driven decision-making and by business  intelligence [3]. The objective is to take the tools and techniques  which have already proven their efficiency in business and import  them into TEL research so as to provide a new model for college  and university leaders to improve teaching, learning,  organizational efficiency, and decision making and, as a  consequence, serve as a foundation for systemic change.  ([3], p.  32).    Learning analytics and Educational data mining are products of  completely different processes and different problmatiques. On  this we might have a different analysis than Siemens and Baker  [13]. These authors emphasize the focus of Educational Data  Mining research on automatic processes, and that of Learning  Analytics of providing information to stakeholders. Actually,  designing open learner models is perfectly in line with the  objectives of Educational Data Mining, and visualizing data after  a relevant treatment is also an objective related to providing tools  to support teachers or trainers. In both cases, learning as both a  process and an epistemic outcome is targeted. On the other hand,  Learning Analytics develops automatic modeling of massive data  in order to provide relevant visualizations. But the key difference  may not be located there. More important and significant, is the  substantial difference of the epistemological grounds of both  expressions so that potential conflicts are less justified by their  conceptual raison dtre than by the difficulty in situating the  respective communities within a larger research landscape. (Big)  data is a shared field of action and confrontation of the two  approaches could most likely be played out there. Within this aim,  we submit the following questions to the communities involved:   - Are the Learning Analytics tools, techniques and  strategies imported from the more general field of analytics  sufficient for relevantly analyzing learning data   - Should all data attached to the activities of a student be  considered as learning data (As opposed to strategies which  are not related to learning but to the social management of the  relations of the learner within his or her referent institution)   - Isnt Learning Analytics reducing successful learning to  the academic success of students in their institutions, limiting  de facto the problmatique of TEL research   - Compared to the classical problmatique of learner  modeling in AI, that of learning trail analysis and  Educational Data Mining, what are the specific  contributions of Learning Analytics Or is this comparison   9    irrelevant (in the case where the problmatique is actually  totally different).   Possibly for the first time in the TEL research area, the issue is not  to address problems raised by multidisciplinarity, but those of  multivocality and interdisciplinarity: that of the relations between  different research communities sharing the same sources of  evidence but not focusing on the same phenomenological  observations, or if so, interpreting them in different frameworks.   In the following section, we use the Productive Multivocality  context to illustrate how problmatiques can converge in some  ways between researchers who do not initially share an approach,  either theoretically or methodologically. Our intent is to argue that  this is an example of the kinds of opportunities available to  Learning Analytics because it is  like CSCL  situated in a  community made up of different disciplines.   4. HOW MULTIVOCALITY CAN TEND  TOWARDS INTERDISCIPLINARITY  In this section we present a selection of results we obtained from  sharing a corpus of Japanese 6th grade fractions within the  Productive Multivocality initiative [15]. The results will show the  difficulties involved in obtaining either methodological or  theoretical convergence, but also specific instances of such  convergence.  Three researchers (Hajime Shirouzu, Ming Ming  Chiu and Stefan Trausan-Matu) analyzed the corpus from their  individual habitual perspectives. The first boundary object was  thus the shared corpus itself, but we also defined a second  boundary object.  We asked the researchers to take pivotal  moments into account during their analysis. At the start, such  moments were not defined in a formal way, but there was a  general and informal consensus that in the course of the learning  process in a collaborative context, these moments correspond  either to a rupture or to qualitative change. A pivotal moment is  seen as a boundary object in the sense that its general and intuitive  definition was understood, but it could be operationalized in  different ways, depending on the researchers problmatique. So,  whereas researchers agreed that learning was a temporal and  dynamic process that corresponds to a change in state, depending  on their problmatique, they would characterize these states in  different ways. They would also characterize the conditions for  change in different ways. Note that a moment can also be an  episode because the temporal granularity changes with the  perspective taken. In other words, the unit of analysis is  determined by the theoretical and methodological framework  employed by the researcher. Viewed in this way, the pivotal  moment becomes a lens with which one can analyze the  divergence or the convergence between the analysts approaches.  We asked the researchers to look for pivotal moments within a  shared corpus provided by one of them. In the next three sections,  we first give an extract from the corpus and an example of a  particular moment that was designated as pivotal by each  researcher. Second, we show how different voices can interpret  the shared data without any type of convergence. Finally, we give  an example where methodological and/or theoretical convergence  occurs, thus tending towards interdisciplinarity.   4.1 An example of a pivotal moment  designated by each researcher  In the figure below, we show a stretch of talk in which the three  researchers who analyzed the corpus of fractions in a Japanese 6th  grade classroom, each determine that a particular moment (of   slightly different temporal lengths) is pivotal, but for different  reasons that are linked to their problmatiques.   The corpus that was recorded involved six Japanese 6th grade  children studying fractions in one classroom session where the  question the teacher asked was  Can you cut  of 2/3 of a piece  of origami paper [12]. After the children had all folded the  origami paper in different ways, in order to attempt the answer the  question, they were asked the question Are the answers the  same. It is during this questioning that the extract in Table 1  occurs.  Table 1. A stretch of talk in which the three researchers each   found a pivotal moment; each text type (i.e. bold, italic,  underlined) corresponds to a researchers definition of pivotal   moment.  470 Y [Moves toward the teachers desk by further   raising his hip]  471 Anon [Whispers] The shapes differ  472 Y Differ. [with clear voice]  473 Y Though areas are the same [with low voice]  474 G The areas are the same  475 T yes  476 G but the shapes and production methods differ.    In this short extract, we first see that student Y moves toward the  teachers desk by raising his hip. The video at this point in time  shows that student Y is the last student out of six to enter into the  learning space (i.e. to pay attention to the origami sheets being  folded and discussed and to what is being written on the  chalkboard). Looking for differential inter-animation patterns  within a Bakhtinian perspective [16] (his pivotal moment is in  bold in Figure 3), Trausan-Matu analyzes Ys behavior as being  characteristic of a divergent thinker and it is for him a pivotal  moment in and of itself. In addition, apart from the anonymous  student at line 471, it is student Y who clearly announces that the  shapes differ and this is the beginning of the pivotal moment that  continues to line 476. At line 474, student G says the areas of the  shapes are the same, but that the shapes and production methods  differ (line 476). This comparison of characteristics of the origami  paper, some of which are the same while others are different  present an opportunity for the students to learn (i.e. change their  thinking about the relationship between area and shape).   Shirouzu was looking in part for moments that helped him  develop his own theory of focus-based constructive interaction,  based on Miyake [11]. His questions concerned why students  chose to focus on a certain aspect of their folding activity (e.g.  shapes or production methods) during the interaction and where  the interaction went as a result.  In the extract in Figure 3,  Shirouzu defines a pivotal moment at lines 472-474 (in italics),  where he considers that one student focuses on the shapes  differing whereas the other focuses on the fact that the areas are  the same. Shirouzu recognizes that the students are speaking about  the different characteristics of the origami paper, but although he  is asking if there is collective understanding about the relationship  between area and shape), he is more focused on how the  individual foci came about and how they were now going to  influence where the interaction would go.  Chius pivotal moments were breakpoints that illustrated a  change in the quality of the interaction. He used his own method   10    called Statistical Discourse Analysis and analyzed how  characteristics of recent turns of talk such as questions and  evaluations are linked to characteristics of subsequent turns of  talk, such as correct ideas, new ideas or justifications. For this  extract, he identified a pivotal moment at line 476 (underlined),  after which there was a sharp increase in new ideas.   Shirouzus and Trausan-Matus pivotal moments are sequences of  conversation turns whereas Chius method restricts his pivotal  moments to one conversation turn (requiring further qualitative  analysis to identify a pivotal moments boundaries). Whereas  Shirouzus and Trausan-Matus pivotal moments focus on  conceptual thinking, Chius pivotal moments cover the entire  classroom interaction. Chius premise is that when students are  asked to solve a new problem, they try to create new ideas  (termed micro-creativity) and they assess their utility via  explanation or justifications [4]. One principal question is how  classroom processes affect new ideas and justifications and  whether their effects differ across time. Pinpointing students  justifications allows the researcher to locate where students may  be arguing about the relationship between area and shape and thus  potentially changing their thinking.     4.2 Multivocality without convergence  In this section we show how the comparison of Shirouzus, and  Chius pivotal moments lead to progress in each others  problmatiques, but not to integrating on either a theoretical or  methodological level. However, the following discussion  illustrates how one researchers methods can be mobilized, once  another researchers goals are understood, even if the method is  not subsequently appropriated by the second researcher.   Chiu performed new analyses focused on the class discussion  activity phase of the pedagogical task after understanding that  Shirouzu had a special interest in it. Shirouzu demonstrated that  he was able to match new meanings to Chius interpretations of  pivotal moments (occurring in Chius framework) that were  relevant to him in his own framework. For example, Shirouzu saw  his first pivotal moment as a collective display of new  understanding whereas Chiu viewed it as indicating the end of a  period of frequent ideas, occurring just after teacher  acknowledgment. Indeed it is compatible that the moment when  collective understanding is reached could correspond to the  beginning of a drop in new ideas because learners are  consolidating their knowledge in terms of concepts already  expressed. Re-examining this moment in terms of Chius  definition of ideas as new or old led Shirouzu to suggest that  in his own framework, new ideas could correspond to conceptual  or procedural changes of how to view the solutions, progressing  potentially towards a collaborative pivotal moment. Shirouzu also  noticed that Chius five breakpoints corresponding to frequency of  new ideas also corresponded to when and how the pedagogical  designers intentions were actualized by students behavior.   The previous discussion shows that when researchers compare the  moments that they consider pivotal for learning, and one of them  (Shirouzu) discovers that a pivotal moment he did not consider  initially as pivotal is considered as being pivotal in another  researchers framework (Chius), he is capable of finding a reason  why those moments could also be meaningful in his own  framework. In this case, neither methodological nor theoretical  convergence is achieved, but a discussion has begun.   4.3 Multivocality with convergence  In this section we show how the comparison of Shirouzus and  Trausan-Matus pivotal moments lead to progress in Trausan- Matus own problmatique, but also to integrating both  reearchers approaches on a methodological level.   Trausan-Matu has used a semi-automatic content-based analysis  system PolyCAFe (Polyphonic Conversation Analysis and  Feedback generation) for the analysis of chat logs taken from  collaborative learning sessions (Trausan-Matu and Rebedea,  2010). The Productive Multivocality collaboration introduced him  to the analysis of transcribed oral conversations with both talk and  gesture, a type of corpus he had not focused on before. Adding  gesture to his analysis of human interaction amounts to extending  the domain of application of his tool but more importantly to  extending the concept of Bakhtins voices to include gestures.  We interpret this re-conceptualization of voices to mean that  when Trausan-Matu was confronted with a corpus that presented  forms of interaction he was not used to analyzing (i.e. gestures),  he was able to re-consider the types of data he took into account  as important for understanding learning and to integrate them  into his theoretical and methodological framework. This change  in conceptualization illustrates how closely related our theoretical  frameworks are to the nature of the data we analyze. We argue  that convergence occurs here in that Trausan-Matu widened  Bakhtins framework in order to take into account new types of  corpora and by doing so, came closer to Shirouzus framework.    Our final example illustrates the possibility of further  methodological and theoretical integration (although this has not  yet occurred). We could imagine Shirouzu using Trausan-Matus  tool in order to locate moments of personal foci (e.g. shape, area  or production methods) since if the tool can locate inter-animation  patterns and if differential patterns of concepts (i.e. shape versus  area) can be considered as being opposed in some way, they are  also differences in focus. There is thus possibility for  methodological integration between Shirouzus and Trausan- Matus problmatiques. In addition, on a theoretical level,  conceptual differential patterns and personal foci (i.e. those  having to do with, for example shape and area and not just  sequences of yes, no, yes, no) can be considered as converging  analytical concepts.   In the next and final section, we take this discussion of  multivocality that tends towards divergence or that tends towards  convergence (and therefore towards interdisciplinarity in the latter  case) and we transfer it to another context, that of using Learning  Analytics as a boundary object whose study may enhance  collaboration between technology and education.   5.  DATA AS A BOUNDARY OBJECT  FOR LEARNING ANALYTICS AND  EDUCATIONAL DATA MINING  As an expression, Learning analytics can be understood in a  radical way as the concatenation of learning and analytics, the  former indicating an objective and the latter a means: using  analytics to improve learning. It is an interesting case since in  most cases the technology push comes from the hard(ware) side  but this time, it comes from the soft(ware) side. The reason why  this borrowing of methods and approaches from outside of TEL  research is considered promising is because their efficiency has  been acknowledged in other areas that potentially share with TEL  research only the emergence of Big Data about users. But the type  of data is not clear and the use of such a generic word induces a  spurious consensus. Indeed, what is there in common between the   11    learning trail, tracks, or traces of a learner or a group of learners  struggling with learning the multiplication of ratios, and the log  file of students using a LMS for a long period in different  disciplines and for different purposes Or, what is shared by a  teacher having to manage a lesson on ratio and proportion, and by  the dean of the university having to ensure the success of the  freshman class Indeed, there is an objective shared goal at stake:  the success of the learners, but the data needed to answer these  questions is very different. In addition, the problems raised are of  a rather different nature and the meaning of learning in both  cases is significantly different. This raises questions whose  responses do not rest in the technology of analytics, but in the  capacity of researchers from different origins and with different  problmatiques to cooperate constructively, like the researchers in  our Productive Multivocality examples. Siemens and Baker [13]  invite researchers from Educational data mining and Learning  analytics to cooperate following an analysis of both research  approaches. Such an invitation deserves a serious discussion for  which we do not have the space in this short communication. For  the purposes of this article, Educational data mining is claimed to  be limited to the search for automated methods and Learning  Analytics is claimed to be characterized by its objective to provide  resources for human judgment. This is surely a much too rapid  analysis or may likely even be misleading. Learning Analytics  should become an object to be discussed and questioned from the  perspective of the different problmatiques interested in taking up  the challenge of its importation into the Learning Sciences. In  other words, it is time for TEL researchers to go beyond the  soft(ware) push and appropriate a meaning for the tools of  analytics from an educational and learning perspective. The  agenda for making this move will include working on the same  data from different perspectives and confronting the different  understandings of Learning Analytics, without necessarily  choosing any one particular definition as the one that is destined  to become canonical. Finally, this type of exercise will allow  researchers to consider to what extent the different disciplines  involved can be integrated on both methodological and theoretical  levels.    In order to reach this goal, we should first be more specific about  what we mean by data in the contexts where Learning Analytics  and Educational Data Mining are used. Why is data a potential  boundary object for both domains The Productive Multivocality  context illustrated that being exposed to new types of data led to  the extension of a theoretical and methodological framework and  thus to a widened capability of analysis. If Learning Analytics  focuses on simple timestamped online forum data, or on  replayable and synchronized traces of multi-source collaborative  activity [6] or on characteristics of students gleaned from  questionnaires or institutional data, or finally on the informal  ways of communicating (e.g. Facebook, Twitter, blogs) or around  formal content (e.g. Moodle course), it seems reasonable to assert  that the theoretical and methodological frameworks used to  perform Learning Analytics or to do Educational Data Mining in  those cases would be different. In order to understand the reasons  for that, we need also to look at the problmatique of the  researchers involved to evaluate the potential for convergence in  the community. What are their assumptions about data About  learning What specific problems are they trying to solve Why is  it interesting to solve them For example, how can different  approaches such as Social Network Analysis, Discourse Analysis  and Multimodal Analysis  each focusing on a particular aspect  of collected data, and each situated within a particular perspective   inform each other on a theoretical level    The Productive Multivocality context also illustrated that a first  researchers methods used on a second researchers data can lead  to new insights for the second researcher even if (s)he does not  appropriate the research method itself. This is another example of  the strength of sharing data and succeeding in understanding each  others problmatiques and is more straightforward and more  common than the previous example.   Researchers who study group interactions  like those in the  Productive Multivocality context, are similar to researchers who  do Learning Analytics and Educational Data Mining in that they  work in multidisciplinary communities where problmatiques are  different.  We join Siemens and Baker [13] in a call for  cooperation between these two domains with the suggestion of an  analysis of the nature of data and of problmatiques in order to  explore the possibilities and potential advantages of partial  convergence, perhaps in the name of a new theoretical, conceptual  and methodological identity for both Learning Analytics and  Educational Data Mining.   6. ACKNOWLEDGMENTS  We thank the TEL community for contributing to the TEL  dictionary and thesaurus as well as the leaders of the ARV 2011  workshops Productive Multivocality (Carolyn Ros, Kris Lund,  Dan Suthers, Gregory Dyke) and DataTEL (Hendrik Drachsler,  Katrien Verbert, Miguel-Angel, Martin Wolpers, Nikos  Manouselis, Riina Vuorikari and Stefanie Lindstaedt. In addition  we thank the analysts of the Japanese fractions corpus (Hajime  Shirouzu, Stefan Trausan-Matu and Ming Ming Chiu). Our thanks  also go to Emilie Manon and Jrme Zeiliger who carried out the  treatment needed for the corpora of texts we use here.   7. REFERENCES  [1] Abend, G. (2008). The Meaning of Theory. Sociological   Theory. Vol. 26, No. 2, pp. 173-199.   [2] van den Besselaar, P. and Heimeriks, G. 2001., Disciplinary,  Multidisciplinary, Interdisciplinary: Concepts and Indicators.  In M. Davis and C.S. Wilson (Eds), ISSI 2001, 8th  international conference of the Society for Scientometrics  and Informetrics, Sydney: UNSW 2001. pp. 705-716.   [3] Bienkowski, M., Feng, M. and Means, B. (2012). Enhancing  Teaching and Learning Through Educational Data Mining  and Learning Analytics, Department of Educations, Office  of Educational Technology, http://ctl2.sri.com/eframe/wp- content/uploads/2012/04/EDM-LA-Brief- Draft_4_10_12c.pdf PDF (Draft version April 12 2012).    [4] Chiu, M. M. (2008). Effects of argumentation on group  micro-creativity. Contemporary Educational Psychology, 33,  382-402.   [5] Choi, B. C. K. & Pak, A. W. P. 2006. Multidisciplinarity,  interdisciplinarity and transdisciplinarity in health research,  services, education and policy: 1. Definitions, objectives, and  evidence of effectiveness. Clinical and Investigative  Medicine. Medecine Clinique et Experimentale, 29(6), 351- 364.   [6] Dyke, G., Lund, K., & Girardot, J.-J. (2010). Tatiana : un  environnement daide  lanalyse de traces dinteractions  humaines. Technique et Science Informatiques, 29(10),  pp.1179-1205.   12    [7] Jaffe, E. 2009. Crossing Boundaries: The Growing Enterprise  of Interdisciplinary Research. Observer Vol.22, No.5  May/June, 2009.   [8] Kienle, A. & Wessner, M. 2006. The CSCL Community in  its First Decade: Development, Continuity, Connectivity.  International Journal of Computer-Supported Collaborative  Learning (ijCSCL) 1 (1), pp. 9- 33.   [9] Leeds-Hurwitz, W. 2012. These fictions we call disciplines.  The Electronic Journal of Communication / La Revue  Electronic de Communication Volume 22 Numbers 3 & 4,  2012   [10] Long, P. & Siemens, G. (2011). Penetrating the Fog:  Analytics in Learning and Education. EDUCAUSE Review  Online, 46, 5, pp. 31-40.   [11] Miyake, N. (1986). Constructive interaction and the iterative  process of understanding. Cognitive Science, 10, 151-177.   [12] Shirouzu, H., Miyake, N., & Masukawa, H. (2002).  Cognitively active externalization for situated reflection.  Cognitive Science, 26, 469-501.    [13] Siemens G., Ryan S J.d. Baker 2012. Learning Analytics and  Educational Data Mining: Towards Communication and  Collaboration, Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge, Pages  252-254, ACM: New York, NY, USA.   [14] Star, S. L., & Griesemer, J. R. 1989. Institutional Ecology,  'Translations' and Boundary Objects: Amateurs and  Professionals in Berkeley's Museum of Vertebrate Zoology.  Social Studies of Science, 19(3), 387-420.   [15] Suthers, D. D., Lund, K., Rose, C., Dyke, G., Law, N.,  Teplovs, C., Chen, W., Chiu, M., Jeong, H., Looi, C-K.,  Medina, R., Oshima, J., Sawyer, K., Shirouzu, H., Strijbos, J- W., Trausan-Matu, S. & van Aalst, J. (2011). Towards  productive multivocality in the analysis of collaborative  learning. In H. Spada, G. Stahl, N. Miyake, N. Law & K. M.  Cheng (Eds.), Connecting Computer-Supported  Collaborative Learning to Policy and Practice: Proceedings  of the 9th International Conference on Computer-Supported  Collaborative Learning (CSCL 2011), (Vol. III, pp. 1015- 1022). Hong Kong: International Society of the Learning  Sciences.   [16] Trausan-Matu, S., & Rebedea, T. (2009). Polyphonic Inter- Animation of Voices in VMT, in Stahl.G. (Ed.), Studying  Virtual Math Teams (pp. 451 - 473). Boston, MA: Springer  US.   [17] Kienle, A., Wessner, M., (2006). The CSCL Community in  its First Decade: Development, Continuity, Connectivity.   International Journal of Computer-Supported Collaborative  Learning (ijCSCL), Vol. 1, No. 1, pp. 9-33.                  13      "}
{"index":{"_id":"3"}}
{"datatype":"inproceedings","key":"Santos:2013:ALI:2460296.2460301","author":"Santos, Jose Luis and Verbert, Katrien and Govaerts, Sten and Duval, Erik","title":"Addressing Learner Issues with StepUp!: An Evaluation","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"14--22","numpages":"9","url":"http://doi.acm.org/10.1145/2460296.2460301","doi":"10.1145/2460296.2460301","acmid":"2460301","publisher":"ACM","address":"New York, NY, USA","keywords":"design based research, evaluation, learning analytics, reflection, visualization","abstract":"This paper reports on our research on the use of learning analytics dashboards to support awareness, self-reflection, sensemaking and impact for learners. So far, little research has been done to evaluate such dashboards with students and to assess their impact on learning. In this paper, we present the results of an evaluation study of our dashboard, called StepUp!, and the extent to which it addresses issues and needs of our students. Through brainstorming sessions with our students, we identified and prioritized learning issues and needs. In a second step, we deployed StepUp! during one month and we evaluated to which extent our dashboard addresses the issues and needs identified earlier in different courses. The results show that our tool has potentially higher impact for students working in groups and sharing a topic than students working individually on different topics.","pdf":"Addressing learner issues with StepUp!: an Evaluation   Jose Luis Santos, Katrien Verbert, Sten Govaerts, Erik Duval    Dept. of Computer Science, KU Leuven,   Celestijnenlaan 200A   B-3001 Leuven, Belgium  {JoseLuis.Santos,Katrien.Verbert, Sten.Govaerts, Erik.Duval}@cs.kuleuven.be     ABSTRACT  This paper reports on our research on the use of learning analytics  dashboards to support awareness, self-reflection, sensemaking and  impact for learners. So far, little research has been done to  evaluate such dashboards with students and to assess their impact  on learning. In this paper, we present the results of an evaluation  study of our dashboard, called StepUp!, and the extent to which it  addresses issues and needs of our students. Through  brainstorming sessions with our students, we identified and  prioritized learning issues and needs. In a second step, we  deployed StepUp! during one month and we evaluated to which  extent our dashboard addresses the issues and needs identified  earlier in different courses. The results show that our tool has  potentially higher impact for students working in groups and  sharing a topic than students working individually on different  topics.    Categories and Subject Descriptors  H.5.2 [Information interfaces and presentation]: User interfaces;  K.3.2 [Computers and Education]: Computer Science Education   General Terms  Design, Experimentation, Human Factors.   Keywords  Learning analytics, Visualization, Reflection, Evaluation, Design  based research.   1. INTRODUCTION  We consider the essence of learning analytics to be the collection  of traces that learners leave behind and the use of those traces to  improve learning [11]. Educational Data Mining can process the  traces algorithmically and point out patterns or compute indicators  [40][37]. Our interest is more in visualizing traces in order to help  learners and teachers to reflect on their activity. We focus on  building dashboards that visualize the traces in ways that help  learners or teachers to steer the learning process [12].   We focus on deploying real tools in real courses and finding out  how these tools address the learner needs. Students can have  different intrinsic and extrinsic motivations to follow a course and  this will affect the use of the tools. The perception on how the  tools address student needs and the analysis of tool use can help  us understand how we can use these tools to improve learning.   Our courses, in which we apply learning analytics visualizations,  follow an open learning approach where engineering students  work individually or in groups of three or four on realistic project  assignments in an open way. Students use twitter (with course  hash tags), wikis, blogs and other web 2.0 tools such as Toggl1  and TiNYARM[6] to report and communicate about their work  with each other and the outside world in a community of  practice kind of way[15][39].   In earlier work, we presented StepUp![31], a tool that empowers  students to reflect on their own activity, and that of their peers, in  open learning environments. In our courses, we encourage  students to be responsible of their own learning activities, much in  the same way as we expect them to be responsible of their  professional activities later on. To support this process, StepUp!  visualizes different learning traces, such as: time spent on the  course, resource use (e.g. wiki and blog use) and social media use  (e.g. Twitter) (see Section 3.2). Our earlier work focused on the  evaluation of usability and usefulness of StepUp!. In these  evaluations, we asked students to rate the usefulness of StepUp! to  support awareness and reflection and to assess its usability using a  standard SUS [4] questionnaire. This research showed: a)  visualization of time spent on activities related to the course is a  powerful trace to understand peer behavior, and b) StepUp!  provides transparency about how other learners communicate.  Although learners indicate that they should increase or decrease  their activity, most of them did not change their behavior.   Whereas these evaluation studies provided some insight in  potential usefulness and usability issues of StepUp!, we can derive  little evidence from these studies about the impact of StepUp! on  learning. This paper therefore focuses on precisely that topic: the  potential impact with respect to issues and needs that our students  have. To this end, we set up three brainstorming sessions at the  start of the semester with a total of fifty-six students in multiple  courses. In these sessions, students discussed their learning issues  and needs. They prioritized the issues and needs derived from the  brainstorming sessions by rating them. In a second step, we  deployed the tool during one month. Afterwards, we evaluated to  which extent StepUp! addresses the issues identified by the  students at the beginning of the semester.    The remainder of this text is structured as follows: the next  section presents how we performed the brainstorming sessions  and the result of these sessions. How StepUp! addresses the issues  identified in the brainstorming sessions is presented in Section 3.  Section 4 presents the evaluation results. Related work is  discussed in section 5. Conclusions and future work are presented  in Section 6.                                                                     1 https://www.toggl.com/      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.     14    2. LEARNING ISSUES  To identify issues that students face, we carried out three  brainstorming sessions with fifty-six participants in different  courses during the first session of the academic year. We present  details of the courses and results of the brainstorming sessions in  this section. These courses were selected from the engineering  study program and they share the same open learning approach  methodology. This allows us to relate the results to other factors -  such as working individually or in groups and the topic of the  course, rather than to different course contexts.   2.1 Multimedia course  Multimedia (abbreviation MUME) is a course that focuses on the  design and implementation of mobile applications. In this course,  students develop an application in HTML5, Android OS and iOS.  The topic this year is mood tracking in the context of Quantified  Self2. Twenty master students are enrolled in this course. They  work in groups of two or three.    Before introducing the topic of the course to the students, we set  up a brainstorming session in which we asked the students to  identify issues they experience in their studies. The result of the  session was a set of thirteen problems.    In a second step, each group of students rated the problems they  had, by assigning a budget of 10 points across the problems. The  list of issues and their relative importance is presented in the first  block of Table 1.    The following issues were rated highest: 1) detection of group  members who do not work, 2) communication within the group,  3) how I distribute my time, 4) how to take decisions in the group  and 5) group composition. The remaining problems got 6 points  or less whereas the ones mentioned above got between 7 and 15.   2.2 Problem solving and design course  In the problem solving and design course (Dutch abbreviation  PENO), students deal with the different phases of software  engineering, from brainstorming, creation of scenarios and use  cases to programming and evaluation of an application. The topic  of this year is learning analytics. The problem solving and design  course is part of the second year bachelors program. Two groups  of eighteen students are subdivided in teams of six.   Before introducing the topic of the course to the students, we  again set up a brainstorming session per group. After ten minutes,  students presented the problems they had identified. Compared  with the outcome of the brainstorming session in the multimedia  course, nine new issues came up in the session with the first group  of students in this course. In the second group, eight additional  issues were identified. These issues are presented in the second  block and the third block of Table 1, respectively. In addition, we  included four issues that we identified in earlier research[18]  (block 4 in Table 1).   The relative importance of each entry in the combined list of all  issues was rated by the students in a second step. Results are  presented in the PENO column of Table 1.   2.3 Master thesis students  We conducted a third case study with students working on their  master thesis in our group, which focuses on human-computer  interaction. The students typically do a literature study, design and                                                                      2 http://quantifiedself.com   Table 1: learning issues and needs (*final selected issues,  +selected in the use case, -non selected)   Issues MUME PENO THESIS   Usefulness of the assignment - + +   * Group member that does not work   (I1)   + - +   * Communication within the group (I2) + + +   * How I distribute my time (I3) + + +   Agreements within the group + - +   Tracking (progress/effort) - - -   Size of the group - - +   Unclear assignments - + +   Taking decisions - - +   Difference in knowledge between team  members   - - -   Group composition + - +   * Alert if something goes wrong (I5) - - +   Too many projects/exams - - +   Sharing (summaries) among students  - +   * Motivation (I4)  + +   Too much, useless info (thick books)  - +   Concentration (learning), distractions  (facebook)    - +   Health and personal life quality   - +   Time lost due to transport, shopping,  sports,    - -   Problems operating Toledo (schedule)  - -   No pressure during the academic year  - -   Packed days compared with other years  - -   Lack of an effective one stop shop for  info (cf. Blackboard)    + -   Lack of a good learning environment  (light, air, sound,)    + +   Availability of learning material when it  is needed (e.g. for commuting students,  or in the week-end)    + +   Just in time mentoring and help  - -   Not knowing what is important  + +   Not knowing how much time is  required to study a course    - -   Fear of failing  - -   * Lack of balance between social  activities and studying (I6)    + +   Understanding how others members of  my group spend their efforts    - +   Understanding how others peers in the  course spend their efforts    - -   Transparency within the community of  practice. How the members of the  community interact with each other.    - -   * To be aware which resource and tools  I and others students use (I7)    - +   15    evaluate scenarios, use cases, paper and digital prototypes and  release and evaluate a working version of their software. Our  group counts thirteen thesis students who mostly work  individually on their thesis topics.   With this group, we did not perform a brainstorming session, but  we asked them to rate the issues identified by the other students.  Rating results are presented in the last column of Table 1.   2.4 Results  The result of the sessions is a list of thirty-four issues, prioritized  by students in three different courses.    After students rated the issues, we selected the issues that were  rated high in more than one case study and that could potentially  be addressed by StepUp! For instance, motivation is an issue that  we think StepUp! can address, but unclear assignments and lack   of a good learning environment (light, air, sound) are not.  Selected issues and needs are preceded by a star in Table 1.    In Section 5, we present results of an evaluation with students that  assesses to which extent StepUp! addresses these issues.   3. HOW DO WE ADDRESS THE ISSUES  In this section, we describe which traces of learner activities we  track and how such traces can help us to address the learning  issues presented in Section 2. Then, we present our tool and how  these traces are visualized. Finally, we present how we think that  our tool addresses the learning issues listed in table1.    3.1 Tracked data  One of the main challenges with learning analytics is to collect  data that reflect relevant learner and teacher activities [12].   Figure 1 StepUp! interface   16    Some activities are tracked automatically: this is obviously a more  secure and scalable way to collect traces of learning activities.  Much of our work in this area is inspired by quantified self'  applications3, where users often carry sensors, either as apps on  mobile devices, or as specific devices, such as for instance Fitbit4  or Nike Fuel5.  We rely on software trackers that collect relevant traces from the  Web: learners post reports on group blogs, comment on the blogs  of other groups and tweet about activities with a course hash tag.  Those activities are all tracked automatically: we basically process  RSS feeds of the blogs and the blog comments every hour and  collect the relevant information (the identity of the person who  posted the blog post or comment and the timestamp) into a  database with activity traces. Similarly, we use the twitter  Application Programming Interface (API) to retrieve the identity  and timestamp of every tweet with the hash tag of the course.  Moreover, we track learner activities that may or may not produce  a digital outcome with a tool called Toggl: this is basically a time  tracking application that can be configured with a specific set of  activities. Those activities can be classified based on an existing  taxonomy [8] in assimilative (blogging and writing reports),  communicative (twitter and comments) and productive activities  (programming). We expect that this clear division between  different kinds of activities will help our students to gain insight  in how they and other students spend their time.   When students use Toggl, they can do so in semi-automatic mode  or manually. Semi-automatic mode means that, when they start an  activity, they can select it and click on a start button. When they  finish the activity, they click on a stop button. Manually means  that the students specify activity, time, and duration to Toggl. In  this way, students can add activities that they forgot to report or  edit them manually. Of course, on the one hand, this kind of  tracking is tedious and error prone. On the other hand, requiring  students to log time may make them more aware of their time  investment and may trigger more conscious decisions about what  to focus on or how much time to spend on a specific activity.  Moreover, we are also tracking other traces such as software  development and participation in the wiki and it is part of our  future work to include such traces in StepUp!    3.2 Visualizing the data with StepUp!  Figure 1 illustrates how the data are made available in their  complete detail in our StepUp! tool: this is a Big Table  overview where each row corresponds with a student. The  students are clustered in the groups that they belong to. For  instance: rows 1-3 contain the details of the students stijnadams,  robindecroon and nielsbillen (see marker 1 at Figure 1). These  three students work together in a group called findinge, the  second column in the table (marker 2). The green cells in that  second column indicate that these students made 4, 4 and 3 posts  in their group blog respectively (marker 3). Rows 4-6 contain the  details of the second group, called followap: they made 0, 1 and  1 comments on the blog of the first group (column 2) and 6, 3 and  6 posts in their own blog (column 3) respectively (marker 4). The  rightmost columns (marker 5) in the table indicate the total  number of comments, the total number of hours spent on the  course (Toggl) and the total number of tweets.                                                                     3 http://quantifiedself.com/  4 http://www.fitbit.com/  5 http://nikeplus.nike.com/plus/products/fuelband   The two rightmost columns are sparklines [20] that provide a  quick glance of the overall evolution of the activity for a  particular student (marker 6). They can be activated to reveal  more details of student activity (marker 7 and 8 at the bottom of  Figure 1). These bar charts show the distribution of the activity  along the weeks. The bar chart (marker 7) visualizes the time  spent on the different kinds of activities, such as reading  (documentation, other blogs, etc.), programming and face-to-face  meetings. In this way, the students get an overview of how they  have spent their time in the past week. The other bar chart (marker  8) shows the distribution of the participation (posts, tweets and  comments) along the weeks. This visualization intends to trigger  reflection about what the students did and why.     4. EVALUATION  We carried out a detailed evaluation six weeks into the course,  based on online surveys. In the evaluation, we used four  instruments, in order to obtain a broad view:   1) The importance of the most important learning issues  (see Table 1) was rated again by students.   2) We asked whether students over-report or under-report  time spent, enquired about their motivation and whether  they thought they were doing well in the course.   3) Fifteen questions assessed to which extent students  believed that StepUp! addressed the issues .    4) a SUS questionnaire  assessed usability of the  application.   All the questions presented in this survey use a 5-likert scale to  grade importance or agreement (1  not important at all to 5   very important and 1  strongly disagree to 5  strongly  agree).   The survey was completed by all the students of the three courses  (see Section 2). The survey results are discussed in the remainder  of this section.    4.1 Analysis of the results   First, students rated the importance of the learning issues that  were selected with the methodology described in Section 2. This  was done to get a full picture, since the set of issues grew during  the collection phase and not all students had rated all issues. The  results of these questions can be seen in the grey highlighted rows  of Figure 2. Afterwards, students rated statements to assess their  perception on how they were doing, their motivation and how  others were behaving in the course and finally, to assess whether  StepUp! addressed the selected learning issues. These statements  and their results are presented in the white rows of Figure 2.  In this section we discuss the results per evaluation section for the  three case studies (MUME, THESIS & PENO, see Section 2). In  addition, the last subsection includes an analysis of the results  drawn by Google Analytics6.   4.1.1 Importance of the issues  The results in the boxplots for issue I1 (To be aware which  resources and tools I and others use) in Figure 2 are indecisive:  whereas this does seem to be a somewhat important issue for the  MUME students, it is much less important for the THESIS and  PENO students. It is not immediately clear to us why this  difference in appreciation exists. However, as MUME students  usually face new challenges, learning how to develop software for  iPhone and Android devices, being aware of what kind of                                                                     6 http:// www.google.com/analytics/   17    resources are used by others can be more relevant than for PENO  and THESIS students. PENO students have less freedom to  choose tools to use. In addition, PENO students are bachelor  students and they may feel less confident to discover and to try  new resources. THESIS students share a methodology and a field  (Human Computer Interaction), but their thesis topic often differs,  which can be the reason that they feel less motivated to pay  attention to what resources and tools others are using.  The students consider issues related to how they distribute their   time more important (median between 3.5 and 4) than I1.    Being aware that team members do not work (I3) is important for  the MUME and PENO students (medians  4). This issue has not  been evaluated with THESIS students because they do work much  more individually.  Motivation (I4) and Being aware when something goes wrong (I5)  are important for all students (all medians  4).   Communication issues are important for the students in the three   Figure 2 Results of the evaluation   18    courses (all medians  4). The results are a bit more spread for  THESIS students. This is probably related to the fact that most of  these students work more individually on their master thesis and  thus care less about interacting with other thesis students.  Problems about finding a balance between social activities and  studying (I7) are somewhat important to the students (all medians   3 or 3.5).   4.1.2 Perceptions in the course  Among students enrolled in MUME and THESIS, there is a high  perception that other students may over-report. However, PENO  students are indecisive. PENO students used less our application  than others. In the PENO course, most of the work is done during  the lab sessions and all the groups have one member who holds  the role of project manager and this fact can decrease the feeling  of over-reporting.  However, all the students are indecisive when it relates to under- reporting time spent (all medians  3). This issue was included  because some students pointed out that others may under-report  trying to avoid that others call them grinds.  Based on these  results, it seems that this fear is not grounded in reality.  All of them are motivated (all medians = 4) and they think that  they are doing well in the course (medians = 4) except for the  thesis students. Although they are motivated, they are indecisive  whether they are doing well. The fact that thesis work has to be  done during a full academic year (as opposed to a semester in the  PENO and MUME courses) and the non-similar topics can  influence an uncertain feeling of how they are doing. However, it  brings an important niche that learning analytics can try to  address.    4.1.3 Issues  The students do not believe that StepUp! addresses statement A1  StepUp! helps me to be aware which resources and tools students  use (all medians  3). This is probably related to the fact that  StepUp! only visualizes blog activity and twitter use, and thus  only covers a minor part of resources use in the three courses.   When we assessed if StepUp! enables students to analyze their  time spent (A2), the results differ over the three courses. MUME  is most positive, while PENO is indecisive and THESIS is more  negative and spread. Further analysis is required to understand  why this is the case: it may be that StepUp! misses more of the  relevant activity in the case of the master thesis work.  StepUp! is not perceived to convincingly help students to analyze  how other students spend their time (A3) (medians are 3 and 3.5).   If we compare A2 and A3 with a previous experiment [31],  StepUp! has decreased its effectiveness. In this experiment, the   activities were defined with higher granularity such as  programming whereas in the previous experiment were more  specific activities such as first prototype. This may have affected  the perception on how StepUp! helps them to understand efforts.   When asked if StepUp! allows to identify a group member that  does not work well, the MUME students are not sure (median =  3), while StepUp! helps the PENO students better (median = 4).  The high perception that other students may over-report time  spent (P1) can influence negatively student perception on this  issue.  According to statement A5, students are not convinced that using  StepUp! increases their motivation (all medians  3). In this  context, it is important to note that the motivation of the students  for the course was high (P3  all medians  4).   They are more positive about StepUp! helping them to assess how  they are doing in the course (A6), especially the THESIS and  PENO students (median  3.5). Moreover, StepUp! allows the  students to better compare themselves with their peers (A7) (all  medians = 4).   The students are not convinced that StepUp! helps them with  being aware of course problems (A8) (all medians  3). When  asked if StepUp! makes them work harder (A9) or slower (A10) if  they are working not enough or too hard, THESIS and PENO  students are not convinced StepUp! makes them work harder  when appropriate, but MUME students are more positive (median  = 4). Students are not convinced that StepUp! makes them work  less (median of THESIS & PENO = 2 and MUME = 3). These  results can be influenced by statement A2 that StepUp! fails  helping them to understand how they spend their efforts.  Students are not convinced (median of THESIS = 2, median of  MUME & PENO = 3) that StepUp! enables them to gain insights  in how others communicate (A11). Regarding this issue, other  visualizations such as network visualizations could improve the  results. When we asked whether StepUp! makes them comment  more on blogs (A12), then we see that the MUME students are not  convinced (median = 3) and the THESIS and PENO students are  motivated by StepUp! to comment more on the blogs of fellow  students (median = 4). When assessing whether StepUp! promotes  the reading of blogs (A13), we see that it helps MUME and PENO  students (median = 4), but the THESIS students are not convinced  (median = 3). The reason for the behavior of the THESIS students  might be that they are more focused on research specific to their  thesis topic, while PENO and MUME students are working on a  shared topic, so the blogs of other students are more directly  related to their own work. When looking whether StepUp! makes  them use Twitter more (A14), we also learn that StepUp!  promotes the use of Twitter more for MUME and PENO students   Figure 3 Google Analytics view  19    (median = 4), than for THESIS students (median = 3). This might  be again caused by the common topic of the PENO and MUME  students and thus closer social interaction.  From the rating of statement A15 StepUp! enables me to plan my  time better, we learn that StepUp! does not help students to plan  more efficiently (all medians  2.5). This might be because  StepUp! only presents the time spent on past events and not the  work that still has to be done. Nor does StepUp! provide real  functionalities for planning. Our hope was that StepUp! would  assist students in becoming better planners through self-reflection  and sensemaking provided by the visualizations, but, at least in  the perception of the students, this goal is not reached.   4.1.4 SUS questionnaire  All the students rated our tool between acceptable and good.  MUME students rated the tool with 67.9, PENO students with   with 67.5 points and thesis with 72.1. In an earlier  experiment[31], SUS scores were 77 and 82 points. Our tool has  thus decreased in usability. This decrease may be due to the fact  that StepUp! has been deployed for a larger group of students   which in terms of scalability also caused some problems that  caused some delay updating the data on the table.    4.1.5 Use of the tool  The students have used the tool regularly as indicated by the  Google Analytics view (see Figure 3). In one month, StepUp has  been visited more than 850 times with an average time spent on  the tool of 4 minutes.   The visits view indicates that StepUp! is more visited during the  week. These visits decrease considerably during the weekend,  when also students reported lower time spent on the activities.  Comparing with the results in an earlier experiment, the average  time has decreased around 50%. This can either be caused by or  influence the usefulness perception for our tool. If students spend  less time, they cannot find out the benefits of our tool. In Section  6, we present some future research plans, including the  deployment of a mobile version, to better engage the students with  StepUp!    5. RELATED WORK  Learning analytics consider the analysis of communication logs  [14][33], learning resource use [26], learning management system  logs, learning designs [24][29], as well as the activity outside of  learning management systems [27][7]. The result of this analysis  can be used to improve the creation of predictive models[30][5],  recommendations [38] and reflection [32][23]. Since we focus on  reflection, we mainly build dashboards to enable self-reflection  and to enable the learner to steer the learning process or teachers  to plan interventions if they are required.     In recent years, several dashboard applications have been  developed to support learning and teaching. These dashboards are  used in traditional face-to-face teaching, online learning and  blended learning settings. Some examples are Classroom View  [16] that shows current activity in a classroom, the dashboard  implemented in the learning management system Moodle [28]  which tracks online activities to support teachers and the  educational monitoring tool based on faceted browsing and data  portraits showing the current status of each student in distance  education [17]. Khan Academy7 dashboard enables tutors to check  progress of students where a table provides a goal status overview  per student. For every student, a timeline shows the distribution of                                                                    7 http:// www.khanacademy.org/   achieved goals and a bar chart visualizes the time spent with  different kinds of resources.   Some dashboards were developed specifically to support learners.  CALMSystem [19] is an example of a dashboard that was  developed on top of an intelligent tutoring system to give a learner  insight into their learner model as a basis to support awareness,  reflection and sensemaking. Performance indicators on different  topics are visualized and can be adjusted by the learner as well.   Tell Me More [22] is a commercial language learning application  that tracks results of exercises as a basis to visualize progress of  learners. S3 [13] is a dashboard that enables practitioners to plan  interventions with students at risk. Narcissus [36] was developed  to support small group work. GLASS [23], SAM [18] and Student  Inspector [41] were developed to support both teachers and  learners. The work presented in this paper is intended to support  students and teacher reflection.   If we analyze dashboards, we find that time spent is a commonly  captured trace. In addition, social interaction can help to gain  insight in collaboration [9][1] and to detect isolated students [9].  Document and tool use can give effort indicators of students  [18][25].    GISMO[21] also offers the possibility to detect students who do  not work well. They highlight the students who have not reached a  minimum number of post messages in the forum. They conclude  that different learning behavior does not mean different reached  goals. It can also influence to the results on the statement  StepUp! helps me to analyze how others spend their time.   LOCO-Analyst[34] and SAM[18] address the issue to be aware  which resource I and others use. However, SAM also struggles  with analyzing time spent, because it uses logs generated by the  LMS, and students claim that some activity happens outside of the  LMS, so that the visualized information does not represent all  their work. This fact inspires GLASS [23] that tracks all  interaction within a Virtual Machine and visualizes also  programming code logs.   Looking at how other researchers evaluate their dashboards, we  find one longitudinal study [2] where a tool was evaluated over  three years, and was found to increase retention rates. Other more  limited studies focus on effectiveness [9][20][24] and perceived  usefulness [1][9][17][30][33]. Most studies on effectiveness and  perceived usefulness assess problems that the lecturer or the  literature describes. On the other hand, other studies [3][20]  reinforce the idea to ask the students what problems they have and  to assess whether these problems are addressed. The work  presented in this paper focuses on potential impact of StepUp!  may have on issues and needs identified by our students.   6. FUTURE WORK  Now that we analyzed the reported issues by the students, we  would like to focus on one important factor of learning:  motivation. Although our students are motivated in our courses  (see 4.1.2), we see from the results of the evaluation that we are  failing addressing some issues that may affect motivation.    Thesis students do not know whether they are doing well in the  course (section 4.1.2.) and this is an important factor for the  motivation of the student [42]. We expected that StepUp! could  address this issue by enabling comparison between peers, but  StepUp! does not help understand how peers as well as  themselves spend their time (section 4.1.3). Furthermore, StepUp!  does not motivate them to increase their social interaction  blogging, commenting and tweeting.    20    In order to address the reported issues, we will focus more on  generated artifacts and student motivation . To this end, we are  currently enriching StepUp! with gamification aspects: StepUp!  will again track blogs, comments, tweets, and, in addition,  bookmarked resources. We will define a series of rules in order to  reward students with badges for positive behavior. Badges will be  strongly linked to the activities of the course in order to increase  awareness of student progress in the course.   However, gamification approaches can have negative effects [42].  By making available an overview of achievable badges, we hope  to increase student awareness about what we expect from them. In  this way, we expect to deploy a more goal-oriented approach  improving perception on how students are doing in the course .   Visualizations such as progress bar chart can also reinforce the  idea of goal-oriented approach and activity streams increase the  awareness of and engagement to the course. Thus, these  approaches will also be integrated in the next version of our tools.       7. CONCLUSIONS  We have set up a series of brainstorming sessions to gather  requirements and to identify the most important issues for  students. As explained in Section 3, we considered which  functionalities of StepUp! could potentially help to address the  issues.  The general conclusion is that our tool has potentially higher  impact for students working in groups and sharing a topic such as  PENO and MUME than students working individually on  different topics. However, overall, students are not that convinced  of the added value of StepUp!.   Thus, we believe that this study helps to point out that  demonstrating the relevance of learning analytics dashboards like  StepUp! is a complex undertaking. We strongly believe that more  critical evaluations of the actual use of such tools are required.  Student perceptions of added value are not the only criterion, but  certainly a most important one!   8. ACKNOWLEDGMENTS  The research leading to these results has received funding from  the European Community Seventh Framework Programme  (FP7/2007-2013) under grant agreement no 231396 (ROLE).  Katrien Verbert is a Postdoctoral Fellow of the Research  Foundation - Flanders (FWO).      9. REFERENCES  [1] Ali, L., Hatala, M., Gaevi, D., & Jovanovi, J. A   qualitative evaluation of evolution of a learning analytics  tool. Computers and Education, 58(1), 470-489. 2012   [2] Arnold, K. E. & Pistilli, M.D. (2012). Course signals at  Purdue: using learning analytics to increase student success.  In S.B. Shum, D. Gasevic, and R. Ferguson  (Eds.)Proceedings of the 2nd International Conference on  Learning Analytics and Knowledge (LAK '12),(pp. 267-270).  NY: ACM   [3] Baillie, C. Motivation and attrition in engineering students,  Geraldine Fitzgerald European Journal of Engineering  Education Vol. 25, Iss. 2, 2000   [4] Bangor, A., Kortum, P.T., Miller, J.T.: An empirical  evaluation of the system usability scale. Int. J. Hum. Comput.  Interaction (2008) 574-594   [5] Barber,R., Sharkey,M. Course correction: using analytics to  predict course success. In Proceedings of the 2nd   International Conference on Learning Analytics and  Knowledge (LAK '12), 2012   [6] Parra, G., Klerkx, J., Duval, E. What Should I Read Next  Awareness of Relevant Publications Through a Community  of Practice. In SIGCHI '13 Case Studies. Accepted.   [7] Blikstein, P. Using learning analytics to assess students'  behavior in open-ended programming tasks. In Proceedings  of the Learning Analytics and Knowledge conferencd  (LAK11), 2011Brooke, J. SUS: A quick and dirty usability  scale. In: Usability Evaluation in Industry. Taylor & Francis,  London, 1996.   [8] Conole, G., Fill, K. (2005). A learning design toolkit to  create pedagogically effective learning activities. Journal of  Interactive Media in Education 2005(08).   [9] Dawson, S., Bakharia, A., & Heathcote, E. SNAPP:  Realising the affordances of real-time SNA within networked  learning environments. In L. Dirckinck- Holmfeld, V.  Hodgson, C. Jones, M. de Laat, D. McConnell, & T. Ryberg  (Eds.),  Proceedings of the 7th International Conference on  Networked Learning (pp. 125-133) 2010   [10] Dollr, A. ,Steif, P. S. Web-based Statics Course with  Learning Dashboard for Instructors. In Uskov, V. (Ed.),  Proceedings of Computers and Advanced Technology in  Education (CATE 2012), June 25  27, 2012, Napoli, Italy.  2012    [11] Duval, E.: Attention please! learning analytics for  visualization and recommendation. In: Proceedings of  LAK11: 1st International Conference on Learning Analytics  and Knowledge,, ACM (2011) 9-17   [12] Duval, E., Klerkx, J., Verbert, K., Nagel, T., Govaerts, S.,  Parra Chico, G.A., Santos, J.L., Vandeputte, B.: Learning  dashboards and learnscapes. In: Educational Interfaces,  Software, and Technology,. (May 2012) 1-5   [13] Essa, A., Ayad, H.. Student success system: risk analytics  and data visualization using ensembles of predictive models.  In Proceedings of the 2nd International Conference on  Learning Analytics and Knowledge (LAK '12), 2012   [14] Ferguson, R., Buckingham, S. Shum. Social learning  analytics: five approaches. In Proceedings of the 2nd  International Conference on Learning Analytics and  Knowledge (LAK '12) 2012.   [15] Fischer, G.: Understanding, fostering, and supporting  cultures of participation. interactions 18(3) (May 2011) 42- 53   [16] France, L., Heraud, J.-M., Marty, J.-C., Carron, T., & Heili,  J. Monitoring Virtual Classroom: Visualization Techniques  to Observe Student Activities in an e-Learning System.  Proceedings of the Sixth International Conference on  Advanced Learning Technologies, (pp.716-720). 2006   [17] Garca-Solrzano,D., Cobo, G.,  Santamara, E., Morn, J.A.,   Monzo, C., Melenchn,J. Educational monitoring tool based  on faceted browsing and data portraits. In Proceedings of the  2nd International Conference on Learning Analytics and  Knowledge (LAK '12). 2012   [18] Govaerts, S., Verbert, K., Duval, E., & Pardo, A. The  Student Activity Meter for Awareness and Self-reflection.  Proceedings of the 2012 ACM annual conference on Human  Factors in Computing Systems Extended Abstracts (pp. 869 884). ACM. 2012   21    [19] Kerly, A., Ellis, R. & Bull, S. CALMsystem: A  Conversational Agent for Learner Modelling, in R. Ellis, T.  Allen & M. Petridis (eds), Applications and Innovations in  Intelligent Systems XV  Proceedings of AI-2007, 27th  SGAI International Conference on Innovative Techniques  and Applications of Artificial Intelligence (pp. 89-102).  Springer Verlag. 2007   [20] Killen, R., Differences between students and lecturers  perceptions of factors inuencing  students academic  success   at  university. Higher  Education  Research  and  Development, 13, 199. 1994   [21] Kobsa, E., Dimitrova, V., & Boyle, R. Using student and  group models to support teachers in web-based distance  education. Proc. of the 10th international conference on user  modeling (pp. 124133). Edinburgh, UK. 2005   [22] Lafford, B. A. Review of Tell Me More Spanish, Journal on  Language Learning & Technology, 8(3), 21-34. 2004   [23] Leony,D., Pardo,A.,  Fuente,L , Snchez,D., Delgado,C..  GLASS: a learning analytics visualization tool. In  Proceedings of the 2nd International Conference on Learning  Analytics and Knowledge (LAK '12), 2012   [24] Lockyer,L.,Dawson, S. Where learning analytics meets  learning design. In Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (LAK  '12). 2012   [25] Mazza, R., & Milani, C. (2004). GISMO : a Graphical  Interactive Student Monitoring Tool for Course Management  Systems. TEL04 Technology Enhanced Learning 04  International Conference (pp. 1-8). Citeseer   [26] Niemann, K., Schmitz, H-C, Kirschenmann, U., Wolpers,M.  Anna Schmidt, and Tim Krones. Clustering by usage: higher  order co-occurrences of learning objects. In Proceedings of  the 2nd International Conference on Learning Analytics and  Knowledge (LAK '12) 2012   [27] Pardo,A., Kloos, C. Stepping out of the box towards  analytics outside the learning management system. In  Proceedings of the Learning Analytics and Knowledge  conference (LAK11), 2011   [28] Podgorelec, V. & Kuhar, S. Taking Advantage of Education  Data: Advanced Data Analysis and Reporting in Virtual  Learning Environments. Electronics and Electrical  Engineering, 114(8), 111-116.   [29] Richards, G.,DeVries, I. Revisiting formative evaluation:  Dynamic monitoring for the improvement of learning  activity design and delivery. In Proceedings of the Learning  Analytics and Knowledge conference (LAK11), 2011   [30] Roijers, D.M., Jeuring, J., Feelders, A. Probability estimation  and a competence model for rule based e-tutoring systems. In  Proceedings of the 2nd International Conference on Learning  Analytics and Knowledge (LAK '12), 2012   [31] Santos, J.L., Verbert, K., Govaerts, S., Duval, E.  Empowering students to reflect on their activity with   StepUp!: Two case studies with engineering students, In  proceedings of EFEPLE11 2nd Workshop on Awareness and  Reflection in Technology-Enhanced Learning, CEUR WS  (2012) (accepted)   [32] Santos, J.L., Govaerts, S., Verbert, K., Duval, E.: Goal- oriented visualizations of activity tracking: a case study with  engineering students. In: LAK12: International Conference  on Learning Analytics and Knowledge, Vancouver, Canada,  29 April - 2 May 2012, ACM (May 2012)   [33] Schreurs,B. , De Laat, M. Network awareness tool - learning  analytics in the workplace: detecting and analyzing informal  workplace learning. In Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (LAK  '12). 2012   [34] Silius, K., Miilumaki, T., Huhtamaki, J., Tebest, T.,  Merilainen, J., & Pohjolainen, S. Students' Motivations for  Social Media Enhanced Studying and Learning. Knowledge  Management & E-Learning: An International Journal  (KM&EL), 2(1). 51-67. 2010   [35] Tufte, E.R.: Beautiful Evidence. Graphics Press (2006)  [36] Upton, K., Kay, J.. Narcissus: Group and individual models   to support small group work. In Proceedings of the 17th  International Conference on User Modeling, Adaptation, and  Personalization: formerly UM and AH, UMAP '09, pages 54- 65,Berlin, Heidelberg, 2009. Springer-Verlag,. 2009   [37] Verbert, K., Manouselis, N., Drachsler, H., Duval, E.:  Dataset-driven research to support learning and knowledge  analytics. Educational Technology and Society (2012) 1-21   [38] Verbert,K., Drachsler, H., Manouselis, N., Wolpers, M.,  Vuorikari, R., Duval,E. Dataset-driven research for  improving recommender systems for learning. In  Proceedings of the Learning Analytics and Knowledge  conference (LAK11), 2011   [39] Wenger, E.: Communities of Practice: Learning, Meaning,  and Identity (Learning in Doing: Social, Cognitive and  Computational Perspectives). 1 edn. Cambridge University  Press (September 1999)   [40] Yacef, K., Zaane, O., Hershkovitz, H., Yudelson, M., and  Stamper, J. (eds.) Proceedings of the 5th International  Conference on Educational Data Mining. 2012   [41] Zinn, C., Scheuer, O. How did the e-learning session go The  Student Inspector. In In Luckin, R., Koedinger, K.R., &  Greer, J. (Eds.) Proceeding of the 2007 conference on  Artificial Intelligence in Education: Building Technology  Rich Learning Contexts That Work (pp. 487-494). IOS Press:  Amsterdam, The Netherlands (2007)   [42] Williams, K., Williams, C. Five key ingredients for  improving student motivation (2011) Vol. 12. (pp. 1-23)  Research in Higher Education Journal           22      "}
{"index":{"_id":"4"}}
{"datatype":"inproceedings","key":"Rivera-Pelayo:2013:LIM:2460296.2460302","author":"Rivera-Pelayo, Ver'onica and Munk, Johannes and Zacharias, Valentin and Braun, Simone","title":"Live Interest Meter: Learning from Quantified Feedback in Mass Lectures","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"23--27","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460302","doi":"10.1145/2460296.2460302","acmid":"2460302","publisher":"ACM","address":"New York, NY, USA","keywords":"data capturing, learning analytics, live feedback, mobile application, quantified self, reflective learning","abstract":"There is currently little or no support for speakers to learn by reflection when addressing a big audience, like mass lectures, virtual courses or conferences. Reliable feedback from the audience could improve personal skills and work performance. To address this shortcoming we have developed the Live Interest Meter App (LIM App) that supports the gathering, aggregation and visualization of feedback. This application allows audience members to easily provide and quantify their feedback through a simple meter. We conducted several experimental tests to investigate the acceptance and perceived usefulness of the LIM App and a user study in an academic setting to inform its further development. The results of the study illustriate the potential of the LIM App to be used in such scenarios. Main findings show the need for motivating students to use the application, the readiness of presenters to learn retrospectively, and distraction as the main concern of end users.","pdf":"Live Interest Meter  Learning from Quantified Feedback in Mass Lectures  Vernica Rivera-Pelayo, Johannes Munk, Valentin Zacharias and Simone Braun FZI Research Center for Information Technology  Haid-und-Neu-Str. 10-14 Karlsruhe, Germany  {rivera, munk, zach, braun}@fzi.de  ABSTRACT There is currently little or no support for speakers to learn by reflection when addressing a big audience, like mass lec- tures, virtual courses or conferences. Reliable feedback from the audience could improve personal skills and work perfor- mance. To address this shortcoming we have developed the Live Interest Meter App (LIM App) that supports the gath- ering, aggregation and visualization of feedback. This appli- cation allows audience members to easily provide and quan- tify their feedback through a simple meter. We conducted several experimental tests to investigate the acceptance and perceived usefulness of the LIM App and a user study in an academic setting to inform its further development. The results of the study illustriate the potential of the LIM App to be used in such scenarios. Main findings show the need for motivating students to use the application, the readiness of presenters to learn retrospectively, and distraction as the main concern of end users.  Categories and Subject Descriptors H.5.2 [Information interfaces and presentation]: User interfaces; K.3.2 [Computers and Education]: Computer Science Education  Keywords Live feedback, Data capturing, Mobile application, Learning Analytics, Reflective Learning, Quantified Self  1. INTRODUCTION For lectures and conferences  one of the main daily activ-  ities of researchers, professors, lecturers and students  there is currently little or no support to learn by reflection, even though in these scenarios there seems to be a great potential to improve professional skills and presenters performance by learning from personal experience. Especially in mass lec- tures, virtual courses or conferences, reliable feedback from the audience is missing and the speaker lacks support to  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 13, April 08 - 12 2013, Leuven, Belgium Copyright 2013 ACM 978-1-4503-1785-6/13/04 ...$15.00.  quickly evaluate the overall development and react accord- ingly. To realize this technical support, research questions such as how to optimally aggregate and present individual feedback of a large group of participants during the talk must be answered. Until now, traditional surveys after the lectures or conferences have been used, which are usually centered on the content of the course/talk and performed only once in retrospective, with the results not always being shared with the participants.  In order to address these shortcomings, we have devel- oped the Live Interest Meter App (LIM App). Its goal is to quantify and track abstract feedback from an audience (e.g. emotions and thoughts especially referred to their percep- tion of the event) and to provide this feedback live as well as retrospectively to the presenter in order to forster learning processes through reflection. The LIM App also aims for improving students attitude, attention and concentration during a lecture. With this approach, we investigate po- tential improvements in managing learning in practice and apply learning analytics on data gathered during the lecture. Since lectures belong to the continuous learning process of students, improving these activities contributes to the im- provement of the general learning process itself.  Reflection in terms of learning is defined by Boud et al. [3], as those intellectual and affective activities in which in- dividuals engage to explore their experiences in order to lead to new understandings and appreciations. The reflec- tive process is based on the re-evaluation of the learners experiences, considered as the total response of a person to a situation, including behavior, ideas and feelings, and thereby producing outcomes. In our concrete scenario, re- flecting about data derived from captured feedback may help users improve their presentation skills and performance when addressing an audience. Besides, such data will help the speaker reflect about the differences between her own perspective on an event and how it is perceived by the par- ticipants. Our approach is based on an integrated model of reflective learning and Quantified Self (QS) [10, 11], that shows how QS tools can technically support the cognitive process described by Boud et al.  In the following, we will present a review of related work, before describing the Live Interest Meter App (Section 3). In Section 4 we describe the experimental tests and the user study with their results. Finally, we conclude this paper with a discussion of the results and future work in Section 5.  2. RELATED WORK Audience Response Systems (ARS), also known as click-  23    ers [5, 9, 12], enable instructors in a large lecture class to instantaneously collect student responses to a posted ques- tion, generally multiple choice. The answers are immedi- ately tallied and graphically displayed on a classroom pro- jection screen where both students and instructor can see and discuss them [4]. These systems generally aim at im- proving student outcomes such as improved exam scores or passing rates, student comprehension, and learning and as well as student attendance and interest on the course. There are also some approaches that explore questions and answers (Q&A) in a bidirectional way using micro-blogging [1, 8], i.e. not only the lecturer can create questions to evaluate students, but students can also pose questions.  There are also some available products with the aim of en- hancing students engagement in class, for example, GoSoap- Box1, ShakeSpeak2 or Socrative3. Several projects have also developed systems to support real-time feedback during lec- tures at university [2, 14], which offered elementary tools for cooperation to enable electronic feedback, hand-raising and multiple-choice-polls with live-results.  Most of the related work above explores and focuses on giving a benefit for students and are tightly related to the content of the lectures and the knowledge they acquire, be- ing limited to polling of questions and answers in many cases. On the contrary, the use case we are considering takes the lecturer and presenter as the center of the scenario and focuses on professional performance improvement. In this case, students play a very important role, as they are the ones who provide feedback, but they are not the only target group to get a benefit from the tool. We enrich the feedback meter function with other features that support the lecture like polls, questions and chats are intended to improve the lecture itself, but also to add context to the feedback that students are giving to the lecturer.  In the field of Technology Enhanced Learning there are few approaches to support self-reflection and increase aware- ness for both learners and teachers, and they are mainly centered on the activities of the students, like the Student Activity Meter (SAM) [7], which provides a set of visual- izations to be used in digital learning platforms (or LMS) or the EnquiryBlogger [6], an extension of a blog developed to support awareness and reflection for enquiry-based learn- ers. Outside of the learning management system there are also approaches to track the activity of students [13], with several visualizations intended to increase motivation of stu- dents and help them reflect on their learning process.  In these related approaches we have reviewed tools to sup- port reflection on several sources of data, but none of them considers the gathering of feedback and other data during the lectures for reflective learning purposes. Additionally, these approaches broadly cover the support for students, but do not take into account the potential of this data for the lecturers self-improvement.  3. LIVE INTEREST METER  LIM APP  3.1 LIM App: Features and Interface The LIM App has two interfaces: an Android App as well  as a JavaScript-based application, which provides a broader  1GoSoapBox, www.gosoapbox.com 2ShakeSpeak, www.shakespeak.com 3Socrative, www.socrative.com  access from any device. The core of the LIM App is the me- ter component (see Figure 1), which allows users to vote on a quantifiable aspect. A title and captions for minimum and maximum value are to be defined by the presenter (master) and can range from aspects like comprehension (difficult or easy to understand) to performance (too fast or too slow for me). The colored background represents the scale and an input slider allows entering the desired value.  Figure 1: Live Interest Meter: meter with touch interface and poll results (both on the Android ver- sion) cVeronica Rivera-Pelayo et al.  Users can update their vote at any time and are encour- aged to do so, whenever they think of it. A given value stays valid only for a specific amount of time, in order to maintain a certain timeliness of the feedback.  Figure 2: Evolution graph showing the feedback in time. a) Android interface b) JavaScript interface cVeronica Rivera-Pelayo et al.  The evolution graph (see Figure 2) shows three different aspects of the meter-values in a timeline (x-axis): (a) the colored area spans between the 10th and 90th percentile of all the users values sent to the master (y-axis), (b) the black line displays the group average feedback and (c) in red is de- picted the feedback value provided by the local user (not available for the master as she is not actively contributing to the evaluation). In the top-right corner the number of currently valid user-votes is displayed  important to esti- mate the relevance of the given feedback. The red cycles at the bottom represent topic markers, that the presenter can introduce as contextualization of the time series.  Both master and client application show this graph live, updated every second. That is how the feedback is pre- sented to the users, for their personal inspection as well as  24    for subsequent reflection. In order to increase the interactivity between audience  and presenter, the tool additionally offers instant-polls that can be prepared and suggested by users, questions that are rated by the audience and optionally an open chat.  3.2 Reflective Learning from Feedback As introduced before, the Live Interest Meter aims at sup-  porting reflective learning from feedback in events where a person addresses a large audience. The way Live Interest Meter tracks and shares feedback, allows both the presen- ter to receive feedback from a large audience as well as for each participant to compare his learning experience with his peers.  Figure 3: LIM App use case: integrated feedback and reflection loop cVeronica Rivera-Pelayo et al.  The model in [10, 11] describes how Quantified Self tools like the LIM app can support reflective learning. It defines three support dimensions, which can be tackled by tools, namely: (i) tracking cues (capturing certain kinds of data as basis for the reflective learning process), (ii) triggering (fostering the initiation of reflective processes in the learner), and (iii) recalling and revisiting experiences (through the enrichment and presentation of data in order to make sense of past experiences).  In our approach, tracking is based on self-reporting through the LIM App. The tracked aspect is the users quanti- fied vote on the meter scale. Upon inspection the evolu- tion graph can passively trigger reflection. Trigger may be visually significant differences between the users vote and that of the group or sudden changes in the evolution graph (Adaption/Reaction in Figure 3). During recalling and re- visiting the user can inspect the topic markers in the time- line, questions and polls. They provide contextualization and help to align the time series with the users memorized experiences.  The collected highly contextualized data can serve as an- chor for several aspects and perspectives of reflection: First, the presenter can assess the evolution of the quality of her presentation through time. Then she can reflect about dif- ferences between the participants perception of events and hers on a very detailed scale. Second, members of the au- dience can compare their own ratings with the group ag- gregation (Peer comparison in Figure 3), which represents their social context and become aware of dissimilarities. By exploring the graph, it is possible to detect topics where a users capability or speed of understanding deviates from the other classmates. Reflection might help identify reasons and  improve the learning experience. The presenter can assess if a topic needs further explanation and examples in order to be comprehensible for the majority of the audience.  That is how the LIM App provides an integrated feedback and reflection loop for both the presenter and the audience.  4. EXPERIMENTS AND USER STUDY  4.1 Experimental Tests The LIM App was firstly investigated through two exper-  iments, in a lesson and a project meeting. The first informal test was conducted in a project meeting of 20 participants, with 10 active LIM users. This event was mainly discussion driven and the participants expressed in one of the ques- tions of the survey how well did the LIM App perform for the following purposes (see Figure 4):  Figure 4: How well did the LIM App work for the following options  Half of the participants opined that the LIM App per- formed well or very well to track their interest in the topic. The participants were willing to compare their interest with the others (50% indicated they could do it well with the app and 20% very well) but they also admitted to have played with the app (so the fun factor was already recognized).  A second technical test was performed in a lecture at the university, where we collected informal feedback from the participants. Technically the app performed well but the acceptance of the app was rather limited, as the lecture had a small group of students and the session itself was quite interactive. In such cases, the use of technical means to capture feedback and response answers are not considered as necessary.  In these informal tests, we got the first insights about the application: the LIM App is suitable for big audiences where personal contact is not established but it is not so suitable for events which are discussion driven, as then the role of the presenter is not well defined. After these experimental tests, and before moving towards bigger audiences, we conducted a user study to help refining the use case and guide the further development of the LIM app.  4.2 User Study The user study was designed around the following research  questions: RQ1: In which scenarios and how can the quantification of feedback performed by the LIM App support reflective learning  25    RQ2: Which features are more appreciated by users, both presenters and audience  Firstly, 20 qualitative interviews with groups of the LIM App end users were conducted in order to investigate the use case scenarios and the application of the app itself. The interviewees included professors, teachers and students.  Many presenters or lecturers already collect feedback from their presentations and lectures, but they do it in the conven- tional way through written questionnaires or direct contact to their students. Technology supported feedback systems were not used among the participants and the idea of hav- ing a system like the LIM App, which allows them to give and get feedback, was very well received. This was also true for the audience, who was willing to give feedback not only after but also during their lectures. According to the participants, the anonymity of the feedback would result in higher audience participation and students would voice their own opinion more openly, without the fear of contradicting opinions or ideas.  The poll function was considered especially useful for lec- tures, because it allows lecturers to quickly evaluate the knowledge of their students. On the other hand, the chat was considered distracting and unnecessary. However, many interviewees were afraid of the distraction of both presenter and audience, if they focus more on the application itself than on the content of the lecture. Many of them also agreed that reflection needs time, and therefore they would better reflect after the event had taken place than during the pre- sentation feeling under pressure.  In a second phase, we conducted an online survey based on our experience and the feedback from the interviews. Our aim was to get to know the disposition, opinion and ideas of potential users. For that purpose, we explained them the concept of reflective learning and its support with the LIM App (also including screenshots), before asking them several questions for assessment and evaluation. Questions referring to the role of the presenter (e.g. as teacher, profes- sor or in conferences) were only asked to participants who had confirmed that they give talks and presentations(44,82 %), whereas other questions concerning the role of the au- dience (e.g. which criteria they would like to evaluate) were only answered by the rest (55,17 %).  The survey was online for a month (May-June 2012) and 120 people participated. We obtained 87 valid and com- plete datasets. In order to diversify the participants back- ground, we distributed our survey through various channels: personal relations to professors, colleagues and institutions, social media and our website. The age distribution of the participants was the following: 17-24 (36,78 %), 25-35 (41,37 %), 36-50 (12,64 %), and more than 50 (9,19 %).  Participants in the survey do presentations in different contexts: teacher at school (7,69 %), lecturer at a university (41,02 %), speaker in conferences (56,41 %) or presentations as part of their professional work (56,41 %). Regarding their audience, only 12,81 % present in front of more than 60 peo- ple, whereas the majority of the participants have audiences of 11-30 (46,15 %) or 31-60 (25,64 %) people. The 48 participants who do attend presentations do it as stu- dents (89,58 %), in their free time (8,33 %), in conferences (6,25 %) and/or at work (22,91 %).  From the presenters side, nearly 90 % of the participants would like to get feedback after each presentation, whereas around 72 % would also agree to get feedback during the  event, for example after each section. Receiving feedback live during the whole presentation was chosen as ideal by 12,82 % of the participants and as OK by 25,64 %, but 33 % were indecisive.  Although the pressure for immediate feedback adaption was a concern among many participating presenters, 53,84 % think that they could achieve it during the presentation. Reacting to the feedback periodically (e.g. some days later) and in relation to the content blocks (e.g. between content blocks) seems to be the most popular option (76,91 %), what may be directly related to reflective learning practices from the data they gathered in past events.  All respondents expressed their opinion regarding several aspects of the LIM App, like if the adoption of a tool like this would completely distract the audience (18,39 %: strongly agree, 56,32 %: agree, 20,68 %: disagree, 1,14 %: strongly disagree) or if feedback must be anonym in order to be re- liable and honest (41,37 %: strongly agree, 27,58 %: agree, 26,43 %: disagree, 3,44 %: strongly disagree).  We also asked in the survey about the capturing of the data. 64,36 % of the participants agreed with data collected live and continuously being better and more significant than only periodically collected data.  The second part of the survey contained questions about the concrete characteristics that the LIM App offers and which new features would improve the application. The chat function in the LIM App was considered unnecessary (4,59 %: very meaningful, 16,09 %: rather meaningful, 43,67 %: unnecesary, 33,33 %: absolutely unnecesary, 2,29 %: I can- not judge). On the contrary, the participants appreciated very much polling questions (34,48 %: very meaningful, 54,02 %: rather meaningful, 8,04 %: unnecesary, 3,44 %: absolutely unnecesary) and also the possibility to review old polled questions (28,73 %: very meaningful, 50,57 %: rather meaningful, 13,79 %: unnecesary, 2,29 %: absolutely un- necesary, 4,59 %: I cannot judge).  We also investigated which information they would like to track. Figure 5 shows that both speakers and audience members agree on pace and clarity of the examples being important, but disagree regarding the slides and documents of the lecture.  Summarizing the positive results, more than 3/4 of the participants (78,16 %) in the survey found the idea behind LIM App very positive and found advantages in using it.  5. CONCLUSION This paper presented the Live Interest Meter, an appli-  cation for gathering, quantification, aggregation and visual- ization of feedback in mass lectures and conferences.  It was developed with the aim of supporting reflective learning from feedback, by reacting to the feedback live as well as by exploring the data retrospectively. We conducted two experimental tests and a user study to investigate the ac- ceptance, perceived usefulness and potential improvements of the LIM App.  Our acceptance study delivered a highly positive reso- nance and the idea of using a technology driven feedback system was well received. Our test subjects recognized the advantages of our feedback system and saw a possible bene- fit in its use. We also confirmed one of the main fears of end users: the potential distraction of the audience, by concen- trating on the application instead of the presentation itself. In order to improve this, we will address this issue in dif-  26    Figure 5: Which information would the presenters like to know about and which information does the audience want to evaluate  ferent directions, e.g. by keeping the application as simple as possible to demand the minimum cognitive effort from users. To this end, the next prototype will only show the necessary information to the users or offer an introductory training to teachers in order to show them how to integrate the LIM App in their lectures and make the best of it.  Our efforts in developing the next prototype will focus on improving the support of recalling and revisiting past events, by creating a platform that allows users to explore their gathered data retrospectively.  Additionally, further improvements based on the feedback from the user study will be adapted, e.g. alternatives to the chat functionality will be explored and futher features to support the presenter will be adopted.  With the new prototype we will then be able to evaluate the LIM App again in the field and in this way validate our current results, which show the expectations and opinions of the end users. The main goal of this evaluation will be to prove that reflective learning among participants takes place with the support of the LIM App and that this brings a benefit and improvement for the learner.  Finally, some concerns regarding the students voluntar- ily participating and giving feedback were also mentioned in our study. To this extent, we currently work on motiva- tion techniques to engage our users to use the application and investigate how gamification techniques can improve the adoption and long term use of the LIM App.  6. ACKNOWLEDGMENTS The authors would like to thank A. Brjezovski, C. Cather-  ine, M. Kohaupt and S. Kuhn for their collaboration in con- ducting the user study. Work presented in this paper was partly conducted within the project MIRROR  Reflective learning at work funded under the FP7 of the European Commission (project number 257617).  7. REFERENCES [1] M. Akbari, G. Bohm, and U. Schroeder. Enabling  communication and feedback in mass lectures. In ICALT, pages 254258, 2010.  [2] M. Bonn, S. Dieter, and H. Schmeck. Kooperationstools der Notebook Universitat Karlsruhe (TH). In Mobiles Lernen und Forschen, pages 6371. Klaus David, Lutz Wegener (Hrsg.), November 2003.  [3] D. Boud, R. Keogh, and D. Walker. Reflection: Turning Experience into Learning, chapter Promoting Reflection in Learning: a Model., pages 1840. Routledge Falmer, New York, 1985.  [4] J. E. Caldwell. Clickers in the large classroom: Current research and Best-Practice tips. CBE Life Sci Educ, 6(1):920, Mar. 2007.  [5] D. Duncan and E. Mazur. Clickers in the Classroom: How to Enhance Science Teaching Using Classroom Response Systems. Pearson Education, 2005.  [6] R. Ferguson, S. B. Shum, and R. D. Crick. EnquiryBlogger: using widgets to support awareness and reflection in a PLE Setting. In ARPLE11, PLE Conference 2011, Southampton, UK, 11-13 July, 2011.  [7] S. Govaerts, K. Verbert, E. Duval, and A. Pardo. The student activity meter for awareness and self-reflection. In CHI 12 Extended Abstracts on Human Factors in Computing Systems, pages 869884, New York, NY, USA, 2012. ACM.  [8] J. Hadersberger, A. Pohl, and F. Bry. Discerning actuality in backstage - comprehensible contextual aging. In EC-TEL, volume 7563 of Lecture Notes in Computer Science, pages 126139. Springer, 2012.  [9] D. Kundisch, P. Herrmann, M. Whittaker, M. Beutner, G. Fels, J. Magenheim, W. Reinhardt, M. Sievers, and A. Zoyke. Designing a Web-Based Application to Support Peer Instruction for Very Large Groups. In ICIS 12, Research in Progress, Orlando, USA, December 2012.  [10] V. Rivera-Pelayo, V. Zacharias, L. Muller, and S. Braun. Applying quantified self approaches to support reflective learning. In 2nd International Conference on Learning Analytics and Knowledge, LAK 12, pages 111114, USA, 2012. ACM.  [11] V. Rivera-Pelayo, V. Zacharias, L. Muller, and S. Braun. A framework for applying quantified self approaches to support reflective learning. In IADIS Mlearning 2012, Berlin, Germany, 2012.  [12] G. Rubner. mbclick - an electronic voting system that returns individual feedback. In WMUTE, pages 221222. IEEE, 2012.  [13] J. L. Santos, S. Govaerts, K. Verbert, and E. Duval. Goal-oriented visualizations of activity tracking: a case study with engineering students. In 2nd International Conference on Learning Analytics and Knowledge, LAK 12, pages 143152, New York, NY, USA, 2012. ACM.  [14] A. Wessels, S. Fries, H. Horz, N. Scheele, and W. Effelsberg. Interactive lectures: Effective teaching and learning in lectures using wireless networks. Comput. Hum. Behav., 23(5):25242537, Sept. 2007.  27      "}
{"index":{"_id":"5"}}
{"datatype":"inproceedings","key":"Koulocheri:2013:CFA:2460296.2460304","author":"Koulocheri, Eleni and Xenos, Michalis","title":"Considering Formal Assessment in Learning Analytics Within a PLE: The HOU2LEARN Case","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"28--32","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460304","doi":"10.1145/2460296.2460304","acmid":"2460304","publisher":"ACM","address":"New York, NY, USA","keywords":"betweeness centrality, centrality, grades, graphs, indegree centrality, learning analytics, metrics, outdegree centrality, social network analysis","abstract":"Personal Learning Environments are used more and more by the academic community. They can coexist with formal courses as a communication and collaboration channel. In this paper, an application of learning analytics into HOU2LEARN, a Personal Learning Environment set by Hellenic Open University is discussed. The present part of research focuses on the social network analysis as a branch of learning analytics, along with formal grading system. Since it is an ongoing research, this paper presents the preliminary results of the study of the correlation between the social network metrics and the formal grades, through a test case course, the PLH42.","pdf":"Considering Formal Assessment in Learning Analytics  within a PLE: The HOU2LEARN Case  Eleni Koulocheri  Hellenic Open University  Parodos Aristotelous 18  26 335 Patra, Greece  (0030) 2610 367617   ekoulocheri@eap.gr   Michalis Xenos  Hellenic Open University  Parodos Aristotelous 18  26 335 Patra, Greece  (0030) 2610 367695  xenos@eap.gr   ABSTRACT  Personal Learning Environments are used more and more by the  academic community. They can coexist with formal courses as a  communication and collaboration channel. In this paper, an  application of learning analytics into HOU2LEARN, a Personal  Learning Environment set by Hellenic Open University is  discussed. The present part of research focuses on the social  network analysis as a branch of learning analytics, along with  formal grading system. Since it is an ongoing research, this paper  presents the preliminary results of the study of the correlation  between the social network metrics and the formal grades, through  a test case course, the PLH42.   Categories and Subject Descriptors  D.3.3 [Programming Languages]: Measurement, Documentation,  Design, Verification.   General Terms  Measurement, Documentation, Design, Verification.   Keywords  Learning Analytics, Social Network Analysis, Graphs, Metrics,  Betweeness Centrality, Indegree Centrality, Outdegree Centrality,  Centrality, Grades.   1. INTRODUCTION  It is commonly accepted that analytics can influence the digital  activities, since they measure and monitor the digital routine,  contributing to the enhancement of a user experience in terms of  cost, quality and personalisation. However, they are also ready to  play a key role in the educational digital life, with the specific term  learning analytics, according the conference proceedings of both  1st and 2nd Learning Analytics and Knowledge Conference [2, 3].  This paper presents the application of learning analytics  (hereinafter, LA), in a personal learning environment,  HOU2LEARN, set by the Hellenic Open University (HOU). More  specifically, this paper discusses the Social Network Analysis  (hereinafter, SNA) aspect of LA in course level in an informal  environment, and investigates it along with the formal grading  system results that HOU applies in its courses.   Section 2 presents the bibliography that supports LA and focuses  in SNA as aspect of one of five axes that describe LA lifecycle.   Furthermore, it outlines the research concept of this paper.   Section 3 presents briefly HOU2LEARN, the PLE under research,  its major functionalities and the way it is applied by HOU in the  PLH42 which is our test case.   Section 4 involves the research part of this paper. Its first part  presents the SNA tool that was used, describes the components of a  typical SNA graph and outlines some standard SNA metrics.  Furthermore, for completeness reasons, it presents briefly a set of  additional metrics that measure the users activity and are part of  ongoing research. It also presents the algorithm of formal grading  that HOU applies in all courses. The second part presents four  SNA graphs: in the first one, nodes size is proportional to final  grade of each student, while in the three other graphs, nodes size  is proportional to an SNA metric (outdegree, indegree and  betweeness centrality respectively). Short discussion after each  graph is also included.   Section 5 includes some first remarks on the analysis of these four  graphs and it also presents some steps of future work that aims to  take into consideration the aforementioned metrics of activities, in  order to complete the analytics concept, both formally and  informally.    2. TYPES OF LEARNING ANALYTICS  The term Learning Analytics was initiated and officially defined  during the 1st International Conference on Learning Analytics and  Knowledge of 2011 as the measurement, collection, analysis and  reporting of data about learners and their contexts, for purposes of  understanding and optimizing learning and the environments in  which it occurs [1, 2, 3]. Fournier et al. [5] advocate that learning- related data collection and analysis would be related to both the  increase of the effectiveness of learning, and possible changes in  the learning process. According to Downes [9], most of the tools  that measure learning engagement, measure trivial variables such  as page access, geographical origin etc. considering these tools  useful for a basic level of assessment, he believes that future  analytic systems that will analyze even the learners contributions  quality will arise. For other researchers [4, 10, 11], both learning  improvement and provision of recommendations based on earlier  learning activity could be covered by providing learners and  moderators with information derived from analytics.  Siemens and Long [1] propose five axes that define LA:    Course level: social network analysis, discourse analysis,  learning trails.    Educational data mining: pattern recognition and  predictive modeling.     Intelligent curriculum: development of semantically  defined curriculum resources    Adaptive content: provision of adaptive content using  recommendation procedures, based on learner behaviour    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.    LAK '13, April 08 - 12 2013, Leuven, Belgium.  Copyright 2013 ACM 978-1-4503-1785-6/13/04 $15.00.   28     Adaptive learning: social interaction and learner support  as an adaptive learner process.   This paper focuses on analytics derived from learning networks in  the context of Personal Learning Environments (PLEs) through the  H2L Open Educational Platform. It investigates the first axis which  refers to course level and relates the research with SNA, learning  trails and discourse measurement using activity metrics. In  addition to that, formal assessment through grades has been taken  into consideration for those H2L members that were also students  of the Hellenic Open University and had to prepare six formal  written essays and to participate to exams.  SNA acts as a quantitative method of LA. Through network  graphs, SNA aims to depict connections, by creating a connection  graph. It also aims to reveal the central nodes of the connections  grid and thus, the people on the network who created impact to  their connections (neighbors) [5]. It consists of two basic building  components: Nodes (or vertices or agents) and edges (or  connections). Nodes usually represent physical members of a  network, such as learners and instructors in case of an educational  social network. They can also represent other social structures such  as communities, groups, institutions, states or even countries, if  necessary [12, 17]. Edges represent the connection between two  nodes and consist of lines that point from one node to another.  Edges may be directed or undirected; directed edges, represented  as arrows, have an origin and a destination and may represent  who is following who, who sends a message to whom [12, 17]  who tells the secret to whom, who affects with a virus who,  etc, in general.   Furthermore, in the context of a personal learning environment,  users have a wide range of activities that occur from the  engagement and involvement in a course. All these activities are  involved in the term discourse according to Downes five axes  [9]. Metrics can quantize in more detail the activity of the learners  (or members in case of an educational social network) producing  measurable results for questions like the following: Who  comments on every type of content and how much  Who does  product more content Who launches his/her own discussion topic   What content is produced per learner Who feeds the most the  public discussion of the course This set of metrics is an additional  tool towards the measurement of the learning activity using PLE  produced data [17].  Both aforementioned aspects of a PLE (i.e. Connectedness and  activities) refer to informal learning: The PLE under research,  H2L, aims to facilitate this informal  communication among the  learners of a formal course in which, at the same time, a formal  grading system is applied. By formal communication, it is meant  that, users communicate in loose and spontaneous way, without  following a specified curriculum or strict rules. Communication  takes place outside the official educational e-environments that are  IBM Lotus Quickr and Moodle, in case of HOU.  The research question is if there is a correlation between grades  and activity within a (informal) PLE that is used in the context of a  (formal) M.Sc course and this papers outlines the preliminary  results from    3. HOU2LEARN  Learning environments come from a period of establishment of  Learning Management Systems (LMSs) that give low  opportunities for interaction, content-creation and collaboration.  The emergence of Web 2.0 influenced also these environments  which became more social, collaborative and provide great vital   opportunities to create, maintain and redistribute content. They  also incorporate strong social networking characteristics as well as  a loosely structured collection of various widely used Web 2.0  tools such as fora, wikis, blogs, fora, glossaries, etc  [13,  14, 15,  17].  The Hellenic Open University (HOU) in order to let its students  experience a PLE, has set up HOU2LEARN (H2L). It supports and  in some cases, replaces the e-Learning process that takes place  through the officially and formally established channels of  traditional LMS environments and conferencing tools. It initially  launched in September 2010 as part of an ongoing research within  the Software Quality Research Group that studies the correlation  between Learning 2.0 challenges, social networking activities and  informal learning. The environment is based on the Elgg  framework (http://www.elgg.org) and provides useful widgets such  as file uploading and sharing, blogs, social bookmarks, personal  pages, polls and status updates. User customization is also  provided. H2L supports the creation of Groups with private or  public access in order to allow uses with common interests to meet,  collaborate and share content [17]. It was HOUs research interest  to use an informal environment such as HOU as a replacement of a  formal e-learning environment e.g. IBM Lotus Quickr and Moodle,  that HOU usually uses in order to support its (formal) classes.  The most essential feature of H2L is the social networking  component. Users are connected each other with the Follower  connection; each user that follows another, receives notifications  about content added or manipulated by the user or his/her activity  in general. This feature allows user to create a network of interest  with focus on his/her own personal needs, abilities and  requirements without the noisy environment of an open social  network platform [17].  In September 2011, the professors responsible for the postgraduate  course Special Issues on Software Engineering (PLH42) of  HOU, promoted H2L as the main environment of use for the  objectives of this course. A special group named PLH42 has been  established in order to allow students to have a common reference  point for content strongly related to this specific course. Professors  or other members of H2L could also be accepted as group  members under request. All PLH42 members were encouraged to  exploit the full potential of the environment and actively  collaborate with each other, professors and other users. All  questions were expressed an replied through H2L and all  communication was accomplished though H2L. Other activities  that H2L permitted were launching a new group discussion topic,  uploading a blogpost on the group blog, or a file on the group files  page, or a bookmark on the group bookmark page and all these  items can be commented by other group members [17].    4. RESEARCH AND TOOLS  During the academic year 2011-2012, the course PLH42 ran  completely through H2L instead of other well-established learning  environments such as Moodle. Students and instructors were  encouraged to register as members to H2L and then to follow the  Members Group dedicated to the course PLH42 named PLH42.  Behind that, the HOU learning analytics research team had set up  direct access to the H2L Database using Navicat Premium and then  Toad for MySQL 6.0.1. Database queries that extract all PLH42  members (nodes) along with the connections (edges) lead to tables  that had to be in format compatible with the SNA software.  For SNA, a notable number of tools (such as Gephi, Sentinel,  NodeXL, GraphViz, Ucinet, Touchgraph, GraphInsight) were   29    tested and based on their capabilities, user friendliness and the  requirements of the present study, NodeXL (1.0.1.209) was finally  used.    4.1 Learning Analytics  4.1.1 Social Network Analysis  Since H2L is an open educational social network, social  connections have been developed among the members  (learners/student, instructors/moderators and others). This is  initially depicted with a SNA diagram that presents the  connections of members of H2L that follow the Group PLH42,  the presented case study course. Every PLH42 member is  represented with a node and the member ID is also displayed.  Directed edges (arrows) have been used and the arrow from  member A to member B means that A follows B. Along with the  basis of SNA that is Connectedness, there is a number of metrics  that allow researchers to systematically slice up the social world,  creating a basis on which to compare networks, track changes in a  network over time and determine the relative position of  individuals and groups within a network [12, 18, 19].   The initial metrics just count the numbers of connections, while  some others become more complex and count centrality, density  transitivity, etc. In this paper, we focus on three SNA metrics [12,  18, 19]:  Indegree Centrality: Degree centrality counts the number of the  total number of connections linked to a node. It presents the  number of co-members a member is followed by. In case of  directed graphs, indegree centrality counts the number of edges  (arrows) that point inward at a node. This metric, in fact, measures  the popularity of the node since it measures the number of other  nodes that follow the node under measurement, and that want to be  aware of its activity.  Outdegree Centrality: This metric counts the number of edges  (arrows) that point outward to other nodes. Outdegree centrality  depicts the number of members, a member follows. It counts the  tension of the node to be connected to other nodes and to be aware  of their activities.  Betweeness Centrality: This is a more sophisticated SNA metric  that measures the brokering capability of a node. Although  popularity is important, it is vital to measure if a node facilitates  the transfer of an information (or content, or a virus, or a new, etc)  to its neighbors. According to Hansen et al. betweeness centrality  is a kind of bridge score and measures how much removing a  person would disrupt the connections between other nodes in the  network. Typically, it is a measure of how often a given node lies  on the shortest path between two nodes. Hence, nodes that are  included in many of the shortest paths between other nodes have a  higher betweeness centrality than those that are not included on  such paths.   4.1.2 Metrics  Aiming to permit learning trails and discoursing and content  trafficking to be measurable, all members activity is planned to be  quantified and operationalized using metrics [17]. These discourse  metrics aim to quantitatively analyze data from activities that take  place during the course lifecycle [6, 7, 8, 17] in order to be used  potentially for the improvement of the learning procedure. For  completeness reasons, this paper includes a set of metrics that aims  to measure the traffic and activity within the pages of the course  PLH42. These metrics which are part of ongoing research, are a   first step towards the setting of a stable baseline implementation  with which learning activity related metrics can be used for  customized and personalized tracking of the quality of learning  effects and the enhancement of the learning procedure [17].  According the aforementioned possibilities that H2L provides to  the members of course PLH42, the metrics intend to quantize the  trafficking content that is uploaded, commented and created. In  this case the term content includes files, bookmarks, comments,  blogposts, group discussion topics and wire posts. Considering that  the set of metrics is not closed yet, at this phase of our research the  proposed metrics follow:  Number of:     new bookmarks in Group   new blogposts in Group Blog   topics that each user has uploaded on Group Discussion.   uploads of new files on Group Files page   new bookmarks in Group   comments on bookmarks uploaded by other Group   members   comments on blogposts in Group Blog   comments on topics of Group Discussion   comments on files uploaded by other Group members   comments on wireposts of other Group Members   Of course this list can be modified, along with the course design,  i.e. in case a twitter widget is added after moderators request,  another metric could be number of twitter with #plh42 hashtag.  These metrics are included in this paper in order to outline the  general idea of the ongoing research, however they are planned to  be further elaborated in the next step of the research. This paper  focuses on the measurements of the SNA metrics along with the  formal grades in a M.Sc. class, in a preliminary level.   4.1.3 Formal Assessment: Grades  In PLH42, formal assessment was conducted using the grade ai,  i=1,..,6 of six formal written essays that were due during the  academic year along with the grade of the final exams e at the end  of the year. Essays and exams were graded using the 0-10 scale.  Students are allowed to participate in the exams if: > 30  The final grade g is calculated as following:  = 0.7  + 0.3    For academic year 2011-12, the minimum grade was 5.05 and the  maximum 9.76.   4.2 Grades as attribute of nodes  The initial concept was to take final grades (hereinafter, grades)  into consideration in order to illustrate the formal assessment into  social network analysis. This was conducted through the nodes  size: although nodes size is used to SNA metrics such as  betweeness centrality or degree, in this research, it illustrates the  grade of each learner. Considering that in NodeXL, nodes size is  between 1 and 100 and that it was intended all grades to be visible  but proportional to each other, the grades were normalized and  classified into 12 classes using statistics. For all 75 students, their  grade corresponds to the central value of a class (which is the   30    average value of the class boundaries), normalized to a scale from  1 to 50. For PLH42 H2L members, but not PLH42 students (i.e.  Tutors, other members), a unified size of 25 was applied. Tutors  were illustrated with a red solid square and other users with a blue  solid triangle (see Figure 1). For all graphs of this paper, the Harel- Koren Fast Multiscale layout arrangement algorithm was applied  [16]. With respect to privacy issues, all 83 members were denoted  with an internal ID and no names or usernames were used. The  student with the highest grade is marked with a black circle. For all  graphs of this paper, the Harel-Koren Fast Multiscale layout  arrangement algorithm was applied [25].   4.3 Centrality as attribute of nodes  In order to compare the performance and the centrality of the users,  two additional graphs were generated: Keeping the nodes position  as is, Figure 2 presents the SNA graph where the size node is  proportional to the indegree centrality normalized to a scale from 1  to 50. It appears that all tutors have high indegree centrality since  are popular in the student community. From the PLH42 students,  the member with the highest indegree centrality didnt have  significantly high final grade; his grade was lower than the average  grade of the course. His node is highlighted with a black circle.  In Figure 3, the nodes size is proportional to oudegree centratility  normalized to a scale from 1 to 50. Here, another user (in blue  solid triangle) appears to be the member with the highest degree;  This user was in charge with administrative duties. From the  PLH42 students, the member with the highest outdegree centrality  had also high final grade (her grade belonged to the highest class,  see 4.2) and her node is marked with a black circle.   Figure 1. H2L: Nodes size depicts the final grade.   Figure 4 illustrates the connections where the size node is  proportional to betweeness centrality; it appears that the node with  the highest brokering power is not a student of PLH and has  administrative duties. From the PLH42 students, the member with  the highest betweeness centrality had also high final grade (her  grade belonged to the highest class, see 4.2) and she was the same  person as in outdegree centrality. Her node is highlighted with a  black circle.   Figure 2. H2L: Nodes size depicts the   indegree centrality.   Figure 3. H2L: Nodes size depicts the   outdegree centrality.      5. DISCUSSION  FUTURE WORK  Since this paper presents a research context, there is a lot of  research interest beyond the presented graphs. It appears that high  grade of a member doesnt mean for sure high betweeness  centrality which expresses his/her brokering capabilities. This  means that in case of content by a given student with high grade, it  is not ensured that it would create significant impact to the others,  if his betweeness centrality of the given student was low.  Furthermore, it appears that the most popular members are the  tutors. However, this requires statistical elaboration in order to  investigate the correlation between grades and SNA metrics and  vital outcomes are expected to occur.  The correspondent data of   31    the next academic year 2012-13 will be also be captured and taken  into consideration.  Furthermore, in parallel, metrics presented in 4.1.2 are planned to  be elaborated and translated into SQL queries and to calculate the  values per member. This will be also combined with the SNA  metrics and the formal assessment measurements, in order to  measure both formal and informal parameters in the context of  application of learning analytics in a personal learning  environment. For example, measuring the activity of a member  through the SQL queries, it will be identified if the member with  the higher betweeness centrality (i.e. capability of brokerage) has  also high levels of content creation; if so, it is possible that he/she  has a vital role in the content distribution and then it could be  identified if he influences his/her contacts accordingly.  Finally, the measurement of formal and informal parameters are  planned to be used in a smart recommendation algorithm so as the  environment to provide more personalized and customized content  to each user according to the needs and weaknesses.     Figure 4. H2L: Nodes size depicts the    betweeness centrality.   6. REFERENCES  [1] Siemens, G., and Long, P. 2011. Penetrating the Fog:   Analytics in Learning and Education, EDUCAUSE Review  Archive, 46, 5 (Sept.-Oct. 2011), 31-40. Available at:  http://net.educause.edu/ir/library/pdf/ERM1151.pdf  (Retrieved: 15/04/2012).   [2] 1st International Conference on Learning Analytics and  Knowledge 2011, February 27-March 1, 2011 in Banff,  Alberta. Available at: https://tekri.athabascau.ca/analytics/  (Retrieved: 15/04/2012).   [3] 2nd International Conference on Learning Analytics and  Knowledge 2012, April 29  May 2, 2012 in Vancouver,  British Columbia, Canada, Available at:   http://lak12.sites.olt.ubc.ca// (Retrieved: 11/10/2012).   [4] Dawson, S. Heathcote, L., and Poole, G. 2010. Harnessing  ICT potential: The adaptation and analysis of ICT systems for  enhancing the student learning experience, In Proceedings of  International Journal of Educational Management, 24, 2,  116-128, DOI=10.1108/09513541011020936.   [5] Fournier, H., Kop, R., and Sitlia, H. 2011. The Value of  learning analytics to networked learning on a personal  learning environment. In Proceedings of the 1st International  Conference on Learning Analytics and Knowledge (Banff,  Alberta, Canada, February 27-March 1, 2011) LAK11  in Banff, Alberta, ACM, 104-109. ISBN: 978-1-4503-0944-8.   [6] Christakis, N., and Fowler, J. H., 2009. Connected: The  surprising power of our social networks and how they shape  our lives. Little, Brown and Company; First Edition edition  (Sept. 28, 2009). ISBN-10: 0316036145.   [7] Ochoa, X. and Duval, E., 2008. Relevance ranking metrics for  learning objetcs, IEEE Transactions on Learning  Technologies, 1, 1 (Jan.-Mar. 2008) 34-48.   [8] Ochoa, X. 2001. Learnometrics: Metrics for Learning  Objects, LAK11, In Proceedings of the 1st International  Conference on Learning Analytics and Knowledge (Banff,  Alberta, Canada, February 27-March 1, 2011) LAK11  in Banff, Alberta, ACM, 1-8. ISBN: 978-1-4503-0944-8.   [9] Downes, S. 2010. Collaboration, Analytics, and the LMS: A  Conversation with Stephen Downes, Campus Technology,  Oct. 14, 2010. Available at:  http://campustechnology.com/newsletters/ctfocus/2010/10/col laboration_analytics_and-the-lms_a-conversation-with- stephen-downes.aspx (Retrieved: 15/04/2012).   [10] Parry, M. 2010. Like Netflix, New College Software Seeks to  Personalize Recommendations. The Chronicle of Higher  Education, October 2010. Available at:  http://chronicle.com/blogs/wiredcampus/like-netflix-new- college-software-aims-to-personalize- recommendations/27642 (Retrieved: 15/04/2012).   [11] Kop, R. 2010. The Design of a Personal Learning  Environment: Researching the Learning Experience.  European Distance and E-Learning Network Annual  Conference 2010, June 2010, Valencia, Spain, Paper H4 32,  2010.    [12] Hansen, D. L, Schneiderman, B., and Smith, M. A. 2011.  Analyzing Social Media Networks with NodeXL, Insighits  from a Connected World. Morgan Kaufmann, imprint of  Elsevier, ISBN: 978-0-12-382229-1, 2011.   [13] Davis, I. 2005. Talis, Web 2.0 and All That, Internet Alchemy  blog, 4 July 2005. Available at:  http://blog.iandavis.com/2005/07/04/talis-web-2-0-and-all- that/ (Retrieved: 15/04/2012).   [14] Van Harmelen, M. 2006. Personal Learning Environments. In  Proceedings of the Sixth IEEE International Conference on  Advanced Learning Technologies, IEEE Computer Society,  2006, 815-816.   [15] Brown, J. S. and Adler, R. P. 2008. Minds on fire: Open  education, the long tail, and learning 2.0, Educause Review,  43, 1 (Jan.-Feb. 2008), 16-32.    [16] Harel, D. and Koren, Y.  2001. A Fast Multi-scale Method for  Drawing Large Graphs. In Proceedings of the 8th  International Symposium on Graph Drawing (Banff, Alberta,  Canada, May 08-12, 2007), GD '00. Springer-Verlag London,  UK, 2001, 183 - 196.   [17] Koulocheri, E., Soumplis, A., Xenos, M., 2012. Applying  Learning Analytics in an Open Personal Learning  Environment: A quantitative approach. In Proceedings of the  16th Pan-hellenic Conference on Informatics (Pireaus,  Greece, Oct 5-7, 2012) PCI 2012, 290-295. doi:  10.1109/PCi.2012.38.   [18] Scott, J., 2000. SNA: A handbook. Second Edition, SAGE  publications, ISBN0-7619-6339-1, UK.    [19] Wasserman, S., Faust, 1994. Social Network Analysis:  Methods and application, Cambridge University Press, ISBN  0-521-38707-8,UK.   32      "}
{"index":{"_id":"6"}}
{"datatype":"inproceedings","key":"Schreurs:2013:VSL:2460296.2460305","author":"Schreurs, Bieke and Teplovs, Chris and Ferguson, Rebecca and de Laat, Maarten and Buckingham Shum, Simon","title":"Visualizing Social Learning Ties by Type and Topic: Rationale and Concept Demonstrator","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"33--37","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460305","doi":"10.1145/2460296.2460305","acmid":"2460305","publisher":"ACM","address":"New York, NY, USA","keywords":"networked learning, social learning analytics, social network analysis, visualization","abstract":"Social Learning Analytics (SLA) are designed to support students learning through social networks, and reflective practitioners engage in informal learning through a community of practice. This short paper reports work in progress to develop SLA motivated specifically by Networked Learning Theory, drawing on the related concepts and tools of Social Network Analytics and Social Capital Theory, which provide complementary perspectives onto the structure and content of such networks. We propose that SLA based on these perspectives needs to devise models and visualizations capable of showing not only the usual SNA metrics, but the types of social tie forged between actors, and topic-specific subnetworks. We describe a technical implementation demonstrating this approach, which extends the Network Awareness Tool by automatically populating it with data from a social learning platform SocialLearn. The result is the ability to visualize relationships between people who interact around the same topics.","pdf":"Visualizing Social Learning Ties by Type and Topic:  Rationale and Concept Demonstrator   Bieke Schreurs1, Chris Teplovs2, Rebecca Ferguson3,   Maarten de Laat1 and Simon Buckingham Shum3     1 Open Universiteit NL  LooK, Postbus 2960   6401 DL Heerlen  The Netherlands   bieke.schreurs@ou.nl  maarten.delaat@ou.nl   2 Problemshift, Inc. &  University of Windsor  401 Sunset Avenue   Windsor, Ontario  Canada N9B 3P4   chris.teplovs@gmail.com   3 The Open University UK  Institute of Educational Technology   & Knowledge Media Institute  Milton Keynes, MK7 6AA, UK    r.m.ferguson@open.ac.uk  s.buckingham.shum@open.ac.uk       ABSTRACT  Social Learning Analytics (SLA) are designed to support students  learning through social networks, and reflective practitioners  engage in informal learning through a community of practice.  This short paper reports work in progress to develop SLA  motivated specifically by Networked Learning Theory, drawing  on the related concepts and tools of Social Network Analytics and  Social Capital Theory, which provide complementary  perspectives onto the structure and content of such networks. We  propose that SLA based on these perspectives needs to devise  models and visualizations capable of showing not only the usual  SNA metrics, but the types of social tie forged between actors,  and topic-specific subnetworks. We describe a technical  implementation demonstrating this approach, which extends the  Network Awareness Tool by automatically populating it with data  from a social learning platform SocialLearn. The result is the  ability to visualize relationships between people who interact  around the same topics.    Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   H.5.3 [Group and Organization Interfaces] Computer- supported cooperative work   General Terms  Design   Keywords  Networked Learning, Social Learning Analytics, Social Network  Analysis, Visualization     1. INTRODUCTION  An online network of learners in a formal or informal educational  context, or reflective practitioners in a community of practice, can   be regarded as constituting a web of social relationships that  reflects the flow of resources among them [1]. Examples include a  group acquiring competence in technology use by sharing  expertise, a community collectively building knowledge of its  history, plus information resources necessary to deal with new  situations [2].    Reflective practitioners, mentors and researchers could benefit  from answers to questions such as: Who learns from whom What  do they learn from each other What kinds of interactions take  place between people who learn together In which directions do  resources flow How frequently do learning interactions take  place How important are these interactions to the people  involved What value do these learning interactions create    From a learning analytics point of view, if it is possible to design  computationally tractable models of such learning networks, and  render them in coherent ways, analytics could draw attention to  potentially significant patterns based on the content, direction,  type and strength of interpersonal interactions. To provide  analytics for complex queries such as these, we need to design  structural signatures in our data models to serve as proxies,  which can be detected by humans and/or machines. In this short  paper, we report work in progress from combining OUNLs  research into the Network Awareness Tool (NAT) for visualizing  professional face-to-face informal learning networks [3-4], with  the OUs proposal that Social Learning Analytics are an important  class of analytic for participatory learning cultures [5].    In 2 we introduce Networked Learning theory, the paradigm  motivating this work. 3 considers the steps needed to move from  this to Social Learning Analytics software which satisfies the  theorys representational requirements. 4 then describes a  demonstrator tool which goes beyond seeing social networks in  topological terms (a well established approach), and seeks to show  (i) the topics of interest (possibly expertise) within the  community, and (ii) the nature of the social ties constituting the  network.    2. NETWORKED LEARNING THEORY  Networked learning theory is an emerging perspective that is  employed to understand learning by investigating how people  develop and maintain a web of social relations to support their  learning. Networked learning is a form of informal learning,  which involves people relying strongly on their social contacts for  assistance and development [6]. Recent research has linked  networked learning to an array of positive outcomes, including  student performance and school improvement [7-10]. Networked      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise,  or republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04      33    learning involves the use of information and communication  technology (ICT) to promote collaborative or cooperative  connections between learners, their tutors/instructors, and learning  resources [11]. The term networked learning was applied to  higher education to refer to ways in which new communication  technologies can influence teaching and learning [12-14].   As ICT drives increasingly varied forms of mediated collaboration  and contact, the field of networked learning seeks to provide  accounts of how learners appropriate these new tools to learn  informally on and through the Internet. The fields focus is on  how learners (or learning designers) can build and cultivate social  networks, seeing technology as just one (albeit critical) enabler,  rather than ICT-innovation as an end in itself [1, 15].    Networked learning focuses on the diversity of social  relationships that people develop, the strategies they use to  maintain them and the value this creates for learning. Networked  learning theory is closely linked to and uses methodologies of  social network theory, including social network analysis [7, 15].    Social network analysis considers networks to be made up of  nodes and ties. Nodes are the individual actors within a network  and ties are the relationships between these actors. The impact of  the structure of social networks can be studied on three levels: the  positions actors have in a network (individual dimension), the  relationships between actors in the network (ties dimension) and  the overall network structure (network dimension).   While social network theory highlights the structural dimensions  of learning networks, we also use social capital theory to frame  social network studies from the perspective of content. Networks  are always about something [6-7]. Social capital theory provides a  lens through which we can examine the relational resources  embedded in social ties and the ways in which actors interact to  gain access to these resources [16].    The first systematic analysis of social capital was produced by  Bourdieu [17], who defined the concept as the aggregate of the  actual or potential resources existing within the relationships of a  durable network. According to Lin [18], the common denominator  of all major social capital theories can be summarized as: The  resources embedded in social relations and social structure which  can be mobilised when an actor wishes to increase the likelihood  of success in purposive action. (p. 24).    A communities-of-practice perspective considers that networks, to  be fruitful and active, require a shared framework of values and  norms [2]. Learning within communities is a process within which  both individual and collective learning goals and agendas are  carefully and constantly negotiated in relation to a topic or domain  that is of interest to each participant [19-20].    3. TRANSLATION INTO ANALYTICS   The Networked Learning position outlined above serves to define  the broad set of phenomena considered to be important in  designing effective informal learning, drawing on disciplinary  perspectives and tools such as Social Network Analysis and  Social Capital Theory. In order to translate this into a Social  Learning Analytics software tool [5], there are minimally three  interdependent steps: data capture, analysis and visualization.    Data capture. Our work on NAT has focused, to date, on face-to- face professional learning, with participants manually  constructing their networks. NAT enabled them to see, literally  and usually for the first time, what these networks looked like,  and where new relevant colleagues might be. The transition to   Social Learning Analytics, with its focus on the appropriation of  social media for learning, required the integration of NAT with an  online learning platform, with the objective of generating NAT  visualizations automatically from social interactions logged in the  database. We selected the OUs SocialLearn system [5, 15] as a  data source, since we have complete control over the platform.  Analysis. It is necessary to translate the networked learning  concepts such as strength of social tie and social capital, and  levels of analysis such as individual, ties and network dimensions,  into a data model whose structures have the potential to answer  queries such as In which directions do resources flow  Visualization. None of this pays off unless stakeholders can  interact with the analytics that render their connected world more  visible [19]. Visualizing networked learning activities can also  assist strategic networked learning by helping learners to decide  which networks they should join and which experts they should  aim to connect with. Commonly used network visualization  software includes NetDraw [21], Gephi [22], NodeXL[23], JUNG  [24], Pajek[25] and several packages for R [26].    The approach taken in the design and development of the NAT  plug-in for SocialLearn differed from these in two major ways.   First, the software is designed to be used by participants who are  not network analysis experts or researchers.  Second, the  conceptual framework we have requires data capture and network  filtering by semantic content (topic or tag) as well as by the type  of the social tie, moving beyond undifferentiated nodes and ties.   4. NAT PLUGIN FOR SOCIALLEARN   In this section we describe how we visualize multiple levels  inherent in networked learning based on the learning activities  within SocialLearn. The plug-in, based on the Network  Awareness Tool [4], is designed to be compatible with any  modern web browser and was developed using widely available  JavaScript libraries.  For the SocialLearn plug-in, we have set up  the following framework, taking into account the theoretical  perspectives introduced above.    4.1 Visualizing the Network Structure  The NAT plugin visualizes the overall structure of the network to  the users. A graphical representation of the ego-network and  overall network structure is visualized reflecting the current state  of the network. The ego-network perspective is the network from  one node. The overall network structure is the total of all nodes.    The resulting network of actors, with multiplex ties, is laid out  using a force-directed layout algorithm. The resulting diagram is  often complex. To reduce this complexity, users are offered a  variety of ways in which they can zoom in on areas of interest,  filter out extraneous data, and request details of any particular  data [27]. In addition to these filters, users can request  computational assistance from the system to reposition the filtered  actors. In this case, the layout algorithm is re-applied to the data  points of interest. A screencast demonstrating the functionality of  the plug-in is online, as shown in Figure 1.   Social network theory considers that the constitution of a network  may influence the accessibility of information and resources and  that its social structure may offer potential for the exchange of  resources [28-29]. Understanding the structure of a network can  reveal the information flow within an online learning environment  [30]. Teams with the same skill composition can act differently  depending on the structure of relations within the team and,  similarly, individual can act differently depending on their  position within a network [8, 31].    34    To gain more insight into the tie level, we combine data about  frequency and quality with the social network analysis. This  supports investigation of the role of strong and weak ties in a  learning network. Combining data on the frequency and the  quality can be very valuable [31].    Levin and Cross [32] found that networked learners rely on weak  ties with competent people they can trust. Raegans and McEvily  [33] add that the transfer of tacit knowledge is a sensitive process  and therefore fewer people are able to engage in this process.  Strong ties are also important, because they are employed to  deepen and embed knowledge that is closely related to day-to-day  shared practice, as well as to build commitment to joint activities.   4.2 Visualizing the expertise and content   What is the focus of the network Which themes are discussed  Who is related to what theme Who is at the centre of that theme  This is represented in a tagcloud, reflecting the topics participants  have declared on their SocialLearn profile pages.   At an overall network level, learners can see in the general  tagcloud all learning topics associated with the whole community.  By clicking on a tag, learners filter the network and see only other  learners who have an interest in that learning theme. By  identifying topographically central people within the network,  they can identify the most active people, as well as potential  experts in the field. So learners can use the NAT plug-in as a  Social Learning Browser to locate people who are dealing with  the same learning topics. This is based on the logic of social  recommender systems, but most recommender systems are based  on people you may know through other connections, rather than  the thematic content around which people form relations.     Social learning is often mediated via artifacts, and social capital  can involve the exchanging of material resources. So both  SocialLearn actors and artifacts are used as sources of tags,  examples of artifacts being Questions posted to the Q&A site, and  Steps on a learning Path. For our analysis, tags on artifacts that  mediated associations between actors were added to the actors  themselves. For example, if Actor A posted a Thought (analogous  to a status update) with Tag T, and Actor B commented on that  Thought, then Tag T would also be associated with Actor B. In  this way we are able to visualize the flow of topics between  actors.    Because people learn through an active social process of meaning  construction [34], it is necessary to take the content of the   interactions into account. The kind of information that is  exchanged may influence the nature of the learning tie. While  most social networking sites focus on finding people with a  certain expertise, the NAT plug-in also focuses on finding people  with the same learning topic and learning problem.    4.3 Multiplexity of Learning Ties  Engaging in networked learning means that learners need to be in  touch with others in their network and need to build the  networked connections that are required to participate in  constructive conversations [4]. However, this is not easy because  networked learning is a complex process situated in a changing  context. It is difficult to pare this process down to one or two  variables. A learning relation is a multiplex set of relations all  acting at the same time.      Figure 1: Using NAT to visualize and filter social ties by person, type of tie, and topic. Due to space limitations, this is a composite  image showing the entire unfiltered network, but when a user is selected this filters the left-panel as shown, to display only her  ego-network and topics. Screencast: http://bit.ly/NAT-SocialLearn   35    In many analyses of social networks, ties between actors are  differentiated only in terms of their relative strength. The  SocialLearn platform supports a wide variety of actions that can  result in the creation of ties between actors. Here we describe  responding ties, follower ties and friendship ties (which as  explained, can be further contextualized through the use of tags).   Actions that can result in interaction ties between actors include  responding to materials contributed by another actor (typically  through commenting on or replying to postings). Thus, if Actor B  comments on a posting by Actor A there would be a respond  type tie from Actor B to Actor A.   Ties can be used to describe relations between actors in  SocialLearn. Two types of actions were used to generate relations:  friending (i.e. identifying another actor as someone who is a  colleague, acquaintance or friend) and following (i.e. identifying  that you want to be notified of the activities of another actor).  Both these actions serve to indicate that one actor is in some way  interested in another actor. Relations are directional and  potentially non-symmetric (e.g. Actor A can identify Actor B as a  friend without Actor B identifying Actor A as a friend).    We found it important to include these friend and follower  learning relationships because learning can be supported if  relations between students in the network are characterized by  trust, openness and confidence [35]. According to Argyris and  Schn [36], trust and openness in social relations make it possible  to test theories, experiences and practices. Borgatti and Cross [37]  found that students are most likely to seek information from work- related experts who they believe will not make them feel  uncomfortable. Figure 2 shows how the network visualization can  be filtered and thus redrawn by combining different ties.   5. SUMMARY AND FUTURE WORK  To summarise, we have presented our Networked Learning  theoretical perspective, and described a demonstrator which  begins to show how this can be translated into a Social Learning  Analytics tool. The NAT plug-in for SocialLearn visualizes  networks by identifying relationships between people who interact  around the same learning topics. From an ego-perspective  learners can see their own learning network, consisting of their  friends, followers and other learners with whom they have  interacted. This means that the NAT plug-in has the potential to  provoke learning-centric reflection by learners on how they use  their peers for learning. Learners can also see the content of the  ties, summarized in one or more tags.    Educators can use the plug-in to guide students in the  development of networked learning competences and to gain  insight into the ability of groups of students to learn collectively  over time. Using this plug-in, educators can detect multiple  (isolated) networks within the online learning environment,  connect ideas and foster collaboration beyond existing boundaries.    The visualizations and network data can be used to carry out  social network analysis of the density of a network, including the  centrality of persons within a network, the structure, cliques, etc,  in real time or over a specified period. For researchers, the  analysis of learning ties and networks helps clarify how  professionals engage in learning relationships, as well as the value  of this engagement.    This work is at an early stage. We have completed one iteration to  put the representational infrastructure in place, which now opens  up many possible lines of enquiry. More research is needed to   investigate which sets of ties can predict or stimulate learning. It  may prove possible to apply the theory of Borgatti and Cross [37]  to an online learning environment and to investigate whether  people who are friends are more likely to seek information from  each other.   We have not yet analysed whether different ties yield  sytematically different structures. Qualitative research is needed  to interview actors about their perceptions of their learning  networks, their (and mentors) reactions to these visualizations.  What do learners themselves perceive as the best types of learning  tie Does the content of ties influence the structure of the learning  network of which it forms a part, and does it help us track the  flow of social capital within a network   We plan to develop the NAT plug-in further in order to make it  possible to conduct temporal SNA in order to study network  dynamics. A replay tool should help see the growth of the  overall network and changes in the networked learning behaviour  of individual students. Do students find more peers to learn from  using the NAT plug-in    6. ACKNOWLEDGEMENTS  We gratefully acknowledge The Open University for making this  work possible through a SocialLearn Project Internship awarded  to the first author.     Figure 2: A subnetwork already filtered by topic. Actors  may be connected by any combination of the ties friend /  follow / respond, reflected in the combination of colours on  the links (top). Link thickness reflects quantitative  strength (e.g. many blue responses between actors).  Filtering on just friend ties (lower) refreshes the network  layout, revealing a different structure in which actors may  become more central/peripheral.   36    7. REFERENCES  1. Lave, J., & Wenger, E. 1991. Situated learning- Legitimate   peripheral participation. Cambridge University Press,  Cambridge, United Kingdom.   2. Wenger, E., Trayner, B., & De Laat, M. 2011. Telling stories  about the value of communities and networks: A toolkit.  Ruud de Moor Centrum, Open Universiteit.   3. De Laat, M.F., & Schreurs, B. (2011). Network Awareness  Tool: Social software for visualizing, analysing and  managing social networks. Ruud de Moor Centrum, Open  Universiteit Nederland, Heerlen.   4. Schreurs, B., & de Laat, M. (2012). Network Awareness  Tool  Learning Analytics in the workplace: Detecting and  Analyzing Informal Workplace Learning. Paper presented at  LAK12: 2nd International Conference on Learning Analytics  and Knowledge (30 April - 2 May), Vancouver, Canada.   5. Ferguson, R., & Buckingham Shum, S. (2012). Social  Learning Analytics: Five Approaches. Paper presented at  LAK12: 2nd International Conference on Learning Analytics  and Knowledge (30 April - 2 May), Vancouver, Canada.   6. Jones, C., Asensio, M., & Goodyear, P. 2000. Networked  learning in higher education: practitioners perspectives.  Journal of the Assoc. for Learning Technology, 8, 2, 18-28.   7. Coburn, C. E., & Russell, J. L. 2008. District policy and  teachers social networks. Education Evaluation and Policy  Analysis, 30, 203-235.   8. Moolenaar, N. M., Daly, A. J., & Sleegers, P. J. C. in press.  Ties with potential: Social network structure and innovative  climate in Dutch schools. Teachers College Record.   9. Penuel, W. R., Riel, M., Joshi, A., Pearlman, L., Kim, C. M.,  & Frank, K. A. 2010. The alignment of the informal and  formal organizational supports for reform: Implications for  improving teaching in schools. Educational Administration  Quarterly, 46, 1, 57-95.   10. Pil, F., & Leana, C. 2009. Applying organization research to  public school reform. Acad. Mngmnt Jnl., 56, 2, 1101-24.   11. Goodyear, P., Banks, S., Hodgson, V. & McConnell, D.  2004. Advances in research on networked learning. Kluwer  Academic Publishers, Norwell, MA.   12. Siemens, G. 2004. Connectivism: A learning theory for the  digital age, International Journal of Instructional Technology  and Distance Learning. (Nov.2006), 226.  http://www.itdl.org/Journal/Jan_05/article01.htm    13. De Laat, M. 2006. Networked learning. Politieacademie,  Apeldoorn.   14. Steeples, C. and Jones, C. (Eds.) 2002 Networked learning:  Perspectives and issues. Springer, London.    15. Ferguson, R., & Buckingham Shum, S.(2012). Towards a  social learning space for open educational resources. In A.  Okada, T. Connolly, & P. Scott (Eds.), Collaborative  Learning 2.0: Open Educational Resources (pp. 309-327):  IGI.   16. Nahapiet, J. 2009. Capitalizing on connections: social capital  and strategic management. In Social capital: reaching out,  reaching in, V.O. Bartkus and J.H. Davis, Editors. Edward  Elgar Publishing, Cheltenham.    17. Bourdieu, P. 1985. The social space and the genesis of  groups. Theory and Society, 14, 6, 723-744.   18. Lin, N. and J. Smith. 2009. A Theory Of Social Structure And  Action. Cambridge University Press, Cambridge.   19. De Laat, M.F. 2011. Bridging the knowledge gap: Using  Social Network Methodology for Detecting, Connecting and  Facilitating Informal Networked Learning in Organizations.  Paper presented at the 44th IEEE Annual Hawaii  International Conference on System Sciences, Kuaui, HI.   20. De Laat, M. 2006. Networked learning. Politieacademie,  Apeldoorn.   21. Borgatti, S.P., 2002. NetDraw Software for Network  Visualization. Analytic Technologies: Lexington, KY   22. Bastian M., Heymann S., Jacomy M. (2009). Gephi: an open  source software for exploring and manipulating  networks. International AAAI Conference on Weblogs and  Social Media.   23. Smith, M., Milic-Frayling, N., Shneiderman, B., Mendes  Rodrigues, E., Leskovec, J., Dunne, C., (2010). NodeXL: a  free and open network overview, discovery and exploration  add-in for Excel 2007/2010, http://nodexl.codeplex.com  Social Media Research Fndn: http://www.smrfoundation.org    24. Madadhain, J., Fisher, D., Smyth, P., White, S.,  Boey, Y.B.  (2005).  Analysis and visualization of network data using  JUNG . Journal of Statistical Software: 125.   25. Batagelj V., Mrvar A.: Pajek - Analysis and Visualization of  Large Networks. in Jnger, M., Mutzel, P., (Eds.) Graph  Drawing Software. Springer, Berlin 2003. p. 77-103.   26. R Development Core Team (2008). R: A language and  environment for   statistical computing. R Foundation for  Statistical Computing, Vienna, Austria. http://www.R- project.org.   27. Shneiderman, B. 1996. The Eyes Have It: A Task by Data  Type Taxonomy for Information Visualizations.  In Proceedings of the 1996 IEEE Symposium on Visual  Languages (VL '96). IEEE Computer Society, Washington,  DC, USA, 336-343.   28. Scott, J. and P. Carrington. 2010. Handbook of Social  Network Analysis. Sage, London.   29. Wasserman, S. and K. Faust. 1994. Social Network Analysis.  Cambridge University Press, Cambridge.   30. Daly, A..J. 2010. Relationships in reform: the role of  teachers' social networks. Journal of Educational  Administration, 48, 3, 359-391.   31. Granovetter, M. 1979. The strength of weak ties. American  Journal of Sociology, 78, 1360-1380.   32. Levin, D. Z. & Cross, R. 2004. The strength of weak ties you  can trust: The mediating role of trust in effective knowledge  transfer. Management Science, 50, 11, 1477-1490.   33. Reagans, R. & McEvily, B. 2003. Network structure and  knowledge transfer: The effects of cohesion and range.  Administrative Science Quarterly, 48, 240-267.   34. Elliot Soloway, Mark Guzdial, and Kenneth E. Hay. 1994.  Learner-centered design: the challenge for HCI in the 21st  century. Interactions 1, 2 (April 1994), 36-48.    35. Bottrup, P. 2005. Learning in a network: A third way  between school learning and workplace learning Journal of  Workplace Learning, 17(8): 508521   36. Argyris, C. & Schon, D. A. (1996). Organizational learning  Il: Theorv, method and practice. Reading. MA: Addison- Wesley.    37. Borgatti, S.P. and Cross, R. 2003. A Relational View of  Information Seeking and Learning in Social Networks.  Management Science. 49(4):432-445.   37      "}
{"index":{"_id":"7"}}
{"datatype":"inproceedings","key":"Southavilay:2013:ACW:2460296.2460307","author":"Southavilay, Vilaythong and Yacef, Kalina and Reimann, Peter and Calvo, Rafael A.","title":"Analysis of Collaborative Writing Processes Using Revision Maps and Probabilistic Topic Models","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"38--47","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460307","doi":"10.1145/2460296.2460307","acmid":"2460307","publisher":"ACM","address":"New York, NY, USA","keywords":"author-topic models, collaborative writing processes, probabilistic topic, visualisation","abstract":"The use of cloud computing writing tools, such as Google Docs, by students to write collaboratively provides unprecedented data about the progress of writing. This data can be exploited to gain insights on how learners' collaborative activities, ideas and concepts are developed during the process of writing. Ultimately, it can also be used to provide support to improve the quality of the written documents and the writing skills of learners involved. In this paper, we propose three visualisation approaches and their underlying techniques for analysing writing processes used in a document written by a group of authors: (1) the revision map, which summarises the text edits made at the paragraph level, over the time of writing. (2) the topic evolution chart, which uses probabilistic topic models, especially Latent Dirichlet Allocation (LDA) and its extension, DiffLDA, to extract topics and follow their evolution during the writing process. (3) the topic-based collaboration network, which allows a deeper analysis of topics in relation to author contribution and collaboration, using our novel algorithm DiffATM in conjunction with a DiffLDA-related technique. These models are evaluated to examine whether these automatically discovered topics accurately describe the evolution of writing processes. We illustrate how these visualisations are used with real documents written by groups of graduate students.","pdf":"Analysis of Collaborative Writing Processes Using  Revision Maps and Probabilistic Topic Models Vilaythong Southavilay*, Kalina Yacef*, Peter Reimann+, Rafael A. Calvo^  * School of Information Technologies   University of Sydney  vstoto@it.usd.edu.au,   kalina.yacef@sydney.edu.au   + Centre for Research on Computer-  supported Learning and Cognition  University of Sydney   peter.reimann@sydney.edu.au   ^ School of Electrical and Information   Engineering  University of Sydney   rafael.calvo@sydney.edu.au     ABSTRACT  The use of cloud computing writing tools, such as Google Docs,  by students to write collaboratively provides unprecedented data  about the progress of writing. This data can be exploited to gain  insights on how learners collaborative activities, ideas and  concepts are developed during the process of writing. Ultimately,  it can also be used to provide support to improve the quality of the  written documents and the writing skills of learners involved. In  this paper, we propose three visualisation approaches and their  underlying techniques for analysing writing processes used in a  document written by a group of authors: (1) the revision map,  which summarises the text edits made at the paragraph level, over  the time of writing. (2) the topic evolution chart, which uses  probabilistic topic models, especially Latent Dirichlet Allocation  (LDA) and its extension, DiffLDA, to extract topics and follow  their evolution during the writing process. (3) the topic-based  collaboration network, which allows a deeper analysis of topics in  relation to author contribution and collaboration, using our novel  algorithm DiffATM in conjunction with a DiffLDA-related  technique. These models are evaluated to examine whether these  automatically discovered topics accurately describe the evolution  of writing processes. We illustrate how these visualisations are  used with real documents written by groups of graduate students.   Categories and Subject Descriptors  I.2.7 [Artificial Intelligence]: Natural Language Processing  text  analysis. K.3.1 [Computers and Education]: Computer Uses in  Education  Collaborative Learning.   General Terms  Algorithms, Design, Experimentation.   Keywords  Collaborative writing processes, visualisation, probabilistic topic  and author-topic models.   1. INTRODUCTION  Collaborative writing (CW) is an essential skill in academia and  industry. It combines the cognitive and communication  requirements of writing with the social requirements of  collaboration. Cognitive studies show that CW challenging in all  these aspects [6]. Yet, CW is not explicitly taught in the school or  higher education systems. Providing support on the processes of  CW can be useful to improve not only the quality of the  documents but also, more importantly, the CW skills of those  involved.   Our project explores how to support collaborative writing skills  by providing feedback to students and teachers about that process.   We provide feedback in the form of mirroring visualisations [8,  21] improving the awareness of a groups writing activities, The  aim is that individual students can perform their collaborative  writing tasks more efficiently and effectively and teachers may  monitor groups more effectively and detect problems early.   There is disparate prolific research for improving the support of  quality writing such as tools for automatic scoring of essays [15],  visualization of documents [11, 22], automatic question  generation [10] and document clustering [1]. However, these  approaches, unlike ours, focus on the final product, not on the  writing process itself which can be used to gain insight on how  student write their documents.   The use of cloud computing tools, such as the collaborative  writing tool Google Docs, is spreading in workplaces and  classrooms. One important benefit of such tools, beyond the fact  that they allow authors to edit text anywhere at any time and  collaborate seamlessly, is that they store all the revisions and the  metadata (i.e. timestamps and authorship) of the documents. This  gives unprecedented historical data of all the text edits made by  students as they write. We exploit this data to gain insights on the  process that students follow to write their documents. Particularly,  we investigate and extract useful information for teachers and  students about CW processes.   In this paper, we propose three visualisation approaches and their  underlying techniques for analysing writing processes. We first  create revision maps showing a snapshot of text edits performed  by students on their jointly authored documents. This  visualization depicts the development of documents at the  paragraph level over a period of time. Based on text edits made to  the paragraphs, we then extract topics by using several types of  probabilistic topic models. We use the topic evolution charts to  gain insights on how topics are created and developed during  writing processes. Finally, we develop topic-based collaboration  networks to analyse student collaboration based on their written  topics. The topic-based collaboration networks present network  diagrams showing students who commonly write about the same  topics during their writing tasks.   This paper is organised as follows. First, we review related work  in Section 2. Then, we outline an overview of our approach in  Section 3. In Section 4, 5 and 6, we present our revision maps,  topic evolution charts and topic-based collaboration networks,  respectively. We validate our techniques with simulated data in  Section 7. We then illustrate the applicability of our techniques  using real word data of documents written by graduate students in  Section 8. Finally, discussion and conclusion are provided in  Section 9.  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   38       Figure 1. Architecture framework of our approach.   Abbreviations: Google Documents List API (GD API), Latent Dirichlet Allocation (LDA)  2. RELATED WORK  The recent work by Perrin and Wildi [12] investigated the  dynamics of cursor movement during the process of writing and  presented the movement in progression graphs. Based on the  graphs, they proposed a statistical method to discover writing  stages that authors went through. Particularly, they considered the  progression graphs as time series consisting of large bursts of  signals and used statistical signal extraction to decompose the  series into sequences of interesting features. Writing stages are  then extracted based on the changes of the features. However, this  analysis focused only on the temporal dynamics of cursor  movements, not on text edits in the content. Unlike this work, we  are interested in how the text content changes over time for  deriving the writing processes. Another work by Kim and  Lebanon [9] proposed a novel representation of document  versions based on the location of the content words. They built a  matrix in which columns correspond to position of each word and  rows represent versions (time). They used the space-time  smoothing method to extract and visualise changing in words over  time [9]. Based on these changes, they discovered revision  patterns. Although this method can discover which parts (i.e. word  contents) of documents change over time, it does not incorporate  topic evolution and author information in the visualisation, which  we propose.   Another writing process visualisation was proposed by Caporossi  and Leblay [5]. The visualisation is based on the graph theory that  captures the overview of the writing processes. The graph of a  writing process consists of nodes and links. The size and colour of  each node indicate the number of text edits it represents and their  nature respectively. For instance, yellow nodes represent additions  that have later been removed, whereas the red ones depict  additions that remain until the final text, and the blue nodes mean  deletions. These nodes are connected by links or edges  representing a spatial or temporal relationship, indicated by the  shape and color of the edges. From the graphs, writing processes  can be analysed. This representation technique of writing process  can handle moving text position. This technique is useful for  analyzing writing processes of documents written by single  authors, but does not apply to collaborative writing contexts.        3. APPROACH  In this paper, we extended the framework proposed in [16] in  order to visualising and analysing writing processes based on text  edits and topic evolutions. Figure 1 depicts the architecture of our  approach. It consists of a writing environment, Google Docs in the  front end with Google Documents List API (GD API) for  retrieving revisions and their information; a text comparison  utility; Topic Model, and Author-Topic Model components.   In Google Docs, each document created is uniquely assigned a   document identification number (document ID). Google Docs also   keeps track of all version numbers (revision ID) of each document.   Every time an author makes changes and edits a particular  document, Google Docs stores the edited text content of the  document as well as a record consisting of the following  information in the revision history of the document:    The version number (revision ID).    The identification of the author (author ID).    The timestamp (date and time) of the committed  changes.   Some technical adjustments need to be made: for instance, several  authors can make changes to the same content at almost the same  time., but, the GD API only provides one main author for each  revision. Therefore, we have to manually reconcile the list of  authors for each revision by using the revision history function on  the web interface of Google Docs.   There are three kinds of visualisations generated. The first  visualisation, which is called a revision map, depicts text edits  performed on individual paragraphs during writing processes. In  order to understand the semantic of these text edits, the second  visualisation, called a topic evolution chart, depicts how written  topics were created and developed during writing processes. The  third visualisation, called a topic-based collaboration network,  shows the network of authors collaboration based on topics. For  instance, it displays whether any two authors have written the  same topics during their writing tasks. These three visualisations  will be detailed in the next three sections.   As depicted in Figure 1, after retrieving the text contents of all the  revisions and the revision history for a particular document, we  use the text comparison utility to identify the text edits in  successive revisions and identify the list of added and deleted  texts. These identified text edits, mapped against the list of author  IDs and the timestamps of corresponding revisions, are used in the  revision map.  They are also used as input to both the topic and  author-topic modelling algorithms. The topic modelling,  especially DiffLDA [19], creates the topic evolution chart, while  the author-topic modelling [13-14] outputs the topic-based   collaboration network using author IDs provided in the revision  history. The detail of how these three visualisations are created  will be described in the next three sections.    4. REVISION MAP  In order to gain insights on how students develop their jointly  authored document over a period of time, it is useful to present a  chronological picture of what happened during the writing of the  document. Revision maps summarise the text edits made by   39    students at the paragraph level over their period of writing. Figure  2 depicts the revision map of a real document written by a group  of students. Each column refers to a revision of the document.  Each small rectangle depicts a paragraph of the document. Each  row shows the evolution of an individual paragraph over time, as  it is created, altered, or deleted during the writing process.  Rectangles are colour-coded to visualise the nature and the extent  of the edits made to the paragraph: green means more words were  added than deleted, red means more words were deleted than  added, and white represents no change in the paragraph. The  intensity of these colours approximately denotes the extent of  those edits. If there are as many added words as deleted ones, the  rectangle is coloured in yellow-green. Lastly, the horizontal bar  under author IDs row shows the aggregated  revisions and the last vertical column represents the aggregated  edits of individual paragraphs across all revisions during the  writing process.   Each paragraph evolution is positioned relative to its position in  the current (final) revision. This means that evolution rows can move up and down over time, especially when  a new paragraph is added. In addition, the paragraph evolutions  were grouped into sections based on the structure of the  document.   For instance, the revision map shown in Figure 2  edits in a document written by a group of students over a period of  6 days (from 04 to 09/05/2011). The text edit P1, P2, P3 and P4 (as indicated in the revision map) can be  described as following. The first paragraph of Section A (P1) was added by c1 on 04/05/2011 22:29 with lots of words. It  edited until 06/05/2011 16:48 (by author  words were deleted than added to it. Toward (on 08/05/2011 21:46), P1 was modified again  words added. The first paragraph of Section B (P2) was inserted  on 05/05/2011 13:57 by author c2. It was not was deleted altogether from the document on 09/05/2011 02:38 c2. A new paragraph (P3) was inserted  removed. A paragraph can be split and merged. For instance, the  paragraph, P4 was inserted on 05/05/2011 13:57 changed once, with few words added on 06/05/2011 16:48 P4 was then split into two paragraphs on 09/05/2011 02:38  Revision maps can help provide answers to the following questions:   1. Which sections of the document were the most or the least  worked on (Location of text edits)   2. When (at what dates) did major edits (i.e. addition and  deletion) occur during the writing process  3. Did students work sequential or in parallel single paragraphs were written at different writing sessions or  days, or parallel  many paragraphs were written almost in  parallel at the same writing sessions or days).   4. Who made the most or the least edit (Authorship)   5. How many authors worked on each paragraph and each  section (Collaboration)      period of writing. Figure  real document written by a group   column refers to a revision of the document.  Each small rectangle depicts a paragraph of the document. Each  row shows the evolution of an individual paragraph over time, as  it is created, altered, or deleted during the writing process.   the nature and the extent  means more words were   means more words were deleted than  represents no change in the paragraph. The   s approximately denotes the extent of  s. If there are as many added words as deleted ones, the   Lastly, the horizontal bar  under author IDs row shows the aggregated edits of individual   vertical column represents the aggregated  s of individual paragraphs across all revisions during the   paragraph evolution is positioned relative to its position in  This means that the paragraph   can move up and down over time, especially when  a new paragraph is added. In addition, the paragraph evolutions  were grouped into sections based on the structure of the   Figure 2 represents the  s in a document written by a group of students over a period of   edits of four paragraphs:  P1, P2, P3 and P4 (as indicated in the revision map) can be  described as following. The first paragraph of Section A (P1) was   on 04/05/2011 22:29 with lots of words. It was not  author c1), in which more   words were deleted than added to it. Towards the end of the week  (on 08/05/2011 21:46), P1 was modified again by c1 with more   added. The first paragraph of Section B (P2) was inserted  It was not modified at all and   from the document on 09/05/2011 02:38 by  . A new paragraph (P3) was inserted by c2 after P2 was   aragraph can be split and merged. For instance, the  paragraph, P4 was inserted on 05/05/2011 13:57 by c2. It has been   with few words added on 06/05/2011 16:48 by c1.  it into two paragraphs on 09/05/2011 02:38 by c2.   can help provide answers to the following five   were the most or the least   did major edits (i.e. addition and  deletion) occur during the writing process (Time)   parallel (i.e. sequential   single paragraphs were written at different writing sessions or   many paragraphs were written almost in  parallel at the same writing sessions or days).    ost or the least edits to the document   each paragraph and each   Figure 2. Revision map of a real document written by a   of five students: c1, c2, c3, c4, and c5.   administrator  First, by examining the vertical bar showing the aggregated  of individual paragraphs, we can easily answer Question 1.  Particularly, we learn that Section A of the document has been  edited more than Section B. The second intere based on Question 2 above is that most of text  text additions happened at the beginning of the process when  students started their writing tasks. There were some extensive  text edits (only by author c3) in the middle of  addition, there were many text deletion writing process. For Question 3, it is interesting to note that  students wrote their document sequentially for Section A,  especially the first three paragraphs by two students contrast, paragraphs of Section B were created almost at the same  time (by student c2). However, these paragraphs of Section B did  not make the final revision. All of them were replaced at the last  stage of the writing.    The revision map also provides an insight on how student  collaborate over the period of writing, in which Question 4 and 5  can be used as a guideline. Among the five students, we observe  that c4 involved the least in the development of the document.  worked mainly on Section B during the period of writing. In  addition, c1 and c5 collaborated a lot to develop their documents,  especially in Section A.   In Figure 2, although c4 perform very little of text edition  comparing relatively to other four students, it is quite difficult to  conclude that c4 contributed the least to the development of the  document. Particularly, we would like to know if small text  editions of c4 increase the assignment of a topic, and thus make  the content text clearer and improve the coherent of the content.  This initial analysis provides  created their jointly authored document will now look at ways to investigate during writing, especially how their      evision map of a real document written by a group   of five students: c1, c2, c3, c4, and c5. ad is the   administrator.   First, by examining the vertical bar showing the aggregated edits  of individual paragraphs, we can easily answer Question 1.  Particularly, we learn that Section A of the document has been  edited more than Section B. The second interesting observation  based on Question 2 above is that most of text edits, especially   happened at the beginning of the process when  students started their writing tasks. There were some extensive   3) in the middle of the process. In  text deletions towards the end of the   writing process. For Question 3, it is interesting to note that  students wrote their document sequentially for Section A,  especially the first three paragraphs by two students, c1 and c5. In  contrast, paragraphs of Section B were created almost at the same   2). However, these paragraphs of Section B did  not make the final revision. All of them were replaced at the last   o provides an insight on how student  collaborate over the period of writing, in which Question 4 and 5  can be used as a guideline. Among the five students, we observe   4 involved the least in the development of the document. c2  n B during the period of writing. In   5 collaborated a lot to develop their documents,   4 perform very little of text edition  ther four students, it is quite difficult to   4 contributed the least to the development of the  document. Particularly, we would like to know if small text   4 increase the assignment of a topic, and thus make  rer and improve the coherent of the content.   provides some insight on how students  their jointly authored document at the paragraph level. We   will now look at ways to investigate how they developed ideas  their topics evolved over time.   40    5. TOPIC EVOLUTION CHART  Knowing how topics have evolved during text edits can provide  help in understanding how students developed their ideas and  concepts during their writing tasks. This is addressed by our topic  evolution chart, shown in Figure 3 with four topics (T1, T2, T3,  and T4). A topic consists of a cluster of words that frequently  occur together in a revision, and each document revision is  represented by a set of topics. The topic evolution chart depicts  changes in the membership of topics throughout the sequence of  revisions. For instance, in Figure 3, T1-T3 appeared at the start of  writing (Revision 1), whereas T4 emerged in the sixth revision  (Revision 6) and disappeared later in the writing process  (Revision 7). The ratio of importance of the other three topics (T1- T3) changes over time. In the beginning of the writing process, the  document contains more text about topic T1 then about T2 or T3  (66% vs 17%.) However, towards the end of the process, topic T2  is more dominant in the document than T1 and T3.      Figure 3. A topic evolution chart of three topics: T1, T2, T3,   and T4 over 17 revisions.   Latent Dirichlet Allocation (LDA) [3] is a popular probabilistic  topic modeling technique, which, to our knowledge, has not been  used to extract the evolution of topics during the writing of the  same document. The closest method to do so is DiffLDA [19],  which has been used for extracting topic evolution in software  repositories. In DiffLDA, the GNU diff utility was used to identify  text edits at the line level before using LDA. We build on these  techniques to extract topics and their evolution during the writing  process.   In our work, we developed a text comparison utility to extract text  edits at both paragraph and word levels. Unlike DiffLDA, the   number of topics and hyper-parameters,  and  (of the two  Dirichlet distributions: authors topic distribution and topic- specific word distribution), are selected using a trade-off between  the model fitting (i.e. perplexity) and the simplicity of model  structure (i.e. the smallest number of topics). Particularly, the  number of topics is selected independently for each document. In  the following subsections, we will first give an overview of the  probabilistic topic model, Latent Dirichlet Allocation (LDA) and  DiffLDA before describing how we extend DiffLDA for mining  topic evolution of writing processes.   5.1 Probabilistic Topic Models  Topic modeling automatically discovers topics within a corpus of  text documents [2] in which topics are collections of words that  co-occur frequently in the corpus. Due to the nature of language  usage, the words that constitute a topic are often semantically  related. Each document is represented as a probability distribution  over some topics, while each topic is represented as a probability  distribution over a number of words. LDA defines the following  generative process for each document in the collection:   1. For each document, pick a topic from its distribution over  topics.   2. Sample a word from the distribution over the words  associated with the chosen topic.   3. Repeat the process for all the words in the document.   More formally, in the generative process, LDA infers, for each of   T topics, an N-dimensional word membership vector z(1:N) that  describes which words appear in topic z, and to what extent. In  addition, for each document d in the corpus, LDA infers a T-  dimensional topic membership vector d(1:T) that describes the   extent to which each topic appear in d. Both  and  have   Dirichlet prior with hyper-parameters  and , respectively. LDA  performs these inferences using Bayesian techniques such as  collapsed Gibbs sampling, a Markov-chain Monte Carlo (MCMC)  method, which is currently in widespread use as an inference tool  among topic modelers [7].   Topic evolution models using LDA suffers the duplication effect  as explained in [18]. These topic evolution models have an  assumption that documents in the corpus are unique across time.  This assumption holds for the collection of journals, blog posts,  and newspaper articles, which are typically studied in the topic  modeling. It is very unlikely that an article published in one year  is only slightly updated and republished the next year in the same  conference proceeding. Instead, each article (i.e. the specific  combination of words within article) is unique across time.  However, the nature of writing process is quite different. Jointly  authored documents are usually updated incrementally from one  revision to another revision as authors developed the documents.  Although sometimes there can be lots of text edits occurring in  one revision, there still exists some overlap of text contents  between the revision and the previous one. This particularity has  been addressed with DiffLDA, which is described next.   5.2 DiffLDA for Mining Writing Processes  In order to address the data duplication effect found in software  repositories, Thomas et al. [18-19] proposed a simple technique  used in the pre-processing step before applying LDA to their  source codes.  On top of the normal pre-processing steps, they  included the diff step to identify text edits between every pair of  two successive versions of each source code. In particular, for  every pair of successive versions, DiffLDA uses the standard  GNU diff utility to compute the edits (i.e. add, delete or change)  at the line levels. According to DiffLDA [19], if an existing line is  changed, it is considered to be deleted and then added again.  Identified edits (added and deleted lines) were then used as  documents, called delta documents [19]. The corpus then  consisted of all delta documents in the software repository. This  diff step effectively removes all duplication and thus prevents the  duplication effect when applying LDA for the corpus.   However, the pre-processing step used in DiffLDA can not be  applied directly in our work. In writing processes, it is common  that authors revised a paragraph, which is a line in pain text,  several times by changing some words in the paragraph.  Therefore, a number of words in the revised paragraph have not  been altered at all. Using the pre-processing step of DiffLDA will  generate many change edits for particular paragraphs or lines.  Consequently, the resulting delta documents will have many  duplicated words.   In our approach, we develop a text comparison utility (TCU) to  compute the edits between successive revisions. TCU first  identify text edits at paragraph levels. In other words, for each  revision, it compares its individual paragraphs to the  corresponding paragraphs of the previous revision, using the GNU  diff utility. As a result, a paragraph can be classified as added,  deleted or changed depending on whether the text edits that  transform the previous to the current revision produces a new  paragraph, remove the existing one, or change some words in the   41    existing one. For changes in the existing paragraphs, TCU then  computes text edits at word levels. Particularly, it identifies words  as added, deleted, or equal (no change) depending on whether  they have been newly added, removed or not altered at all. The  added and deleted words and paragraphs are then used as  documents for LDA to extract topics and topic evolution.  Therefore, our method can prevent the duplication effect.    The procedure used in our work can be described below. For each  document, we first identify text edits (at paragraph and word  levels) between two consecutive revisions Rj and Rj(j=j+1) using  the text comparison utility, as explained above. For each revision   of document, we create two delta documents,   and   to capture  both types of text edits: addition and deletion, similar to [19]. We  place all added word and paragraph edits between Rj and Rj into     and all deleted paragraph and word edits into  . For the first  revision (j=1), we classify the entire revision as added paragraphs,  and therefore we add the entire paragraphs of the revision to the   delta document, . Using this method, each revision has at most  two delta documents. Sometimes a revision can have one delta  document of either added or deleted paragraphs. After that, we  apply LDA to the entire set of delta documents. As a result, we  obtain a set of extracted topics and membership values for each  delta document.   LDA requires us to set parameters,  and , of the two Dirichlet  distributions: authors topic distribution and topic-specific word   distribution. We use the strategy to fix  and  to depend on the  number of topics, T and explore the consequence of very T as  explained below. Therefore, based on the techniques proposed by  Griffiths and Steyvers[7] and our empirical testing, we set the   value of =50/(#topics) and =200/(#words).   After defining the hyper-parameter values as mentioned above,  we chose the number of topics (T) by using perplexity [7], which  is a standard measure for estimating the performance of a  probabilistic model based on its ability to predict words on new  unseen documents. We selected T as small as possible while  maintaining a good model fit. The number of topics is selected  independently for each document. We used the technique  proposed [4] to ensure that the chosen model is not too fit and can  be generalized for modeling data.   The extracted topic and topic evolutions provide an overview of  how topics are created and evolved. Knowing whether students  collabrate and commonly write about the same topics can assist  instructors and learners understand how the documents have been  developed. To perform analysis about learner collaboration, we  created topic-based collaboration networks which will be  explained below.   6. TOPIC-BASED COLLABORATION  NETWORK  Let us now turn towards visualising the presence of collaboration  of students around topics. In particular, we are interested to know  whether students develop their ideas and concepts independently  or work together on the same topics. Figure 4 shows a topic-based  collaboration network from a group of four students writing a  document. Each node represents a student (an author). A square  depicts a group coordinator. Circles represent group members. A  connection (link) between two nodes shows that the  corresponding students have written about the same topics during  their writing tasks. From the figure, we can see that the group  coordinator a1 has worked with all group members to draft,  revise, and edit some topics written in the document. Similarly, a2   also worked on some topics with all other group members.  However, a3 and a4 have not written about the same topics. In  other words, independently a3 and a4 have worked with both a1  and a2 to develop some topics.       Figure 4. A topic-based collaboration network. Nodes   represents students: a1 to a4. The square is the group   coordinator and circles are group members. A connection   between two nodes means that the two corresponding students   have written about the same topics.   Our contribution in the creation of this visualisation resides in the  creation of a Diff Author-Topic Model (DiffATM), which is an  extension of Author-Topic Model (ATM), proposed by [14]. As  DiffLDA can overcome the duplication effect in LDA, DiffATM  is developed to deal with the duplication effect in ATM. For this  work, similar to DiffLDA, DiffATM is applied to text edits  identified at the paragraph and word levels in order to extract  topics. However, DiffATM does not provide a cluster of topics  per revision, but a cluster of topics per author. Based on a number  of revisions, a particular author can be represented by a  membership of topics written in those revisions. Similar to  DiffLDA for writing processes, DiffATM is developed by  selecting the number of topics and hyper-parameters based on the  trade-off between the model fitting and the simplicity of model  structure. In addition, we apply social networks proposed by [4]  for collaborative writing tasks based on the membership of topics  of individual authors.   In this section, the author-topic model will be described first.  Then, how topic-based collaboration networks are constructed  will be explained.   6.1 The Author-Topic Model  The Author-Topic Model (AT Model) is an extension of LDA,  which was first purposed in [14] and further extended in [13].  Under this model, each word w in a document is associated with  two variables: au author, x and a topic, z. Similar to LDA, each  author is associated with a multinomial distribution over T topics,   denoted as . Each topic is associated with a multinomial   distribution over words, denoted as . Differently to LDA, the  observed variables for an individual document are the set of  authors and the words in the document. The formal generation  process of Author-Topic Model is as follows [13]:   For each document, given the vector of authors, ad :   For each word in the document :   1. Conditioning on ad, choose an author xdiUniform(ad).  2. Conditioning on xdi, choose a topic zdi.  3. Conditioning on zdi, choose a word wdi.   One important difference between the Author-Topic Model and  LDA is that there is no multinomial distribution over T topics for  an individual document. Therefore, if we want to model  documents and authors simultaneously, further treatment is  needed. A detailed description can be found in [13].       42    6.2 Diff Author-Topic Model for Writing  Processes  The DiffATM model provides an analysis that is guided by the  authorship data of the documents (provided by revision histories),  in addition to the word co-occurrence data used by DiffLDA.  Each author is modeled as a multinomial distribution over a fixed  number of topics that is selected empirically as explained below.  Each topic is, in turn, modeled as a multinomial distribution over  words.   As described in Subsection 5.2, the Text Comparison Utility  (TCU) outputs the delta documents (i.e. added and deleted words  and paragraphs). As mentioned in Section 3, each revision is  produced by one or more authors.  The authors of each revision  are mapped to delta documents of that revision. Next we apply the  Author-Topic Model (ATM) to the entire set of delta documents.    As in DiffLDA, the hyper-parameters defining each Dirichlet  prior ( and ) of DiffATM are depended on the number of topics,  which is selected independently for each document using the  trade-off between the model fitting and the simplicity of the  model structure as described in the previous section. The  likelihood of two authors co-writing on the same topic will  depend on the hyper-parameters chosen. In general, larger values  of  will lead to a larger topic overlap for any given corpus,  motivating the use of a consistent hyper-parameter selection  algorithm across all corpora analysed. All hyper-parameter  settings used for the analyses presented here follow the guidelines  derived empirically by Griffiths and Steyvers [7]. In particular,   =50/(# topics), inducing topics that are mildly smoothed across  authors, and  =200/(# words), inducing topics that are specific to  small numbers of words.   Like DiffLDA, DiffATM is fit using a MCMC approach.  Information about individual authors is included in the Bayesian  inference mechanism, such that each word is assigned to a topic in  proportion to the number of words by that author already in that  topic, and in proportion to the number of times that specific word  appears in that topic. Thus, if two authors use the same word in  two different senses, the DiffATM will account for this polysemy.  Details of the MCMC algorithm derivation are given by Rosen- Zvi et al. In [14].   After the number of topics, T, has been selected, a T-topic  DiffATM is fit to all delta documents. 10 samples are taken from  20 randomly initialised Markov chains, such that there are 200  samples in total. The result of the final samples are used to  construct the  topic-based collaboration network, described next.   6.3 Construction of Topic-Based  Collaboration Network  The aim of this network visualisationis to mirror whether students  commonly use the same topics of discourse over the period of  writing. We use the same method proposed by [4], in which we  compute each pair of authorsjoint probability of writing about the  same topic as:             |  |        If the joint probability of two authors exceeded 1/T (e.g. 0.1 if  T=10), we create a link between the two nodes. The reason for  choosing this condition is explained in [4]. We construct a square  author-author matrix with entries equal to one for each linked  author pair, and entries equal to zero otherwise. For each  document, we then repeat this procedure several times as   suggested by [4] to average across whatever probabilistic noise  might exist in the DiffATM fit. Authors who link across multiple  DiffATM fits more often than would be expected due to chance  were considered to be linked in the network for that document.  After sampling DiffLDA for 200 times, we obtain the author- author maxtrix as a result. Based on the matrix, for each author  pair if its entry is more than 125, we consider there is a link  between that pair of authors.   7. TECHNICAL VALIDATION  We describe how we validate the accuracy of DiffLDA and  DiffATM, which are used to create topic evolution and topic- based collaboration networks. Since there is no public dataset for  evaluating the accuracy of topic evolution models, we created a  synthetic dataset, as inspired by [19] where we simulated text  edits on a document. In particular, we created two simple  scenarios representing several types of text edits so that we were  able to evaluate whether the evolutions discovered by the models  were accurate. Especially, we would like to check if the text edit  events detected by the models correspond to the actual changes  that happened during writing (precision) and if the discovered  evolutions contained all the text edits that were actually  performed during writing (recall).   7.1 Data Generation  To evaluate the DiffLDA model for collaborative writing, we first  created a document with 17 revisions (R1  R17). The document  consisted of three paragraphs, which are generated from three  topic distributions with equal weight. Table 1 shows the  dictionary and topic distribution of the data. After created (or first  added to the document), each paragraph was changed three times.  The changed paragraphs were also generated from the tree topic  distributions as presented in Table 1. The text changes of these  paragraphs were depicted in the event log file shown in Table 2. It  is important to note that there were no text changes in some  revisions. The 17 revisions formed our baseline scenario.   Table 1. The dictionary and topic distribution of a simulated   data   Words T1 T2 T3   River 0.37     Stream 0.31     Bank 0.22 0.28    Money  0.3 0.07   Loan  0.2    Debt  0.12    Factory   0.33   Product   0.25   Labor   0.25   News 0.05 0.05 0.05   Reporter 0.05 0.05 0.05   We set up two simulated scenarios as follows. The first scenario  modifies the baseline scenario by adding one paragraph in the  revision R6 as shown in Table 2, and deleting it in the revision  R7. The added paragraph was generated from a new topic (i.e. the  four code names of Ubuntu operating system) totally unrelated to  the three topics in the baseline scenario. This scenario simulated  two types of text editions: the addition of a new paragraph and the  deletion of an existing paragraph.   The second scenario was created based on the first scenario by  adding two paragraphs: 1) A paragraph from a new topic   43    unrelated to the four topics mentioned above was added in the  first revision R1, remained (unchanged) in the revision R2 and  R3, and deleted in the revision R4. 2) A paragraph from another  unrelated new topic was added in R14 and R15. The first half of  the paragraph was added in the revision R14, while the second  half of the paragraph was added in the final revision R16. These  two scenarios were created for testing multiple text edits  happening simultaneously in the same revision.   Table 2 shows the text edition events. The simulation was  designed in the way that there are no more than four paragraphs in  any revisions at any time. The baseline scenario consists of three  paragraphs P1, P2, and P3. The first controlled scenario (C1) is to  add and delete P4. The second one (C2) is to add and delete P5  and to add and change P6. There are four text edition events: no  change, adding, changing, and deleting a corresponding  paragraph, presented as -, a, c, and d, respectively. Each revision  is produced by at most two authors. There are five authors: a1   a5.   Table 2. Event log file presenting text edition events of   revisions of a simulated document.   Rev. P1 P2 P3 P4 P5 P6 C1 C2   R1 a    a  a1    R2 -    -  a1    R3 - a   -  a2 a5   R4 - - a  d  a3    R5 c - -    a1    R6 - - - a   a4 a1   R7 - - - d   a4 a1   R8 - c -    a5    R9 - - c    a3    R10 c - -    a1    R11 - c -    a5    R12 - - c    a3    R13 c - -    a1    R14 - c -   a a2 a5   R15 - - -   - a2    R16 - - -   c a2    R17 - - c   - a3    7.2 Preprocessing and Study Setup  After identifying text editions and creating delta documents as  described earlier, we preprocessed them. For the analysis reported  in this paper, a word-document matrix and author-document  matrix were constructed using doc2mat utility from the CLUTO  package[17], which removes all stop-words and stems all words to  their roots using the Porter stemming algorithm.   For Scenario 1, the preprocessing results in a total of 417 words  (15 of which are unique) in 23 (delta) documents. There are  (M=18.13, STD=0.81) words per revision. The Scenario 2  consists of 485 words (23 of which are unique) in 26 (delta)  documents. There are (M=18.65, STD=4.25) words per revision in  Scenario 2.   For the actual LDA and ATM computations, we used the Topic  Modeling Toolbox [20] implemented in MATLAB. We ran 500  sampling iterations. Because our simulated data is quite small, we  did not perform any parameter optimisation and thus set the  burning period.   7.3 Results  Scenario 1 consists of a change in Topic 4 when a paragraph is  added. Figure 5(a) shows that the model detects the topic because  the evolution of T4 has a value of 0 at all revisions except at  revision R6, where its distribution spikes to a bit more than 20%.  We checked the corresponding revision, especially the added  paragraph, and found that the paragraph has high membership in  this topic and low membership in all other topics. In fact, except  this paragraph, there are no paragraph having a non-zero  membership in this topic.   Figure 5(b) shows the discovered topic evolutions for Scenario 2.  The model indeed captured all three changes of the topic  evolutions.      (a) The topic evolution of four topics T1-T4 in Scenario 1.       (b) The topic evolution of T4, T5, and T6 in Scenario 2.   Figure 5, topic evolution for the simulated scenarios.   Based on the simulated authors: a1-a5, we also evaluate the  technique used in constructing the topic-based collaboration  networks. We correctly obtain a network diagram showing five  nodes and two links: the first link is between a2 and a5 who work  on P2 either alone or both at the same time, and the second link is  between a1 and a4 who together add and delete P4.   From the evaluation above, we can conclude that DiffLDA can be  used discovering topic evolutions for writing processes. In  addition, the topic-based collaboration networks showing authors  who write about the same topics can be constructed correctly as  described above. In the next section, we will illustrate the  applicability of our techniques using the real documents in the real  learning environment.   8. Case Study  We conducted a case study in a semester-long graduate course  called Foundation of Learning Science at the Faculty of  Education and Social Work, University of Sydney in 2012. The  study aimed at deploying our techniques within a course and  exploring how the visualisations were used for understanding the  writing processes of documents written by students.   8.1 Data  There were 22 students in the course. The course was structured in  the following way. Every two weeks, students were divided into  five different groups of four to five students each. During each  fortnight, groups were required to write about a topic, which vary  each fortnight, in a jointly authored document of approximately  3000 words. For this study, a writing duration of each fortnight is  called a cycle. This writing component of this course lasts for 12  weeks. Thus, there are six cycles. Throughout the semester,  therefore each student was asked to write collaboratively about six  documents. At the end of the semester, there were 30 documents   44    in total to be analysed. All documents were assessed and graded  as Pass (P), Credit (C), Distinction (D), and High Distinction  (HD).   During the two weeks of writing about an assigned topic,  individual students in each group were assigned reading materials.  There were six readings per group. Students were encouraged to  incorporate ideas and concepts learned in the class lectures and  reading materials in their writing tasks. For every document,  students had to make a plan for their writing tasks and discuss it  among group members during the first week.   Each document comprised of two sections: In the first section, A,  students were required to write about their assigned reading  materials. They had to describe the main ideas of the articles they  read and show evidence that they were grappling with the ideas,  that they could articulate difficult concepts, or put into context.  They also had to provide evidence of critical thinking in this  section. In the second section, B, students were required to  identify relationships between the reading materials of this cycle  and the ones from the previous cycle. Students had to identify the  big ideas that the readings can be seen as a contribution to.   8.2 Hypotheses  Based on the task description mentioned above, we explored two  hypotheses in our analysis. First, individual students develop their  own ideas and topics from their assigned readings by writing  several paragraphs to explain their ideas and show evidence of  their understanding in Section A. Therefore, we would expect that  each of these individual paragraphs would be mainly developed  by one student only. Second, student collaborate a lot to develop  Section B in order to relate their idea developed from the  readings. We would expect paragraphs in this section to be edited  by several group members.   For each topic evolution chart (of each document), the creation  and development of main topics are examined. In particular,  topics in Section B will emerge and developed later than topics in  Section A or vice versa. We expected the former scenario will be  more likely because students may start their writing tasks and  developed their idea in Section A based on their assign reading  materials. They then work with others to develop idea in Section  B.   In term of topic based author collaboration, it is obvious to expect  that for each group (each topic-based collaboration network) there  is at least one link connecting two nodes. This is because at least  two students collaborated and wrote about the same topics in  Section B as explained above. This link, if exists, may be a link  connecting a group coordinator (node) to other team member  (node) depending on the nature of text edits performed by  individual group coordinators. If a group coordinator only edits  (e.g. performs surface changes), there will not be any links  connecting the coordinator to the other group members. However,  if the coordinator revises (e.g. elaborates on topics developed by  other group members), there will be a link connecting the  coordinator to the others. This is not a strong requirement and is  quite difficult to check since a group coordinators responsibility  is to assign writing tasks to individual group members and make  sure the assigned tasks are progressing according to the plan.  Thus, the coordinator may not spend time collaborating and  writing about the same topics with other group members.   In order to test our hypotheses mentioned above and gain insights  on how students developed their documents, we use the revision  maps, topic evolution charts, and topic-based collaboration  networks, as described below.   8.3 Analysis  Among the six fortnightly cycles of writing, we randomly select  the third cycle for our analysis. There were five groups of students  in this cycle, thus five documents. After downloading all the  revisions of these documents, we used the text comparison utility  to identify text edits performed to produce these revisions. As a  result, we obtained delta documents containing the added and  deleted paragraphs.    Table 3, numbers of revisions, vocabularies, delta documents,   authors per revision, and final marks of 5 documents.   G ro u p    # re v is io n s   # d el ta    d o cu m en ts    # v o ca b u la ri es    # to ta l  a u th o r s   #  i n fe rr ed    to p ic s   M a rk    01  49 73 821 4 10 P   02  61 85 1040 5 11 P   03 144 229 1056 5 32 P   04 36 47 640 4 9 D   05 46 67 844 4 11 C   Table 3 summarises the numbers of revisions, delta documents,  vocabularies (unique words), and authors of the five documents.  The table also shows the final grades. We can see that the number  of revisions varies from 144 for Group 03 (receiving Pass) to 36  for Group 04 (receiving Distinction). Group 4 which received the  highest mark among the five groups actually produced less  number of revisions.   8.3.1 Revision Maps  After identifying text edits made on the five documents as  described in Section 5, we created the revision maps for each  document. Using the revision maps of individual documents, we  can observe how individual paragraphs of the two sections (A and  B) have been created and have evolved during the process of  writing. We use the five questions presented in Section 3 to guide  our analysis.   Particularly, we perform the analysis on the five documents of the  third cycle. From the five revision maps, we see that students  performed approximately equal amount of text edits in the two  sections. Secondly, as expected, in all five documents, Section A  was created before Section B. In fact, we see that a lot of text edits  happen in Section A at the beginning of the writing process and a  lot of text edits produced in Section B towards the end of the  writing, suggesting that most students spent their time writing in  the beginning and rush their writing toward the end.  Thirdly, in  all five documents, more than 50% paragraphs were created and  changed during the same writing sessions or days, revealing that  most students prefer to do their writing in one session rather than  drafting them sequentially over several days.   We then looked at the authorship of the edits made. In all five  documents, most of the paragraphs in Section A were edited and  revised by one student. For Section B, many paragraphs were  edited by more than one student. The number of paragraphs in  Section B written by several students is more than 10 for Group  11, 6 for Group 2, 4 for Group 3, 9 for Group 4, and 9 for Group  5. This suggests that most students collaboratively wrote Section  B as we expected.      45       T3 person learn hamilton student creat connect peer specif tool classroom   T4 individu develop phase affect posit support student type content time   T9 metacognit teacher recognitmotiv process appreci learn help goal mean   Figure 6.Topic evolution chart of three topics T3, T4 and T9 over 50 revisions of Document 2 and the top 10 words of each topic.  8.3.2 Topic Evolution Charts  We first performed the preprocessing step outlined in subsection  7.2 and chose the number of topics for each individual document.  As stated in Section 5, unlike other works [19], the number of  topics, T for each document was determined by fitting the LDA  models to its delta documents and selecting the model providing  the good perplexity. The number of topics chosen for each  document is shown in Table 4. After that, we applied the  technique described in Section 5 to extract topics and create topic  evolution maps.   Figure 6 shows the topic evolution chart of some topics of  Document 2. There are 11 topics for this document. The topic  evolution chart only depicts three topics: T3, T4, and T9. The top  10 words of the three topics are also shown below the figure.  From the topic evolution chart, we gain insights on how topics  have been developed during students writing. Particularly, T4 is  about the instruction and explanation of the assignment that  appears since the beginning of the document and decreases it  assignment over time. Unlike T4, T3 is about a reading material  related to the work of Hamilton about a theory of personalized  learning communities. Students wrote about this topic to reflect  on the topic. The topic spikes up at the third revision. On the other  hand, T9 arrives later than the two topics because it is for Section  B of the document. The topic is about teachers recognition of  their learners cognitive and motivational potential. Although we  can learn how topic evolve during writing, we also would like to  know if students wrote about the same topics over time, in which  we will perform the analysis base on the topic-based collaboration  networks.   8.3.3 Topic-Based Collaboration Networks  Using the technique described in Section 6, we obtain the  networks shown in Figure 7.   In all five groups, the coordinators have at least one link to group  members. In other words, students who coordinated their groups  worked with other group member on the same topics to develop   their documents. In fact, the group coordinator worked on the  same topics with all group members for all groups, except Group  2 and 4.   All of the networks except Group 4 are mostly connected. In some  groups, especially Group 1, all students wrote on the same topics.   Let us turn our attention on Group 4. Although there are four  students in this group, the revision history of Group 4 shows that  only three (i.e. a1, a2, and a3 shown in the figure) involved in  developed the document. There were 36 revisions for this  document. After checking the revision map of Group 4, we realize  that a2 and a3 were only involved in 6 revisions each. Four  revisions edited by a2 were also edited by a1, the group  coordinator. After checking with the revision maps, we found that  the four revisions were for paragraphs in Section B. In fact, most  of the revisions were produced only by the coordinator a1.  Nevertheless, the group managed to score a high grade.    9. DISCUSSION AND CONCLUSION  We have presented a case study with real documents written by  graduate students and illustrated the use of our visualisations to  analyse the students writing processes, since simple statistics and  access to the final documents did not provide information about  the writing process. The revision map allowed us to visualise text  edits made by students at the paragraph level overtime, the topic  evolution chart showed how topics evolve during students  writing and the topic-based collaboration network showed which  students wrote about the same topics during their writing.   This is a first step to gain some understanding about how students  worked and created their collaborative document. Our aim is to  support this collaborative writing process by providing  visualisation as feedback to students about that process. This is to  provide an awareness of the groups writing activities to  individual students so that they can perform their collaborative  writing tasks more efficiently and effectively. The support can  also be tools for teachers to monitor groups effectively and detect  problems early.       Figure 7. Author collaboration of three groups: Group 1 - 5 with their marks in the parenthesis. Authors represented by circle and square nodes:   a1-a5. Square nodes represent group coordinators. The links show authors who commonly use the same topics of discourse. Note: a1-a5 are   different authors across different groups.   46    10. ACKNOWLEDGMENTS  This project has been funded by Australian Research Council  DP0986873 and a Google Research Award. We would like to  thank Anindito Aditomo for setting up and administrating the  course in the case study.   11. REFERENCES  [1] Andrews, N. O. and Fox, E. A. 2007. Recent   Developments in Document Clustering.  TR-07-35.  Computer Science, Virginia Tech.   [2] Blei, D. M. and Lafferty, J. D. 2009. Topic models, in Text  Mining: Classification, Clustering, and Applications. A.  Srivastava and M. Sahami, Eds.: Chapman & Hall/CRC  Data Mining and Knowledge Discovery Series.   [3] Blei, D. M., Ng, A. Y., and Jordan, M. I. 2003. Latent  Dirichlet Allocation. Journal of Machine Learning  Research. 3 (Mar. 2003), 993-1022.   [4] Broniatowski, D. A. and Christopher, L. M. 2012.  Studying Group Behaviours: A Tutorial on Text and  Network Analysis Methods. IEEE SIGNAL PROCESSING  MAGAZINE. (Mar. 2012), 22-32.   [5] Caporossi, G. and Leblay, C. 2011. Online Writing Data  Representation : A Graph Theory Approach. In  Proceedings of the 10th international conference on   Advances in intelligent data analysis X (Porto, Portugal,  October 29-31, 2011), 80-89.   [6] Flower, L. and Hayes, J. 1981. A Cognitive Process  Theory of Writing. College Composition and  Communication. 32 (Dec. 1981), 365-387.   [7] Griffiths, T. L. and Steyvers, M. 2004. Finding scientific  topics. Proceedings of the National Academy of Sciences  of the United States of America. 101 (Suppl. 1) (Apr.  2004), 5228-35.   [8] Kay, J., Maisonneuve, N., Yacef, K., and Reimann, P.  2006. The big five and visualisations of team work  activity. In Proceedings of the 8th International  Conference on Intelligent Tutoring Systems (Jhongli,  Taiwan, June 26-30, 2006), 197-206.   [9] Kim, S. and Lebanon, G. 2010. Local Space-Time  Smoothing for Version Controlled Documents. In  Proceedings of the 23rd International Conference on   Computational Linguistics (Beijing, China, August 23-27,  2010).   [10] Liu, M. and Calvo, R. A. 2011. Question Taxonomy and  Implications for Automatic Question Generation. In  Proceedings of Artificial Intelligence in Education  (Auckland, New Zealand, 2011), 504-506.   [11] O'Rourke, S., Calvo, R. A., and McNamara, D. 2011.  Visualizing Topic Flow in Students Essays. Journal of  Educational Technology and Society. 14 (Jul. 2011), 4-15.   [12] Perrin, D. and Wildi, M. 2010. Statistical modeling of  writing processes, in Traditions of Writing Research. C.  BAZERMAN, et al., Eds.: Routledge, 378-393.   [13] Rosen-Zvi, M., Chemudugunta, C., Griffiths, T., Smyth,  P., and Steyvers, M. 2010. Learning author-topic models  from text corpora. ACM Transactions on Information  Systems. 28 (Jan. 2010), 1-38.   [14] Rosen-zvi, M., Griffiths, T., Steyvers, M., and Smyth, P.  2003. The Author-Topic Model for Authors and  Documents. In Proceedings of the 20th Conference on  Uncertainty in Artificial Intelligence (Banff, Canada, July  7-11, 2003).   [15] Shermis, M. D. and Burstein, J. 2003. Automated Essay  Scoring: A Cross-disciplinary Perspective. MIT Press.   [16] Southavilay, V., Yacef, K., and Calvo, R. A. 2009.  WriteProc: A Framework for Exploring Collaborative  Writing Processes. In Proceedings of Australasian  Document Computing Symposium (Sydney, Australia,  2009).   [17] Steinbach, M., Karypis, G., and Kumar, V. 2000. A  Comparison of Document Clustering Techniques. In  Proceedings of Proceedings of the International KDD   Workshop on Text Mining 2000. (2000).  [18] Thomas, S. W., Adams, B., Hassan, A. E., and Blostein,   D. 2010. DiffLDA : Topic Evolution in Software Projects.  Technical Report 2010-574 2010-574. School of  Computting, Queen's University.   [19] Thomas, S. W., Adams, B., Hassan, A. E., and Blostein,  D. 2011. Modeling the Evolution of Topics in Source  Code Histories. In Proceedings of the 8th IEEE working  conf on mining software repositories (Honolulu, HI, USA,  May 21-28, 2011), 173-182.   [20] Toolbox, T. M. 2012.  http://psiexp.ss.uci.edu/research/programs_data/toolbox.h  tm.   [21] Upton, K. and Kay, J. 2009. Narcissus: interactive activity   mirror for small groups. In Proceedings of the 17  International Conference on User Modeling, Adaptation   and Personalisation (Trento, Italy, June 22-26, 2009), 54- 65.   [22] Villalon, J. and Calvo, R. A. 2011. Concept maps as  cognitive visualizations of writing assignments. Journal of  Educational Technology and Society. 14 (Jul. 2011), 16- 27.      47      "}
{"index":{"_id":"8"}}
{"datatype":"inproceedings","key":"Wise:2013:LAO:2460296.2460308","author":"Wise, Alyssa Friend and Zhao, Yuting and Hausknecht, Simone Nicole","title":"Learning Analytics for Online Discussions: A Pedagogical Model for Intervention with Embedded and Extracted Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"48--56","numpages":"9","url":"http://doi.acm.org/10.1145/2460296.2460308","doi":"10.1145/2460296.2460308","acmid":"2460308","publisher":"ACM","address":"New York, NY, USA","keywords":"asynchronous discussion groups, computer mediated communication, learning analytics, online learning, student participation","abstract":"This paper describes an application of learning analytics that builds on an existing research program investigating how students contribute and attend to the messages of others in online discussions. A pedagogical model that translates the concepts and findings of the research program into guidelines for practice and analytics with which students and instructors can assess their discussion participation are presented. The analytics are both embedded in the learning environment and extracted from it, allowing for integrated and reflective metacognitive activity. The pedagogical intervention is based on the principles of (1) Integration (2) Diversity (of Metrics) (3) Agency (4) Reflection (5) Parity and (6) Dialogue. Details of an initial implementation of this approach and preliminary findings are described. Initial results strongly support the value of student-teacher dialogue around the analytics. In contrast, instructor parity in analytics use did not seem as important to students as was expected. Analytics were reported as useful in validating invisible discussion activity, but at times triggered emotionally-charged responses.","pdf":"Learning Analytics for Online Discussions:   A Pedagogical Model for Intervention with Embedded and   Extracted Analytics Alyssa Friend Wise  Simon Fraser University   250 - 13450 102nd Avenue  Surrey, BC, V3T0A3 Canada   1-778-782-8046  alyssa_wise@sfu.ca  Yuting Zhao  Simon Fraser University   250 - 13450 102nd Avenue  Surrey, BC, V3T0A3 Canada   1-778-782-8046  yza174@sfu.ca   Simone Nicole Hausknecht  Simon Fraser University   250 - 13450 102nd Avenue  Surrey, BC, V3T0A3 Canada   1-778-782-8046  shauskne@sfu.ca     ABSTRACT  This paper describes an application of learning analytics that  builds on an existing research program investigating how students  contribute and attend to the messages of others in online  discussions. A pedagogical model that translates the concepts and  findings of the research program into guidelines for practice and  analytics with which students and instructors can assess their  discussion participation are presented. The analytics are both  embedded in the learning environment and extracted from it,  allowing for integrated and reflective metacognitive activity. The  pedagogical intervention is based on the principles of (1)  Integration (2) Diversity (of Metrics) (3) Agency (4) Reflection  (5) Parity and (6) Dialogue. Details of an initial implementation  of this approach and preliminary findings are described. Initial  results strongly support the value of student-teacher dialogue  around the analytics. In contrast, instructor parity in analytics use  did not seem as important to students as was expected. Analytics  were reported as useful in validating invisible discussion activity,  but at times triggered emotionally-charged responses.   Categories and Subject Descriptors  K.3.1 Computer uses in education   General Terms  Measurement, Design, Human Factors.    Keywords  Online learning, Computer mediated communication, Learning  analytics, Asynchronous discussion groups, Student participation.   1. INTRODUCTION  This paper presents a learning analytics application that builds on  an existing research program investigating how students  contribute and attend to the messages of others in online  discussions. Each of these activities is important in realizing the  theoretical potential of online discussions to support group   knowledge construction and individual development of  understanding [36]. However a substantial research base shows  that in actual discussions learners often pay limited attention to  others posts and make disconnected comments [32,14], resulting  in conversation patterns that can be characterized as shallow and  incoherent rather than dialogic [12, 33]. Early research into these  problems reported disturbingly low overall statistics, suggesting  that the problems were global and thus in part systemic products  of the online discussion environment [14, 19, 31]. These findings  spurred efforts to improve discussion environments to support  productive engagement and discussion [e.g. 22, 28]. However,  more recent work disaggregating data across individuals has  revealed that students in fact engage in very different kinds of  behaviors in online discussions [35, 36, 37]. This suggests that  discussion participation can also be improved with more targeted  efforts to provide guidance to students individually. This work  unites these two kinds of efforts to present a pedagogical model  using both embedded and extracted analytics to support online  discussion participation.   1.1 Situating the Present Effort in the Field  The field of learning analytics is concerned with the collection  and analysis of data traces related to learning in order to inform  and improve the process and/or its outcomes [29]. Within this  space, a distinction can be made between classes of analytics  based on the types of data collected and the kinds of decision  making targeted [8]. At a macro level, administrators and policy  makers have the opportunity to use learning analytics to make  programmatic or legislative decisions. In such situations data on  past learning events is used to make decisions about future ones;  these choices tend to be based on relatively long data time-cycles  [5], affect large numbers of people, and involve outcome-type  data, for example summative assessments, performance indicators  and the like [3]. For these purposes data may also be aggregated  at various levels (i.e. student, class, department, institution etc.).  In contrast, at a micro level learners and teachers have the  opportunity to use learning analytics to make more local decisions  about the current learning event they are involved in [5]. In this  case, the relevant data relates to tracking learning processes and  an important element of interpretation is having a model of  learning for the particular environment - i.e. a research-based  framework for understanding what productive activity in the  specific learning context looks like. In some cases the model may  be specified such that analytics data is processed and interpreted  according to some system of rules leading to automatic changes in  the learning system [e.g. 26]. In other cases, data may be  processed into categories according to the model, but then   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08-12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   48    presented to stakeholders to support decision making [e.g. 17]. In  this latter situation, two core challenges that are currently  receiving a great deal of research attention are how to determine  meaningful traces of learning, and how to present them in a form  that will be useful to decision makers [7]. Another important, but  less studied component is the model for intervention [5]; that is  the framing of the activity (for teachers and/or students) of  interpreting and making decisions based on the analytics.  This work contributes to all three of these areas of interest:  capturing meaningful traces of learners activity in online  discussions based on a specific model of learning through  discussions, presenting these traces to learners in a useful form,  and framing this presentation as part of pedagogical activity to  guide the use of the analytics in decision-making by learners and  teachers. While the traces and analytics presentation are specific  to online discussions, the pedagogical model for framing  reflective activity is described in terms of general principles that  can be applied to a variety of learning contexts. This pedagogical  and analytics model has recently been piloted in a semi- automated implementation with a small group of learners in an  authentic class setting. It is argued that such lightweight testing of  the efficacy of theoretically-grounded analytics implemented with  the minimally necessary toolset is a valuable validation step prior  to the development of full-blown learning analytics platforms and  dashboard.   2. LEARNING ANALYTICS FOR ONLINE  DISCUSSIONS  The increasing amount of information automatically captured by  online learning environments is attractive as a source of data with  which to better understand and support learning processes.  However, there is a wide gap between the kinds of data that are  easily capturable in social learning environments such as online  discussions and the kinds of constructs that have been established  as pedagogically valuable [3].   To address this challenge, we begin by laying out our theoretical  framework for learning in online discussions with the relevant  research base. We then describe two classes of analytics for  online discussions that we have developed based on this model:  embedded analytics and extracted analytics. The first class refers  to a set of analytics that are embedded in the discussion interface  and can be used by learners in real-time to guide their  participation; here interpretation of the analytics and participation  in the learning activity are unified as a single endeavor. In  contrast, the second class refers to analytics that are extracted  from the traces of learning activity and presented back to learners  for interpretation as a separate exercise from participating in the  learning activity itself. As an example, analytics presented to  learners via dashboard-like systems fall into the second category  of this taxonomy. That is while the presentation of the analytics  may be integrated into the overall learning environment, the  activity of interpreting them is separate from that of engaging in  the learning activity itself. This distinction will be clarified  through the description of the specific instantiation of each type  of analytic that follows in the subsequent sub-sections.  In addition to the development of these analytics as a source of  informative data, we describe their intended use in terms of  pedagogical design. That is the intervention consists not simply of  providing these analytics to learners but also framing their  interpretation as an integral course activity tied to goals and  expectations.    2.1 Theoretical Framework and Research  Base for Learning through Discussions  In this work online discussions are conceptualized from a social  constructivist perspective as a venue in which learners can  interact to build both collective and individual understanding  through dialogue [2, 30]. Scholars differ in their theoretical  accounts of the mechanisms underlying the learning process,  referring variously to the importance of being exposed to multiple  viewpoints, articulating ones own ideas, experiencing socio- cognitive conflict, and the negotiation and internalization of group  understandings [36, 21, 30]. However, at a fundamental level all  explanations depend on two basic processes that learners must  engage in: speaking (externalizing ones ideas by contributing  posts to the discussion); and listening (taking in the  externalizations of others by accessing existing posts) [36].  Speaking in online discussions is clearly visible to others; this  may explain why the bulk of research on and guidance for  participation in online discussions is focused on posting activity  [13]. However, while listening is largely invisible, it is also  critical for discussions that build understanding in the ways  described above [36]. Recent work has begun to document the  different kinds of listening behaviors in which students engage  [35, 36, 37] and has shown empirical connections between  students listening and speaking behaviors [34].   While the language of speaking and listening draws on a  metaphor based in face-to-face conversations, online discussions  offer different affordances and constraints for these activities [34,  35]. Specifically, in online discussions learners have greater  control over the timeline and pace of their engagement [16]. This  creates opportunities for thoughtful listening and reflective  speaking [21], but also additional challenges of time management,  especially for prolific discussions [25]. For this reason, helping  learners to actively monitor and regulate how they speak and  listen in online discussions is an important tool for supporting  productive engagement in discussions.    Given the above-described goal of using dialogue to build  individual and collective understandings and the existing research  base on online discussion, particular speaking and listening  behaviors can be characterized as more or less productive. For  example, in terms of speaking quantity, multiple posts are needed  to respond to others ideas and questions, elaborate on the points  made, and negotiate with others [24]. These posts should be  distributed throughout the discussion (rather than concentrated at  the start or end), relating to the temporal distribution of  participation. In addition posts of moderate length best support  rich dialogue since very short posts tend to be shallow in their  presentation of ideas [6] but long posts are often perceived as  overwhelming and thus not read [25]. Precise specifications of  moderate length may differ by context and age-level; in higher  education this is often specified around 100 to 200 words. In  terms of speaking quality, posts that are clear, critical and  connected to the existing conversation support the generation of  insight and understanding of the topic [27]. Posts whose  arguments are based on evidence and/or theory can also trigger  others to productively build on or contest the points [4], and  responses that clarify points, elaborate or question existing ideas,  or synthesize different ideas together help deep the exploration of  ideas and move the discussion forward [24]. In terms of listening  activity there are also multiple dimensions to attend to.  Considering breadth of listening, viewing a greater proportion of  others posts exposures students to more diversity of ideas and is   49    associated with richer responses [37]. Depth of listening is also  important as an indication of the degree to which learners are  considering others ideas and greater listening depth is predictive  of richer argumentation in posts made [34]. Listening reflectivity  (revisiting one own and others posts from earlier in the  discussion) is also important to provide context for interpreting  recent posts and examine how thinking has changed. Revisiting  others posts has also been shown to be predictive of more  substantive responses to others ideas [34]. Finally temporal  distribution of listening is important since engaging in multiple  sessions during a discussion and integrating listening and posting  in the same session can support making connections between  ideas and posts which productively build on previous ones [36].   2.2 Analytics Embedded Real-Time in the  Discussion Interface  In this work, we have chosen a specific discussion forum tool to  use because of its inherent affordances for providing embedded  analytics. The Visual Discussion Forum environment [22] was  developed to present discussion threads as a hyperbolic (radial)  tree structure, allowing students to easily see the structure of the  discussion and the location of their comments within it (see  Figure 1). Posts are represented as scaled colored spheres  connected by lines indicating their reply relations. When a post is  selected, it enlarges to the maximum size and moves to the center  of the diagram while the other posts are rearranged around it;  sphere size decreases with distance from the currently selected  post. For each student, new posts are shown in red and previously  viewed posts are shown in blue. The initial (seed) post always  remains yellow. The design rationale and general benefits over  traditional linear text-based forums have been described  previously [22] and include more purposeful reading and replying  behaviors by students, the ability to focus on a part of the  discussion in the context of the whole, and increased reading of  posts in a connected fashion.  In addition to these general benefits for interaction, the Visual  Discussion Forum also provides an embedded visual analytic of   listening and speaking behaviors in terms of how the groups  discussion is proceeding and how each individual is attending and  contributing to it. At the group level, the graphical representation  of the trees branches allows for easy inspection of the structure  of the discussion. Learners can see how many different threads  have been created thus far and how deep each is, using this  information to inform their decision about where to read and  contribute [22]. They can also examine which threads and posts  are receiving the most attention (responses) and if any are being  neglected. For example, in Figure 1. the post labeled  Transfer has not yet been taken up by the group. This kind  of analytic can be useful in addressing the problem of inadvertent  thread abandonment or death [14].   At the individual level, the red/blue coloring of the posts helps  each student easily track which parts of the discussion they have  listened to already and which parts they have not yet attended to.  For example in Figure 1 the student has been heavily attending to  one thread, moderately attending to a second, and very minimally  attending to a third. In addition, for the current application, we  made an adaptation from the previous version of the forum to  color the active learners posts differently (light blue). In in this  way, we provide an analytic to each student of how they have  contributed to the discussion thus far in term of quantity (are their  contributions high or low volume compared to overall quantity of  discussion), breadth (to what extent are their posts distributed  throughout the discussion), and intensity (if they have contributed  multiple times to a specific thread). Students can also easily see  which of their posts has been taken up by others and which have  not. In the example shown in Figure 1.the learner has made five  posts in two of the three active threads. Several of their posts have  stimulated further discussion, in particular the one labeled  Synthesizing. Here they can also see their round-trip interaction  [1, 11] with the student responding with A question, and also  that no one has yet addressed their post entitled A concern.   This section has described a set of analytics that are naturally  embedded in the design of the Visual Discussion Forum tool. An    Figure 1. Visual Discussion Forum environment adapted for analytics   50    advantage to such embedded analytics is that they can be used  seamlessly to support metacognitive aspects of the discussion  activity itself. However, a weakness of their being embedded is  that there is also the possibility that they may be ignored; i.e.  there  is no reason to assume that students will naturally use these  affordances of the tool to support their participation individually  or as a group. Thus a key aspect of effective use is support for  such activity through pedagogical framing. In other words, such  use needs to be specifically encouraged by structuring it in to the  discussion activity parameters. The details of our pedagogical  model for doing so and a description of its initial implementation  are explained in Sections 3 and 4.   2.3 Analytics Extracted Periodically from the  Discussion  In contrast to the embedded analytics described above, there is  other useful information about students online discussion activity  that does not easily lend itself to presentation through the  graphical interface (e.g. temporal distribution of participation).  Thus, in our work with extracted analytics, we sought to make  log-file trace data of speaking and listening activity visible to  learners. The metrics used were developed based on our prior  research investigating how students attend to the messages of  others in online discussions described earlier [34, 35, 36, 37] and  are summarized in Table 1. Data processing was implemented  using a toolkit consisting of a combination of mySQL queries and  Excel VBA macros as described below.    Table 1. Summary of discussion participation metrics   Metric Definition Participation Criteria   Range Span of days a student logged in to the discussion.  Temporal   distribution   Sessions Number of times a student logged in to the discussion.  Temporal   distribution  Percent of  sessions   with posts   Number of sessions in which a  student made a post, divided by  his/her total number of sessions.   Temporal  distribution   Average  session  length   Total length of time spent in the  discussion divided by his / her   number of sessions.   Temporal  distribution   Posts Total number of posts a student contributed to the discussion.  Speaking  quantity   Average  post length   Total number of words posted  by a student divided by the   number of posts he/she made.   Speaking  quantity   Percent of  posts read   Number of unique posts that a  student read divided by the total  number of posts made by others   to the discussion.   Listening  breadth   Number of  reviews of  own posts   Number of times a student  reread posts that he/she had   made previously.   Listening  reflectivity   Number of  reviews of   others posts   Number of times a student  reread others posts that he/she   had viewed previously.   Listening  reflectivity     Data was initially extracted from the log-file and posts tables in  the discussion forum database and merged into a single  spreadsheet file. This file lists each action taken by a student in  the system in a row with the following information: action type  (view-post, create-post, edit-post, delete-post), a time-date stamp,   ID of user performing the action, ID of post being acted on,  length of post being acted on, ID of user who created post being  acted on. Macros were then used to clean the data, separate data  by user, calculate action duration (through subtraction of  sequential time stamps), divide actions into sessions-of-use (based  on a 60-min abandonment threshold, see [36]), and make adjusted  estimates for duration of session-ending actions (based on the  relevant posts length and the average speed of the user in  conducting the indicated action). View actions made on a users  own posts were re-coded as self-review actions and all view and  review actions were sub-categorized as reads or scans based on a  maximum reading speed of 6.5 words per second (wps) see [15].  Finally, nine variables were calculated based on the definitions  shown in Table 1. Averages for the group were also calculated  and a summary table was created for each learner (see Figure 2)  as a straightforward way of presenting the data.   Pleasereviewtheanalyticsaboutyourdiscussionparticipation below,notinganything interestingyousee in thedata in the observationsboxandusingthisinformationtohelpyouasyou answerthereflectionquestions.  Metric YourData(WeekX)  Class Average (WeekX)  Observations  Rangeof participation 4days 5days     #ofsessions 6 13   Averagesession length 33min 48min   %ofsessions withposts 67% 49%   #ofpostsmade 8 12   Averagepost length 149words   125 words   %ofpostsread 82% 87%  #ofreviewsof ownposts 22 13   #ofreviewsof othersposts 8 112     Figure 2. Sample learner analytics summary   The metrics chosen for the extracted analytics were designed to  be complementary to the embedded analytics; however two  overlaps occur. First, the metric Number of Posts Made is  viewable through the embedded analytics. However in the  embedded analytic, the total number of posts made is less salient  than their distribution. Thus providing this sum and the average  for the group (which is not easily determinable from the interface)  was deemed useful additional information. Second, the metric  Percent of Posts Read is similar to the embedded red/blue color  tracking of posts viewed in the interface. However, while the  interface tracks all posts viewed, this metric is only based on posts  actually read (not scanned) and thus provides complementary  (and often quite different) information.   51    3. PEDAGOGICAL DESIGN OF THE  LEARNING ANALYTICS INTERVENTION  When working with learning analytics a number of concerns have  been raised about the dangers of rigidity of interpretation, lack of  transparency with regards to data capture and access, the  hegemony of optimizing to only that which can be measured, and  possible impediment of learners development of metacognitive  and self-regulative learning skills [3, 5, 7]. Addressing such  concerns requires attention not only to the how the analytics are  developed and presented, but the pedagogical framework of  activity that surrounds their use [5]. Thus while the analytics  described above have the potential to be useful to instructors and  students in monitoring and improving discussion participation,  doing so requires active interpretation in the larger context of a  shared understanding about the qualities of productive  participation described earlier. Our pedagogical design for the use  of the online discussion analytics carefully considered these  challenges and attempted to address them through a number of  core guiding principles.  While the principles are described below  in the context of our work with online discussion analytics, we  believe that at the conceptual level they have the potential for  applicability in a broad variety of learning analytics contexts.   3.1 Guiding Principles and their Instantiation  3.1.1 Integration   In order for analytics to be used by students in meaningful ways,  there is a need for the metrics to be connected to the larger  framework of purpose and expectations for the activity. For this  reason it is important to integrate the analytics metrics with the  goals of the learning activity. In other words, students need to  understand (1) the purpose of the learning activity, (2) the  characteristics of what the instructor views as productive  engagement in the activity, and (3) how the learning analytics  provided serve as a representation of this. In our work, we  instantiate the first element by introducing the online discussions  at the start of a course with a conversation about the goals of the  activity as a vehicle for clarifying and building understandings  through dialogue. This is particularly important given that  students may view discussions quite differentlyfor example as a  social space, a place to receive information, or a chance to show   off what they know [20]. Depending on the scale and context of  the learning experience, this first element might alternatively be  enacted through a simple presentation or by including the students  in the determination of the activitys goal. For the second  element, we provide the students with clear guidelines for what is  expected (and will be evaluated) for their discussion participation  in terms of quantity, quality and timing of posting as well as  broad, deep, integrated and reflective attention to the posts of  others (see Figure 3).    These guidelines are reflective of the learning framework and  research described earlier and students are made privy to this  rationale. Finally, the analytics are introduced in the context of  this framework. For the embedded analytics, mention is made in  the participation guidelines in the appropriate section of how the  interface can support this element of their participation (see  Figure 3); for the extracted analytics, a separate guideline sheet is  given with a chart describing each metric and how it relates to the  participation criteria (see Figure 4). In this way, the metrics are  not presented simply as a set of numbers, but ones which have  clear meaning in the context of the learning activity.    3.1.2 Diversity & Agency   One important concern in using learning analytics is that the  analytics alone will dictate how people engage in the learning  activity and thus we become what we measure, even though the  metrics only capture some aspects of the overall activity [5, 7]. To  address this concern it is important to include multiple diverse  measures (so no one metric becomes the sole focus) and support  students in actively interpreting their meaning (in the context of  the larger framework of the activity goals and criteria described  earlier). Of course the drive to provide multiple metrics needs to  be balanced with care not to overload or overwhelm students  unproductively. In our work, a selection of metrics are presented  to students in a simple table format (see Figure 2). Importantly the  guidelines present them as a starting point for consideration, not  as absolute arbiters of ones engagement in the activity. This is  done to help students develop an awareness of how they are  participating in the discussion and take responsibility for the  discussion and their actions in it. We also provide students with  class averages for each metric to give them a context in which to  consider the numbers.    Discussion Participation Guidelines       Attending to Others Posts    Broad Listening: Try to read as many posts as possible to  consider everyones ideas in the discussion. This can help you  examine and support your own ideas more deeply. However,  when time is limited it is better to view a portion in depth, then  everything superficially.    *The visual interface shows posts that you have viewed in blue  and new ones in red to help you track this.    Purposeful Participation     Group Responsibility: As a group, we have a collective  responsibility to tend to our discussion and make sure there  arent parts being neglected.     *The visual interface allows you to see which branches of  discussion have received more attention than others.   Figure 3. Excerpts from discussion participation guidelines     Learning Analytics Guidelines      Attending to Others Posts   % of  posts  read   The proportion of posts you read (not scanned)  at least once.    It is good to read as many posts as possible to  consider everyones ideas in the discussion  However, when time is limited it is better to  view a portion in depth, then everything  superficially.   # of  reviews  of others  posts   The number of times you revisited others posts.    It is good to review others posts to help you  develop a higher level of response by relating  back to others ideas. This number may be  inflated by click-through, so should be  evaluated relative to the group average.      Figure 4. Excerpts from learning analytics guidelines   52    Finally, as described in the following sub-section, students are  encouraged to set personal goals for their participation and to use  the analytics to help monitor these. This supports individual  student agency in using the analytics and sets up a situation of  multiple possible profiles of productive participations, rather than  a single goal to which all students must aspire. By making visible  previously hidden listening activity, this approach can also  highlight different students various strengths in discussion  participation; for example some students may need to work on  their listening, while others may discover they are posting far less  than others. In these ways we aim to create an environment of  analytics use that is active, dynamic and personalized, with the  goal of empowering students [3].   3.1.3 Reflection  One of the key attractions of learning analytics is the possibility  to support the learner in actively reflecting on and taking action to  manage their learning process [10]. From a constructivist  perspective, reflection is an essential part of constructing ones  understanding; in turn, as ones understanding develops,  reflection can also be used more effectively [23]. However, online  activities that can happen at anyplace and anytime often happen  nowhere and never [18]; conversely attention to constantly  available analytics can draw away from engagement in the  activity itself. To support productive reflective activity, we  provide explicit time and space for reflection on the analytics. In  our work this is operationalized in the form of an online reflective  journal shared between each student and the course instructor.  The technology we employ for this purpose is a series of private  wikis, however a variety of other technological solutions could  serve the same function. At the start of the term, after students are  given the guidelines, they are asked to set concrete goals for their  participation in the discussion in the journal. They are then  periodically provided with their analytics (as well as class  averages as a benchmark) and given a series of reflective  questions to respond to in the online journal (See Figure 5).  Because the journal is shared, the instructor can review students  analytics and reflections as needed and respond in the same space.  In this way interpretation of the analytics is not owned solely by  the teacher or student but becomes negotiated between them.  Storing the analytics, prompts, reflections and instructor  comments in a digital journal also facilitates longitudinal review  of changes and progress over the course of the term by both the  student and instructor. The frequency with which the analytics are   provided and reflective activity engaged in will vary depending  on the context, but the goal of setting up specific timing is to  avoid overwhelming students or making them overly reliant on  the analytics [3]. As described in Section 4, in our current  implementation we have found it most meaningful to provide  analytics at the end of each week-long discussion since this is the  unit of activity that students experience.   3.1.4 Parity & Dialogue  Another set of important issues in implementing learning  analytics relate to questions of power, access and transparency [7,  9]. We address these issues through the principle of establishing  parity between the students and instructor in analytics use and  creating a space of dialogue around the analytics and their  interpretation. To implement parity within our pedagogical  design both the students and instructor keep a reflective journal  based on their own analytics as described above. The instructor  has access to and the ability to comment on students reflection,  creating a space of dialogue around the interpretation of the  metrics in the context of each students current goals. In turn, the  students are free to read the instructors reflections allowing him  or her to model the reflective process and creating a sense of  openness and equity around the use of the data. In addition, by  having the instructor go through the same experience as the  students he or she will have a better sense of how the metrics  relate to actual activity, helping them to work with students to  interpret meaning. The instructor is also thus exposing themselves  to the same vulnerability as the students, can experience the same  kinds of reactions to seeing their own analytics and thus have  greater empathy in working with students. These activities alter  the power balance from one in which the instructor collects data  on the students into one in which data is used as a reflective and  dialogic tool between the instructor and students.   It is important to note, however, that there are also possible  negative implications of setting up parity with the instructor.  Specifically, instructors may be unaccustomed to having their  activity scrutinized by students and thus this principle may make  them more hesitant to use analytics. It may also add additional  self-imposed pressure on their discussion participation activities  as they feel the need to set a good (or even ideal) model for  students. In this sense the experiences of teachers and students are  unbalanced as the teacher faces a situation in which the entire  class may be focused on his or her activity traces, while each  student knows that only the teacher (and possibly a few assistants)  will be looking at theirs. Finally, the need for and expectations of  teacher participation in a discussion may be different from that of  students; if this is not communicated clearly then the teachers  analytics may create an inappropriate reference point for students.   4. IMPLEMENTATION AND INITIAL  FINDINGS  The pilot implementation of this learning analytics approach was  conducted in a semester-long blended graduate seminar on  educational technology. This setting afforded willing students and  a manageable class size with which to roll out the approach. This  was done as a reasonably light-weight way to test our theoretical  notions of how to provide useful analytics prior to the large  investment needed to build a fully-automated system. After the  model has been evaluated (and likely revised) in this best case  scenario, it will be developed into a more automated and robust  system and sequentially rolled out with larger classes, to the  undergraduate population, and in fully online courses.    Below you are provided with some detailed information about  your discussion participation for the past week generated from  the system clickstream data. Please refer to the analytics  guideline sheet to aid your interpretation and remember that  more isn't always better. Note anything interesting you see in  the data in the observations box and then use this information  to help you answer the reflection questions.    1. What do you think went well in the groups discussion this   week What could be improved  2. What do you think went well in your individual discussion   participation in this weeks discussion What could be  improved    3. How does your participation in this weeks discussion  compare to previous weeks    4. How well did you do in meeting the goal(s) you set for your  participation in this week How do you know   5. Please set at least one goal for your participation in the  coming week and write it below.   Figure 5. Sample reflective journal question prompts.   53    4.1 Implementation Context and Approach  The graduate seminar met once a week in a face-to-face session  with a series of ten week-long online discussions interspersed  between meetings. While discussions remained open for  continuing the conversation after their designated week, in  practice students focused their posting activity almost exclusively  on the current-week discussion. The first discussion week was  facilitated by the instructor to model good practice and give  students a chance to acclimate to the tool; each subsequent  discussion week was facilitated by one of the courses nine  students. The assigning of student facilitators can be both a  negative and positive in that by increasing learner responsibility  for the discussion in one week, we potentially induce some level  of abdication of responsibility for it in the others [39]. However,  this is a tradeoff that we think is worthwhile since in past  implementations of the course, the opportunity to facilitate was  something that students found very valuable about the discussion  activity and also something that they reported helped them  understand the purpose of the discussions more deeply.   The parameters of the assignment were designed to support  students in taking both individual and collective responsibility for  the discussion. At the start of the term, students were introduced  to the Visual Discussion Forum and engaged in a conversation  about the goal and purpose of online discussions in the course,  effective discussion participation strategies, effective discussion  facilitation strategies and use of the ongoing embedded analytics  as objects of reflection to understand and effect change on their  discussion participation. Students were also provided with  discussion participation guidelines and an online wiki-based  reflective journal as described above. At the beginning of each  four-hour class session students were given 10-15 min to write in  their reflective journal based on the prompts described earlier. In  between classes the instructor was invited to read students  comments and respond as needed.    For the first half of the term (which included five week-long  discussions) the reflections were based solely on students  perceptions of the discussions and the embedded analytics. This  was done to separate out the effects of the embedded analytics  and provide a baseline for comparison once the extracted  analytics were introduced. In the second half of the term (which  also included five week-long discussions) students were provided  with extracted analytics as described above. Because of the  structure of the course, analytics were calculated using the  discussion week as the unit of analysis. Using the semi-automated  toolkit described, the weekly extraction, processing and preparing  of the data took approximately 45 minutes. Both the embedded  and extracted analytics were presented explicitly as objects of  reflection to understand and effect change on ones discussion  participation and not a tool of evaluation.   4.2 Initial Findings  In this section we report initial findings on the experience of using  the analytics from the perspective of the course instructor and  discuss their implications for the future revision of the analytics  and pedagogical intervention model. In the future we will enhance  this understanding by reporting on the experience from the  student perspective and presenting a detailed analysis of the text  of students reflective journals and their log-file data (both efforts  are currently in progress). Through this work we aim to assess  both the degree to which the participants found the analytics  useful for monitoring and reflecting on discussion participation,   and to what extent this resulted in actual impact on students  discussion participation across the term.   4.2.1 Power of Dialogue  The reflective journal was created a space to encourage dialogue  between the students and instructor around the interpretation of  the analytics; however we were unsure to what extent this use  would be taken up, especially in the first half of the course when  only the embedded analytics were available. Surprisingly, the  instructor reported that the reflective journaling was eagerly  engaged in by students, even before the extracted analytics were  provided. She further reported that this journaling provided a  useful window into students thinking around their discussion  participation; both explaining external circumstances affecting  activity and also showing that in many cases students were aware  of the areas in which they needed to improve. While she had not  planned to comment on every students reflection every week, she  found herself spending the time each week to do so because she  felt it was useful in connecting with students individually,  especially for those less vocal in the face-to-face setting. In  contrast, the instructors reflective journal, set up to enact the  parity principle, was not taken up as a site for dialogue and in fact  seems to be viewed rarely, if at all, by students.   Our reaction to the usefulness of the reflective journal before the  extracted analytics were provided is mixed: on one hand this  element seems to have contributed to a productive class  environment and metacognitive attention by students to their  participation in the online discussions; on the other hand there is a  concern both with the evidentiary base for these reflections and  that the pattern of reflection without the extracted analytics may  have reduced attention to them when they were introduced.   4.2.2 Diverse Reactions to Extracted Analytics  The instructor reported that students had diverse reactions to the  extracted analytics when they were introduced. Some students  found them useful in providing hard numbers; however many  pointed out that there is much they dont capture. While the  analytics were referred to in the reflective journals, many learners  still based much of their reflections on general perceptions. This  may be because they found the analytics only moderately useful,  they were continuing patterns from the initial reflections without  data, or because the reflection prompts were not explicit enough  in referring to the analytics. These questions will be addressed  through the analysis of student interview data.   In general students reflections on the analytics seemed to fall into  two classes: validation of things students were already aware of  and metrics that were surprising. Some of the surprises were  positive; for example the instructor had felt she wasnt  contributing enough to the discussions but the metrics showed her  she was far above the class average. Other surprises were  negative; for example, for many people the extracted metric of  percent of posts read was substantially lower than what they  expected based on the embedded red/blue posts-viewed analytic.  The difference was due to scanning of posts; thus showing certain  individuals that while they were attending to all posts at a  minimal level, they were not listening to all of their peers  comments deeply. While this is an important piece of feedback  for students to receive, there seemed to be an emotionally-charged  element to some of the reactions to these results, with students  variously feeling pleased, upset, or ashamed by their metrics.   54    4.2.3 Validating Invisible Activity  Finally, one of the most valuable outcomes of using the analytics  that the instructor reported was that it honored and validated  discussion forum activity occurring under the surface.  Specifically with respect to the metrics capturing listening data it  made her aware of the intense involvement of certain students  who were very engaged with the discussion but did not always  post a lot of comments. It also highlighted a lack of listening by  some of the vociferous speakers. This led to reflective journal  dialogues in which some students were able to point out their  listening efforts while others realized that they needed to listen  more deeply.   5. LIMITATIONS  The current efforts have several limitations; these are primarily  related to conducting the pilot in a small context with advanced  and highly motivated students. First, the model of one-on-one  dialogue between the instructor and each student is not  sustainable at scale. To replicate this interaction in more populous  contexts, instructors can consider using peer commenters or  consolidating the reflections in larger units as a formal  assignment. We have implemented the latter approach to  reflection (without analytics) in a large undergraduate class,  suggesting it would also be viable for this purpose. Second,  findings in this context may not be generalizable to students who  are earlier in their post-secondary studies or less keen on learning;  thus further testing will be needed. Finally, as measures of  speaking quality are not directly assessable from log-file data,  they are not currently included in the analytics; future metrics  using computational linguistic approaches [e.g. see 9] are needed.   6. CONCLUSIONS  In conclusion, this paper has presented a theoretical framework  for considering students speaking and listening in online  discussions, used this to develop analytics both embedded in and  extracted from the learning environment to inform and improve  these activities, and explicated a pedagogical model for the  analytics intervention based on the principles of Integration,  Diversity, Agency, Reflection, Parity, and Dialogue. Together  these elements address the challenges of establishing traces of  learning that are meaningful and presenting them in a format that  is useful and transparent to learners while avoiding  rigidity of  interpretation, a myopic focus on only that which can be  measured, and impediment of learners development of  metacognitive skills [3, 5, 7]. By framing the use of the analytics  in a carefully designed pedagogical model of intervention, we  seek to present them as a guide for sense-making that can  empower students to take responsibility for regulating their own  learning processes.   7. REFERENCES  [1] Anderson, T. 2008. Towards a theory of online learning. In   T. Anderson (Ed.) The theory and practice of online  learning. (pp. 45-74). Edmonton, Canada: Athabasca  University Press.   [2] Boulos, M. N., & Wheeler, S. 2007. The emerging web 2.0  social software: An enabling suite of sociable technologies in  health and health care education. Health Information and  Libraries Journal, 24, 2-23.   [3] Buckingham Shum, S.& Ferguson, R. 2012. Social learning  analytics. Educational Technology & Society, 15(3),3-26.   [4] Clark, D., Sampson, V., Weinberger, A., & Erkens, G. 2007.  Analytic frameworks for assessing dialogic argumentation in  online learning environments. Educational Psychology  Review, 19, 343-374.   [5] Clow, D. 2012.The learning analytics cycle: closing the loop  effectively. In Proceedings of the 2nd International  Conference on Learning Analytics & Knowledge,  (Vancouver, Canada, 2012).   [6] Dennen, V.P. 2001. The design and facilitation of  asynchronous discussion activities in web-based courses:  Implications for instructional design theory. Doctoral thesis,  IU-Bloomington   [7] Duval, E. & Verbert, K. 2012. Learning analytics. E- Learning and Education, 1(8).    [8] Ferguson, R. 2012. The state of learning analytics in 2012: A  review and future challenges. Technical Report KMI-12-01,  Knowledge Media Institute, The Open University   [9] Ferguson, R. & Buckingham Shum, S. 2011. Learning  analytics to identify exploratory dialogue within  synchronous text chat. In Proceedings of the 1st  International Conference on Learning Analytics &  Knowledge, (Banff, Canada, 2011).   [10] Govaerts, S., Verbert, K., Klerkx, J., Duval, E., 2010.  December). Visualizing activities for self-reflection and  awareness. In Proceedings of 9th International Conference  on Web-based Learning, (Shanghai, China, 2010).   [11] Henri, F. 1992. Computer conferencing and content analysis.  In A. R. Kaye (Ed.), Collaborative learning through  computer conferencing: the Najaden papers (pp. 117 36).  Berlin: Springer-Verlag.    [12] Herring, S. 1999. Interactional coherence in CMC. Journal  of Computer-Mediated Communication, 4(4).   [13] Hew, K.F., Cheung, W.S., & Ng, C.S.L. 2010. Student  contribution in asynchronous online discussion: A review of  the research and empirical exploration. Instructional Science,  38(6), 571-606.   [14] Hewitt, J. 2003. How habitual online practices affect the  development of asynchronous discussion threads. Journal of  Educational Computer Research, 28, 31-45.   [15] Hewitt, J., Brett, C., & Peters, V. 2007. Scan rate: A new  metric for the analysis of reading behaviors in asynchronous  computer conferencing environments. American Journal of  Distance Education, 21(4), 215-231.   [16] Jonassen, D. H., & Kwon, H. I. 2001. Communication  patterns in computer mediated versus face-to-face group  problem solving. Educational Technology Research and  Development, 49(1), 35-51.   [17] Jovanovi,  J., Gaevic, D., Brooks, C., Devedic, V., Hatala,  M., Eap, T., et al. 2008. LOCO-Analyst: Semantic web  technologies in learning content usage analysis. International  Journal Of Continuing Engineering Education And Life  Long Learning 18(1), 54-76.).   [18] Jun, J. 2005. Understanding e-dropout. International Journal  on E-Learning, 4(2), 229-240.   [19] Kear, K. 2001. Following the thread in computer  conferences. Computers & Education, 37, 8199.   55    [20] Knowlton, D. S. 2005. A taxonomy of learning through  asynchronous discussion. Journal of Interactive Learning  Research, 16(2), 155-177.   [21] Lipponen, L. 2002. Exploring foundations for computer- supported collaborative learning. In Stahl, G. (Ed.) In  Proceedings of Computer Supported Collaborative Learning   (Boulder, USA, 2002)     [22] Marbouti, F. 2012. Design, implementation and testing of a  visual discussion forum to address new post bias. Masters  thesis, Simon Fraser University.   [23] McAlpine, L., & Weston, C. 2000. Reflection: Issues related  to improving professors' teaching and students' learning.  Instructional Science, 28(5), 363-385.   [24] Pena-Shaff, J.B. & Nicholls, C. 2004. Analyzing student  interactions and meaning construction in computer bulletin  board discussions. Computers & Education, 42, 243-65.   [25] Peters, V. & Hewitt, J. 2010. An investigation of student  practices in asynchronous computer conferencing courses.  Computers & Education, 54(4), 951-961.     [26] Roll, I., Aleven, V., & Koedinger, K. R. 2010. The invention  lab: Using a hybrid of model tracing and constraint- based  modeling to offer intelligent support in inquiry  environments. In Proceedings of the 10th International  Conference on Intelligent Tutoring Systems. (Berlin,  Germany, 2010)   [27] Rovai, A. P. 2007. Facilitating online discussions effectively.  The Internet and Higher Education, 10, 77-88.   [28] Scardamalia, M. 2004. CSILE/Knowledge Forum. In  Education and technology: An encyclopedia (pp.183-192).  Santa Barbara: ABC-CLIO.   [29] Siemens, G.,Gaevi, D., Haythornthwaite, C., Dawson, S.,  Buckingham Shum, S., Ferguson,R., et al. 2011. Open  Learning Analytics: An Integrated and Modularized  Platform (Concept Paper): SOLAR.    [30] Stahl, G. 2005. Group cognition in computerassisted  collaborative learning. Journal of Computer Assisted  Learning, 21(2), 79-90.   [31] Swan, K. 2003. Learning effectiveness: What the research  tells us. In J. Bourne & J. C. Moore (Eds), Elements of  quality online education, practice and direction (pp.13-45).  Needham,MA: Sloan Center for Online Education.   [32] Thomas, M. 2002. Learning within incoherent structures:  The space of online discussion forums. Journal of Computer  Assisted Learning, 18, 351-366.   [33] Webb, E., Jones, A., Barker, P., & van Schaik, P. 2004.  Using e-learning dialogues in higher education. Innovations  in Education and Teaching International, 41(1), 93-103.   [34] Wise, A. F. Hausknecht, S., & Zhao, Y. In review.  Relationships between speaking and listening in online  discussions: An empirical examination. In Proceedings of  Computer Supported Collaborative Learning, (Wisconson,  USA, 2013)   [35] Wise, A. F., Perera, N., Hsiao, Y., Speer, J. & Marbouti, F.  2012. Microanalytic case studies of individual participation  patterns in an asynchronous online discussion in an  undergraduate blended course. The Internet and Higher  Education, 15, 108117.   [36] Wise, A. F., Speer, J., Marbouti, F. & Hsiao, Y. 2012.  Broadening the notion of participation in online discussions:  Examining patterns in learners' online listening behaviors.  Advance online publication. Instructional Science.    [37] Wise, A., Hsiao, Y., Marbouti, F., Speer, J. & Perera, N.  2012. Initial validation of listening behavior typologies for  online discussions using microanalytic case studies. In  Proceedings of International Conference of Learning  Sciences, (Sydney, Australia, 2012).   [38] Wise, A. F., Marbouti, F., Hsiao, Y. & Hausknecht, S. In  press. A survey of factors contributing to learners  listening behaviors in asynchronous discussions. Accepted  for publication in the Journal of Educational Computing  Research.   [39] Wise, A. F. & Chiu, M. M. 2013. The effects of summarizing  roles on learners listening behaviors in online discussions.  Paper presented at the 2013 Annual Meeting of the American  Educational Research Association, San Francisco, CA.   8. Acknowledgements  We would like to thank the Social Sciences and Humanities  Council of Canada for their generous support of this work and  Farshid Marbouti for the use of the Visual Discussion Forum and  making the adaptations to use it in this work.    56      "}
{"index":{"_id":"9"}}
{"datatype":"inproceedings","key":"Gunnarsson:2013:UPC:2460296.2460309","author":"Gunnarsson, Bjorn Levi and Alterman, Richard","title":"Understanding Promotions in a Case Study of Student Blogging","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"57--65","numpages":"9","url":"http://doi.acm.org/10.1145/2460296.2460309","doi":"10.1145/2460296.2460309","acmid":"2460309","publisher":"ACM","address":"New York, NY, USA","keywords":"assessment, blogging, knowledge community, learning analytics, liking, promoting, quality content, social blogging","abstract":"Promoting blog content is a social activity; it is a means of communicating one student's appreciation of another student's work. This paper explores the feasibility of using student promotions of content, in a blogosphere, to identify quality content, and implications for instructors. We show that students actively and voluntarily promote content, use promotion data to select which posts to read, and with considerable accuracy identify quality material. We explore the benefits of knowing which students are good and poor predictors of quality content, and what instructors can do with this information in terms of feedback and guidance.","pdf":"Understanding Promotions in a case study of student blogging.  Bjorn Levi Gunnarsson Brandeis University  Computer Science Department Waltham, MA 02454 USA  bjornlevi@cs.brandeis.edu  Richard Alterman Brandeis University  Computer Science Department Waltam, MA 02454 USA  alterman@cs.brandeis.edu  ABSTRACT Promoting blog content is a social activity; it is a means of communicating one students appreciation of another stu- dents work. This paper explores the feasibility of using student promotions of content, in a blogosphere, to identify quality content, and implications for instructors. We show that students actively and voluntarily promote content, use promotion data to select which posts to read, and with con- siderable accuracy identify quality material. We explore the benefits of knowing which students are good and poor pre- dictors of quality content, and what instructors can do with this information in terms of feedback and guidance.  Keywords Blogging, Social Blogging, Liking, Promoting, Assessment, Quality content, Learning analytics, Knowledge community  1. INTRODUCTION The research reported on in this paper analyzes learner  created content within a single class of 107 students. The class is a blended course, with lectures twice per week, and ongoing participation in an online knowledge community was a requirement. Improvements in learning are visible in the online data, but for a class of our size there is not go- ing to be as much data as there is for institution-sized data sets or MOOCS (massively open online courses). Given the smaller data set size, the type of generalizations that can be made from the data is limited, but in some ways more important, because they represent the current conditions of many learning environments.  In a traditional classroom a student typically only has access to verified high quality content like books, articles, and lectures supplied by the instructor. Online there might be videos, more articles, and tutorials by other profession- als. Other sources of content are peers. Offline peer content can be produced by study groups, group projects, or class- room discussions. Online peer content can be produced in a knowledge community, which uses technologies that enable  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK13, April 8 - 12 2013, Leuven, Belgium. Copyright 2013 ACM 978-1-4503-1785-6/13/04 ... $15.00  content creation and distribution online (e.g., wikis, blogs, forums, and chats).  Online content is an important resource for several rea- sons  giving students access to each others contributions is the basis for a knowledge community [30], with each con- tribution representing a different, but valid, viewpoint that provides scaffolding for others to build upon and improve. Different contributions and examples on the same subject are multiple representations, which are valuable for learn- ing [1], and can lead to higher order thinking [28, 14]. De- pending on the technology that mediates peer collaboration, there will be different benefits. For example, a blogosphere is especially valuable as resource for co-reflection amongst the students, which is very helpful for learning [9].  Students in our case study blogged throughout the semester, each producing over 1000 words a week. This is a tremen- dous amount of content for the students to navigate and for the instructor to assess, which is why three graduate graders and three undergraduate graders did the assessments; how- ever, these kind of resources are not typical for most classes.  The students produce content of mixed quality. Ideally, the best content is foregrounded in some way. One solution is to have the instructor and graders identify the quality content for the rest of the class. However, as the size of the class increases, the cost of finding good content, without any automated assistance, becomes quite labor intensive  perhaps prohibitively so. An alternative solution is to have students themselves locate and label important content as they read. If the students are willing to do so on a regular basis and can accurately identify the relevant material, the cost-reduction would be tremendous.  Social feedback is common in social networks, where par- ticipants can share, like, pin, upvote, or give a +. This is classified as transactional content and is an exchange of af- fect [35], e.g., one student liking another students work. We call this kind of social feedback in a knowledge building community a promotion  the reader is labeling content for the rest of the community, potentially reducing the amount of material everybody has to read to accomplish their learn- ing goals, and providing instructors with valuable informa- tion on what the students consider to be quality content. In the study presented in this paper, students promote content by attaching badges andlikesto each others contributions. Promoted material, by definition, is content the reader con- siders noteworthy and wants to publicly label it as such for the community.  What it means to promote content can vary in different applications. Social feedback can be a subjective evaluation  57  suthers Rectangle    of content given the setting, for example sharing cute pic- tures or funny jokes. In other applications it is used to ap- prove of solutions to a given problem (stackoverflow.com). In such a setting the different proposed solutions can be rated for usefulness and also tested against the given prob- lem and verified, giving objective results. In our environ- ment, the purpose is to identify high quality content using a subjective promotion tool to give objective results. The premise that noteworthy content is of high quality is not a given. This paper explores the viability of using subjective evaluation by peers in a knowledge building environment as a method for identifying high quality content.  Exploring the learner created promotions within the scope of the field of learning analytics is a perfect match. By defi- nition learning analytics apply analytical methods to learner created content [32]. Within the field researchers have cre- ated sense-making visualizations for instructors and admin- istrators [8], explored knowledge building in online social settings [16], and data mined institution-sized data sets [33, 5]. Learning analytics gives the exact framework to explore the potential of promotions.  Our primary concern is filtering for high quality content. Given the amount of content created in the blogosphere, be- ing able to sort out the important posts is valuable for every- body involved. Direct access to the highest quality content means a leaner and more efficient knowledge community. With the addition of some learning analytic methods, the potential exist, for converting student promotions into valu- able commodities for both instructors and students. For ex- ample, student promotions could be input to a mechanism that highlights quality material. These highlights could be used to reduce the amount of grading work and could also be used as a template for providing feedback to students.  In this paper, we analyze the the effect of promoting posts in a blogosphere, measure the overall value of student pro- motions as a form of assessment of quality, and identify stu- dents that act as reliable predictors of quality based on the posts they promote. This leads to a more accurate sorting of the quality content, beneficial for both the students and the instructor. The main result is that leaning analytic meth- ods verify that student promotions can be used as a basis for identifying high quality material in the blogosphere.  2. BACKGROUND Blogs are widely used as a learning tool in numerous disci-  plines and for various purposes [9, 12, 3]. The most frequent use of student blogging is as an open journal [40] or a tool of reflection [9, 26]. The core features of any student blogging system is the capability to create and edit posts. Blogs can either be a community blog (where the whole class shares a single blog) [39] or a collection of blogs  with each stu- dent owning and posting to his/her blog. The blogs can be collected into a single application, open to the class but not the public [4], or students can use a standard blog site like wordpress.com, where students connect to each others blogs via a Really Simple Syndication (RSS) feed [13].  Several researchers have reported student blogging has positive effects on, for example: higher order thinking skills [28, 14], knowledge sharing and reflection [25, 19, 11], the learning process beyond the class room [15], sense of own- ership [19], sense of community as measured by the com- munity dimension [9], and identity development [24]. Some have cautioned that student blogging does not automatically  guarantee positive learning outcomes [23, 10]. Blogs have a low technical threshold and a simple intu-  itive delivery system (click a button to write and then click another button to post). Blogs are versatile, used in many different settings and for different purposes [26]. Various methods of measuring content quality have been applied to blogs [37, 21], such as search and clustering algorithms [37], automated and voting methods based on various quality measures [22], classification of different types of blog posts using machine learning techniques [27], and using natural language processing [29]. New features have been added to blogs attempting to enhance learning, e.g., topic maps [20] and awareness graphs [17].  Given the widespread use of educational blogs, a good source of learner created content, a success story about ap- plying learning analytic methods to student blogging data will have significant value. The text of posts and comments are not the only data that can be analyzed. The number of contributions and reads can be counted, as well as any trans- actional content (i.e., promotions) that is produced by the students. Examples of data other than the raw text that has analytic value includes: counting reads [18], counting com- ments, counting external links, login data, load time, and many other features as listed in [22].  When analyzing blogs, the social aspect can not be ig- nored. A blog is meant to be read by others. Social learning analytics [16] offers great potential to enhance the learners experience. In this paper, we analyze promotions, an impor- tant social feature, as a basis for determining and filtering quality content.  3. CASE STUDY The paper presents a case study of a class on Internet &  Society. During the semester, the students read four books (Here Comes Everybody: The Power of Organizing Without Organizations; by Clay Shirky, Convergence Culture: Where Old and New Media Collide; by Henry Jenkins, The Social Life of Information; by John Seely Brown and Paul Duguid, and Remix: Making Art and Commerce Thrive in the Hy- brid Economy; by Lawrence Lessig) on the social impact of the Internet on society. During the semester the students had weekly writing assignments that were to be completed in the blogosphere.  The online part of the course functions as a knowledge community [31, 30] and a discourse community [6, 38]. A place for students to question, negotiate, share, construct, explore, and develop their understanding of the course mate- rial, assignments, and concepts. Online they can participate at their leisure in an environment where everyone have a common learning goal [34]. Student contributions to the bl- ogosphere have a social orientation [7]. Each contribution further develops their online identity [24] within the knowl- edge community and ownership [19, 15] of content.  Each student had her own blog, which was composed of multiple posts she authored. In addition to the 11 writing assignments, the student also had weekly assignments to of- ficially provide peer comments and assessment on the posts of other students. Students were also encouraged to read freely throughout the semester in the blogosphere  joining conversations that struck their interest.  Students were encouraged to leverage each others work, using the blogosphere as a resource to improve their own posts. Just browsing in the blogosphere and looking at  58    other students work has tremendous educational value [2]. While working on an assignment, it was allowed to review the posted work of other students. It was also permissible to revise posts, and revise again, up until the deadline. In this manner, the blogosphere is a platform for peer tutoring, peer assessment, and collaborative learning [36].  For each book, the students had to write both an edito- rial post and a book review post. Each post was required to be between 600-750 words in length; in practice many of the posts were longer than that. The editorials required the student to review an issue raised by the book and then take a position, either expanding on the argument of the book with examples or presenting counterarguments. Each review explicated the central argument of the entire book. Students explained the key points, referenced the editorial posts of fellow students as support for their analysis, pro- vided additional examples, and argued for (or against) the core argument of the book.  Each student was required to write two comments and two peer assessments each week (an additional 400 words). After the assignment due date, two posts were randomly assigned to each student on which she would officially comment, and another two for which she was responsible for giving peer as- sessment using a 3 point questionnaire form. The peer com- ment was posted under the students user name but the peer assessment was anonymous. We instructed that comments and assessments were to be both thoughtful and judicious. To ensure quality, both forms of feedback were evaluated. Students were also encouraged to do additional comments, respond to comments on their own posts, and promote posts by giving them likes and badges. The additional activities were not directly graded but were taken into consideration when assessing overall participation in the blogosphere.  In the later part of the semester, the students were re- quired to write three reflections that synthesized material from two or more books and referenced posts written on the books. A requirement for each reflection was to reference at least three editorials or reviews that were written by other students. The same comment and assessment process also applied to the reflections.  Because of the specific requirement of the blogging assign- ment, the students got plenty of feedback on their contri- butions to the blogosphere. The comments and peer assess- ments produced by other students were one form of feedback. Grades and publicly displayedgold stars for excellent work were given out by the TA as another form of feedback. The peer promotions (badges and likes) were a third significant form of feedback.  3.1 Student promotions in the blogosphere While browsing in the blogosphere, students could pro-  mote posts by assigning them merit badges and likes. Lik- ing a post means exactly that  the student supposedly liked it. Merit badges are a more specific type of promoting a post. Instead of clicking a button to say you like the post, you can click a button to say that you liked it because it was, for example, nicely written. The badges were:  Good example The post contains an interesting example or case as a basis for its argument.  Good question The post raises and interesting issue.  Nicely written The post is well written.  Good argument The post makes a persuasive argument.  Good references The references are interesting and relevant.  Good summary The post provides an accurate and succinct summary of an issue or the book.  There was incentive to identify high quality posts in the blogosphere  earlier posts were referenced in later assign- ments. By finding high quality posts that clearly explained a topic in one of the books, the student could more easily build her argument when reviewing or reflecting on the cen- tral issue of the book. Reading poor quality posts would not be as helpful. Searching for good quality posts takes work, but also depends on a students ability to identify what is a good post. The data will show that not all students were good at assessing quality.  The gold stars that were provided by the TA were one re- liable source of information for identifying quality content. The data shows that each gold star post was read on av- erage 44 times during the semester  more frequently than posts that did not get gold stars. Promoted posts got, on average, 30.1 reads and the average for all posts was 22.9 times per post. The data shows the number of promotions a post received was directly related to how much it was read (see Figure 1).  Figure 1: Number of promotions vs. number of reads  There are three parts to the question of whether promo- tions were a useful tool for identifying good content. Do students actually promote content (this is explored in detail in Section 4.2) and if so do they act on it As can be seen in Figure 1, the students use the promotions as a way to select which posts to read. The third part is whether, or not, the promoted content was actually of good quality, which is also explored later (see Section 4.3).  59    Our analysis will show that the class can be divided into good, average, and poor predictors of quality content. With this refinement, it becomes possible to automatically iden- tify, with a great deal of accuracy, both good and poor con- tributions. By coupling this analysis with a highlighting mechanism, it would be possible to foreground quality posts in the blogosphere. This could either be a replacement for the existing gold star mechanism or an add-on that fore- grounds material in a much more timely fashion. Also it has great potential for supporting grading as a preliminary sorting of student work.  3.2 The technology The blogging environment was developed at Brandeis Uni-  versity over a number of years; the current version is a recent rebuild by the first author of this paper. It has already been used in several classes.  Students could browse content in the blogosphere using several different views. Regardless of the view, posts were always listed in reverse chronological order, with the most recently update posts at the top of the page. For each post in a list, there is a header that includes the name of the author of the post, the title of the post, the type of post (editorial, review, or reflection), the book(s) the post dis- cussed, the number of comments the post had accrued, and the number of promotions it had received. Students could preview the first paragraph of a post by hovering over its title, and click on title to view the post in its entirety, as well as any comments that had been made on the post.  Most of the time, students would start a session by view- ing the front page, which was a list of the most recent posts. From there, a student could switch to an alternate view. Ex- amples of available views are: all posts on a specific book, all posts that have a specific type of promotion, all posts that had received a gold star, all posts the student had bookmarked within the system, and all posts the student had commented upon. It was also possible to do a keyword search to retrieve a list of all posts that contained the search term.  We have sometimes referred to the kind of blogging the students do in our environment as co-blogging. It is co- blogging as opposed to blogging, because it is a closed col- laborative blogging community. In this case study, the students still collaboratively blog, but the blogging envi- ronment includes additional social features (like the promo- tions). In this paper we will refer to this species of student blogging as social blogging.  3.3 Grading There were six graders for the Internet & Society class.  Posts were graded differently than comments and peer as- sessments. The posts were graded by three graduate stu- dents; a third of the posts were randomly assigned to each of the graders for grading after the due date of each home- work assignment. The grading was done by filling out a questionnaire of six questions. Each homework type (edi- torial, review, and reflection) had different questions. For example, grading questionnaire for an editorial post was:  1. The issue is clearly explained.  2. The opinion is interesting and substantial.  3. The references are relevant to the argument.  4. The post is well written.  5. The post demonstrates understanding of the subject of the book.  6. Overall grade for this post  The purpose of the overall grade question was to be an approximation factor. Clearly not all of the items are of equal value. A well written post might be more rele- vant than a post with good references. Generally the overall grade followed a trend set by the previous questions, but for some borderline cases the overall grade would add or deduct points based on over which border the post fit into as a whole.  Each of the questions could be given a grade between 0 and 3.  0 means that part was not completed.  1 is not good.  2 is good work.  3 exceeds expectations.  For example, no references in the post would yield a 0 for that part of the questionnaire. An exceptionally clear and thoughtful introduction to the issue would get a 3 while a post that did not conclude with a logical and original opinion on the issue would receive a 1. A post with a outstanding introduction, no references and a simple this is what the book said opinion would in turn get a total of 4 for those parts of the questionnaire. Most commonly a good intro- duction was followed by a well written account with relevant references, understanding, and opinion of the stated issue. A poor introduction followed a similar pattern.  Students were told to expect a 2 to be a good passing grade. Grading was done inside the blogosphere but in a special grading view where only the questionnaire and the text of the blog post were visible. The graders could not see the blog posts comments, likes or badges without explic- itly browsing to that blog post in the standard view. This was done to minimize any bias the posts comments and promotions could have on the grader.  The comments were graded by three undergraduate stu- dents. The comment and peer review forms were simpler  just asking if the comment or peer review met expectations  the graders could give a 0 for not completed, 1 for not good and 2 for good work.  The grading process was quality controlled in several phases. First in weekly meetings between the graders and instructor where random posts were graded together. Then, individu- ally, each grader got randomly assigned a batch of posts to grade and the head TA was then responsible for reviewing the grade submissions. The third phase, again for the head TA, was to reply to grade inquiries by students, check the post and grades and if needed contact the original grader for comments. Finally all high quality graded posts were reviewed by the head TA as a part of the gold star process.  The grading data we use in our analysis was generated by the graduate level graders who were on their second semester of grading blogosphere posts together. While not every sin- gle grade was reviewed by the head TA the graders were consistent and the grading scheme simple enough to pre- vent major discrepancies in grading. All high quality graded posts were officially reviewed and approved.  60    We evaluated of how much work per week it took to grade the posts, comments, and peer evaluations. For each assign- ment, the instructor would meet weekly with all six graders (three graduate students and three undergraduates) to give an overview of how to grade the most recent assignment  recall there was one assignment per week. These meetings typically lasted around an hour (totaling ten person hours per week). From interviews with the graduate graders (the ones who graded the posts), we learned that the average time spent on grading posts was around four hours per grader (twelve hours per week total). The average time spent grad- ing comments and assessments about three hours per grader (nine hours per week total). The average number of person hours spent grading each week was 31 hours and over the en- tire semester roughly 350 hours. Clearly, this is an enormous amount of work and many courses will not have available re- sources sufficient to commit to this level of grading. Being able to automate some of the process of identifying good content, providing feedback, and evaluating student work, would have tremendous value.  4. EVALUATION The last assignment of the semester is left out of the data,  as it is an outlier. Students participation in the last as- signment was very different than in the previous ten. This is most likely due to the fact that the semester was end- ing and students were busy finishing up their work in many classes. The data also shows declining promotion activity in assignment 9 and 10, they were sufficiently numerous to be contributors to the data.  The assignments were (numbers used in figures): (1) edi- torial on Here Comes Everybody, (2) review of Here Comes Everybody, (3) editorial on Convergence Culture, (4) review on Convergence Culture, (5) editorial on The Social Life of Information, (6) review of The Social Life of Information, (7) editorial on Remix, (8) review of Remix, (9) reflection on a cross cutting theme from two or more books, and (10) another reflection on a different cross cutting theme from two or more books.  The data compiles the promotion statistics of 92 students. The 15 students filtered out did not participate fully in the blogosphere for various reasons; for example, because they dropped the class.  4.1 Outline of the argument An outline of the evidence presented in the evaluation is  shown in Figure 2. The basic idea is to convert student pro- motions into highlights in the blogosphere or preliminary assessments for graders. The scheme will only work if stu- dents are actively promoting content during the semester (see point 1). In the case study, the raw data shows that the students were producing a lot of promotions: students voluntarily produced merit badges and likes on average at the rate of 108 per assignment. Not every post received a promotion, and those that received promotions sometimes received many.  1. Students promote posts as they read in the blogo- sphere. See Section 4.2.  2. The promoted posts are on average are of higher qual- ity than the average blogosphere post. See Section 4.3.  3. Evaluating the reliability of predictors. See Sec- tion 4.4. (a) Some students are better promoters than others. (b) Some students were poor predictors of quality ma-  terial. In fact, there were students who tended to promote poor material!  (c) As the semester progressed, all students became better predictors of quality content.  4. High quality content can be predicted based on who promotes it. See Section 4.5.  Figure 2: Summary of evidence  A second issue is whether the promotions were warranted. The data shows that the students tended to promote high quality material (2), but some students were better than other students at this activity (point 3a & point 3b). As the semester progressed, the students became more accu- rate with their promotions (point 3c). All of this evidence bodes well for the possibility that student promotions can converted into highlights for quality material and prelimi- nary assessments for graders (point 4).  4.2 Promoting content For the entire class, 89.7% of students used the promotion  feature at least once. Students did not promote a post in every assignment. In the first assignment, many students used the feature, but the number of students that promoted declined in later assignments (see Figure 3). On average, roughly a third of the students promoted content in each assignment.  Figure 3: Ratio of class giving likes  During the whole semester, the average student promoted 6.77 posts from 3.75 assignments. The number of students that gave promotions dropped considerably from the first assignment (78%) to the last assignment (16%). The mid- part of the semester was stable with only a drop in promo-  61    tion activity from 50% to 34% from assignment two through assignment eight.  Based on these numbers, and the argument below, having a third of the community promote content seems sufficient to filter content. Information from the initial promotion frenzy could be used to encourage more promotions from the portion of the community who are good at it and give feedback to the portion of the community who are bad at promoting.  Promotions have a significant influence on a which posts students choose to read as shown on Figure 1. Simply put, the more promotions a post gets, the more participation that post represents in the community. Reading a post created by another student is considered to be the most basic defi- nition of participating because of the so called lurkers  community members that only read.  Obviously there is a participation feedback loop. If a post gets a promotion, it gets more readers. These additional readers can give even more promotions which lead to more reads, and so on. In this way the highest quality posts (as deemed so by the community) have also reached the largest audience and had the greatest impact.  4.3 Quality of promoted content What was the quality, in terms of grades, of the posts that  received promotion There are three parts to the answer: 1. Higher quality posts got more promotions. (number of  promotions per post) 2. Higher quality posts were more likely to be promoted.  (promotion hit rate) 3. All types of promotions were useful (different types of  promotions)  The number of promotions per post. Figure 4 shows the total number of posts that were pro-  moted and also the total number of posts that did not get promoted, both grouped by grade. It also shows the number of total promotions for posts of each grade. It is important to note that each post can we awarded multiple promotions, so in the case of posts that received, a grade between 14 and 18 (see shaded region), received more promotions than there were posts. The graph shows that there was an interesting increase in the number of promotions on content with high grades (again see the shaded region in figure 4).  Figure 4: Posts and promotions per grade.  In other words, all posts can receive promotions, but posts with high grades get significantly (p-value: 0.002045) more promotions than posts with a lower grade (see Figure 5).  Figure 5: Average promotions per grade  The promotion hit rate. Were high quality posts more likely to get promoted than  lower quality posts Only 36% of low quality posts (graded 8 or lower) were promoted. This means low quality posts have a 36% promotion hit rate. The hit rate significantly increased to 45% for posts of average grade (graded between 8 and 13), and high quality posts (graded 14 or higher) had a hit rate of 54%, which is again a significant increase.  Different types of promotions. As mentioned before, we counted all promotions (both  likes and badges) as the same activity of promoting. If we dig a bit deeper into the data and explore each type of promotion we get an interesting picture (see Figure 6).  Figure 6: Average grade per type of like  The average grade for all posts during the semester was 10.7 (see horizontal line for average class grade). With one exception, all types of promoted content had significantly higher grades than the average class grade. Posts that had  62    been promoted by the like feature had by far the highest average grade. Posts that received more specific kinds of merit badges were graded on average with a slightly lower grade, but were still, as a whole, significantly above the aver- age grade for a post. The exception was posts that received a good question badge; these posts received an above av- erage grade, but not significantly so.  Summary. Combining all of these results  high quality posts get  more promotions, the promotion hit rate is higher for qual- ity content, and all types of promotions are useful  means promotions are a powerful method for identifying quality material: students can successfully identify high quality con- tent.  4.4 Promotion predictions Are some students better at promoting quality material  than others We wanted to know which students were con- tributing the reliable promotion data and which ones were not  they might be reliably providing promotions to poor quality material. Figure 7 shows us that some students tended to promote high quality posts, other students tended to promote low quality posts, but the majority of students are less predictable. About 25% of students regularly pro- mote post with high grades and 8% regularly promote posts with poor grades.  In Figure 7, good predictors refers to students that on average promoted posts that received a good grade (on or above the top dividing line), and poor predictors are the students that promoted posts that received a poor grade (on or below the bottom dividing line). Obviously, the posts good predictors promote can be used to highlight content in the blogosphere.  In terms of assessment, the users classified aspoor predic- torspresent an interesting case. First of all, getting them to stop promoting content will immediately increase the aver- age quality of promoted content. Secondly, this information can be used by the instructor as a teaching opportunity  these users lack an important critical reading skill.  Figure 7: Average grades vs average liked grades  The distribution is interesting in that the grades of stu- dents and the grades of the posts they like do not corre- late. We would have expected the students who promote high quality content to also able to identify quality content.  However, some students that get high grades only promote average or low quality content. Perhaps, even more surpris- ingly, there are students that receive low grades on the posts they produce, but are very good at promoting high quality content. This suggests that students are developing two dif- ferent skills: (1) writing and (2) identifying quality content. A student can be good at both, either, or neither. One of the benefits of the promotion data for instructors is that it can be used to measure the writing and critical reading abilities of each student.  Figure 8: Number of predictors per assignment  As the semester progressed, the students became better at identifying good blogosphere material (see Figure 8). The number of good predictors increased and the number of poor predictors decreased. This is an indication that the students are learning.  4.5 Summary of analysis The analysis above provides evidence that it is possible to  filter content for the purposes of either highlighting or pre- liminary assessment. Students identify high quality content using promotions. High quality content is more likely to be promoted  have a higher hit rate. Grading promoted con- tent enables the instructor to sort out the students who are good or bad predictors, thus providing another dimension to evaluation and an opportunity for instruction.  5. CONCLUDING REMARKS A large online knowledge community can be a confus-  ing place for a student, but she is not alone. Together the students can make sense of everything (but not necessarily good sense). The promotion mechanism is a social feedback feature that allows students to collectively identify content that is interesting or of particular value. In this paper we found evidence that the promotion mechanism is a signifi- cant source of information for the knowledge community and a powerful tool for assessment of quality.  In our class, students get feedback through grades, com- ments, peer assessments and promotions. Future work will evaluate the effectiveness of the promotion activity without the rigorous grading done in this class. Would student pro- motion improve over the course of the semester as the data  63    has shown, if instructors were to grade fewer posts What if the only posts graded were the promoted ones  Consider, as a possible grading scheme, a plan for the in- structor to grade only promoted posts. This reduces work for the instructor. Under this grading scheme it would be possible to ascertain which students are good predictors and which ones are bad ones, which students are less predictable, and which students dont participate. This level of effort on the instructor part is sufficient to develop some kind of high- lighting mechanism. The obvious downside of this scheme its that some students would not be receiving feedback or grades from the instructor (or the graders). A potential fix is to randomly sample, for grading purposes, the con- tributions of students who never get promoted. With this addition work, the instructor would be able to monitor the progress of students from assignment to assignment, both as readers and contributors. With regards to feedback, the students will be receiving regular feedback from the instruc- tor (but not every week). They would also be receiving peer feedback in the form of comments and promotions, and have access to highlighted content to compare to their own work or to build upon.  A good-sized sample of highlighted quality content should be sufficient for scaffolding purposes. But what about the high quality content that did not get promoted In the above scheme, some of those posts would be highlighted by the instructor after random grading. Whether or not stu- dents would be discovered as a result of their posts being highlighted, and thereafter be more likely to receive promo- tions from their peers, is an interesting question.  Alternative scenarios for promotions are possible. One where next to no high quality content is promoted at all. Analyzing the promotions in such a setting would be invalu- able for the instructor, perhaps indicating that the content created by the community is on average very poor or the readers are unable to find the actual high quality content. Both situations provide a path towards a solution. Another scenario could be where participants of the learning commu- nity are promoting content not created by themselves. Ex- ploring how well learners identify and promote high quality content that is beyond their current knowledge level would be very interesting.  Considerable amount of content was created in our knowl- edge community, created by a number of participants. Other possible scenarios for a knowledge community could be one where there are few users and little content, few users and a lot of content, or a lot of users and not much content. Each of these knowledge communities could have different prob- lems with using the promotion feature. For example, a lot of users with little content could result in over-exposure of the promotion feature; or few users with a lot of content could result in many high quality contributions being overlooked.  Because promoted content is, on average, of higher quality than non-promoted content, it is intuitive to assume that if someone likes a post, it is probable that another student will like it too. If we take into account that students dis- played skill at identifying quality content at the start of the semester, before any feedback had been given, we conclude that the promotion mechanism will prove useful in most set- tings. What the number of promotions might mean, for a post, will vary from one setting to another, but the promo- tion feature will always have social value.  6. REFERENCES [1] S. Ainsworth. Deft: A conceptual framework for  considering learning with multiple representations. Learning and Instruction, 16(3):183198, 2006.  [2] R. Alterman and B. Gunnarsson. The blogosphere as representational space. Technical Report CS-12-282, Brandeis University, 2012.  [3] R. Alterman and J. Larusson. Collaborative sensemaking in the blogosphere. Technical report, Technical Report CS-09-272, Brandeis University, Department of Computer Science, 2009.  [4] R. Alterman and J. Larusson. Student producing thick descriptions. In Proceedings of the 2011 conference on Computer support for collaborative learning. International Society of the Learning Sciences, 2011.  [5] K. Arnold. Signals: Applying academic analytics. Educause Quarterly, 33(1):n1, 2010.  [6] A. Brown, D. Ash, M. Rutherford, K. Nakagawa, A. Gordon, and J. Campione. Distributed expertise in the classroom. Distributed cognitions: Psychological and educational considerations, pages 188228, 1993.  [7] D. Cameron and T. Anderson. Comparing weblogs to threaded discussion tools. 2006.  [8] S. Dawson. seeing the learning community: An exploration of the development of a resource for monitoring online student networking. British Journal of Educational Technology, 41(5):736752, 2009.  [9] L. Deng and A. Yuen. Towards a framework for educational affordances of blogs. Computers & education, 56(2):441451, 2011.  [10] M. Divitini, O. Haugalokken, and E. Morken. Blog to support learning in the field: lessons learned from a fiasco. In Advanced Learning Technologies, 2005. ICALT 2005. Fifth IEEE International Conference on, pages 219221. IEEE, 2005.  [11] H. Du and C. Wagner. Weblog success: Exploring the role of technology. International Journal of Human-Computer Studies, 64(9):789798, 2006.  [12] L. Ducate and L. Lomicka. Exploring the blogosphere: Use of web logs in the foreign language classroom. Foreign language annals, 38(3):410421, 2005.  [13] P. Duffy and A. Bruns. The use of blogs, wikis and rss in education: A conversation of possibilities. 2006.  [14] N. Ellison and Y. Wu. Blogging in the classroom: A preliminary exploration of student attitudes and impact on comprehension. Journal of Educational Multimedia and Hypermedia, 17(1):99122, 2008.  [15] R. Ferdig and K. Trammell. Content delivery in theblogosphere. The Journal, 31(7):1220, 2004.  [16] R. Ferguson and S. Shum. Social learning analytics: five approaches. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge, pages 2333. ACM, 2012.  [17] R. Ferguson, S. Shum, and R. Crick. Discussion paper: Enquirybloggerusing widgets to support awareness and reflection in a ple setting.  [18] B. Gunnarsson and R. Alterman. Predicting failure: a case study in co-blogging. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge, pages 263266. ACM, 2012.  [19] W. Hong. Exploring educational use of blogs in us  64    education. Online Submission, 2008.  [20] T. Huang. Creating a knowledge development model for blog-based learning.  [21] T. Huang, S. Cheng, and Y. Huang. A blog article recommendation generating mechanism using an sbacpso algorithm. Expert Systems with Applications, 36(7):1038810396, 2009.  [22] M. Kargar and F. Azimzadeh. A framework for ranking quality of information on weblog. World Academy of Science, Engineering and Technology, 56:690695, 2009.  [23] S. Krause. When blogging goes bad: A cautionary tale about blogs, email lists, discussion, and interaction. Kairos, 9(1), 2004.  [24] A. Luehmann. Using blogging in support of teacher professional identity development: A case study. The Journal of the Learning Sciences, 17(3):287337, 2008.  [25] A. Luehmann and R. MacBride. Classroom blogging in the service of student-centered pedagogy: Two high school teachers use blogs. THEN: Technology, Humanities, Education, & Narrative, 6:536, 2009.  [26] B. Nardi, D. Schiano, M. Gumbrecht, and L. Swartz. Why we blog. Communications of the ACM, 47(12):4146, 2004.  [27] X. Ni, G. Xue, X. Ling, Y. Yu, and Q. Yang. Exploring in the weblog space by detecting informative and affective articles. In Proceedings of the 16th international conference on World Wide Web, pages 281290. ACM, 2007.  [28] R. Philip and J. Nicholls. Group blogs: Documenting collaborative drama processes. Australasian Journal of Educational Technology, 25(5):683699, 2009.  [29] V. Rubin and E. Liddy. Assessing credibility of weblogs. In Proceedings of the AAAI spring symposium: Computational approaches to analyzing weblogs (CAAW), pages 721, 2006.  [30] M. Scardamalia and C. Bereiter. Computer support for knowledge-building communities. The journal of the learning sciences, 3(3):265283, 1994.  [31] M. Scardamalia and C. Bereiter. Knowledge building: Theory, pedagogy, and technology. The Cambridge handbook of the learning sciences, pages 97115, 2006.  [32] G. Siemens. Learning analytics: a foundation for informed change in higher education, 2011.  [33] D. Smolin and S. Butakov. Applying artificial intelligence to the educational data: an example of syllabus quality analysis. In Proceedings of the 2nd International Conference on Learning Analytics and Knowledge, pages 164169. ACM, 2012.  [34] J. Swales. Approaching the concept of discourse community. 1987.  [35] N. Tichy, M. Tushman, and C. Fombrun. Social network analysis for organizations. Academy of Management Review, pages 507519, 1979.  [36] K. Topping. Trends in peer learning. Educational Psychology, 25(6):631645, 2005.  [37] B. Ulicny, K. Baclawski, and A. Magnus. New metrics for blog mining. Technical report, DTIC Document, 2007.  [38] J. Wertsch and J. Wertsch. Voices of the mind: Sociocultural approach to mediated action. Harvard  University Press, 1991.  [39] J. Williams and J. Jacobs. Exploring the use of blogs as learning spaces in the higher education sector. Australasian Journal of Educational Technology, 20(2):232247, 2004.  [40] J. Zagal and A. Bruckman. Gamelog: fostering reflective gameplaying for learning. In Proceedings of the 2007 ACM SIGGRAPH symposium on Video games, pages 3138. ACM, 2007.  65      "}
{"index":{"_id":"10"}}
{"datatype":"inproceedings","key":"Halatchliyski:2013:AFI:2460296.2460311","author":"Halatchliyski, Iassen and Hecking, Tobias and Gohnert, Tilman and Hoppe, H. Ulrich","title":"Analyzing the Flow of Ideas and Profiles of Contributors in an Open Learning Community","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"66--74","numpages":"9","url":"http://doi.acm.org/10.1145/2460296.2460311","doi":"10.1145/2460296.2460311","acmid":"2460311","publisher":"ACM","address":"New York, NY, USA","keywords":"idea flow, knowledge artifacts, main path analysis, scientometrics","abstract":"This paper provides an introduction to the scientometric method of main path analysis and its application to detecting idea flows in an online learning community using data from Wikiversity. We see this as a step forward in adapting and adopting network analysis techniques for analyzing the evolution of artifacts in knowledge building communities. The analysis steps are presented in detail including the description of a tool environment (workbench) designed for flexible use by non-computer experts. Through the definition of directed acyclic graphs the meaningful interconnectedness of learning resources is made accessible to analysis in consideration of the temporal sequence of their creation during a collaborative process. The potential of the method is elaborated for analyzing the overall learning process of a community as well as the individual contributions of the participants.","pdf":"Analyzing the Flow of Ideas and Profiles of Contributors   in an Open Learning Community    Iassen Halatchliyski   Knowledge Media Research Center   Schleichstr. 6  72076 Tbingen, Germany   +49 7071 979 303   i.halatchliyski@iwm-kmrc.de  Tobias Hecking, Tilman Ghnert,  H. Ulrich Hoppe   University of Duisburg-Essen  Lotharstr. 63/65   47048 Duisburg, Germany   +49 203 379 3553   {hecking,goehnert,hoppe}@collide.info        ABSTRACT  This paper provides an introduction to the scientometric method   of main path analysis and its application to detecting idea flows in   an online learning community using data from Wikiversity. We   see this as a step forward in adapting and adopting network   analysis techniques for analyzing the evolution of artifacts in   knowledge building communities. The analysis steps are   presented in detail including the description of a tool environment   (workbench) designed for flexible use by non-computer experts.   Through the definition of directed acyclic graphs the meaningful   interconnectedness of learning resources is made accessible to   analysis in consideration of the temporal sequence of their   creation during a collaborative process. The potential of the   method is elaborated for analyzing the overall learning process of   a community as well as the individual contributions of the   participants.         Categories and Subject Descriptors  H.3.1 [Content Analysis and Indexing]: Abstracting methods;   H.5.3 [Group and Organization Interfaces]: web-based   interaction; K.3.1 [Computer Uses in Education]: collaborative   learning; K.4.3 [Organizational Impacts]: computer-supported   collaborative work;    General Terms  Algorithms, Main Path Analysis   Keywords  Knowledge Artifacts, Idea Flow, Scientometrics, Main Path   Analysis   1. INTRODUCTION  Nowadays, it is commonplace to perceive learning and knowledge   building as closely related activities on the web. Bereiter and   Scardamalia [4] point out that knowledge building is essential for   learning but has a wider scope in that it is not necessarily limited   to explicit learning scenarios. Knowledge building is based on the   creation of epistemic artifacts [15] that are shared in a   community. Research is an example of a knowledge building   activity that takes place in a scientific community (without being   characterized as learning). According to Scardamalia and Bereiter   [21], knowledge building pedagogy takes scientific knowledge   creation as a blueprint of the learning processes to be facilitated.   During a knowledge-building process learners are expected to   discuss ideas and develop their shared knowledge in the manner   of scientists. The philosophical foundation of this view dates back   to Popper [18], who explains the development of scientific   knowledge as a constant process of emergence of new ideas and   their gradual improvement or abandonment after discovering   contradictory evidence. In fact, any learning community will   define concepts and build its knowledge base in a similar way   [24].   Due to the relation between scientific production and learning in   communities, it should be possible to study both phenomena   using the same analytical approaches. Scientometrics as a research   field is particularly concerned with the quantitative measurement   of scientific work, so it offers a variety of approaches that are new   to the area of learning analytics but might be fruitful to borrow.   Scientometric methods are tailored for the analysis of knowledge   artifacts, most prominently publications, and their authors. One   well-known method is the calculation of the h-index as a measure   of scientific reputation [13]. In the context of learning   communities, however, individual excellence is not a primary   concern. Rather more interesting would be an approach to the   long-term characteristics and the dynamics of interactive learning   environments.    Hummon and Doreian [14] have proposed a method to detect the   main idea flows based on citation networks using a corpus of   publications in DNA biology as an exemplar. Our work reported   in this paper takes this method, namely main path analysis, as a   starting point in the analysis of a broad range of knowledge   processes that take place in formal as well as informal   collaborative settings.   After a first promising application of main path analysis to   networks of knowledge artifacts created for educational purposes      Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. To copy   otherwise, or republish, to post on servers or to redistribute to lists,   requires prior specific permission and/or a fee.   LAK '13, April 08 - 12 2013, Leuven, Belgium.   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   66       [10], we now want to elaborate on the adaptation and adequate   formalization of the method. Our guiding question in this   endeavor is: What kind of insights can be gained from the main   path analysis of knowledge creation in online learning   communities We will explore this question using data from   Wikiversity as an example1. Wikiversity is understood by its   active members as an Open Learning Community in which   users can actively produce learning resources for a broad range of   topics in the form of web-based hypermedia. In our view it   represents a challenging and yet relevant field for exploring the   potential of scientometric methodology to tackle the dynamics of   computer-supported learning processes.   2. BACKGROUND   2.1 Community learning  New knowledge in the world might be the accomplishment of an   individual, but it is inconceivable without the body of previously   existing knowledge that in turn has been established by many   other individuals. Consequently, learning and development of   new knowledge require to be examined in the context of the   community where they take place.   Online communities like Wikiversity facilitate learning through   the creation of a shared knowledge base in the form of digital   artifacts such as texts, pictures or other multimedia. Users can   passively learn by making use of the existing artifacts. Users can   also actively learn by participating in the creation of new artifacts.   The knowledge building theory suggests incorporating such   activity in formal education [21]. Students are expected to benefit   from self-motivated exploration of knowledge areas when they   share and build on each others findings in a collaborative online   environment. During this long-term process the shared community   knowledge develops as ideas are constantly being improved by the   participants.   The collaborative production of digital knowledge artifacts has   become widespread since the emergence of Web 2.0. Widely and   easily available tools such as wikis afford a long-term process of   mass collaboration, as artifacts are built piece-by-piece and   individual contributions can have a variable size. Moreover, a   single contribution to an artifact can be revised or be built upon in   order to produce newer versions. Every change to the shared   artifacts of a wiki community can be logged as an individual   contribution activity, but the ongoing development of the   knowledge base is an emergent product of the community efforts   as a whole. Intersubjectivity and shared meaning-making are   epiphenomena of the interaction among individuals in a   community [24]. From the systemic view of the co-evolution   model of individual learning and collaborative knowledge   building [5], a community and the participating individuals   function as two different types of systems that co-evolve through   mutual fertilization. Knowledge development is reflected in the   changing shape and content of the artifacts.   Knowledge artifacts often hold connections among themselves   that are marked by higher-level semantic structures like topical   relations, problem-solution chains, discourses, etc. Regardless   whether these connections are deliberately made by the   participants in a community or whether they are automatically   produced by the online environment, in any case hypermedia links                                                                      1 http://www.en.wikiversity.org/   bear meaning. That meaning is an integral part of the knowledge   created by a community. It is also subject to change, as   connections are added or deleted in parallel with the artifact   development.    In sum, the learning in a community represents a complex process   that is dependent on the activities of many participants and   supported by the use and development of artifacts as learning   resources. The process evolves with the constant change of the   shared knowledge base at the level of single resources or their   interconnections.   2.2 Temporality of a learning process  The learning of an individual or of a whole community is a   process that essentially develops over periods of time. New   knowledge is built upon the existing knowledge. A knowledge   base develops gradually as its information contents evolves.   Single ideas become more concrete, they can flow together or split   into independent directions, marking a convergence or a   divergence in the development process [9]. At a higher level of   abstraction, the interconnections within the knowledge base also   develop when new ideas are added to the existing content, or   when the already existing connections are subsequently changed.   All these changes are themselves a focal point of study in order to   understand the corresponding learning processes. Accordingly,   the temporal dimension should be regarded as a main component   of learning analytics. However, the modeling of the overall   process of knowledge development is challenging, as it should be   kept track of the sequential relations between all the changes in   the knowledge base. Any aggregation across time easily leads to a   biased analysis of individual and community-level variables. A   longitudinal study of different points in time is also an   unsatisfactory option, as it misses out the authorship of the   changes that have been made between the chosen time points.   Especially difficult is to grasp the nonlinear flow of ideas that is   characteristic of any learning process.   Previous work in the area of computer-supported learning has   paid attention to the interactivity of collaborative processes and   thereby implicitly to learning dynamics. Environment data logs   have been used to describe and map interaction patterns. Their   interpretation has often been supported by additional analysis of   the content in the case of discussion board messages (see for   example Hara et al. [11] and Schrire [23]). Suthers et al. [25] also   presented a universal framework for describing interactivity in the   form of uptakes between contributors independent of the   environment that is used. Nevertheless, the field of learning   analytics still needs a method to address the temporality of   learning processes quantitatively. Aspects that have to be further   kept into account include: who influenced whom, which ideas   were taken up in later stages and which were not, and how   differently do the participants contribute to the overall learning   process. The method should also be adaptable to the multiplicity   of learning environments and communities that have emerged   with Web 2.0.   Social network analysis (SNA) has been used in various areas,   including computer-supported collaborative learning   [1][6][12][19]. The basic approach relies on representing   communication events as links between the actors in the network.   The resulting network structure will very much depend on the   time span during which these events are collected [28]. However,   the target representation does no longer represent temporal   67       characteristics. This is the reason why SNA has been criticized for   eliminating time. Although advances are being made to analyze   the development of networks, these rather rarely address true   network dynamics. Process temporality represents a major   dimension of online learning and should not be ignored in an   analysis. In this paper we present a network analysis technique   that can explicitly address learning dynamics in the context of an   open learning community.   3. USING MAIN PATH ANALYSIS TO   MODEL KNOWLEDGE EVOLUTION   3.1 Analyzing actor-artifact networks  The knowledge building process develops around the creation of   knowledge artifacts. A specific version of a so-called two-mode-  network can be build on the basis of the relation between the actor   (or author) and the artifact (or product). In the SNA methodology   [26], such two-mode-networks are also called affiliation networks.   In the pure form, these networks are assumed to be bi-partite, i.e.   only alternating links actor-artifact (created/modified) or   artifact-actor (created/modified-by) would be allowed. Using   simple matrix operations such bi-partite two-mode-networks can   be folded into homogeneous (one-mode) networks. Here, e.g.,   two actors would be associated if they have acted upon the same   artifact. We would then say that the relation between the actors   was mediated by the artifact. A typical example of such a   transformation is offered by co-publication networks based on co-  authorship. Similarly, we can derive relationship between artifacts   by considering agents (engaged in the creation of two different   artifacts) as mediators.    The pure view of actor-artifact relations as bi-partite networks   has a clear mathematical-operational structure. However, there are   good reasons to extend this approach: Both, actors and artifacts   may be interrelated in other ways than by this type cross-wise   mediation. For instance, social relations between actors may   operate independent of the artifact mediation. Semantic relations   may be salient between knowledge artifacts as in the semantic   web. Mika [17] was one of the first to elaborate on methods and   potential gains of blending social and semantic network   structures. Other approaches allocate actors and artifacts on   different layers of a multi-layer model with homogeneous relation   within each layer and heterogeneous relations in between [20].   Such multi-relational representations may appear to be superior in   expressiveness, however, operations in such structures are more   difficult to define.   As any other network representation also actor-artifact networks   fail to capture the notion of time explicitly. However, time may be   implicitly modeled in the network relations. In the scientometric   field, this is the case for citation networks: If a publication X cites   another publication Y, we can safely assume that Y is older than   X. The ensuing network structure does not contain cycles   (excluding specific rare cases of cross-citation). The main path   analysis method builds on such acyclic citation networks and can   also be adapted to the dynamics of networks of knowledge   artifacts built in the process of online collaborative learning.   3.2 Main path analysis  The main path analysis [14] is a network analysis technique for   the scientometric study of scientific citations over a period of   time. Its major application is the identification of key publications   in the development of a scientific field. While a lot of   scientometric methods, such as the analyses of co-citation and co-  authorship networks, accent the semantic structure of scientific   work, the main path analysis additionally considers the temporal   structure of development. Temporality is explicitly accounted for   through the very definition of a directed acyclic graph (DAG)   where nodes are single publications and directed edges represent   citations between publications. The direction of an edge   corresponds to the flow of knowledge from the cited publication   to the citing publication. Therefore, these links incorporate both   the dimension of content relations and the temporal order of the   contributions.    A DAG always contains at least one node with no ingoing edges   (i.e., a source) and at least one node with no outgoing edges (i.e.,   a sink). In the citation network of scientific publications within   one field, often one important publication is chosen as a starting   point of the development of the field. This publication represents   the first source. Later on, other sources can become prominent   and cited, even if they have not cited previous publications in the   field. Sink nodes then represent either unimportant or very new   publications that have not been cited yet.    The main path can be described as the most used path in a citation   network taking all possible paths from the source nodes to the   sink nodes. This most used path can be found by a two step   procedure where, firstly, the traversal counts for each edge are   calculated as the number of paths between each source and sink   nodes that go through this edge and, secondly, an algorithm is   used to identify the main path based on the edge traversal counts.   This paper employs the search path count (SPC) algorithm [2]   that introduces one virtual source node and one virtual sink node   and links these to each of the actual source and sink nodes,   respectively. In the example in Figure 1 the new nodes are 1 and   10. Their only purpose is to simplify the original procedure [14]   of weight calculation for the edges connecting the real nodes.   Starting at the fictive source node, the main path is identified by   successively following the edge with the maximal weight to the   next node until the fictive sink node is reached. At node 7 in   Figure 1, there are two possible alternatives to reach the next   node, because both outgoing edges have the same traversal   weight. In this case the main path branches.     Figure 1. Example of a main path calculation.   68       The SPC algorithm might present a too strict approach to the idea   of main path depending on the nature of the graph. For the case   when the analysis requires a broader view on the main   contributions in a field Liu and Lin [16] suggested lowering the   search constraint by defining a threshold. By this modification in   each step one chooses not only the edges with the maximum   weight but also edges with weight above a certain percentage of   the maximum weight. This procedure was called multiple main   paths analysis [16].   Methods related to the main path analysis represent a structural   approach that is appropriate for addressing the dynamics of online   community learning. Depending on the nature of hyperlinks, a   DAG may trace the flow of idea influences or the change in   meanings that marks knowledge development. The technique   allows identifying the most influential contributions and their   authors in the course of building up a community knowledge base   over time. It also facilitates the characterization of the overall   discourse trajectory in collaborative learning [9].   4. WIKIVERSITY ENVIRONMENT AND   DATA   4.1 General description  Wikiversity is an online learning environment operating on a wiki   technology. Like its larger and also older sister projects Wikipedia   and Wikibooks Wikiversity is offered in many languages and   directed at any Internet user. It is not designed as an online   version of a university structure providing courses or exam   certificates. It is rather an open space for collaborative learning to   be used by any groups of participants according to their learning   goals. A major feature is the openness of the created artifacts and   the community practices to constructive suggestions and   participation by any interested user. In effect, Wikiversity as a so-  called open learning community constantly developing   accumulates a rather diverse body of many types of learning   resources that are loosely structured in topics reaching from   accounting to zoology.   The project pages in Wikiversity categorized under the same   category are often set up by different users and may serve   different purposes. Nevertheless, there are often hyperlink   interconnections between them and contributors often join   multiple projects after they have been started. Because of the   openness there is a great variety of participation modes within and   between the different topic categories.   4.2 Extracting wiki data for main path   analysis  As explained in section 3 the main path analysis has originally   been developed as a method to investigate the main discourse   structure of scientific fields, using networks of publications linked   by citations. However, the analysis method is not restricted to this   field of application. Moreover, it can be applied to any kind of   directed acyclic graphs (DAG). In this paper we show how to   employ the main path analysis approach to examine the   development of interconnected learning resources related to a   knowledge domain in the context of a wiki environment.   For all analyses presented in this paper we do not examine the   complete data of a wiki but make use of the concept of   MediaWiki2 categories in order to identify the body of artifacts   related to a specific knowledge domain. Each wiki page can be   categorized under one or more categories. The categories are   themselves structured in subcategories. The actual data gathering   process usually starts with extracting the complete subcategory   structure by following subcategory link structures of arbitrary   length starting from a given category. In a second step all pages   which are categorized into at least one of the categories found in   this structure are identified. It is not mandatory that each wiki   page is categorized, but approximately 70 percent of all articles in   the English Wikiversity belong to at least one category. Thus, we   assume that our procedure yields a complete selection of the   major learning resources in a knowledge domain. The opposite   case of wrongly considering pages that are unrelated to a domain   needs to be eliminated. This can happen when complete   subcategory structures are extracted. One example is the category   Electrical engineering which contains Wikiversity as a   subcategory with its large number of administrative pages that are   factually unrelated to electrical engineering. Therefore, a list of   subcategories for exclusion from the extraction process needs to   be predefined.   As a next step, a directed acyclic graph is constructed describing   the complete flow of knowledge within a single domain in a wiki.   Networks of hypermedia resources in a wiki are analogous to   networks of publications that are interconnected by citations. Wiki   pages can be regarded as publications that are linked by   hyperlinks instead of citations. Both, citations and hyperlinks   have a direction indicating flow of knowledge from a source (i.e. a   cited paper or a hyperlinked page) to a target (i.e. a citing paper or   a hyperlinking page).   In contrast to scientific publications, which are published once   and then stay the same from that point on, wiki pages evolve over   time under the collaborative efforts by community members. The   temporal stability of publications is crucial for the generation of a   DAG from citation networks. Publications and the citations   featured in their content are published once and do not change   over time. Moreover, only works that have already been published   can be cited. Therefore publication networks form a DAG in a   natural way. This, however, does not apply to wiki pages as their   content can be changed at any time. Furthermore, it is a quite   natural situation that one page is hyperlinked to a second page   and, at the same time, the second page links back to the first one,   thus introducing a cycle. To overcome this problem we propose   using revision information.    Regarding stability over time, revisions of a wiki page behave like   classical publications. They are created (published) at a certain   point in time and do not change later on. A change to a wiki page   will result in a new revision and thus a modified content of that   page but not in a modification of the former revision. This   approach suggests using page revisions instead of wiki pages as   nodes in a DAG extracted from a wiki data. We distinguish   between two types of directed edges in such graphs: update edges   and hyperlink edges. Update edges can be introduced between   any two directly subsequent revision nodes that belong to the   same page. Update edges are directed from the older revision to   the newer, updated revision and, thus, represent knowledge flow   over the course of the collaborative process on a single wiki page.                                                                      2 http://www.mediawiki.org/   69       Hyperlink edges can be traced between two revision nodes that   belong to different pages with a hyperlink pointing from one to   the other. Hyperlinks can be interpreted as inversely directed   knowledge flows, so in the proposed DAG hyperlink edges go in a   direction opposite to the direction of the hyperlinks. The target   revision node includes the hyperlink in its content and thus   belongs to the hyperlinking page. The corresponding source   revision node has to be inferred by taking the creation time of the   revisions into account, because the hyperlink used in a wiki   almost exclusively points to a page and not to a specific revision.   The source revision node of a hyperlink edge can be established   as the latest revision of the hyperlinked page at the moment of   creation of the target revision. Thus, a hyperlink edge in the DAG   starts at the latest revision of a hyperlinked page relative to the   creation time of the relevant hyperlink and points to the revision   node of another page containing that hyperlink.     Figure 2. On the left: a DAG with all extracted edges.    On the right: the graph after filtering out redundant hyperlink   edges.      The described construction procedure results into a two-relational   DAG that feature update edges between revisions of a single page   on the one hand and hyperlink edges between revisions of two   related pages on the other hand. The procedure also guarantees   that all update and all hyperlink edges are directed from a   preceding revision to a succeeding revision in time. An example   for such a DAG can be seen on the left-hand side of    Figure 2. All edges are pointed downwards depicting the   knowledge flow over time. Update edges are represented as gray   arrows, hyperlink edges are represented as (solid and dashed)   black arrows. The dashed arrows are redundant hyperlink edges   that point to all subsequent target node revisions that retain the   same hyperlink after it has initially been added to the content of   the hyperlinked page. All redundant hyperlink edges are filtered   out in the subsequent step of our procedure as shown on the right-  hand side of Figure 2. The rationale behind this step is that a   knowledge flow between two wiki artifacts is established once by   the initiating act of a hyperlink creation between them.   Together with author information for the single revisions this   filtering allows determining who the initiator of a certain link is.   Additional information used for the author profile analysis   presented in section 4.4 is time and size of single revisions. While   for the construction of the DAG the time information is only   needed for ordering the revisions from older to newer the absolute   time information on revisions allows distinguishing between   continuous updates of a page in a short time interval and more   separated edits in case of long intervals between the edits which is   also used for the author profile analysis.   In order to visualize the main paths of idea flows in a wiki we use   the visual metaphor of a swim lane diagram introduced in   Figure 2. All revisions of one page are represented as nodes   connected by update edges and ordered in a vertical line. The   lines of revisions of different pages are drawn parallel forming   vertical swim lanes. The titles of the single wiki pages are shown   as labeled nodes at the top of the diagram. Hyperlink edges   between different pages are depicted as diagonal lines crossing the   swim lanes. Time is represented on the vertical axis along the   swim lanes. For any pair of nodes the node closer to the top is the   earlier of the two revisions. This is true for pairs of revisions of   the same page as well as for revisions of different pages. Each   single revision is labeled with an identifier of its author.   4.3 Analyses and results  Medicine is a well developed category in Wikiversity and hence it   is a good choice as an example dataset to explain our analysis   method. We first applied simple main path analysis with SPC (see   section 3.2). The result is depicted in Figure 3. The resulting main   path is traced by articles from the subcategory gynecology starting        Figure 3. Simple main path in the medicine domain   70       with an article about the historical development of gynecology.   This indicates that articles of this subcategory are well connected   by page links and that the articles are well maintained with long   update chains.The multiple main paths method as explained in   section 3.2 was also applied in order to broaden the range of   important article topics and medicine subcategories in the further   analysis. Figure 4 shows the multiple main paths as a swim lane   diagram with additional branches of nodes and edges with a   branching weight starting with 40 percent of the maximum edge   weight at a split point. Only at this threshold new article revisions   appear on the main path. Articles on diabetes as well as tutorials   on medical advice are now visible in addition to the other   important contributions.        Figure 4. Multiple Main Paths in the medicine domain      Conspicuous about both the simple main path as well as the   multiple main paths of the medicine category is that interlinking   between articles has taken place in the early phase of the   development of the medicine category. In this phase the main path   branches into revisions of several special articles.   This may indicate that in the beginning, short after the category   medicine was founded, the authors in this category built up the   basic structure of the knowledge domain. The main relations and   idea flows between the learning materials were established early   in the development of the domain. After that the authors have   been focusing on elaborating the articles without introducing new   important hyperlinks. The overall picture of the learning process   in this domain suggests a divergent evolution of ideas after an   initial period of mutual fertilization between different topics. This   conforms to the idea of groups of learners that followed different   interests in the medicine domain with little inter-group   collaboration on the creation of new shared learning resources.   4.4 Author profiles  As explained in section 4.1, Wikiversity is an open environment   and so there is no standard guideline on how authors should   interact and use the environment. However, our data reveals   differences in the contribution activity of authors that can be   interpreted in terms of a division of roles of contribution to the   flow of ideas in a Wikiversity knowledge domain. Authors with   many contributions mostly work on one particular article,   performing many successive revisions and probably creating the   largest amount of content in the article. We call them workers.   Other authors are specialized in linking articles and emphasize the   relations between different topics. They can be regarded as   collectors because they channel the idea flows stemming from   different articles to create more general articles. A third profile of   authors is that of initiators, who create important article   revisions that are often referred by other articles later on.   It can be misleading to define author profiles by only regarding   the number of created hyperlinks and article revisions. The   articles, and thus the contributions to them, are not of equal   importance to the collaborative learning process of the   community. Many articles are stubs and are not interlinked with   any other articles within the corresponding category. However,   the results of the main path analysis of a domain can be   incorporated in the analysis of the author profiles. Isolated and   largely unimportant articles are not part of the main path, so each   update or linking contribution of an author to an article can be   evaluated differently depending on whether it is part of the main   path(s). As discussed in section 4.3, the recognition of a single   main path leads to a strong focus on a small number of revisions   and articles from a restricted subcategory. Hence, in this paper the   author profiles are related to the extracted multiple main paths. In   this way, a more balanced view on activity and division of roles of   authors can be reached.      Table 1. Sample author profiles in the medicine domain   Author   Id   # contributions   / on main path   # received   pagelinks / on   main path   # established   links / on main   path   10242 272 / 34 10 / 1 14 / 4   15539 253 / 21 20 / 10 15 / 8   12385 98 / 0 1 / 0 0 / 0   9357 349 / 10 2 / 0 1 / 0      Table 1 shows the profiles of five different authors in the category   medicine. The profiles of authors 12385 and 9375 clearly show   that output quantity, that is, the number of contributions does not   necessarily correspond to output quality, that is, the importance   for the evolution of the discourse structure in a Wikiversity   category. For example, author 9375 has the highest number of   contributions among all authors in the medicine category but only   ten of these contributions appeared on the main paths.   Furthermore, this author was not significantly involved in   establishing connections between articles, that is, in facilitating   knowledge flows. The ten main contributions of this author were   done successively to the same page Diabetic foot exam, as   shown by the multiple main paths diagram in Figure 4. It seems   71       that author 9375 contributed by mainly constructing the content of   particular articles, and so this author can be profiled as a worker,   who focused on elaborating articles.   A good example for a collector is author 10242. This author was   also very active in the medicine domain, and the revisions made   by her or him are often part of the main paths. As Table 1 shows,   this author created more links to other articles, but her or his own   revisions received less links. Exemplary of such collective   behavior are the links established by author 10242 between the   articles Diabetes mellitus, Diabetes Type I and Diabetes   Type II. The high number of successive contributions of this   author was followed by long chains of updates on several articles.   Author 15539 was also very active and made important   contributions that appear on the multiple main paths. This author   edited a large number of different articles and also created a lot of   links between them. She or he can be regarded as an initiator   because the article revisions of the author are more often linked by   other articles than they contain link to other articles. In contrast to   authors like 10242 this author did not perform long chains of   successive page revisions but rather authored early article   revisions, as can be seen on the simple as well as the multiple   main paths in Figures 3 and 4 respectively.   5. DEVELOPMENT OF ANALYTICS   TOOLS  The analysis processes described in this paper have been   integrated into our network analytics workbench. A form of this   workbench is used in the ongoing EU project SISOB3 which   has the goal to measure the influence of science on society based   on the analysis of (social) networks of researchers and created   artifacts. One area of research in this project is knowledge   sharing. Thus the analysis techniques based on main path analysis   presented in this paper are also of essential value in the project   context.   We conceive workbenches as a general type of software   environments designed to serve active and skilled users, without   assuming the users to be computer experts. We have decided to   develop a network analytics workbench as a web-based   environment for several reasons, such as ease of deployment,   access and update, and independence of the local computing   facilities and devices. An important part of our experience with   network analysis and network analysis tools is the need to   combine several tools even for a single analysis process. The use   of several tools sometimes also results in the need for conversion   between the different data formats used by these tools. Therefore   one important goal behind the development of the network   analytics workbench is the integration of multiple tools and   conversion mechanisms into one interface.   The workbench provides readily available processing chains for   known use cases and furthermore allows for setting up new ones.   The user interface (UI) is built upon a pipes and filters metaphor   for processing chains in order to reduce the complexity of the   underlying system for users who are not computer experts. An   example of the UI which has been created using the WireIt4   JavaScript library can be seen in Figure 5. In using the pipes and                                                                      3 http://sisob.lcc.uma.es/   4 http://neyric.github.com/wireit/docs/   filters metaphor and being web-based the workbench is similar to   mashup projects like YAHOO pipes5.    In contrast to these projects, the actual processing of data in our   workbench is not part of the user interface code itself but is done   by a multi agent system which is controlled by the workbench.   The multi agent system approach allows for combining several   mostly independent tools into one workflow. These tools can be   existing tools as well as newly developed tools. Examples for   existing tools which have been successfully integrated into the   workbench are the network text analysis tool AutoMap [7], the   network analysis tool Pajek [3] and a wrapper for the R language6.   Examples for newly developed components are a MediaWiki   extraction component based on the mechanism presented in this   paper and a main path analysis filter also used for the analyses   presented in this paper. The communication between the web-  based user interface and the agents is based on the SQLSpaces   [27], an implementation of the tuple space architecture [8]. From   the user interface a description of the constructed workflow is   posted into the SQLSpaces server, which contains a message for   each agent (filter) type that is part of the workflow. These   messages contain information about the input data and the   parameter configuration of that filter.   Figure 5 shows one of the workflows used for the analyses   described in this paper. The first filter is used to provide input for   the following filters. In this case it is a filter which connects to a   MediaWiki database with Wikiversity data and creates a DAG for   a given category from it. The extraction process follows the   approach outlined in section 4.2 of this paper. The filter accepts   two parameters. The name of the category for which the DAG   should be extracted is a mandatory parameter. The second   parameter accepts a list of categories to be excluded from the   search and is optional. The next filter in the workflow presented   here just duplicates all input into two parallel outputs. Thus, it   allows performing different analyses on the same possibly   preprocessed input data in one workflow. In this example the two   outputs are used to perform main path analysis and analysis of   author profiles in the same category of a wiki, as present in this   paper (sections 4.3 and 4.4). On the left hand side the Main Path   Analysis filter allows selecting a weighting scheme to be used in   the main path analysis and defining a threshold for the multiple   main path analysis. The results of this filter are then visualized   using the swim lane metaphor also used throughout this paper.   The other branch of the duplicator leads into the Main Path Role   Assignment filter which generates the tables used for the author   profile analysis as described in section 4.4. These tables are then   fed into the Result Downloader which allows downloading these   results to the local machine for further usage.   6. CONCLUSION  Our approach presented in the paper is the first application of   scientometric methodology for analyzing the flow of ideas in the   context of an open learning wiki environment. Taking the   knowledge domain of medicine in Wikiversity as an example, we   showed how main path analysis can be employed to various kinds   of knowledge artifacts that are collaboratively created by web-  based communities where learning and knowledge building are   supported.                                                                      5 http://pipes.yahoo.com/pipes/   6 http://www.r-project.org/   72          Figure 5. Screenshot of the Network Analytics Workbench      The paper presented a procedure for creating directed acyclic   graphs in this context and for illustrating the obtained main idea   flows in swim lane diagrams. Our visualization technique allows   for a unified view of multiple relationships within an artifact   network of knowledge flows. We further provided sample results   and interpretations in order to characterize the contribution   profiles of different authors. We would plausibly assume that   contributions that lie on a main path have been more valuable for   the elaboration of ideas in the knowledge network. Finally, our   methods have been embedded into a web-based analytics   workbench that supports the definition and re-use of analysis   modules in a user-friendly visual environment.   For our future work, we plan to elaborate on the characterization   of contributions and contributors also in other educational   knowledge building scenarios. It appears to be promising to   provide moderators, teachers, tutors or the productive teams   themselves with results of such analyses, in order to support   reflective practices [22]. This will raise further challenges   regarding visualization and cognitive ergonomics.   7. ACKNOWLEDGMENTS  The work presented in this paper was partially conducted in the   context of SISOB, which is funded by the European Community   under the Science in Society (SIS) theme of the 7th Framework   Programme for R&D (Grant agreement 266588). This document   does not represent the opinion of the European Community, and   the European Community is not responsible for any use that might   be made of its content.   8. REFERENCES  [1] Aviv, R., Erlich, Z., Ravid, G., and Geva, A. 2003. Network   Analysis of Knowledge Construction in Asynchronous   Learning Networks. Journal for Asynchronous Learning   Networks, 7, 1-23.   [2] Batagelj, V. (2003). Efficient Algorithms for Citation   Network Analysis. arXiv:cs /0309023 [cs.DL].   http://arxiv.org/abs/cs.DL/0309023   [3] Batagelj, V. and Mrvar A. 1998. Pajek: A program for large   network analysis. Connections, 21, 47-58.   [4] Bereiter, C., and Scardamalia, M. 2003. Learning to Work   Creatively with Knowledge. In Powerful learning   environments: Unravelling basic components and   dimensions, E. De Corte, L. Verschaffel, N. Entwistle, and J.   van Merrinboer, Eds. Elsevier Science, Oxford, 73-78.   [5] Cress, U., and Kimmerle, J. 2008. A systemic and cognitive   view on collaborative knowledge building with wikis.   International Journal of Computer-Supported Collaborative   Learning, 3, 105-122.   [6] de Laat, M., Lally, V., Lipponen, L., and Simons, R.-J. 2007.   Investigating patterns of interaction in networked learning   and computer-supported collaborative learning: A role for   Social Network Analysis. International Journal of   Computer-Supported Collaborative Learning, 2, 87-103.   [7] Diesner, J., and Carley K. 2005. Revealing Social Structure   from Texts: Meta-Matrix Text Analysis as a novel method   for Network Text Analysis. In Causal Mapping for   73       Information Systems and Technology Research: Approaches,   Advances, and Illustrations, V.K. Naraynan, and D.J.   Armstrong, Eds. Idea Group Publishing, Harrisburg, PA, 81-  108.   [8] Gelernter, D. 1985. Generative communication in Linda.   ACM Trans. Progr. Lang. Syst. (TOPLAS), pp. 80-112.   [9] Halatchliyski, I., Kimmerle, J., and Cress, U. 2011.   Divergent and convergent knowledge processes on   Wikipedia. In Connecting Computer-Supported   Collaborative Learning to Policy and Practice: Conference   Proceedings (Hong Kong, China, July 4-8, 2011). CSCL   2011. International Society of the Learning Sciences, Hong   Kong, China, 2, 566-570.   [10] Halatchliyski, I., Oeberst, A., Bientzle, M., Bokhorst, F., and   van Aalst, J. 2012. Unraveling idea development in discourse   trajectories. In The future of learning: Proceedings of the   10th international conference of the learning sciences   (Sydney, Australia, July 2-6, 2012). ICLS 2012. International   Society of the Learning Sciences, Sydney, Australia, 2, 162-  166.   [11] Hara, N., Bonk, C.J. and Angeli, C. 2000. Content analysis   of online discussion in an applied educational psychology   course. Instr. Sci. 28, 115-152.   [12] Harrer, A., Malzahn N., Zeini S., and Hoppe H. U. 2007.   Combining Social Network Analysis with Semantic   Relations to Support the Evolution of a Scientific   Community. In Mice, Minds, and Society - The Computer   Supported Collaborative Learning Conference Proceedings   (New Brunswick, USA, July 16-21, 2007). CSCL 2007.   Lawrence Erlbaum Associates, Mahwah, NJ, 267-276   [13] Hirsch, J. E. 2005. An index to quantify an individuals   scientific research output. In Proceedings of the National   Academy of Sciences of the United States of America. Vol.   102, Nr. 46 (Nov. 2005), 1656916572.   [14] Hummon, N. P. and Doreian, P. 1989. Connectivity in a   Citation Network: The Development of DNA Theory. Soc.   Networks, 11, 39-63.   [15] Knorr-Cetina, K. 2001. Objectual practice. In The practice   turn in contemporary theory , T. R. Schatzki, K. Knorr-  Cetina, and E. von Savigny, Eds. Routledge, London and   NY, 175-188.   [16] Liu, J. S. and Lu, L. Y. Y. 2012. An integrated approach for   main path analysis: Development of the Hirsch index as an   example. J. Am. Soc. Inf. Sci. Technol. 63, 3 (March 2012),   528-542. DOI= http://dx.doi.org/10.1002/asi.21692   [17] Mika, P. 2007. Social Networks and the Semantic Web.   Springer, New York.   [18] Popper, K. R. 1968. Epistemology without a knowing   subject. Studies in Logic and the Foundations of   Mathematics, 52, 333-373.   [19] Reffay, C. and Chanier, T. 2002. Social Network Analysis   used for modelling collaboration in distance learning groups,   In ITS 2002 Lecture Notes in Computer Science, S. A. Cerri,   G. Gouardres, and F. Paraguau, Eds., Springer, Berlin,   Heidelberg, 2363, 31-40.   [20] Reinhardt, W., Moi, M., and Varlemann, T. 2009. Artefact-  Actor-Networks as tie between social networks and artefact   networks. In Proceedings of the 5th International   Conference on Collaborative Computing: Networking,   Applications and Worksharing (Washington DC, USA,   November, 11-14,  2009). CollaborateCom 2009. IEEE   Computer Society.   [21] Scardamalia, M., and Bereiter, C. 1994. Computer support   for knowledge-building communities. J. Learn. Sci., 3, 265-  283.   [22] Schn, D. 1983. The Reflective Practitioner, How   Professionals Think In Action. Temple Smith, London.   [23] Schrire, S. 2004. Interaction and cognition in asynchronous   computer conferencing. Instr. Sci. 32, 475-502.   [24] Stahl, G., Koschmann, T., and Suthers, D. 2006. Computer-  supported collaborative learning: An historical perspective.   In Cambridge handbook of the learning sciences, R. K.   Sawyer, Ed. Cambridge University Press, Cambridge.   [25] Suthers, D. D., Dwyer, N., Medina, R., and Vatrapu, R.   2010. A framework for conceptualizing, representing, and   analyzing distributed interaction. International Journal of   Computer-Supported Collaborative Learning, 5, 5-42.   [26] Wasserman, S. and Faust, K. 1994. Social Networks   Analysis: Methods and Applications. Cambridge University   Press, Cambridge.   [27] Weinbrenner, S., Giemza, A., Hoppe H. U. 2007.   Engineering heterogeneous distributed learning   environments using tuple spaces as an architectural platform.   In Proceedings of the 7th IEEE International Conference on   Advanced Learning Technologies (Niigata, Japan, July 18-  20, 2007). ICALT 2007. IEEE Computer Society, 434-436.   [28] Zeini, S., Ghnert, T., Hoppe, H.U. 2012. The impact of   measurement time on subgroup detection in online   communities. In Proceedings of the IEEE/ACM International   Conference on Advances in Social Networks Analysis and   Mining (Istanbul, Turkey, August 26-29, 2012). ASONAM   2012. IEEE Computer Society.     74  http://en.wikipedia.org/wiki/Donald_Sch%C3%B6n     "}
{"index":{"_id":"11"}}
{"datatype":"inproceedings","key":"Knight:2013:EPA:2460296.2460312","author":"Knight, Simon and Buckingham Shum, Simon and Littleton, Karen","title":"Epistemology, Pedagogy, Assessment and Learning Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"75--84","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460312","doi":"10.1145/2460296.2460312","acmid":"2460312","publisher":"ACM","address":"New York, NY, USA","keywords":"discourse analytics, educational assessment, epistemology, learning analytics, pedagogy, social learning analytics","abstract":"There is a well-established literature examining the relationships between epistemology (the nature of knowledge), pedagogy (the nature of learning and teaching), and assessment. Learning Analytics (LA) is a new assessment technology and should engage with this literature since it has implications for when and why different LA tools might be deployed. This paper discusses these issues, relating them to an example construct, epistemic beliefs -- beliefs about the nature of knowledge -- for which analytics grounded in pragmatic, sociocultural theory might be well placed to explore. This example is particularly interesting given the role of epistemic beliefs in the everyday knowledge judgements students make in their information processing. Traditional psychological approaches to measuring epistemic beliefs have parallels with high stakes testing regimes; this paper outlines an alternative LA for epistemic beliefs which might be readily applied to other areas of interest. Such sociocultural approaches afford opportunity for engaging LA directly in high quality pedagogy.","pdf":"Epistemology, Pedagogy, Assessment   and Learning Analytics   Simon Knight1, Simon Buckingham Shum1 and Karen Littleton2  1  Knowledge Media Institute   2  Centre for Research in Education & Educational Technology   The Open University, Milton Keynes, MK7 6AA, UK  +44 1908 654672   simon.knight@open.ac.uk     ABSTRACT  There is a well-established literature examining the relationships   between epistemology (the nature of knowledge), pedagogy (the   nature of learning and teaching), and assessment. Learning   Analytics (LA) is a new assessment technology and should   engage with this literature since it has implications for when and   why different LA tools might be deployed. This paper discusses   these issues, relating them to an example construct, epistemic   beliefs  beliefs about the nature of knowledge  for which   analytics grounded in pragmatic, sociocultural theory might be   well placed to explore. This example is particularly interesting   given the role of epistemic beliefs in the everyday knowledge   judgements students make in their information processing.   Traditional psychological approaches to measuring epistemic   beliefs have parallels with high stakes testing regimes; this paper   outlines an alternative LA for epistemic beliefs which might be   readily applied to other areas of interest. Such sociocultural   approaches afford opportunity for engaging LA directly in high   quality pedagogy.   Categories and Subject Descriptors   K.3.1 [Computers and Education]: Computer Uses in Education    collaborative learning.   General Terms  Measurement, Documentation, Design, Human Factors, Theory,    Keywords  Learning analytics; epistemology; pedagogy; educational   assessment; discourse analytics; social learning analytics   1. INTRODUCTION  Assessment is one area where notions of truth, accuracy and   fairness have a very practical purchase in everyday life [62]. It   sits at the heart of learning, but is hugely controversial. This is   directly relevant to Learning Analytics (LA), because  we argue    LA implicitly or explicitly promote particular assessment   regimes.    Presently, many education systems are predicated on assessment   regimes seeking to accredit knowledge and skills gained by   students through formal assessments  often exam- based.   Proponents of such exams suggest they are the fairest way to   assess competence and learning under controlled, reliable,   conditions. Assessment, pedagogy and curriculum are   fundamentally related [26], but many regimes of what has come to   be termed high stakes testing are criticised. For example,   standardised assessments, including the Programme for   International Student Assessment (PISA), American Standardised   Assessment Tests (SATs) and English National Curriculum   assessments (Sats), face myriad problems. Not least among these   is that the exams are criticised comprehensively (e.g. [12, 23, 29])   for failing to represent adequately the types of problem people are   likely to face in their everyday lives (external validity), and that   they fail to represent an adequate conceptualisation of what it   means to know  of what knowledge is (internal validity). The   latter claim is that, while assessments clearly measure something,   a good grade does not necessarily reflect mastery [12]. These   fundamental issues are highlighted in a significant body of   research (e.g. [12, 23, 29]), and one of the objectives in writing   this paper is to clarify the implications of these issues for the   Learning Analytics community.    In this paper, Section 2 considers the relationship between   assessment systems and the sorts of epistemic challenges students   might encounter. Section 3 introduces the concept of epistemic   beliefs, and Section 4 goes on to discuss the relationships between   LA, epistemology, pedagogy and assessment. Section 4.2.1 then   introduces pragmatic, sociocultural approaches to LA, which we   suggest are well placed to probe or assess facets of learning which   other LA may not adequately address. To exemplify this   argument, we draw a parallel between the psychometric   measurement of epistemic beliefs and high stakes testing regimes   (Section 5). Our suggestion is that pragmatic, sociocultural   approaches offer alternative LA which are well placed for   exploring these areas of learning (Section 6). The final section   discusses the role for established LA in this pragmatic,   sociocultural LA. Throughout the paper, we particularly associate   our approach to LA with that of Assessment for Learning (AfL)   which uses continuous assessment with formative feedback to   facilitate learning, in contrast to a focus on summative   assessment, often through examinations.   2. WHY WORRY ABOUT  EPISTEMOLOGY   A primary concern of this paper is the relationship between   epistemology, pedagogy and assessment. Epistemology is the   philosophical study of what knowledge is, and what it means for   someone to know something. Central to the field of   epistemology are questions regarding the nature of truth, the   nature of justification, and types of knowledge, e.g. knowing how   (skills), or knowing that (facts). Whatever knowledge is, it is   uncontroversial, pre-philosophically, that education aims at the      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,   requires prior specific permission and/or a fee.   LAK '13, April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.       75    imparting of knowledge: students are educated in part so that they   may come to know things. [54]. Thus, pedagogy may be seen in   part to be the study of how to impart this knowledge to students    the science and development of approaches to teaching and   learning for knowledge. However, epistemologys relationship to   the more familiar concepts of pedagogy and assessment is a topic   of educational debate [12, 15, 32, 62], and we will consider this in   relation to LA throughout this paper.    Harlen [26] depicted a triadic relationship between pedagogy,   assessment, and practice. Influenced by this, and Katzs [31]   description of competency, epistemology and pedagogy:   curriculums holy trinity we depict the triad as in Figure 11.          Figure 1: The EpistemologyAssessmentPedagogy triad   In this picture, epistemology could be seen as driving assessments   that are aimed at uncovering student knowledge, and driving   pedagogy to build high quality knowledge to that end. In this   view, assessment is targetted at the learning of high level   knowledge  it is assessment for learning. However, these   relationships are not fixed; neither pedagogies nor epistemologies   necessarily entail the other [13] (although they may implicate).   Furthermore, as we will explore in this paper, assessment is   always concerned with devising proxies for knowledge, around   which there are philosophical (epistemological) and   methodological issues. Some epistemological stances hold that it   is not possible to map the knowledge that students hold onto   their responses in assessments in reliable and valid ways. This   issue is further confounded by the methodological limitations of   all assessment methods, and by extension LA. The situation is,   therefore, a complex one  which facet of the triad has primacy   over the others is not clear in either theory or practice, and may be   dynamic according to need and circumstance. However,   relationships between the three can certainly be identified, and   throughout this paper we draw out some of these with respect to   LA  which may be conceptualised as a component of assessment.   Furthermore, we suggest that, although the relationship may not   be a necessary one, assessment regimes do implicate particular   epistemological stances.   Consider the following example from Denmark to illustrate the   argument that implicitly or explicitly, epistemological   assumptions fundamentally shape pedagogy and assessment, and   hence, the kinds of LA that one deploys to achieve those ends. In   Denmark, a pilot project was conducted permitting the use of the   internet (but not communication sites) to support students in five                                                                      1   We could also introduce the notion of folk psychology as a mediating   factor between teachers views on knowledge, and pedagogy  for  example, if we hold that some (particular) children will never learn x,   we are unlikely to attempt to teach it (a pedagogical move) regardless   of our epistemological stance regarding the nature of x [41]. Although,  in that paper [41] Olson and Bruner implicate epistemology in a number   of their points regarding folk pedagogy.   of the school leaver subject exams2. This made it possible to set   questions requiring the use of multimedia and individual internet   search. For example, a student might be asked to write about a   poet whom they have not studied (and rote learned about), based   on a poem by them and that of a contemporary, a short biography   and perhaps an image from the time. They may be given   unfamiliar resources, and permitted to source information for   themselves from the internet. Thus, while Danish students are   expected to evidence knowledge-that  knowledge of facts    they must also exhibit a higher level of knowing-how, for   example around information processing, synthesis, and   metacognitive abilities  which remain unassessed in countries   restricting access to external resources which might enhance the   students capability. While this is of course simply one other   (controlled) context, the example illustrates how even within a   system reliant on exams, those exams might be conducted on a   rather different epistemological grounding. Assessment regimes   such as the Danish example may be taken to reflect a holistic   epistemology in which how one comes to know is as important as   what one comes to know, and in which it makes little sense to   pick out individual tokens of knowledge in decontextualized ways   [9, 11, 13, 31].   We can contrast such assessments with high stakes testing   regimes whose construct validity and external validity have been   questioned. For instance, Davis [10][12] argues that such   instruments neither assess those facets of learning they set out to   test, nor those facets of learning which would likely be utilized in   the everyday deployment of knowledge in any particular domain.   Davis has argued that high stakes testing is inadequate for   understanding learning, in so far as its construal of that learning is   necessarily restricted by a desire for highly reliable metrics of   success. As such, it must exclude the nuanced understanding of   student meaning-making, and the social context in which learning   occurs, and how knowledge is constituted and enacted. He argues   that this, as opposed to acquisition, is the appropriate way to talk   about knowledge. Davis draws on notions of situated cognition   [48] and sociocultural approaches [46]  particularly Sljs   Literacy, Digital Literacy and Epistemic Practices: The Co-  Evolution of Hybrid Minds and External Memory Systems [47].   Slj highlights that:    From the learning and literacy points of view, such tools [memory   aides and knowledge management systems of various sorts] imply   that users knowledge and skills, as it were, are parasitic on the   collective insights that have emerged over a long time and which   have been entered into the instrument in a crystallized form:   algorithms, grammatical rules and concepts, etc. The user will   manipulate the artificial memory system in a number of ways in   order to see what comes out of the processing that goes on in the   machine [47]   However,    Engaging with external memory systems thus requires familiarity   with a varied set of epistemic practices that range from   deciphering letters on a page through familiarity with meaning-  making in relation to discourses and genres of texts and other   media, to meta-knowledge about how such resources may be used.   [47].                                                                       2   Steen Lassen (a Danish Education Minister) on the piloting of internet   access in exams: http://vimeo.com/8889340 subsequently adopted by   some Danish universities [8].   76  http://vimeo.com/8889340   Slj is making an epistemological claim, specifically, a   sociocultural, pragmatist claim: that there are important literacies   and practices to be mastered in learning; that those should   themselves be objects of assessment; and language and discourse   are critical filters on our grasp of the world. Such an epistemology   has implications for how we teach, what we assess, and which   analytics techniques might be deployed. Success can no longer   be defined as a matter of regurgitating, unaided, the correct   information in a two hour exam. Such an epistemology also  we   argue  offers a perspective on why it is that, even in those   technologically advanced societies which assess knowledge in   less abstracted, socially embedded ways  such as Denmark    information retrieval (IR) and processing via the internet and   search engines is a significant area of difficulty for students [59];   namely, that although this provides some wider access to   information, this does not equate to knowledge. Student   engagement with information should consider both the kinds of   knowledge which we might call transferable competencies or   skills  including those higher order skills often known as   metacognitive abilities  and more propositional or fact based   knowledge. In this context, we might consider information   management, and IR not only as a means to an ends, but as a way   to encourage interaction with a complex network of information.   As argued by Tsai, as not only:   a cognitive tool or a metacognitive tool; rather, it can be   perceived and used as an epistemological tool. When the Internet   is used as an epistemological tool for instruction, learners are   encouraged to evaluate the merits of information and knowledge   acquired from Internet-based environments, and to explore the   nature of learning and knowledge construction. [57]    In this conception, learners are encouraged to think about the   context, reliability, validity, certainty, and connectedness of   knowledge.    To summarise, this section has argued that a consideration of   epistemology is important to LA in two related senses:    The ways that we assess, the sorts of tasks we set and the  kinds of learning we believe to take place (and aim for) are   bound up in our notions of epistemology. LA are not   objective or neutral: data does not speak for itself but has   been designed by a team who, implicitly or explicitly,   perpetuate the pedagogical and epistemological assumptions   that come with any assessment instrument.    The Danish example shows concretely how epistemology   relates to assessment regimes. When knowledge is seen as   something that can only be evidenced in contextualised   activity, and when it is embedded in ones physical and   digital environment, the role of the internet is redefined as a   metacognitive tool which cannot be excluded in assessment.    These epistemological considerations foreground the quality of a   students enquiry processes as important, not just whether they get   the right answer. Analytics that provide process traces become   particularly important, as we shall discuss in Section 6.   3. EPISTEMIC BELIEFS  One facet of students dynamic interaction with the world of   information relates to how they conceptualise the information they   require to answer any particular question  their epistemic beliefs   regarding the nature of the question, and how it may be answered.   The sorts of assessment, and pedagogy, which students are   exposed to will relate to the types of epistemic challenge they   encounter in their education  systems with a focus on right   answerism and limited access to external epistemic resources   offer fewer opportunities for challenging knowledge claims [12,   31]. This paper thus talks about two related concepts:   1. Epistemology: Which we introduce above, and is related to  the philosophical analysis and conceptualisation of   curriculum content and assessment for knowledge   2. Epistemic Beliefs: Which we now introduce, and relates to  the intrapersonal, psychological conceptualisations that   individuals hold regarding knowledge    Indeed, a key component of AfL may be the disambiguation of the   epistemic requirements of questions  in terms of understanding   the question, its context, and the knowledge required to answer   the question [2].    Table 1 indicates four dimensions of epistemic beliefs, for which   there is general agreement across the various models of belief3.   These dimensions are useful to consider in relation to student   understanding of knowledge domains. For example, in the context   of search engine IR tasks, epistemological beliefs are a lens for a   learners views on what is to be learnt [4]. In such tasks, student   search activity may be analysed using the dimensions in Table 1   (e.g. [38]), providing a lens onto students understanding of their   own learning, task demands, and how to meet those demands.   Table 1: Dimensions of epistemic belief (adapted from [39])   Dimension Description   Certainty of   knowledge   The degree to which knowledge is conceived   as stable or changing, ranging from absolute   to tentative and evolving knowledge   Simplicity of   knowledge   The degree to which knowledge is conceived   as compartmentalised or interrelated, ranging   from knowledge as made up of discrete and   simple facts to knowledge as complex and  comprising interrelated concepts   Source of   knowledge   The relationship between knower and known,   ranging from the belief that knowledge resides   outside the self and is transmitted, to the  belief that it is constructed by the self   Justification   for knowing   What makes a sufficient knowledge claim,   ranging from the belief in observation or   authority as sources, to the belief in the use of  rules of inquiry and evaluation of expertise      Epistemic beliefs are thus one example of the type of construct   which sociocultural LA may probe. However, they are also a   particularly good example given epistemic beliefs relationship to   our everyday dealings with the world of information, and their   relationship to pedagogy, assessment, and classroom practices   [28]. Section 5 will discuss epistemic beliefs in relation to their   measurement, but we shall first introduce some established   approaches to pedagogy.   4. OUR LEARNING ANALYTICS ARE  OUR PEDAGOGY   Buckingham Shum [6] has used the shorthand our LA are our   pedagogy  a relationship which we explore in this section.                                                                      3  See e.g. [53] for a review of the multiple theoretical frameworks    77    4.1 Pedagogy and LA  The relationship between LA and pedagogy is important because   they are both bound up in epistemology  what knowledge is. This   section explicitly introduces the relationship between a number of   established pedagogic approaches and LA. These are not intended   as comprehensive reviews, but rather as brief overviews of how   the relationship between pedagogy and LA might be   conceptualised. The following section expands on some key ideas   here, before moving on to explicate the core topic of this paper  a   sociocultural learning analytic  and one proposed instantiation of   an LA based on this approach.    4.1.1 Transactional or instructionalist approach  Transactional approaches hold that learning entails the transfer of   knowledge from the knower (teacher) to the learner (student).   They are characterized by a perspective on assessment in which   success is out there, in the degree of correspondence between   the claims that learners make, and the facts that they have been   taught.   Analytics Implications: LA based on transactional approaches    both in learning, and more broadly  will tend to focus on very   simple metrics such as test scores and hit counters, as opposed to   any deeper analysis of project outputs or processes.   4.1.2 Constructivist approach  Constructivist models hold that learning occurs in the guided   experimentation of the learner (student) on the world, typically in   classrooms in which such experimentation is age-targeted, and   guided by a teacher. Constructivist models are likely to hold a   notion of success which highlights construction, with learners   experimenting with their environment, and being capable of using   tools which are appropriate for their given age.    Analytics Implications: LA with a focus on constructivist   approaches of learning will focus on progress, particularly   through a set of materials, resources or tools selected and arranged   by the teacher.   4.1.3 Subjectivist or affect based approach  Subjectivist perspectives can be characterised as deemphasizing   learning qua academia, in pursuit of personal affect. While   individual affect is a concern for educationalists, it is rarely if ever   the overarching concern in the consideration of learning.   However, for example in IR, subjectivist approaches are more   interested in whether the user is satisfied with the information   they have found, than whether the information is good.   Analytics Implications: In tandem with other approaches, LA   based on subjectivist approaches are likely to provide motivation   assessments for understanding why someone is (or is not)   undertaking particular actions (see, e.g. [20]). Such analytics may   focus on explicit moves (feedback forms, affect-based semantic   markup such as blog tagging) alongside more implicit analysis   such as sentiment analysis of communication data.   4.1.4 Apprenticeship approach  Apprenticeship approaches are sometimes used in LA with an   interest in whether the learner has become part of a community of   activity. In this view, success is about being part of a given   group; it is bound up in notions of communities of practice  that   to know x is to act towards x in some way that is defined by (or   reflected in) the behaviours of some community or other.   Analytics Implications: Analytics based on apprenticeship   approaches are likely to focus on classifying expert and novice   users, and the shift from novice to expert. Such analysis may   explore behavioural markers which mirror those made by   experts, but may not explore the reasons or meanings implicated   in such moves.   4.1.5 Connectivist approach  Connectivism [55] claims to highlight a perspective on   epistemology which translates into a LA framework. Within this   view, learning is about understanding how to connect ideas   appropriately, and where to find such information. The suggestion   is that in the case of the connectivist knower the act of knowing   is offloaded onto the network itself [55]. Within this perspective   then, success is about building connections between ideas.    Analytics Implications: Connectivist approaches use network   analysis to explore the connectedness of a learners knowledge   in terms of both concepts, and social connections. Analytics   would look at how networks size, quality and changes over time   can serve as proxies for effective learning.    4.1.6 Pragmatic, sociocultural approach  Pragmatic approaches (building on for example, Dewey [16]) hold   that learning occurs in the development of  and negotiation of  a   mutually shared perspective between learners. Such approaches   focus less on truth  where truth reflects facts about the world    than how meaning is co-constructed, and used in context.   Pragmatists suggest that, as human knowers, our conception of   some given thing is bound up in our understanding of its practical   application  and that is all. When we attempt to understand truth   beyond such a conceptualisation of practical activity, we are likely   to fail. Thus, success is in use  the measure of success is how   useful the information is for the purposes it is employed; it is   socioculturally embedded and mediated, and may be in flux as   activities are defined and redefined.   Analytics Implications: Pragmatic approaches have traditionally   focused less on assessing the products of learning (except where   they are being used for something), and more on the process.   Analytics tools in sociocultural approaches encourage learners to   reflect on their own activity, in an attempt to understand how they   can develop their skills in information processing, in their own   particular contexts. Analytics within this approach might attend   particularly to quality of discourse for learning, for creating a   mutuality of perspectives [18] including in collaborative IR tasks   [22, 27, 35]. Our previous work is in this tradition, drawing on   sociocultural discourse analysis [40], argumentation theory [60,   61] and argumentation in sensemaking deliberation [45]. This   research foregrounds how students interact with information;   make sense of it in their context; and co-construct meaning in   shared contexts. These are on-going processes which highlight the   question of how LA fits into the context of AfL and pedagogy.   4.2 Epistemology and LA  The stance we take with regard to the relationship between   epistemology, assessment and LA relates to the issue of whether   we envisage analytics as a form of diagnosis on the one hand or a   kind of biofeedback on the other  is LA (and assessment) the end   point of, or a component of, pedagogy. In the former we seek to   accredit learning through defining behavioural proxies taken as   evidence of knowledge and competencies. LA may also be used to   support learners in their own self-regulated learning activities,   giving them feedback on changes they make and their impact on   learning outcomes, but without  generally  making strong   evaluative judgments regarding such changes. The former is thus   more closely aligned with assessment of learning  often   instantiated in high stakes summative assessment, while the latter   is closer to Assessment for Learning  in which assessment is a   continuous process through which formative feedback may be   78    given to further develop the students learning (see e.g. [3, 23]). If   process-centric competencies are declared to be part of the   summative assessment criteria, then the two categories converge.   The relationships highlighted in 4.1.1-4.1.6 serve as general   pointers to the sorts of relationships we might see between   pedagogy and LA. There we also highlight views on learning,   alongside notions of how success may be defined within these   approaches; that is, when these systems might accredit knowledge   to the student. Fundamentally, this accreditation implicates   epistemological stances regarding when knowledge may be   claimed (or not). These are general claims, but illustrative of how   such notions relate to those of LA, in particular notions of:    Mastering curriculum content: this is the dominant focus  of analytics approaches at present, seeking behavioural   markers using e-assessment technologies of varying   sophistication, in order to generate summaries at varying   granularities, for both individuals and cohorts. (Particularly   transactional and some constructivist approaches)    Evidencing membership and processes: this approach to  LA looks for behavioural proxies which indicate a student is   part of a particular subgroup; positive feedback is given   towards moving students into successful subgroups, but   little attention is paid to the qualities of those groups except   instrumentally. (Particularly affect-based, apprenticeship,   and possibly connectivist approaches)    Success is use: this approach looks for students developing  personal and collective representations of curriculum   content, and engagement in sensemaking about not only this   material, but also their own analytics. Social Learning   Analytics [7, 21] in which students are encouraged and   supported to do so may work towards this end. (Particularly   pragmatist approaches).   These three broad conceptualisations of LA relate to the issue of   whether or not we are deemed to consume, discover, or create   (internally or/and externally) knowledge  is it out there for us   to take, do we need to investigate to find it, or is it formed in our   developing understandings of the relationships between entities   and the new representations we create in such activities This is   not a claim about the learning or pedagogy, but a related claim   about the status of knowledge, and its assessment, which we   discuss further in section 6.4 with reference to one particular   example.    4.2.1 Pragmatism and sociocultural approaches to  assessment    The nuance of claims surrounding epistemology and assessment is   important. In the introduction we referred to research arguing that   conventional exams are designed to maximise the reliability of   results, at the cost of straitjacketing what can be defined as   learning (poor internal or construct validity) and thus what   constitutes evidence of learning (poor external validity).   Moreover, if we are to argue that individual tokens of knowledge   cannot be identified (and owned), then we should accept that   the content of a specific item of knowledge depends in part on   how it is related to other knowledge [10]. Thus, sociocultural   setting, interaction, and the purposes for which any artefact or   knowledge  in the broadest sense  is being used, are all of   fundamental importance in understanding how people make   meaning, and learn. Contextual sensitivity is thus a key facet of   pragmatist approaches.    Pragmatic approaches, broadly, are likely to focus on the dynamic   nature of information needs, and the discourse and other artefacts   which mediate our relationship with information in the world. It is   not a postmodern approach, in the sense that postmodern   approaches take either a relativist approach (there is no fixed   truth) or a normative one (the dominant theme is correct at that   time) to knowledge, but rather one which focuses on use, and   meaning, over accreditation of facts to things in the world.   4.2.1.1 Pragmatic Analytics Revisited  As described in Section 4.1.6, pragmatic approaches have   traditionally focused less on assessing the products of learning,   and more on the process. LA in these approaches might encourage   learners to reflect on their own contextualised activity, in order to   instil an ethos and capacity to become reflective practitioners. The   key development with the emergence of digital LA is that   previously ephemeral processes are now persistent, not just for   researchers studying those processes, but for the learners and   educators co-constructing those processes. Moreover, the process   traces are now amenable to computational analysis which opens   new possibilities for assessment and feedback, both formative,   and possibly even summative (e.g. where the assessment regime   defines those process skills to be an important form of student   evidence).   Given the salience of context in this approach, it deserves further   explication. As with LA generally, context may be taken as very   mechanistic, for example the claim that a person in   place/course/role/ability band x should see resource y, or other   approaches which would include time, topic, or social-group   resource discovery. No doubt some of these features will prove   useful, and indeed the use of semantic web technology in social   learning analytics [21] may be particularly interesting. However,   in addition to temporal, linguistic, aptitude, and geo-spatial   markers, we draw attention to the following:   1. We emphasise the discourse in which, and through which,   context is constituted [17, 44]. That is, we take the discourse   to have a multifaceted role in constituting, and helping   learners make sense of, the context.    2. Discourse is fundamentally associated with the sensemaking   which occurs in respect of any particular task being   undertaken; the use being targeted is fundamental to   context. Stark examples highlight this importance, for   example where we ask students to critique versus   summarise a paper we expect rather different outcomes.   Assessment regimes which make this explicit may facilitate   capture of context around doing x for purpose y LA   3. These assessment systems (2, above), and the broad range of   tools, technological and otherwise, which people utilise also   act as mediating artefacts impacting on how people perceive   their task, and its solution  mediating the context of use.    We have, therefore, expounded a view of LA which highlights the   importance of context. This relates to a salient point for epistemic   beliefs that:   A sophisticated epistemology entails context-sensitive   judgements. Thus they point out that it is not very   sophisticated to view the idea that the earth is round rather   than flat as tentative whereas theories of dinosaur   extinction do require a more tentative stance [1].    Similarly, building spurious connections between ideas as a way   of indicating a complex view of knowledge (within the simplicity   dimension) is likely to be less sophisticated than those who   understand the need for moderation, and so on. Context is thus   79    key to understanding epistemic beliefs, the analysis of which   seems highly suited to the biofeedback approach to formative   assessment analytics, introduced earlier.    The next section further expands this claim in the context of   psychological assessment of epistemic beliefs, firstly in   mainstream psychological approaches, and then that of the   discursive approach  which similarly holds context and discourse   to be fundamental to understanding thinking. Section 6 then   returns to LA, drawing out the relationship between analytics, and   the measurement of epistemic beliefs in our illustrative example   for sociocultural, pragmatic analytics.   5. MEASURING EPISTEMIC BELIEFS  The complexity of epistemic cognition suggests a particular   perspective on how we are to understand these beliefs. No   approach mirrors reality with a true, immutable, incontrovertible   perspective on a learners epistemic cognition. This concern is a   dual one. Firstly, it is a methodological concern regarding our   access to the world, our ability to get at what is out there.   Secondly, it is a conceptual and psychological concern, regarding   the nature of epistemic cognition and whether it itself is stable    developmentally, and across domains  or shaped in some way by   resources or beliefs. These two concerns are reflected in the   epistemic beliefs literature. Firstly, cognitive developmental   models [33, 34] suggest that individuals progress through a   sequence of increasingly sophisticated epistemic beliefs, while   multidimensional perspectives [28; 52] suggest that epistemic   beliefs can be separated into dimensions, within which levels of   sophistication can be identified [24]. However, both of these   assume a fixed uni-directional developmental trajectory, where   beliefs are seen as global across (and within) domains. The   resources view, in contrast, emphasizes the interaction of believer,   with resources, highlighting that at various points in any task a   cognizer may invoke differing resources [25].   Secondly, methodologically the developmental models have   tended towards interviews and laboratory tasks, while   multidimensional models have emphasised paper and pencil self-  report measures [14]. Both of these approaches reflect the fixed   perspective on beliefs from which theory they stem. Importantly,   although three major survey instruments have been developed and   deployed,  including in IR tasks [37, 52]  they are heavily   criticised for their psychometric properties [14]. Furthermore,   while some studies have used interview [1, 39], think-aloud   protocols [1, 19] or systematic observation [51] such methods   may be limited in their insights, particularly where self-report data   is to be used and interpreted by researchers. Importantly, they are   also not appropriate for the study of online, collaborative, or   geographically and temporally spread activities  in particular,   online IR, or information processing more broadly. These   approaches reflect the epistemology of current assessment   regimes, as indicated in Section 2, and seem to implicate the view   of fixed psychological constructs  whether intelligence, or   epistemic beliefs, as further discussed throughout Section 3.    In contrast, while those adopting a resources view of epistemic   beliefs may also utilize such methods  in particular those   involving think aloud and interview data  they also accord well   with sterholms discursive stance, which suggests that we   should not see beliefs and communication as two separate   objects that can affect each other, but as more integrated aspects   of cognition and/or behaviour [42]. The resources view describes   the activity, the discourse, as the site where epistemological   beliefs come to existence, through explicit or implicit references   to prior experiences (epistemological resources) [43].   sterholms argument is that the resources perspective can be   combined with Hammer and Elbys [25] resources model. In this   model epistemic beliefs are not viewed as fixed, or developing   cognitive models ranging over one or more domains, but are   rather seen as dependent upon the resources available to the   cognizer at any time. This view of epistemic beliefs as theory-in-  action  in which context, domain, culture, and task conditions   interact  accords well with the idea that context is fundamental to   understanding meaning.   5.1 Learning Analytics and Trace Data  While sterholm is primarily interested in spoken interactions,   LA may extend this interest into the exploration of users   interactions with artefacts. A tool for such analysis may come   through the use of trace data, which is more or less implicitly   created by the student. For example, Stadtler and Bromme [56]   analysed the ways participants found, extracted, and moved   information  which could be used to explore information about   their beliefs (e.g. visiting few websites indicates trust in those   sites visited [24]). Importantly in this study, users were either   given evaluation prompts regarding multiple documents in the   medical domain, or not, and those who received such prompts   subsequently recalled more facts and were better able to evaluate   sources. If systems of prompts promote laziness, we should be   concerned. Where, however, they improve outcomes, analytics   should explore the best ways to implement them effectively and   sustainably to support high quality pedagogy and AfL.   Furthermore, Greene et al. [24] point out that many behaviours   which would ordinarily be difficult to observe can be explicitly   elicited in the context of Computer Based Learning Environments   (CBLEs), for example:   participants who report belief in objective truth and omniscient   authority may self-regulate quite differently than participants with   a desire to evaluate multiple forms of justification. Likewise,   participants who believe in the inherent subjectivity of all   knowledge may, on average, select more representations than   those who look for an objective truth. [24]   The claim is thus that epistemic beliefs will be brought to bear on   knowledge tasks in ways that can be meaningfully captured, in   particular using technology systems (e.g. the way people represent   knowledge in mind maps). Trace data thus offers direct access to   real-time behaviours in unobtrusive ways, and is thus high in   external validity, although it is of course within the context of the   system which is set up to capture such information. Furthermore,   while trace data is unobtrusive, it may give an incomplete picture.   In particular, people may have reasons for some behaviours which   cannot be probed using such data; these reasons may range from   epistemic (as discussed above, for example with regard to the flat   earth issue), to practical (ICT failures), to pragmatic (the   demands of the task place a short time restriction on the activity),   and so on. Thus, it is important to remember that while analytics   regarding epistemic beliefs may be  at best  a dirty lens onto   those beliefs, when analytics are considered in action as a tool for   sensemaking, they may provide an insightful tool for learners to   dissect their own metacognitive and self-regulatory behaviours.   6. TRACE FOR EPISTEMIC BELIEFS   Trace data thus provides one means by which epistemic beliefs   could be examined. However, trace could refer to many things,   and as discussed in sections 4.1.1-4.1.5 the data collected may not   represent an appropriate teaching epistemology, nor capture   adequately student epistemologies (see section 2). The next   section will discuss some LA which may address this issue.   80    6.1 LA  Tools for Trace  Building on sections 4.1.1-4.1.5, we can identify a number of   analytic tools and their relationships to particular forms of data.   Some forms of analytics rely on a belief that particular methods   (self-report in particular) are: a) true reflections of reality, b)   whole reflections of reality (i.e. they cover all the relevant ground)   and c) probe real constructs. However, while self-report   measures may be useful particularly as discussion prompts with   students, they are not necessarily the most useful approach for   many purposes. In both assessment and psychological testing,   they suffer from issues of validity (Sections 2 and 6.2). Thus,   other LA tools may prove more useful.    Much LA thus delves into network analysis, in relation to social-  networks, or in relation to concept networks based upon semantic   relations identified more or less explicitly by the student. While   these approaches offer useful insights into the sensemaking   process, they too can fall into the trap of accrediting group   memberships, over group activities (section 4.1.4) or map   networks, as opposed to map uses (section 4.1.5).    An interesting notion then, is attempting to delve further into the   sensemaking significance behind particular semantic moves in a   given environment. Thus, Greene et al. [24] (see 5.1) described   one method of trace analysis for epistemic beliefs built on   information moves. Other examples of such trace capture could   also be structured such as to gather student data in particular ways    some of which may be quite naturalistic (capturing search   queries, or Facebook posts to explore problems encountered, or   interactions made [36]), and others of which might push students   into information structuring activity in which they would not   otherwise engage, such as argument mapping.   6.2 Trace and Traceability  However, in encouraging such structuring by learners, and   claiming capture information about what they are doing, some   may argue that we are simply reifying the constructs we have set   out to explore. That is, if we are interested in epistemic beliefs,   and set up a system to push students to make epistemic beliefs   explicit, it does not matter whether those students have underlying   epistemic beliefs because the system forces them into making   some (it makes them reify). While for psychologists who wish to   uncover underlying beliefs this is problematic, we do not see this   as a concern for our project, because in our discursive,   sociocultural, pragmatic approach the interest is in beliefs as   theory-in-action. In this view, the claim is not that the   measurement of beliefs is not possible, but rather that when we   take measurements, the discursive context is fundamental to the   practices being observed, and the ways that the beliefs are   instantiated in action. Thus, LA provides a means to tackle the   static, decontextualized view of epistemic beliefs instantiated by   questionnaire methods, offering a more authentic perspective on   epistemic action than experimental contexts.   6.3 Discourse-centric Trace  A Path to  Epistemic Cognition   A number of tools can be conceptualised to probe trace gathered   around higher order thinking exercises, and some already exist.   One example  which will be used for illustrative purposes here    is being developed at the Open University, based around the   Cohere argument mapping tool [5] and previous work on   sociocultural discourse-centric LA [36]. Cohere is a web   application for mapping ideas, concepts and arguments, which can   be annotated directly onto source websites. Users enter ideas    nodes with meaningful classifications  and are then invited to   make the connection with meaningfully labelled edges, to create   a conceptual graph. Both ideas and connections may also be   tagged, to add a further level of semantic data. Cohere is designed   as a tool to enable users to build their own structures, but also to   share these, and integrate the nodes and connections of other   users, thus building up communities of enquiry around particular   disciplinary topics.    Cohere facilitates exploring the ways that users create nodes, and   the epistemic implications of such creation. At a basic level, this   could simply be an analysis of the number of idea and connection   types used. A more advanced analysis might compare individuals   Cohere use on the same task, and provide analytics based on such   comparison; these notions are discussed further below. However,   neither of these explores the semantic qualities of ideas and   connections. Using the broad epistemic dimensions described   above (Table 1) some correspondences between those descriptors,   and possible trace can be identified as in Table 24 which also   gives suggested guidance, intended to be indicative of the sorts   of challenges which might be posed to students to extend their   epistemic cognition and probe their learning processes.   However, within the approach described above it should be   understood that while the trace data given here is theoretically tied   to the constructs, both the constructs and the trace should be seen   in their situated context  as components of a sociocultural   environment, interacting with the relevant agents (students,   teachers, designers, etc.), and the wider cultures and subcultures.   Thus, the possible trace markers and guidance are conceptually   related to the work discussed above but these should be dynamic   tools, and empirical work will be needed to explore the   relationship between feedback given, representations allowed,   student responses to feedback and the impact of this on learning.   6.4 Many Lenses on Epistemic Beliefs  Table 2 thus proposes one set of traces from which meaningful   data could be captured. This is not, however, to dismiss other   approaches discussed in Section 4.1. The epistemological   approach discussed throughout this work is instead intended to   indicate that what drives our Learning Analytics  and assessment    is not what they are, but rather, what we do with them. Our   suggestion is that many of these approaches to LA  these dirty   lenses on the world  provide insights into different levels of   learning, and tools for meaning-making. For example, with this   richer than normal data model in place, it is very simple,   computationally, to feed back the number of ideas, and connection   types used, but this may provoke meaningful dialogue regarding   what these other types might be used for, or why they have not   thus far been used. Similarly, constructive discourse might occur   around the reasons why one students map is more connected (but   perhaps not appropriately so) than anothers.                                                                           4   Following previous work [36] the basic analytic statistic is constructed   as a percentage representation of the target type, over the total types  created by the user. For example, the number of opinion nodes created,   as a percentage of the total number of nodes created by that user.    81    Table 2: Trace & Guidance for Epistemic Beliefs   Trace Guidance/Challenge    C er  ta in  ty  Presence of competing   claims (e.g. supports/   challenges).   Presence of stability   markers  e.g. current   references, geographic   repetition.   Are there two sides to this   idea Could you explore   XY contrasting example   Is this idea consistent across   time/place Have you   looked at XY map   S im  p li  ci ty   Number of connections   between nodes.   Are any of these ideas   connected Have you   considered how WX and   YZ might be connected   S o  u rc  e Presence of I think or   restatement of fact, few   additional nodes made   other than those created as   quotations.   What do you think of these   ideas or How does the   evidence relate to your   view   J u  st if  ic a  ti o  n  Judgments of relevance,   and supporting or   explanatory notes (this   evidences/ explains x).   Ties to method ideas.    What evidence do we have   for this idea Is it good   evidence Why/why not      There is a strong relationship between analytics, assessment,   pedagogy, and epistemology (Figure 1), which sociocultural   analytics bridges well. Our approach should be seen as one of   many lenses for many contexts, used in combination with the   more conventional forms of LA currently dominating. In the last   section before concluding, we outline how the approaches   discussed in Section 4.1 relate to epistemic beliefs, and some   strengths and limitations of these approaches.   6.4.1 Lenses Onto the World  LA based on Transactional approaches. Approaches which   emphasise fixed, correct knowledge, over how those facts are   used to display understanding, are likely to encourage lower   epistemic cognition, and implicate more realist epistemologies   which see knowledge as a reflection of things in the world.    LA based on Constructivist approaches. Similarly, there may   be an overemphasis on a limited range of knowledge in   constructivist approaches which emphasise development qua   progression, but without considering the sociocultural context in   which that progression occurs, nor the wide range of uses for   which it may be deployed. This may be particularly true in   constrained systems which guide students through pre-set tasks   and levels of attainment to meet, pre-specified software, and so   on, as compared to those exploring knowledge co-constructed in   iterative dialogic discourse [49, 50]. Understanding the ways that   students build knowledge claims  understanding connections,   justifications, change over time, and nuance  is fundamental to   understanding their epistemic beliefs. Knowing that a student is at   stage x of y in development may be less significant.    LA based on Apprenticeship approaches. In a similar vein,   apprenticeship approaches can offer useful insight into group   membership and the development of a students thinking.   However, the approach described in this paper suggests the best   way in which to think about such approaches is with respect to the   functional role that such community membership plays in a   students epistemic action, and their normative standards.    LA based on Subjectivist approaches. LA based on affect   could be useful to analysis of epistemic beliefs, with their analysis   of satisfaction with information, e.g. enquiry based learning   [20]; self-efficacy in IR [58]; satisfaction with search results [30].   As such, affective analytics might be used to explore whether   learners are prematurely satisfied with findings that a peer or   educator deems to be inadequate, or if they have an appropriate   sense of disquiet or frustration with a flawed argument or   methodology.   7. CONCLUSIONS  This paper started with the premise that assessment, pedagogy and   epistemology are fundamentally entwined. Furthermore, we   suggested that a focus on high stakes assessment  which learning   analytics may well be used to perpetuate  is detrimental to the   wider enterprise of education, prioritising the reliability of tightly   defined assessments over continuing, formative assessment for   learning, and authentically situated learning which is harder to fit   into formal examination contexts. This is problematic in so far as   it limits the ways we can challenge students in assessments, and   fails to reflect their encounters with knowledge claims in the   world beyond the classroom walls.    We have highlighted that transactional approaches may emphasise   use of facts; constructivist the broad (and contextual) application   of skills; subjectivist the self-efficacy and motivators of students;   apprenticeship the dynamic practical based learning which may   occur through high level membership of communities of practice;   connectivism the ability of students to build up, link and curate   their knowledge networks. A sociocultural, pragmatic, approach   may offer an additional toolset, alongside a theoretical frame   through which to use other LA lenses. All are partial (in bias, and   hence in their coverage of all that might be measured), but may be   used in complementary ways.    Analytics from user traces provide a means to track and record   previously ephemeral process data, which could benefit   assessment for learning in significant new ways. Pragmatist   approaches, which emphasise use and meaning-making over the   accrediting of true statements may have an important role here.   The grasp of curriculum facts and methods remains critical but the   emphasis shifts to their effective, contextualised use, in argument   structures, in discussion, in problem-solving. A focus on the   sociocultural learning system draws attention to how analytics   take into account the centrality of discourse for sensemaking, and   in constituting context.   We have gone beyond our learning analytics are our pedagogy   [6], arguing that they embody epistemological assumptions, and   they perpetuate assessment regimes. Moreover, as with any tool, it   is not only the design of the tool, but the way in which it is   wielded in context, that defines its value.    8. ACKNOWLEDGEMENTS  We are grateful to Cindy Kerawalla and 3 anonymous reviewers   for helpful comments on an earlier version of this paper.   9. REFERENCES  [1] Barzilai, S. and Zohar, A. 2012. Epistemic Thinking in   Action: Evaluating and Integrating Online Sources.   Cognition and Instruction. 30, 1 (2012), 3985.   [2] Black, P. and Wiliam, D. 2009. Developing the theory of   formative assessment. Educational Assessment, Evaluation   and Accountability. 21, 1 (2009), 531.   82    [3] Black, P. and Wiliam, D. 2001. Inside the black box.   BERA.   [4] Bromme, R. et al. 2009. Epistemological beliefs are   standards for adaptive learning: a functional theory about   epistemological beliefs and metacognition. Metacognition   and Learning. 5, 1 (Dec. 2009), 726.   [5] Buckingham Shum, S. 2008. Cohere: Towards Web 2.0   argumentation. Proc. 2nd Int. Conf. Computational Models   of Argument (28-30 May 2008, Toulouse), IOS Press.   pp.97-108 .   [6] Buckingham Shum, S. 2012. Our Learning Analytics Are   Our Pedagogy. Keynote Address, Expanding Horizons   2012 (Macquarie University, Oct. 2012).   [7] Buckingham Shum, S. and Ferguson, R. 2012. Social   Learning Analytics. Educational Technology & Society.   15, 3 (2012), 326.   [8] Cunnane, S. 2011. The Danish gambit: online access, even   during exams. Times Higher Education.   [9] Davis, A. 1998. 3: Understanding and Holism. Journal of   Philosophy of Education. 32, 1 (1998), 4155.   [10] Davis, A. 2006. High Stakes Testing and the Structure of   the Mind: A Reply to Randall Curren. Journal of   Philosophy of Education. 40, 1 (2006), 116.   [11] Davis, A. 2005. Learning and the Social Nature of Mental   Powers. Educational Philosophy and Theory. 37, 5 (Sep.   2005), 635647.   [12] Davis, A. 1999. The Limits of Educational Assessment.   Wiley.   [13] Davis, A. and Williams, K. 2002. Epistemology and   curriculum. The Blackwell guide to the philosophy of   education. N. Blake et al., eds. Blackwell Reference   Online.   [14] DeBacker, T.K. et al. 2008. The Challenge of Measuring   Epistemic Beliefs: An Analysis of Three Self-Report   Instruments. The Journal of Experimental Education. 76, 3   (2008), 281312.   [15] Dede, C. 2008. A Seismic Shift in Epistemology.   EDUCASE Review.   [16] Dewey, J. 1998. Experience and education. Kappa Delta   Pi.   [17] Edwards, A.D. and Furlong, V.J. 1978. The language of   teaching: Meaning in classroom interaction. Heinemann   London.   [18] Edwards, D. and Mercer, N. 1987. Common knowledge:   the development of understanding in the classroom.   Routledge.   [19] Ferguson, L.E. et al. 2012. Epistemic cognition when   students read multiple documents containing conflicting   scientific evidence: A think-aloud study. Learning and   Instruction. 22, 2 (Apr. 2012), 103120.   [20] Ferguson, R. et al. 2011. EnquiryBlogger: Using widgets to   support awareness and reflection in a PLE setting. 1st   Workshop on Awareness and Reflection in PLEs, Personal   Learning Environments Conference (Southampton, , 2011).   [21] Ferguson, R. and Buckingham Shum, S. 2012. Social   Learning Analytics: Five Approaches. (Vancouver, BC,   2012).   [22] Foster, J. 2009. Understanding interaction in information   seeking and use as a discourse: a dialogic approach.   Journal of Documentation. 65, 1 (Jan. 2009), 83105.   [23] Gardner, J. 2011. Assessment and learning. SAGE.   [24] Greene, J.A. et al. 2010. The Role of Epistemic Beliefs in   Students Self-Regulated Learning With Computer-Based   Learning Environments: Conceptual and Methodological   Issues. Educational Psychologist. 45, 4 (2010), 245257.   [25] Hammer, D. and Elby, A. 2003. Tapping Epistemological   Resources for Learning Physics. Journal of the Learning   Sciences. 12, 1 (2003), 5390.   [26] Harlen, W. 2007. Assessment of learning. SAGE.   [27] Hertzum, M. 2008. Collaborative information seeking: The   combined activity of information seeking and collaborative   grounding. Information Processing & Management. 44, 2   (Mar. 2008), 957962.   [28] Hofer, B.K. 2001. Personal epistemology research:   Implications for learning and teaching. Educational   Psychology Review. 13, 4 (2001), 353383.   [29] Hopmann, S.T. et al. eds. 2007. PISA According to PISA:   Does PISA Keep What It Promises Wien Lit-Verlag.   [30] Huffman, S.B. and Hochster, M. 2007. How well does   result relevance predict session satisfaction Proc. ACM   SIGIR Conference on Research and Development in   Information Retrieval (2007), 574.   [31] Katz, S. 2000. Competency, epistemology and pedagogy:   curriculums holy trinity. Curriculum Journal. 11, 2   (2000), 133144.   [32] Kelly, G.J. et al. 2008. What Counts as Knowledge in   Educational Settings: Disciplinary Knowledge,   Assessment, and Curriculum. Review of Research in   Education. 32, 1 (Feb. 2008), viix.   [33] King, P.M. and Kitchener, K.S. 2004. Reflective   Judgment: Theory and Research on the Development of   Epistemic Assumptions Through Adulthood. Educational   Psychologist. 39, 1 (Mar. 2004), 518.   [34] Kuhn, D. and Weinstock, M. 2002. What is   epistemological thinking and why does it matter. Personal   epistemology: The psychology of beliefs about knowledge   and knowing. B.K. Hofer and R. Pintrich, eds. Lawrence   Erlbaum Associates. 121144.   [35] Lazonder, A.W. 2005. Do two heads search better than   one Effects of student collaboration on web search   behaviour and search outcomes. British Journal of   Educational Technology. 36, 3 (May. 2005), 465475.   [36] De Liddo, A. et al. 2011. Discourse-centric learning   analytics. Proceedings of the 1st International Conference   on Learning Analytics and Knowledge (New York, NY,   USA, 2011), 2333.   [37] Lin, C. and Tsai, C. 2008. Exploring the Structural   Relationships between High School Students Scientific   Epistemological Views and their Utilization of Information   Commitments toward Online Science Information.   International Journal of Science Education. 30, 15 (2008),   20012022.   [38] Mason, L. et al. 2011. Epistemic beliefs in action:   Spontaneous reflections about knowledge and knowing   during online information searching and their influence on   learning. Learning and Instruction. 21, 1 (Feb. 2011), 137  151.   [39] Mason, L. et al. 2009. Epistemic metacognition in context:   evaluating and learning online information. Metacognition   and Learning. 5, 1 (Jul. 2009), 6790.   [40] Mercer, N. and Littleton, K. 2007. Dialogue and the   Development of Childrens Thinking: A Sociocultural   Approach. Routledge.   [41] Olson, D.R. and Bruner, J.S. 1996. Folk psychology and   folk pedagogy. The handbook of education and human   development. (1996), 927.   83    [42] sterholm, M. 2010. Relationships Between   Epistemological Beliefs and Properties of Discourse: Some   Empirical Explorations. Madif 7, the 7th Swedish   Mathematics Education Research Seminar (Stockholm,   Sweden, 2010), 241250.   [43] sterholm, M. 2009. Theories of epistemological beliefs   and communication: A unifying attempt. Proceedings of   the 33rd Conference of the International Group for the   Psychology of Mathematics Education (2009), 275264.   [44] Potter, J. and Edwards, D. 2003. Sociolinguistics,   cognitivism and discursive psychology. IJES, International   Journal of English Studies. 3, 1 (2003), 93110.   [45] Rittel, H. and Kunz, W. 1970. Issues as elements of   information systems. Working Paper No. 131. Institute of   Urban and Regional Development.   [46] Slj, R. 1999. Learning as the use of tools: A   sociocultural perspective on the human-technology link.   Learning With Computers: Analysing Productive   Interaction. K. Littleton and P. Light, eds. Psychology   Press.   [47] Slj, R. 2012. Literacy, Digital Literacy and Epistemic   Practices: The Co-Evolution of Hybrid Minds and External   Memory Systems. Nordic Journal of Digital Literacy. 01   (2012), 519.   [48] Salomon, G. 1996. Distributed Cognitions: Psychological   and Educational Considerations. Cambridge University   Press.   [49] Scardamalia, M. and Bereiter, C. 2003. Knowledge   Building. Encyclopedia of Education. Macmillan   Reference. 13701373.   [50] Scardamalia, M. and Bereiter, C. 2006. Knowledge   Building: Theory, Pedagogy, and Technology. Cambridge   Handbook of the Learning Sciences. K. Sawyer, ed.   Cambridge University Press. 97118.   [51] Scherr, R.E. and Hammer, D. 2009. Student Behavior and   Epistemological Framing: Examples from Collaborative   Active-Learning Activities in Physics. Cognition and   Instruction. 27, 2 (2009), 147174.   [52] Schommer, M. 1990. Effects of beliefs about the nature of   knowledge on comprehension. Journal of Educational   Psychology. 82, 3 (1990), 498504.   [53] Schraw, G. 2013. Conceptual Integration and Measurement   of Epistemological and Ontological Beliefs in Educational   Research. ISRN Education. (2013).   [54] Siegel, H. 1998. KNOWLEDGE, TRUTH AND   EDUCATION. Education, knowledge, and truth: beyond   the postmodern impasse. D. Carr, ed. Routledge. 1936.   [55] Siemens, G. 2006. Knowing Knowledge. Lulu.com.   [56] Stadtler, M. and Bromme, R. 2007. Dealing with multiple   documents on the WWW: The role of metacognition in the   formation of documents models. International Journal of   Computer-Supported Collaborative Learning. 2, 2 (2007),   191210.   [57] Tsai, C. 2004. Beyond cognitive and metacognitive tools:   the use of the Internet as an epistemological tool for   instruction. British Journal of Educational Technology. 35,   5 (Sep. 2004), 525536.   [58] Tsai, M.J. and Tsai, C.C. 2003. Information searching   strategies in web-based science learning: the role of   internet self-efficacy. Innovations in Education &   Teaching International. 40, 1 (2003), 4350.   [59] Undervisningsministerie (Ministry of Education) and   Afdelingen for Gymnasiale Uddannelser (Department of   Secondary Education) 2010. Endelig rapport fra   flgegruppen for forsget med digitale prver med adgang   til internettet i udvalgte fag p stx og hhx (Final report of   the Monitoring Group experiment with digital samples   with access to Internet in selected subjects at stx and   HHX). Undervisningsministerie (Ministry of Education).   [60] Walton, D. et al. 2008. Argumentation schemes.   Cambridge University Press.   [61] Walton, D. 1995. Argumentation schemes for presumptive   reasoning. Lawrence Erlbaum.   [62] Williams, K. 1998. Assessment and the challenge of   scepticism. Education, knowledge, and truth: beyond the   postmodern impasse. D. Carr, ed. Routledge         84      "}
{"index":{"_id":"12"}}
{"datatype":"inproceedings","key":"Ferguson:2013:ELA:2460296.2460313","author":"Ferguson, Rebecca and Wei, Zhongyu and He, Yulan and Buckingham Shum, Simon","title":"An Evaluation of Learning Analytics to Identify Exploratory Dialogue in Online Discussions","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"85--93","numpages":"9","url":"http://doi.acm.org/10.1145/2460296.2460313","doi":"10.1145/2460296.2460313","acmid":"2460313","publisher":"ACM","address":"New York, NY, USA","keywords":"k-nearest neighbour, MaxEnt, SocialLearn, computational linguistics, cue-phrase matching, discourse analytics, educational assessment, educational dialogue, exploratory dialogue, learning analytics, self-training framework, social learning, social learning analytics, synchronous dialogue","abstract":"Social learning analytics are concerned with the process of knowledge construction as learners build knowledge together in their social and cultural environments. One of the most important tools employed during this process is language. In this paper we take exploratory dialogue, a joint form of co-reasoning, to be an external indicator that learning is taking place. Using techniques developed within the field of computational linguistics, we build on previous work using cue phrases to identify exploratory dialogue within online discussion. Automatic detection of this type of dialogue is framed as a binary classification task that labels each contribution to an online discussion as exploratory or non-exploratory. We describe the development of a self-training framework that employs discourse features and topical features for classification by integrating both cue-phrase matching and k-nearest neighbour classification. Experiments with a corpus constructed from the archive of a two-day online conference show that our proposed framework outperforms other approaches. A classifier developed using the self-training framework is able to make useful distinctions between the learning dialogue taking place at different times within an online conference as well as between the contributions of individual participants.","pdf":"An Evaluation of Learning Analytics To Identify  Exploratory Dialogue in Online Discussions     Rebecca Ferguson1, Zhongyu Wei2, Yulan He3, Simon Buckingham Shum3     1 Institute of Educational Technology   3 Knowledge Media Institute  The Open University   Milton Keynes, MK7 6AA, UK  +44-1908-654956   rebecca.ferguson@open.ac.uk   2 Information Systems  Dept. Systems Engineering & Engineering Management   The Chinese University of Hong Kong  Hong Kong   (852) 3943 8461   zywei@se.cuhk.edu.hk    ABSTRACT  Social learning analytics are concerned with the process of  knowledge construction as learners build knowledge together in  their social and cultural environments. One of the most important  tools employed during this process is language. In this paper we  take exploratory dialogue, a joint form of co-reasoning, to be an  external indicator that learning is taking place. Using techniques  developed within the field of computational linguistics, we build  on previous work using cue phrases to identify exploratory  dialogue within online discussion. Automatic detection of this  type of dialogue is framed as a binary classification task that  labels each contribution to an online discussion as exploratory or  non-exploratory. We describe the development of a self-training  framework that employs discourse features and topical features  for classification by integrating both cue-phrase matching and   k-nearest neighbour classification. Experiments with a corpus  constructed from the archive of a two-day online conference show  that our proposed framework outperforms other approaches. A  classifier developed using the self-training framework is able to  make useful distinctions between the learning dialogue taking  place at different times within an online conference as well as  between the contributions of individual participants.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education    collaborative learning, distance learning.  General Terms  Measurement, Design.   Keywords  computational linguistics; cue-phrase matching; discourse  analytics; educational dialogue; exploratory dialogue; learning  analytics, educational assessment; k-nearest neighbour; MaxEnt;   self-training framework; social learning analytics; social learning;  SocialLearn; synchronous dialogue   1. INTRODUCTION  Learning analytics are concerned with the measurement,  collection, analysis and reporting of data about learners and their  contexts, for purposes of understanding and optimising learning  and the environments in which it occurs [1]. As learning is a  complex process, encompassing both knowledge construction and  identity formation [2], researchers are unable to measure learning  itself. Instead, they take measures such as group performance,  behavioural engagement and student grades as proxies for  learning [3-5]. However, these proxies reveal little about the  dynamic processes involved in the joint creation of meaning,  knowledge and understanding [6].   Learner dialogue has the potential to be more revealing, because  language functions as a psychological tool directed towards the  mastery of mental processes [7]. In forms of talk in which  knowledge is made publicly accountable and reasoning is visible,  it is possible to observe learning taking place as speakers  negotiate and express shifts in understanding.  Mercer and his colleagues identified three social modes of  thinking employed in the classroom (see (2.1): disputational,  cumulative and exploratory talk [8-10]. Of these, exploratory talk  is most characteristic of an educated discourse. In classroom  contexts, exploratory talk can be employed by teachers and taught  to students, thus producing measurable improvements in their  learning achievements [11]. Although these forms of dialogue  were first identified in face-to-face classrooms, learners also  employ them within online text-based discussion [6, 12, 13].   A previously reported pilot study analysed synchronous text chat  that took place during an online conference and identified cue  words and phrases that are indicative of exploratory dialogue [14- 16]. This suggested that learning analytics could be developed to  distinguish different types of contribution within text chat, and to  support learner engagement in fruitful learning discussions.  However, the manual approach employed in the pilot study was  time consuming and was not capable of identifying all relevant  cue phrases.   The research reported here therefore uses methods from  computational linguistics, the interdisciplinary field that deals  with statistical and rule-based modeling of natural language from  a computational perspective. These methods are used to assess the  pilot study [16] and to identify the associated challenges, to      Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. Copyrights for  components of this work owned by others than ACM must be honored.  Abstracting with credit is permitted. To copy otherwise, to republish, to  post on servers or to redistribute to lists, requires prior specific permission  and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   85  mailto:rebecca.ferguson@open.ac.uk mailto:zywei@se.cuhk.edu.hk   develop and test a self-training framework for analysis of online  textual discussion, and to develop prototypes of learning analytics  with the potential to support both learners and teachers.   The paper is organised as follows. We review related work in the  fields of educational research and computational linguistics in  order to clarify the research challenge (2). We then set out our  experimental method and, in so doing, introduce self-training  frameworks, the k-nearest neighbours approach, n-grams and cue  phrases (3). We then analyse our results (4) before describing  prototypes that show how our method of exploratory dialogue  detection could be employed to support learners and teachers (5).  We go on to outline ethical considerations of visual analytics (6)  before concluding with a consideration of the significance and  originality of our study and the identification of possibilities for  future research and development (7).   2. DIALOGUE ANALYSIS  2.1 Exploratory dialogue  An extensive programme of work by Mercer and his colleagues  has identified three social modes of thinking employed in the  classroom: disputational, cumulative and exploratory talk [6, 8, 9,  11-13, 17-20]. Disputational talk is unproductive, characterised by  individuals restating their own point of view while rejecting or  ignoring the views of others. Cumulative talk is potentially more  constructive; speakers build on each others contributions, adding  their own information and constructing a body of shared  knowledge and understanding, but they do not challenge or  criticise each others views.   Exploratory talk is more characteristic of an educated discourse  because it involves constant negotiation. Explanations and  reasoning are made explicit where necessary and all participants  make critical evaluations in order to reach joint conclusions.   Mercer and Littleton provide a clear description of its use in a  school environment:  Exploratory talk represents a joint, coordinated form of co- reasoning in language, with speakers sharing knowledge,  challenging ideas, evaluating evidence and considering options in  a reasoned and equitable way. The children present their ideas as  clearly and as explicitly as necessary for them to become shared  and jointly analysed and evaluated. Possible explanations are  compared and joint decisions reached. By incorporating both  constructive conflict and the open sharing of ideas, exploratory  talk constitutes the more visible pursuit of rational consensus  through conversation [11, p62].  Studies have shown that, in classroom contexts, teachers can  employ exploratory talk and teach it to students, thus producing  measurable improvements in learning achievement [19, 21-23].   Exploratory dialogue has also been found to support learning in  online textual discussions [12, 13]. The classification of these  three types of dialogue is well suited to large-group and informal  discussion in which participants may not be working through a  single argument, but are likely to be engaged in a more messy  process that intertwines multiple strands of argument and varying  personal goals. For this reason, we prefer exploratory dialogue  to collaborative reasoning, which is specifically a taught  approach [24]; to teacher-led effective discourse [25]; to  accountable talk, which is concerned primarily with content- focused talk [26]; and to argumentative knowledge construction,  which assumes a goal of convergence on a joint solution [27].   Previous studies of exploratory dialogue have employed  sociocultural discourse analysis to locate and study examples [28].  This method combines detailed analysis of dialogue in specific  events with comparative analysis across a sample of cases.    Our pilot study applied this method to the synchronous discussion  related to a two-day online teaching and learning conference (see  3.3 for dataset details) and identified 94 cue words or phrases  that could be indicative of exploratory dialogue, including:  Critiques  eg However, Im not sure, maybe  Discussion of resources  eg Have you read, more links  Evaluations  eg Good example, good point  Explanations  eg Means that, our goals  Explicit reasoning  eg Next step, relates to, thats why  Justifications  eg I mean, we learned, we observed  Others perspectives  eg Agree, here is another [15]  These cue phrases could be used to distinguish meaningfully  between conference sessions and to support evaluation of those  sessions. However, this was a labour-intensive approach that  made little use of the data-crunching power of computers and it  was therefore not possible to identify all possible discourse cues  signaling the presence of exploratory dialogue.   2.2 Automatic detection of   exploratory dialogue    The automatic detection of exploratory dialogue is closely related  to dialogue act detection, an area of computational linguistics. A  dialogue act is the meaning of an utterance at the level of  illocutionary force [29]. In other words, it is the function of a  sentence, or part of a sentence, within the dialogue. Hi, Hello  and Good morning are different words and phrases, but all  function as the dialogue act of greeting. Other dialogue acts  include: question, thank, introduce, suggest, feedback, confirm  and motivate [30]. Based on detailed analysis of extensive  annotated datasets, some dialogue act tag-sets have emerged as  pseudo-standards in this area [31]. These large annotated datasets  and tag sets are used to train classifiers that can distinguish  between different dialogue acts.  The detection of exploratory dialogue in online discussion can be  seen as a binary classification problem, requiring a classifier able  to label each turn in the dialogue as exploratory or non- exploratory. Our pilot study highlighted three difficulties  associated with the development of a classifier for exploratory  dialogue.   1. The annotated dataset is limited. Although there are  many online discussions, there are very few annotated  corpora developed for the detection of exploratory  dialogue. This rules out most supervised training  methods.   2. Text classification problems are typically topic driven.  In the case of exploratory dialogue, the focus is on  dialogue features that are not topic dependent.   3. Despite this focus on dialogue features, the topic of the  dialogue is relevant when identifying discussion that is  off-topic and should not be classified as part of an  ongoing exploratory dialogue. Therefore, both discourse  and topical features should be considered when  identifying exploratory dialogue.   86    The research challenge for this study was, taking these difficulties  into account, to develop a classifier capable of discriminating  between exploratory and non-exploratory contributions to online  text dialogue.   3. METHOD  To address the three challenges outlined above, we propose a  Self-training from Labelled Features (SELF) framework to carry  out automatic detection of exploratory dialogue from online  content. Our proposed SELF framework makes use of a small set  of annotated data and a large amount of un-annotated data. In  addition, it employs both cue-phrase matching and knn-based [32]  instance selection to incorporate discourse and topical features  into classification model training. The SELF framework makes  use of self-learned features instead of pseudo-labelled instances to  train classifiers by constraining the models predictions about  unlabeled instances. It avoids the incestuous bias problem of self- training approaches that use pseudo-labelled in stances in the  training loop. This problem arises when instances are consistently  mislabeled, which makes the model worse instead of better in the  next iteration.   3.1 Self-training frameworks  Self training is a form of semi-supervised learning [33] that makes  use of both labelled and unlabeled data for training. In self  training, a supervised model is first trained using labelled data.  The trained model then assigns a pseudo-label to each instance of  unlabeled data. These pseudo-labels are associated with a  confidence value indicating how certain the model is about this  identification. Instances with high confidence values are retained,  classed as pseudo-labelled instances because they have been  identified by the model rather than by humans, and automatically  merged into the existing set of annotated data. The process is  repeated, using the expanded dataset, until a given endpoint. This  may be reached when there is no improvement in performance,  when few or no labels are changed when the process is run, or  when the process has run a specified number of iterations.   However, the instance-based self-training method suffers from an  incestuous bias problem because adding mislabeled instances to  the training pool can degrade model performance, progressively  reducing the performance of the classifier as the training process  is repeated. We therefore employed a feature-based self-training  approach, training a classification model using labelled features  instead of labelled instances. While it is possible to employ any  classifier  for example the Nave Bayes [34] or Support Vector  Machines [35]  we only focused on training the Maximum  Entropy (MaxEnt) model [36] from labelled features, using the  Generalised Expectation (GE) criteria [37], and left the  investigation of other classifiers as future work. The self-training  loop derived labelled features from pseudo-labelled instances and  added these self-labelled features to the original labelled feature  set in order to re-train the model. The feature-based self-training  approach calculates word-class association probabilities by  averaging over many pseudo-labelled examples. This has a  smoothing effect, making this more tolerant to class prediction  errors than an instance-based approach and thus avoiding the  incestuous bias problem.   3.2 Running a self-training classifier with a   k-nearest neighbours approach   A k-nearest neighbours approach works on the basis that turns in  the discourse are likely to have the same classification as those   closest to them in terms of their topical features. When using this  approach, a section of dialogue is first assigned a pseudo-label.  These labels are described as pseudo because they are temporary   the classifier may or may not decide they are correct. The  classifier then assesses the probability that the pseudo-label is  correct by checking the labels of a number (k) of the nearest  neighbours of the pseudo-labelled instance.    A benefit of this approach is that it makes use of local topical  information within the dialogue to improve classification  accuracy. When the classifier method is applied to a specific piece  of dialogue, knn provides a way of increasing the salience of  domain-specific vocabulary. This can reduce the errors introduced  by pseudo-annotated instances generated by the classifier.   The combined process begins, as with the self-training framework  described in 3.1, by training the GE MaxEnt classifier on  manually annotated data, then running unlabeled data through the  classifier. The classifier assigns each unlabeled turn in the  dialogue p (p1, p2, pn) a pseudo-label (l) and a confidence value  (c) for that label.   For each instance of p, the classifier examines its set, pni (pni1,  pni2,  pnik), of k-nearest neighbours in terms of topical features.  These nearest neighbours have the pseudo-label set lni (lni1, lni2,  lnik) and confidence values cni (cni1, cni2, cnik). The support  value of pi is indicated by  which is computed using this  equation.       Here,  is an indicator function that takes a value of 1 if x is  true, 0 otherwise.  Only the pseudo-labels with a support value above a value (R)  determined by the researchers are considered to be correct.  Instances with a lower support value for their pseudo-labels are  discarded. This provides a new list of pseudo-annotated instances,  (p1, p2, pn)  the labeling of these is not only based on their  features, but has also been checked against their local context.  This list is merged into the annotated dataset, and the revised  dataset is used to retrain the classifier.   3.3 Dataset  For the pilot study, data were collected from Elluminate, a web  conferencing tool that supports chat alongside video, slides and  presentations. The focus was on the synchronous text-based  discussion related to a two-day online teaching and learning  conference organised by The Open University in 2010  (OUC2010). The Elluminate text chat in four conference sessions,  each between 150 and 180 minutes in length (24,530 words in  total) was investigated. During these four sessions, 233  participants logged in to the Elluminate sessions at one or more  times and 164 of these contributed to the synchronous discussion.  The majority of participants were higher education researchers  and practitioners from around the world, although most were UK- based [for more information on the dataset, see 15].   Each contribution to the OUC2010 text chat was considered to be  a turn in the dialogue.  There were 2,636 of these, containing  6,789 distinct word tokens in all. Turns in the dialogue were  typically short, containing a mean average of 10.14 word tokens.   is  ij 1  ( ) k  i ij j  i  l ln cn s  k   =  =  =   ( )x  87    For this study, we constructed an additional, un-annotated dataset  from three massive open online courses (MOOCS), LAK11,  LAK12 and CHANGE. Again, synchronous text-based discussion  from Elluminate sessions was collated. In this case, 49 sessions  were considered. During these sessions, 1152 participants  contributed 10,568 turns to the dialogue. Turns in the dialogue  were once again short, containing a mean average of 9.24 word  tokens.   Each of the Elluminate sessions included in the dataset was an  open, public event. Participants signed in using recognisable  names, recognisable online identities, role titles (for example,  moderator) and a variety of other pseudonyms. Participants were  aware that the sessions would be archived and would be made  openly available for replay online. In many cases, participants  were also aware that the archived sessions would be used as  research data. We therefore consider the records of these sessions  to be in the public domain.   3.4 Data preparation  We hired two postgraduate students to annotate a subset of  OUC2010, using the coding scheme set out in Table 1, above.  Each coder received one mornings training. Their task was to  classify each turn in the dialogue as exploratory or non- exploratory. All exploratory turns were also assigned one or more  sub-category labels (challenge, evaluation, extension, reasoning).  Dialogue transcripts were presented in chronological order so that  annotators could make decisions based on contextual information.   As in many learning environments, participants did not only learn  about subject matter, but also about the tools available to them  (such as Elluminate) and about their fellow learners. Only  dialogue related to subject matter was coded as exploratory (for  example I dont think your microphone is working was not  classed as evaluation).   Cohens Kappa coefficient was used to assess pairwise agreement  between coders making category judgments, correcting for  expected chance agreement [38]. Inter-annotator agreement was  0.5977 for the binary classification exploratory / non-exploratory.  This was taken to be moderate agreement, meaning that this  coding was reliable enough for the data to be used to train a  classifier.    Inter-annotator agreement was only 0.3887 for the multi-class  classification into sub-categories. Agreement on sub-categories  was therefore considered to be unreliable and these were not used  to train the classifier.    In order to increase reliability, only turns in the dialogue that were  classified in the same way by each coder were included in the  annotated dataset used to train the classifier. The coding work  therefore provided 2087 coded turns in the dialogue, 1417 coded  as exploratory and 670 coded as non-exploratory.   As OUC2010 was a two-day conference, with an afternoon and a  morning session each day, the annotated dataset was divided into  four subsets of roughly equal size, based on date and time of day:  OU22AM, OU22PM, OU23AM and OU23PM.   3.5 Discourse features incorporating   cue phrases   In order for the self-training process to run, dialogue turns had to  be presented as features. In this study, the features were n-grams   n-character slices of a longer string [39], including one-word  segments (unigrams), two-word segments (bigrams) and three- word segments (trigrams). As an example, the phrase that is why  would form three unigrams (that is why), two bigrams (that  is and is why) and one trigram (that is why). These three types  of n-gram were initially employed because the cue phrases  identified in the pilot study included unigrams, bigrams and  trigrams. All three types were retained because preliminary   1.1 Category 1.2 Description 1.3 Examples   1.4 Challenge   1.5 A challenge  identifies that  something may be  wrong and in need  of correction   1.6 calling into question  1.7 calling to account  1.8 contradicting  1.9 disputing  1.10 finding fault with  1.11 proposing revision  1.12 putting forward an opposing view   1.13 raising an objection   1.14 Evaluation  1.15 An evaluation has   a descriptive  quality   1.16 appraising  1.17 assessing  1.18 expressing in terms of something already known  1.19 judging   1.20 Extension   1.21 An extension  builds on, or  provides resources  that support,  discussion   1.22 applying idea to a new area  1.23 increasing the range of an idea  1.24 linking to, developing or providing related resources  1.25 requesting additional resources to support understanding  1.26 taking the same line of argument further   1.27 Reasoning  1.28 Reasoning is the   process of thinking  an idea through.   1.29 asking questions about content  1.30 changing position in the light of arguments presented  1.31 explaining  1.32 inferring  1.33 justifying your position  1.34 reaching a conclusion  1.35 working ideas out in a logical manner   Table 1: Coding scheme for sub-categories of exploratory dialogue. Dialogue turns coded in any of these categories   were also coded as exploratory. All other turns were coded non-exploratory   88    experiments showed that combining unigrams with bigrams and  trigrams gave better performance than using any one of them  alone, or any two of them together.   The 94 cue phrases identified in the pilot study were found to  have high precision (see Table 2, below) when used in a classifier   when they identified as a dialogue turn as exploratory it was  almost always a turn that a human classifier also identified as  exploratory. However, they missed many exploratory turns and  therefore had low recall performance. This meant they were not  suitable for use as the only training elements for a classifier.  Nevertheless, due to their high level of accuracy, they were used  to identify exploratory dialogue within the un-annotated data in  order to improve the accuracy of un-annotated dataset handling.   3.6 Experimental set-up  The study compared seven approaches to the development of an  exploratory dialogue classifier. The aim was to explore the overall  effectiveness of the proposed self-training framework (described  as approach 7, below), and the effectiveness of the two integrated  components, cue-phrase matching and kNN-based instance  selection. The seven approaches were:   1. Cue-phrase labeling (CP) Developed manually in the pilot  study, this approach searches turns in the dialogue for cue phrases   2. Supervised MaxEnt (MaxEnt) A supervised MaxEnt  classifier, trained using annotated data   3. Generalised Expectation (GE) A MaxEnt classifier, trained  using labeled features based on GE criteria. Labelled features are  accepted if the probability that they are associated with one  category exceeds 0.65   4. Self-training features (SF) The feature-based self-learning  framework, without cue-phrase matching or kNN instance  selection, described in 3.1.   5. Self-training features, including kNN (SF+KNN) kNN-based  instance selection is used to select the pseudo-labelled instances  from which the self-labelled features are derived  6. Self-training instances, including kNN and cue phrases  (SI+CP+KNN) Documents labelled by an initial classifier are  taken as training examples for a subsequent classifier  7. Self-training features, including kNN and cue phrases  (SF+CP+KNN) Our proposed method. The feature-based self- learning framework with kNN described in 3.2, incorporating the  cue-phrase matching method   In each run of the experiment, one of the four sections of the  annotated dataset OUC2010 was used as a test set, and all or part  of the remaining annotated dataset was used to train the classifier.  The un-annotated dataset was used for self-training. In order to  evaluate performance, all possible training/testing combinations  were tested, and the results of these runs were averaged. Where  cue phrases were added, this was done using the same  exploratory/non-exploratory ratio that was present in the initial  training set.   Four evaluation criteria were used to evaluate the experiment  outcomes.   Accuracy: How many of the classifiers decisions were correct  Calculated by dividing the number of correctly identified turns in  the dialogue by the total number of turns in the dialogue.   Precision: How many of the turns classified as exploratory were  actually exploratory Calculated by dividing the number of  exploratory turns by the number of turns classified as exploratory.    Recall: How many exploratory turns are classified as exploratory  Calculated by dividing the total number of exploratory turns by  the number of exploratory turns correctly identified by the  classifier.   F1: A weighted average of the precision and recall. F1 values  range from 0 (completely inaccurate) to 1 (completely accurate).   4. RESULTS   4.1 Overall performance  Table 2 shows the exploratory dialogue results obtained from the  OUC2010 dataset by using the seven methods identified above. In  each case, half a session was selected from the four conference  sessions for training purposes. The total number of dialogue turns  used for training therefore ranged between 220 and 330. The cue- phrase method (CP) provided the greatest precision, over 95%. At  the same time, it had the lowest recall value, only 42%. The  manually identified cue phrases were accurate indicators of  exploratory dialogue, but they missed over half the instances of  exploratory dialogue.   Approach Accuracy Precision Recall F1   CP 0.5389 0.9523 0.4241 0.5865  MaxEnt 0.7886 0.8262 0.8609 0.8301  GE 0.7658 0.7753 0.8717 0.8017  SF 0.7659 0.7572 0.8710 0.8062  SF+KNN 0.7701 0.7865 0.8539 0.8148  SI +CP+KNN 0.7888 0.8273 0.8602 0.8302  SF+CP+KNN 0.7924 0.8083 0.8688 0.8331   Table 2: Exploratory dialogue classification results, with the best  result for each measure highlighted in bold   Table 3: Examples of exploratory and non-exploratory features,  together with the probability that these unigrams, bigrams and   trigrams fall into the non/exploratory category  The original self-labelled features method (SF) yielded very  similar results to the GE method, and both performed worse than  the supervised classifier (MaxEnt). SF+KNN outperformed SF   89    alone, showing the effectiveness of employing a k-nearest  neighbours approach.   Our proposed self-training features method, including k-nearest  neighbours and cue phrases (SF+CP+KNN) outperformed all  others in terms of accuracy and F1 value. The instance-based  alternative (SI+CP+KNN) received the second-highest values for  accuracy and F1, but these results were only marginally better  than those generated by the MaxEnt approach.  Table 3 provides examples of the results obtained using our  proposed self-training features method, including k-nearest  neighbours and cue phrases.   4.2 Varying the size of the training set  We also explored the influence of the amount of training data on  the accuracy of these approaches, and the effectiveness of  incorporating cue phrases and the k-nearest neighbours approach.  To do this, we varied the size of the annotated dataset from an  eighth of a session to one session and compared the performance  of different approaches. Figure 1 shows the results for the four  approaches for which the amount of training data could be  manipulated.     Figure 1: Variation in accuracy depending on the size of the   training set  The performance of all approaches increased steadily as the size  of the training set was increased. Again, our proposed approach,  SF+CP+KNN, consistently out-performed the others. As the size  of the training set increased, the accuracy of the GE approach rose  rapidly, exceeding both SF and SF+KNN when the size of the  training set reached one session. This was expected; supervised  classifiers are typically more accurate than self-trained classifiers  if a large enough annotated database is available for training.   Incorporating both cue phrases and k-nearest neighbours  improved the self-training features method significantly. In Figure  1, the crosses indicating SF+CP+KNN are consistently higher  than either the diamonds representing the self-training features  method alone or the triangles representing the self-training  method incorporating k-nearest neighbours without cue phrases.  However, the k-nearest neighbours component did not prove to be  stable in its influence on effectiveness. When the size of the  training dataset reached one session, the k-nearest neighbours  (+KNN) component degraded performance when compared to the  self-training framework (SF).   4.3 Varying k in k-nearest neighbours  instance selection    In order to explore the impact of k, the number of neighbours in  kNN-based instance selection, on the performance of our  proposed framework, we varied its value in SF+CP+KNN. During   this part of the experiment, we used half a session of the annotated  dataset to train the classifier. As shown in Table 4, the best  performance is achieved when k is set to 3; this was the value for  k used in the experiment that gave the results reported in Table 2.     k Accuracy Precision Recall F1  1 0.7868 0.8007 0.8666 0.8282  3 0.7924 0.8083 0.8688 0.8331  5 0.7881 0.8005 0.8685 0.8292  7 0.7586 0.7505 0.8640 0.8001   Table 4: Performance of proposed method  using different values for k   5. APPLICATION OF EXPLORATORY  DIALOGUE DETECTION   Automatic detection of exploratory dialogue has many potential  uses in online learning environments. In the case of online  interactions that occur over long periods of time, such as  conferences and MOOCs, it could be used to enable learners to  focus on the most productive sections of an extensive resource.  Teachers could use the distribution of exploratory dialogue within  a learning session as a means of evaluating the learning that had  taken place. On an individual level, both the volume of  exploratory dialogue contributed by a learner and the ratio of  exploratory to non-exploratory dialogue could provide a basis for  self-reflection or for guided improvement.    In 5.2 and 5.3, we describe two prototype implementations  developed to facilitate more effective learning on The Open  Universitys SocialLearn platform. All analysis described in these  sections is based on session OU23AM of the OUC2010 dataset.   5.1 SocialLearn  SocialLearn (sociallearn.open.ac.uk) is a social media space tuned  for learning. It has been designed to support online social learning  by helping users to clarify their intention, to ground their learning  and to engage in learning conversations [16]. The systems  architecture includes a recommendation engine; a pipeline  designed to process data and output it in a form suitable for  analysis by SocialLearn recommendation services.  Our exploratory dialogue detection module (EDDM) has been  integrated within a development build of SocialLearn. There it  can be used to process online dialogues such as online discussions  in order to generate recommendations and contribute to user  profiles. When a dialogue is run through EDDM, each turn in the  dialogue is assigned a rating in the range -100 to +100 to indicate  the likelihood that it is exploratory (with a positive-value rating)  or non-exploratory (with a negative value). These ratings can be  used to generate timeline visualisations and user visualisations.   5.2 Timeline visualisation  The EDDM timeline visualisation presents an overview of the  distribution of exploratory dialogue over time. Learners could use  such a visualisation to focus on particular sessions of a discussion,  while teachers could use it to explore the reasons for this  distribution.  A straightforward way of generating such a visualisation would be  to plot the ratings of each turn in the dialogue, sorted into  chronological order.   However, synchronous online dialogue is noisy, with many  different conversations intertwined and overlapping, so this   90    approach generates a very volatile time series. In order to derive a  more consistent signal, we smoothed the series by selecting a  representative time point every m transcripts and then calculating  a rating for that time period by averaging over the window of m  transcripts. Figure 2 (above) shows a 150-minute conference  session in 10-message groupings (note that these are shown with  time stamps, but the time intervals are not equal). There are 48  representative time points of which 15 (represented in blue) have  negative ratings. It is clear that the dialogue exchanges at the  beginning and end of the session are mainly non-exploratory. This  is not surprising, as at the beginning of the conference people  introduced themselves and exchanged greetings, while at the end  of the conference they thanked the speakers, said goodbye and left  the session. What is more interesting is the intensive engagement  in exploratory dialogue in the final 40 minutes of the session, a  finding that was supported when the conference was examined  manually.   5.3 User visualisation  A user visualisation (Figure 3) provides a view of the contribution  of each participant to the online discussions. This could provide  teachers with a tool for monitoring and supporting engagement,  and a personalised version of this view could be used to support  student self-reflection. In this visualisation, each user is plotted in  a two-dimensional space defined by the total volume of user  contributions and the number of turns rated as contributions to  exploratory dialogue.   In Figure 3, each point represents a user. The gradient of the solid  line could be set at any level to differentiate between participants  in a learning group. Here it represents the level at which five posts  in every six are exploratory. In the conference section visualised  in this figure, a high proportion of the dialogue was exploratory,  with around one in three of those participants who made more  than five contributions to the dialogue represented on or near the  solid line. As highlighted, the most productive participant  contributed over 50 times, and 47 of these contributions were  classified as exploratory. At the other end of the spectrum was a  participant who contributed 15 times without engaging in any  exploratory dialogue.      Figure 3: User visualisation of session OU23AM in OUC2010      6. DISCUSSION   6.1 Ethical considerations of visual analytics  As discussed in [16], our social learning analytics perspective  draws attention to instilling reflective, self-regulating qualities in  learners, and to the importance of providing analytics tools  feedback to them, not just the institution tracking them. Given the  two visual analytics proposed here, we must therefore consider  whether it is appropriate to give all learners access to them,  bearing in mind the principles of good assessment for learning.   6.1.1 Visual Literacy  One issue (equally applicable to educators and administrators) is  literacy in the visual language. We propose that the bar graph  timeline visualisation (Figure 2) is relatively simple to interpret  but there are, of course, issues as to where thresholds are set for  classification and on what scale the bars are rendered. The user  visualisation (Figure 3) requires an understanding of why the  threshold line is set at that angle, and whether this should remain  fixed, or if it can be varied to support exploratory enquiry for  different kinds of learner or course. If we were to take the Purdue  Signals approach [40], we would not consider exposing this level  of detail to untrained users, but would provide only a very simple  feedback mechanism such as red/amber/green lights, with no      Figure 2: Timeline visualisation of the conference session OU23AM with the granularity m set to 10   91    rationale. This has demonstrated its power and ease of adoption,  but remains opaque to the end user.    6.1.2 Assessment for Learning   If we imagine visual analytics such as these properly embedded in  a learning environment, the graphs would be directly linked to the  underlying source texts and learner profiles. The research base  underpinning the work on designing formative assessment for  learning [41] emphasises the importance of providing motivating  feedback, and clear guidance on the next steps to take to improve,  in order to help develop self-directed learners. If learners are  performing very poorly, according to these metrics, we would  argue that they should be informed of this, and that they should be  guided to examples of better contributions.   What remains to be tested is how learners respond when presented  with such analytics, and whether they do indeed explore them in  ways that advance their learning. Another issue is whether  learners who are not performing well feel exposed by social  learning analytics, which render visible in an uncompromising  way the contribution patterns of individuals or (to take another  example) who is on the edge of a social network. It might be  argued that social learning analytics of this sort could have a  destructive impact on the individuality and creativity of learners  who feel pressured into conforming to what the analytics have  deemed to be good learning behaviours. Ultimately, this comes  down to the external validity of these proxies.    6.1.3 Participatory design and deployment  The participatory design (PD) movement has a long tradition  within human-centred computing and ethics [cpsr.org/issues/pd].  We hypothesise that PD that involves learners and educators from  the start may be particularly important for social learning  analytics (cf. [42]), given the sensitivity of social processes to  observation and quantification (even formative assessment, not  high-stakes summative assessment).    Arguably, but not yet proven, the process of engaging a  community of learners in reflecting on what such tools can or  should do, and how they might be used, is a pedagogically  effective strategy, since it introduces very explicitly to learners  what good could or should look like.    7. CONCLUSION   This paper has been the outcome of productive multivocality in  the middle space within which learning and analytics intersect.  Our interest in the possibilities offered by social learning analytics  prompted us to combine machine learning methods and natural  language processing techniques with theoretical frameworks that  have been developed in the classroom using qualitative methods.  In addition, this work draws on insights from psychology   particularly Vygotskys conceptualization of language as a  conceptual tool with which we simultaneously interpret and  construct our experience of the world [7]  and from English  literature  particularly Ongs understanding of how writing  enlarges the potentiality of language and restructures thought [43].   This paper is significant in that it has proposed and tested a self- training framework for the detection of exploratory dialogue  within online discussions. This self-training framework employs  cue phrases to make use of discourse features for classification. It  also uses a k-nearest neighbours instance selection approach to  draw on topical features and thus reduce the number of wrongly  labelled instances introduced by use of a self-training method.  Experimental results show that this approach out-performs six   alternative methods. In the future, this approach could be used to  provide visualisations and prompts that would support learners to  reflect on and develop their learning dialogue. This approach  could also be applied to recordings of long conference sessions or  presentations to highlight areas of interest to learners.   This paper is original in that it pioneers the development of  automatic exploratory dialogue detection. In order to develop our  self-training framework, we have built OUC2010, the first  annotated corpus for exploratory dialogue detection. We also  developed prototype visualisations on the SocialLearn platform  that show how this method could be used to produce learning  analytics visualisations to support both learners and teachers. In  the future, we intend to develop and test these prototypes.   In the research reported here, we focused only on the use of   n-grams. Future studies could explore other features, such as the  position of dialogue transcripts within a session. As illustrated in  Figure 2, dialogue turns at the beginning and end of a dialogue  session are likely to be non-exploratory. In addition, if one turn in  the dialogue is exploratory, this increases the likelihood that the  next turn will also be exploratory, as participants respond to a  challenge or develop a train of thought. Contextual information  could therefore be used to increase the effectiveness of the self- training framework. Another interesting area for future study  would be to explore automatic ways of expanding the cue phrase  list and combining it with machine learning methods for the  detection of exploratory dialogue.    8. ACKNOWLEDGMENTS  We gratefully acknowledge The Open University for making this  work possible by resourcing the SocialLearn project.   9. REFERENCES  [1] SoLAR, Open Learning Analytics: An Integrated &   Modularized Platform. White Paper, Society for Learning  Analytics Research. 2011.   [2] Sfard, A., On two metaphors for learning and the dangers of  choosing just one. Educational Researcher, 27, 2, (1998), 4- 13.   [3] Goodman, P. S. and A, D. L., Methodological issues in  measuring group learning. Small Group Research, 42, 4,  (2011), 379404.   [4] Koch, J., Does distance learning work A large sample,  control group study of student success in distance learning.  E-Journal of Instructional Science and Technology, 8, 1,  (2005).   [5] Axelson, R. D. and Flick, A., Defining student engagement.  Change: The Magazine of Higher Learning, 43, 1, (2010),  38-43.   [6] Littleton, K. and Whitelock, D., The negotiation and co- construction of meaning and understanding within a  postgraduate online learning community. Learning, Media  and Technology, 30, 2, (2005), 147-164.   [7] Vygotsky, L. S., The instrumental method in psychology.  In: R. W. Rieber and J. Wollock (Eds.), The Collected  Works of L. S. Vygotsky. Plenum Press. (Original work  written 1924-1934), New York, 1997.   [8] Wegerif, R. and Mercer, N., Computers and reasoning  through talk in the classroom. Language and Education, 10,  1, (1996), 47-64.   92    [9] Mercer, N. and Wegerif, R., Is 'exploratory talk' productive  talk In: P. Light and K. Littleton (Eds.), Learning with  Computers: Analysing Productive Interaction. Routledge,  London and New York, 1999.   [10] Mercer, N., Littleton, K. and Wegerif, R., Methods for  studying the processes of interaction and collaborative  activity in computer-based educational activities.  Technology, Pedagogy & Education, 13, 2, (2004), 195-212.   [11] Mercer, N. and Littleton, K., Dialogue and the Development  of Children's Thinking. Routledge, London, 2007.   [12] Ferguson, R., The Construction of Shared Knowledge  through Asynchronous Dialogue. PhD, The Open  University, Milton Keynes. http://oro.open.ac.uk/19908/  2009.   [13] Ferguson, R., Whitelock, D. and Littleton, K., Improvable  objects and attached dialogue: new literacy practices  employed by learners to build knowledge together in  asynchronous settings. Digital Culture and Education, 2, 1,  (2010), 116-136.   [14] Ferguson, R. and Buckingham Shum, S., Social Learning  Analytics: Five Approaches. In: LAK12: 2nd International  Conference on Learning Analytics and Knowledge (30 April  - 2 May) (Vancouver, Canada, 2012). ACM.   [15] Ferguson, R. and Buckingham Shum, S., Learning analytics  to identify exploratory dialogue within synchronous text  chat. In: Proc. 1st Int. Conf. on Learning Analytics and  Knowledge (27 Feb  1 Mar) (Banff, Canada, 2011). ACM.   [16] Buckingham Shum, S. and Ferguson, R., Social learning  analytics. Educ. Technology & Society, 15, 3, (2012), 3-26.   [17] Mercer, N., Words & Minds: How We Use Language To  Think Together. Routledge, London, 2000.   [18] Mercer, N., Developing dialogues. In: G. Wells and G.  Claxton (Eds.), Learning for Life in the 21st Century.  Blackwell Publishers, Oxford, 2002.   [19] Mercer, N., Wegerif, R. and Dawes, L., Children's talk and  the development of reasoning in the classroom. British  Educational Research Journal, 25, 1, (1999), 95-111.   [20] Wegerif, R., Using computers to help coach exploratory talk  across the curriculum. Computers & Education, 26, 1-3,  (1996), 51-60.   [21] Rojas-Drummond, S. and Mercer, N., Scaffolding the  development of effective collaboration and learning. Int.  Jnl. Education Research, 39, (2003), 99-111.   [22] Rojas-Drummond, S., Perez, V., Velez, M., Gomez, L. and  Mendoza, A., Talking for reasoning among Mexican  primary school children. Learning and Instruction, 13,  (2003), 653-670.   [23] Wegerif, R., A dialogic understanding of the relationship  between CSCL and teaching thinking skills. Computer- Supported Collaborative Learning, 1, (2006), 143-157.   [24] Anderson, R. C., Chinn, C., Waggoner, M. and Nguyen, K.,  Intellectually stimulating story discussions. In: J. Osborn  and F. Lehr (Eds.), Literacy for All. Guildford Press, New  York, 1998.   [25] Mezirow, J., Transformative learning: theory to practice.  New Directions for Adult and Continuing Education, 74,  (1997), 5-12.   [26] Michaels, S., O'Connor, C. and Resnick, L. B., Deliberative  discourse idealized and realized: accountable talk in the  classroom and civic life. Studies in the Philosophy of  Education, 27, (2008), 283-297.   [27] Weinberger, A. and Fischer, F., A framework to analyze  argumentative knowledge construction in computer- supported collaborative learning. Computers & Education,  46, (2006), 71-95.   [28] Mercer, N., Sociocultural discourse analysis: analysing  classroom talk as a social mode of thinking. Journal of  Applied Linguistics, 1, 2, (2004), 137-168.   [29] Austin, J. L., How To Do Things with Words. Clarendon  Press, Oxford, 1962.   [30] Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R.,  Jurafsky, D., Taylor, P., Martin, R., Ess-Dykema, C. and  Meteer, M., Dialogue act modeling for automatic tagging  and recognition of conversational speech. Computational  Linguistics, 26, 3, (2000), 339-373.   [31] Kral, P. and Cerisara, C., Dialogue act recognition  approaches. Computing and Informatics, 29, (2010), 227 250.   [32] Cover, T. and Hart, P., Nearest neighbor pattern  classification. EEE Transactions on Information Theory, 13,  1, (1967), 21-27.   [33] Zhu, X., Semi-supervised learning literature survey.  University of Wisconsin. Available online from  http://citeseerx.ist.psu.edu/viewdoc/summarydoi=10.1.1.10 3.1693, Madison, 2005.    [34] Domingos, P. and Pazzani, M., On the optimality of the  simple Bayesian classifier under zero-one loss. Machine  Learning, 29, 2-3, (1997), 103-130.   [35] Cortes, C. and Vapnik, V., Support-vector networks.  Machine Learning, 20, 3, (1995), 273-297.   [36] Nigam, K., Lafferty, J. and McCallum, A., Using maximum  entropy for text classification. Proc. IJCAI-99 Workshop on  Machine Learning for Information Filtering, (Stockholm,  Sweden, 1999), 61-67.   [37] Druck, G., Mann, G. and McCallum, A., Learning from  labeled features using generalized expectation criteria. In:  Proc. 31st ACM SIGIR Conf. (Singapore, 2008). ACM. 595- 602.   [38] Carletta, J., Assessing agreement on classification tasks: the  kappa statistic. Computational Linguistics, 22, 2, (1996),  249-254.   [39] Cavnar, W.B. and Trenkle, J. M. N-gram-based text  categorization, Symposium on Document Analysis and  Information Retrieval (Las Vegas, 1994), 161-175.   [40] Pistilli, M. D., Arnold, K. and Bethune, M., Using  Academic Analytics to Promote Student Success.  EDUCAUSE Review Online, July/Aug, (2012).   [41] ARG, Assessment for Learning: 10 Principles. Assessment  Reform Group (assessment-reform-group.org). 2002.   [42] Govaerts, S., Duval, E., Verbert, K. and Pardo, A., The  Student Activity Meter for awareness and self-reflection.  ACM. 2012.   [43] Ong, W. J., Orality and Literacy: The Technologizing of the  Word. Methuen, London, 1982.   93  http://oro.open.ac.uk/19908/ http://citeseerx.ist.psu.edu/viewdoc/summarydoi=10.1.1.10     "}
{"index":{"_id":"13"}}
{"datatype":"inproceedings","key":"Worsley:2013:TDM:2460296.2460315","author":"Worsley, Marcelo and Blikstein, Paulo","title":"Towards the Development of Multimodal Action Based Assessment","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"94--101","numpages":"8","url":"http://doi.acm.org/10.1145/2460296.2460315","doi":"10.1145/2460296.2460315","acmid":"2460315","publisher":"ACM","address":"New York, NY, USA","abstract":"In this paper, we describe multimodal learning analytics techniques for understanding and identifying expertise as students engage in a hands-on building activity. Our techniques leverage process-oriented data, and demonstrate how this temporal data can be used to study student learning. The proposed techniques introduce useful insights in how to segment and analyze gesture- and action-based generally, and may also be useful for other sources of process rich data. Using this approach we uncover new ideas about how experts engage in building activities. Finally, a primary objective of this work is to motivate additional research and development in the area of authentic, automated, process-oriented assessments.","pdf":"Towards the Development of Multimodal Action Based  Assessment   Marcelo Worsley  Stanford University   520 Galvez Mall, CERAS 102  Stanford, CA 94305, USA   mworsley@stanford.edu     Paulo Blikstein  Stanford University   520 Galvez Mall, CERAS 232  Stanford, CA 94305, USA   paulob@stanford.edu   ABSTRACT  In this paper, we describe multimodal learning analytics  techniques for understanding and identifying expertise as students  engage in a hands-on building activity. Our techniques leverage  process-oriented data, and demonstrate how this temporal data can  be used to study student learning. The proposed techniques  introduce useful insights in how to segment and analyze gesture-  and action-based generally, and may also be useful for other  sources of process rich data. Using this approach we uncover new  ideas about how experts engage in building activities. Finally, a  primary objective of this work is to motivate additional research  and development in the area of authentic, automated, process- oriented assessments.    Categories and Subject Descriptors   General Terms  Algorithms, Human Factors.   Keywords  Keywords are your own designated keywords.   1. INTRODUCTION  As much as we might like to think otherwise, assessment remains  a critical component of the educational system. Whether students  are engaged in a formal classroom lesson, or participating in play- based learning, there is the expectation that one can identify a  measurable outcome concerning how the student thinks, acts or  feels. Systematically demonstrating such learning outcomes in  project-based learning environment has long been a challenge  faced by education researchers [1, 2]. Early education researchers  [3, 4] recognized the merits of project-based learning, but  widespread adoption of the practice has largely been hampered by  this need to demonstrate its effectiveness at scale. The observed  challenge manifests itself in researchers having to choose between  traditional assessments that scale, but may be fundamentally  inconsistent with the process-oriented goals of project based  learning; and finding creative ways to use student portfolios,  micro-genetic analysis and ethnographies, all of which are unable  to scale to larger populations. Fortunately, we are arriving at a  time when the technological tools that are available through   machine learning and artificial intelligence can help make  process-oriented analyses for project-based learning, more  scalable.   Beyond the goal of moving away from traditional assessments,  that tend to neglect learning processes, we are also concerned  about how some traditional assessments are divorced from the  actual practices of the discipline in which they are administered.  This is particularly the case within many engineering disciplines.  In computer science, for example, it is not uncommon to have  students write pseudo code on an exam as a method for assessing  their programming proficiency, despite the fact that when writing  pseudo code, the student is restricted from utilizing the various  tools that may be used when actually programming. Similarly,  mechanical engineering students may be asked to derive an  equation or prove a theory on an exam, but seldom engage in  activities that are directly akin to the practices of the field. In  order to fill this gap, our current work looks to advance our ability  to understand and utilize forms of assessment that are more  closely tied to the practices of their respective disciplines. More  specifically, we study patterns in how students of different levels  of expertise go about completing the design and construction of  simple machines and structures.  At a high-level, this paper intends to:   - Present techniques for doing automated multimodal analysis  of student expertise while they engage in building tasks,   - Justify the pedagogical merit of our techniques  - Discuss the implications that these techniques have on the   future of assessments, and on our understanding of how  expertise is manifested through building.   - Motivate more widespread development and adoption of  process-oriented assessments through the use of multimodal  learning analytics.   2. THEORETICAL FRAMEWORK  2.1 Constructivism/Constructionism  This work fundamentally builds on Piagets notion that  knowledge is actively and dynamically constructed by the learner  based on resources that she already has, and Paperts  constructionism [5]. The study takes place within a constructionist  learning environment and involves students participating in the  physical construction of artifacts. These construction activities  give students the opportunity to develop their ideas by completing  several cycles of building and debugging.  Furthermore, students  have an opportunity to explore engineering design in an authentic  way that is challenging and engaging.        Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK 13 April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.     94    2.2 Knowledge in Pieces  This work also builds the knowledge in pieces (KiP) framework  [6] which considers how students make sense of fundamental  concepts in science by dynamically articulating and reorganizing  atomistic intuitions about the physical world, rather than making  use of robust theoretical systems. Moreover, KiP speaks to the  transition from being a novice to being an expert in a given field  by reorganizing rather than replacing ideas. According to diSessa  [6] experts share many of the same intuitions as novices, but have  the additional ability to know when those intuitions apply, how  they are connected, and when one must employ other concepts in  order to fully understand a scientific phenomenon. While diSessa  is primarily concerned with spoken descriptions of phenomena in  physics, we hypothesize that similar types of dynamic re- articulation of intuitions could be at play when students engage in  simple engineering building activities.  We also borrow from KiP theorists their focus on microgenetic  analysis [7] in that we look at student actions over small  timescales in order to interpret the mental constructs governing  their thinking and actions.   2.3 Multimodal Learning Analytics  Finally, the approach described, follows in a growing body of  literature concerned with developing learning analytics [10, 11]).  Here we specifically look at approaches that are multimodal in  nature, as we posit that identifying student learning likely requires  the ability to analyze and synthesize a variety of data streams.  Much of the previous work in this area of research has looked at  speech, gaze, sentiment and drawings as primary elements of  analysis [12, 13, 14]. Here we depart from those and study the  interaction of actions and gestures.    3. METHODS  3.1 Data  Data is drawn from thirteen participants. Each participant is given  everyday materials, and asked to build a tower that could hold a  mass of approximately 3 lbs. Participants were also challenged to  make the structure as tall as possible. Figures 1 and 2 depict  structures created by two different participants.     Figure 1 - Sample Expert Structure     Figure 2 - Sample Novice Structure   The task was designed to successfully students are able to take  their intuitions about mechanical engineering and physics and  translate them into a stable, well-engineered structure.  As such,  we expected students to use knowledge about forces, symmetry,   and the affordances of different geometric objects, to enable them  to complete the task. The additional challenge of making the  structure as tall as possible was introduced to push all students,  regardless of expertise, to the limits of their ability.   An additional design consideration for this task was the existence  of explicit metrics for measuring the success of their work. These  metrics include whether the structure could hold the mass, how  tall the structure is and how long the structure is able to hold the 3  lbs. mass.   In terms of the actual building task, students were given four  drinking straws, five wooden popsicle sticks, a roll of masking  tape and a paper plate; and were told They were told that they  would receive approximately ten minutes to complete the activity.  However, they were permitted to work for as long as they wanted,  with participation time ranging from eight minutes to fifty-two  minutes.      Figure 3  The Data Capture Environment   Figure 3 depicts the capture environment used to record the audio,  video and gesture data streams. Audio was used to capture  meaningful utterances made by the participants, though students  were not required to engage in a think-aloud. Audio was also  captured of each students metacognitive analysis of their building  approach. Video captured the movement of objects as students  progressed through the task, while gesture data, which consisted  of twelve upper-body parts, recorded the students physical  actions.   3.1.1 Defining Expertise  Prior to the study students were classified based on their perceived  level of expertise in the domain of engineering design. Expertise  was primarily based on participants previous experience with  engineering design. Such experiences could be in either a formal  or informal context. More specifically classification was made  along two main dimensions. The first dimension pertains to the  amount of formal instruction students had received in engineering.  Individuals who had completed bachelors or graduate degrees in  engineering were labeled as experts. The second dimension for  determining expertise in engineering was based on observations  that the researchers made while watching the students over the  course of more than two-hundred hours in an engineering and   Overhead camera for object  tracking   Skeletal overlay of gesture  capture   Building  materials   95    digital fabrication class. As a part of these two-hundred hours of  observation, the researchers also had the chance to learn about the  ways that participants engaged in engineering activities in extra- curricular activities and at home.   This definition of expertise resulted in population of three experts  (graduate students in mechanical engineering), two high expertise  students, five medium expertise students, and three low expertise  students.   3.2 Coding  In order to establish a basis for comparing across the thirteen  students, we created a coding scheme. This coding scheme  consists of eleven object manipulation codes. This set of codes  was identified through open coding of a sample of the videos, and  agreed upon by a team of research assistants. The codes are  entirely based on participant object manipulation, or lack thereof,  and are not an attempt to explicitly interpret a students intentions.  Nonetheless, we would argue that in most cases, the codes are  necessarily tied to user intent, since they are strictly action  oriented.     Table 1- Fine-Grain Object Manipulation Codes   Code Description   Building Joining things together by tape or other means that is relatively permanent.    Prototyping  Mechanism    Seeing if putting two (or more) things  together will work well. This could also  include acting out a mechanism with the  materials.   Testing  Mechanism   Involves testing of a subsection of the  overall system.   Undoing Taking things apart as to make a change to a previous build.  Single Object  Examination   Pressing or bending on an object to explore  its properties   Thinking  without an  object in hand   Simply surveying the pieces, but not  touching anything, or actively doing  anything.   Thinking with  an object in  hand   Not building, or testing the objects  properties explicitly, but still holding the  object.   System Testing  Putting force on a collection of relatively  permanently affixed pieces to see if they  will hold the mass   Organizing Repositioning the raw materials but not actually building, examining or prototyping.   Breaking Breaking apart sticks, bending straws, or ripping tape (in an usual way)    Adjusting  Often times involves moving something to  slightly reposition it, or applying more tape  to make something stay better.     Using the above codes we were able to condense the students  actions into comparable sequences of time-stamped codes. These  codes will serve as a primary data source for the analysis  described in the following section.    3.3 Object Manipulation Data Analysis  3.3.1 Sequence Construction  We begin the automated portion of the analysis with the time- stamped action logs for each student. We first compress similar  action codes. More specifically, we compress the codes to the  following five classes:     Table 2 - General Object Manipulation Action Classes      With these more general classes of behaviors, we construct a  sequence of user actions that are based on half-second increments.  Thus, for each user we will have an ordered list of actions, as  observed every half a second.    3.3.2 Sequence Segmentation  Each sequence of actions is then segmented any time a TEST  action occurs. Our assumption is that we need to have a logical  way for grouping sequences of user actions and each time a user  completes a TEST action, they are essentially signaling that they  expect for their previous set of actions to produce a particular  outcome. Each segments is recorded, based on the proportion of  each of the five action classes (BUILD, PLAN, TEST, ADJUST,  UNDO) that took place during that segment. Put differently, we  now have a five dimensional feature vector for each segment,  where each dimension corresponds to one of the action classes. As  an example, consider the following set of codes:   PLAN, PLAN, BUILD, TEST, ADJUST, UNDO, BUILD, TEST   This sequence of 8 codes would be partitioned into four segments.  The first segment would be PLAN, PLAN, BUILD; the second  would be TEST; the third would be ADJUST, UNDO, BUILD;  and the fourth would be TEST. These four segments would then  be used to construct four feature vectors based on the proportion  of each of the action classes. Accordingly, we would have the  following:   Table 3 - Sample Segmented Feature Set   Segment ADJUST BUILD PLAN TEST UNDO   1 0.00 0.33 0.67 0.00 0.00   2 0.00 0.00 0.00 1.00 0.00   3 0.33 0.33 0.00 0.00 0.33   4 0.00 0.00 0.00 1.00 0.00     3.3.3 Segment Standardization  Each column of the feature set is then standardized to have unit  variance and zero mean. This step is taken in order to ensure that  there are no biases when we perform clustering in the next step.   Class Codes  BUILD Building and Breaking   PLAN  Prototyping mechanism, Thinking with or without  an object, Single object examination, Organizing  and Selecting materials   TEST Testing a mechanism and System testing  ADJUST Adjusting  UNDO Undoing   96    3.3.4 Segment Clustering  Following standardization the segments are clustered into ten  clusters using k-means. Each segment is now associated with one  of ten clusters. Each participants action sequence is reconstructed  to reflect one of the ten clusters for each segment, recalling that  the action sequence is segmented based on TEST.   3.3.5 Dynamic Time Warping  Finally, dynamic time warping [15] is used to compute the  minimum distance between each pair of participants. The distance  between two clusters is determined by the cluster centroids from  k-means, and is based on Cosine distance. This computation  yields an n-by-n matrix of minimum distances, where each  distance is normalized by the length of the vectors being  compared.   3.3.6 Distance Clustering  The n-by-n matrix from the dynamic time warping calculation is  standardized along each column, before being used to construct  the final clustering, again with k-means. In order to compare the  clusters to expertise classifications, we find the cluster to expertise  alignment that minimizes the total error.   In summary, this algorithm converts an action sequence into  segments based on when a subject tests their structure or tests a  mechanism. The proportions of actions in the different segments  are used to find representative clusters, which are used to re-label  each users sequence of segments. Finally, we compare sequences  across participants and perform clustering on the pair-wise  distances in order to find a natural grouping of the participants.   3.4 Gesture Data Analysis  The gesture data analysis, while similar in spirit to the object  manipulation analysis, involves markedly less complexity. This is  partially due to the particularly fine-grained nature of the data,  which was captured every millisecond. Capturing millions and  sometimes billions of data points for each user and attempting to  use these for doing sequence alignment is a computationally  expensive task, which we may endeavor to explore further in later  work. Instead, for this analysis we take a simpler approach. This  approach is motivated by an observed difference between the  amounts of two-handed, coordinated, movement among  individuals of differing expertise. Here we consider two-handed  coordinated movement to be when a participant is using both of  their hands within a given action. Figures 4 and 5, which graph  the cumulative displacements for the right and left hand, depict  this difference. The expert's hands typically move in sync with  one another, whereas the novice's hand movements are markedly  asynchronous. We look to exploit this difference in constructing  our algorithm.  Given the gesture data from each individual's hands, we begin by  constructing a vector based on the absolute difference in the  cumulative displacement of their two hands. We then sample each  of those distributions at five percent increments, such that all  participants will have feature vectors of equal length. These  feature vectors are then used to compute the pairwise Euclidean  distance between every set of two participants. Those distances  are standardized by column, and used as the input for Hierarchical  Agglomerative Clustering, with four clusters. We tried using K- means clustering also, but found that most students were being  assigned to the same clusters. In future work we will more closely  examine why Hierarchical Agglomerative clustering was most  successful for this analysis. Finally, the clusters are aligned to the  levels of expertise as to minimize the total error.        Figure 4 - Novice Cumulative Hand Movements        Figure 5  Expert Cumulative Hand Movements      4. RESULTS  This study focuses on the nature and frequency of building  patterns that we observed among the students, through process- oriented data analysis techniques. In order to motivate the utility  of our approach, we begin by taking a static, non-process- oriented, view of the students' actions. Here we take non-process- oriented to mean that instead of looking at the entire sequence as  an ordered set of data points, we will only look at the data in  aggregate.   4.1 Non-Process Oriented Analysis       Figure 6 - Proportion of Object Manipulation Classes by   Expertise     97    Figure 6 presents the proportion of time that each student spent on  the five general action classes. From the graph it is quite unclear  as to how one would go about accurately predicting expertise  based solely on these overall proportions. More specifically, there  does not seem to be a linear relationship between any of the five  general classes and expertise. Instead we see that in some cases,  as in the case of time spent in PLAN, experts are most similar to  novices. However, in other cases, as in the case of ADJUST  (Figure 6), experts and people of medium expertise are the most  similar. This is merely one example of a non-linear progression.  Nonetheless, we can take these values and learn models that are  aligned with expertise. Figure 7 presents the results from a logistic  regression model, with 10-fold cross validation, as well as k- means clustering. As a point of comparison, two baseline  measures are also reflected in Figure 7. Similar analyses were also  completed using other machine learning algorithms: Decision  Trees, Neural Networks and Bayesian Networks, but all with  similar results. Furthermore, we are cautious about using  supervised learning with such a small dataset, because the  algorithms are likely to over fit to the data.        Figure 7  Classifier Accuracy Based on Proportion of Object   Manipulation Classes by Expertise     Another non-process-oriented metric for comparison could be the  time spent to complete the task and the overall success of a given  build. Table 4 shows the amount of time each student took to  complete the task, as well as a binary scoring concerning the  success of their structure.   While previous literature would suggest that experts take less time  to complete tasks [16] this is only partially true for our population  and task. Using these values to differentiate between different  levels of expertise worked better than the action code proportions,  (see Figure 8) elapsed time and success represent very  unsatisfying features. They are unsatisfying because the nature of  the problem is not one that would easily align with this paradigm.  For example, because of the challenge to make the structure as tall  as possible, experts may find themselves spending more time than  novices in an effort to perfect their design. This would distort the  expected time trend. At the same time, it could also distort our  expectations around success, since an expert may take a  functioning structure and render it unsuccessful in an effort to  make it taller.        Table 4 - Elapsed time and success for each participant   Subject Expertise Time(s) Success   1 Medium 1387 Yes   2 High 909 Yes   3 Medium 491 Yes   4 Low 1550 No   5 Low 3077 No   6 Medium 1265 Yes   7 Medium 1366 Yes   8 Medium 1373 Yes   9 Low 1730 No   9 Medium 2363 No   10 High 713 Yes   11 Expert 834 Yes   12 Expert 1100 Yes   13 Expert 1122 No          Figure 8 - Classifier Accuracy Based on Elapsed Time and   Success  Taken as a whole, these non-process oriented analyses fail to  account for the temporality of the data, and the important ways  that the temporality of actions is associated with user expertise. At  the same time, simply using time and success takes a very nave  view of expertise and begs for an algorithm that can more closely  capture the nuances of expertise.   4.2 Object Manipulation Results  In contrast to the non-process-oriented approach, our object  manipulation analysis algorithm is able to significantly  outperform both random assignment and majority class  assignment, all while preserving the process-oriented nature of the  task. Figure 9 highlights the accuracy attained through our object  manipulation analysis, and the other techniques, keeping in mind  that our approach has been completely unsupervised.   98      Figure 9  Classifier Accuracy Based on Object Manipulation   Algorithm as Compared to Other Techniques     Similarly, the confusion matrix derived from our work is seen in  Table 5.   Table 5 - Confusion Matrix of Expertise    Low Medium High Expert   Low 3 0 0 0   Medium 3 1 1 0   High 0 0 2 0   Expert 0 0 0 3     From the confusion matrix we see that the algorithm worked best  at uniquely clustering expert behavior which it did at an accuracy  of 1. It also attained recall of 1 for individuals of low expertise.  However, for those individuals of intermediate levels of expertise,  the algorithm was less accurate, but was still able to do a  reasonably job, considering that our metric of expertise may be  noisy for participants of medium expertise.   Of additional interest is the cluster centroids for the segments, as  these elucidate what each cluster segment represents. Figure 10  highlights these differences along the dimensions of the five  general object manipulation action classes (the cluster centroids  that we discuss here do not correspond to clustered students, but  to clusters of segments.) Showing the clusters centroids for the  students would only show how different each cluster is from the  other clusters based on average dynamic time warp distance.       Figure 10  Cluster Centroids from K-means Clustering      4.2.1 TEST Cluster  Cluster 1 represents our TEST action, and was used for  segmenting the sequence of actions. Accordingly, we expect for  this to be small in magnitude, and for all of the other clusters to  include below average TEST action proportions.    4.2.2 UNDO Clusters  Beyond this, one immediate observation is the amount of UNDO  actions. For clusters 2, 4, 6, 9 and 10, undoing represents the  primary component of that segment. This, on the whole, suggests  that undoing is an important behavior to pay attention to when  studying expertise. However, simply looking at UNDO by itself is  not sufficient. Instead, one needs to observe what other actions are  taking place in the context of the UNDO action. In the case of  cluster 2, the user is performing significant UNDO actions in the  absence of any other action. This is in contrast to cluster 4, for  example, where the user is completing a large number of UNDO  actions, but is also doing several BUILD actions. From this  perspective, cluster 2 seems to correspond to doing a sustained  UNDO, without any building. An example of this would be a  student completely deconstructing their structure. Cluster 4, on the  other hand, is more akin to undoing a few elements of ones  structure with the intent of immediately modifying the structure.  These may be more microscopic UNDO actions, whereas cluster 2  consists of more macroscopic UNDO action. Clusters 6 and 9  appear to be characterized by a combination of UNDO actions and  ADJUST actions. So in this case, the user is undoing, not to make  large structural changes to their design, but to make small  adjustments. Cluster 6 differs from cluster 9, however, in that  cluster 6 also contains both BUILD and ADJUST elements.   4.2.3 PLAN, BUILD, ADJUST Clusters  The remaining clusters, 3, 5 and 7, involve few UNDO actions,  but can be characterized as different combinations of PLAN,  BUILD and ADJUST. Cluster 3 almost exclusively consists of  PLAN actions, whereas clusters 5 and 7 primary include BUILD  and PLAN actions.   In summary we see that six of the cluster centroids play a large  emphasis on UNDO actions, and the context that they appear in  while the remaining four are aligned with different proportions of  TEST, PLAN, BUILD and ADJUST actions.     4.3 Gesture Analysis Results  The gesture analysis also yielded promising results. Recall that  here we used the difference between the cumulative displacement  of the right hand and the cumulative displacement of the left hand.      Table 6 - Confusion Matrix from Gesture Analysis    Low Medium High Expert   Low 1 2 0 0   Medium 1 2 1 1   High 0 1 0 1   Expert 0 0 1 2     From the confusion matrix in Table 6 we see that the gesture  channel appears to be less conclusive than the action code  modality. And, in fact, this is expected given the fact that we were  unable to take as fine-grained of an approach to this analysis. The   99    results are also reflective of only looking at a single set of gesture  data points, namely the hands. That said, when we relax our levels  of expertise to simply be binary, we see that the algorithm  performs significantly better (see Table 7)     Table 7 - Confusion Matrix from Binary Expertise Gesture  Analysis   Expertise Low-Medium High-Expert   Low-Medium 6 2   High-Expert 1 4    Again, this resulted in an accuracy of .77, surpasses accuracy  from single class assignment, .62. Thus, while it is apparent that  this model does not perfectly segment the data, is does correlate  with previous findings concerning two-handed inter-hemispheric  interaction [17]. More specifically, previous work on the brain has  identified that two-handed interaction is crucial for successful  problem solving. By using two hands, individuals can  simultaneously engage the right and left hemispheres of the brain.  Doing so permits them to create new ideas, which are mediated by  the right hemisphere, and logically choose which of those ideas to  utilize, which is mediated by the left hemisphere. These results  can therefore be interpreted to suggest that more expert  individuals are able to engage both of the processes needed to  successfully solve the problem: idea generation and logical  selection of the appropriate idea. Furthermore, this ability to select  the most applicable idea is analogous to the reprioritization and  appropriate use of intuitions that diSessa [6] observed in his  expert-novice comparisons. Thus it may not be that the novices  are unable to develop the same ideas, it may instead be that they  are less capable of identifying which of their structural building  ideas to use, and when each one should be used. As we will  describe later, future research will help us explore this theory in  more detail.   5. DISCUSSION  5.1 Pedagogical Considerations  From a pedagogical perspective, we would like to begin this  discussion by first taking a moment to acknowledge the non- traditional, yet well-received nature of this form of assessment on  the part of the students. Many of the students that we work with  have difficulty fully engaging with STEM content. The students  often times require frequent encouragement from their instructors  in order to successfully complete their assignments, and, if left  alone, will quickly deviate from their assigned task. However, for  a number of these students, the construction of the simple tower as  a form of assessment, not only increased their engagement, but  caused some to ask for additional opportunities to demonstrate  their knowledge through building. This is largely because the  activity didnt feel like a test, but, instead, was a fun engineering  challenge. In particular, one student, who typically was shy and  apprehensive about attempting to tackle STEM assignments,  experienced a significant boost in confidence from participating in  the building task. This is merely to suggest that at least for the  population of students that tend to struggle within traditional  STEM classrooms, making available to them novel forms of  assessment that allow them to demonstrate their knowledge  through other means represents a promising opportunity.   5.2 Object Manipulation Analysis Discussion  Moving now to the results of the object manipulation analysis, we  see three primary contributions. On the whole, we have presented  an algorithm that can effectively be used to group students based  on the actions that they take while participating in the building of  simple machines and structures. A key component of this  algorithm is the identification of the appropriate unit of analysis.  We showed that looking at the proportion of different actions  across the entire building task fails to generate meaningful  comparisons. Instead one should use an approach that captures the  temporality of the data. We also explored the use to constant time  based segmentation - segmenting every 10 seconds, for example -  and normalized time based segmentation - segmenting every five  percent of someones codes - however, neither of these  approaches were met with success. Instead, segmentation should  take place based on mechanism testing and system testing, as its  these actions that appear to accurately represent a unit of work.   Another key insight has to do with the nature of collapsing the  original eleven codes. Collapsing codes has important cognitive  and computational implications. Given that we would like to  enable automatic labeling of the different actions taken by a  participant, code collapsing makes this increasingly feasible.  Instead of having to identify very fine grained, hard to detect  differences between building and breaking, for example, the  action classification algorithm will only need to be trained on five  classes of actions. From a cognitive perspective, these findings  may suggest that while an observer may see the activities in each  state, prototyping a mechanism or examining an object, for  example, as distinct activities, sets of activities may actually serve  the same cognitive role within the participant. This is to say that  prototyping a mechanism may be cognitively the same as  examining an object  and we can say they are the same because it  appears as though individuals of the same level of expertise use  them in similar ways, as they plan their design.  Nonetheless,  further analysis is required to gain additional insight into these  potential cognitive similarities.   Finally, the algorithm provides a very fine-grained representation  of the action states that are salient for the data set. Following  the first instantiation of k-means, we were left with a set of  representative states that were shared across several  participants. Recall that each state consisted of the proportion of  time spent doing each of the five general action classes, within a  given segment. This representation of the action states is several  levels of granularity beyond what could reasonably be inferred by  a human observer. Instead, humans tend to be limited to seeing  states that are largely characterized by a single action code. For  example, a human may be inclined to group all UNDO actions  into the same state, when, in fact, the context in which UNDO  actions are happening is very important. Our analysis is able to get  states that are characterized by relative proportions of all of the  action codes. This provides a much more precise representation of  the different states and helps in articulating a clearer difference  among participants of differing expertise.   5.3 Gesture based analysis  The gesture based analysis also produced a number of key  findings. First, there are clearly correlations between the gestures  individuals make and the object manipulation action that they  undertake. This finding is inferred from the fact that both  techniques were able to yield relatively accurate results. This,  again, may be useful for improving automatic detection of object  manipulation actions. Additionally, the analysis was able to make  use of a theory concerning two-handed coordination and the   100    implications that this has on problem solving. In our case we  found that two-handed coordinated actions were correlated with  expertise. It is our conjecture that there are additional theories  related to embodied cognition that can be discovered or leveraged  in research concerning building-based assessments.   Finally, the gesture-based analysis highlights a potential area of  easy intervention for trying to effect behavioral changes among  students. Though we have yet to explore these interventions, one  can imagine showing a student a plot of their own hand  movements while they are participating in a building task, and see  how this additional awareness of their body movements either  helps, or hurts their ability to successfully complete the task. Such  an intervention could be enhanced by sharing with the student  knowledge about two-handed inter-hemispheric interactions, to  see how this helps the student perform more like an expert.   Looking at the analysis as a whole, we are looking to motivate the  development of authentic, process-oriented assessments that can  be enacted in minimally instrumented environments. Our interest  in doing this is to create additional ways for validating student  learning in project-oriented environments. This goal is also  grounded in a desire to develop techniques that can eventually be  utilized within both formal and informal learning environments.   In future work we plan to combine our data capture technique  with a think-aloud protocol, as so we can begin to align user  actions and user cognition more explicitly. We will also endeavor  to study how collaboration influences the emergence of expert- like behaviors. Finally, we will continue to work towards  developing techniques for automatically labeling user object  manipulation actions during the task explored in this analysis, as  well as with other tasks.   6. CONCLUSION  In this paper, we have presented a pair of techniques for analyzing  and detecting expertise as recognized through object manipulation  and gestures. In so doing, we identified key elements in how to  segment and compress object manipulation codes, while also  showing how dynamic time warping combined with clustering can  be used to accurately classify student expertise. In addition to  classification, we have generally motivated the use of multimodal  learning analytics for supporting authentic, process-oriented  assessments, as this technique has permitted us to realize a more  fine-grained level of expertise delineation than could have been  reasonably perceived by a human. Finally, the approach has made  it evident that meaningful analysis can be gleaned from simply  watching and measuring student actions as they participate in  building tasks, a realization that we hope will encourage other  researchers to embark upon this promising, yet challenging, area  of study.   7. REFERENCES  [1] Pea, R. (1987). Programming and problem-solving:   Childrens experiences with Logo. In T. OShea & E.  Scanlon (Eds.), Educational computing (An Open University  Reader). London: John Wiley & Sons.   [2] Barron, B., & Darling-Hammond, L. (2010). Prospects and  challenges for inquiry-based approaches to learning: OECD  Publishing   [3] Dewey, J. (1897). My Pedagogic Creed. School Journal 54,  77-80.   [4] Dewey, J. (1913). Interest and effort in education.  Cambridge, MA: The Riverside Press.   [5] Papert, S. (1980). Mindstorms: children, computers, and  powerful ideas. New York: Basic Books.   [6] diSessa, A.A. 2002. Why conceptual ecology is a good  idea. In M.Limn & L.Mason (Eds.), Reconsidering  conceptual change: Issues in theory and practice (pp. 2960).  Dordrecht: Kluwer.   [7] Siegler, R.S., & Crowley, K. (1991). The microgenetic  method: A direct means for studying cognitive development.   American Psychologist, 46, 606-620.   [8] Kirsh,D. (2009). Problem Solving and Situated Cognition. In  Philip Robbins & M. Aydede (eds.), The Cambridge  Handbook of Situated Cognition. Cambridge.   [9] Wagner, S., Nusbaum, H., & Goldin-Meadow, S. (2004)  Probing the mental representation of gesture: Is handwaving  spatial Journal of Memory and Language. 50, 395-407.   [10] Siemens, G. and Baker, R. (2012). Learning analytics and  educational data mining: towards communication and  collaboration. In Proceedings of the 2nd International  Conference on Learning Analytics and Knowledge (LAK  '12), Simon Buckingham Shum, Dragan Gasevic, and  Rebecca Ferguson (Eds.). ACM, New York, NY, USA, 252- 254.   [11] US Department of Education (2012) Enhancing Teaching  and Learning through Educational Data Mining and Learning  Analytics: An Issue Brief. Washington, D.C.   [12] Worsley, M. and Blikstein P. (2011). What's an Expert  Using learning analytics to identify emergent markers of  expertise through automated speech, sentiment and sketch  analysis. In Proceedings for the 4th Annual Conference on  Educational Data Mining. Eindhoven, Netherlands.   [13] Worsley, M. (2012). Multimodal Learning Analytics:  Enabling the Future of Learning Through Multimodal Data  Analysis and Interfaces. 2012 IEEE International Conference  on Multimodal Interfaces (ICMI).  Santa Monica, California,  USA   [14] Blikstein, P. (2013). Multimodal Learning Analytics: a  research agenda. In Proceedings of the 3rd International  Learning Analytics Knowledge Conference (LAK 2013),  Leuven, Belgium. ACM, New York, NY, USA, 252-254.   [15] Needleman S. & Wunsch C. (1970). A general method  applicable to the search for similarities in the amino acid  sequence of two proteins. Journal of Molecular Biology   [16] Anderson, J.R. & Schunn, C.D. (2000). Implications of the  ACT-R Learning Theory: No Magic Bullets in R. Glaser  (Ed.), Advances in instructional psychology (Vol. 5).  Mahwah, NJ.   [17] Hoppe, K.D. (1988). Hemispheric specialization and  creativity. The Psychiatric Clinics of North America 11, 3,  303-315.      101      "}
{"index":{"_id":"14"}}
{"datatype":"inproceedings","key":"Blikstein:2013:MLA:2460296.2460316","author":"Blikstein, Paulo","title":"Multimodal Learning Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"102--106","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460316","doi":"10.1145/2460296.2460316","acmid":"2460316","publisher":"ACM","address":"New York, NY, USA","keywords":"assessment, constructionism, constructivism, learning analytics, multimodal interaction","abstract":"New high-frequency data collection technologies and machine learning analysis techniques could offer new insights into learning, especially in tasks in which students have ample space to generate unique, personalized artifacts, such as a computer program, a robot, or a solution to an engineering challenge. To date most of the work on learning analytics and educational data mining has focused on online courses or cognitive tutors, in which the tasks are more structured and the entirety of interaction happens in front of a computer. In this paper, I argue that multimodal learning analytics could offer new insights into students' learning trajectories, and present several examples of this work and its educational application.","pdf":"Multimodal Learning Analytics  Paulo Blikstein   Stanford University Graduate School of Education and (by courtesy) Computer Science  520 Galvez Mall, CERAS 232 Stanford, CA  94305  USA   paulob@stanford.edu       ABSTRACT  New high-frequency data collection technologies and machine  learning analysis techniques could offer new insights into  learning, especially in tasks in which students have ample space  to generate unique, personalized artifacts, such as a computer  program, a robot, or a solution to an engineering challenge. To  date most of the work on learning analytics and educational data  mining has focused on online courses or cognitive tutors, in which  the tasks are more structured and the entirety of interaction  happens in front of a computer. In this paper, I argue that  multimodal learning analytics could offer new insights into  students learning trajectories, and present several examples of  this work and its educational application.   Categories and Subject Descriptors  D.3.3 [Programming Languages]   General Terms  Algorithms, Measurement, Human Factors.   Keywords  learning analytics, multimodal interaction, constructivism,  constructionism, assessment.   1. INTRODUCTION  New high-frequency data collection technologies and machine  learning analysis techniques could offer new insights into learning  in tasks in which students have ample space to generate unique,  personalized artifacts, such as a computer program, a robot, a  movie, an animation, or a solution to an engineering challenge. To  date most of the work on learning analytics and educational data  mining has focused on online courses or cognitive tutors, in which  the tasks are more structured and scripted, and the entirety of  interaction happens in front of a computer. In this paper, I argue  that multimodal data collection and analysis techniques  (multimodal learning analytics) could bring novel methods to  understand what happens when students can generate unique  solution paths to problems, interact with peers, and act in both the  digital and the physical worlds.   Assessment and feedback is particularly difficult within those  tasks, and has hampered many attempts to make them more  prevalent in schools. Therefore, these automated approaches   would be particularly useful in a time when the need for scalable  project-based, interest-driven learning and student-centered  pedagogies is growing considerably [e.g., 5]. Both K-12 and  engineering education [10, 11], within a transformed societal and  economic environment, now demand higher level, complex  problem-solving rather than performance in routine cognitive  tasks [15]. These approaches have been advocated for decades [9,  12, 16, 18] but failed to become scalable and prevalent, and came  under attack during the last decade [e.g., 13, 14]. Automated,  fine-grained data collection and analysis could help resolve this  tension in two ways. First, they could give researchers tools to  examine student-centered learning in unprecedented scale and  detail. Second, these techniques could improve the scalability of  these pedagogies since they make both assessment and formative  feedback, which are more complex and laborious in such  environments, feasible. They might not only reveal students  trajectories throughout a learning activity, but they also would  help researchers design better scaffolds.   At the same time, in the well-established field of multimodal  interaction, new data collection and sensing technologies are  making it possible to capture massive amounts of data in all fields  of human activity. These techniques include logs of computer  activities, wearable cameras, wearable sensors, biosensors (e.g.,  skin conductivity, heartbeat, and EEG), gesture sensing, infrared  imaging, and eye tracking. Such techniques are enabling  researchers to have an unprecedented insight into the minute-by- minute development of several activities, especially when they  involve multiple dimensions of interaction and social interaction.  However, these techniques are still not popular in the field of  learning analytics. In this paper, I propose that multimodal  learning analytics could coalesce these multiple techniques in  order to evaluate complex cognitive abilities, especially in  environments where the process or the outcome are unscripted.   In addition to their unimodality, traditional assessment  privileges product over process, and are divorced from actual  learning activities. In computer science courses, for example, it is  common to have students write pseudo code on a paper exam for  assessing their programming proficiency. The proposed work  looks to advance the fields capability to understand and utilize  forms of assessment that are more closely tied to the actual  practice of the respective disciplines. I want to study patterns in  how students of different ages and expertise levels complete tasks  such as programming, building a robot, designing a device, or  conducting a scientific investigation. This work is informed by  many of the current lines of research within educational data  mining and learning analytics. For example, Rus et al. [20] makes  extensive use of text analytics within a computer-based  application for science learning, using expert-generated answers  as a baseline. Beck and Sison [6] have demonstrated a method to  assess reading proficiency combining speech recognition and  probabilistic monitoring. DMello et al. [8] designed an  application that could use spoken dialogue to recognize the states      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK 13 April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.     102    of boredom, frustration, flow, and confusion. Other researchers  [1, 3] have been using machine learning in many contexts to  measure students learning and affect, in particular, focusing on  their trajectory within a learning environment [2]. The goal of this  paper is to be a proof of concept of novel assessment techniques  along several modes. I will describe examples of work in  multimodal learning analytics, in three different areas: analysis of  students learning to program, learning to build mechanical  structures, and inventing machines.   2. STUDENTS PROGRAMMING  2.1 Programming modes  In this first example, I focus on students learning to program a  computer using the NetLogo language. Hundreds of snapshots for  each student were captured and analyzed. I will briefly describe  the results of the study [4] and some prototypical trajectories, and  discuss how they relate to students programming experience.  Nine students in a sophomore-level engineering class had a three- week programming assignment. Students had different levels of  programming expertise at the beginning of the activity. The task  was to write a computer program to model a scientific  phenomenon in materials science. Every time student ran, saved,  or compiled their code, a snapshot of their code was stored. The  analysis consisted in counting the amount and pattern of changes  (additions/deletions) in the code, by calculating the number of  characters and lines of code changed during the three-week  assignment, as well as the frequency of compilation. At every  inflection point in the character-count curve, I visually examined  the data to examine the snapshots before and after the point and  inspect the types of changes that were being made to the code.  The data analyzed suggested three coding profiles:    Copy and pasters: characterized by a step-shaped  growth curve, alternating plateaus with few code changes  (looking for code / adapting the code), and rapid growth in  code size (pasting the external code)  mostly observed in  novice coders.     Self-sufficients: characterized by a linear curve of  steady increase in code size and almost no use of external  code  mostly observed in more expert programmers.   I conducted a second analysis on the frequency of code  compilation. Instead of expert/novice differences, the data showed  that compilation frequency could be used as a proxy for how  stuck students were in the assignment. Despite the small sample  size of this study, the analysis of open-ended programming  projects enabled the identification of patterns with real-world  significance and important implications for design. Each coding  strategy and profile might demand different support strategies:  advanced students (self-sufficients) might require detailed and  easy-to-find language documentation, whereas copy and pasters  need more working examples with transportable code. In fact, it  could be that expert programmers find it enjoyable to figure the  solutions out themselves, and would dislike to be helped when  problem solving. Novices might welcome some help, since they  exhibited a much more active help-seeking behavior.   2.2 Programming styles  In order to explore these questions further, we extended the data  collection to a much larger population. The second example is  about a fully automated system to capture snapshots of students  code during large-scale programming assignments, and the use of   several computational techniques to understand their progression  and learning throughout an introductory undergraduate course in  computer science. Again, seemingly erratic and highly personal  trajectories of  students generating a computer program in fact  contain identifiable patterns, states, and transition probabilities,  and can be successfully utilized to both understand the  programming process, predict future performance, and potentially  provide real-time assessment to instructors (about their  effectiveness) and students (about their learning). Additionally,  when this data is combined with student specific help seeking  logs, we can construct a complete picture of the learner in terms  of achievement, motivation and previous experience.  We researched the creation of computer programs using 10,000  code snapshots from 74 students enrolled in an introductory  undergraduate programming course, using two methods for the  discovery of patterns in the data. The first method was a greatly  enhanced version of the pattern detection used in the previous  study, using machine-learning techniques such as cluster analysis  and dynamic time warping. All code updates were characterized  based on their size and type, by identifying the number of lines  and characters added, removed and modified between successive  snapshots. This data was then used to construct a sequence of  multidimensional vectors. This process was repeated for each  assignment that the student completed. Using this sequence of  vectors, the progress of each student was compared over the  course of the class, by comparing their sequence of updates on a  given assignment to their sequence of updates on the previous  assignments. Students were then clustered based on the similarity  of their update sequences. Six clusters emerged from the data.   Figure 1 shows the different programming processes that students  follow, and it resulted in different outcomes on the midterm and  final examination. Of particular interest is the contrast between  cluster five and its peers. Figure 2 goes deeper into the differences  in examination grades, as students in certain clusters initiated help  requests at very different frequencies. Moreover, it is possible to  observe how help seeking changes over time for different groups.      Figure 1  Avg. Midterm and Examination Score by Cluster     Figure 2  Average Help Requests by Cluster and by Month   103    In further analysis, instead of simply clustering categories of code  updates, the code snapshots were mapped into a state machine,  using Hidden Markov techniques. The results of this work  confirmed that indeed it was possible to cluster students in terms  of robust behaviors that are dependent on expertise, and that those  behaviors were correlated with their grades in the assignments  and exams [19]. We will further discuss the significance of this  data collection model in the conclusions.   3. TEXT MINING  3.1 Construction competence  A second important area for MMLA application is text mining. A  variety of features can be extracted from text or transcripts, some  of which I describe below. Prosodic analysis uses the pitch,  intensity and duration of speech to infer intentionality and  emotion. Linguistic analysis looks at several features such as  pauses, filled pauses, and restarts, and can infer elements such as  certainty. Sentiment analysis [17], using databases of terms such  as the Linguistic Inquiry and Word Count (LIWC) and the  Harvard Inquirer, infers sentiment from words or groups of words  found in text. Content word analysis determines the knowledge  contained in the text by using web-mined lexicons from  chemistry, mathematics, computer science, material science and  general science. Finally, dependency parsing and n-gram analysis  can be used to inspect latent structures or meaning in the text by  looking at group of words, sentences, and their relationship [7].  Many of these techniques were used in a study about engineering  expertise [21]. Data for this study came from interviews of  approximately 30 minutes with 15 students from a tier-1 research  university (8 female, 7 male). Participants were asked to draw and  think aloud about how to build various electronic and mechanical  devices. Graduate and undergraduate students transcribed student  speech. Prior to the interviews, the subjects were labeled as being  experts, intermediates or novices in engineering and robotics,  based on their major and previous experience. The data was  analyzed using expectation maximization (EM), with an intra- cluster Euclidean distance objective function. Before running EM,  the data was normalized and t-tests were performed to check  statistical significance. The goal of the study was to try to predict  the expertise level of the participants based on a combination of  the extracted features.  Of particular interest to this study is the presence of several  features that accurately predicted expertise based on certainty.  Though not presented here at length, analysis of this data involved  extracting n-grams from each transcript and looking for patterns  across the different classes of participants. Not surprisingly, n- grams that indicated uncertainty, e.g. dont know and well,  you know, were more common among novices than among non- novices. These initial results confirm the work of Beck [6], which  indicates that increasing expertise tends to increase student self- confidence. These results were further corroborated through  sentiment analysis: certainty terms was much more common  among experts, while understatements terms were more  frequently employed by novices.  Additionally, novices exhibited a much higher frequency of  disfluencies (average of 1.06 per normalized time interval) than  experts (0.5), and experts had much longer and more frequent  pauses. Taken together with the field observations, this suggests  that novices were uncomfortable with moments of uncertainty,  and thus filled the silence with disfluencies. However, experts  would just stay in silence thinking about the problem and   articulating their next utterance. This finding is yet another  example of how multimodal data can potentially categorize  students based on seemingly small differences in discourse and  behavior. Changes in these elements could indicate learning or  more familiarity with the field being explored.  A second study using text mining was designed to investigate  students identity as engineers and scientists, and their level of  identification with these professions [22]. Students participated in  a weeklong robotics workshop and, subsequently, in one-on-one  interviews with a member of the teaching staff. These semi- clinical interviews consisted in think-alouds of students designing  different inventions and devices while drawing them on paper.  For example, one of the questions was to design a piggy bank that  automatically counts money as it is dropped in. All interviews  were transcribed and analyzed using standard text mining  techniques such as n-grams analysis. The interest here was to  examine if students who did well in the robotics workshop would  employ different vocabulary and linguistic structures. Indeed, the  data showed that when students were describing a project they  were proud of (as measured by the facilitators field notes), they  would use I or my, in sentences such as, my original design  was When the same student was describing a project that they  were not proud of, they would instead use it, in sentences such  as, itd be programmed to turn on in place of I programmed  it to turn on.  Counting word frequencies, we were able to determine that, for  the group as a whole, students identified as high achievers in the  workshop used I more than seven times more frequently than  low-achievers. Again, here we can observe how even simple  automated n-gram counting within transcripts can reveal  meaningful elements of students affect.   4. OBJECT/BODY TRACKING  4.1 Tracking actions using video  Another example of the use of multimodal techniques is video and  gesture tracking. As an example, I will describe a study on  students ability in building simple structures [23]. Data was  drawn from thirteen participants, each given everyday materials  and asked to build a tower that could hold a mass of  approximately 3 lbs. Participants were also challenged to make  the structure as tall as possible. The task was designed to see how  well students are able to take their intuitions about mechanical  engineering and physics and translate them into a stable structure.  Students were given four drinking straws, five wooden popsicle  sticks, a roll of masking tape and a paper plate; and were told that  they had approximately ten minutes to complete the activity.  However, they were permitted to work for as long as they wanted  with participation times ranging from six to 52 minutes. Of the  twelve students, three were mechanical engineering graduate  students and nine were high school students. Prior to the study  students were classified based on their perceived level of  expertise in the domain of engineering design. Three participants  were labeled as experts, two as being of high expertise, five were  classified as medium, and three as low.   A coding scheme was developed consisting of eleven object  manipulation codes, identified through open coding of a sample of  the videos, and agreed upon by a team of research assistants. This  set was later collapsed into five codes, based on further coding  sessions and discussions amongst coders (Table 1). The codes are  entirely based on participant object manipulation, or lack thereof,   104    and are not an attempt to interpret a students intentions  explicitly.   Table 1. Final codes for object manipulation   Class Codes   BUILD Building and Breaking   PLAN  Prototyping mechanism, Thinking with or without  an object, Single object examination, Organizing  and Selecting materials   TEST Testing a mechanism and system testing   ADJUST Adjusting   UNDO Undoing        Figure 3. The capture environment used to record the audio,   video and gesture data streams   Video captured the movement of objects as students progressed  through the task, while gesture data, which consisted of twelve  upper-body parts, recorded the students actions. To analyze the  data, many different approaches were attempted (Figure 4). First,  a simple count of the number and duration of each of the codes  (single class assignment), and seeing how well that predicted  expertise based on the previously assigned expertise labels. Next,  a more sophisticated cluster analysis was used, but it did not  consider the sequence of codes/actions (just their quantity and  duration, non-process oriented classification). Finally, the  temporal sequence of actions was added, thus taking into  consideration the sequentiality of building the object. In contrast  to the non-process-oriented approach, the final object  manipulation analysis algorithm was able to significantly  outperform both random assignment and majority class  assignment, all while preserving the process-oriented nature of the  task. Figure 4 highlights the accuracy attained through the object  manipulation analysis, compared to the other techniques.  Similarly, the confusion matrix is in Table 2.   Table 2. Confusion matrix of the object manipulation data,  using the process-based algorithm, in terms of expertise levels    Low Medium High Expert  Low 3 0 0 0  Medium 3 1 1 0  High 0 0 2 0  Expert 0 0 0 3     Figure 4. Comparison between different approaches to data   classification     The confusion matrix shows that the algorithm worked best at  uniquely clustering expert behavior, which it did at an accuracy of  100%. It also attained 100% accuracy for individuals of low  expertise. However, for individuals of intermediate levels of  expertise, the algorithm was less accurate. However, considering  that the metric may be somewhat noisy for participants of  medium expertise, it was still able to do a reasonably accurate  job.    4.2 Tracking gestures using sensors  The gesture data analysis, while similar in spirit to the object  manipulation analysis, involves markedly less complexity. This is  preliminary work that I mention here only as an illustration of the  possibilities of MMLA. Using the same setting as described in the  previous study (4.1), gesture data was collected with a Kinect  sensor. The hypothesis was it would be possible to cluster experts  and novices in terms of the well-documented difference between  the extents of two-handed coordinated movement among  individuals of differing expertise. Here we consider two-handed  coordinated movement to be when a participant uses both of their  hands within a given action. Figure 5 shows the cumulative  displacements for the right and left hand and depicts this  difference. The experts hands (right) typically move coordinated  with one another, whereas the novice's hand movements are  markedly asynchronous (left). These preliminary results deserve  deeper examination, but are examples of low-cost techniques that  could be useful when coordinated with other sources of  multimodal data.        Figure 5. Novices hands are used asynchronously (left), while   experts hands move at the same time (right).   5. CONCLUSIONS  In this paper, I presented a series of proof-of-concept studies for  what I termed multimodal learning analytics: a set of techniques  that can be used to collect multiple sources of data in high- frequency (video, logs, audio, gestures, biosensors), synchronize  and code the data, and examine learning in realistic, ecologically  valid, social, mixed-media learning environments.   Overhead camera for object tracking   Skeletal overlay of gesture captureBuilding materials   105    The incorporation of multimodal techniques, which are  extensively used in the multimodal interaction community, would  allow researchers to examine unscripted, constructionist, complex  tasks in more holistic ways. In particular, as a proof-of-concept, I  focused on clustering participants in terms of multiple features of  their actions and matching those clusters to the previously known  expertise levels. First, I showed how the analysis of hundreds of  programming snapshots could reveal patterns in students  programming such as tinkerers vs. planners. I then showed  how even simple word counting and n-gram analysis techniques  can reveal learners affect and identity towards engineering, and  how behavioral traces such as confidence, as well as disfluencies  or pauses, can predict a subjects level of expertise. Using video  analysis, I showed how the clustering of students actions during a  construction task, paired with human labeling, could also be a  predictor of learning. Finally, I explored hand coordination as a  possibly meaningful metric.   The goal of the paper is to be a proof of concept of novel  assessment techniques along several modes. In all studies, I was  interested in the definition of expertise and in the categorization  of learners based purely on their behavior, actions, or  utterancesnot on the assumed level of their knowledge or their  performance on extraneous tests. Many of these studies are  preliminary; further studies should get deeper into the nuances of  expertise, which was oversimplified for the purposes of this  paper. Even with these simplifications, I was able to show that  important aspects of learning hide in the details, and both overt  and tacit elements could be indicative to determine students  knowledge. The implication of this work is that, ultimately,  multimodal learning analytics could be used to devise naturalistic  assessments which would be, at the same time, social,  ecologically valid, more inclusive as to the types of knowledge  they measure, and enabling real-time evaluation in realistic tasks,  either off or online.   6. REFERENCES  [1] Amershi, S., & Conati, C. 2009. Combining Unsupervised   and Supervised Classification to Build User Models for  Exploratory Learning Environments. Journal of Educational  Data Mining, 1(1), 18-71.   [2] Baker, R. & Yacef, K. 2009. The State of Educational Data  Mining in 2009: A Review and Future Visions. Journal of  Educational Data Mining, 1(1).   [3] Baker, R. S., Corbett, A. T., Koedinger, K. R., & Wagner, A.  Z. 2004. Off-task behavior in the cognitive tutor classroom:  when students game the system. Proceedings of the  SIGCHI conference on Human factors in computing systems.    [4] Blikstein, P. 2011. Using learning analytics to assess  students' behavior in open-ended programming tasks, in  Proceedings of the 1st International Conference on Learning  Analytics and Knowledge. ACM: Banff, Canada. 110-116.   [5] Barron, B., & Darling-Hammond, L. 2010. Prospects and  challenges for inquiry-based approaches to learning: OECD.   [6] Beck, J. E., & Sison, J. 2006. Using Knowledge Tracing in a  Noisy Environment to Measure Student Reading  Proficiencies. International Journal of Artificial Intelligence  in Education, 16(2), 129-143.    [7] Chris Manning and Hinrich Schtze, Foundations of  Statistical Natural Language Processing, MIT Press.   [8] DMello, S., Craig, S., Witherspoon, A., McDaniel, B., &  Graesser, A. 2008. Automatic detection of learners affect  from conversational cues. User Modeling and User-Adapted  Interaction, 18(1), 45-80.   [9] Dewey, J. 1902. The school and society. U of Chicago Press.  [10] Dutson, A. J., Todd, R. H., Magleby, S. P., & Sorensen, C.   D. 1997. A Review of Literature on Teaching Engineering  Design through Project-Oriented Capstone Courses. J of  Engineering Education, 86(1), 17-28.    [11] Dym, C. L. 1999. Learning Engineering: Design, Languages,  and Experiences. J of Engineering Education, 145-148.    [12] Freire, P. 1970. Pedagogia do Oprimido. Paz e Terra, Rio de  Janeiro.   [13] Kirschner, P. A., Sweller, J., & Clark, R. E. 2006. Why  minimal guidance during instruction does not work: An  analysis of the failure of constructivist, discovery, problem- based, experiential, and inquiry-based teaching. Educational  Psychologist, 41(2), 75-86.    [14] Klahr, D., & Nigam, M. 2004. The equivalence of learning  paths in early science instruction. Psychological Science,  15(10), 661.   [15] Levy, F., & Murnane, R. J. 2004. The new division of labor:  How computers are creating the next job market: Princeton  University Press.   [16] Montessori, M. 1965. Spontaneous activity in education.  Schocken Books, New York.   [17] Pang, Bo and Lillian Lee. 2008. Opinion mining and  sentiment analysis. Foundations and Trends in Information  Retrieval 2(1): 1135.   [18] Papert, S. 1980. Mindstorms: children, computers, and  powerful ideas. Basic Books, New York.   [19] Piech, C., et al. Modeling how students learn to program.  2012. In Proceedings of the 43rd ACM Symposium on  Computer Science Education (SIGCSE '12). ACM.   [20] Rus, V., Lintean, M. and Azevedo, R. 2009. Automatic  Detection of Student Mental Models During Prior  Knowledge Activation in MetaTutor. In Proc. of the 2nd Int.  Conference on Educational Data Mining. 161-170.   [21] Worsley, M. and Blikstein P. 2011. Whats an Expert Using  learning analytics to identify emergent markers of expertise  through automated speech, sentiment and sketch analysis. In  Proceedings for the 4th Annual Conference on Educational  Data Mining, Eindhoven, The Netherlands.   [22] Worsley, M. and Blikstein P. 2012. A Framework for  Characterizing Student Changes in Student Identity During  Constructionist Learning Activities. Proceedings of  Constructionism 2012, Athens, Greece.   [23] Worsley, M. and Blikstein, P. 2013. Toward the  Development of Multimodal Action Based Assessment. In  Proceedings for the 2013 Learning Analytics and Knowledge  (LAK 2013) Conference, Leuven, Belgium.        106      "}
{"index":{"_id":"15"}}
{"datatype":"inproceedings","key":"Schneider:2013:TCS:2460296.2460317","author":"Schneider, Bertrand and Abu-El-Haija, Sami and Reesman, Jim and Pea, Roy","title":"Toward Collaboration Sensing: Applying Network Analysis Techniques to Collaborative Eye-tracking Data","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"107--111","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460317","doi":"10.1145/2460296.2460317","acmid":"2460317","publisher":"ACM","address":"New York, NY, USA","keywords":"awareness tools, computer-supported collaborative learning, eye-tracking, network analysis","abstract":"In this paper we describe preliminary applications of network analysis techniques to eye-tracking data. In a previous study, the first author conducted a collaborative learning experiment in which subjects had access (or not) to a gaze-awareness tool: their task was to learn from neuroscience diagrams in a remote collaboration. In the treatment group, they could see the gaze of their partner displayed on the screen in real-time. In the control group, they could not. Dyads in the treatment group achieved a higher quality of collaboration and a higher learning gain. In this paper, we describe how network analysis techniques can further illuminate these results, and contribute to the development of 'collaboration sensing'. More specifically, we describe two contributions: first, one can use networks to visualize and explore eye-tracking data. Second, network metrics can be computed to interpret the properties of the graph. We conclude with comments on implementing this approach for formal learning environments.","pdf":" 107   Toward Collaboration Sensing: Applying Network  Analysis Techniques to Collaborative Eye-tracking Data   Bertrand Schneider1,2, Sami Abu-El-Haija2, Jim Reesman2, Roy Pea1  Stanford University   Graduate School of Education1, Computer Science department2  schneibe@stanford.edu, haija@stanford.edu, jreesman@cs.stanford.edu, roypea@stanford.edu      ABSTRACT  In this paper we describe preliminary applications of network  analysis techniques to eye-tracking data. In a previous study, the  first author conducted a collaborative learning experiment in  which subjects had access (or not) to a gaze-awareness tool: their  task was to learn from neuroscience diagrams in a remote  collaboration. In the treatment group, they could see the gaze of  their partner displayed on the screen in real-time. In the control  group, they could not. Dyads in the treatment group achieved a  higher quality of collaboration and a higher learning gain. In this  paper, we describe how network analysis techniques can further  illuminate these results, and contribute to the development of  'collaboration sensing'. More specifically, we describe two  contributions: first, one can use networks to visualize and explore  eye-tracking data. Second, network metrics can be computed to  interpret the properties of the graph. We conclude with comments  on implementing this approach for formal learning environments.    Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Collaborative Learning  General Terms  Algorithms, Experimentation, Human Factors.  Keywords  Network Analysis, Eye-tracking, Computer-Supported  Collaborative Learning, Awareness Tools.    1. INTRODUCTION  Nowadays, massive datasets are becoming available for a wide  range of applications. Education is no exception to this  phenomenon: cheap sensors can now detect every movement and  utterance of a student. On the web, Massive Open Online Courses  (MOOCs) collect every click of users taking classes online. This  information can provide crucial insights into how learning  processes unfold in situ or in a remote situation. However,  researchers often lack the tools to make sense of those giant  datasets; our contribution is to propose additional ways to explore  massive log files, such as eye-tracking data.   2. RELATED LITERATURE  Our work lies in the intersection between basic network analysis  and studies of the effects of gaze awareness on collaborative  learning. While there is literature in both of these areas, there   appears to be none squarely in the intersection of those two  domains; as such, we believe the proposed work is novel and  relevant to generating insights. We discuss the literature from  related areas in order to justify our proposed work. In this section  we look briefly at eye-tracking studies on collaborative learning,  basic network analysis techniques, and at examples employing  simple network analysis of eye tracking data.   Previous studies in CSCL (Computer-Supported Collaborative  Learning) have used eye-trackers to study joint attention in  collaborative learning situations. For instance, Richardson & Dale  [8] found that the number of times gazes are aligned between  individual speakerlistener pairs is correlated with the listeners  accuracy on comprehension questions. Jermann, Nuessli, Mullins  and Dillenbourg [4] used synchronized eye-trackers to assess how  programmers collaboratively worked on a segment of code; they  contrasted a 'good' and a 'bad' dyad, and their results suggest that a  productive collaboration is associated with high joint visual  recurrence. In another study, Liu [6] used machine-learning  techniques to analyze users gaze patterns, and was able to predict  the level of expertise of each subject as fast as one minute into the  collaboration (96% of accuracy). Finally, Cherubini, Nuessli and  Dillenbourg [2] designed an algorithm which detected  misunderstanding in a remote collaboration by using the distance  between the gaze of the emitter and the receiver. They found that  if there is more dispersion, the likelihood of misunderstandings is  increased. In all those studies, however, no data-mining  techniques were used to uncover more complicated patterns. We  thus propose to build large networks based on eye-tracking data.  Our work deals mainly with basic graph property determination,  since it is an exploratory attempt at building networks on top of  gaze movements. This includes but is not limited to network size,  degree distribution, clustering coefficient, and so forth [5]. By  analyzing these basic attributes of the networks of eye tracking  data, we lay the foundation for future research, which can control  for various network properties in order to determine their effect on  study outcomes.   We are unaware of existing studies that have tried to apply  network analysis tools on eye-tracking data We believe that  analyzing networks based on subjects gaze behavior can shed a  new light on collaborative learning processes. In the next section  we describe our dataset and describe our work.   3. THE CURRENT STUDY  The first author previously conducted an experiment [9] where  dyads of students remotely worked on a set of contrasting cases  [1]. The students worked in pairs, each in a different room, both  looking at the same diagram on their computer screen. Dyads  were able to communicate through voice. Their goal was to learn  how the human brain processes visual information (Fig. 2). In one  condition, members of the dyads saw the gaze of their partner on  the screen; in a control group, they did not have access to this  information. This intervention helped students achieve a higher     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.        108   quality of collaboration and a higher learning gain compared to  the control group. To our knowledge, this is the first study that  was able to highlight the learning benefits of this kind of gaze- awareness tool.    Participants in the visible-gaze group outperformed the dyads in  the no-gaze condition for the total learning gain: F(1,40) = 7.81,  p < 0.01. For the sub-dimensions, they also scored higher on the  transfer questions F(1,40) = 4.47, p < 0.05. With a larger sample,  the difference is likely to be also significant for the terminology  questions F(1,40) = 3.59, p = 0.065 and for the conceptual  questions F(1,40) = 2.11, p = 0.154, since the effect sizes are  between medium and large (Cohens d are 0.62 and 0.5,  respectively). Additionally, in our previous submission [9] we  manually categorized each member of the dyad as leader and  follower. The leader was chosen to be the member that starts  more conversations, and leads the dyad's problem-solving  processes during the experiment. After the collaboration on the  diagram, all participants then individually completed a short test  that examined their learning gain about the topic. Their  performance (number of correct answers) on this test was taken as  their total learning score. Interestingly (see Fig. 1), we found an  interaction effect between those two factors (experimental  conditions and individuals status) on the total learning score:  F(1,38) = 5.29, p < 0.05. Followers learned significantly more  when they could see the gaze of the leader on the screen. We also  rated the quality of collaboration of each dyad using Meier, Spada  and Rummels [7] rating scheme. Our results show that Dyads in  the treatment group (visible gaze) had a higher quality of  collaboration: F(1,10) = 24.68, p < 0.001 (mean for the treatment  group = 7.18, SD = 3.75; mean for the control group = 0.6, SD =  6.2). More specifically, those dyads were better at sustaining  mutual understanding (F(1,10) = 21.78, p < 0.01), pooling  information (F(1,10) = 15.94, p < 0.01), reaching consensus  (F(1,10) = 31.25, p < 0.001) and managing time (F(1,10) = 27.50,  p < 0.001).    The eye-tracking data of this study, however, has been largely  unexploited so far. In the current work, we use network analysis  techniques to describe how subjects in the visible-gaze  condition outperformed subjects in the no-gaze condition.      Figure 1: results of the learning test in the previous study. We  found that students using the gaze-awareness tool  outperformed the students who did not have access to it on the  learning test. Follower in particular benefited from this  intervention.     4. CONSTRUCTING GRAPHS WITH EYE- TRACKING DATA   4.1 Goals  Our goal is twofold: first, we want to provide an alternative way  to explore eye-tracking data. This approach involves data  visualization techniques, such as force-directed graphs. We  believe that uses of visualization techniques for representing  massive datasets can provide interesting insights to researchers.  Second, we want to compute network measures based on those  graphs; our goal is to examine whether some metrics are  statistically different across our two experimental groups.  Additionally, those metrics can provide interesting proxies for  estimating dyads quality of collaboration.   4.2 Deriving Nodes and Edges from Gaze  Movements  Nodes can be created in various ways from this dataset. One  choice would be to convert every pixel on the screen to a node.  For obvious performance reasons, and to make computations  more meaningful, nodes should contain an area larger than a pixel.  A better choice would be dividing the screen into a uniform grid  of nodes. Other methods involve building/detecting nodes  empirically. For example, it is possible to sum the total gaze time  for each pixel (by all dyads), and then cluster those pixels into  nodes by the Mean-Shift algorithm. To limit the scope of this  paper we limit ourselves to the first approach. For all the  subsequent analyses, we divided the screen into 44 different areas  based on the configuration of the diagrams (Fig. 2). Students had  to analyze 5 contrasting cases; the answer to the top left and top  right cases were given. Possible answers were given on the right.  Thus, we have 30 areas that cover the diagrams of the human  brain and 8 areas that cover the answer keys.    Edges can represent many aspects of the subjects behavior. For  example, one can do path analysis and convert a dyads gaze into  a path on the nodes. Here, it is possible to analyze the steps in  the gaze processes that dyads went through (e.g., by counting  common sub-paths), how fast they switch between nodes, and the  average number of times that they visit a node.    In one analysis, we constructed undirected weighted graphs,  where a time-sliding window (e.g., of 2 seconds width) detects  when a given dyad gazes at two screen areas within that time- frame. Then one can increment the edge weight by a certain  quantity. Unlike the path analysis, this undirected graph has no  notion of chronology because the temporal relationship between  nodes is expressed as edge weights.     Figure 2: To create the nodes, we choose to divide the screen  in 44 different areas based on its visual configuration.   0   0.1   0.2   0.3   0.4   0.5   0.6   Le ar ni ng (G ai n( (%  )(  Experimental(Groups(  visible0gaze follower   visible0gaze leader   nogaze follower   nogaze leader      109   For our first attempt, we focused on the simplest solution: each  time a participant gazed between two regions, we created (or  incremented the weight of) an edge. Future work should explore  alternative ways of creating both nodes and edges based on eye- tracking data. However this simple method generates quite  interesting results, described in the following section.   4.3 At the Individual Level  In this section we describe graphs created with individuals as the  unit of analysis: each network is built by using the eye-tracking  data of one subject (Fig. 3). The label on each node corresponds to  a screen region as defined in Fig 2. The size of a node shows the  number of fixations on this area. Node colors correspond to screen  section. Blue nodes correspond to a diagram region (major/left  side of the screen). Orange nodes correspond to answer keys  (right column of the screen). An edge represents a saccade  between two regions. The width of an edge shows the number of  times a subject compared those two regions. Those graphs were  implemented with a force-directed layout and can be directly  manipulated on a web page1.  This approach already shows interesting results: we can observe  that subject 1 (on the top) spent a lot of time understanding the  diagram on the top right corner of the screen; however (s)he  mostly neglected the answers on the right. Subject 2 (on the  bottom), had a completely different strategy: (s)he intensively  compared answers and diagrams. Thus, with this visualization one  can quickly identify patterns and build hypotheses to investigate.    One limitation of this approach is known as the hair ball  problem in data visualization: since the graph is quite dense, every  node is connected to a lot of other nodes and thus makes  interpretations difficult. This problem is inherent to eye-tracking  dataset: since an edge is a saccade, each node is going to be  connected to at least two other nodes. Moreover, due to the  limited amount of potential nodes, our graphs are bound to be  highly connected and highly clustered. Another limitation is the  fact that this visualization totally ignores the collaborative aspect  of the study [9]. In previous results, dyadic synchronization was  found to be a critical factor for a positive learning experience.    In the next section, we describe how we circumvented those  issues. We sought to create smaller and more informative graphs  by focusing on dyads instead of individuals. Those graphs provide  a different window into our dataset.   4.4 At the Dyad Level (Joint Attention)  Our next attempt involved building one graph for each dyad.  Here, we want to capture the moments in which dyad members  were cooperating. The nodes correspond to the screen areas, as  previously defined. At the dyad level, however, a node will only  appear in the dyad graph if both dyad members gazed at the  respective screen area within a 2 seconds time-window of each  other.   Small graphs with few nodes are characteristic of poor  collaboration, and large graphs with highly connected nodes show  a strong flow of communication across members of the dyad.  Figure 4 provides an example of this kind of contrast.                                                                           1 The visualizations described in this paper can be accessed via   stanford.edu/~schneibe/cgi-bin/d3/examples/force/force.php        Figure 3: Two graphs created based on individuals' data. Blue  means brain diagram, orange means answer key on the right  of the screen. Both graph suffer from the hair ball problem  since they contain a lot of edges.  The color scheme of the nodes is identical to the individual  graphs. However, the node size in the dyad graphs is proportional  to the number of times dyad members looked at the respective  screen area within a 2 second time window. The choice of 2  seconds is based on the work done by Richardson & Dale [8],  where they find that it takes a follower about 2 seconds to look at  the screen area that the leader is discussing. Edges are defined as  previously (i.e., number of saccades between two areas of the  screen).      110        Figure 4: Graph build on dyads' data. The size of each node  reflects the number of moments of joint attention members of  the group shared on one area of the screen. Graph on the top  is from a dyad in the no-gaze condition; graph on the  bottom is from a dyad in the visible-gaze condition.  Again, from a data visualization perspective, this approach  conveys key patterns in collaborative learning situations. The top  graph in Fig. 4 shows a dyad in the no-gaze condition; one can  immediately see that students did not share a common attentional  focus very often. Nodes are small and poorly connected. The  graph on the bottom represents a dyad in the visible-gaze  condition and is a strong contrast to the previous example: here  students are looking at common things much more frequently and  those moments of joint attention provide opportunities to compare  different diagrams or answers. Nodes are bigger and better  connected.   Based on this new dataset, we computed various network metrics.  We found that the average size of the nodes was significantly  bigger in the visible-gaze condition: F(1,12) = 7.19, p = 0.02 and  there were more edges: F(1,12) = 4.9, p = 0.047. As expected, the  average betweenness centrality (a measure of a  node's centrality in a network, computed by counting the number  of shortest paths from all vertices to all others that pass through   that node) was also higher for this group: F(1,12) = 6.44, p =  0.026. Those results indicate that we can potentially separate our  two experimental conditions solely based on network  characteristics.   More interestingly, several measures were significantly correlated  with the groups quality of collaboration (as defined by the rating  scheme described in the methods section [7]). The average size of  a node was correlated with the overall quality of collaboration  (r(16) = .52, p = 0.039), students orientation toward the task  (each participant actively engages in finding a good solution to  the problem): r(16) = .54, p < 0.001, and students reciprocal  interaction (partners treat each other with respect and encourage  one another to contribute their opinions and perspectives): r(16)  = 59, p < 0.001. The number of the nodes in the graph was  correlated with the sub-dimension Reaching Consensus  (Decisions for alternatives on the way to a final solution (i.e.,  parts of the diagnosis) stand at the end of a critical discussion in  which partners have collected and evaluated): r(16) = .65, p <  0.001 and the sub-dimension Information pooling (Partners try  to gather as many solution-relevant pieces of information as  possible): r(16) = .52, p = 0.002. Betweenness centrality was  correlated with all the sub-dimensions above - but also with the  sub-dimension Sustaining Mutual Understanding (Speakers  make their contributions understandable for their collaboration  partner, e.g., by avoiding or explaining technical terms from their  domain of expertise): r(16) = .37, p = 0.037.    5. DISCUSSION  Our preliminary results show the relevance of using network  analysis techniques for eye-tracking data. In particular, we found  this approach fruitful when applied to social eye-tracking data  (i.e., a collaborative task where the gaze of two or more  participants are recorded at the same time).   More specifically, we found that different aspects of collaborative  learning were associated with different network measures. The  average size of a graphs nodes appeared to be a good proxy for  students orientation toward the task and their level of reciprocity  toward their partner; the number of nodes can be used to estimate  to what extent dyads try to reach a consensus and pool  information to find a good solution to the problem at hand.  Finally, the betweenness centrality of a graph appears to be an  indicator of how well students try to sustain mutual understanding  between one another. Of course, more work is needed to replicate  those results. But overall, we find that network analysis  techniques can be used advantageously to further our  understanding of group collaboration processes.   In terms of future work, we are currently exploring several aspects  of the research described above. The most direct extension is to  apply machine-learning techniques to predict dyads quality of  collaboration using network metrics. Early results make us believe  that networks characteristics can be used as relatively good input  features for a Support Vector Machine (SVM) algorithm. Another  interesting line of work is to explore the way graphs evolve over  time: for instance, we would expect the betweenness centrality to  increase as the team develop a higher mutual understanding an  agreement over the time. Finally, more work is needed to interpret  the meaning of the correlations described above: for instance, it is  not entirely clear why betweenness centrality is strongly  associated with developing mutual understanding in the group. At      111   this stage it is probably necessary to conduct a more fined-grained  analysis of those results, for instance by focusing on one  particular group and defining key events associated with a higher  betweenness centrality (e.g. students jointly revisiting a particular  node, and making connections with hypotheses previously  developed during the activity).     Our work has limitations. First, we studied only one particular  kind of collaboration (i.e., remote collaboration). It is likely that in  situ interactions are different from online collaborative work.  Secondly, we only had 9 dyads in each experimental group; with  more subjects we would probably find more statistically  significant patterns. Thirdly, it is possible that the two  experimental conditions are interfering with our results:  collaboration traits may be exacerbated by our gaze-awareness  tool and thus not reflect natural patterns of social interaction.  Finally, there are other interesting network metrics that we did not  use for this preliminary analysis. Future work shoulde replicate  those results in other settings and look at more complex properties  of graphs.   6. CONCLUSION  This work provides two significant contributions. First, we  developed new visualizations to explore social eye-tracking data.  We believe that researchers can take advantage of this approach to  discover new patterns in existing datasets. Second, we showed  that simple network metrics can serve as proxies for evaluating  the quality of group collaboration. As eye-trackers become  cheaper and widely available, we can develop automatic measures  for assessing peoples collaboration. Such instrumentation would  enable researchers to spend less time coding videos and more time  exploring patterns in their data. In formal learning environments,  such measures could be computed in real time; teachers could  employ such metrics of 'collaboration sensing' to target specific  interventions while students are at work on a task. In informal  networked learning, collaboration sensor metrics could trigger  hints or provide other scaffolds for guiding collaborators to more  productive coordination of their attention and action. We also  envision the extension of such network analyses as these for eye- tracking during collaboration to other interaction data related to  interpersonal coordination and learning, such as gestures and  bodily orientation.   7. ACKNOWLEDGMENTS  We would like to thank the teaching staff of CS224W (Social  and Information Network Analysis taught by Assist. Prof. Jure   Leskovec at Stanford) for their help and support during this  project.    8. REFERENCES  [1] Bransford, J.D. and Schwartz, D.L. Rethinking transfer: A   simple proposal with multiple implications. Review of  research in education, (1999), 61100.   [2] Cherubini, M., Nssli, M.-A., and Dillenbourg, P. Deixis and  gaze in collaborative work at a distance (over a shared map):  a computational model to detect misunderstandings.  Proceedings of the 2008 symposium on Eye tracking  research & applications, ACM (2008), 173180.   [3] Erds, P. and Rnyi, A. On the Evolution of Random  Graphs. Publication Of The Mathematical Institute Of The  Hungarian Academy Of Sciences, (1960), 1761.   [4] Jermann, P., Mullins, D., Nssli, M.A., and Dillenbourg, P.  Collaborative Gaze Footprints: Correlates of Interaction  Quality. Proceedings of CSCL, (2011), 184191.   [5] Kleinberg, J. The small-world phenomenon: an algorithm  perspective. Proceedings of the thirty-second annual ACM  symposium on Theory of computing, ACM (2000), 163170.   [6] Liu, Y., Hsueh, P.-Y., Lai, J., Sangin, M., Nussli, M.-A., and  Dillenbourg, P. Who is the expert Analyzing gaze data to  predict expertise level in collaborative applications. IEEE  International Conference on Multimedia and Expo, 2009.  ICME 2009, (2009), 898 901.   [7] Meier, A., Spada, H., and Rummel, N. A rating scheme for  assessing the quality of computer-supported collaboration  processes. International Journal of Computer-Supported  Collaborative Learning 2, 1 (2007), 6386.   [8] Richardson, D.C. and Dale, R. Looking To Understand: The  Coupling Between Speakers and Listeners Eye Movements  and Its Relationship to Discourse Comprehension. Cognitive  Science 29, 6 (2005), 10451060.   [9] Schneider, B., & Pea, R. (submitted).  Using Eye-Tracking  Technology to Support Visual Coordination in Collaborative  Problem-Solving Groups. International Conference on  Computer-Supported Collaborative Learning, 2013.           "}
{"index":{"_id":"16"}}
{"datatype":"inproceedings","key":"Munoz-Merino:2013:IHL:2460296.2460318","author":"Mu~noz-Merino, Pedro J. and Valiente, Jos'e A. Ruip'erez and Kloos, Carlos Delgado","title":"Inferring Higher Level Learning Information from Low Level Data for the Khan Academy Platform","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"112--116","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460318","doi":"10.1145/2460296.2460318","acmid":"2460318","publisher":"ACM","address":"New York, NY, USA","keywords":"hints, learning analytics, visualization","abstract":"To process low level educational data in the form of user events and interactions and convert them into information about the learning process that is both meaningful and interesting presents a challenge. In this paper, we propose a set of high level learning parameters relating to total use, efficient use, activity time distribution, gamification habits, or exercise-making habits, and provide the measures to calculate them as a result of processing low level data. We apply these parameters and measures in a real physics course with more than 100 students using the Khan Academy platform at Universidad Carlos III de Madrid. We show how these parameters can be meaningful and useful for the learning process based on the results from this experience.","pdf":"Inferring Higher Level Learning Information from Low  Level Data for the Khan Academy Platform      Pedro J. Muoz-Merino, Jos A. Ruiprez Valiente, Carlos Delgado Kloos  Universidad Carlos III de Madrid   Avenida Universidad 30, 28911 Legans (Madrid) Spain   {pedmume, jruipere, cdk}@it.uc3m.es      ABSTRACT  To process low level educational data in the form of user events   and interactions and convert them into information about the   learning process that is both meaningful and interesting presents a   challenge. In this paper, we propose a set of high level learning   parameters relating to total use, efficient use, activity time   distribution, gamification habits, or exercise-making habits, and   provide the measures to calculate them as a result of processing   low level data. We apply these parameters and measures in a real   physics course with more than 100 students using the Khan   Academy platform at Universidad Carlos III de Madrid. We show   how these parameters can be meaningful and useful for the   learning process based on the results from this experience.   Categories and Subject Descriptors  K.3.1 [Computer Uses in Education]: Distance learning; H.1.2   [User/Machine Systems] Human information processing    General Terms  Algorithms, Measurement, Experimentation, Human Factors   Keywords  Learning analytics, visualization, hints   1. INTRODUCTION  An analogy can be established between learning and the business   sector, where the use of data mining techniques and business   intelligence tools [1] has become widespread in companies. A big   issue for business intelligence is how to deal with thousands of   data that are difficult to understand and convert these into high   level meaningful information that can be used as a basis for   decision making by organizational stakeholders [2]. We are   looking for the same outcome in the e-learning area.   We understand low level learning data to be the collection of   event entities and all their related data (e.g., time or resources   involved), which are usually stored in a database. They do not   usually convey any meaningful sense alone, but if we process   them properly, then useful information can be obtained. Our   motivation for this work is to transform a huge amount of low   level learning data into high level parameters that can be   meaningful for students and teachers, in order to answer questions   such as: Can this user be motivated by gamification techniques   In this work, we define a collection of high level learning   parameters that give insights into the learning process. These   parameters are calculated based on low level data. We have   applied these parameters to the Khan Academy1 platform,   extending the Khan Academy learning analytics module. Some   cases and results show the importance and meaningfulness of   these parameters for the learning process. We illustrate them in   our case study on a physics course using the Khan Academy   platform with more than 100 students.   2. RELATED WORK  The collection of low level educational data is very important.   Approaches such as Contextualized Attention Metadata (CAM)   [3] allow the retrieval of all the events from distributed sources.   The data can be collected in different formats, such as the   Resource Description Framework (RDF) [4].   Low level educational data as well as high level information have   been reported in different works. Some of the approaches [5, 6]   focus on the prediction of events by applying data mining   techniques and statistical methodologies. Other works present   practical specific visualization tools such as goals, activities, or   number of events per item [7], social network visualizations in the   CAMERA tool [8], or resources used, average time spent per   resource, or the evolution of the students in the SAM tool [9].   However, there are many new high level information parameters   that have not been addressed by the literature.    We present some high level information parameters that have   been applied to the Khan Academy platform. These parameters   were not present in the Khan Academy learning analytics module,   so we extended it. Some of the parameters relating to exercise   solving habits (e.g., hint abuse) have already been presented in the   literature (e.g. in [10]), but we have adapted the method of   calculating them as the semantic of the Khan Academy platform is   different from the Geometry Cognitive Tutor framework [10].     3. EDUCATIONAL ENVIRONMENT   An instance of the Khan Academy software is installed on our   own application server2. This instance is personalized with our   own pages, badges, etc. Specific materials, including videos and   exercises, were developed at Universidad Carlos III de Madrid for   a course on physics and were uploaded onto our instance of the   Khan Academy software. The course comprised a complete set of   27 videos and 35 exercises. This course took place in August 2012   over the entire month. More than 100 students were registered on   the course, which is a review of prerequisites that the students   should know before starting an engineering degree.                                                                     1 https://www.khanacademy.org/    2 http://uc3m-ka.appspot.com/       Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,   requires prior specific permission and/or a fee.   LAK '13, April 08 - 12 2013, Leuven, Belgium.  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.      112  https://www.khanacademy.org/ http://uc3m-ka.appspot.com/      Figure 1: Exercise interface at Khan Academy.   Figure 1 shows an exercise example based on the scalar product   running on our own Khan Academy instance. Each exercise has a   related video (4) and usually some hints (3). Exercises can be   parametric (1). When a student accesses an exercise, this exercise   will not change (even if it is accessed at different moments) until   the student solves it correctly. The next time that the exercise is   accessed, a new exercise on that same topic is obtained, with only   the parametric values changing. In order to be proficient at a   certain exercise, considering maximum efficiency, a student must   complete eight exercises correctly in a row without asking for any   hint (5) and answering correctly at the first attempt (2).   Khan Academy also incorporates gamification aspects such as   points that users can earn. Moreover, a student can win badges,   e.g., by achieving proficiency in a certain group of exercises or   topics.   In order to calculate these high level parameters, a set of python   scripts using the Google App Engine (GAE) API were developed   since Khan Academy runs over GAE infrastructure.   4. PROPOSED PARAMETERS  In this section we propose a set of high level information   parameters and show how to obtain them from the processing of   the low level data. The parameters are grouped into five blocks.   4.1 Total Use of the Platform  These parameters do not take into account if a user has done very   well or badly, but only the total use of the platform.   4.1.1 Total Effective Use of the Platform (TUP)  We denote the total available exercises and videos as AE and AV,   respectively. In the case of videos, we use two measures: one for   completed videos (CV) and another for started videos (SV):                                        Three measures are proposed for exercises relating to: the number   of different types of exercises a student attempted (DEA), that a   student spent enough time (DET), and that a student attempted at   least some number of times or solved correctly (DEN).                                                             Moreover, a global measure of total use of videos plus exercises is   possible, taking into account a weight for different videos and   exercises for all presented measures.   4.1.2 Total Efficient Use of the Platform (TEP)  In the case of videos, a measure is proposed: dividing the   maximum possible video time length (TVL) by the total time   spent on completed videos by the user (TEV). Therefore, if a user   repeats watching a video, his efficiency will decrease:                     Two parameters are proposed for exercises: the total number of   exercises attempted (TEA) divided by DEA gives a measure of   the number of times a user repeats his exercises on average; the   total time invested in all exercises (TTE) divided by the total   normal estimated time (TNT) gives a measure of whether the   student spent more time than expected, on average, on solving   exercises.                                        4.1.3 Total Time and Use of Optional Items  The total time on platform (TT) is the time (in minutes) that a user   has spent with exercises (TTE) and videos (TTV). This time can   exceed the real time a user has spent interacting with videos and   exercises, because they can have the video or exercise windows   open without watching videos or doing exercises.   Moreover, the platform offers several features that are not   mandatory to use in an educational environment, e.g., for our   Khan Academy environment the profile personalization or the   establishment of goals. We measure with this parameter if the   student is motivated by features of the platform other than the   normal ones. A simple example of this measure can be to   distinguish people who had some interaction with an optional item   from people who did not.    4.2 Correct Progress on the Platform  This block contains parameters that represent how well users have   interacted with the platform. This block does not take into account   the total use but the performance of the student with the materials.   4.2.1 Effective Correct Progress on the Platform  (ECP)  Correct progress on the platform for videos can be given as the   percentage time of all the videos that have been watched, although   other video measures presented for total use might also be used.   For exercises, three measures are proposed:      is the total  correct exercises (TCE) divided by the minimum total number of   correct exercises to obtain proficiency (8 in the case of the Khan   Academy platform) and multiplied by AE, with a limit so that   TCE for each type of exercise cannot be greater than 8;      is  the number of proficiencies achieved (UP) divided by AE;       is the total progress, that is the average of the progress on all   exercises, to obtain proficiency. Each exercise has a related   progress for each student from 0 to 1 depending on the number of   correct exercises, incorrect attempts or hints used in that type.                                                                4.2.2 Efficient Correct Progress on the Platform  (EP)  The efficient correct progress on the platform for videos can be   the same as for the efficient total use. Regarding exercises, one   measure is defined as the division of the total different types of   exercises correctly solved (TDCE) by the total time spent in   solving them (TTE) and multiplied by the average expected time   to solve an exercise. This time has been set to 40 seconds for all   the exercises of our Khan Academy educational environment.   Another measure is the division of TCE by TEA:                                                    113    In addition, a maximum time limit is set for the contribution of   each exercise to TTE (180 seconds for our case) so that noise is   not introduced for the measure.     values considerably less than  1 indicate non-efficient users, and values considerably greater   than 1 indicate that users solve many exercises correctly in a   reduced amount of time.   4.3 Time Distribution of Use of the Platform  This section represents an analysis of the times when the user   interacted with the platform.    4.3.1 Total Working Schedules  These parameters show at what time users watch their videos and   do exercises. We set three time schedules (TS) by time intervals:   morning [7:00 to 13:59] (TM), afternoon [14:00 to 20:59] (TA)   and night [21:00 to 06:59] (TN). The percentages of use in each   time schedule are calculated.   4.3.2 Efficiency by Working Schedules  These parameters use the same time schedule as in the last   subsection, but in this case the efficiency (ESP) of the user doing   exercises is measured in each time interval.                                                                                                                We propose this measure aiming to find that some users might   work better at different times of the day.   4.3.3 Constancy of Users  This parameter checks if a user was studying in a constant way   during several days or was studying strongly only for a few days.   The sample mean and the variance of time spent on the platform   by day are calculated:                                                                           N being the number of days that the course took and x the time   spent each day. The learning constancy is calculated using the   sample variance of the time from each day, i.     4.4 Gamification Habits  Here we try to analyze whether a user is motivated by the   gamification elements. A measure related to user badges (UB) is   proposed. This parameter consists of the total number of badges   that the user has earned (EB) divided by the exercise correct   progress on the platform (ECP). A user that achieves more badges   than another on the platform (their correct progress on the   platform being the same) will be more motivated because of the   badges. In addition, if two users have earned the same number of   badges but have different exercise correct progress on the   platform, the one with better correct progress on the platform will   be less motivated by badges, because the more a user advances on   the platform correctly, the more badges he can earn.                   4.5 Exercise Solving Habits  These parameters represent the way a user solves an exercise.   4.5.1 Explorer or Recommendation Listener  Khan Academy allows users to define prerequisites between   exercises and also includes an exercise recommender. Therefore,   checking whether a user has accessed a certain number of   Acces to an   exercise  Answers   correctly Correct behaviorYES  Has user seen   related video  NO  Increase video   avoidance NO  Did user ask   for hints YES  Increase hint   avoidance NO  Did user   answered   reflexively  YES  Correct behavior  YES  Increase   unreflective user NO   Figure 2. Model flow diagram.   exercises according to the system recommendation will give us an   indication of whether a user usually follows the recommended   learning path (RL):                                4.5.2 Hint Avoidance, Video Avoidance, Unreflective  User and Hint Abuser  We propose a model that tries to cover all the possible situations   that a learner can encounter when interacting with an exercise on   the Khan Academy platform. Figure 2 shows this model. The flow   diagram starts at the point where a user is answering an exercise.   If the user answers correctly, then the counter of correct behavior   is increased. If the answer is wrong, then the system checks   whether the student had watched the related video. If he did not   watch it, then the local value for video avoidance (VA) profile is   set to 1, otherwise is set to 0. Next, the system checks whether the   student requested for hints. If he did not, the local value for hint   avoidance (HAV) profile is set to 1, otherwise it will be a number   in the range [0,1] representing the percentage of requested hints   for that type of exercise. Lastly, if the student answers too fast,   (e.g., less than 10 seconds), then the local value for unreflective   user (US) profile is set to 1, otherwise is 0. It is important to note   that we apply this profile only until the student does one exercise   of that type correctly; otherwise we would be contaminating with   noise since the user already knows how to solve the problem and   subsequent similar problems as they are parametric.   Each time that a student accesses a different type of exercise, each   of the aforementioned parameters is set from [0, 1]. This is the   local value of the parameters for a type of exercises. The global   value of these parameters is the mean of all the local ones among   the exercises in which students had some interaction and where   these parameters were set to some value.   In addition, the hint abuser parameter takes into account the exact   time for the first hint as well as time intervals between hint   requests. For example, if a user starts an exercise and in less than   10 seconds he has already requested a hint, then the hint abuser   counter is increased. Similarly, if the time difference between two   hint requests is lower than 10 seconds the hint abuser counter is   also increased.   5. RESULTS AND DISCUSSION OF THE  MEANING OF PARAMETERS  In this section, we show how the presented parameters have a   utility in the learning context. Moreover, we present typical   situations in our course where they can be used. We analyze their   meaning in the context of our Khan Academy educational   environment and present some illustrative results.    114     Figure 3. Exercises efficiency in time.   Although a total of more than 100 students interacted in the   course, we have not taken into account the ones who we consider   that did not interact enough; therefore the following results are   based on the analysis of 66 students.   The plan for the physics course is based on the methodology of   flipping the classroom, so that students prepare the lessons using   the Khan Academy platform in advance of face to face lessons. It   is important for teachers to know which students are prepared well   for the face to face lessons; therefore the parameters for the   effective correct progress on the platform are important. Based on   these parameters, teachers can set a threshold for considering   which students are prepared well, e.g., a minimum number of   completed videos or minimum number of exercises where   proficiency is obtained. Usually, teachers will set a combination   of conditions on these parameters to consider that a student is well   prepared for the face to face course (e.g., an AND of conditions or   a global measure taking everything into account with a weight).   Considering a minimum of 16 videos totally completed (from       ) or 21 exercises where a student obtained proficiency (from      ), we can say that 22 out of 66 students did well on the  platform and were well prepared for the face to face sessions.   Among the students who did well on the platform (passed this   threshold of correct progress), the measures of effectiveness give   us an idea of who was more effective in terms of time or less   repetition of items. Students who are not efficient in their learning   might be advised or guided so that they can take better advantage   of their time, because it is not only important to learn but to do so   in an efficient way.    The parameters relating to effectiveness can be given in the form   of graphs. Figure 3 shows a representation of     for the exercise  solving time efficiency for all students in our experiment. The   middle value line set upon 100 percent would be the average time   that a normal user should spend to solve the exercises. If students   are far above that line, it means that they solve the exercises faster   than the critical value, while students below the line need more   time per correct exercise. Therefore, two students can have similar   values of correct progress but one can be more efficient in time   (or number of attempts) than the other.   Among the students that did wrong on the platform (did not pass   the threshold of correct progress), the parameters of total use of   the platform will let us know whether the students made some   effort to learn and where (videos, exercises, time) or if they did   not make any effort. For example, for a time (TT) exceeding 225   minutes of interaction with the platform, and more than 15 started   videos (from     ), or more than 20 attempts at different types of  exercises (from     ), we can detect that 8 out of the 44 students  that did badly made a considerable effort on the platform. These   students might need more remedial support.   A Pearson correlation test shows that there is a statistically   significant difference at 99% level between the total time (TT)   and the following parameters: videos completed from       (r=0.80), videos started from      (r=0.81), exercises attempted  from      (r=0.71), and exercises with proficiency from       (r=0.73). Therefore, the total time is related strongly to these   measures, and it is a good parameter to predict the number and   quality of interactions with the platform.    Another important issue is to identify whether or not students are   motivated to achieve badges, and this is indicated by the   gamification habits parameters. Two students might have a strong   activity on the platform, but one of them might have a lot of   badges while the other has only a few badges, indicating that he is   not motivated by them. Students that are motivated by   gamification can be identified and participate in future   gamification activities.   The parameter of total use of optional elements gives information   about whether students were interested in extra functionalities of   the platform that were not mandatory and about which they were   not given any information. A total of 17 students used some type   of optional functionality. This may denote curiosity and identify   students who like to explore things. The Pearson correlation   between the use of optional items and the total time (r=0.16,   p=0.19) and the percentage of proficiencies obtained (r=0.3,   p=0.014) suggest that the use of optional items, or not, is not   strongly related to the total time of use of the platform or whether   or not the user obtains proficiency in exercises.   In addition, surprisingly, there was not a statistically significant   relationship between the use, or not, of optional items and the   recommender/explorer parameter from RL (r=0.1, p=0.42). One   might tend to think that students who use optional items would   tend to be explorers and not take into account the   recommendations, but this relationship was not found in the   experiment.   The parameters relating to constancy in learning give us an   indication of whether or not students learn in a constant way. The   variancebut also the meanshould be taken into account for   the interpretation. In many situations, students might learn better   for the long term if they do it in a constant way, so a system might   recommend non-constant students to learn in a more constant way   or hide some activities from them until some specific date.   Figure 4 shows the constancy measures (mean and variance)   applied to top users according to their activity on the platform   during a time interval from [01/08/2012] to [08/09/2012]. We can   see, for example, that user 4 is a constant student because his   variance is very low with respect to the mean. In the same way,   user 8 has not been learning in a constant way but only for a few   days. With a similar analysis we can reach conclusions for all the   users.   Moreover, time schedules where students spent more time and   where they were more efficient can be of interest, e.g., for   personalization of tasks to time slots.   115    Figure 4. Constancy measure applied to top users.   Finally, some help-seeking bugs, such as help abuse, are   correlated with learning gains according to the literature [10].   Therefore, knowing the exercise solving habits parameters is of   special importance as teachers can act and intervene to change the   behavior of students so that they can learn in a more effective   way.   Considering a level of more or equal than 25% as the threshold for   each of the problem solving habits parameters, we can say that   30.3 % of students had the profile of hint avoider, 25.8 % of video   avoider, 40.9 % of unreflective user and 12.1% of hint abuser.   In addition, table 1 shows the correlations among the different   categories of problem solving. The only correlations that are   statistically significant at the 99% level are 1) the unreflective   user with respect to hint abuser and hint avoider, which makes   sense as a user that does not reflect on his learning can select, or   not, a hint by chance, and 2) video and hint avoidance, which also   makes sense as a user who avoids hints can also have tendency to   avoid videos. In addition, the relationships that were not   statistically significant also make sense, e.g., the hint abuser with   respect to hint and video avoidance, as a user who tends to use   abuser techniques will not tend toward avoidance ones.    Table 1: Correlations among problem solving habits     Hint   avoid.   Video   avoid.   Unrefl.   user   Hint   abuser   Hint avoidance 1 0.382 0.607 -0.186   Video avoid. 0.382 1 0.289 0.096   Unrefl. user 0.607 0.289 1 0.317   6. CONCLUSIONS  This paper proposes a set of high level parameters that can give   useful information for students and teachers about the learning   process, and we have illustrated it with a case study of more than   100 students using the Khan Academy platform. The information   obtained using our proposed parameters is not easy to obtain with   the present Khan Academy learning analytics module. For   example, if a teacher wants to know about problem solving habits,   he must go through different windows to see which videos a   student has watched, the time spent, and the details of the   resolution of each exercise and make many complex calculations.   The proposed parameters can be applied not only for the Khan   Academy platform but for other systems, incorporating the proper   adaptations. In this direction, the semantics of each platform   influences the types of parameters that can be used and the way to   measure them, e.g., the correct use of exercises can be redefined   in other platforms where the same exercise is not repeated to   obtain proficiency. In addition, on some occasions some   parameters cannot give useful information, for example, a student   may obtain all the badges and solve everything correctly, but we   cannot say that he is or is not motivated by the badges.    The results of this study can be applied for useful interventions,   for example, in recommender applications. Another challenge is   how to visualize all the information in an easy way for teachers.   7. ACKNOWLEDGMENTS  Work partially funded by the EEE project,  Plan Nacional de   I+D+I TIN2011-28308-C03-01  and the  Emadrid: Investigacin   y desarrollo de tecnologas para el e-learning en la Comunidad de   Madrid  project  (S2009/TIC-1650) .   We would like to thank the professors in Universidad Carlos III   de Madrid who developed the physics materials for the course   (videos and exercises).   8. REFERENCES  [1] P. Baepler and C. J. Murdoch. Academic analytics and data   mining in higher education. International Journal for the   Scholarship of Teaching and Learning, 4(2), July 2010.    [2] H. Drachsler, and W. Greller. The pulse of learning analytics  understandings and expectations from the stakeholders. In   LAK 12 Conference Proceedings, pages 120129, 2012.    [3] M. Wolpers, J. Najjar, K. Verbert, and E. Duval. Tracking  actual usage: the attention metadata approach. Educ.   Technol. Soc., 10(3):106121, 2007.   [4] P. J. Muoz-Merino, M. Wolpers, K. Niemann, M. Friedrich,  A. Pardo, C. Delgado Kloos, and M. Muoz-Organero. CAM   in the semantic web world. In I-SEMANTICS 10 Conference   Proceedings, 2010.   [5] L. P. Macfadyen, and S. Dawson. Mining LMS data to  develop an early warning system for educators: A proof of   concept. Computers & Education, 54(2):588599, February   2010.   [6] C. Romero, S. Ventura, and E. Garca. Data mining in course  management systems: Moodle case study and tutorial.   Computers & Education, 51(1):368384, August 2008.   [7] E. Duval, J. L. Santos, K. Verbert, and S. Govaerts. Goal- oriented visualizations of activity tracking: a case study with   engineering students. In LAK 12 Conference Proceedings,   pages 143152, 2012.    [8] H. Schmitz, M. Schefel, M. Friedrich, M. Jahn, K. Niemann,  and M. Wolpers. CAMera for PLE. In EC-TEL 09   Conference Proceedings, pages 507520, 2009.   [9] S. Govaerts, K. Verbert, E. Duval, and A. Pardo, The student  activity meter for awareness and self-reflection. In CHI EA   12 Conference Proceedings, pages 869884, 2012.     [10] V. Aleven, B. M. McLaren, O. Roll, and K. Koedinger.  Toward tutoring help seeking; Applying cognitive modeling   to meta-cognitive skills. In ITS-2004 Conference   Proceedings, pages 227-239, 2004.         116      "}
{"index":{"_id":"17"}}
{"datatype":"inproceedings","key":"Pardos:2013:ASS:2460296.2460320","author":"Pardos, Zachary A. and Baker, Ryan S. J. D. and San Pedro, Maria O. C. Z. and Gowda, Sujith M. and Gowda, Supreeth M.","title":"Affective States and State Tests: Investigating How Affect Throughout the School Year Predicts End of Year Learning Outcomes","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"117--124","numpages":"8","url":"http://doi.acm.org/10.1145/2460296.2460320","doi":"10.1145/2460296.2460320","acmid":"2460320","publisher":"ACM","address":"New York, NY, USA","keywords":"affect, boredom, confusion, data mining, detectors, high stakes tests, prediction, tutoring","abstract":"In this paper, we investigate the correspondence between student affect in a web-based tutoring platform throughout the school year and learning outcomes at the end of the year, on a high-stakes mathematics exam. The relationships between affect and learning outcomes have been previously studied, but not in a manner that is both longitudinal and finer-grained. Affect detectors are used to estimate student affective states based on post-hoc analysis of tutor log-data. For every student action in the tutor the detectors give us an estimated probability that the student is in a state of boredom, engaged concentration, confusion, and frustration, and estimates of the probability that they are exhibiting off-task or gaming behaviors. We ran the detectors on two years of log-data from 8th grade student use of the ASSISTments math tutoring system and collected corresponding end of year, high stakes, state math test scores for the 1,393 students in our cohort. By correlating these data sources, we find that boredom during problem solving is negatively correlated with performance, as expected; however, boredom is positively correlated with performance when exhibited during scaffolded tutoring. A similar pattern is unexpectedly seen for confusion. Engaged concentration and frustration are both associated with positive learning outcomes, surprisingly in the case of frustration.","pdf":"Affective states and state tests: Investigating how affect  throughout the school year predicts end of year learning   outcomes  Zachary A. Pardos   Massachusetts Institute of Technology  77 Massachusetts Avenue   Cambridge, MA 02139  pardos@mit.edu     Sujith M. Gowda   Worcester Polytechnic Institute  100 Institute Road   Worcester, MA 01609  sujithmg@wpi.edu      Ryan S.J.d. Baker   Columbia University Teachers College   525 W. 120th St., Box 118  New York, NY 10027   Baker2@exchange.tc.columbia.edu       Supreeth M. Gowda  Worcester Polytechnic Institute   100 Institute Road  Worcester, MA 01609  smgowda@wpi.edu   Maria O.C.Z. San Pedro  Columbia University Teachers College   525 W. 120th St., Box 118  New York, NY 10027   mzs2106@tc.columbia.edu         ABSTRACT  In this paper, we investigate the correspondence between student  affect in a web-based tutoring platform throughout the school year  and learning outcomes at the end of the year, on a high-stakes  mathematics exam. The relationships between affect and learning  outcomes have been previously studied, but not in a manner that is  both longitudinal and finer-grained. Affect detectors are used to  estimate student affective states based on post-hoc analysis of  tutor log-data. For every student action in the tutor the detectors  give us an estimated probability that the student is in a state of  boredom, engaged concentration, confusion, and frustration, and  estimates of the probability that they are exhibiting off-task or  gaming behaviors. We ran the detectors on two years of log-data  from 8th grade student use of the ASSISTments math tutoring  system and collected corresponding end of year, high stakes, state  math test scores for the 1,393 students in our cohort. By  correlating these data sources, we find that boredom during  problem solving is negatively correlated with performance, as  expected; however, boredom is positively correlated with  performance when exhibited during scaffolded tutoring. A similar  pattern is unexpectedly seen for confusion. Engaged concentration  and frustration are both associated with positive learning  outcomes, surprisingly in the case of frustration.   Categories and Subject Descriptors  H.1.2 [Human Factors]: Models and Principles    General Terms  Algorithms, Measurement, Human Factors.   Keywords  Affect, confusion, boredom, high stakes tests, tutoring, detectors,  prediction, data mining.      1. INTRODUCTION  In recent years, researchers have increasingly investigated the  relationship between fine-grained details of student usage of  tutoring systems and performance on high-stakes examinations  [cf. 14, 18]. Understanding how different student behaviors  correspond to student outcomes can help us to understand the  larger implications of student choices that might seem only  momentary. This information can be useful both in terms of  advancing theory on meta-cognition and engagement [cf. 1, 6],  and to provide actionable information for teachers about factors  potentially influencing their students learning outcomes [2].  Within this paper, we analyze the relationships between a  students affect and their outcomes. Several studies have indicated  that affect can lead to differences in learning [12, 19, 21];  however, past research on these relationships has been limited by  the use of observational or survey methods, which are either  coarse-grained, or can only be applied over brief periods of time  (year-long field observations are possible, but prohibitively  expensive to conduct for large numbers of students). Within this  paper, we use automated detectors of affect that can be applied to  every student actions in an entire years log file data to analyze  this question, asking how predictive a students affect, throughout  the school year, is of his or her end of year high stakes test  outcome. In specific, we investigate overall relationships between  affect and learning, and dig deeper to ask, are there some contexts  where a particular affect is constructive and others where it is not  We investigate these questions in the context of two school years  of student learning within the ASSISTments tutoring system [14],  involving over a thousand students.     1.1 The Tutor and the Test  ASSISTments is a web-based tutoring platform, primarily for 7th- 12th grade mathematics. Within ASSISTments, shown in Figure 1,  students complete mathematics problems and are formatively  assessed  providing detailed information on their knowledge to  their teachers  while being assisted with scaffolding, help, and  feedback. Items in ASSISTments are designed to correspond to  the skills and concepts taught in relevant state standardized  examinations. Figure 1 shows how after the student answers the  original question incorrectly, the system provides scaffolding that  breaks the problem down into steps. Hints are provided at each   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.     117    step and the student can ask for a bottom-out hint that eventually  tells the answer. Students in the data sets studied within this paper  used the ASSISTments in classroom computer lab sessions  targeted towards preparation for the standardized state test, during  school hours. While teachers had the ability to assign students  questions of a particular skill, the most popular problem set within  the data set that will be analyzed in this paper was one that  randomly sampled 8th grade math test prep questions from the  system. Because of this, students sometimes received questions  with skills they had not encountered in class yet. One data set,  used to develop models of student affect, represented a few days  of software usage. The other data set, used to study the  relationship between student affect and learning outcomes,  represents an entire year of data of students using the  ASSISTments system.   Near the end of their school year, students took the MCAS  (Massachusetts Comprehensive Assessment System) state  standardized test. We collected scores for the math portion of the   test. Raw scores range from 0 to 54 and are later scaled by the  state after all tests are in. The scaling maps to four categories;  Failing, Needs Improvement, Proficient, and Advanced. Students  must score above Failing to graduate high school and an   Advanced score earns them an automatic state college  scholarship.    2. METHODOLOGY  In this section we will describe both the methodology for  employing the automatic affect detectors to our dataset and the  methodology for conducting the correlation analysis. Our detector  development methodology built off of past work in other learning  systems [cf. 7], but this paper represents the first publication of  our affect detectors for the ASSISTments platform.    2.1 Affect and Behavior Detection  In order to assess student affect and behavior across contexts, we  adopt a two-stage process: first labeling student affect and  behavior for a small but reasonably representative sample with  field observations [cf. 4], and then using those labels to create  automated detectors that can be applied to log files at scale. The  detectors are created by synchronizing log files generated by the  ASSISTments system with field observations conducted at the  same time. To enhance scalability, only log data is used as the  basis of the detectors; physical sensors can enhance detector  goodness [cf.11, 13], but reduce the applicability of the resultant  models to existing log files. The detectors are constructed using  log data from student actions within the software occurring at the  same time as or before the observations, making our detectors  usable for automated interventions, as well as the type of  discovery with models analysis conducted in this paper. Our  process for developing sensor-free affect and behavior detectors  for ASSISTments replicates a process which has been successful  for developing affect detectors for a different intelligent tutor,  Cognitive Tutor Algebra [7].   2.1.1 Data Collection  Two sets of data from ASSISTments were used in this study.    The first data set was used to develop the automated detectors of  affect. This data set was composed of field observations of affect  and behavior that were conducted over a few days in an urban  middle school in central Massachusetts, sampled from a diverse  population of 229 students. Within this school, 40% of students  were Hispanic, 14% were African-American, 4% were Asian- American, and 39% were Caucasian. In this school, per capita  income was significantly lower than the state average.  Information from these observations and the corresponding  interaction logs was used to develop and validate the affect  detectors discussed below.   The second data set was used to conduct analyses of the  relationships between affect and learning. This data set was  composed of action log files that were distilled from a diverse  population (racially and socio-economically) of 1,393 students  that came from middle schools in the same city in central  Massachusetts, in 2004-2005 and 2005-2006 (these years were  chosen due to the availability of standardized examination data).  629 students used the software in 2004-2005, and 764 students  used the software in 2005-2006. This data set involved a whole  year of students using the software for two hours, twice a week.  As this data set represented whole-year usage of the software,  810,000 student actions (entering an answer or requesting help)  were represented in the data. The affect models were applied to  this larger dataset.    2.1.2 Affect and Behavior Observations  Student affect and behavior was coded by a pair of expert field  observers as students used ASSISTments in 2010. An observation  protocol developed for coding affect during the use of educational   Figure 1. An example of an ASSISTments item where the student  answers incorrectly and receives scaffolding help   118    software [cf. 4] was implemented using field observation  synchronization software [7] developed for Google Android  handheld devices. Each observation lasted up to twenty seconds,  with elapsed observation time so far displayed by the hand-held  observation software. If affect or behavior was labeled before  twenty seconds elapsed, the coder moved to the next observation.  Each observation was conducted using side glances, to reduce  observer effects. To increase tractability of both coding and  eventual analysis, if two distinct affective states were seen during  a single observation, only the first state observed was coded. Any  affect or behavior of a student other than the student currently  being observed was not coded. The observers based their  judgment of a students affect or behavior on the students work  context, actions, utterances, facial expressions, body language,  and interactions with teachers or fellow students. These are,  broadly, the same types of information used in previous methods  for coding affect [e.g., 8], and in line with Planalp et al.s [20]  descriptive research on how humans generally identify affect  using multiple cues in concert for maximum accuracy rather than  attempting to select  individual cues. Affect and behavior coding  was conducted on a handheld app previously designed for this  purpose [7]. Student affect or behavior was coded according to the  following set of categories: boredom, frustration, engaged  concentration, confusion, off-task behavior, gaming, and any  other affective or behavior state. These categories were chosen  due to past evidence that they are relatively common and are  either associated with learning or hypothesized to be associated  with learning [cf. 1, 3, 4, 7, 9, 12, 16, 17, 21]. The affective  categories were defined for coding according to the definitions in  [4], and the behavior categories were defined according to the  definitions in [3, 4].   At the beginning of data collection, an inter-rater reliability  session was conducted, where the two coders coded the same  student at the same time, across 51 different coding instances  across multiple students. With reference to the categories of affect  studied in this paper, inter-rater reliability achieved Cohens  Kappa of 0.72, indicating agreement 72% better than chance. For  categories of behavior, inter-rater reliability achieved Cohens  Kappa of 0.86, agreement 86% better than chance. This level of  agreement is substantially higher than the level of agreement  typically seen for video coding of affect [13, 24]. After this  session, the observers coded students separately, for a total of  3,075 observation codes.   Within the observations, not counting observations marked as   (which represent cases where coding was impossible, due to  uncertainty, behavior outside the coding scheme, a student leaving  the room, impossible positioning, or other factors), boredom was  observed 21.7% of the time, frustration was observed 5.4% of the  time, engaged concentration 65.0% of the time, confusion was  observed 7.9% of the time. In terms of behavior, off-task behavior  was observed 21.9% of the time, and gaming was observed 1.5%  of the time. This distribution of affect and behavior corresponds to  previous studies, where engaged concentration is the most  prevalent affect in a classroom environment [4, 7, 22].   2.1.3 ASSISTments Interaction Logs  During observations, both the handhelds and the educational  software logging server were synchronized to the same internet  time server, using the same field observation data collection  software as was used in [7]. This enabled us to determine which  student actions within the software were occurring when the field  observations occurred. Interactions with the software during the   twenty seconds prior to data entry by the observer were  aggregated as a clip, and data features were distilled.    The original log files consisted of data on every student attempt to  respond (and whether it was correct), and requests for hint and  scaffolding, as well as the context and time taken for each of these  actions. In turn, 40 features were distilled from each action,  including features distilled for detecting other constructs in  ASSISTments [cf. 5], and features developed for detecting student  behavior and affect in Cognitive Tutors [cf. 3, 7]. Many of the  distilled features pertained to the students past actions, such as  how many attempts the student had previously made on this  problem step, how many previous actions for this skill or problem  step involved help requests, how many incorrect actions the  student had made on this problem step, and so on. To aggregate  individual student actions into twenty-second clips, the sum,  minimum, maximum, and average values were calculated across  actions for each clip. This relatively simple approach to  summarizing features was used due to its success in similar  problems in other learning systems [cf. 7]. Thus, for the creation  of affect and behavior models, a total of 160 features were used.   2.1.4 Creation of Affect and Behavior Models  A detector for each affective state or behavior was developed  separately, comparing that affective state to all other affective  states (e.g., Bored was compared to Not Bored, Frustrated  was compared to Not Frustrated, Engaged Concentration was  compared to Not Engaged Concentration, and Confused was  compared to Not Confused), or comparing that behavior to all  other behaviors (e.g., Off-task was compared to Not Off-task,  Gaming was compared to Not Gaming,). Each detector was  evaluated using 5-fold cross-validation, at the student-level (e.g.  detectors are trained on four groups of students and tested on a  fifth group of students). By cross-validating at this level, we  increase confidence that detectors will be accurate for new groups  of students. Further, in this student-level cross-validation, students  were stratified into fold assignments based on their training labels.  This guarantees that each fold has a representative number of  observations of the majority and minority class. In addition, for  unbalanced classes, re-sampling was used on the training sets to  make the class frequency more equal for detector development  (but detector goodness was validated on a data set that was not re- sampled, to ensure model validity for data with natural  distributions). We attempted to fit sensor-free affect detectors  using eight common classification algorithms, including J48  decision trees, step regression, JRip, Naive Bayes, K*, and REP- Trees. These algorithms were chosen as a sample of the space of  potential algorithms, which can represent data with different  patterns, but each of which is relatively conservative and not  highly prone to over-fitting. (Further discussion of the specific  algorithms that were effective is given below).   Feature selection for machine learning algorithms was conducted  using forward selection, where the feature that most improves  model goodness is added repeatedly until no more features can be  added which improve model goodness (Table 1). During feature  selection, cross-validated kappa on the original (non-re-sampled)  data set was used as the goodness metric. Prior to feature  selection, all features with cross-validated kappa equal to or below  zero in a single-feature model were omitted from further  consideration, as a check on over-fitting.    The affect and behavior detectors performance was evaluated on  their ability to predict the presence or absence of each affective  state or behavior in a clip. Detectors were evaluated using A' [15],  Cohens Kappa [10], and F-measure [25] goodness metrics. The   119    A' metric (equivalent to area under the ROC curve) is the  probability that the model will be able to discriminate a randomly  chosen positive case from a randomly chosen negative case. An A'  value of 0.5 for a model indicates chance-level performance, and  1.0 performing perfectly. Cohens Kappa assesses the degree to  which the model is better than chance at identifying the affective  state or behavior in a clip. A Kappa of 0 indicates chance-level  performance, while a Kappa of 1 indicates perfect performance. A  Kappa of 0.45 is equivalent to a detector that is 45% better than  chance at identifying affect or behavior. The F-measure of F1- score is a measure of the model's accuracy, computing for the  weighted average of the model's precision and recall where the  best F1 score is at 1 and the worst score is 0.   All of the affect and behavior detectors performed better than  chance (Table 1). Detector goodness was somewhat lower than  had been previously seen for Cognitive Tutor Algebra [cf. 7], but  better than had been seen in other published models inferring  student affect in an intelligent tutoring system solely from log  files (where average Kappa ranged from below zero to 0.19 when  fully stringent validation was used) [7, 11, 13, 22]. The best  detector of engaged concentration involved the K* algorithm,  achieving an A' of 0.678,a Kappa of 0.358, and an F-measure of  0.687. The best boredom detector was found using the JRip  algorithm, achieving an A' of 0.632, a Kappa of 0.229, and an F- measure of 0.632. The best frustration detector achieved an A' of  0.682, a Kappa of 0.324, and an F-measure of 0.677, using the  Naive Bayes algorithm.  The best confusion detector used the J48  algorithm, having an A of 0.736, a  Kappa of 0.274, and an F- measure of 0.667. The best detector of off-task behavior was  found using the REP-Tree algorithm, with an A value of 0.819, a  Kappa of 0.506, and an F-measure of 0.693. The best gaming  detector involved the K* algorithm, having an A value of 0.802,  a Kappa of 0.370, and an F-measure of 0.687. These levels of  detector goodness indicate models that are clearly informative,  though there is still considerable room for improvement.    Table 1. Performances of affect and behavior models   Affect Algorithm A Kappa F-Measure   Boredom JRip 0.632 0.229 0.632   Frustration Naive Bayes 0.682 0.324 0.677   Engaged  Concentration K* 0.678 0.358 0.687   Confusion J48 0.736 0.274 0.667   Off-Task REP-Tree 0.819 0.506 0.693   Gaming K* 0.802 0.370 0.750      Detector features for boredom include the total number of actions,  the total time spent on the last action before the clip and the first  action after the clip, and the students history of help requests and  correct answers. For example, students were deemed bored when  they spent over 83 seconds inactive immediately before or after  the observation (lengthy pauses are also an excellent predictor of  off-task behavior [cf. 3], a behavior thought to be associated with  boredom). Students were also deemed bored when they worked on  the same problem during the entire observation but did not  provide any correct answers either during the observation or   immediately afterwards (a serious and actively working student  will generally obtain some correct answers in ASSISTments, as  increasingly easy scaffolding is given when students make errors).      The detectors features for frustration involve the percent  occurrence in the past of incorrect answers on a skill, the largest  hint count in that clip, the average correct actions in that clip, the  largest number of scaffolding for a problem in that clip, the total  number of past help request for that clip, the total number of  actions that were second to the last hint for that clip, the largest  number of consecutive errors in that clip, and least sum of right  actions in that clip. The resulting model showed students that had  low average of correct actions were frustrated.     Features used in the engaged concentration detector included the  number of correct answers during the clip, the proportion of  actions where the student took over 80 seconds to respond,  whether the student followed scaffolding with a hint request,  whether the student received scaffolding on the first attempt in a  problem, and how many of the students previous five actions  involved the same problem. The model was created using the K*  algorithm, which is an instance-based classifier. Instance-based  classifiers predict group membership based on similarities to  specific cases in the training set, rather than general rules,  enabling them to identify constructs which can manifest in several  distinct ways. For example, one group of students in engaged  concentration repeatedly answered correctly in less than 80  seconds. Another group of students in engaged concentration  answered incorrectly on their first attempt at a problem but then  spent considerable time making their first response to the  scaffolding question they received.   For confusion, detector features included the total number of  consecutive incorrect actions for that clip, number of hints used  for that clip, the number of correct actions in the clip, total  number of past incorrect actions for a skill in that clip, correct  actions that took time to answer, actions for a skill that the student  got incorrect previously and that took time to answer. The  resulting model was fairly complex, but one relationship  leveraged in the model is that students who commit consecutive  errors in a row for a skill are deemed confused. Another  relationship is when students committed a number of incorrect  actions in the past for a skill and took a long time to answer the  current one, they are seen as confused.   The off-task detector included the total number of attempts made  for a skill in that clip, the time taken by a student to answer, if a  student has a correct action for that clip, average number of  scaffold in that clip, and total number of incorrect actions in the  past in the clip. The resulting model also was complex, but one  relationship shows that if there were few attempts for a problem,  and it took them a long time to answer, the student exhibits off- task behavior.   The features for the gaming detector included the use of a bottom- out hint in the clip, the number of hint usage for that clip, the  average hint counts for a skill in that clip, the total number of  actions for that clip that were answered incorrectly, and the  occurrence of scaffold in that clip. The resulting model for  gaming, like engaged concentration, used the K* algorithm.  Hence, similarities that resulted to the group of gaming students  included those that usually used bottom-out hints, scaffolding and  hints.   120    2.2 Application of Models to Broader Data  Set  Once the detectors of student affect and behavior were developed,  they were applied to a broader data set consisting of two school  years of student usage of the ASSISTments system by Worcester  middle schools, 2004-2005 and 2005-2006. As discussed above,  these schools represented a diverse sample of students in terms of  both ethnicity and socio-economic status. This data set included  1,393 students and around 810,000 student actions within the  learning software. The same features as discussed above were  distilled for these data sets. Using these detectors, we were able to  predict student affect and behavior for each student action within  the ASSISTments system.   2.2.1 Correlation analysis  In order to correlate students affect estimates with their raw state  test scores we first had to summarize their affect during the year,  calculating one number per affective state per student. For each  affective state we calculated the mean of the predicted  probabilities for that state during performance on each skill in the  system. This list of means for each skill was then averaged to  produce summarized overall proportion of affect  for the student.  This averaging gives equal weighting of affect for each skill. This  procedure was used because the MCAS test, which we are  correlating to, consists of a random selection of skills. The  weighting prevents a more frequently studied skill from having an  influence on the students summarized affect that is  disproportionate to its representation on the test.   Table 2. Example student affect dataset to be summarized   Student    Skill    Probability   of   Bored    Is   Original     Tricia    Subtraction    0.20    Yes     Tricia    Subtraction    0.50    No     Tricia    Subtraction    0.50    No     Tricia    Addition    0.90    Yes     Tricia    Addition    0.70    Yes        Table 2 shows example affect data for calculating the summary of  the bored affective state for one student. To calculate the degree  of boredom during the year for the student in Table 2, the  following calculation would be used:   ! #$#!!! ! ! #$% =  0.20 + 0.50 + 0.50 3 +  0.90 + 0.70 2  2 = 0.60   We also calculate the summarized affect for each student for  original and scaffold questions separately. In ASSISTments,  scaffold questions are given when a student asks for help or  answers an original question (main question) incorrectly. The  scaffolding often consists of several sub questions and students  know that they will be required to go through the scaffolding if a  question is answered incorrectly; therefore we wanted to allow for  the possibility of observing different affect during original  questions than scaffolds.   3. RESULTS  After summarizing the estimates of each students affect, we used  Pearson correlation to observe the correspondence between their  affect and their end-of-year state test score. The results below   show the correlation of affect to test score for the two years of  data. We report separately on the affect experienced by students  while answering original questions and the affect while answering  scaffold questions, as the patterns of affect were substantially  different in these two cases. Across tests, the high sample size  resulted in most correlations being statistically significant (using  the standard t-test for correlation coefficients, two-tailed).   Table 3. Correlation of student affect to their raw state test score.  Statistically significant results (p<0.05) are given in boldface;  results where p<0.01 are also italicized.   Correlation    ORIGINAL    SCAFFOLD     AFFECT    04-05    05-06    04-05    05-06     Boredom    -0.11930    -0.27977    0.32082    0.26884     Engaged    Concentration   0.44923    0.25794    0.20988    0.09238     Confusion    -0.16538    -0.08912    0.37370    0.23457     Frustration    0.30524    0.20376    0.26182    0.22418     Off-Task    0.14820    -0.00662    0.16985    -0.10793     Gaming    -0.43083    -0.30125    -0.32933    -0.24688        The strongest positive correlation, as shown in Table 3, was for  engaged concentration on original questions. For 2004-2005,  r=0.45, t(624)= 12.56, two-tailed p<0.01. For 2005-2006, r=0.26,  t(760) = 7.36, two-tailed p<0.01.  This finding is unsurprising, and  maps to previous results showing a positive relationship between  this affective state and learning [cf. 12, 21]. Even on scaffolding  items, this relationship remained positive.  For 2004-2005, r=0.21,  t(624)= 5.36, two-tailed p<0.01. For 2005-2006, r=0.09, t(760) =  2.56, two-tailed p=0.01.  Boredom on original questions was negatively associated with  learning outcomes, again matching previous research [cf. 12, 19,  21]. For 2004-2005, r= -0.12, t(624)= -3.00, two-tailed p<0.01.  For 2005-2006, r= -0.28, t(760) = -8.03, two-tailed p<0.01.  However, boredom on scaffolding questions was associated with  better learning. For 2004-2005, r= 0.32, t(624)= 8.46, two-tailed  p<0.01. For 2005-2006, r= 0.27, t(760) = 7.69, two-tailed p<0.01.  In interpreting this finding, it is worth considering why a student  would become bored on a scaffolding question. One possibility is  that the student knew the skill in the original question, but was  careless [cf. 23], which would explain these positive correlations.  Another possibility is that high scoring students may know most  of the skills involved with an original problem but not enough to  answer correctly. When they are forced into the scaffolding,  which breaks the main problem into individual skill sub questions,  they become bored because they are being made to work on  simpler questions that they already know the answers to.   Confusion had a similar pattern to boredom, with weak negative  associations for original questions. For 2004-2005, r= -0.17,  t(624)= -4.19, two-tailed p<0.01. For 2005-2006, r= -0.09, t(760)  = -2.47, two-tailed p=0.01. By contrast, positive associations were  found for scaffolding questions. For 2004-2005, r= 0.37, t(624)=  10.06, two-tailed p<0.01. For 2005-2006, r= 0.23, t(760) = 6.65,  two-tailed p<0.01. Recent work has suggested that confusion  impacts learning differently, depending on whether it is resolved  [16], and that in some situations, confusion can be beneficial for  learning [17]. The finding here accords with those papers,  suggesting that confusion can be positive if it occurs on items  designed to resolve that confusion.    121    Frustration had a positive correlation to learning, both for original  items and scaffolding items. For original items, for 2004-2005, r=  0.31, t(624)= 8.01, two-tailed p<0.01. For 2005-2006, r= 0.20,  t(760) = 5.74, two-tailed p<0.01. For scaffolding items, for 2004- 2005, r= 0.26, t(624)= 6.78, two-tailed p<0.01. For 2005-2006, r=  0.22, t(760) = 6.34, two-tailed p<0.01. This finding is unexpected.  Past research has suggested little relationship between frustration  and learning [12, 21], contrary to hypotheses of a negative  correlation. One possibility is that frustration in ASSISTments  shows up in teacher reports in terms of negative performance, and  that these students receive greater support from their teachers.  Clearly, it will be valuable to follow up and study this unexpected  result further.    Gaming the system had a negative correlation with learning  outcomes.  For original items, for 2004-2005, r=  -0.43, t(624)= - 11.92, two-tailed p<0.01. For 2005-2006, r= -0.30, t(760) = -8.71,  two-tailed p<0.01. For scaffolding items, for 2004-2005, r=  -0.33,  t(624)= -11.92, two-tailed p<0.01. For 2005-2006, r= -0.25, t(760)  = -8.71, two-tailed p<0.01. These findings match previous  evidence that gaming is associated with poorer learning [1, 9].    The relationship between off-task behavior and learning was  unstable between years, and weak in all cases. It varied between  positive and negative, between years. For original items, for 2004- 2005, r=  0.15, t(624)= 3.74, two-tailed p<0.01. For 2005-2006, r=  -0.01, t(760) = -0.18, two-tailed p=0.86. For scaffolding items, for  2004-2005, r=  -0.17, t(624)= 4.31, two-tailed p<0.01. For 2005- 2006, r= -0.11, t(760) = -2.99, two-tailed p<0.01. It is not clear  why the relationships between off-task behavior and learning were  inconsistent between years.    3.1 Affect by Test Proficiency Category  Within this section, we ask: based on the results above (as well as  prior research), are successful students mostly in a state of  engaged concentration Are unsuccessful students mostly gaming  the system To answer these questions we plot the affective state  estimates by test proficiency category to reveal the dominant  affective states with respect to test outcomes.  Figure 2 plots the state test proficiency category against the  average estimate of affect on original questions for all students in  that proficiency category. This is an average of the same  probability estimates calculated in section 2.2.1. Note that these  are the summarized affect estimates and therefore do not  necessarily add to one. Non-summarized estimates may also not  add to one because separate classifiers were used for each affect  detector. While a multi-nominal classifier would guarantee a  summing to one of predictions for each clip, it would not  guarantee a more accurate prediction overall, particularly for  underrepresented classes.  We can observe from Figure 2, that the top affective state on  original questions among failing students was boredom followed  by engaged concentration and frustration. The margin between  boredom and engaged concentration narrows until there is equal  parts of the two, including frustration, among students scoring in  the Proficient category. For students scoring in Advanced, a  category which earns the students a college scholarship,  frustration is unexpectedly the most probable affective state,  trading places with boredom and with engaged concentration  splitting the difference. This graph suggests that equal parts  engaged concentration and boredom on the system is estimated of  students who pass the test with proficient. If estimates of boredom  increase over concentration, this may be a signal for the teacher to  provide a motivational intervention.  The position of frustration,  on the other hand, is somewhat surprising, raising the question of   whether students react with frustration instead of boredom in  response to material they find too easy.      Figure 2. Probability of affect on original questions by test score  category (average of both years data)     Figure 3. Probability of affect on scaffolds by test score category  (average of both years data)   The breakdown of affective state estimation on scaffold questions,  shown in Figure 3, shows similarities to Figure 2 with frustration,  engaged concentration and boredom being the most probable  affective states. One difference is that frustration is dominant  across all test proficiency categories, and engaged concentration  and boredom showing little to no difference in probability  between one another. On original questions, the interesting  interaction was engaged concentration and frustration increasing  in probability over boredom with higher scoring students. On  scaffolds, the interesting interaction is among gaming, off-task  behavior, and confusion. Among failing students, gaming is  strongest, followed by off-task behavior, and then confusion. As  the proficiency level increases, off-task and confusion become  more probable as gaming becomes less common than they are.  There are equal parts of these three states at the proficient level  much like there were equal parts frustration, engaged  concentration and boredom at the proficient level for original  questions. The takeaway for teachers here may be that gaming is  generally undesirable, but confusion is not entirely problematic   successful students experience confusion on scaffolding items  (perhaps because they are engaging with the material rather than  disengaging by gaming the system).    Curiously, once again, highly successful students become  frustrated more often on scaffolding items than less successful   0.1  0.2  0.3  0.4  0.5  0.6  0.7  Failing Needs Improvement  Proficient Advanced  Pr ob  ab ili ty    o f   A  ffe ct  State   Test   Outcome   Category  Affect   on   Original   Questions  Frustration  Concentration  Boredom  Off-task  Gaming  Confusion  0.1  0.2  0.3  0.4  0.5  0.6  0.7  Failing Needs Improvement  Proficient Advanced  Pr ob  ab ili ty    o f   A  ffe ct  State   Test   Outcome   Category  Affect   on   Scaffold   Questions  Frustration  Concentration  Boredom  Off-task  Gaming  Confusion  122    students. It may be that in these cases, students become annoyed  and then frustrated at receiving scaffolding after making a  mistake; or it may be that they are frustrated with themselves  when they do not succeed. Higher levels of frustration may reflect  a higher level of student emotional investment or pride in  mastering the knowledge required to answer the problem. Since  the problem sets used by students in these years of the tutor gave a  random sampling of 8th grade skills, it is conceivable that this  random ordering was a significant source of reasonable frustration  for high and low proficiency students alike.    There is an observable difference in the magnitudes of affect  estimates on originals and scaffolds. Table 4 quantifies this  difference by calculating the estimate on scaffolds subtracted by  the estimate on originals for each proficiency category. The  average of these values across categories is shown in Table 4  along with the standard deviation among the four categories. If the  shape of the trend line curve stays the same but is off-set from  Figure 1 to Figure 2 uniformly across categories, this will result in  an average difference but zero standard deviation. A high standard  deviation indicates that the change in affect between scaffolds and  originals is not of uniform magnitude across categories.    Table 4. Scaffold estimate subtracted by Original affect estimate  and standard deviation across proficiency categories   Affect    Std.    Avg.     Frustration    0.0142    0.1543     Confusion    0.0404    0.0566     Concentration    0.0165    0.0365     Boredom    0.0301    0.0333     Gaming    0.0262    -0.0286     Off-task    0.0067    -0.0778       Table 5. Difference between Scaffold and Original affect estimates  with the highest standard deviation across the proficiency categories   Affect    Failing    Needs    Imp.     Proficient    Advanced    Std.     Confusion    0.0205    0.0323    0.0626    0.1111    0.0404     Boredom    0.0037    0.0183    0.0376    0.0735    0.0301     Gaming    -0.0008    -0.0191    -0.0313    -0.0631    0.0262        Table 4 shows that students are more likely to be frustrated in  scaffolding than when answering original questions. Frustration  increases by 0.1543 on average, the most of the affective states.  This increase is fairly uniform across proficiency categories with  a standard deviation of only 0.0142. The estimates of Confusion,  Concentration and boredom increase in the Scaffolds but to a far  lesser degree than Frustration. Gaming and Off-task behavior  estimates decrease in Scaffolding. The change in these estimates  were uniform across proficiency categories, indicated by the low  standard deviation. The states with the highest standard deviation,  shown in Table 5, although still low, were; Confusion, Boredom,  and Gaming. The increase in Confusion on Scaffolds was greater  as the proficiency level increased, with Failing students showing a  0.0205 increase and Advanced students showing a 0.1111  increase. A similar, lower magnitude, trend was observed for  Boredom. A decrease in Gaming was observed with increasing   magnitude as proficiency level increased. Boredom and Confusion  change from being negatively correlated with proficiency on  Originals to being positively correlated with proficiency in  Scaffolds, as shown in Table 3. With this kind of change we  would expect to see a variance in the change in estimates across  proficiencies for these states.   4. CONCLUSIONS  In this paper, we evaluate the relationship between affect in a  tutoring system over the course of a year, to performance on an  end of year high stakes test. Differentiating affect on original  problems versus scaffolding help problems elicited interesting  results, in terms of boredom and confusion. Students who were  bored or confused while answering the main problems, tended to  do poorly on the test; however, boredom and confusion on  scaffolding were associated with positive performance on the test.  Gaming the system was, as expected, associated with poorer  learning, while off-task behavior was not consistently associated  with poorer learning. One unexpected finding was a positive  relationship between frustration and learning, which should be  investigated further. These findings are clearly not yet conclusive,  representing just a single online learning environment; but the  methodological step that they represent  enabling analysis of  affect that is both longitudinal and fine-grained, in the service of  understanding the relationships between affect and learning  is a  potentially valuable step. The data set produced through the  application of these detectors is amenable to considerable further  analysis of the ways that the context of affect influences learning.  This will be a productive and valuable area for future work.    Overall, these findings may be useful in the design of reporting on  student behavior and affect for teachers using systems like the  ASSISTments system. When reporting on student boredom and  confusion, it will be important to report context as well. For  example, it may be useful to recommend interventions to teachers  if a student is bored or confused on original questions, but not if  these affective states occur during scaffolding. We see this work  as leading in the direction of better support for teachers on  intervening based on students affect. Real time integration of  affect detection into a teachers tutor dashboard along with an  expanded understanding of the conditions that can make an  affective state constructive or not, could greatly assist a teacher in  signaling when to intervene in a crowded classroom.   5. ACKNOWLEDGEMENTS  We would like to thank Neil Heffernan for sharing the  ASSISTments data with us, and access to ASSISTments classes,  Adam Nakama, Adam Goldstein, and Sue Donas, for their  participation and support in the original data collection, and  support from the Bill and Melinda Gates Foundation, award  #OPP1048577 and the NSF, award #DRL-1031398.   6. REFERENCES  [1]  Aleven, V., McLaren, B., Roll, I., and Koedinger, K. 2004.   Toward tutoring help seeking: Applying cognitive modeling  to meta-cognitive skills. In J. C. Lester, R. M. Vicario, and F.  Paraguau (Eds.), Proceedings of Seventh International  Conference on Intelligent Tutoring Systems, ITS 2004, 227- 239.    [2]  Arnold, K.E. 2010. Signals: Applying Academic Analytics.  Educause Quarterly, 33, 1.    [3]  Baker, R.S.J.d. 2007. Modeling and Understanding Students'  Off-Task Behavior in Intelligent Tutoring Systems. In   123    Proceedings of ACM CHI 2007: Computer-Human  Interaction, 1059-1068.    [4]  Baker, R.S.J.d., D'Mello, S.K., Rodrigo, M.M.T., and  Graesser, A.C. 2010. Better to Be Frustrated than Bored: The  Incidence, Persistence, and Impact of Learners' Cognitive- Affective States during Interactions with Three Different  Computer-Based Learning Environments. Intl. J. Human- Computer Studies. 68, 4, 223-241.   [5]  Baker, R.S.J.d., Goldstein, A.B., and Heffernan, N.T. 2011.  Detecting Learning Moment-by-Moment. International  Journal of Artificial Intelligence in Education. 21, 1-2, 5-25.   [6]  Baker, R.S.J.d., Gowda, S., Corbett, A.T. 2011. Towards  predicting future transfer of learning. In Proceedings of 15th  International Conference on Artificial Intelligence in  Education, 23-30.   [7]  Baker, R.S.J.d., Gowda, S.M., Wixon, M., Kalka, J., Wagner,  A.Z., Salvi, A., Aleven, V., Kusbit, G., Ocumpaugh, J., and  Rossi, L. 2012. Towards Sensor-Free Affect Detection in  Cognitive Tutor Algebra. In Proceedings of the 5th  International Conference on Educational Data Mining, 126- 133.   [8]  Bartel, C.A., and Saavedra, R. 2000. The Collective  Construction of Work Group Moods. Administrative Science  Quarterly 45, 2, 197-231.   [9]  Cocea, M., Hershkovitz, A., Baker, R.S.J.d. 2009. The  Impact of Off-task and Gaming Behaviors on Learning:  Immediate or Aggregate In Proceedings of the 14th  International Conference on Artificial Intelligence in  Education, 507-514.   [10] Cohen, J. 1960. A coefficient of agreement for nominal  scales. Educational and Psychological Measurement 20, 1,  37-46.   [11]  Conati, C., and Maclaren, H. 2009. Empirically building and  evaluating a probabilistic model of user affect. User  Modeling and User-Adapted Interaction 19, 3, 267-303.   [12] Craig, S. D., Graesser, A. C., Sullins, J., and Gholson, B.  2004. Affect and learning: an exploratory look into the role  of affect in learning. Journal of Educational Media 29, 241 250   [13]  DMello, S.K., Craig, S.D., Witherspoon, A. W., McDaniel,  B. T., and Graesser, A. C. 2008. Automatic Detection of  Learners Affect from Conversational Cues. User Modeling  and User-Adapted Interaction 18, 1-2, 45-80.   [14]  Feng, M., Heffernan, N.T., and Koedinger, K.R. 2009.  Addressing the assessment challenge in an Intelligent  Tutoring System that tutors as it assesses.  Journal of User  Modeling and User-Adapted Interaction, 19, 243-266.   [15]  Hanley, J., and McNeil, B. 1980. The Meaning and Use of  the Area under a Receiver Operating Characteristic (ROC)  Curve. Radiology 143, 29-36.   [16]  Lee, D.M., Rodrigo, M.M., Baker, R.S.J.d., Sugay, J.,  Coronel, A. 2011. Exploring the Relationship Between  Novice Programmer Confusion and Achievement. In  Proceedings of the 4th bi-annual International Conference  on Affective Computing and Intelligent Interaction.   [17]  Lehman, B., DMello, S. K., and Graesser, A. C. 2012.  Confusion and Complex Learning during Interactions with  Computer Learning Environments. The Internet and Higher  Education, 15, 3, 184-194.   [18]  Pardos, Z.A., Wang, Q. Y., Trivedi, S. 2012. The real world  significance of performance prediction. In Proceedings of the  5th International Conference on Educational Data Miningm,  192-195   [19]  Pekrun, R., Goetz, T., Titz, W., and Perry, R. P. 2002.  Academic emotions in students self-regulated learning and  achievement: A program of quantitative and qualitative  research. Educational Psychologist, 37, 91106.   [20]  Planalp, S., DeFrancisco, V.L., and Rutherford, D. Varieties  of Cues to Emotion in Naturally Occurring Settings. 1996.  Cognition and Emotion 10, 2, 137-153.    [21]  Rodrigo, M.M.T., Baker, R.S., Jadud, M.C., Amarra,  A.C.M., Dy, T., Espejo-Lahoz, M.B.V., Lim, S.A.L., Pascua,  S.A.M.S., Sugay, J.O., Tabanao, E.S. 2009. Affective and  Behavioral Predictors of Novice Programmer Achievement.  In Proceedings of the 14th ACM-SIGCSE Annual Conference  on Innovation and Technology in Computer Science  Education, 156-160.   [22]  Sabourin, J., Mott, B., and Lester, J. Modeling Learner  Affect with Theoretically Grounded Dynamic Bayesian  Networks. 2011. In Proceedings of the 4th International  Conference on Affective Computing and Intelligent  Interaction, 286-295.   [23]  San Pedro, M.O.C., Baker, R., Rodrigo, M.M. 2011.  Detecting Carelessness through Contextual Estimation of  Slip Probabilities among Students Using an Intelligent Tutor  for Mathematics. In Proceedings of 15th International  Conference on Artificial Intelligence in Education, 304-311.   [24]  Sayette, M.A., Cohn, J.F., Wertz, J.M., Perrott, M.A., and  Parrott, D.J. 2001. A psychometric evaluation of the facial  action coding system for assessing spontaneous expression.  J. Nonverbal Behavior 25, 3.   [25] Van Rijsbergen, C.J. 1974. Foundation of evaluation. Journal  of Documentation, 30, 365-373.              124      "}
{"index":{"_id":"18"}}
{"datatype":"inproceedings","key":"Vatrapu:2013:ESN:2460296.2460321","author":"Vatrapu, Ravi and Reimann, Peter and Bull, Susan and Johnson, Matthew","title":"An Eye-tracking Study of Notational, Informational, and Emotional Aspects of Learning Analytics Representations","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"125--134","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460321","doi":"10.1145/2460296.2460321","acmid":"2460321","publisher":"ACM","address":"New York, NY, USA","keywords":"affordances, computer supported collaborative learning (CSCL), learning analytics, open learner models representational guidance, teaching analytics","abstract":"This paper presents an eye-tracking study of notational, informational, and emotional aspects of nine different notational systems (Skill Meters, Smilies, Traffic Lights, Topic Boxes, Collective Histograms, Word Clouds, Textual Descriptors, Table, and Matrix) and three different information states (Weak, Average,  Strong) used to represent student's learning. Findings from the eye-tracking study show that higher emotional activation was observed for the metaphorical notations of traffic lights and smilies and collective representations. Mean view time was higher for representations of the average informational learning state. Qualitative data analysis of the think-aloud comments and post-study interview show that student participants reflected on the meaning-making opportunities and action-taking possibilities afforded by the representations. Implications for the design and evaluation of learning analytics representations and discourse environments are discussed.","pdf":"An Eye-Tracking Study of Notational, Informational, and  Emotional Aspects of Learning Analytics Representations     Ravi Vatrapu1, 2, Peter Reimann3, Susan Bull4, and Matthew Johnson4   1Computational Social Science Laboratory (CSSL), ITM, Copenhagen Business School, Denmark  2Norwegian School of Information Technology (NITH), Norway   3MTO Psychologische Forschung und Beratung, Germany  4 Electronic, Electrical and Computer Engineering, University of Birmingham, United Kingdom  vatrapu@cbs.dk, preimann.undefined@gmail.com, s.bull@bham.ac.uk, johnsmdy@bham.ac.uk         ABSTRACT  This paper presents an eye-tracking study of notational,  informational, and emotional aspects of nine different  notational systems (Skill Meters, Smilies, Traffic Lights,  Topic Boxes, Collective Histograms, Word Clouds, Textual  Descriptors, Table, and Matrix) and three different  information states (Weak, Average, & Strong) used to  represent students learning. Findings from the eye-tracking  study show that higher emotional activation was observed  for the metaphorical notations of traffic lights and smilies  and collective representations. Mean view time was higher  for representations of the average informational learning  state. Qualitative data analysis of the think-aloud comments  and post-study interview show that student participants  reflected on the meaning-making opportunities and action- taking possibilities afforded by the representations.  Implications for the design and evaluation of learning  analytics representations and discourse environments are  discussed.    ACM Classification Keywords  H.5.3 Group and Organization Interfaces: Theory and  models, Asynchronous interaction Collaborative computing,  Evaluation/methodology; H.1.2 User/Machine Systems:  Software Psychology.   Author Keywords  Learning analytics, teaching analytics, computer supported  collaborative learning (CSCL), open learner models  representational guidance, affordances   GENERAL TERMS  Design, Human Factors, Theory   INTRODUCTION  One of the core concerns of research in learning analytics,  teaching analytics, and open learner models (OLM) is the  design, development, and evaluation of methods and tools  for visualizing students and teachers learning and teaching  processes and products. Learning analytics is the use of  intelligent data, learner-produced data, and analysis models  to discover information and social connections, and to  predict and advise on learning.1 Teaching analytics [23-25]  as a sub-field of learning analytics is concerned with the  design, development, and evaluation of visual analytics  methods and tools for teachers pedagogical decision- making. An  open learner model  is a learner model that  can also be externalised to the user [4]. This externalised  (open) learner model may be simple or complex in format  using, for example: text, skill meters, concept maps,  hierarchical structures, animations [3]. Since learners and  teachers perceive and act upon representations of their  learning and teaching in these systems, conceptual and  empirical attention needs to be devoted to notational,  emotional, informational and interactive aspects of the  underlying representations.   This paper presents the study of notational, emotional, and  informational aspects of nine different kinds of learning  analytics representations Skill Meters, Smilies, Traffic  Lights, Topic Boxes, Collective Histograms, Word Clouds,  Textual Descriptors, Table, and Matrix). The  representations are referred to as Static Representations  as they only present snapshot views of three knowledge  states (Weak, Average and Strong) without offering any  interactive capabilities to the student participants.    The remainder of the paper is organized as follows. The  Theoretical Framework section to follow presents and  discusses three lines of conceptual and empirical work  relevant to the design and evaluation of representations.  Methodology section presents details on the experimental  study design, participant recruitment, sampling and                                                              1http://www.elearnspace.org/blog/2010/08/25/what-are- learning-analytics   Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than ACM  must be honored.   Abstracting with credit is permitted. To copy otherwise, to republish,  to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK '13, April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   125    assignment, materials, task, and the protocol. The section  on Results presents the eye-tracking findings and  qualitative observations from the exit interviews.  Substantive interpretations and implications for the design  of learning analytics representations are reported in the  Discussion section.   THEORETICAL FRAMEWORK  Representation as a proxy to information plays a crucial  role in design in general. The nature of representations,  their structures and interactions is one of the central  concerns of cognitive science [26]. Philosophically  speaking, the function of representation is to re-present.  Representation, in the philosophy of mind sense of the  term, is something that stands for something else.  Representations employed in learning analytics, teaching  analytics, and open learner models re-present the ongoing  learning processes and artifacts of the individual student  and/or group of students.  The technological, interactional,  social and pedagogical aspects of representations have  received significant conceptual and empirical attention in  the fields of human computer interaction and learning  sciences. Three following lines of conceptual and empirical  work are particularly relevant for the design, development,  and evaluation of representations in learning analytics,  teaching analytics, and open learner models.     Perception and Appropriation of Socio-Technical  Affordances [21, 22]    Representational Guidance [16, 19, 20]   Cognitive Dimensions of Notations [1, 10]   Perception and Appropriation of Socio-Technical  Affordances  The notion of affordance was introduced by J. J. Gibson  [9]. Gibson was primarily concerned with providing an  ecologically grounded explanation to visual perception. By  drawing upon ecological psychology research on  affordances, Vatrapu [21] defined a socio-technical  affordance as    action-taking possibilities and meaning-making  opportunities in a socio-technical system relative  to actor competencies and system capabilities.   With regard to learning analytics representations,  Perception of Affordances (PoA) refers to the action-taking  possibilities and meaning-making opportunities that  become available (that is, perceivable) to students in a  given situation. Appropriation of Affordances (AoA) refers  to the intentional utilization of the action-taking  possibilities. AoA refers to the enactment of an  interactional practice (generative, creative, or  transformative). The eye-tracking experimental study  reported here focused on the Perception of Affordances  aspect of the phenomena. As such, the key issues of study  were to what extent were the nine different kinds of OLM  representations meaningful and actionable to the students.    Representational Guidance  The central premise of the representational guidance line of  work is articulated by Suthers (2001) as:    The major hypothesis of this work is that variation  in features of representational tools used by  learners working in small groups can have a  significant effect on the learners knowledge- building discourse and on learning outcomes. The  claim is not merely that learners will talk about  features of the software tool being used. Rather,  with proper design of representational tools, this  effect will be observable in terms of learners talk  about and use of subject matter concepts and  skills.    The above hypothesis follows from two lines of reasoning.  First, the guiding ontological dimensions of  representations constraint and salience prompt a user  for what is missing as well for what is present [18]. The  ontological dimensions of representations are not  intrinsically social. Second, external representations play a  role in guiding collaborative learning by amplifying certain  kind of social interactions [19] and knowledge building  interactions [20].    Definition of Representational Guidance  Representational guidance  refers to how these  software environments facilitate the expression  and inspection of different kinds of information.  [17]      Figure 1: Schematic of Representation Guidance   Figure 1 above, taken from Suthers [18] indicates that  representational guidance has tripartite origins in the (a)  affordances of a representational notation, (b) in how that  notation is realized in a representational tool such as  software, and (c) in the actual configuration of  representational artifacts created by users of that tool.    126    How representational notations (such as Smilies, Word  Clouds, and Traffic Lights etc.) are realized in the software  and the actual configuration of representational artifacts are  issues of concern for the design and evaluation of learning  analytics systems. The eye-tracking study reported here  primarily deals with the affordances (meaning-making  opportunities and action-taking possibilities) of the nine  different kinds of representational notations for learning  analytics systems in three different information states.    Cognitive Dimensions of Notations  The cognitive dimensions framework [1, 10] is relevant to  understanding the notational aspects of the learning  analytics systems as it provides insights into the ontological  characteristics of notations and their potential pedagogical  implications. Further, Gibson's ecological optics [9] and  Green and Blackwell's cognitive dimensions [1] share  conceptual terms such as medium and environment. he next  section presents key concepts in the cognitive dimensions  framework and discusses their relevance to learning  analytics systems. The following definitions are taken from  Green and Blackwell [11]:     Information Artefacts:  the tools we use to store,  manipulate, and display information  (p.5)   Information artefacts are further classified as non- interactive artefacts and interactive artefacts. Learning  analytics representations are information artefacts and the  static representations studied here are an example of non- interactive artefacts.    Environment:  The environment contains the  operations or tools for manipulating those marks  (p.8).  The environments in the learning analytics systems are  the various dashboards designed for the different  stakeholders.     Medium:  The notation is imposed upon a medium,  which may be persistent, like paper, or evanescent, like  sound (p.8). In the case of learning analytics systems,  the medium is persistent and dynamically changed.    advance (p.10)   Students and teachers engage in all four kinds of user  activities listed above with learning analytics systems. The  nine different kinds of representations have been selected  with Exploratory Design in mind with the student  participants engaging in all four activities as detailed in the  Methodology section.    Definitions of Cognitive Dimensions   Abstraction:  An abstraction is a class of entities, or a   grouping of elements to be treated as one entity, either  to lower the viscosity or to make the notation more like  the users conceptual structure  (p.24)    Closeness of Mapping:  Closeness of representation to  domain  (p.39)    Consistency:  similar semantics are expressed in  similar syntactic forms (p.39)    Diffuseness:  verbosity of language  (p.39)   Error-Proneness:  notation invites mistakes  (p.40)   Hard Mental Operations:  high demand on cognitive   resources  (p.40)   Hidden Dependencies:  A hidden dependency is a   relationship between two components such that one of  them is dependent on the other, but that the  dependency is not fully visible  (p.17)    Premature Commitment:  Constraints on the order of  doing things force the user to make a decision before  the proper information is available  (p.21)    Progressive Evaluation:  work-to-date can be checked  at any time  (p.40)    Provisionality:  degree of commitment to actions or  marks  (p.41)    Role-Expressiveness:  the purpose of a component (or  an action or a symbol) is readily inferred  (p.41)    Secondary Notation:  Extra information carried by  other means than the official syntax  (p.29)    Viscosity:  Resistance to change: the cost of making  small changes (p.12)    Visibility:  ability to view components easily. (p.34)   Juxtaposability:  ability to place any two components   side by side (p.34)   The nine different kinds of learning analytics  representations selected and evaluated for this study  embody and exemplify at least one of the above listed  cognitive dimensions of notations. For example, the Textual  Representations exemplify verbosity of language  (Diffuseness), Traffic Lights and Smilies are high on  Abstraction but also high on Role-Expressiveness,  Skillmeters provide Progressive Evaluation, Horizontal and  Vertical Tables offer Closeness of Mapping to the domain  of formative assessment, Collective Bar Chart  representation embodies the Juxtaposition dimension.   METHODOLOGY   Experimental Design  Given the central role that motivation plays in learning [6,  7] and the use of representations for formative assessment  practices, emotional impacts of representations are of  research interest with implications for practice. The  exploratory research question is stated below:    RQ1: What, if any, are the relationships between  notational, informational, and emotional aspects of   127    different kinds of representations of learning and  teaching processes and products     To empirically answer this exploratory research question,  we designed a controlled a laboratory study of a selected set  of nine different kinds of learning analytics representations  with varying notational aspects but isomorphic  informational aspects. It is to be noted that the nine  different notational systems selected for the study are not an  exhaustive list. The research objectives were to investigate  how students perceived different kinds of representations  and what, if any, are the differences in emotional arousals  between them. The nine different kinds of representations  are presented in the materials section below.   Materials  As mentioned earlier, nine different kinds of notational  systems (Skill Meters, Smilies, Traffic Lights, Topic Boxes,  Collective Histograms, Word Clouds, Textual Descriptors,  Table, and Matrix) were used to generate three  informational states (Weak, Average, & Strong) of the  learning analytics representations. For each of the nine  notational systems, three isomorphic representations were  created to embody the three informational states of the  individuals learning state. Thus, a total of twenty seven (9  notational systems x 3 informational states = 27) different  static representations were developed. Further, we  developed domain-specific (business and engineering) and  domain-generic (nondescript) versions of the 27  representations. The domain-specific representations for  Business subject area were used as the study sample  consisted of international undergraduate business students  at the Copenhagen Business School in Denmark. The 27  representations are presented below   Skill Meters (Weak, Average, Strong)         Smilies     Traffic Lights     Topic  Boxes     Collective Histograms      128    Word Clouds     Textual Descriptors     Table     Matrix      Participants  The sampling frame for study was the student population of  the International Summer University Program (ISUP) 2011  the Copenhagen Business School, Denmark. Study  recruitment was done by an email solicitation sent by the  program secretariat to all enrolled students. Participants  expressed their interest and indicated their availability on  the study registration form.  A total of 15 students  participated in the study. The gender composition was 6  males and 7 females.    Tasks  Participants were instructed to talk aloud about what sense  they can make of the representations displayed on the  screen and their subjective preferences of the nine different  notational systems while playing one of the four roles  (themselves, a close friend taking the same course, a  classmate who is not a close friend, and the teacher of the  course).    3.5 Procedure    Pre-Investigative Session   Participant was welcomed and seated in the laboratory.   They were reminded that they are about to participate  in an eye-tracking study and a short interview will be  conducted after the study.    An informed consent form was given. Participants  were explicitly informed that it is the software that is  being tested and not them. Further, participants were  explicitly informed that they may withdraw consent at  any time during the study session. Participants were  informed that they would still be compensated with a  movie ticket coupon in case they stop participating  before completing the study session.    A copy of the informed consent form was given to the  participants after they signed it. An anonymized  session code was then assigned.    This concluded the Pre-Investigative Session    Investigative Session   The participant was seated in front of the eye-tracker   and the position was adjusted so that participant eyes  were visible in the eye-finder of  SMI iView X eye- tracker device2  driver software and iMotions Attention  Tool 4.13 study software     9-point eye calibration was then conducted followed by  light calibration                                                               2http://www.smivision.com/en/gaze-and-eye-tracking- systems/products/red-red250-red-500.html   3 http://www.imotionsglobal.com/    129    28.4329.1330.4733.0336.07 38.0745.2746.80  53.57  0 20 40 60  M ea n Vi ew  T im  e (in   se co nd  s)  SROLM:NotationalSystems  34.81 41.42 37.38  0 10 20 30 40 50 60  Weak Average StrongM ea n Vi ew  T im  e (in   se co nd  s)  SROLM:InformationState   The study session consisted of a randomized  presentation of the 27 static representations.    Post-Investigative Session   A brief open-ended interview was conducted about the   participants study experiences, reflections, and  subjective preferences.     The participant was then given the movie ticket coupon  and a signature of receiving the movie ticket coupon  obtained. The participant was then thanked and shown  out of the laboratory. This concluded the Study  Session.   RESULTS  Eye-tracking data analysis was conducted at the aggregate  level for each of the 27 static representations of open  learner models. Three different kinds of analysis of the eye- gaze data were conducted using the iMotions Attention  Tool 4.1 software: (a) emotional activation, (b) heatmaps,  and (c) area of interest (AOI) analysis. Emotional activation  is calculated based on the changes in participants pupil  diameters [2, 13]. Emotional activation measures the level  of arousal and engagement towards the stimulus image. The  higher the emotional activation measure, greater the  emotional impact of that learning analytic representation.  Heatmap presents the spatial distribution of students gaze  on a particular representation.  Heatmaps are composite  images that contain an overlay of a gradient colour layer on  the stimulus image (in our case, one of the 27  representations corresponding to the nine different  notational systems and three different informational states)  with areas of the stimulus image that received a greatest  allocation of students gaze ranging from red to yellow and  with areas that received the least gaze allocation ranging  from yellow to green. The heatmaps presented below are  static images of the aggregate gaze distribution on a  particular image for all respondents. The Area of Interest  (AOI) analysis was conducted on regions of the images that  were of particular importance from pedagogical and/or user  interface design perspectives.   The results section is organized as follows.  First, findings  about mean view time and mean emotional activation are  presented. Second and last, for each of the 27 stimulus  images, descriptive statistics, emotional activation,  heatmap, and AOI results are presented and important  observations are discussed.   Mean View Time  As can be seen from Figure 2, mean view time was the  highest for the Collective Histogram notation followed by  the Skillmeter and the Word Cloud representations. As can  be seen from Figure 3, average view time was highest for  the average learning representations compared to the Strong  and the Weak learning state representations across all nine  notational systems.       Mean Emotional Activation  As mentioned earlier, emotional activation is calculated  based on the changes in participants pupil diameters. The  higher the emotional activation score, greater the emotional  impact of it. Figure 4 presents the average emotional  activation for the nine notational systems.     The least preferred notation of Word Cloud also received  the least emotional activation. The Traffic Lights  representations received the highest emotional activation  followed by the Collective Histogram and Smilie notations.   Figure 5 presents the average emotional activation for the  three information states (weak, average, and strong). Unlike  mean view time, no differences in emotional activation  were found across the three information states.          2.57 3.50 3.50 3.60 3.60 3.80 4.23 4.80  6.10  01 23 45 67 89 10  Em ot io na  lA ct iv at io n  SROLM:NotationalSystems  4.07 4.01 3.82  0 1 2 3 4 5 6 7 8 9 10  Weak Average StrongM ea n Em  ot io na  l Ac  tiv at io n  SROLM:InformationState  Figure 3: Mean View Time across Information States  Figure 4: Mean Emotional Activation for Notational Systems   Figure 5: Emotional Activation across Information States   Figure 2: Mean View Time for the Nine Notations   130    Heat Map Analysis   Figures 6, 7, and 8: Heatmap Analysis of Skillmeters      Heatmap analysis showed that regions with greater  information variance receive higher aggregate gaze  allocation. In the Average Information State for the  Notational System of Skillmeters (Figure 6), the gaze  distribution is between the topic names, the skillmeter bars  and the legend. In the Strong case (Figure 7), the hotspot  is at the boundary between the green and the white areas of  the topic. In the Weak case (Figure 8), the aggregate gaze  distribution is greater around the region representing  misconceptions in the individual students current  knowledge (the red colored bar)   Area of Interest Analysis         Figures 9, 10, and 11: AOI Analysis of Smilies   Regions corresponding to the Smilies Notational Systems in  the main learning representation and the legend were  selected for Area of Interest Analysis. AOI results show  that participants found disambiguation of the Excellent and  Very Good Smilies to be an issue. This was confirmed by  the analysis of talk-aloud and semi-structured interviews  data. Disambiguation of the OK and Weak Smilies were  also problematic. Figures 9, 10, and 11 present the AOI  results for the Average, Strong, and Weak information  states of the Smilies representation.   131    DISCUSSION  Mean view time was the highest for the Collective  Histogram notation followed by the Skillmeter and the  Word Cloud representations. An analysis of the talk-aloud  and post-investigative session interviews shows that  participants found the Collective Histogram to be  informative but challenging initially. It was informative  because participants could perceive the individuals  learning within the context of the whole class and it was  challenging as it required additional decoding of the social  significance of the relative positioning of the individual  student with respect to the collective of the classroom.  Further, Word Cloud and Skillmeter were the two  representations that participants liked the least and have  higher view times. Unlike the Collective Histogram, the  primary reason here is the difficulty of making sense of the  representations for action-taking. The Traffic Lights and  Smilie metaphors received relatively lower view time than  the Word Cloud, Skillmeter and Collective Histogram.  Average view time was the lowest for the representations  that participants described with phrases such as simple,  easy and straightforward during the talk-aloud and post- investigative session interviews. Given the prevalence of  the Skill Meter notations in gaming, and in the context of  current arguments about the gamification of learning [5,  8, 12, 14, 15], it is interesting that Skill Meters  underperform both on sense-making and satisfaction  dimensions.   Mean view time was highest for the average learning  representations compared to the Strong and the Weak  learning state representations across all nine notational  systems. This could be due to the fact that decoding of the  average case representations with a combination of both  weak and strong learning sub-states is cognitively more  demanding than decoding information at the extremes of  weak and strong information states.    The least preferred notation of Word Cloud also received  the least emotional activation. The Traffic Lights  representations received the highest emotional activation  followed by the Collective Histogram and Smilie notations.  Many participants felt that traffic lights and Smilies were  creative and easy to decipher. Traffic lights were also  perceived as being simplistic and not depicting the semantic  range of learning assessments from very weak to excellent  as with other notations. There were little to no differences  in emotional activation of static representations across the  three information states of Weak, Average, and strong.  Implications for the design of the nine notational systems  are discussed below.   Implications for Design of Learning Analytics Notations   Skillmeters  Participants like the Skillmeters notation as it provided  nuanced information. Several suggested adding a numeric  value to the proportion (10%, 90% and such). The utility  of   for representing uncovered curriculum areas is not clear to  some participants.   Smilies  Many participants thought that the use of Smilies was  creative. In the future, we could consider the possibility of  implementing Chernoff faces4.   Traffic Lights  Participants in general thought that the use of Traffic Lights  like the Smilies was creative. Negative comments included  the decreased range of knowledge level representations  (only Strong, Average and Weak). Some participants  suggested that the green light should be at the top as it is the  best. One participant wanted the topics to be arranged from  the strongest to the weakest so that they can know the  bottom knowledge level. Adding more colors for the  other knowledge levels is an option (Very Good and Very  Weak). The design challenge here is to extend but at the  same time preserve the metaphor of the traffic lights.   Topic Checkboxes  Topic Checkboxes are the second most disliked  representation after Word Clouds. The design issue is the  low color contrast between the different knowledge levels.  Design implications are to increase the color contrast or to  use multiple colors as in other representations (dark green  to dark red).   Class Histogram   Class Histogram had the highest mean view time but  participants found the absolute information of the  individual embedded with the relative information about the  whole class to be highly informative. The notational mark  star needs to be changed to a neutral symbol and the  legend should indicate clearly that it is a collective  representation.    Word Clouds   Word Clouds are the most disliked and the most confusing  representation. One design implication is not to repeat the  topics between good and weak understanding level and to  create just one simple Word Cloud with color coded topics.  For example greenred color range indicating positive vs.  negative category with topic size also coded for level of  understanding.   Textual Descriptions  Most participants said that this representation was not that  useful. They found the repetition of the text tedious and the  overall representation boring. Unlike other representations,  the Textual descriptors dont reveal the full scale of the  ratings. One design change to explore is to color code the  knowledge level terms.    Tables  Many participants liked the Table representation and the  mean view time was the lowest. Participants made                                                              4 (http://people.cs.uchicago.edu/~wiseman/chernoff/   132    contradictory suggestions during the debriefing interviews.  Some participants would like the vertical scale to range  from the negative to the positive (unlike the traffic lights  case) while others would keep it as it is.   Matrix  Matrix was by far the representation that most participants  find as the easiest to interpret and has the lowest mean view  time but second lowest emotional activation. Many  participants suggested that the scale should be re-organized  from the left to the right being negative to positive.   Other scale related suggestion was to add Excellent and  Unacceptable as anchors (Excellent, Very Good, Good, Ok,  Weak, Very Weak, and Unacceptable). For formative  assessment purposes, unacceptable might be too strong a  term and could be counter-productive.    Applications to the Design of Representations  A close analysis of participants talk-aloud comments  during the investigative session and their observations in  the  post-investigative session interviews shows that  learning analytics representations in themselves might be  necessary but not sufficient for supporting meta-cognituve  reflection and collaborative discourse. Within the context of  the NEXT-TELL EU project,5 the results of this eye- tracking study of representations informed the design of  representations for the Open Learner Model, the  Communication and Negotiation Tool (CoNeTo), and the  Teaching Analytics Dashboards for Repertory Grids. As  mentioned earlier, an  open learner model  is a learner  model that can also be externalised to the user [4]. This  externalised (open) learner model may be simple or  complex in format using, for example: text, skill meters,  concept maps, hierarchical structures, animations [3].  CoNeTo provides computational support for the socio- cultural process of intersubjective meaning-making  between students and teachers centered on their learning  analytics representations (in this case, from the open learner  models). Teaching analytics dashboards for RGFA allow  teachers to use visual analytics techniques to conduct  collective analysis of students personal constructs and  ratings of domain concepts from the Repertory Grids for  Formative Assessment application [23]. Figures 12, 13, and  14 present screenshots of different notations from the  NEXT-TELL Open Learner Model.                                                               5 http://www.next-tell.eu              Figures 12, 13, and 14: NEXT-TELL OLM Representations   In closing, based on the empirical findings from the study  reported here, we argue that learning analytics  representations are not always already artifacts that can  support meaning-making and action-taking. Instead,  learning analytics representations require explicit discursive  support. Towards this purpose, we suggest the design,   133    development and evaluation learning analytics systems that   facilitate artifact-centered discussion and negotiation.    ACKNOWLEDGEMENTS  This work is supported by the NEXT-TELL - Next  Generation Teaching, Education and Learning for Life  integrated project co-funded by the European Union under  the ICT theme of the 7th Framework Programme for R&D  (FP7). This document does not represent the opinion of the  EC and the EC is not responsible for any use that might be  made of its content.   REFERENCES  Blackwell, A. and Green, T. Notational systemsthe  cognitive dimensions of notations framework. HCI Models,  Theories, and Frameworks: Toward an Interdisciplinary  Science. Morgan Kaufmann (2003).  Bradley, M. M., Miccoli, L., Escrig, M. A. and Lang, P. J.  The pupil as a measure of emotional arousal and autonomic  activation. Psychophysiology, 45, 4 (2008), 602-607.  Bull, S., Gakhal, I., Grundy, D., Johnson, M., Mabbott, A.  and Xu, J. Preferences in Multiple View Open Learner  Models. In M. Wolpers, P. A. Kirschner, M. Scheffel, S.  Lindstaedt and V. Dimitrova (eds.) EC-TEL 2010, (2010),  476-481.  Bull, S. and Kay, J. Student Models that Invite the Learner  In: The SMILI Open Learner Modelling Framework.  International Journal of Artificial Intelligence in Education  17, 2 (2007), 89-120.  Cohen, A. The Gamification of Education. Futurist, 45, 5  (2011), 16-17.  Cordova, D. and Lepper, M. Intrinsic motivation and the  process of learning: Beneficial effects of contextualization,  personalization, and choice. Journal of Educational  Psychology, 88, 4 (1996), 715-730.  Eales, R. T. J., Hall, T. and Bannon, L. J. The motivation is  the message: comparing CSCL in different settings. In Proc.  Proceedings of the Conference on Computer Support for  Collaborative Learning: Foundations for a CSCL  Community, International Society of the Learning Sciences  (2002), 310-317.  Edmonds, S. Gamification of learning. Training and  Development in Australia, 38, 6 (2011), 20-22.  Gibson, J. J. The ecological approach to visual perception.  Houghton Mifflin, Boston, 1979.  Green, T. Cognitive dimensions of notations. People and  Computers V (1989), 443-460.  Green, T. and Blackwell, A. Cognitive Dimensions of  Information Artefacts: a tutorial. BCS HCI Conference  (1998).  Lee, J. J. and Hammer, J. Gamification in Education: What,  How, Why Bother Academic Exchange Quarterly, 15, 2  (2011), 146.  Partala, T. and Surakka, V. Pupil size variation as an  indication of affective processing. International Journal of  Human-Computer Studies, 59, 1 (2003), 185-198.  Renaud, C. and Wagoner, B. The Gamification of Learning.  Principal Leadership, 12, 1 (2011), 56-59.   Smith-Robbins, S. This game sucks: How to improve the  gamification of education. EDUCAUSE Review, 46, 1  (2011), 58-59.  Suthers, D. Representational Bias as Guidance for Learning  Interactions: A Research Agenda. In Proc. 9th World  Conference on Artificial Intelligence in Education  (AIED'97), July 19-23, 1999 (1999).  Suthers, D. The Representational Guidance Project.  Laboratory for Interactive Learning Technologies (LILT).  City, n.d.  Suthers, D. Towards a Systematic Study of Representational  Guidance for Collaborative Learning Discourse. Journal of  Universal Computer Science, 7, 3 (2001),  http://lilt.ics.hawaii.edu/lilt/papers/2001/Suthers-JUCS- 2001.pdf.  Suthers, D. and Hundhausen, C. An Experimental Study of  the Effects of Representational Guidance on Collaborative  Learning. Journal of the Learning Sciences, 12, 2 (2003),  183-219.  Suthers, D., Vatrapu, R., Medina, R., Joseph, S. and Dwyer,  N. Beyond Threaded Discussion: Representational Guidance  in Asynchronous Collaborative Learning Environments.  Computers and Education, 50, 4 (2008), 1103-1127.  Vatrapu, R. Explaining Culture: An Outline of a Theory of  Socio-Technical Interactions. Proceedings of the 3rd  International Conference on Intercultural Collaboration  (ICIC 2010) (2010), 111-120.  Vatrapu, R. Toward a Theory of Socio-Technical Interactions  in Technology Enhanced Learning Environments. In U.  Cress, V. Dimitrova and M. Specht (eds.) EC-TEL 2009,  Lecture Notes in Computer Science (LNCS) 5794, (2009),  694-699.  Vatrapu, R., Reimann, P. and Hussain, A. Towards Teaching  Analytics: Repertory Grids for Formative Assessment. In  Proc. International Conference of the Learning Sciences  (ICLS) 2012 (2012).  Vatrapu, R., Tanveer, U. and Hussain, A. Towards teaching  analytics: communication and negotiation tool (CoNeTo). In  Proc. Proceedings of the 7th Nordic Conference on Human- Computer Interaction: Making Sense Through Design, ACM  (2012), 775-776.  Vatrapu, R., Teplovs, C., Fujita, N. and Bull, S. Towards  Visual Analytics for Teachers' Dynamic Diagnostic  Pedagogical Decision-Making. Paper presented at the 1st  International Conference on Learning Analytics &  Knowledge (LAK 2011), Banff, Canada. (2011).  Winn, W. Cognitive Perspectives in Psychology. In D. H.  Jonassen (ed.) Handbook of research on educational  communications and technology, (2004), 79-112.   134      "}
{"index":{"_id":"19"}}
{"datatype":"inproceedings","key":"Ahn:2013:WLF:2460296.2460323","author":"Ahn, June","title":"What Can We Learn from Facebook Activity?: Using Social Learning Analytics to Observe New Media Literacy Skills","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"135--144","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460323","doi":"10.1145/2460296.2460323","acmid":"2460323","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, literacy, new media literacy, social learning analytics, social media","abstract":"Social media platforms such as Facebook are now a ubiquitous part of everyday life for many people. New media scholars posit that the participatory culture encouraged by social media gives rise to new forms of literacy skills that are vital to learning. However, there have been few attempts to use analytics to understand the new media literacy skills that may be embedded in an individual's participation in social media. In this paper, I collect raw activity data that was shared by an exploratory sample of Facebook users. I then utilize factor analysis and regression models to show how (a) Facebook members' online activity coalesce into distinct categories of social media behavior and (b) how these participatory behaviors correlate with and predict measures of new media literacy skills. The study demonstrates the use of analytics to understand the literacies embedded in people's social media activity. The implications speak to the potential of social learning analytics to identify and predict new media literacy skills from data streams in social media platforms.","pdf":"What Can We Learn from Facebook Activity Using Social  Learning Analytics to Observe New Media Literacy Skills      June Ahn  University of Maryland, College Park   College of Information Studies and College of Education  2117J Hornbake Building, South Wing, College Park, MD, USA   juneahn@umd.edu      ABSTRACT  Social media platforms such as Facebook are now a ubiquitous  part of everyday life for many people. New media scholars posit  that the participatory culture encouraged by social media gives  rise to new forms of literacy skills that are vital to learning.  However, there have been few attempts to use analytics to  understand the new media literacy skills that may be embedded in  an individuals participation in social media. In this paper, I  collect raw activity data that was shared by an exploratory sample  of Facebook users. I then utilize factor analysis and regression  models to show how (a) Facebook members online activity  coalesce into distinct categories of social media behavior and (b)  how these participatory behaviors correlate with and predict  measures of new media literacy skills. The study demonstrates the  use of analytics to understand the literacies embedded in peoples  social media activity. The implications speak to the potential of  social learning analytics to identify and predict new media literacy  skills from data streams in social media platforms.   Categories and Subject Descriptors  K.3.2 [Computer and Information Science Education]:  Literacy   General Terms  Measurement, Human Factors.   Keywords  Learning Analytics, Social Learning Analytics, Social Media,  Literacy, New Media Literacy.   1. INTRODUCTION  Some of the most popular websites in the world are social  networking applications such as Facebook, video sharing sites  such as Youtube, and massive, user-generated resources such as  Wikipedia. Online applications are increasingly ubiquitous, social,  and participatory [19, 29]. Against this backdrop, educators and  policymakers agree that individuals must develop so-called 21st  century skills which include critical thinking, collaboration,   communication, and information literacy [13, 26]. The concept of  21st century skills is not new, but the nature of these skills is  profoundly different in media-rich environments. For example,  collaboration is an enduring human skill, but individuals must  now be able to collaborate in environments that are mediated by  technology, across distances, with mass numbers of users, and  with easy access to information.   Literacy scholars also contribute to this dialogue by elaborating  particular skills that are important when individuals interact with  new media. For example, Jenkins [19] observes that the rise of  online communities such as Facebook facilitates a participatory  culture where individuals must develop literacies such as  networking, information appropriation, remix, judgment, and  collective intelligence. As individuals leave traces of their online  behavior in platforms such as social network sites, there are ripe  opportunities to examine whether these behaviors can be utilized  as indicators of such literacies. The emerging field of learning  analytics offers strategies to assess and examine the development  of new media literacy skills in naturalistic, online environments  [12].   Learning analytics describes different ways of collecting and  analyzing data about learner interactions within their contexts,  with the goal of understanding and optimizing learning  environments [5]. One particular opportunity is to leverage the  data individuals produce via their participation in social media  communities. Buckingham Shum & Ferguson [8] identify this  opportunity as social learning analytics, which focuses on  combining sociocultural frameworks of learning with analytics to  understand peoples learning processes and dispositions. This  study demonstrates this idea by utilizing traces of participatory  behavior that individuals enact in a popular social media platform  (Facebook) to directly analyze, measure, and predict new media  literacy skills.   In the following paper, I demonstrate the use of social media data,  culled from the popular social network site (SNS) Facebook, to  observe new media literacy skills.  First, the paper integrates ideas  about social media, sociocultural learning, and social learning  analytics. Second, I argue for the utility of using publicly  available social data from popular platforms to assess literacy  skills. Third, the paper outlines an approach that utilizes  Facebooks application programming interface (API) to collect  individuals activity data. In this study, I consider two exploratory  questions:   1. Do Facebook interactions cluster into conceptually distinct  behaviors   2. And do these behaviors correlate with and predict new media  literacy skills     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK13, April 812, 2013, Leuven, Belgium.  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   135    I report two analyses from this data: (1) an exploratory factor  analysis to examine how raw activity data may group into broader  categories of behavior, and (2) regression models that explore  whether these behaviors predict new media literacy skills. The  analyses present exploratory evidence towards the validity of  analyzing raw activity data from social media platforms, as  indicators of different literacy skills. Such indicators could be  used in a variety of ways such as to make learners cognizant of  the new media literacy skills they develop through their everyday  online activities or validate the presence and importance of 21st  century literacies as measurable outcomes for learners. The  following study is exploratory in nature, but offers implications  for the use of publicly available social data streams to assess and  predict literacy skills in situ, from an individuals natural  participation in online communities.   2. SOCIAL MEDIA, LITERACY SKILLS,  AND SOCIAL LEARNING ANALYTICS  Educators are increasingly interested in the use of social media for  learning. Scholars argue that platforms such as Facebook or  Twitter allow individuals to practice new skills and ways to  engage with learning processes [15, 16, 17]. A natural link  between social technologies and learning is the development of  new media literacy skills. Jenkins [19] observes that individuals  now participate in online communities and engage with new  technologies that encourage media creation and sharing. Todays  learner does not merely read or consume content. They create  information in a myriad of media channels and formats, such as  status updates on Facebook, tweets on the Twitter platform,  videos on Youtube, or personal blog posts. These affordances  create a participatory culture where creating, sharing, and  collaborating in online networks become vital skills for learning.  Jenkins suggests that the following literacy skills are vital when  working in new media environments:    Play  capacity to problem solve, tinker, and experiment  with ones surroundings    Performance  ability to adopt different identities to explore  and learn    Simulation  ability to construct and interpret models to  explore real-world phenomena    Appropriation  understanding of how to sample and remix  other sources in ones own work    Multitasking  capability to shift attention as needed    Distributed Cognition  skills to effectively use information  and tools to expand ones mental capacity    Collective Intelligence  ability to collaborate and pool  knowledge effectively    Judgment  skills to evaluate information sources as  credible and reliable    Transmedia Navigation  use of multiple means and  platforms to follow information streams    Networking  ability to search for, synthesize, and share  information through online networks    Negotiation  ability to participate in and adapt to different  communities   These examples highlight how literacy skills are uniquely tied to  our current environment and technological tools. For example,  networking is an enduring skill that exists in a variety of contexts.  One can network in a face-to-face setting and one can also  network in an SNS such as Facebook. The skills required in either  setting differ in many ways. New media literacies, and the related  concept of 21st century skills, describe abilities and practices that  are intimately tied to the opportunities afforded by new  technologies.   Sociocultural theories of learning help to frame how learning is  comprised of social interaction that occurs in situated settings. A  sociocultural perspective recognizes that learning is defined  through interaction with others, with tools available to the learner,  and within specific institutional and cultural constraints [20, 22].  To illustrate, one can ask the question, How do I know if a  person has learned or developed expertise in appropriation skills  The evidence to answer this question is entirely dependent on the  situated context one considers. For example, the proper practice of  citing prior research in an academic paper consists of particular  skills, cultural expectations from different communities of  scholars, and institutional norms such as publication venues and a  plethora of other drivers. Learning appropriation skills in this  context is uniquely tied to these complex social interactions,  cultural contexts, and institutions within which academic writers  work.   With new media tools, the skill of appropriation takes on entirely  new meanings. Memes that emerge and spread via Internet forums  are a common example of new definitions of appropriation.  Shirky [27] documents one such phenomenon, lolcats, which  feature images of cats with humorous or poignant captions. These  images are often shared, voted on and spread, via online social  forums. The creators of these images combine, or remix, already  existing images and words, to create combinations that exhibit  new forms of thought, humor, or social commentary. Learning  how to be a good remixer requires different appropriation skills   technical cultural, and institutional  than the appropriation  practices seen in formal academic work [3].   Sociocultural theories also recognize that artifacts, technology,  and tools mediate learning [20]. The use of language and the tools  available in a given environment are examples of how artifacts  mediate the way we learn. For example, within primary and  secondary science education classrooms, scholars observe that  cultural norms of communication styles define whether and how  one can observe learning happening [4, 22, 24]. A young person  learns science in particular ways in a formal classroom and in  other ways via everyday life activities [10, 11].   Similarly, recent studies of youths and social media highlight how  the technological language of new media environments also shape  learning. For example, Jenkins [19] identifies performance as an  important literacy skill. A good performance can be defined in  various ways depending on a situation such as acting in a stage  play, giving a speech, or participating in a classroom discussion.  Young people also readily adopt the technological tools available  to them, such as social network sites, to develop their everyday  performance skills. Researchers have found that adolescents will  actively configure their SNS profiles to perform their identities   friends, interests, hobbies, and personalities etc.  in these  online communities [6, 16].    A sociocultural framework suggests that different environments  may afford very distinct behaviors and literacy skills. This  understanding leads to natural questions of what specific literacies   136    might develop from using different social media platforms such as  Facebook. Observing and measuring these literacy skills has  traditionally been limited to qualitative observation. New media  literacy skills are difficult to quantitatively measure, predict, or  assess because they are often processes, or interactions that are  cumbersome to observe via traditional methods such as surveys  [28]. Learning analytics may aid in observing indicators of  literacy skills in natural settings.   2.1 A social learning analytics approach to  understand new media literacy  In the nascent literature of learning analytics, a few scholars have  forwarded the idea of understanding social learning in online  communities through analytics. For example, Buckingham Schum  & Ferguson [8] define the term social learning analytics to  delineate strategies to measure and render visible, and in some  cases potentially actionable, behaviours and patterns in the  learning environment that signify effective process [sic]. In  addition, Dawson [12] observes that researchers are now able to  use big data culled from online communities to measure and track  new literacy skills. These ideas arise from the fact that users  actions are now readily tracked and recorded in online platforms,  providing a direct view into some learning behaviors.   These authors highlight how online social platforms allow  researchers to utilize different analysis strategies to understand  social learning. For example, one could use social network  analysis to ascertain an individuals position in a network of  learners or network of interactions [8, 12]. Network position then  serves as a measure of influence (e.g. a person is more central in a  learning network) or perhaps collaboration (e.g. a person is  connected with diverse others). In addition, researchers could  utilize strategies such as analyzing discourse patterns in online  conversation or content analysis of user-generated media [8].   Social media sites, such as Facebook, collect detailed interaction  data. These platforms might collect user activity such as how  many messages are posted, updates shared, photos uploaded and  other user behaviors. If one takes a sociocultural learning  perspective, that these actions themselves define potential literacy  behaviors, then there is opportunity to better understand whether  social media activities could serve as general indicators of  underlying new media literacies. Traditional measurement tools,  such as surveys, might instead give way to natural observation  from digital traces left by individuals in their everyday online  activity. In this study, I consider this opportunity by exploring  whether publicly shared Facebook activity data can be used to  measure and predict new media literacy skills.   2.2 Research questions  Individuals participate in a variety of activities within social  network sites such as Facebook. Members of these communities  can create their own profiles, add and delete friends to their  networks, and browse the profiles of friends in their network [7].  In addition, users can write email messages, leave short wall posts  on friends pages, broadcast status updates, create groups, share  links, upload media such as photos and videos, and spread  information by sharing posts from ones friends. A growing  literature has documented how these affordances are used to  create diverse examples of participatory culture such as college  students using the platform to create campus groups, teenagers  learning media skills, or individuals developing social capital [1,  2, 9, 14, 16, 21].   However, it is unclear whether raw Facebook activity might  reflect specific behaviors that can be linked to new media  literacies. Thus, the first research question in this study explores  whether Facebook activity (e.g. messages sent, status updates,  photos shared etc.) coalesce in ways that suggest broader  categories of behavior:   R1: Do Facebook interactions cluster into conceptually  distinct behaviors   The first research question is exploratory and asks whether  Facebook activity can be categorized into higher-order categories  of behavior. The second research question then examines whether  Facebook activity is significantly correlated to measures of new  media literacy.   R2: Do these behaviors (in Facebook) correlate with and  predict new media literacy skills   In this analysis, I apply Jenkins [19] framework of new media  literacy skills: Play, Performance, Simulation, Appropriation,  Multitasking, Distributed Cognition, Collective Intelligence,  Judgment, Transmedia Navigation, Networking, and Negotiation.   3. METHODOLOGY  Data for this analysis was collected via a custom Facebook  application that was developed for this project. A website was  developed that presented participants with a clear consent form  detailing all research activities. Once participants gave consent  they were presented with an online survey. The survey collected  basic demographic data in addition to measures of new media  literacies and general technology literacy (described below). Once  participants completed the survey they were invited to login to  their Facebook accounts and share their public social network data  for this study. I utilized the publicly available Facebook API to  connect the research website, online survey, and participants  Facebook accounts. The API allows developers to access  particular data fields that represent the users interactions on the  Facebook platform.   Facebook policies for data use require that any application that  utilizes user data must request clear and explicit permissions.   When a participant connected their Facebook account, they were  presented with a standard Facebook screen that listed all potential  permissions for data sharing. The participant was able to decline  or opt out at any time in this process. Thus, all participants in this  study went through two explicit phases of consent. They were  given all research information at the outset and gave their consent  to take the online survey. They then went through a second  opportunity for consent when sharing their Facebook data for the  research project. There were 189 participants who took the online  survey, and 99 then connected their Facebook data in the second  phase.   The participants in this study clearly took part in both consent  phases and took full advantage of opportunities to opt out at any  moment, as nearly 50% of participants opted out of Facebook  sharing. I employed this strategy of multiple consent processes  because the ethics of data in social media platforms is a vital  consideration that remains an unexplored issue in current  research. The data collection experience reported here provides a  demonstration for researchers who might leverage social media  data in future studies. This type of data collection should require  explicit consent at every phase and researchers can expect  substantial opt-out rates. Finally, researchers could also plan to  develop web applications that benefit the user in addition to  serving as a data collection mechanism, as a way to improve both   137    the participant experience and participation rate. These are among  the best practices recommended by Facebook for application  developers, and are highly salient for researchers as well.   Participants were recruited through email, announcements via the  authors Twitter and Facebook network, personal contacts and  flyers distributed on a college campus in the eastern United States.  As noted earlier, 189 participants completed the online survey  with 99 who also provided additional consent and shared their  Facebook data for the study. I note from the outset that this is a  convenience sample and the design of the present study is  exploratory in nature. However, this exploratory study is the first  from the authors knowledge, to quantitatively examine new  media literacy skills using a novel approach of data collection  from participants Facebook activity.   3.1 Measures and variables  I employed another safeguard for participants in the data  collection by only collecting aggregate measures of Facebook  activity, and not gathering any personally identifiable information.  For example, the data used in this analysis considered aggregate  measures such as the total number of friends a user had in their  network, but I did not collect the actual usernames or IDs of these  friends. This detail is important because it shapes analytic  strategies. User privacy remains generally intact in this data  collection, but methods such as social network analysis are not  possible since exact ties to particular friends are not collected.   These ethical decisions also require researchers and analysts to be  critical about what insight is gained or lost from a particular data  collection strategy. In this study, I explore whether different  Facebook activities group together into broader categories of  behavior (see findings below). This analytic strategy of discerning  fewer, broader variables can be potentially useful for data streams  culled from popular social media platforms where the number of  variables can be large. However, this decision necessarily  abstracts behavior at a higher-level and blurs our understanding of  more detailed cultural practices. For example, the variable  Links  (see below)  can serve as a relatively clear proxy for members  sharing information through hyperlinks. However, without direct  observation of what the hyperlinks refer to, one cannot analyze  the diverse cultural practices that might emerge from the same  function of sharing links. I note these tensions at the outset.   When a participant connected their Facebook account, the  application collected an aggregate record of the users activity  over the previous 30 days. The variables collected for each  participant included:   Emails received  number of emails received on Facebook   Emails sent  number of emails sent by the user on   Facebook   Friends - number of friends in the users network   Friend Lists  Facebook users can organize their friends into   lists (close friends, coworkers etc.).  This variable is the  number of lists created.    Links  number of hyperlinks shared by the user with his or  her network    Member Pages  number of Facebook pages a user had  joined    Networks  number of networks a user joined (e.g.  geographic network, a college network etc.)    Notes  number of blog posts a user wrote    Photos  number of photos posted   Status Messages  number of status messages posted by the   user   Videos  number of videos posted by the user   Wallposts  number of wall posts received by the user from   their network  The Facebook application for this study only collected this data at  a single time point (Spring 2011) and ceased any data collection  after a participant completed the study. The variables were chosen  based on a combination of decision points: whether the specific  data was directly available via the Facebook API at the time of  development (with no need to parse users news feeds) and  indicators that might signify active behaviors such as sending  messages or joining groups (versus news items that appear on a  members news feed, but would be unclear whether the member  actually read the item or not).  The participants completed a survey that sought to measure the  new media literacy skills outlined by Jenkins [19] using 5-point  likert scale question (see [23] for more details of the new media  literacy scales). To illustrate a typical scale, the networking scale   Table 1: Descriptive Statistics   Variable Mean (St. Dev.) or  Percentage   Gender (Female) 68.69%  Age 26.42 (7.99)  New Media Literacy Scales    Negotiation 10.65 (3.03)  Networking 14.61 (3.27)  Judgment 15.67 (2.58)  Play 11.58 (2.52)  Multitasking 9.85 (3.21)  Remixing 8.71 (2.76)   Transmedia  Navigation   4.67 (1.85)   Technology Literacy Scale 58.65 (14.25)  Facebook Data    Emails Received 60.36 (90.74)  Emails Written 64.31 (134.02)  Friends 444.64 (381.62)  Friend Lists 3.35 (4.41)  Links 5.04 (8.35)  Member Pages 169.24 (501.37)  Networks 0.87 (0.93)  Notes 0.21 (0.95)  Photos 17.98 (42.84)  Status Messages 13.69 (20.73)  Videos 0.19 (0.77)  Wall Posts 22.53 (12.38)   N = 99      138    consisted of 5 statements where participants answered with their  agreement level:    I like to share my favorite links or creative work on social  media sites like Facebook or YouTube or Twitter.    I often share links on Facebook, Twitter, my blog, etc.   I am happy that I can interact online or on Facebook with   people from all over the world.   I like the fact that I can see all my friends on my Facebook   profile.   When I go online, I like to feel like I am part of a   community.   Similar scales asked participants about their propensity to engage  with the other literacy behaviors. In this study, I collected survey  scales for 7 new media literacies that had well-developed question  items (see [23]): negotiation, networking, judgment, play,  multitasking, remixing, and transmedia navigation.   In addition to the literacy scales, I collected several control  variables for the analysis. For demographic variables, participants  reported their gender and age. In addition, I collected a general  scale of technology literacy developed by Hargittai & Hsieh [18]  that asks participants about their general level of comfort with  common technology terms such as weblog, pdf, spyware, wiki and  other items. Descriptive statistics are provided in Table 1.   4. ANALYSIS AND RESULTS  In the following, I present two related analyses to address the  research questions posed in this study. First, an exploratory factor  analysis was used to explore whether Facebook activity data  grouped into distinct participatory behaviors. Second, I used the  factors that arose from this analysis in regression models to  examine whether behaviors found in the Facebook environment  correlated with survey measures of new media literacies.   4.1 Exploratory factor analysis  Before factor analysis, I conducted descriptive exploration of the  Facebook variables. As is often the case with data culled from  online communities, almost all of the variables followed a power   law distribution except the variable, networks. For example, the  majority of members in this sample sent and received a few  emails with others in the Facebook platform. However, there were  a few individual users who exchanged increasing amounts of  emails, creating a long tail distribution. Due to these non-normal  distributions, I transformed the variables using the natural log.  I utilized the data shared by Facebook participants in a principal  components analysis to explore whether these online activities  coalesced into distinct factors. Varimax rotation was applied to  identify distinct, independent factors that were orthogonal to one  another. Four factors were identified that had eigenvalues above  1.0. These four factors explained approximately 71% of the  variance among variables. The first factor accounted for 34% of  the variance, the second factor accounted for 17%, the third factor  for 12%, and the fourth factor accounted for 8%.   Table 2 reports the factor loadings of the variables. The first factor  included Facebook activity such as emailing, writing status  messages, and receiving wall posts on ones profile page. These  variables represent different messaging behaviors in the Facebook  platform. The second factor included Facebook activity such as  writing status messages, sharing links, sharing photos, managing  networks by creating friend lists, and joining member pages. The  interactions suggest different ways that Facebook members  manage their information sharing behaviors. The online  community is a place for individuals to share personal  information, links to other sources, and media such as photos. In  addition, individuals are increasingly using features such as friend  lists to manage what they share and with whom. For example,  creating a friend list of coworkers and another list of personal  friends requires an explicit thought process that one will share  particular information with one group and not the other.   One interesting result was the overlap of status messages with  both messaging and information sharing. This finding lends face  validity to the analysis, as status messages are often used in  diverse ways. Most of the variables in the messaging factor refer  to direct communication where an individual directly writes to  another via a Facebook email or on their wall. Most of the  variables in the information sharing factor refer to updates or   Table 2: Factor Loadings of Facebook Activity    Factor 1: Messaging Factor 2: Info. Sharing Factor 3: Friending Factor 4: Affiliating   Emails Received 0.91     Emails Written 0.91     Wall Posts 0.55     Status Messages 0.63 0.53    Friend Lists  0.63    Links  0.65    Member Pages  0.55    Photos  0.71    Friends   0.83   Notes   -0.83   Videos   -0.52   Networks    0.92   Factor loadings < 0.50 suppressed, N = 99      139    media that Facebook members share broadly. Status messages  appear to be a part of both distinct behaviors.   The third factor identified in this analysis was friending behavior.  The main variable in this factor was the size of ones friend  network. Having more friends on Facebook means an explicit  practice of accepting friend requests and seeking out more  connections. Interestingly, this factor had some substantial  negative loadings for writing notes (e.g. Facebooks version of  blog posts) and sharing videos. Writing notes and creating videos  might be examples of inward, individual-focused behaviors,  which is different than outward, networking-focused behavior  such as accumulating larger networks. The fourth factor was  largely made up of one major variable, networks, which denote  how many groups with which a person affiliated. In Facebook one  can join networks such as an alumni group, a regional group (e.g.  I am a part of the New York City network), and other group  identifications. This feature may represent affiliating or group  identity behaviors in the Facebook platform.   The exploratory factor analysis suggests that Facebook data from  this sample of participants can form higher-order categories of  participatory behavior. In this sample, the behaviors that appear to  be represented also align with the major features and activities one  would undertake in a social networking platform: messaging  others, sharing media and information, linking with friends, and  affiliating with networks. Using the results of this factor analysis,  I then created four composite variables that represent these  behaviors, to use in the subsequent analysis.   4.2 Regression analysis  The second research question asks whether the participatory  behaviors observed in the Facebook platform might also reflect or  correlate with measures of new media literacy skills. To examine  this question, I conducted an ordinary least squares (OLS)  regression analysis. The regression model examined the survey  measures of new media literacy skills as the dependent variable. I  examine seven literacy skills: negotiation, networking, judgment,  play, multitasking, appropriation, and transmedia navigation [19].  The independent variables in the regression model included the  four factor variables  messaging, information sharing, friending,  and affiliating  in addition to three control variables (technology   literacy, gender, and age). The resulting linear model is:   y = 1messaging + 2information sharing +3friending +  4affiliating + 5tech-literacy + 6gender + 7age +    The results of the regression analyses are presented in Table 3. I  do not display the demographic variables in the table, but report  the instances where these variables were statistically significant  predictors of particular new media literacies to aid in the  interpretation.   An overarching finding from this analysis is that particular  Facebook behaviors uncannily relate to new media literacy skills  in ways that are ecologically valid. Popular discussions of the  impact of social media for learning generally suggest that these  online communities help develop literacy skills. The results  presented in Table 3 contribute empirical evidence of these claims  and also illuminate with more specificity, what types of activities  in the platform relate to what particular media literacy skills. The  results suggest that Facebook activity is particularly salient for the  literacy skills of negotiation, networking, appropriation, and  transmedia navigation.   Negotiation. As noted earlier, the literacy of negotiation describes  the ability for individuals to participate in and adapt to diverse  communities of others. The results of this study suggest that  messaging behaviors in Facebook are highly related to an  individuals negotiation literacy (Table 3). A standard deviation  increase in Facebook messaging behaviors was related to a 0.67  standard deviation increase in the negotiation scale. Interestingly,  the other Facebook behaviors (information sharing, friending, and  affiliating) were not statistically significant. These results begin to  make sense however, when considering what these behaviors  actually mean in the situated context of Facebook. Sharing media  is largely about information dissemination, not about negotiating  different community norms. Friending others in Facebook is about  growing the size of ones network, but does not necessarily mean  active interaction. Affiliating with networks in Facebook is a  largely passive activity; the user does this affiliation once and  rarely engages with this affiliation explicitly in their daily  Facebook use. It seems that active communication behaviors are  related to an individuals opportunity to negotiate diverse  relationships with others.   Table 3: Regression Analysis Predicting New Media Literacies    Negotiation Networking Judgment Play Multitasking Appropriation Transmedia  Navigation   Messaging 0.67 ** 0.60 * 0.31 0.19 -0.01 0.48 0.30  Information Sharing 0.20 1.39 ** -0.002 -0.10 0.36 0.55 * 0.32 *  Friending -0.34 0.48 -0.32 -0.18 -0.52 0.21 0.32  Affiliating 0.33 0.77 ** -0.09 0.14 0.34 -0.06 -0.01  Control Variables1          Technology  Literacy   0.06 ** 0.01 0.09 ** 0.08 ** 0.04 0.01 0.02            Adjusted R2 0.09 0.23 0.17 0.31 0.01 0.04 0.08  N 94 95 94 93 95 93 95   Standardized coefficients  (1) Control variables gender and age not displayed in the table, but included in the model.  ** p < 0.05, * p < 0.10      140    Networking. Not surprisingly, the participatory behaviors seen in  Facebook are most predictive of networking literacy (Table 3).  These results confirm intuitive notions of social networking sites,  that they are designed for networking, and also confirm the  conceptual model proposed in this study. The findings offer  indicators of validity that the exploratory factor analysis identified  participatory behaviors that are highly salient and related to a  survey measure of networking literacy. The results in Table 3  show that messaging, information sharing, and affiliating  behaviors were all highly related to networking literacy. Also  interesting to note, was that information sharing behaviors had an  extremely large relationship to a persons networking score. In the  context of Facebook, this finding again makes perfect sense.  Individuals who share more information in the platform, create  more opportunities to actively interact and glean the benefits of  networking behavior. Conversely, merely adding friends to ones  network (and not necessarily sharing with them) was not  significantly related to networking literacy.   Appropriation. The skill of appropriation describes the ability to  take previous ideas and artifacts, to create ones own work. In the  context of new media, practices of remix (e.g. appropriating  digital media to create new projects) are an example of  appropriation skills. The results in Table 3 show that information  sharing behaviors in Facebook are highly correlated to  appropriation literacy. Facebook members who share more media  and information in their Facebook activity are likely to have more  opportunities to practice appropriation skills. In fact, the very act  of sharing itself could be interpreted as an act of appropriation,  since one is actively using other information sources to present to  others. An example of appropriation situated in Facebook is  sharing of links to other media sources or creating and sharing  ones own photos with the network.   Transmedia Navigation. Transmedia navigation is the ability to  follow information across multiple platforms and pathways. For  example, fans of the popular television series LOST also often  participated in online forums, played reality-based games  connected to the plot of the show, and discussed actual scientific  knowledge to explain elements of the series. In current media  environments, individuals can follow ideas through multiple  platforms such as television, online communities, books,  magazines, mobile applications and other means. The results in  Table 3 show that information sharing behaviors in Facebook  were highly correlated with transmedia navigation. The social  network site itself is designed for individuals to share media, and  thus could act as a connector for transmedia behaviors. Facebook  members who share more information are also more likely to  navigate this information by following links, or bringing other  media sources into the online community.   What Didnt Facebook Activity Predict The Facebook  behaviors identified here were not significant predictors of three  new media skills: judgment, play, and multitasking. A deeper  interpretation of these findings also lends face validity to these  results. Judgment is the ability to evaluate information sources as  credible and reliable. There is nothing inherent about sending  messages to others, sharing media, adding friends, or affiliating  with networks that might lead a person to feel more capable to  effectively evaluate credibility of information sources. The only  statistically significant predictor of judgment in this model was a  general measure of technology literacy. This result suggests that  perhaps a broader experience with diverse media types is more  likely to help one be a better evaluator of credibility, than only  interacting in Facebook.   Play is the ability to problem-solve, tinker, and experiment with  artifacts and objects. None of the social interactions in Facebook  were significantly correlated with play literacy, and this finding  makes intuitive sense as well. There is little designed into  Facebook that require individuals to tinker or experiment in ways  that other activities might allow (e.g. programming a computer or  building a model). General technology literacy was correlated  with a persons propensity to play. Interestingly, gender was  highly correlated with the play scale. Males in this sample were  substantially more likely to report tinkering, problem solving, and  experimenting with objects than the female participants.   Finally, the ability to multitask was not significantly predicted by  Facebook activity or general technology literacy. The only main  predictor of multitasking literacy was age, with older individuals  scoring lower on the multitasking scale compared to younger  individuals. This finding mirrors popular thought, which suggests  that younger generations are more comfortable with multitasking  in new media environments [25].   5. DISCUSSION  This study makes several contributions to learning analytics,  social media and learning research. First, the study demonstrates  ways to utilize social data from the popular social media platform  Facebook, to explore the new media literacy skills embedded in  peoples everyday online networking behaviors. Applying  analytics to this social interaction data, I show how Facebook  behaviors group into distinct participatory behaviors: messaging,  information sharing, friending, and affiliating. In addition, this  study illuminates how Facebook activity correlates and predicts  particular new media literacy skills. Social media platforms  represent a ripe arena to examine the social learning behaviors and  sociocultural learning processes embedded in peoples everyday  activities. Social learning analytics provide a methodological lens  to make meaning from these massive data streams.  Second, this study contributes empirical evidence of the literacy  skills that are involved in ones Facebook participation. The  results introduce additional clarity to the broader discussions  about social media and learning. Moving beyond general claims  that peoples participation in social media are related to new  media literacies, the use of analytics on this Facebook data begins  to clarify what particular activities in these online communities  relate to which specific literacy skills. Facebook behaviors appear  to be most related to networking, negotiation, appropriation, and  transmedia navigation. However, literacies such as play,  judgement, and multitasking where not related to Facebook  behavior. Such findings lend more detail to scholarly discussions  about which media tools may promote which specific literacy  skills.    Furthermore, the findings illuminate new questions such as what  other types of media-based activities might lend themselves to the  literacies not represented in Facebook behavior For example,  Facebooks design enables certain types of participatory behavior:  messaging, information sharing, friending, and affiliation. Future  work might consider whether other major social media platforms  such as Twitter, Tumblr, Pinterest, Instagram, and others, enable  diverse modes of behavior. Future work might also consider  whether particular tools used to access these platforms also  engender diverse practices and literacies. For example, one can  interact with Twitter via the web-browser interface, Twitter  clients on the iPhone and Android devices, or desktop clients such  as Tweetdeck. Behaviors may differ with various tools, even  within the same social media community. Social media platforms   141    tap into different human needs to connect and share with one  another, and future work might consider whether different  platforms are related to particular literacy skills.   Third, this study contributes to the emerging vision for social  learning analytics. Researchers can now collect large streams of  social interaction data, which directly reflect on learning  dispositions that are difficult to assess via traditional instruments  such as surveys. The implication of this technological capability is  that researchers who utilize a sociocultural perspective on learning  can actively observe, quantify, and test hypotheses about social  learning processes and soft-skills in ways not possible before. For  example, the exploratory study presented here begins to show how  common, everyday platforms such as Facebook can show the  emerging literacy skills that people develop in new media  environments. Learning analytics of social data add to the  repertoire of research strategies to understand learning in online  platforms.  Finally, the results of this study offer potential design implications  from social learning analytics. If researchers identify measures of  literacy skills that are directly reflected in peoples everyday  social media participation, there is great potential to design  dashboards and visualizations that can help individuals understand  the learning benefits they accrue from this online activity.  Already, for-profit services such as Klout are creating dashboards  that attempt to calculate ones level of influence in their online  social networks. A similar potential exists for educators to value,  and make visible to learners, the literacy behaviors that are being  developed in popular social media platforms. The idea to leverage  popular platforms such as Facebook to help learners understand  their own literacy development is potentially a powerful and  exciting idea where learning analytics plays a central role.   5.1 Limitations  There are several limitations to note when interpreting the present  study. First, the analyses presented here are clearly exploratory in  nature. I utilized a convenience sample and the participant pool  was relatively small, as 99 participants opted to share their  Facebook data. Any interpretations are bounded by this sample  only and not generalizable to the broader Facebook population.  Despite these boundaries, the present study is among the first to  utilize analytics on data collected directly from the Facebook  platform, to assess new media literacy skills. Future work that  might gain broader access to members of the platform will be  helpful to validate and refine the analyses performed here.   Second, as noted in the methodology section, I made explicit data  collection choices to protect the privacy of participants. Only  aggregate numbers of user activity were collected in this study.  While this strategy values the privacy of Facebook members by  not collecting personally identifying information, it precludes  certain analytic strategies such as social network analysis.  However, this study demonstrates how analytic strategies that are  accessible to social scientists in general, such as factor analysis  and regression models can be applied to social media data in  productive ways. Future studies that examine different social  learning processes via network analysis, different clustering  algorithms, and other modeling techniques will also make a  needed contribution to the literature.   6. CONCLUSIONS  Learning analytics is a multidisciplinary field that combines the  technical opportunities of data with social theories drawn from  learning research. This study demonstrates how these multiple   perspectives may combine to derive deeper insight into literacy  development with social media. The meeting of data, theory, and  methodology in a learning analytics framework also illuminates  some fundamental implications and tensions.   Opportunities to Integrate Sociocultural Learning  Frameworks with new Data Streams. In traditional social  science and education research traditions, it has often been  difficult to directly and quantitatively observe sociocultural  learning concepts and dispositions in a natural way. I argue that  new media literacies are an example of learning dispositions or  skills that could be readily observed and measured from social  media data streams. However, to recognize such opportunities,  requires researchers to integrate ideas from multiple fields. In this  study, I was inspired by a theoretical framework of literacy  forwarded by humanists and media literacy scholars such as  Jenkins [19]. I was then able to link these concepts of literacy to  behavioral data culled from a popular, global social network site,  Facebook. As new media tools become increasingly social in  focus, researchers may be able to link data streams from these  platforms to traditionally difficult-to-measure facets of literacy.   Social media platforms represent a ubiquitous part of individuals  everyday lives. Not only are platforms such as Facebook, spaces  for networking, but they also represent a place where tremendous  levels of social learning can occur. Many of these platforms share  user data through their APIs that offer researchers new  opportunities to directly examine social learning processes using  analytics. However, the availability of large amounts of social  data in popular platforms introduces considerations of privacy and  ethics in collecting this data. Many companies (such as Facebook)  have explicit requirements for using participant data and  researchers would do well to follow these best practices.    Beyond these regulations there are critical issues related to  research design that this study demonstrates for future scholars.  For example, recruitment remains a salient issue for this type of  research. Despite the potential availability of massive amounts of  big data, access to this data for social scientists is still bounded by  actively recruiting participants, unless one is a part of the  companys data team itself and can access all the raw data. In this  study, participants were given two explicit consent opportunities  (one for the study, and another to share their Facebook data). This  helps to protect participant privacy, but also demonstrates issues  such as substantial opt-out rates. Future researchers in this area  will have to plan for these considerations.    In addition, I chose to value participant privacy by only collecting  aggregate numbers of activity. I did not collect identifying  information such as a Facebook members direct ties to others.  Thus, certain analytic strategies are possible such as factor  analysis and regression models, while other potentially helpful  metrics are not available (e.g. social network analysis). These data  collection choices reflect a tension between research needs and  privacy of participants. These dilemmas are likely to be worked  out in each respective study, but future work that can demonstrate  ethical modes of data collection, while maintaining opportunities  for diverse analytic strategies, will make a contribution to the  literature.   What is Gained and Lost in Analytics Related to issues of  theory, data collection and methodology, is a critical discussion of  what insights are gained and lost with analytics. This study  demonstrates the tension between abstraction and depth that is  inherent in learning analytics. On the one hand, the present  analysis distills the diverse behaviors one can enact in Facebook  into clear, broader categories of general behavior: messaging,   142    information sharing, friending, affiliating. Such abstraction helps  in our conceptual thinking, aiding us in being honest about what  learning interactions and literacies Facebook activities may  engender; and equally important, does not afford. Understanding  that Facebook affords particular behaviors, but not others, helps  scholars develop finer-grained understanding of how different  social media platforms might influence learning. Future work in  social learning analytics is needed to explore whether the  behaviors observed here also apply to different platforms, and  have similar or different implications for learning. In addition,  research that can clearly describe new forms of behavior that  occur in other platforms will help build deeper understanding of  how and when social media might be employed to improve  learning experiences.  With learning analytics, this abstraction and analysis can occur at  large scale, and data can be collected unobtrusively in naturally  occurring online environments. However, a clear consequence of  abstraction is lack of rich description and detail. An equally  important discussion with analytics is what insight is lost when  making specific methodological decisions. For example, in this  study behaviors such as sharing links and creating friend lists  were highly related in a factor that I generally organized as  information sharing. However, individuals always appropriate  technical features, such as the ability to share links or organize  ones Facebook friends into groups, to create diverse and  surprising cultural practices. The analytic strategy used here  necessarily abstracts these details out, and other methodologies  including ethnography of online communities and discourse- focused methods, are vital to inform learning analytics.   Implications for learning. One major goal of learning analytics  is to measure, render visible, and make actionable, individuals  learning behaviors [8]. The implications of pursuing these goals in  the area of new media literacy are numerous. For education  policymakers and evaluators, having indicators of literacy skills  may help develop a deeper understanding of learners beyond  common measures such as standardized tests of content  knowledge. This study contributes to the growing understanding  that we may be able to measure such literacies in popular and  everyday activities such as Facebook participation. Increasingly,  education stakeholders will be able to observe and value soft- skills, such as different forms of literacy, in conjunction with  traditional measures of content knowledge.   For educators, recognizing the skills that are embedded in  everyday activity such as Facebook also informs popular debate  about the role of social media in education. Perhaps the major  narrative of social media and education should not be about  whether new social tools can aid in learning traditional content,  but instead recognize and value literacy skills as a worthy  outcome in itself. By making literacy skills visible and measurable  through indicators and metrics, such debates might be more  grounded in artifacts of shared data. Broad statements such as  Using Facebook helps develop new literacy skills may instead  evolve into more useful and testable claims such as Facebook  activities of messaging and information sharing help young people  develop critical networking skills or Particular activities such as  more frequent direct communication with others online, may  influence positive negotiation skills among learners. The findings  presented here begin to add more clarity and detail to such  popular discussions.   This study only considered the Facebook platform, but nearly 1  billion individuals participate in this social network, and thus it  represents a significant context to understand human behavior and   learning. However, there are a plethora of social media platforms  that now boast robust membership such as Twitter, Pinterest,  Instagram and others. These sites cater to very different types of  participatory behavior and also represent potential opportunities to  understand learning in social media environments. Future research  is needed that can examine multiple social media platforms, gain  access to a larger population of users, and employ diverse  analytics methods while understanding the constraints of  collecting data from popular platforms. Continued work that  explores literacy through learning analytics on popular platforms,  will make a substantial contribution to education research.   7. ACKNOWLEDGMENTS  I thank Ezra Schwartz for his programming and development of  the Facebook data collection application for this study; Elizabeth  Bonsignore, the blind reviewers, and the conference chairs for  their comments and edits to the manuscript; and the College of  Education at the University of Maryland, College Park for grant  support through the Support Program for Advancing Research and  Collaboration (SPARC).   8. REFERENCES  [1] Ahn, J. 2011. The effect of social network sites on   adolescents academic and social development: Current  theories and controversies. Journal of the American Society  for Information Science & Technology 62(8), 1435-1445.   [2] Ahn, J. 2012. Teenagers experiences with social network  sites: Relationships to bridging and bonding social capital.  The Information Society 28(2), 99-109.   [3] Ahn, J., Subramaniam, M., Fleischmann, K. R., Waugh, A.,  Walsh, G., and Druin, A. 2012. Youth identities as remixers  in an online community of storytellers: Attitudes, strategies,  and values. In Proceedings of ASIST 2012: The 75th Annual  Meeting of the American Society for Information Science and  Technology.   [4] Barton, A. C., Tan, E., and Rivet, A. 2008. Creating hybrid  spaces for engaging school science among urban middle  school girls. American Educational Research Journal 45(1),  68-103.   [5] Bienkowski, M., Feng, M., and Means, B. 2012. Enhancing  teaching and learning through educational data mining and  learning analytics: An issue brief. U.S. Department of  Education, Office of Educational Technology, Washington,  DC.   [6] boyd, d. m. 2008. Why youth (heart) social network sites:  The role of networked publics in teenage social life. In  Youth, Identity, and Digital Media, D. Buckingham, Ed. The  MIT Press, Cambridge, MA, 119-142.   [7] boyd, d. m. & Ellison, N. B. 2007. Social network sites:  Definition, history, and scholarship. Journal of Computer- Mediated Communication 13(1), article 11.   [8] Buckingham Shum, S., and Ferguson, R. 2012. Social  learning analytics. Educational Technology & Society 15(3),  3-26.   [9] Burke, M., Kraut, R., and Marlow, C. 2011. Social capital on  facebook: Differentiating uses and users. In Proceedings of  CHI 2011, ACM Conference on Human Factors in  Computing Systems. ACM, New York, NY, 571-580.   143    [10] Chinn, C. A., and Malhotra, B. A. 2002. Epistemologically  authentic inquiry in schools: A theoretical framework for  evaluating inquiry tasks. Science Education 86(2), 175-218.   [11] Clegg, T., and Kolodner, J. 2007. Bricoleurs and planners  engaging in scientific reasoning: A tale of two groups in one  learning community. Research and Practice in Technology  Enhanced Learning 2(3), 239-265.   [12] Dawson, S. 2011. Analytics to literacies: Emergent learning  analytics to evaluate new literacies. Workshop on New  Media, New Literacies, and New Forms of Learning.  http://blogs.ubc.ca/newliteracies/files/2011/12/Dawson.pdf.   [13] Dede, C. 2010. Comparing frameworks for 21st century  skills. In 21st century skills: Rethinking how students learn, J.  A. Bellanca & R. S. Brandt, Eds. Solution Tree Press,  Bloomington, IN, 51-76.   [14] Ellison, N. B., Steinfield, C., and Lampe, C. 2011.  Connection strategies: Social capital implications of  facebook-enabled communication practices. New Media &  Society 13(6), 873-892.   [15] Greenhow, C., and Gleason, B. 2012. Twitteracy: Tweeting  as a new literacy practice. The Educational Forum 76, 463- 477.   [16] Greenhow, C., and Robelia, E. 2009. Old communication,  new literacies: Social network sites as social learning  resources. Journal of Computer-Mediated Communication  14(4), 1130-1161.   [17] Greenhow, C., Robelia, B., and Hughes, J. E. 2009. Web 2.0  and classroom research: What path should we take now  Educational Researcher 38(4), 246-259.   [18] Hargittai, E., and Hsieh, Y. P. (2012). Succinct survey  measures of web-use skills. Social Science Computer Review  30(1), 95-107.   [19] Jenkins, H. 2006. Confronting the challenges of participatory  culture: Media education for the 21st century. The  MacArthur Foundation, Chicago, IL.   [20] John-Steiner, V. and Mahn, H. 1996. Sociocultural  approaches to learning and development: A Vygotskian  framework. Educational Psychologist 31(3-4), 191206.   [21] Lampe, C., Wohn, D. Y., Vitak, J., Ellison, N. B., and Wash,  R. 2011. Student use of facebook for organizing  collaborative classroom activities. International Journal of  Computer-Supported Collaborative Learning 6(3), 329-347.   [22] Lemke, J. L. 2001. Articulating communities: Sociocultural  perspectives on science education. Journal of Research in  Science Teaching 38(3), 296-316.   [23] Literat, I. (May 2011). Measuring new media literacies:  Towards the development of a comprehensive assessment  tool. Teachers College Educational Technology Conference  (TCETC). New York, NY.   [24] Polman, J. L., and Miller, D. 2010. Changing stories:  Trajectories of identification among African American youth  in a science outreach apprenticeship. American Educational  Research Journal 47(4), 879-918.   [25] Prensky, M. 2001. Digital natives, digital immigrants. On the  Horizon 9(5), 1-6.   [26] Rotherham, A. J., and Willingham, D. T. 2010. 21st century  skills: Not new, but a worthy challenge. American Educator  34(1), 17-20.   [27] Shirky, C. 2011. Cognitive surplus: How technology makes  consumers into collaborators [reprint edition]. Penguin  Press, New York, NY.   [28] Silva, E. 2009. Measuring skills for 21st-century learning. Phi  Delta Kappan 90(9), 630-634.   [29] Valkenburg, P. M., and Peter, J. 2009. Social consequences  of the internet for adolescents. Current Directions in  Psychological Science 18(1), 1-5.      144      "}
{"index":{"_id":"20"}}
{"datatype":"inproceedings","key":"Wolff:2013:IRP:2460296.2460324","author":"Wolff, Annika and Zdrahal, Zdenek and Nikolov, Andriy and Pantucek, Michal","title":"Improving Retention: Predicting At-risk Students by Analysing Clicking Behaviour in a Virtual Learning Environment","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"145--149","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460324","doi":"10.1145/2460296.2460324","acmid":"2460324","publisher":"ACM","address":"New York, NY, USA","keywords":"distance learning, predictive modelling, retention, student data, virtual learning environment","abstract":"One of the key interests for learning analytics is how it can be used to improve retention. This paper focuses on work conducted at the Open University (OU) into predicting students who are at risk of failing their module. The Open University is one of the worlds largest distance learning institutions. Since tutors do not interact face to face with students, it can be difficult for tutors to identify and respond to students who are struggling in time to try to resolve the difficulty. Predictive models have been developed and tested using historic Virtual Learning Environment (VLE) activity data combined with other data sources, for three OU modules. This has revealed that it is possible to predict student failure by looking for changes in user's activity in the VLE, when compared against their own previous behaviour, or that of students who can be categorised as having similar learning behaviour. More focused analysis of these modules applying the GUHA (General Unary Hypothesis Automaton) method of data analysis has also yielded some early promising results for creating accurate hypothesis about students who fail.","pdf":"Improving retention: predicting at-risk students by  analysing clicking behaviour in a virtual learning   environment  Annika Wolff, Zdenek Zdrahal   Knowledge Media Institute  The Open University   Milton Keynes, MK7 6AA  +44 (0) 1908 659462   {Annika.wolff,  zdenek.zdrahal}@open.ac.uk   Andriy Nikolov  fluid Operations AG1   Altrottstrae 31  69190 Walldorf, Germany   +49 (0) 6227 3849-567  andriy.nikolov@fluidops.com   Michal Pantucek  Czech Technical University1   Dept. of Cybernetics  Karlovo namesti 13, 12135 Praha 2   +42 (0) 22435 7666  pantumic@fel.cvut.cz        ABSTRACT  One of the key interests for learning analytics is how it can be  used to improve retention. This paper focuses on work conducted  at the Open University (OU) into predicting students who are at  risk of failing their module.  The Open University is one of the  worlds largest distance learning institutions. Since tutors do not  interact face to face with students, it can be difficult for tutors to  identify and respond to students who are struggling in time to try  to resolve the difficulty. Predictive models have been developed  and tested using historic Virtual Learning Environment (VLE)  activity data combined with other data sources, for three OU  modules.  This has revealed that it is possible to predict student  failure by looking for changes in users activity in the VLE, when  compared against their own previous behaviour, or that of  students who can be categorised as having similar learning  behaviour. More focused analysis of these modules applying the  GUHA (General Unary Hypothesis Automaton) method of data  analysis has also yielded some early promising results for creating  accurate hypothesis about students who fail.   Categories and Subject Descriptors  H.2.8 [Database Applications]: Data Mining; D.4.8  [Performance]: Modelling and prediction    General Terms  Algorithms, Experimentation.   Keywords  Predictive modelling, retention, student data, virtual learning  environment, distance learning   1. INTRODUCTION  In learning analytics, student data is analysed in order to provide  insight into what students are doing with learning materials. Some  applications of learning analytics feed back directly to students  about their own behaviour in order to help them to become more  strategic learners.    Others inform tutors or module design teams about student  behaviour, to better facilitate student support. A key aim of the  latter is to improve student retention. This paper will discuss work  conducted by the Open University using data from the Virtual  Learning Environment (VLE) combined with other data sources to  predict students at risk of failing a module.  This work suggests  that changes in students activity on the VLE is a reliable indicator  of failure. Additional, ongoing, work will also be discussed which  in early stages seems to show that it is possible to use the GUHA  method (using LISp Miner) with the VLE data to generate  hypotheses in the form of rules about factors contributing to  student failure. These rules can then be applied to new data sets  and predict accurately which students will fail their course.    2. RETENTION AND ONLINE LEARNING  Studies have  shown that online courses have larger attrition rates  than traditional bricks and mortar establishments [4].  Investigations into the differences between click and brick  establishments suggest several contributory factors, but with  conflicting results among different studies as to which have the  biggest impact on student drop out [3, 4, 14]. Possible factors  include a difference in the student demographics: there are usually  more students from lower economic backgrounds, those with less  formal qualifications and a higher proportion of disabled students.  Also, there may be difficulties with the technologies needed to  study, greater time constraints and less academic preparedness  for study.  Also, due to lack of face to face contact, students can  feel isolated and unsupported by their tutors. On this latter point, a  study by Frankola [5] reveals this to be a major contributing factor  to student drop out.    Previous research has indicated that initiating telephone contact  with students can improve retention figures [15, 16]. The  difficulty faced by large distance learning institutions is that to  resource this contact on a large scale is financially unviable. What  is needed is some intelligence as to which students will most  benefit from an intervention, to allow resources to be targeted  more effectively [2].    2.1 Student tracking  A broad-brush approach for identifying the students who might  benefit from more focused support, particularly at the start of a  module, is to use a tracking system [11]. This can flag up those  students who fit the sorts of profile shown to be more likely to                                                                          1 This work was carried out at the Knowledge Media Institute   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.   LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.     145    fail. However, since these precise factors may vary by institution,  it is advisable for each institution to undertake some analysis of  their own data to determine which factors are most informative in  their specific case [6]. A further issue is that, whilst these students  are more at risk than others, the vast majority of them will go on  to be successful, while others who dont fit that profile will fail.   Therefore, these factors can be more usefully integrated with other  data sources that reveal something of the students behaviour and  performance on the module. This can include, for example, data  from Virtual Learning Environments, or other static data sources  such as assessment submission patterns and outcomes.   Integrating these data sources increases the burden on the tutor in  terms of analysing and interpret the tracking statistics.  Visualisations can go a long way towards helping tutors to  manage the data load (e.g. 11), but the more data that is involved,  then the harder it is for tutors to know what to look for or to  uncover the interesting patterns that might tell them who is  struggling, or even why. Factors that might on the surface seem a  good predictor of failure may become unreliable when the myriad  of other factors involved in undertaking an online learning course  come into play.    2.2 Predictive modelling  One possible solution is to build predictive models [10].  Computational methods can identify consistent patterns in learner  behaviour that are hard for the human tutors to perceive, but  which can be used to accurately predict what they will do next (in  this case, either pass or fail the course). Course Signals [1]  visualizes predicted performance to both tutors and students  themselves. The model is built on several data sources, including  engagement with resources on the learning management system,  as well as assessment performance, past history and  demographics. The authors demonstrate the effectiveness of the  system for improving retention. Smith and Sweeley [17] used data  such as frequency of logins to learning system and some  interaction with course materials (as determined by clicking) to  try to predict student failure, weighting the model to look most at  recent behaviour. They found a significant correlation between  logging in, looking at materials and course outcome. The  developed model had 70% accuracy in predicting unsuccessful  students.    These predictive methods build models of average learners  against which individual students are compared. But distance  learners are a diverse group, which means it is not possible to  build a typical student profile [18]. Pistilli and Arnold suggest that  the most effective models are built on a course by courses basis  [12]. This is because most courses are structured differently and  place different demands on learners. This is especially true if  looking at activity data, where the course design will, to a large  extent, dictate what learners ought to be doing in terms of  engaging with learning materials, or chatting on forums etc.    3. BUILDING THE PREDICTIVE MODELS  FROM OU DATA  In common with other learning institutions, the Open University is  interested in using analytic techniques as part of an approach to  improving the student experience and increasing retention figures.  As the OU is a distance learning institution, most of the  knowledge about students is held in various data sources.  Crucially, this includes knowledge about their learning activity in  the form of their engagement with the Virtual Learning  Environment (VLE). Other static data sources contain information  on students assessment results, as well as other demographic   data, financing, disability flags etc. The aim was to develop  models for predicting student failure using data from the VLE in  combination with the assessment data: each OU module has a  number of Tutor marked Assessments (TMAs) as well as a final  exam. These contribute to the overall pass mark.    3.1 Understanding the modules  The models were developed and tested on historic data sets from  three modules, which were chosen for having large student  numbers and for making good use of the VLE for delivering the  course content. The modules were from the different subject areas  of art, mathematics and business. The student numbers and VLE  accesses are in Table 1.   Table 1. Profiles of the three selected modules for developing  and testing models   Module ID No. of students No. of VLE clicks   A 4397 1570402   B 1292 2750432   C 2012 1218327   These modules were also considered to be fairly typical of OU  courses, and more importantly to reflect how modules will  increasingly be delivered in the future. However, even so it is  necessary to note that all modules were significantly different in  some important respects. Each module can set different criteria for  passing the module. There can be different numbers of TMAs,  some of which may be optional or substitutable with other  TMAs. Therefore, predictive models were developed on a  module by module basis to determine whether or not it is  important to take these differences into account.    3.2 Understanding the data  The VLE clicks are recorded by category, such as whether they  were clicks for a module page, or clicks on a forum or discussion  topic.  The level of detail is relatively coarse-grained. Analysis  was undertaken to understand whether or not it was necessary to  filter out some categories of data, if they were not directly related  to learning but may interfere with the predictive results. This  analysis revealed that there were two categories of data that might  be filtered. One category was the home-page for the module, that  the user always visited before proceeding to look at further  materials. However, since this page is accessed uniformly, it does  not affect the quality of the data. Conversely, the other pages  (such as updating personal details) were accessed so infrequently  they had a similar non-effect. The remaining data categories  related to forums, course content and online tools.  It was also necessary to spend some time ensuring that not only  was each category of data was understood but also whether or not  it was used consistently from one module to the next. While the  issue of data cleaning for all data within the OU was not resolved,  it was possible to gain enough knowledge about the data from the  three selected modules to start building models.    3.3 Understanding the students  The overall aim is to be able to understand the students behaviour  by looking at their data alone and to therefore predict struggling  students, even when the students dont say anything themselves.  In order to do this, it is important to understand how students  would be using the VLE and how this usage would affect their  final performance. Students VLE activity was analysed on the  three modules, firstly by looking at all clicks, and then by  breaking it down by the VLE data categories. The first general  finding was that student activity, on average, increases in the   146    week that an assessment is due. This finding is so marked that it is  possible, without prior knowledge, to pinpoint the date of each  assessment on each of the modules. This information is not usable  in itself to predict failure of a TMA, since the student can access  the VLE only at the last minute and still pass the assessment.  Instead, it suggests the need to identify activity time periods  between assessments for analysing the students VLE accesses. A  second important finding was that there is no such thing as an  average student. This is consistent with the view of Thompson  [17]. There were students who clicked a lot and still failed, or  those who clicked hardly (if) at all and yet passed.  These students  may have a printed version of course materials, or may have been  retaking the module and therefore required less VLE access. This  was true of all modules.  It was therefore not possible to find any  general measure of clicking for a given module that students could  be compared against.    4. PREDICTING PERFORMANCE DROP  AND FINAL OUTCOME  The next stage involved developing classifiers to predict risk,  which is defined as either:   a) performance drop  predicting a previously well- performing student will fall below the pass threshold in  the next activity period.   b) final outcome  predicting whether a student will pass  or fail the course. This was tested at different time- periods in the course, namely the different assessment  submission points.   Based on the previous analysis of students clicking behaviour, the  data from students who did not engage with the VLE was filtered  out. Training and testing were performed with the historical data  from three modules using 10-fold cross-validation (whereby the  data is divided into 10 random samples, 9 of which are used to  train the model and the remaining 1 is used to test it). Testing was  done using VLE data only, TMA data only and a combination of  both. The VLE data was not processed to exclude any categories  of data (based on the findings discussed in section 3.2).  For both  types of models, decision trees were found to outperform SVMs  (state vector machines). The decision tree algorithm used was  C4.5, a version of Quinlan's ID3 [13].   4.1 Performance drop  The features of the performance drop model included the  students assessment scores and the number of VLE clicks in a  time window k (a period between TMA submission). The feature  to predict was the nominal class label (drop/no-drop). As the  goal is primarily to recognise the at risk students, the results are  measured using the standard precision (p), recall (r), and F1  measures for the class drop1. With the window size 3, the  performance drop classifier was able to achieve high precision for                                                                        1 The precision is defined as ! = ! #$ ! #$%$&'#  ! #$ ! #$%$&'#!! #$% ! #$%$&'#  and   recall as ! = ! #$ ! #$%$&'# ! #$ ! #$%$&'#!! #$% ! #$%&' (  , where true  positives are the instances correctly recognised as belonging to  the class drop, false positives are the cases where the drop  was expected, but the student actually performed well, and  false negatives are the cases where the students performance  was not expected to drop, but actually dropped. Then, the  combined F1 measure is defined as the combination of the two:  !1 = !!   !!! .    all three courses (between 0.77 and 0.98) and good overall  accuracy (F-measure between 0.61 and 0.94), on the VLE+TMA  data combination. These results can be seen in Figure 1.  Interestingly, the number of VLE clicks occurring just before the  TMA being predicted was found to be the most informative  attribute: a student who used to work with the VLE before but  then stopped is likely to fail at the next TMA. Thus, even the time  window size ! = 1 was sufficient to build the model, while  increasing it could only lead to marginal increase in performance.  TMA data on its own was not found to be very good at predicting  performance drop. VLE data on its own was marginally better.  The best prediction occurred when these two data sources were  combined.     Figure 1. Predicting performance drop, using VLE and TMA   data, with a window size k = 3.   4.2 Final outcome  The final outcome prediction model uses, as features, the scores  for TMAs, the average TMA score, and the number of VLE clicks  in periods between submission dates of each two subsequent  TMAs. The findings from running the model on the available data  was that precision is reduced as the module progresses, in other  words it is easier to predict failing students in the early stages of a  module. VLE clicks were again found to be better for prediction  than the assessment scores. Again, VLE and TMA data combined  are generally better for prediction, especially when compared to  using only assessment data. See Figure 2 for an example using  Module A data.     Figure 2. Predicting final outcome at TMA 3 for Module A -   comparing TMA, VLE and combined.   4.3 Adding demographic data to module A  Demographic data was added to see if this would improve  prediction of final outcome. The type of data to include was  chosen in consultation with the student statistics team who have   147    used static data sources previously for prediction, though not  combined with the VLE data. The demographic data was added to  module A. It was found that this data did indeed improve  prediction (see table 2). The selected demographic feature is not  reported, as demographic data can be considered sensitive.  However, it is likely that the chosen feature is specific to the Open  University, since the demographics invariably vary between  institutions. For example, the Open University solely offers  distance learning and has a higher percentage of mature and  disabled students than other institutions.  It seems reasonable to  suggest therefore that selecting which demographic data to use  needs to be done on a case by case basis.   The importance of VLE, demographic or TMA data for prediction  depends on the point in the module at which the prediction is  being made. The data suggested that in the early stages the VLE  data is best for prediction, then the demographic data and finally  the assessment scores. However after the third assessment, the  assessment scores become more informative. This seems  reasonable, since the final result depends in some way on the  assessment performance. Another possible explanation is that the  students who drop out due to lack of motivation/difficulties, as  evidenced by their VLE activity, tend to do so earlier in the  module. Those who drop out later may do so for more  unpredictable reasons, such as sudden personal problems.    Table 2. Module A with additional demographic data   TMA  number   with demographics without demographics    p r F1 p r F1   2 0.62 0.23 0.34 0.73 0.21 0.32   3 0.70 0.37 0.49 0.65 0.37 0.47   4 0.7 0.35 0.47 0.74 0.33 0.46   4.4 Applying models across modules  In a final test of using decision tree classifiers for prediction, the  trained models were applied to different data sets. Unsurprisingly,  the models generally proved best when applied to the same  module. However, the quality on the transferred models were  still of reasonable quality and even in some cases better than on  the original course it was trained on. These results suggest that  there is a strong module-independent pattern of activity when  looking at general student behaviour in terms of VLE activity.  However, they do not give much precision for later stages of  student drop out, nor do they provide module specific information  for informing managers of why students might be struggling.   5. HYPOTHESIS GENERATION  The next step has involved trying to generate hypotheses that can  provide more detailed explanations for student failure, since the  explanations produced through the decision trees are fairly  generalized. This work, in early stages but yielding promising  results, has used LISp Miner to implement the GUHA (General  Unary Hypothesis Automaton) method of data analysis to produce  hypotheses about failing students in the form of a set of  association rules [8, 9]. The GUHA method is a data mining  technique for generating as many plausible hypotheses as possible  from the data, in accordance with initial set up parameters for  confidence (the probability that a generated hypothesis correctly  classifies the cases), support (a minimum percentage of examples  that fit the rule) and maximum number of antecedents.   In the first stage, previous findings from the decision tree were  replicated when it was confirmed that it was not possible to get   useful results by looking at the VLE data in terms of total number  of clicks. Instead, for each student, the attribute used for building  the hypotheses was the change in the users VLE activity  compared to a previous period activity, expressed as a percentage.    The TMA performance was also categorised into bands of <40%  (the failure threshold), 40-60%, 60-80% and 80-100%.  GUHA  was trained to predict module failure (rather than performance  drop).  Parameters were set to improve performance, e.g. setting the  maximum number of antecedents to 5. The confidence was set to  70% (i.e. only generate rules that have a confidence of 70% or  greater) so that it would not generate hypotheses that had equal  chance of being invalid as they did of being valid.  A suitable  support value is dependent on the individual data set. In our case,  because of the relatively low number of cases to classify in the  large dataset (only a small proportion of students will fail) the  support needed to be set very low in order for GUHA to find  hypotheses (in some cases as low as 0.001).    GUHA produced a set of rules which, when applied to the same  module data from the subsequent year (the model developed  association rules from 2010 data which was then applied to 2011  data) produced extremely accurate results. As an example, shown  in Table 3, GUHA has produced the following finding: a fail in  TMA 4 can predict failure with 88% confidence in 2010 (537  cases) and 83% confidence in 2011 (516 cases). GUHA produced  a more specific version of this rule that included the change in  VLE activity between TMAs 6 and 7. This improved  performance to 94% confidence in both 2010 (472 cases) and  2011 (394 cases).    Table 3. Example of GUHA rules    2010 2011    conf supp conf supp   Tma4(<0;40))  0.88 0.1696 0.83 0.15   Tma4(<0;40)) &  Vle6_vle7(<-133;- 30)...<0;1))   0.94 0.1578 0.94 0.13   6. FUTURE WORK  Future work will continue to develop, refine and test both the  decision tree and GUHA methods on expanding data sets and  validate findings across multiple presentations of the modules.  Additional demographic data will be added in pursuit of  refinements. One possible area of interest is the use of disability  data to highlight accessibility issues around module materials.  Another key issue is to develop a formal representation of module  design, to allow parameters to be set for each module that  describes important aspects of the module that can help to  improve the modelling. For example, to know the number and  timing of TMA s, whether or not they are compulsory, if they can  be substituted, or if they are made deliberately harder or easier.  The overall goal of future work is to implement predictions using  real-time data. The current barrier is the disparate nature of data  sources, although this is being addressed through an upcoming  data warehouse.    7. SUMMARY AND CONCLUSIONS  The results of this work indicate that using even fairly coarse- grain data about students activity on a VLE and combining this  with some other data sources, it is possible to predict ailing  students. Decision trees have been demonstrated to be suitable for   148    prediction, particularly at the start of a module, when there is  commonly a high attrition rate. The main finding has been that the  best predictor is based on changes in the students own VLE  activity, compared to their previous activity. In other words, if a  student has started out clicking and then stops, this is more of a  warning than if their clicking behaviour has been low to start with.  Similarly, if a low-clicking student reduces clicking by only a  small amount, then this may be significant in terms of the  percentage drop compared to their previous clicking behaviour,  rather than in terms of their overall level of activity.   GUHA has been investigated as a method both for improving the  explanations of student failure and for being more able to  accurately predict later drop out. The preliminary results  demonstrate that GUHA is successful in this aim. For both  decision trees and GUHA, the results are accurate when applied  across different presentations of the same module. Whereas the  decision tree, being more general, can have some success when  applied across modules, the varying nature of module design  means that the more detailed explanations of GUHA are unlikely  to hold from one module to the next.    Taken overall, these findings suggest that online learners have  different learning approaches and this is reflected in their use of  the VLE. Online learners do not have to sit in front of a computer  and click to learn, instead they may read a page once and make  notes, or print it out, or save it onto their computer for offline use.  It is not possible to draw conclusions about learner engagement  solely based the amount they click, nor even by the types of  activities they are clicking on. In terms of informing future  research, both within the Open University and for other  institutions, the suggestion is to develop profiles of online  learning styles with which to classify the different online learning  styles and from which it is possible to identify changes in  behaviour that can indicate a developing problem. Similarly, there  is a need to take into account the interplay between how a module  is structured and how the VLE is intended to be used within that  structure. This is especially true when making predictions using  VLE activity, since it could easily be some feature of a particular  module that influences VLE behaviour, such as an assessment has  been made deliberately easy (which could mean less required  activity) or else a lot of module materials need to be read or  referenced for another (thereby increasing activity for that TMA).   The work described in this paper opens up several interesting  possibilities for future work and already provides results that  could be integrated with live data to produce information to help  tutors to provide earlier interventions to struggling students.   8. ACKNOWLEDGMENTS  Our thanks to JISC for their funding and support of this work.   9. REFERENCES  [1] Arnold, K.E. and Pistilli, M.D. (2012) Course Signals at   Purdue: Using Learning Analytics to increase student  success. LAK12, 29 April  2 May, Vancouver, Canada   [2] Campbell, J.P. and Oblinger, D.G. (2007) Academic  Analytics. White paper. Educause.  http://net.educause.edu/ir/library/pdf/pub6101.pdf   [3] Dekker, G., Pechenizkiy, M. and Vleeshouwers, J. (2009)  Predicting Students Drop Out: a Case Study, In Proceedings  of the 2nd International Conference on Educational Data  Mining (EDM'09), pp. 41-50   [4] Diaz, D. P. (2000). Comparison of student characteristics,  and evaluation of student success, in an online health  education course. Unpublished doctoral dissertation, Nova  Southeastern University, Fort Lauderdale, Florida. Retrieved  from  http://academic.cuesta.edu/ltseries/sitepgs/pdf/dissertn.pdf   [5] Frankola, K. (2002). Why online learners drop out. Crain  Communications, Inc. Retrieved from  http://www.kfrankola.com/Documents/Why%20online%20le arners%20drop%20out_Workforce.pdf   [6] Fusch, D. (2011) Identifying At-Risk Students: What Data  Are You Looking At Retrieved from  http://www.academicimpressions.com/news/identifying-risk- students-what-data-are-you-looking   [7] Galusha, J. M. (1997). Barriers to learning in distance  education. Retrieved from  http://www.infrastruction.com/barriers.htm.   [8] Hjek, P., Havel, I., and Chytil, M. (1966). The GUHA  method of automatic hypotheses determination. Computing,  1, 293-308   [9] Hjek P., Holea and Rauch J. (2010) The GUHA method  and its meaning for data mining. J. Computer and System  Sciences, 76, 3448.   [10] Hammang, J.M., Campbell, J.P., Smith, V.C., Sweeley, D.,  Ushveridze, A. and Woosley, S. (2010) Predictive Analytics:  Building a Crystal Ball for Student Success. Webinar.  http://www.uregina.ca/orp/PapersPresentations/SCUP_Webi nar_Sept29_2010.pdf   [11] Mazza, R. and Dimitrova, V. (2004) Visualising Student  Tracking Data to Support Instructors in Web-Based Distance  Education, 13th International World Wide Web Conference -  Alter-nate Educational Track, 154-161.    [12] Pistilli, M.D. and Arnold, K.E. (2010) Purdue Signals:  Mining real-time academic data to enhance student success.  About Campus, July-August, 22-24 DOI: 10.1002/abc.20025   [13] Quinlan, J. R. (1993) C4.5: Programs for Machine Learning.  Morgan Kaufmann Publishers.   [14] Rivera, J. C. and Rice, M. L. (2002). A comparison of  student outcomes & satisfaction between traditional & web  based course offerings. Online Journal of Distance Learning  Administration. The State University of West Georgia, 5(3).   [15] Simpson, O. (2004) The impact on retention of interventions  to support distance learning students, Open Learning: The  Journal of Open and Distance Learning , 19, 1, 79-95   [16] Simpson, O. (2008) Motivating learners in open and distance  learning: do we need a new theory of learner support Open  Learning: the journal of open and distance learning, 23, 3,  159-170   [17] Smith, V.C. and Sweeley, D. (2010) Predictive Analytics:  Increasing Online Student Engagement and Success.  Retrieved from  http://www.uregina.ca/orp/PapersPresentations/SCUP_Webi nar_Sept29_2010.pdf   [18] Thompson, M.M. (1998). Distance learners in higher  education. In C.C. Gibson (Ed.), Distance learners in higher  education (pp. 9-24). Madison, WI: Atwood Publishing.        149      "}
{"index":{"_id":"21"}}
{"datatype":"inproceedings","key":"Lauria:2013:OAA:2460296.2460325","author":"Laur'ia, Eitel J. M. and Moody, Erik W. and Jayaprakash, Sandeep M. and Jonnalagadda, Nagamani and Baron, Joshua D.","title":"Open Academic Analytics Initiative: Initial Research Findings","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"150--154","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460325","doi":"10.1145/2460296.2460325","acmid":"2460325","publisher":"ACM","address":"New York, NY, USA","keywords":"course management systems, data mining, intervention, learning analytics, open source, portability","abstract":"This paper describes the results on research work performed by the Open Academic Analytics Initiative, an on-going research project aimed at developing an early detection system of college students at academic risk, using data mining models trained using student personal and demographic data, as well as course management data. We report initial findings on the predictive performance of those models, their portability across pilot programs in different institutions and the results of interventions applied on those pilots.","pdf":"Open Academic Analytics Initiative:   Initial Research Findings   Eitel J.M. Laura, Erik W. Moody , Sandeep M. Jayaprakash, Nagamani Jonnalagadda, Joshua D. Baron  Marist College, Poughkeepsie, NY, USA     ABSTRACT  This paper describes the results on research work performed by  the  Open Academic Analytics Initiative, an on-going research  project aimed at developing an early detection system of college  students at academic risk, using data mining models trained using  student personal and demographic data, as well as course  management data. We report initial findings on the predictive  performance of those models, their portability across pilot  programs in different institutions and the results of interventions  applied on those pilots.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning   General Terms  Algorithms, Measurement, Design, Experimentation     Keywords  Learning Analytics, Open Source, Data Mining, Course  Management Systems, Portability, Intervention   1. INTRODUCTION  During the spring 2012 semester, the Open Academic Analytics  Initiative (OAAI), led by Marist College, successfully deployed  an open-source Learning Analytics solution, developed by the  project during the fall, at two community colleges (Cerritos  College and College of the Redwoods) and one Historically Black  College and University (HBCU) (Savannah State University) as a  means to further research in this emerging field.  Our spring pilots  involved a total of 1379 students, 67% of whom were considered  low-income students, who were enrolled in introductory-level  courses with, generally, three sections each being taught by the  same instructor (e.g. BIOL 101 Section 1, 2 and 3).  Each course  section was then assigned to either a control or one of two  treatment groups, thereby standardizing the instructional delivery  to the extent possible across all three.  Students in the two  treatment groups who had been identified by our predictive  model, which uses student demographic, aptitude and course  management system usage data, as being likely to not complete  the course received interventions designed to help them succeed.      This paper discusses our results from our spring pilots with  regards to our two primary research areas:  the portability of  predictive models from one academic context to another and the  effectiveness of different intervention strategies.  In summary, we  have found the following to date:   The predictive model developed using student data from Marist   College was very similar with regards to the predictive  elements (e.g. cumulative GPA) and correlation strengths as the  predictive model developed at Purdue University by Dr. John  Campbell.    The accuracy of the predictive model built using Marist student  data performed considerably higher than sheer randomness  when deployed at both community colleges and HBCUs which  exceed our expectations.    We have found a statistically significant difference between  mean course grades when comparing all students in our two  treatment groups (awareness and OASE) to our controls.    In addition, we have found a statistically significant difference  with regards to content mastery (a C grade or above) as well  as withdrawal rates between our combined treatment groups  and our controls.   These findings, which are detailed on the following pages, are  noteworthy as they indicate that predictive models used in  Learning Analytics are more portable than initially anticipated  as well as the fact that our intervention strategies have been  effective in improving student outcomes.   2. PREVIOUS WORK  Academic analytics is still a developing field, which has arisen in  higher education as a natural reverberation of the successful  application of data mining in the business world. Although it has  yet to be implemented broadly across a range of institutional  types, student populations and learning technologies, the  increasing amount of research shows the level of interest that this  domain has attained in both the data mining and the educational  communities. A sample of this on-going research is listed below.  As early as 2004, clustering algorithms were used to find affinity  patterns of user behavior in course management systems  [9]. Data  mining of temporal participation indicators was applied to  measure student contributions to discussion forums in online  courses [6]. In 2005, researchers at the University System of  Georgia used discriminant analysis on high school GPA and SAT  mathematics scores to predict completion of fully online general  education courses[7]. In his dissertation work at Purdue  University, Campbell[3] used factor analysis and logistic  regression on course management system usage data and student  demographics to produce predictive models of student  performance. Romero et al [8] applied data mining techniques on  Moodle data (Moodle is a popular open source course  management system).  More recently Bravo Agapito et al [2] used  C4.5 decision tree rules to detect symptoms of low performance in   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   150    e-learning systems. The interest in this domain  by the data mining  community as a rich applied research field is evidenced by the  fact that the KDD Cup 2010s  theme was prediction of student  performance on mathematical problems from logs of student  interaction with Intelligent Tutoring Systems [4].   3. OVERVIEW OF INITIAL  PORTABILITY FINDINGS  Many questions exist in the emerging field of Learning Analytics  with regards to the degree to which predictive models which are  built based on data from one particular institution type and student  population can be effectively deployed in academic contexts  where the institution type and/or student population differ  significantly.  The degree to which predictive models may be  portable has major implications for the scaling of learning  analytics across multiple institutions and even higher education  itself.  The OAAI has researched this issue of portability in two  phases, first looking at the degree to which a model built using  data from Marist College compared to the original model built at  Purdue University (which lead to the development of Course  Signals) and then, second, deploying the Marist-developed model  at community colleges and HBCUs and investigating the accuracy  of the model in these different academic contexts.  Results from  these two research efforts, which are detailed below, have shown  that predictive models appear to be more portable than initially  anticipated. See [5] for details regarding model implementation,  algorithms and software platforms used by the OAAI project.   3.1 Phase One Portability Research Results:  Comparing Marist Predictive Model to  Purdue Model  During the summer of 2011 the analytics research team at Marist  College developed a predictive model using the same student  demographic (e.g. age), aptitude (e.g. SAT scores), and course  management system usage (e.g. number of course visits) that were  used by Dr. John Campbell at Purdue University in his original  dissertation research [3], except our data was from Marist students  and based on usage of the Sakai Course Management System  (CMS).  Although Marist College and Purdue University differ in  obvious ways (e.g., institutional type, size, and instructional  approaches) they do share a number of similarities which are  particularly pertinent to this study.  These include percentage of  students receiving federal Pell Grants (Marist 11%, Purdue 14%),  percentage Asian/Black/African American/Hispanic students  (Marist 11%, Purdue 11%), and ACT composite 25th/75th  percentile (Marist 23/27, Purdue 23/29).   We compared the model predictors that were correlated with  student grades (as was done at Purdue) as means to understand to  the degree to which the models differed.  In general, we found the  same statistically significant elements as Purdue with similar  correlation strengths.  These initial findings on portability were  included in a paper presented at the 2012 international Learning  Analytics and Knowledge (LAK) conference [5].   3.2 Phase Two Portability Research:  Deployment of Marist Model in Different  Academic Contexts   After the phase one research efforts, additional time was spent  improving the initial predictive model through additional machine  learning techniques and the introduction of additional data  elements (e.g. gradebook data from the CMS).  The enhanced   predictive model was then evaluated using a Marist test data set  which had been excluded during the development of the model.   Table 1 provides a summary of the outcomes from this evaluation  (a total of ten trials were run but the table only includes the means  from these tests).   Table 1. Prediction analysis using Marist test data      This model was then deployed as part of our course pilots during  representing vastly different educational contexts as compared to  Marist.  For example, Savannah State has a 94% Black non- Hispanic student population with 67% of the student receiving  Pell Grants, College of the Redwoods student population has  22% minorities and 60% are receiving Pell Grants and Cerritos  Colleges student body is 41% Hispanic and 45% of the students  are receiving Pell Grants.  At the conclusion of the spring semester, a similar evaluation was  completed to determine how well the model performed when  deployed in these different academic contexts.  This evaluation  included assessing the models performance at three points during  the semester (25%, 50% and 75% of the semester completed),  which correspond to when Academic Alert Reports were provided  to instructors, to evaluate how the models performance improved  as more CMS and gradebook data was available.     Table 2. Prediction analysis from Spring 2012 pilots      Table 2 provides a summary of the results of this evaluation.   Looking at the accuracy of the model (percentage of students that  were correctly identified), just as one indicator, it is clear that the  results from the predictive analysis were considerably higher than  sheer randomness.  For example, when one compares the accuracy  at the 75% of the semester completed point, which ranged from  75% to 79%, to the accuracy of the model when tested with   151    Marist data (which was historical and thus represented 100% of  the semester completed), which was 86-87%, we find only a 6- 10% difference.  Given that we expected a much larger difference  between how the model performed when tested with Marist data  and when deployed at community colleges and HBCUs, this was  an encouraging finding.   3.3 Portability Research Discussion  Our findings from Phase One and Two of our portability research  seem to indicate that predictive models that are developed based  on data from one institution may be scalable to other institutions,  even those that are different with regards to institutional type,  student population and instructional practices.  We believe this  unexpected finding may be the result of the specific elements of  the predictive models which have shown to be the most powerful  predictors.  The elements that are most predictive of student  outcomes, which were also identified by Purdue in their research,  are cumulative GPA and data from the CMS gradebook.  Given  that these two elements are such fundamental aspects of academic  success it is not all that surprising that the predictive model has  fared so well across these different institutions.   If this explanation is correct, it does point to the importance of  instructors using the gradebook within their CMS if they wish to  take advantage of learning analytics.  It also indicates that models  may not port well to institutions where cumulative GPA is not  available (e.g. non-credit training programs) or if the student  population is entering an institution and thus GPA is not yet  available.  Thus, although our initial findings are encouraging  with regards to portability, important questions remain with  regards scaling up models across more diverse academic settings.   4. OVERVIEW OF INITIAL  INTERVENTION RESEARCH  OAAI has also conducted research on two different intervention  strategies which were deployed to at risk students as means to  help them succeed in the course.  One of these strategies is  awareness messaging, which is closely aligned with the  approach taken by Purdues Course Signals project, which entails  the instructor sending a message to the at risk student noting  their concern over the students academic performance and then  suggesting specific steps the student should take to improve (e.g.  meet with a tutor, attend a study group session, etc.).  The text of  these messages, outside of what the instructor suggests as means  to improve, was standardized across all of the treatment groups.   The other intervention strategy is referred to as the Online  Academic Support Environment or OASE and it parallels the  awareness messaging strategy in that students receive the same  initial message noting concern but are then invited to join an  Sakai-based online support site in which they are given access to  Open Educational Resources (OER) instructional materials (e.g.  Khan Academy videos, Flat World Knowledge textbooks, etc.).   In addition to these materials, they are provided with a range of  mentoring from peers and professional support staff.  As with the  awareness messaging, the text of the messages is standardized  across instructors and the text becomes increasingly serious in  tone as students receive their second and third message.   4.1 Intervention Research:   Impact on  General Student Academic Success   We began our analysis by comparing, through a one-way  ANOVA, the overall academic success, as measured by average  course grades, between the two treatment groups and the controls,   The results indicate there was no difference between the  Awareness and the OASE treatment groups. However, we did find  a statistically significant difference between both treatment groups  and the control group (Awareness: M =77.47, SD = 13.34; OASE:  M =77.5, SD = 13.44; control group: M =75.17, SD = 13.8;  F  (2,299) = 3.065,  p = .047*, see Fig.1) which we feel is impressive  as the groups include all students in the classes assigned to the  three groups regardless of whether they were identified by the  model as being at risk or not.   50  60  70  80  90  100  Awareness OASE Control  Fi na  l G ra  de  (%  )  Mean Final Grade for All Students     Figure 1. Average Course Grade Analysis    We then refined this analysis by comparing, also through a one- way ANOVA, what we believe represents those students who  were the most at risk.  In the treatment groups (awareness  messaging and OASE), we consider only students who had  received at least one intervention based on any of the three  Academic Alert Reports that were posted during the semester.     50  60  70  80  90  100  Awareness OASE Control  Fi na  l G ra  de  (%  )  Mean Final Grade for  at Risk  Students     Figure 2. Refined Course Grade Analysis   For controls, which by definition did not receive interventions, we  selected those students who had been identified as having an  average risk level of three or higher across all three Academic  Alert Reports.  Students were categorized, based on scoring by the  predictive model, as having:  no risk (1), low risk (2), medium  risk (3) or high risk (4) with regards to their likelihood of not  completing the course successfully. Once again we did not find a  difference between the Awareness and OASE treatment groups.  However, in this analysis we found even greater statistical  significance between both treatment groups and the control group  (Awareness: M=72.05, SD=13.5; OASE: M=72.43, SD = 14.06;  control group: M =65.38, SD = 11.8; F (2,448) = 8.484, p = .000*,  see Fig. 2) which may indicate that the interventions were of  particular benefit to those at risk to not succeed academically.  Finally, we also analyzed the impact the interventions had on  low income students with regards to their average course  grades. This comparison, like the first ANOVA, includes all   152    students in every class regardless of risk level. This pilot study did  not provide adequate N to all for an analysis of the at risk  students with low income1 status.   50  60  70  80  90  100  Awareness OASE Control  Fi na  l G ra  de  (%  ) Mean Final Grade for Low Income Students     Figure 3. Low Income Student Analysis   In these case there were no statistically significant differences  between any of the groups. There is however, a trend consistent  with the two previous ANOVAs where the Awareness and OASE  groups are approaching significance difference from the control  group (Awareness: M =76.85, SD = 13.2; OASE: M =76.61, SD =  13.7; control group:  M =74.14, SD = 13.7; F (2,691) = 2.601,  p =  .075, see Fig.3). The trend seems to indicate that low income  students in the treatment groups performed better than those in the  control groups.  Since the number is close to being significant we  are particularly interested in performing analysis on a larger  sample containing Pell Grant students which we hope to collect  during the Fall pilots.   4.2 Intervention Research:   Impact on  Content Mastery   We also examined, through a  Chi-square analysis, the impact  our interventions had on  student content mastery (i.e.  received a C or better in the  course).  We performed this  analysis by comparing the  controls to both treatment  groups combined to increase  our sample size.          Figure 4. Students who Mastered Content (C or above)   1 Low-income students were defined as students who were receiving Pell  Grants.   We found a statistically significant difference ( 2 (1) = 8.913, p =  .003*, see Fig. 4) indicating that our treatment groups achieved  better learning outcomes than those in our control groups.   4.3 Intervention Research:   Impact on  Withdrawal Rates   Finally, we examined the impact our interventions had on  withdrawals by comparing  withdrawal rates among those students  identified as being at risk on the first Academic Alert Report in  our control groups to the withdrawal rate of the combined  treatment group (both awareness messaging and OASE).  We  chose to focus on those students identified during the first  Academic Alert Report as we have additional evidence that  interventions early in the semester have the largest impact on  student outcomes and come at a time when withdrawal is possible  without incurring the maximum penalties (e.g. no tuition refund).    Based on the Chi-square analysis we have identified a statistically   significant difference ( 2 (1) = 7.097, p = .008*, see Fig. 5)  between the two aforementioned groups which indicates that  students in our combined treatment group were more likely to  withdraw than those in our control groups.  Although this finding  may at first seem to be a negative outcome, it is consistent with  findings at Purdue and may be an indication that students who  receive interventions are withdrawing earlier in the course (as  opposed to remaining enrolled and failing) than those who do not.   While we would much prefer that students complete their course  successfully, withdrawing, particularly before incurring major  penalties, is preferable over failing and thus we see this as a  potentially positive result.  We are working to explore at what  point in the semester students did withdraw as means to  investigate this further.   0  100  200  300  400  500  Yes No Yes No  Withdrawal rates for  at Risk  Students  Control Intervention  Fr eq  ue nc  y    Figure 5. Analysis of Withdrawal Rates   5. CONCLUSION AND FURTHER  RESEARCH AGENDA  This paper reports on initial research findings of the Open  Academic Analytics Initiative2. Predictive models were trained  and tested using Marist College data, and those models were then  applied on pilot runs using data from several partner institutions.  The research tested the portability of those models, and the  success of intervention strategies in improving at Risk student  outcomes. The results are promising as they seem to point at a  higher portability of those models than initially anticipated. These   2 We apologize if weve left out any material desired by prospective  readers, due to lack of space.  However, we will provide more complete  coverage of these topics at the conference.     0  100  200  300  400  500  Yes No Yes No  Content Mastery for  at Risk  Students  Control Intervention  Fr eq  ue nc  y  153    results had a subsequent positive impact of the effectiveness of  interventions on students at academic risk. We hope that these  results will encourage researchers from other institutions to  develop similar strategies of early detection and intervention of  academic risk.  Additional pilots were completed during the Fall 2012. We hope  to use the Fall 2012 data to: (a) confirm our findings from the  Spring 2012 data set;  (b) increase our overall sample sizes (n)  which we believe will allow us to identify additional correlations  which are statistically significant;  (c) look at our impact on  persistence rates which require two semesters worth of data; (d)  explore how differences between course subject matter or size  might affect the impact of our interventions and/or be a factor in  our predictions; (e) look for relationships between student  responses to questions provided by the National Survey of  Student Engagement (NSSE) (which were incorporated into our  OAAI Student Survey) and the receipt of an intervention.  We are  also planning on conducting interviews with select instructors and  students to identify strengths and weakness of the interventions.  These insights will help us identify ways to refine our intervention  approaches.   We have also begun to discuss and identify areas of research that  we believe will be important to the field of Learning Analytics as  it begins to be deployed more widely.  These research questions,  which are outlined below, could form the basis for a national  research agenda in this new and emerging field.   Does Learning Analytics allow us to identify students at risk to  not complete a course that the typical instructor would miss   . Although many of the instructors in our pilots have noted with us  that they have found the identification of at risk students very  helpful, it remains unclear to us if they would have identified the  same students as our model if they attempted to do so on their  own.  Thus, we believe it will be important to conduct research  studies in which instructor predictions are compared to model  predictions    What are the characteristics of students who seem to have  immunity to the treatment (those who got interventions but  never improved) and those who were effectively treated after just  one intervention   From our initial research we have found that students seem to fall  into one of two broad categories, those who improved after  receiving just one treatment or intervention and those who did  not improve regardless of the number of treatments received.   Very few students who did not improve after the first intervention  went on to improve after the second or third.  Our theory is that  some students respond very well to the treatment and thus  improve after just one intervention while other seem immune to  the treatment and do not improve regardless of how many  treatment their receive.  Understanding why this is the case and  what characteristics are associated with these two categories of  students would help us better understand how to most effectively  deploy interventions.   How portable are predictive models that are designed for one type  of course delivery (e.g. face-to-face) when they are deployed in  another delivery format (e.g. fully online)   We are particularly interested in exploring the issue of portability  with regards to face-to-face and fully online programs given how  much more CMS usage takes place in the later mode of  instruction.     6. ACKNOWLEDGEMENTS  This research is supported by EDUCAUSEs Next Generation  Learning Challenges, funded through the Bill & Melinda Gates  Foundation and The William and Flora Hewlett Foundation. It is  also partially supported by funding from the National Science  Foundation, award numbers 1125520 and 0963365.   7. REFERENCES  [1] Baepler, P., & Murdoch, C. J. (2010). Academic Analytics   and Data Mining in Higher Education. International Journal  for the Scholarship of Teaching and Learning 4(2).   [2] Bravo Agapito, J., Sosnovsky , S., & Ortigosa, A. (2009).  Detecting Symptoms of Low Performance Using Production  Rules. Paper presented at the 2nd International Conference  on Education Data Mining, Cordoba, Spain.   [3] Campbell, J. P. (2007). Utilizing Student Data within the  Course Management System to Determine Undergraduate  Student Academic Success: An Exploratory Study (Doctoral  dissertation, Purdue  University, 2007).  (UMI No. 3287222).   [4] KDD Cup 2010. This Year's Challenge. Available:  http://www.sigkdd.org/kddcup   [5] Laura E., Baron J., Devireddy M., Sundararaju V.,  Jayaprakash S. (2012),  Mining academic data to improve  college student retention: An open source perspective ,  Proceedings of LAK 2012, Vancouver, BC, Canada, April 29  - May 2, 2012   [6] Laurie, P. D., & Timothy, E. (2005). Using data mining as a  strategy for assessing asynchronous discussion forums.  Comput. Educ., 45(1), 141-160.   [7] Morris, L. V., Wu, S., & Finnegan, C. ( 2005). Predicting  retention in online general education courses. The American  Journal of Distance Education, 19(1), 23-36.   [8] Romero, C., Ventura, S., & Garcia, E. (2008). Data mining in  course management systems: Moodle case study and tutorial.  Comput. Educ., 51(1), 368-384.   [9] Talavera, L., & Gaudioso, E. (2004). Mining student data to  characterize similar behavior groups in unstructured  collaboration spaces. Paper presented at the Workshop on AI  in CSCL.   8. AUTHORS ADDRESSES  Eitel J.M. Laura, Nagamani Jonnalagadda, School  of  Computer  Science  and   Mathematics, Marist   College, Poughkeepsie, NY  12601. USA   Erik W. Moody, School of Social and Behavioral Sciences, Marist    College, Poughkeepsie, NY 12601. USA   Sandeep M. Jayaprakash, Learning Analytics Specialist, Marist    College,  Poughkeepsie, NY 12601. USA   Joshua D. Baron, Senior Academic Technology Officer, Marist    College,  Poughkeepsie, NY 12601. USA   Email: {Eitel.Lauria, Nagamani.Jonnalagadda1, Erik.Moody ,  Sandeep.Jayaprakash1,  Josh.Baron}@marist.edu      154    1. INTRODUCTION  2. PREVIOUS WORK  3. OVERVIEW OF INITIAL PORTABILITY FINDINGS  3.1 Phase One Portability Research Results: Comparing Marist Predictive Model to Purdue Model  3.2 Phase Two Portability Research: Deployment of Marist Model in Different Academic Contexts   3.3 Portability Research Discussion   4. OVERVIEW OF INITIAL INTERVENTION RESEARCH  4.1 Intervention Research:   Impact on General Student Academic Success   4.2 Intervention Research:   Impact on Content Mastery   4.3 Intervention Research:   Impact on Withdrawal Rates    5. CONCLUSION AND FURTHER RESEARCH AGENDA  6. ACKNOWLEDGEMENTS  7. REFERENCES  8. AUTHORS ADDRESSES     "}
{"index":{"_id":"22"}}
{"datatype":"inproceedings","key":"d'Aquin:2013:IDM:2460296.2460327","author":"d'Aquin, Mathieu and Jay, Nicolas","title":"Interpreting Data Mining Results with Linked Data for Learning Analytics: Motivation, Case Study and Directions","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"155--164","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460327","doi":"10.1145/2460296.2460327","acmid":"2460327","publisher":"ACM","address":"New York, NY, USA","keywords":"course enrolment, data mining, interpretation, leaning analytics, linked data, sequence mining","abstract":"Learning Analytics by nature relies on computational information processing activities intended to extract from raw data some interesting aspects that can be used to obtain insights into the behaviours of learners, the design of learning experiences, etc. There is a large variety of computational techniques that can be employed, all with interesting properties, but it is the interpretation of their results that really forms the core of the analytics process. In this paper, we look at a specific data mining method, namely sequential pattern extraction, and we demonstrate an approach that exploits available linked open data for this interpretation task. Indeed, we show through a case study relying on data about students' enrolment in course modules how linked data can be used to provide a variety of additional dimensions through which the results of the data mining method can be explored, providing, at interpretation time, new input into the analytics process.","pdf":"Interpreting Data Mining Results with Linked Data for Learning Analytics:  Motivation, Case Study and Directions  Mathieu dAquin Knowledge Media Institute, The Open University  Walton Hall, Milton Keynes, MK7 6AA, UK mathieu.daquin@open.ac.uk  Nicolas Jay Universit de Lorraine, LORIA, UMR 7503  Vandvre-ls-Nancy, F-54506, France nicolas.jay@loria.fr  ABSTRACT Learning Analytics by nature relies on computational infor- mation processing activities intended to extract from raw data some interesting aspects that can be used to obtain insights into the behaviours of learners, the design of learn- ing experiences, etc. There is a large variety of computa- tional techniques that can be employed, all with interesting properties, but it is the interpretation of their results that really forms the core of the analytics process. In this paper, we look at a specific data mining method, namely sequen- tial pattern extraction, and we demonstrate an approach that exploits available linked open data for this interpreta- tion task. Indeed, we show through a case study relying on data about students enrolment in course modules how linked data can be used to provide a variety of additional dimensions through which the results of the data mining method can be explored, providing, at interpretation time, new input into the analytics process.  Categories and Subject Descriptors I.5 [Pattern Recognition]: Applications; H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing  General Terms Design, Experimentation  Keywords leaning analytics, course enrolment, data mining, sequence mining, linked data, interpretation  1. INTRODUCTION The most commonly found definition of Learning Analyt-  ics is given in [9] (citing the LAK 2011 call for paper) as:  Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 13, April 08 - 12 2013, Leuven, Belgium Copyright 2013 ACM 978-1-4503-1785-6/13/04 ...$15.00.  the measurement, collection, analysis and re- porting of data about learners and their con- texts, for purposes of understanding and opti- mising learning and the environments in which it occurs.  Such tasks naturally require computational tools and tech- niques in order to process the raw data obtained from educa- tional systems or through other data collection method, to support the overall analysis of these data and the generation of exploitable insights from them by an expert, a teacher or directly by the learner.  Here, we are especially interested in the use of data min- ing to support learning analytics. The use of data mining methods on education-related data is generally studied in the area of Educational Data Mining (see [15] for a com- parison of the two fields of Educational Data Mining and Learning Analytics). We however do not focus in this paper on the algorithmic techniques used on educational datasets, but on the aspect of using data mining which is specially rel- evant to Learning Analytics (as a process centred on human understanding and decision making): the interpretation of the results of data mining.  Indeed, a data mining method can extract from raw data patterns of interest to the application domain (because, for example, of their frequency). While these patterns are useful as the starting point of an analytics process, the challenge here is to navigate and explore these patterns in order to come up with a meaningful analysis: an interpretation or a model that can explain the patterns and be used to exploit them as useful insights for decision making. This interpre- tation process cannot however be purely based on the raw data and the patterns extracted from them. Indeed, in or- der to achieve such an understanding of the results of a data mining method, the person conducting the analysis (which we call here the analyst) needs to bring into the process additional information about the domain being analysed. This is especially challenging in a Learning Analytics sce- nario since, first, the analytics process does not pre-suppose any particular dimension in the data to be more significant than any other. In other terms, Learning Analytics is much more related to exploratory analysis than hypothesis testing or model validation. It is only once the patterns have been extracted that the analyst can explore what aspects of the domain might be relevant to interpret them, and therefore, what external information to bring into the process. Sec- ond, because of the nature of the education domain, there are a potentially infinite number of such dimensions that can explored for interpretation (from the subject of the courses  155    to elements of the learning environment and context). Un- derstanding and identifying what analytical dimensions to bring into the analytics process therefore cannot be done a priori, but requires an interactive process where views over the results of the data mining method can be created out of selected dimensions at runtime.  In this paper, we present a method that exploit external information available as linked open data to support the in- terpretation of data mining results, through automatically building a navigation/exploration structure in the results of a particular type of data mining tool (namely, sequence min- ing) based on data dimensions chosen by the analyst. We demonstrate this method through a use case based on data about students enrolment in course modules. Through the results we obtain in this use case, we show the need and advantages of combining data mining with linked data for Learning Analytics, as providing both a way to automati- cally identify patterns in raw data, and to support the hu- man interpretation and exploitation of these results through customisable views based on analytical dimensions found in external data.  We further discuss how this approach provides an initial instantiation of a more general approach to the combination of data mining and linked data (as promoted for example in [5]), and how such an approach is generally relevant to Learning Analytics.  2. MINING ENROLMENT DATA In order to illustrate the need and general idea for linked  data-based interpretation of data mining results, we rely on a concrete scenario: the analysis of student enrolment data. For many years, students could freely choose what course modules to take at the Open University1. There are how- ever naturally a number of criteria they would use to choose what course to enrol to at what time: the level, number of credits, subjects of the course, how it would count to- wards a degree, etc. It is therefore generally interesting to look at concrete data about the enrolment of students into these modules, and to try to extract indicators of recurrent patterns of enrolment across time.  2.1 Data The data we use is obtained from the Open Universitys  Course Profile Facebook application2, where students and prospective students can indicate what courses they are fol- lowing, intend to follow or have followed in the past. The applications is useful to students as it helps identifying rele- vant resources to the courses of interests, as well as potential contacts following similar courses.  We use of snapshot of the database behind this appli- cation with anonymised identifiers, which contains 43,226 records on 8,806 students. Each record contains the informa- tion about the relationship between a student and a course (currently studying, completed or intend to study) at a certain date. For example, typical records are shown in Table 1.  2.2 Mining Method: Sequential Pattern Min- ing  1http://www.open.ac.uk 2http://apps.facebook.com/courseprofiles/  Table 1: Example records from the student enrol- ment data.  Student ID Course Code Status Date  112 dse212 Studying 2007 112 d315 Intend to study 2008 109 A207 Completed 2005  In order to analyse the type of data described above, i.e. students enrolment in course modules across time, sequen- tial pattern mining [1] appears as a natural approach. It can be seen as an extension of the well known association rule problem, applied to data that can be modelled as se- quences of itemsets, indexed for example by dates. It helps to discover rules such as: customers frequently buy DVDs of episodes I, II and III of Stars Wars, then buy within 6 months episodes IV, V, VI of the same famous epic space opera. Sequential pattern mining has been successfully used so far in various domains, including for example the analysis of patients trajectories in hospitals (see [8]).  In order to apply sequential pattern mining to our en- rolment data, we re-model the data as a set of sequences of itemsets. Itemsets contain items which in our case are the course codes mentioned in the data, and are ordered in sequences based on the dates at which the corresponding courses have been studied or completed by a given student. In other terms, each sequence represents the trajectory of a particular student in our data. An example of such a tra- jectory sequence is given below:  (DD100) (D203, S180) (S283)  This means the student followed course DD100 the first year, courses D203 and S180 the second year, and course S283 the third year.  Once reformulated in this way, the data is used as input of a sequence mining tool: the Prefixspan algorithm [12]. The main parameter of the tool is the support threshold, which is the minimum number of sequences in which a pattern should appear to be considered frequent. In the considered example, we used 100 as a support threshold.  2.3 Results and why they need Interpretation The result of the sequential pattern mining is a set of  frequent sequential patterns, which are sequences that fre- quently occur in the set of sequences given as input of the process, as well as their respective support, which is the num- ber of input sequences that include the pattern. Using a minimum support threshold of 100, we obtained from the data described above 126 different patterns, which include 1 to 3 itemsets each. Examples of some frequent patterns are given in Table 2.  Table 2: Examples of frequent sequential patterns.  Sequential Pattern Support  (DD100)  (DSE212) 232 (DSE2012)  (ED209)  (DD303) 150 (B120)  (B201) 122  156    We can clearly see here what would be the benefit of using such an approach if the goal was the automatic recommen- dation of relevant courses to certain students, or to give them options that draw on the most common trajectories (e.g., once you have done DD100, you might want to do DSE212; If you have done DSE212, ED209 and/or DD303 and/or some other frequent associations are also relevant). However, in a Learning Analytics scenario, where our goal is to better understand how students choose courses, what are the rationals behind these choices and how it could affect the design of the course modules and of proposed standard trajectories, these results raise more questions than they an- swer: What are the relationships between the topics of the course in common patterns How do the steps relate to course levels How are the patterns affected by the type of assessments used in each course by the credits obtained for each course etc.  Answering these questions, and more generally exploring the data mining results according the corresponding dimen- sions, requires background knowledge about all the different aspects of the items included in patterns. Here for example, to understand the second pattern in Table 2, it is useful to know that DSE212 is a level 2 course generally on psychol- ogy, and that ED209 is also a level 2 course but focusing more specifically on child development within psychology.  This illustrates our idea that interpreting the results of a data mining method within a Learning Analytics scenario requires to bring into the process external information about the various dimensions through which the items (here, the courses) included in extracted patterns can be explored. The difficulty is however that it is hard to anticipate in advance which of these dimensions will be relevant to the analyst at the time of interpretation, and more generally that we need a convenient way to integrate within these data min- ing results external information about a large variety of dif- ferent perspectives on the analysed data, in order to leave sufficient flexibility to the analyst to explore and interpret the results according to different views. Such difficulties motivate our approach to the interpretation of data mining results through the integration with linked data. We detail this approach and the tools built to implement it in the next sections, and illustrate them on the course enrolment exam- ple presented above, using linked open data about course modules from the Open Universitys linked data platform3.  3. INTERPRETING DATA MINING RESULTS WITH LINKED DATA  Linked Data [11] is a set of principles and technologies that rely on the architecture of the Web (URIs and links) to share, model and integrate data. The basic idea is that data objects (e.g., a book) are identified by web addresses (URIs), and the information attached to these objects are represented through links (themselves labeled with URIs) to values (e.g., the book title) or other URIs representing other objects (e.g., the author of the book). Besides this simple technological model, the main novelty introduced by linked data is this idea that raw data is represented and exposed directly on the Web, making the Web a collective data space connecting contributions from any possible sources. In our example, information about the book might be contributed  3http://data.open.ac.uk, see [4, 19]  by an organisation and information about the author by another.  This idea of Web-scale, global data integration has led to the principles of Linked Data being widely adopted espe- cially by organisations taking advantage from the widespread dissemination of public information and open data. This includes in particular governments (see e.g., http://data. gov.uk), libraries (see e.g., http://data.bnf.fr) and, of course, educational institutions (with the Open University pioneering the use of Linked Data for education; see http: //data.open.ac.uk, http://linkeduniversities.org, as well as [3]).  Considering this state of Linked Data, it seems therefore natural to investigate it as a source of additional information to support the interpretation of the results of data mining methods, such as the ones presented in the previous sec- tion. Below, we describe an approach using a linked data endpoint to collect descriptive dimensions about the items that constitute the extracted patterns, and to use these di- mensions to automatically construct exploration/navigation structures into these results. Figure 1 gives an overview of this approach, which relies on a linked data-based de- scription of the data mining results, on extracting from an external linked data source selected information about these items and on organising the extracted patterns in a hierar- chy (a lattice constructed using formal concept analysis [17]) along the selected dimension.  3.1 Selecting a Dimension for the Exploration of the Data Mining Results  The data mining results we want to explore in our case are a set of sequential patterns (e.g., the ones presented in Table 2). A sequential pattern is an ordered list of itemsets (e.g., {DD100}, {D203, S180}), each of them being a set of items (e.g., DD100, D203, S180). In order to provide an exploration structure in these results, we need to support the analyst in selecting relevant dimensions on the items being included in sequential patterns. In other words, we want to use Linked Data to obtain simple descriptors of the items in the patterns, so that these patterns can be explored alongside these descriptors.  The first step in achieving this is to represent the results of data mining in a way compatible with a linked data rep- resentation, and that can be easily manipulated jointly with external linked data representations of the items being con- sidered. This includes two aspects: 1- modelling the data mining results in RDF4, in accordance with the principles of linked data and 2- ensuring that the items in the patterns are identified with URIs in reference to existing Linked Data sources.  The way we model sequential patterns in RDF/Linked Data is summarised in Figure 2. This representation is in- spired by the Sequence Ontology Design Patterns5 and is intended to be generic (i.e., it is independent from the course enrolment use case we are investigating here, and can be reused to represent any result of a sequential pattern mining process).  The second step of the representation is the one that con- nects the generic pattern mining representation described above with Linked Data-based external information about  4http://www.w3.org/RDF/ 5see http://ontologydesignpatterns.org/wiki/ Submissions:Sequence  157    Figure 1: Overview of the approach to using Linked Data in interpreting the results of data mining.  the domain. Here, we will use information about the Open Universitys course catalogue, as available in http://data. open.ac.uk. In this Linked Data platform, a course such as DSE212 is associated with a URI of the form http://data. open.ac.uk/course/dse212. For courses that are currently available, data.open.ac.uk provides information related the subject of the course (here, for example, Psychology, repre- sented by the URI http://data.open.ac.uk/topic/ psychology as a skos:Concept), the course level, the num- ber of credits, the modes of assessment, etc. For courses which are no longer available (e.g., DD100), the informa- tion provided includes links to the course material avail- able at the Universitys library (e.g., set books), as well as any other resource that relate to the course (e.g., sim- ilar courses, units of open educational material, etc.) All this information is available as RDF through accessing the URI, or through a SPARQL [18] query endpoint (http: //data.open.ac.uk/query).  The representation of sequential patterns connecting them to course items in data.open.ac.uk is loaded into a lo- cal triple store (here, we use Fuseki6), providing us with a SPARQL query endpoint for the extracted sequential pat- terns.  6http://jena.apache.org/documentation/serving_data  Figure 2: RDF Model of the pattern (DSE212)  (ED209) (DD303) with support 150.  Besides making it convenient to manipulate the results of a data mining tool, the advantage of the representation above is that, following the principles of Linked Data, the use of existing URIs for the representation of the items make it possible to bring external information about them, from linked data sources that reference these URIs. As briefly discussed above, it is from these external information that we want to build additional dimensions to explore the mined patterns. Indeed, considering an existing data endpoint (in our case, data.open.ac.uk), the properties attached to the items represent as many descriptors that enrich the initial sequences used as input of the data mining process. This applies not only to properties that are directly attached to the items, but also indirectly to any path that can be built from them in the linked data graph (i.e., any property chain) starting with the items (e.g., the labels of the subjects of the books used as course material for the selected course).  In order to help the analyst selecting the property chain he/she wants to apply as a dimension for exploration, we built an interface that allows him/her, given a specific linked data endpoint, to check what properties apply to typical items in the mined patterns (see Figure 3). It first allows the analyst to select a representative item amongst the ones present. A series of simple SPARQL queries are then used to find out, in the given linked data endpoint, what properties apply to the item, and subsequently, what properties apply to the values of these properties. In the example Figure 3, the item http://data.open.ac.uk/course/aa316 has been selected, showing that, amongst others, the property http: //courseware.rkbexplorer.com/ontologies/courseware#  has-courseware applies to this item. It then shows that the values of these properties (books and other material used as part of the course) have a number of other properties that apply to them (including http://purl.org/dc/terms/ subject: the subject of the considered resource) and that can be selected as part of the property chain used as a nav- igation dimension, as shown in the next section. For the interested reader, we show below the SPARQL query used to list these properties in data.open.ac.uk. If another prop- erty is selected, the properties of the values of this property are then shown.  SELECT distinct p where { <http://data.open.ac.uk/course/aa316}>  <http://courseware.rkbexplorer.com/ontologies/courseware #has-courseware> v.  v p [] }  3.2 Lattice-based Classification of Mined Pat- terns  158    Figure 3: Screenshot of the property chain selection tool to be used as exploration/navigation dimension in the data mining results.  The above method to select a chain of properties in a linked data source that contains information about items contained in the mined pattern has for purpose to enrich the patterns with an additional dimension for exploration, which was not present in the original data. In other terms, the values of the chain of properties that apply to the items of a pattern can be seen as a set of descriptors for the pat- tern. This enriched description of the patterns can be used to meaningfully organise them into a hierarchy, structured according to the chosen dimension.  To achieve this, we apply Formal Concept Analysis to the patterns and their set of new descriptors and build a con- cept (or Galois) lattice [16]. A concept lattice is a hierarchy of concepts (E, I), formed of an extension E (i.e., a set of objects) and an intension I (i.e., a set of attributes). Each concept groups together the objects (E) that have the same set of attributes (I). They are organised in a lattice accord- ing to relation <, which can be read as is more specific than. For example, (Ei, Ii) < (Ej , Ij) (the first concept is more specific than the second) means that Ei  Ej and Ij  Ii.  Concept lattices are not only the product of a classifica- tion method, but are also often used as we intend to do here: to provide a navigation structure to an originally raw set of data objects. In our case, the objects are the sequential patterns mined in the data (such as the ones in Table 2), and the attributes are the values of the chain of proper- ties obtained from Linked Data, from the items included in the patterns. For example, assuming we use the prop- erty chain [ http://purl.org/dc/terms/subject, http:// www.w3.org/2000/01/rdf-schema#label ], the course B120 (http://data.open.ac.uk/course/b120) is represented by the attributes {Accounting and Finance, Business and  Management}, and the course B201 (http://data.open. ac.uk/course/b201) by the attributes {Business and Man- agement, Business Management Studies}. The third pattern in Table 2 is therefore represented by the set of attributes {Business and Management, Business Management Studies, Accounting and Finance}.  Building a concept lattice is a well known problem for which many tools exist. It is a computationally expensive task, requiring a lot of resources. However, after the se- quential pattern mining phase, input data are already sig- nificantly reduced at this step (126 patterns in our example, out of the 8,806 original sequences). The number of at- tributes varies depending on the chosen property chain, but we believe that this approach is lightweight enough to be integrated within the learning analysts workflow. This is somehow demonstrated through the reasonable response time we obtained from our naive javascript implementation of a concept lattice construction algorithm (a few seconds in the case of property chains with less than 100 possible values, to a couple of minutes for property chains with more than a thousand values).  Figure 4 shows an portion of the lattice built through using the previously mentioned property chain as the nav- igation dimension ([ http://purl.org/dc/terms/subject, http://www.w3.org/2000/01/rdf-schema#label ]). In this figure, thesubjectsare showed that represent the intension of the concept, together with the support of the concept, i.e., the number of objects (here, sequential patterns) in its ex- tension.  3.3 Exploring Mined Patterns with the Lat- tice  Since the built lattice constitutes a hierarchy, it is natural to use it for navigating the data on which it was built. This  159    Figure 4: Portion of the lattice built from the sub- jects labels dimension.  has been explored before in several different domains [2]. Here however, we use the lattice to provide a further level of abstraction with respect to the original data: First, the sequential pattern mining method provides an additional structure over individual sequences of courses and second, the lattice provides a way to classify and explore these pat- terns according to dimensions brought through Linked Data.  Following this idea, Figure 5 shows the previously intro- duced interface with the navigation structure created. The chain of properties corresponding to the labels of subjects of courses has been selected here as the navigation dimension. The central part of the interface now shows an expandable hierarchy based on the built lattice, starting from the top concept (the one with an empty extension and all the pat- terns in the intension). Concepts which include subjects such as Psychology, Science or Law are included at the first level, meaning that these concepts group sequential patterns of courses that cover these subjects, together with any number of other subjects. In the figure, the concepts Science, Social SciencesandBusiness and Management have been expanded, showing concepts in which the corre- sponding subjects appear together with other subjects (e.g., Biology, Sociology). The concept Science | Biology has also been expanded, showing two sub-concepts.  Next to each concept are indicators of their size/importance. The first number is the support of the concept (in terms of number of patterns). For example, there are 30 sequential patterns of courses that cover the subjects Psychological Studies and Social Sciences. Next is the number of more general concepts, and the number of more specific concepts.  Using this basic hierarchy, we can already start investigat- ing interesting elements of the extracted patterns. For exam- ple, it is very clear from the supports of the concepts at the first level that we have managed to extract much more fre-  quent sequences in the areas of Psychology, Science and Social Sciencesthan in other areas. The analyst here could therefore consider the hypothesis either that these particu- lar topics might be transversal to many others and therefore included in a lot of frequent trajectories, or that we simply had (for unknown reasons) much more data regarding stu- dents enrolling into courses in these topics. Both hypothesis can be quickly verified by inspecting the corresponding pat- terns and the original data. Either ways, this represents an interesting finding, whether it is about the design of the stu- dent trajectories or about a previously unknown bias that should affect the analysis of these data.  Similarly, we can see by exploring further the hierarchy that frequent sequential patterns inScience tend to branch into sub-topics with clear boundaries (Physics, Biology, Chemistry, etc.) whileSocial Sciencesquickly introduces elements from other disciplines. In particular, it appears that the Social Sciences branch shares a lot of patterns with the Psychology branch, explaining the high number of patterns in both.  Finally, each concept in the hierarchy can be selected, to show the details that relate to it. In Figure 5, the concept Social Sciences has been selected, showing on the left part of the interface the items (courses) that are mentioned in the corresponding patterns, as well as the patterns themselves (with their support and number of steps). The pattern sp36 (with three steps and corresponding to 111 student trajec- tories) has been expanded here, showing that it covers the courses DSE212, SD226 and DDS307, as well as the corre- sponding topics. This can allow the analyst to drill down into the details of a sub-set of the patterns, and see how it relates to the original data. It is important to notice here that the mention of subjects is due to the selection of the la- bels of subjects of courses as the exploration dimension, and the hierarchy would show different elements of the items if another dimension was to be selected (as discussed in the next section).  4. ADVANTAGES OF THE APPROACH The method presented above is generic in the sense that  it only requires the results of the sequential pattern mining method to be represented in accordance with the Linked Data principles, and some relevant external Linked Data sources providing information about the considered items to function. We focus here however on our case study regarding course enrolment, as it clearly demonstrates the benefits of the approach within a Learning Analytics workflow, in terms of supporting result interpretation, as well as the reuse of existing Linked Data sources.  The core benefit of the approach in our view is that, by relying on Linked Data, we give the analyst access to a large number of customisable views over the results that have been produced. Indeed, the examples above mostly focus on the subjects of courses, as it is a rather natural one to be used as a navigation dimension. However, any other characteristic of the courses could be used in exactly the same way. For example, we can quickly build a lattice that organises the courses based the course level, possibly to verify that there is no common patterns representing an unexpected type of tra- jectories, such as doing courses in the wrong order or courses of different levels in the same year, to see if we can identify sequential patterns that span over the whole cycle or to focus on the patterns covering a certain level. It is also interesting  160    Figure 5: Screenshot of the interface with the lattice built with the dimension subjects labels.  to check how patterns would combine different numbers of credits or different modes of assessments. To illustrate this, the lattice built for the assessment methods is represented in Figure 6. Here we can see that a large majority of the tra- jectories would include TMAs (tutor marked assignments), which is indeed the most common form of assessment at the Open University. We can also see that combinations that would seem to the analyst to be probably rare (such as Exam and EoCA  end-of-course assessment) still lead to a number of frequent patterns. This specific combina- tion however, consistently with the intuition somebody with knowledge of Open University courses would have, does not appear in frequent patterns that do not also include TMAs.  Another important point here is that the approach makes it possible to identify relevant dimensions in the data at the time of interpretation. Indeed, it is possible to select any di- mension after having produced the sequential patterns, and to compare how different dimensions produce different nav- igational structures in the results. These dimensions could of course be included originally in the data being mined, but besides the added difficulty that this would generate in terms of computational complexity for the data mining tool, it is not always possible to identify in advance what dimensions are relevant (including everything from the Linked Data source would of course be unfeasible). To give a concrete ex- ample, looking at thesubjectdimension considered before,  an analyst might find it, after having ran the data mining process, not to be granular enough to allow for a meaningful analysis of frequent trajectories. Also, this dimension suf- fers from the issue that not all courses are associated with a subject (only the ones that are currently being taught). Having realised that, the analyst might cleverly change the dimension to use the subjects of the books that are associ- ated with each course, providing a much more granular set of subjects available for almost every course (currently avail- able or not). In this way for example, it would be possible to detect that several of the frequent trajectories do com- bine Computer Science with Social Aspects, while the original subject-based lattice (see Figure 4) did not show a connection between computing and the social sciences. This idea that we can try out different exploration dimensions at the time of interpretation is especially powerful consid- ering that, with Linked Data, we are not even limited to one particular source: one can easily switch from one data endpoint to the other to explore the different dimensions provided by different sources.  Finally, we argue that the approach presented here pro- vides a suitable way of supporting the work of the (learning) analyst in interpreting the results of data mining, by pro- viding him/her with a flexible tool to identify patterns or a sub-parts of the patterns that are especially relevant. This has been shown in some of the examples discussed above,  161    Figure 6: Lattice built on the dimension related to the mode of assessment associated with the courses. Exam means examination, TMA means tutor marked assignment, CMA mean computer marked assignment, ECMA means electronic com- puter marked assignment and EoCA means end-of- course assignment.  and understanding the concrete use of this tool in concrete analytics process would be outside the scope of this paper (although very interesting and one of the key points in our future work). However, we can already foresee three distinct ways in which this approach can help in the analysis:  1. Providing an overview of the mined patterns: by show- ing the relationships between the patterns, we can bet- ter understand how they distribute along a certain di- mension, and how they relate to each other. This helps in understanding generally the results of the data min- ing process, as well as to quickly navigate to specific patterns of interest.  2. Identify gaps and issues in the original data/process: As shown for instance with the example related to course subjects, the approach can help (better than listing the found patterns) to identify a strong bias in the data. It would be difficult, without inspecting this specific dimension, to get any indicator that an exaggeratedly large portion of the patterns are about psychology, which, as it turns out, is due to more use of the Facebook application from which the data was col- lected by psychology students (for reasons that remain to be explained).  3. Identify areas in need of further exploration: In rela- tion to the point above, it is also easy to see how the approach can help in identifying parts of the data that would require special attention. By part of the data, we mean either (or both) subsets of the original set of  sequences that can be viewed as representing a consis- tent cluster (e.g., the ones about science), or (and) a specific dimension that would require further analysis.  Furthermore, considering the status of the approach as a preliminary step towards a more complete analytics method- ology that would combine data mining and Linked Data, the last point above leads to the interesting idea that such a methodology can be seen as interactively iterating over the data mining interpretation processes. Indeed, starting with raw data, the analyst could obtain patterns through data mining and use the approach presented here to iden- tify parts of the data as well as linked data-based dimensions requiring further, refined analysis. By re-injecting the se- lected data together with the selected dimension as input of the data mining process, refined patterns would be obtained that would directly integrate the dimension of interest, mak- ing it possible to re-run the full cycle with more and more refined data, as well as more and more refined patterns, un- der the control of the analyst.7 Ultimately, combining in this way dimensions, sources of (linked data), as well as a larger variety of data mining techniques could lead to a power- ful analytics environment, truly taking benefit from Linked Data.  5. RELATED WORK The approach presented in this paper naturally relates to  to the general field of Educational Data Mining, i.e., the application of data mining to traditional educational sys- tems [13]. As discussed in [15], Learning Analytics is, in comparison, more concerned with the human-centric pro- cess of obtaining insight and input for decision making than with the algorithmic and technical aspects of the process- ing of data for extracting patterns or recommendations. In this sense, we see what we have proposed here as connecting to certain extent the two fields, being concerned with the use of data mining on educational data, while focusing on the interpretation of the results by a human analyst with a Learning Analytics purpose.  The other core aspect of the work presented in this paper relates to the use of Linked Data as part of a Learning An- alytics process. Linked Data technologies seems to be natu- rally relevant to the Learning Analytics area, as illustrated for example by the mention of Semantic Web and Linked Datain the topics of the Learning Analytics and Knowledge conferences call for paper, as well as dedicated events such as the Learning Analytics and Linked Data workshop8. In these initiatives however, Linked Data is often seen as the base technology for the integration of data at the input of the learning analytics process rather than, as here, a way to enrich the results for the sake of interpretation.  We also investigate here one form of integration between data mining and linked data. Data mining has long been recognised as a potentially useful technique for achieving linked data, both as a way to extract more information from the raw available data regarding connections between various objects (see e.g., link mining [10]), as well as to support the extraction of useful information from the data (for example to provide human-friendly interfaces to linked datasets [6]). Mining with linked data sources is also an  7A similar, general idea was described in [5] regarding the connection between data mining and ontologies. 8http://lald2012.wordpress.com/  162    interesting area of research (see e.g., [14]), which remains surprisingly under investigated. Closer to the approach pro- posed here, another aspect concerns the combination of data mining and linked data where they both contribute to a more general knowledge discovery process (see [5] for a discussion on such a knowledge discovery methodology combining data mining and ontologies). We see the approach presented in this paper as a step towards such more general knowledge discovery cycles, especially relevant to the Learning Analyt- ics process.  6. CONCLUSION In this paper, we present an initial approach for explor-  ing open Linked Data sources in interpreting the results of a data mining method, as part of a Learning Analytics pro- cess. We demonstrate on a use case relying on data about students enrolment in course modules how results from a se- quential pattern mining process can be automatically organ- ised in a variety of dimensions, obtained from a connected Linked Data source. We also discuss the advantages of this approach in a Learning Analytics process, and how it con- stitutes an first step towards a methodology combining data mining and Linked Data for Learning Analytics.  Since we see this work as a first step, future work nat- urally revolve around closing the loop between mining and interpretation, especially through the ability to re-inject di- mensions identified as relevant in Linked Data back into the data mining process, to refine this process and the obtained patterns. Also, while the case study presented in this paper provided us with valuable insight into the potential of this approach, and generally of Linked Data as a basis for the interpretation of analytics results, the concrete use of this approach in a variety of Learning Analytics scenarios is seen as a crucial next step of this work. Of course, while provid- ing a more complete and valuable evaluation of the approach than the first case study presented in this paper, it will also require to address essential usability aspects of the tools. An interesting aspect of data mining, and more generally of knowledge discovery processes, which has not been investi- gate in this paper is the one of interaction with the analyst, who might not be familiar with the underlying (linked data and data mining) technologies. One approach we intend to investigate (in line with our previous work described in [6]) is to transform navigational elements, such as concepts of the lattice, into queries in pseudo-natural language that can be used to interrogate the generated sequential patterns.  Another assumption on which the idea of using linked data to interpret results in learning analytics relies is that sufficient linked data are available about the entities and concepts relevant to such interpretation. In the presented case study, linked data from the Open University (see [4]) was used. However, we can imagine scenarios in which results obtained would require additional information that might come from external, and possibly multiple sources. The availability and exploitability of linked data relevant to education-related applications is the one of the main aims of the LinkedUp support action (see http://linkedup-project. eu), which is in particular creating a large and widely avail- able catalog of linked-datasets for education9. Through this  9see http://http://datahub.io/group/ linked-education  and other preliminary initiatives10, there is no doubt that large amounts of data will be available that could feed into our approach for data mining interpretation in Learning An- alytics.  Naturally, this raises the additional issue of finding and identifying semantic data of use for interpretation, at the time of analysis. Combining the method presented here with the use of semantic web search engines (see e.g. [7]) to find such data could lead to a promising toolkit for interpretation in Learning Analytics.  Acknowledgments This work is funded in part by the LinkedUp project (Grant Agreement: 317620) under the FP7 programme of the Eu- ropean Commission. See http://linkedup-project.eu for information.  We want thank Stuart Brown from the Open Universitys Communication Services for providing the students enrol- ment data, supporting the authors in the exploitation of Linked Data and providing useful feedback on the process, approach and tool presented in this paper.  7. REFERENCES [1] R. Agrawal and R. Srikant. Mining sequential  patterns. In P. S. Yu and A. S. P. Chen, editors, Eleventh International Conference on Data Engineering, pages 314, Taipei, Taiwan, 1995. IEEE Computer Society Press.  [2] C. Carpineto and G. Romano. Using concept lattices for text retrieval and mining. In B. Ganter, G. Stumme, and R. Wille, editors, Formal Concept Analysis, volume 3626 of Lecture Notes in Computer Science, pages 161179. Springer, 2005.  [3] M. dAquin. Linked Data for Open and Distance Learning. Commonwealth of Learning report  Available from http://www.col.org/resources/ publications/Pages/detail.aspxPID=420, 2012.  [4] M. dAquin. Putting linked data to use in a large higher-education organisation. In Proceedings of the Interacting with Linked Data (ILD) workshop at Extended Semantic Web Conference (ESWC), 2012.  [5] M. dAquin, G. Kronberger, and M. Suarez-Figueroa. Combining data mining and ontology engineering to enrich ontologies and linked data. In Proceedings of the Knowledge Discovery and Data Mining Meet Linked Open Data (Know@LOD) at the Extended Semantic Web Conference (ESWC), 2012.  [6] M. dAquin and E. Motta. Extracting relevant questions to an rdf dataset using formal concept analysis. In Proceedings of the The Sixth International Conference on Knowledge Capture - K-CAP 2011, 2011.  [7] M. dAquin and E. Motta. Watson, more than a semantic web search engine. Semantic Web Journal, 2, 2011.  [8] E. Egho, N. Jay, C. Rassi, and A. Napoli. A FCA-based analysis of sequential care trajectories. In A. Napoli and V. Vychodil, editors, Proceedings of The Eighth International Conference on Concept  10see http://linkeduniversities.org and http: //linkededucation.org/  163    Lattices and their Applications - CLA 2011, Nancy, France, Oct. 2011. INRIA Nancy Grand Est - LORIA.  [9] R. Ferguson. The state of learning analytics in 2012: A review and future challenges. Technical Report KMI-12-01, Knowledge Media Institute, The Open University, UK, 2012.  [10] L. Getoor. Link mining: A new data mining challenge. ACM SIGKDD Explorations newsletter, 2005.  [11] T. Heath and C. Bizer. Linked Data: Evolving the Web into a Global Data Space. Synthesis Lectures on the Semantic Web: Theory and Technology. Morgan & Claypool, 2011.  [12] J. Pei, J. Han, B. Mortazavi-Asl, H. Pinto, Q. Chen, U. Dayal, and M. Hsu. Prefixspan: Mining sequential patterns by prefix-projected growth. In D. Georgakopoulos and A. Buchmann, editors, ICDE, pages 215224. IEEE Computer Society, 2001.  [13] C. Romero and S. Ventura. Educationaldatamining: A survey from 1995 to 2005. Expert Systems with Applications, 33, 2007.  [14] H. Sheng, H. Chen, T. Yu, and Y. Feng. Linked data based semantic similarity and clustering. In Proceedings of the IEEE international conference on Information Reuse and Integration (IRI), 2010.  [15] G. Siemens and R. S. J. Baker. Learning analytics and educational data mining: Towards communication and collaboration. In Proceedings of the Learning Analytics and Knowledge Conference, 2012.  [16] R. Wille. Restructuring lattice theory: an approach based on hierarchies of concepts. In I. Rival, editor, Ordered Sets. Reidel, 1982.  [17] R. Wille. Why can concept lattices support knowledge discovery in databases J. Exp. Theor. Artif. Intell., 14(2-3):8192, 2002.  [18] World Wide Web Consortium. SPARQL Query Language for RDF. Avaliable online from http://www.w3.org/TR/rdf-sparql-query/; last accessed 2012-08-02, 2008.  [19] F. Zablith, M. dAquin, S. Brown, and L. Green-Hughes. Consuming linked data within a large educational organization. In Proceedings of the second International Workshop on Consuming Linked Data (COLD) at International Semantic Web Conference (ISWC), 2011.  164      "}
{"index":{"_id":"23"}}
{"datatype":"inproceedings","key":"Martin:2013:NLA:2460296.2460328","author":"Martin, Taylor and Aghababyan, Ani and Pfaffman, Jay and Olsen, Jenna and Baker, Stephanie and Janisiewicz, Philip and Phillips, Rachel and Smith, Carmen Petrick","title":"Nanogenetic Learning Analytics: Illuminating Student Learning Pathways in an Online Fraction Game","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"165--169","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460328","doi":"10.1145/2460296.2460328","acmid":"2460328","publisher":"ACM","address":"New York, NY, USA","keywords":"documentation, experimentation, fractions, games, keywords microgenetic research, mathematics education, measurement, performance, process analysis, rational numbers","abstract":"A working understanding of fractions is critical to student success in high school and college math. Therefore, an understanding of the learning pathways that lead students to this working understanding is important for educators to provide optimal learning environments for their students. We propose the use of microgenetic analysis techniques including data mining and visualizations to inform our understanding of the process by which students learn fractions in an online game environment. These techniques help identify important variables and classification algorithms to group students by their learning trajectories.","pdf":"Nanogenetic Learning Analytics: Illuminating Student Learning Pathways in an Online Fraction Game  Taylor Martin Ani Aghababyan  Jay Pfaffman Jenna Olsen  Utah State University Department of ITLS 2830 Old Main Hill  Logan, UT 84322-2830 +1-435-797-0814  taylormartin@usu.edu  Stephanie Baker Philip Janisiewicz  University of Texas at Austin Department of Curriculum and  Instruction 1 University Station D570  Austin, TX 78712 +1-956-536-4847  speacock@math.utexas.edu pjanisiewicz@gmail.com  Carmen Petrick Smith The University of Vermont  College of Education 411 Waterman  Burlington, VT 05405 +1-802-656-1411  carmen.smith@uvm.edu  Rachel Phillips University of Washington  Department of Educational Psychology and Learning  Sciences Seattle, WA 98105 +1-206-616-4480  rachelsp@uw.edu  ABSTRACT A working understanding of fractions is critical to student success in high school and college math. Therefore, an un- derstanding of the learning pathways that lead students to this working understanding is important for educators to provide optimal learning environments for their students. We propose the use of microgenetic analysis techniques in- cluding data mining and visualizations to inform our under- standing of the process by which students learn fractions in an online game environment. These techniques help identify important variables and classification algorithms to group students by their learning trajectories.  Categories and Subject Descriptors K.3 [Computers and Education]: General  General Terms Measurement, Documentation, Performance, Experimenta- tion.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK13 April 08 - 12 2013, Leuven, Belgium Copyright 2013 ACM 978-1-4503-1785-6/13/04 ...$15.00.  Keywords Measurement, Documentation, Performance, Experimenta- tion. Keywords Microgenetic research, process analysis, math- ematics education; rational numbers; fractions; games  1. INTRODUCTION Fractions have been described as an extremely challeng-  ing part of mathematics curriculum [5, 2, 10, 15, 12]. An understanding of fractions is critical to success in Algebra [15], a gatekeeper course to higher education [4, 16, 19].  A variety of models have been suggested to explain how students learn fractions. In our analysis, we draw on the splitting learning model [12, 3, 13, 16]. In this model, stu- dents learn fractions by first using their judgments of rela- tive magnitude and their ability to split a whole into equal parts. In this way, students develop an understanding of ra- tional numbers through creating equal divisions of a whole part and considering the numerical values for these dividing splits (see figure 1). Splitting is generally considered to be the most promising concept for teaching rational numbers to students because it draws on childrens intuitive under- standing of halving and because it is consistent with more advanced rational number concepts [3, 9, 14, 16, 17].  Microgenetic methods allow the discovery of learning pro- cesses and examination of how those processes relate to learning outcomes (e.g., [8, 18]). In classic microgenetic studies, researchers examined how participants strategies and understanding of tasks changed at the problem level over the course of several interviews during critical peri- ods of change for learning the specific content under study [8, 18]. Rather than relying on pre- and post-test perfor- mance to determine student learning, microgenetic analy-  165    Figure 1: A student splits a rectangle in half, then splits each half in half again, shades one of the re- sulting pieces, and concludes that it is one fourth of the rectangle.  sis has traditionally documented learning processes through video recordings and human codinga time consuming pro- cess.  We use learning analytics methods to examine how el- ementary students learn in Refraction [1]1, an online game designed to teach fractions using the splitting or equal parti- tioning model. The use of online game environments like Re- fraction simplifies and expands the data collection and anal- ysis process. We were able to collect data at the keystroke level on students play activity. As we analyze this data, we can expose patterns in students learning processes at a much finer grain size that was possible before, giving rise to the term nanogenetic analysis [8, 20].  2. METHODS In this study, ten and eleven year olds (N = 24; 13 boys,  11 girls) played Refraction over a period of approximately seven weeks in their classrooms. Refraction levels require users to solve fraction problems using 1/3 and 1/2splitters to divide a laser beam into the required pieces. Figure 2 shows an example of a level solution.  3. VISUALIZING STUDENTS LEARNING PROCESS  As a first step in examining our log data to understand learning process, we created visualizations of the pathway each student took through each level. Because we wanted to understand students learning process in regard to frac- tions, we focused only on the moves that had a mathemati- cal impact on the level (ignoring those moves which merely changed the direction of the laser). To illustrate how we vi- sualized students mathematical progressions, we will use a level that asked players to use 1/3 and 1/2 splitters to power a 1/6 and a 1/9 ship.  We started by creating representations of the state of the game space after each mathematical move. Figure 3 shows the initial state for each student; the whole laser beam aimed off the screen into space. The yellow circle represents the laser source, and the black circle indicates that the laser goes out in space without striking either target ship.  Figure 3: The initial state in an individual visual- ization.  1http://games.cs.washington.edu/refraction/  Figure 2: Steps for solving a level in Refraction. As students moved through the levels, the fractions became more difficult to solve.  As the student adds mathematical moves, the visualiza- tion became more complicated. In Figure 4, the student has now placed a 1/3 splitter on the laser beam.  Figure 4: A player places a 1/3 splitter on the beam.  Figure 5 shows a students progress through an entire level. The colors of the arrows shade from blue to red to indicate the order of the transitions.  166    Figure 5: A complete visualization for an individual student on a single level.  This student started the level as described above. Then, she placed a 1/2 splitter on one of her 1/3 laser beams. None of the resulting 1/3 or 1/6 beams hit a ship. Then she removed and replaced splitters to end up with the same resulting state. Next, she used benders to position one of her 1/6 beams so that it hit the 1/6 ship with the correct amount (the green diamond at the end of the beam shows that she hit the 1/6 ship with the correct amount). Next, she placed a 1/3 splitter on the other 1/6 beam and used bender to direct one of her resulting 1/18 laser beams into the 1/9 ship (the red square at the end of the 1/18 beam shows that she hit the 1/9 ship with an incorrect amount). Next, she removed the 1/3 splitter from the 1/6 beam, placed it on one of the 1/3 beams and used benders to direct one of the resulting 1/9 beams into the 1/9 ship (the green square shows this was accurate). The final state of the level has a red border. The color of the border only indicates that  this was the final state and not whether the final state was a successful solution for the level.  Comparing visualizations demonstrates differences in how students progressed through the same level. For example, Figures 5 and 6 both show students working on the 1/6, 1/9 level.  Figure 6: Visualization of a students in-game pre- level.  First, we can see that Figure 5 shows a successful attempt, while Figure 6 does not. Second, it is noticeable that Fig- ure 6 has many more repeated states (and therefore less states) than Figure 5.  4. CLASSIFYING STUDENTS LEARNING PROCESSES  Exploratory data analysis comparing and contrasting vi- sualizations of students pathways through individual levels and across levels helps us see aspects that differ across time and across students. The next step in our process has been to create variables that capture these aspects in ways that allow grouping of students in order to generate categories of learning pathways. For the dataset reported on here, students individual process visualizations showed a variety of patterns. However, many of these patterns were either (1) not repeated frequently enough in our small sample to provide a good basis for grouping, or (2) not meaningful. However, one pattern we noticed was that students tended to explore the space of the game in different ways. Some students seemed to avoid exploration and move quickly to solutions, while others seemed to experiment more with dif- ferent types of moves. This phenomenon captured our in- terest because there is significant debate about the role of exploration in learning (e.g., [6, 7, 11]). To begin to un- derstand these types of exploration better, we developed a coding scheme for transitions and a classification scheme for patterns of transitions over time.  4.1 Coding Transitions First, we identified each unique transition between states  in the students individual process visualization data de- scribed above. Then we coded each transition based on whether the move added or removed a splitter in a way that led the resulting game state to be potentially closer to a successful solution or not.  The primary coder created the coding scheme and coded all of the transitions. A secondary coder independently  167    coded one-third of the transitions. The two coders reached 90% agreement on their transition coding.  4.2 Categorizing Pathways Next, we developed a scheme to classify students trajec-  tories of transitions over the course of their pre- and post- levels. We categorized a student as experimenting with fail- ure if he made at least three transitions in a row that added potential failure. Similarly, we characterized a student as experimenting with success if she made at least three transi- tions in a row that added potential success. We also noticed that some students did not experiment much, but simply solved the problem.  Applying these simple criteria resulted in 3 trajectories:   Rapid success;   Experimenting primarily with success;   Experimenting primarily with failure;  Figures 7-9 show examples of these patterns.  Figure 7: Rapid success.  These patterns are intriguing in that they represent dif- ferent ways of exploring the space of the game. However, in this small dataset, it is difficult to draw conclusions relating type of pattern to outcome measures such as success on a level or time to complete a level.  5. CONCLUSIONS AND NEXT STEPS In this iteration of creating visualizations and categoriz-  ing pathways of learning, we developed our process of work and 1) illustrated that students can develop better frac- tion understanding using the splitting construct and 2) that nanogenetic approaches to data analysis do illuminate fine grained patterns showing that students explored the space of the game in a variety of ways.  Our small sample size and choice of using human coding for this iteration of our work were clear limitations. We are currently taking several steps to mitigate these limitations.  Figure 8: Experimenting with success.  Figure 9: Experimenting with failure.  In our exploratory analyses, we are developing visualiza- tions that make discovery of important variables easier. For example, our new visualization scheme shows the entire state space of the level as well as the states in that space that an individual student progressed through while solving that level. This allows us to compare areas of the state space across students and for the same student over time more easily.  In our work to develop better classification schemes, as we see patterns in learning pathways, we are analyzing these patterns to generate potential variables of interest such as time in state, number of unique states in a level, number of repeat moves, and distance between each state and the goal state. Using these variables, we are creating classification algorithms to group students based on patterns of behavior with a new larger dataset (N > 3000). To better under- stand splitting, we plan to characterize the development of fraction concepts using the individual trajectories through the levels, classify students based on how they do splitting,  168    and investigate whether different patterns are more or less associated with success.  6. ACKNOWLEDGMENTS This material is based on work supported by the Bill &  Melinda Gates Foundation, the Defense Advanced Research Projects Administration, and the National Science Founda- tion (Grant No. EEC 0748186). Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect posi- tions or policies of these agencies.  7. REFERENCES [1] E. Andersen, E. ORourke, Y. E. Liu, R. Snider,  J. Lowdermilk, D. Truong, S. Cooper, and Z. Popovic. The impact of tutorials on games of varying complexity. In Proceedings of the 2012 ACM annual conference on Human Factors in Computing Systems, pages 5968. ACM, 2012.  [2] M. Behr, G. Harel, T. Post, and R. Lesh. Rational number, ratio, and proportion. In D. Grouws, editor, Handbook of research on mathematics teaching and learning, page 296aAS333. National Council of Teachers of Mathematics, New York, NY, 1992.  [3] J. Confrey. Splitting, similarity, and rate of change: A new approach to multiplication and exponential functions. The development of multiplicative reasoning in the learning of mathematics, pages 293330, 1994.  [4] J. Confrey and E. Smith. Splitting, covariation, and their role in the development of exponential functions. Journal for Research in Mathematics Education, pages 6686, 1995.  [5] P. Gould, L. Outhred, and M. Mitchelmore. One-third is three-quarters of one-half. In Annual meeting of the Mathematics Education Research Group of Australia, 2006.  [6] M. Kapur. Productive failure. Cognition and Instruction, 26(3):379424, 2008.  [7] P. A. Kirschner, J. Sweller, and R. E. Clark. Why minimal guidance during instruction does not work: An analysis of the failure of constructivist, discovery, problem-based, experiential, and inquiry-based teaching. Educational psychologist, 41(2):7586, 2006.  [8] D. Kuhn. Microgenetic study of change: What has it told us Psychological Science, pages 133139, 1995.  [9] S. J. Lamon. The development of unitizing: Its role in childrens partitioning strategies. Journal for Research in Mathematics Education, pages 170193, 1996.  [10] S. J. Lamon. Rational numbers and proportional reasoning: Towards a theoretical framework for research. In F. Lester, editor, Second Handbook of Research on Mathematics Teaching and Learning, pages 629667. Information Age Publishing, Charlotte, NC, 2007.  [11] T. Martin and D. L. Schwartz. Physically distributed learning: Adapting and reinterpreting physical environments in the development of fraction concepts. Cognitive Science, 29(4):587625, 2005.  [12] J. Moss. Pipes, tubes, and beakers: New approaches to teaching the rational-number system. In S. Donovan and J. Bransford, editors, How students  learn: History, mathematics, and science in the classroom, page 309aAS350. National Academies Press, Washington, D.C., 2005.  [13] J. Moss and R. Case. Developing childrens understanding of the rational numbers: A new model and an experimental curriculum. Journal for Research in Mathematics Education, pages 122147, 1999.  [14] M. Myers, J. Confrey, K. Nguyen, and G. Mojica. Equipartitioning a continuous whole among three people: Students attempts to create fair shares. In S. L. Swars, D. W. Stinson, and S. Lemons-Smith, editors, Proceedings of the 31st annual meeting of the North American Chapter of the International Group for the Psychology of Mathematics Education, Atlanta, GA, 2009. Georgia State University.  [15] National Mathematics Advisory Panel. Foundations for success: The final report of the National Mathematics Advisory Panel. US Department of Education, 2008.  [16] A. Norton and A. J. Hackenberg. Continuing research on students fraction schemes. LP Steffe & J. Olive, Childrens fractional knowledge, pages 341352, 2010.  [17] J. Olive and L. P. Steffe. The construction of an iterative fractional scheme: The case of joe. The Journal of Mathematical Behavior, 20(4):413437, 2001.  [18] R. Siegler, T. Carpenter, F. Fennell, D. Geary, J. Lewis, Y. Okamoto, L. Thompson, and J. Wray. Developing effective fractions instruction for kindergarten through 8th grade. ies practice guide. ncee 2010-4039. What Works Clearinghouse, 2010.  [19] J. W. Stigler, K. B. Givvin, and B. J. Thompson. What community college developmental mathematics students understand about mathematics. MathAmatyc Educator, 1(3):416, 2010.  [20] A. Venezia, K. R. Bracco, and T. Nodine. One-shot  deal StudentsaAZ perceptions of assessment and course placement in CaliforniaaAZs community colleges. WestEd, San Francisco, 2010.  169      "}
{"index":{"_id":"24"}}
{"datatype":"inproceedings","key":"Kizilcec:2013:DDA:2460296.2460330","author":"Kizilcec, Ren'e F. and Piech, Chris and Schneider, Emily","title":"Deconstructing Disengagement: Analyzing Learner Subpopulations in Massive Open Online Courses","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"170--179","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460330","doi":"10.1145/2460296.2460330","acmid":"2460330","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, learner engagement patterns, learning analytics","abstract":"As MOOCs grow in popularity, the relatively low completion rates of learners has been a central criticism. This focus on completion rates, however, reflects a monolithic view of disengagement that does not allow MOOC designers to target interventions or develop adaptive course features for particular subpopulations of learners. To address this, we present a simple, scalable, and informative classification method that identifies a small number of longitudinal engagement trajectories in MOOCs. Learners are classified based on their patterns of interaction with video lectures and assessments, the primary features of most MOOCs to date. In an analysis of three computer science MOOCs, the classifier consistently identifies four prototypical trajectories of engagement. The most notable of these is the learners who stay engaged through the course without taking assessments. These trajectories are also a useful framework for the comparison of learner engagement between different course structures or instructional approaches. We compare learners in each trajectory and course across demographics, forum participation, video access, and reports of overall experience. These results inform a discussion of future interventions, research, and design directions for MOOCs. Potential improvements to the classification mechanism are also discussed, including the introduction of more fine-grained analytics. ","pdf":"Deconstructing Disengagement: Analyzing Learner Subpopulations in  Massive Open Online Courses  Ren F. Kizilcec Dept. of Communication  Stanford University Stanford, CA-94305  kizilcec@stanford.edu  Chris Piech Dept. of Computer Science  Stanford University Stanford, CA-94305  piech@cs.stanford.edu  Emily Schneider School of Education Stanford University Stanford, CA-94305  elfs@cs.stanford.edu  ABSTRACT As MOOCs grow in popularity, the relatively low completion rates of learners has been a central criticism. This focus on completion rates, however, reflects a monolithic view of dis- engagement that does not allow MOOC designers to target interventions or develop adaptive course features for particu- lar subpopulations of learners. To address this, we present a simple, scalable, and informative classification method that identifies a small number of longitudinal engagement tra- jectories in MOOCs. Learners are classified based on their patterns of interaction with video lectures and assessments, the primary features of most MOOCs to date.  In an analysis of three computer science MOOCs, the classifier consistently identifies four prototypical trajectories of engagement. The most notable of these is the learners who stay engaged through the course without taking assess- ments. These trajectories are also a useful framework for the comparison of learner engagement between different course structures or instructional approaches. We compare learners in each trajectory and course across demographics, forum participation, video access, and reports of overall experi- ence. These results inform a discussion of future interven- tions, research, and design directions for MOOCs. Poten- tial improvements to the classification mechanism are also discussed, including the introduction of more fine-grained analytics.  Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Education; K.3.1 [Computers and Education]: Distance learningMassive Open Online Course, Learner Engage- ment Pattern  equal contribution  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 13 Leuven, Belgium Copyright 2013 ACM 978-1-4503-1785-6/13/04 ...$15.00.  General Terms Algorithms, Measurement  Keywords Learning Analytics, Learner Engagement Patterns, MOOCs  1. INTRODUCTION Massive open online courses (MOOCs) are the most re-  cent and highly publicized entrant to a rapidly expanding universe of open educational resources. As of late 2012, the majority of MOOCs are virtual, distributed classrooms that exist for six to ten weeks at a time. These MOOCs are structured learning environments that emphasize instruc- tional videos and regular assessments, centralizing activities on a single platform. This is a distinct model from the set of learner-directed, open-ended courses that are now known as cMOOCs because of their grounding in connectivist theo- ries of learning [25, 20, 8].  The relatively low completion rates of MOOC participants has been a central criticism in the popular discourse. This narrative implies a binary categorization of learners: those who pass the class by adhering to the instructors expecta- tions throughout the courseand everyone else. This mono- lithic view of so-called noncompleters obscures the many reasons that a learner might disengage from a MOOC. It also makes no allowances for learners who choose to partic- ipate in some aspects of the MOOC but not others, staying engaged with the course but not earning a statement of ac- complishment. In contrast, one could emphasize the impor- tance of individual differences and consider all learners to be unique in their interactions with the platform. But whereas the monolithic view overgeneralizes, this individualist per- spective overcomplicates. In this paper, we seek to strike a balance by identifying a small yet meaningful set of pat- terns of engagement and disengagement. MOOC designers can apply this simple and scalable categorization to target interventions and develop adaptive course features [5].  With no cost to entry or exit, MOOCs attract learners with a wide range of backgrounds and intentions, as well as personal or technical constraints to participation. Given the heterogeneity of the population, we would be remiss to make a priori assumptions about the appropriate characteristics or behaviors around which to categorize learners, or which pathways and outcomes are more or less valuable for their learning. Analogous challenges can be found in research on  170    Figure 1: Labels for learners (GS-level)arcs show movement of students from one assignment period to next.  community collegesthe closest brick-and-mortar analogue to MOOCs in terms of the diversity of educational objec- tives among their students [14]and on unstructured virtual inquiry environments, where there is not a clear notion of correct pathways through the available resources. Using unsupervised clustering techniques, community college re- searchers have developed meaningful typologies of students based on longitudinal enrollment patterns [2] and survey measures of engagement [23]. Likewise, cluster-based analy- ses for inquiry environments have distinguished meaningful patterns in learner engagement with content [1].  In this paper we employ a methodology for characterizing learner engagement with MOOCs that builds on methods used in this previous literature. We define learner trajecto- ries as longitudinal patterns of engagement with the two pri- mary features of the coursevideo lectures and assessments. We uncover four prototypical categories of engagement con- sistently across three MOOCs by clustering on engagement patterns. We focus on interactions with course content, be- cause learning is a process of individual knowledge construc- tion that emerges in a dynamic process of interactions among learners, resources, and instructors [4, 25]. In MOOCs, these interactions are shaped by the design of instruction, content, assessment, and platform features. To inform effective de- sign changes and interventions along these dimensions that would target the needs of learners on a particular trajec- tory, we compare clusters based on learner characteristics and behaviors.  2. COURSE DEMOGRAPHICS Our analysis of learner trajectories is based on three com-  puter science courses that vary in their level of sophistica- tion: Computer Science 101 covers high school level con- tent (HS-level), Algorithms: Design and Analysis covers undergraduate level content (UG-level), and Probabilistic Graphical Models is a graduate level course (GS-level). Ta- ble 1 provides basic demographic information and summa- rizes how many learners were active on the course website at any point in time (as opposed to simply enrolling and never participating). In all three courses, the vast majority of active learners are employed full-time, followed by grad- uate and undergraduate students. Moreover, most learners in the UG-level and GS-level courses come from technology- related industries. The majority of learners in the UG-level course report to hold a Masters or a Bachelors degree. Ge-  ographically, most learners are located in the United States, followed by India and Russia.  Table 1 also reports the distribution of active learners over the quantiles of the 2011 Human Development Index (HDI) a composite measure of life expectancy, education, and in- come indices [29]. The distribution in the GS- and HS-level courses is very similar, with over two-thirds of active learn- ers from very high-HDI countries. The distribution in the UG-level course is less skewed between very high-, high-, and medium-HDI countries, though low-HDI countries account for a similarly low 3% of learners.  Table 1: Course Demographics  HS UG GS  Active Learners 46096 26887 21108 Gender (M/F) 64%/36% 88%/12% 88%/12% Age 33 (14) 31 (11) 36 (12)  HDI Very High 69% 54% 70% High 13% 17% 14% Medium 15% 26% 15% Low 3% 3% 1%   Mean (Std. Dev.)  3. CLUSTERING Our learning analytics methodology is designed to identify  a small number of canonical ways in which students interact with MOOCs. In our analysis we first compute a descrip- tion for each student of the way in which the student was engaged throughout the duration of a course and then ap- ply clustering techniques to find subpopulations in these en- gagement descriptions. Running this methodology over the courses in our study uncovers four prototypical engagement patterns for learners interactions with the contemporary in- stantiation of MOOCs.  The first step in our methodology is to generate a rough description of each students individual engagement in a course. For each assessment period, all participants are la- beled either on track (did the assessment on time), be- hind (turned in the assessment late), auditing (didnt do the assessment but engaged by watching a video or doing a quiz), or out (didnt participate in the course at all). These labels were chosen because they could be easily col-  171    lected, and would make sense in any MOOC that is based on videos and assessments, regardless of content area or the pedagogical strategies of the course. Figure 1 visualizes the longitudinal distribution of learners assigned to each label for the GS-level course. For each assessment period, nodes represent the number of learners in each category; between assessment periods, an arc represents the number of learn- ers who retain the same label or move between labels. Due to space constraints the Out nodes are not to scale. The complete list of labels that a participant is assigned for each assessment periods is called her engagement description As a concrete example of an engagement description: imag- ine a learner in the GS-level course had completed the first five assignments on time, finished the sixth assignment late and then continued to watch videos without bothering with the last three assignments. Using the notation in Figure 1, that particular students engagement description would have been, [T, T, T, T, T, B, A, A, A].  Once we had engagement descriptions for each learner in a course, we applied the k-means clustering algorithm the standard centroid-based clustering algorithmto identify prototypical engagement patterns. To calculate the simi- larity between engagement descriptions for two students, a computation which is needed for clustering, we assigned a numerical value to each label (on track = 3, behind = 2, auditing = 1, out = 0) and computed the L1 norm of the list of numbers. Since we wanted to account for the ran- dom properties of k-means we repeated our clustering one hundred times and selected the solution with the highest likelihood. Though clustering was performed separately on all three courses, the process extracted the same four high- level, prototypical engagement trajectories (Table 2 shows their distribution in the three classes):  1. Completing: learners who completed the majority of the assessments offered in the class. Though these par- ticipants varied in how well they performed on the as- sessment, they all at least attempted the assignments. This engagement pattern is most similar to a student in a traditional class.  2. Auditing: learners who did assessments infrequently if at all and engaged instead by watching video lectures. Students in this cluster followed the course for the major- ity of its duration. No students in this cluster obtained course credit.  3. Disengaging: learners who did assessments at the be- ginning of the course but then have a marked decrease in engagement (their engagement patterns look like Com- pleting at the beginning of the course but then the stu- dent either disappears from the course entirely or sparsely watches video lectures). The moments at which the learners disengage differ, but it is generally in the first third of the class.  4. Sampling: learners who watched video lectures for only one or two assessment periods (generally learners in this category watch just a single video). Though many learn- ers sample at the beginning of the course, there are many others that briefly explore the material when the class is already fully under way.  To evaluate the clusters produced by this methodology we tested that (1) the trends derived were robust to perturba- tions in the methodology, (2) the clusters that we arrived at had a healthy goodness of fit for the data, and (3) that the trends made sense from an educational perspective. The  Table 2: Cluster Breakdown  Course Auditing Completing Disengaging Sampling  HS 6% 27% 28% 39% UG 6% 8% 12% 74% GS 9% 5% 6% 80%  results below lend support that the clusters extracted are meaningful and useful.  (1) Though we had extracted trends, it was necessary to test whether they reflected meaningful patterns in learning, or if they were a manifestation of the parameters that we used to explore engagement. We hoped to show that the pat- terns we identified were so strong that even if we had made a few minor changes in our methodology, the same trends of engagement would hold. First we tested whether the pat- terns in the class were robust enough that the clusters did not change substantially when we experimented with differ- ent feature sets. Including assignment pass and removing behind from the set of labels we assigned to learners in the Algorithms course produced highly analogous centroids and similar labeling, 95% overlap in cluster labels and 94% over- lap respectively. In addition, we tried running our clustering with a different choice for k (number of clusters) and found that increasing k divided the four high level patterns into sub-clusters. For example using k = 5 and clustering on the UG level course split the Sampling cluster into learners who sampled a video at the beginning of the course and learners who sampled a video in one of the later assessment periods.  (2) It was also necessary to show that the four high-level clusters of students provided an accurate generalization of the data. To verify the goodness of fit of our clustering we ran the Silhouette cluster validation test [22]. A positive sil- houette score reflects that, on average, a given engagement description is more similar to other descriptions in its cluster than to descriptions in the other clusters (which in turn sug- gests that the clusters reflect true subgroups of the original population). The maximum silhouette score of 1.0 means that all learners in a cluster are exactly the same. Though our clustering classified some students that were halfway be- tween two of the categories, the overwhelming majority of learners fit cleanly into one of the trajectories (98% positive silhouette, average silhouette score = 0.8).  (3) The final evaluation of our clustering methodology was that the algorithm returned trends that make sense from an educational point of view. The trends of engagement pass a common sense test: it is plausible to imagine a posteri- ori that students would interact in an educational platform in these high level ways. This is important because it pro- vides a framework which enables research that can hypoth- esize other properties of students in these clusters. Since our labels were drawn from a small discrete set of engage- ment labels, we extracted meaningful patterns of engage- ment (Completing, Auditing, etc). In contrast, using assign- ment grades or lecture counts as features produced clusters that were mostly defined by student scores in the first week (e.g. learners who got a high grade in assignment one and then dropped out, learners who received a medium grade in assignment one and then dropped out, etc.). These clus- ters are less informative of learning processes and potential pedagogical improvements.  172    4. CLUSTER ANALYSIS The plurality of engagement trajectories calls for an equally  diverse set of tools and interventions to support these sub- populations of learners. We compare the clusters along be- havioral features routinely recorded in the MOOC database, as well as self-report features collected through optional sur- veys. The goal is to provide educators, instructional de- signers, and platform developers with insights for designing effective, and potentially adaptive, learning environments that best meet the needs of MOOC participants. In this section we first describe and motivate the set of features to compare trajectories on, and then present the results of our cross-cluster analyses. In the following section we offer in- terpretations of these findings, suggest design changes for future MOOCs, and highlight research opportunities.  4.1 Features Understanding who learners are, why they enroll in the  course, and other activities in the course is a first step to- wards illuminating potential influences on the self-selection of learners into these engagement patterns. Differences in the distribution of particular features across clusters may indicate that these demographic variables or learning pro- cesses affect learners engagement decisions. In all courses, learners received a survey at the end of the course. In the UG-level course, an additional pre-course survey was admin- istered. Table 3 contains survey response rates by engage- ment group for each course. Note the high response rates in the UG-level course.  Table 3: Survey Response Rates  HS UG (pre) UG (post) GS  Auditing 13% 23% 14% 23% Completing 43% 31% 45% 65% Disengaging 4% 25% 3% 29% Sampling 3% 20% 1% 5%  Survey Demographics: The demographic section of the optional surveys included age, gender, employment status, highest level of education achieved, and years of work expe- rience.  Geographical Location: Learners IP addresses were recorded and looked up on a country level using MaxMinds GeoLite database. The country labels were then merged with the 2011 Human Development Index data [29]. Are MOOCs meeting their promise of global access How do learners in different parts of the world interact with these courses  Intentions: At the start of the course, learners reported their reasons for enrolling by choosing applicable options from a set of predefined reasons. (E.g. Enhance my re- sume for career or college advancement or Its free) We computed the probability of indicating each reason given the learners engagement trajectory. MOOCs attract a variety of learners with particular sets of objectives and motiva- tions. Understanding learners goals is a precondition to ef- fective designs that provide affordances for the varied needs of learners.  Overall Experience: In post-course surveys, learners rated their overall experience with the course on a 7-point Lik- ert scale from Poor to Excellent. This measure provides insight into learners satisfaction with the course experience.  Forum Activity: A rich history of research in computer- supported collaborative learning, as well as classroom and informal settings, shows that learning is enhanced through collaboration and discourse with a community [27]. The discussion forum provides the opportunity for this type of social learning in MOOCs. We measure learners active par- ticipation on the forum by counting the number of posts and comments each learner created during the course.  Streaming Index (SI): This measure serves as a proxy for learners access to in-video assessments, which are only avail- able when streaming videos off the course website. Access to in-video assessments is pedagogically important because formative assessment that gives leaners instant feedback has been associated with positive learning outcomes: Opportu- nities for frequent, formative testing enable learners to re- flect on their knowledge state [3] and actively retrieve infor- mation in a way that facilitates learning [21]. Although the clustering of engagement patterns is partly based on video consumption, video access (streaming versus downloading) is independent of clustering. SI is defined as the propor- tion of overall lecture consumption that occurs online on the platform, as opposed to offline (downloaded).  Streaming Index (SI) = online lecture consumption  total lecture consumption  4.2 Results Learner clusters are compared along the feature dimen-  sions introduced above using formal statistical tests. A one- way analysis of variance (ANOVA) is performed on each di- mension (Table 4) and Tukey Honest Significant Differences (HSD) adjustments (pHSD) are used for post hoc pair-wise cluster comparisons (Table 6) [11]. The tables report the sta- tistical and practical significances of the comparisons. The latter is reported in terms of effect size: partial eta-squared (partial 2) for multiple clusters and Cohens d for two clus- ters [6]. By convention, partial 2>.14 is considered a large effect, and partial 2>.06 medium; d>.8 is considered a large effect, and d>.5 medium. Absolute effect sizes can be ex- tracted from group averages in Table 4. In the case of con- trasting intentions to enroll, the statistical tests are based on 10,000 bootstrapped permutations of engagement group labels. To test for significance we evaluated the likelihood of observing the reasons that learners reported given their actual engagement group.  4.2.1 Survey Demographics Note that the following demographic comparisons between  engagement groups are only valid under the assumption that responding to the survey is independent of the demographic indicators (e.g. males and females are equally likely to re- spond to the survey).  Gender: All three courses enrolled more male than fe- male learners, though this trend was much more prominent for courses with more sophisticated content. There were around seven times more men than women in the UG- and GS-level courses (odds ratio of 7.4 and 7.5, respectively). The gender gap was much less prominent in the HS-level course, with only about twice as many men than women (odds ratio of 1.8). A log linear model of gender on cluster membership yields log odds for each engagement trajectory with confidence intervals for each course (Figure 2). Within each course, the gender ratios across the four engagement trajectories are not significantly different from each other  173    Table 4: Comparisons between Engagement Trajectories (One-Way ANOVAs)  Average  Indicator Auditing Completing Disengaging Sampling F p Partial 2  HS Overall Experience .894 .912 .830 .796 109 <.001 .047 Streaming Index .869 .880 .900 .855 61.8 <.001 .004 Forum Activity .242 .788 .189 .017 1536 <.001 .091  UG Overall Experience .731 .874 .716 n.a. 84.1 <.001 .153  Streaming Index .643 .664 .723 .743 48.0 <.001 .006 Forum Activity .251 1.71 .238 .024 1315 <.001 .128  GS Overall Experience .771 .794 .657 .687 44.9 <.001 .056 Streaming Index .519 .667 .655 .661 64.8 <.001 .009 Forum Activity .536 7.18 1.98 .090 2692 <.001 .277  n.a. = not available due to low survey response rate  Significant at p < .05 or d > .8  Self-report measure (scaled to unit interval)  Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  H S  U G  G S  2 4 6 8 10 12 14 16  Odds Ratio (Male/Female)  Figure 2: Odds ratio between number of males and females with 95% C.I. and overall gender odds ratio in course (dotted line)  (except for Sampling learners in the HS-level course), sug- gesting that gender is not associated with engagement tra- jectories. However, the ratio for Completing learners lies significantly below the the course-wide average (dotted lines in Figure 2) in the HS-level course (p=.05), but just sig- nificantly above in the GS-level course (p=.06). This may indicate a trend where females are relatively less frequently Completing learners in higher-level courses.  Age: Learner engagement groups are approximately equally distributed within age brackets, except in the GS-level course, where there were fewer elderly (65+) Completing and Au- diting learners, and none under the age of 18.  Employment status: In all courses, learners on different engagement trajectories are approximately equally distributed within the three most represented employment statuses: work- ing full-time, graduate and undergraduate student.  4.2.2 Geographical Location To extend the analysis of how active learners are dis-  tributed over countries with different HDI levels, Table 5 shows the distribution over engagement trajectories within  each HDI tier. As HDI increases, the proportion of Com- pleting and Disengaging learners increases, while the pro- portion of Sampling learners decreases. However, the distri- bution for low-HDI countries might not be representative, given that learners from low-HDI countries account for only 1% of all active learners. To circumvent this issue, we an- alyzed the distribution of engagement patterns for the four most represented countries (US, India, Russia, and the UK) which happen to span over three HDI levels: the US and UK rank very high, Russia ranks high, and India ranks medium. The analysis confirms the pattern observed for medium-HDI countries: in all three courses, learners from India participate considerably more as Sampling (ca. 14% points above other three countries), than as Completing and Disengaging learners (ca. 9% and 7% points below).  Table 5: HDI Level Breakdown (GS-level)  Very High High Medium Low  Auditing 13% 8% 11% 14% Completing 8% 6% 4% 2% Disengaging 10% 9% 5% 4% Sampling 69% 77% 80% 80%  All Learners 70% 14% 15% 1%  4.2.3 Intentions For all three courses the two most frequently chosen rea-  sons for enrolling are, because they find it fun and chal- lenging, and they are interested in the topic. Moreover, the probability of enrolling to enhance their resume is partic- ularly high for Completing learners (15% in the HS-, 33% in the UG-, and 20% in GS-level course). In ALGO, Com- pleting learners were the most likely to say they were in the class because they thought it was fun and challenging (61%, p<.001), followed by Auditing (58%, p<.05), Disengaging (55%) and Sampling learners (52%, p<.001).  4.2.4 Overall Experience Ratings of overall experience (Figure 3) are highly sig-  nificantly different between engagement groups in all three courses (p<.001 in all courses; partial 2=.153 in the UG- level, and partial 2=.056 in the GS-level course). In the HS-  174    Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  H S  U G  G S  3.0 3.5 4.0 4.5 5.0  Overall Experience  Figure 3: Overall experience with 1 standard error bars  level and GS-level courses, the overall experience of Com- pleting and Auditing learners is not significantly different from each other, but significantly above Disengaging (d=5.66 in the HS-level and d=.648 in the GS-level course) and Sam- pling learners (d=.785 in the HS-level and d=.465 in the GS-level course). The UG-level course exhibits a different pattern, with Completing learners having a significantly bet- ter overall experience than the other engagement groups.  4.2.5 Forum Activity Forum activity (Figure 4) varies significantly between en-  gagement trajectories with medium to large effect sizes, with Completing learners participating at significantly higher rates than learners in other engagement trajectories (p<.001). For example, in the GS-level course, Completing learners exhibit significantly higher levels of activity on the discussion board compared to Auditing (d=.721, mean=.536), Disengaging (d=.480, mean=1.98), and Sampling learners (d=1.97, mean =.09). The significance of these differences is preserved when controlling for the different durations of these learners participation in the course. On average, Completing learn- ers write 1.71 posts and comments in the UG-level, .788 in the HS-level, and 7.18 in GS-level course.  4.2.6 Streaming Index A consistent pattern in all three courses is an average  Streaming Index (SI) above 0.5 for each engagement trajec- tory, which indicates that streaming is the dominant form of access to video lectures (Figure 5). In the HS-level course, the SI is consistently higher than the other courses across all engagement patterns: streaming accounts for around 88% of video consumption, compared 70% in the UG-level and 63% in GS-level courses. Surprisingly, within each course, there are significant differences in SI between most engagement trajectories, though effect sizes are only marginal. The most notable difference is that of Auditing learners in the GS-level course, who watch about half (SI=.519) of the lectures of- fline, compared to Completing, Disengaging, and Sampling learners (with SIs between .655 and .667). In the UG-level course, the SI of Completing and Auditing, and Disengaging and Sampling learners are not significantly different (p=.405 and p=.068, respectively), while all other pair-wise compar-  Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  H S  U G  G S  0.1 0.5 1.0 2.0 4.0 7.0 10.0  Average Activity  Figure 4: Forum activity with 1 standard error bars (square-root scale)  Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  Sampling  Disengaging  Completing  Auditing  H S  U G  G S  0.5 0.6 0.7 0.8 0.9 1.0  Streaming Index  Figure 5: Streaming Index with 1 standard error bars  isons are statistically significant (p<.001). This indicates that Completing learners in the UG-level course tend to watch lectures offline less than Disengaging learners (10% points difference in SI).  5. DISCUSSION First, we reflect on our classification methodology and  propose ways to extend and adapt it for future MOOC re- search. Then, we discuss the results of the cluster analysis in terms of future research questions and design directions for MOOCs. We conclude with cross-course comparisons and a broad set of remarks on the scope of MOOC design, research, and global accessibility.  5.1 Extension of Analytics Methodology The process of extracting patterns of engagement has given  us new insights into potential next steps for analytics re- search applied to the MOOC context. Given the unprece- dented volume of data collected through massive open access courses, this environment provides an exciting opportunity for the Learning Analytics community. In this paper we  175    Table 6: Post Hoc Pair-Wise Comparison between Engagement Trajectories  Comp.-Audi. Comp.-Dise. Comp.-Samp. Audi.-Dise. Audi.-Samp. Dise.-Samp.  Indicator pHSD d pHSD d pHSD d pHSD d pHSD d pHSD d  HS Overall Experience .132 .133 <.001 .566 <.001 .785 <.001 .336 <.001 .461 .005 .149 Streaming Index .225 .043 <.001 .078 <.001 .082 <.001 .123 .121 .041 <.001 .152 Forum Activity <.001 .205 <.001 .282 <.001 .413 .023 .056 <.001 .546 <.001 .278  UG Overall Experience <.001 .846 <.001 1.03 n.a. n.a. .804 .059 n.a. n.a. n.a. n.a. Streaming Index .405 .053 <.001 .150 <.001 .195 <.001 .200 <.001 .246 .068 .049 Forum Activity <.001 .318 <.001 .367 <.001 .917 .963 .007 <.001 .407 <.001 .259  GS Overall Experience .354 .114 <.001 .648 <.001 .465 <.001 .500 <.001 .342 .145 .121 Streaming Index <.001 .390 .832 .041 .932 .019 <.001 .352 <.001 .336 .945 .016 Forum Activity <.001 .721 <.001 .480 <.001 1.97 <.001 .284 <.001 .356 <.001 .982  n.a. = not available due to low survey response rate  Significant at p < .05 or d > .8  Self-report measure (scaled to unit interval)  outlined our first foray into the new dataset. While our algorithm was useful for identifying high level patterns in how students are approaching the contemporary instances of MOOCs, there are several improvements that we would recommend for future analytics research.  The strategy behind our clustering technique was to cre- ate a single variable for engagement, and to look for trends in how that variable changed over time. Our coarse fea- ture set was useful for consistently identifying very high level patterns of engagement across different courses. We are interested to see what details in learning patterns can be expressed through a more nuanced measure of engage- ment, particularly one that is built from finer-grained time slices and the incorporation of more user information. In our study we used an assessment period (approximately one week) as the smallest granule of time for assigning labels of engagements to learners, a modelling simplification which was the result of the data which was immediately available on the first MOOC classes. Since all user interactions with the learning system are time stamped, we could construct a model of engagement with a granularity on the order of hours (if not smaller). A finer view of time could allow our understanding of students to delve into the details of user work sessions. Moreover, in conjunction with a more precise time frame we could also incorporate more types of learner data in our clusteringfor example, the timing of learners participation in the forum or the resources they turn to while in the process of completing assessments. Scores received on quizzes and assignments would add the dimension of achieve- ment levels to the engagement trajectory model. A richer set of features, one that included a smaller time granular- ity and more user data, would allow a clustering algorithm to uncover more subtle patterns. However, the cost of us- ing complex feature sets is that the patterns extracted may miss the big picture, which we have sought to provide in this paper.  Another adjacent space in this line of analytics research is exploration into the different applications of learner engage- ment classification. The clustering detailed in this paper provides a quick and easy way to compare different course instances in the MOOC context. Being able to contrast stu- dent engagement patterns could be used to explore both the impacts of different pedagogies and how students themselves change over time. Since MOOCs are relatively new to most  learners, it is reasonable to hypothesize that users are going to adapt over time to better take advantage of free material. As a result we predict that learner patterns of engagement will also changea trend which could be explored through clustering engagement over present and future offerings of the same course.  In general, for those studying MOOCs in the future, we recommend that they incorporate an understanding of the high level ways in which students engage. This lens, we believe, is much more insightful than a raw report of the number of students who enrolled or the number of students who obtained a certificate.  5.2 Interpretation of Results The clusters reveal a plurality of trajectories through a  course that are not currently acknowledged in the design and discourse around MOOCs. Auditing appears to be an alter- native engagement pathway for meeting learner needs, with Auditing learners reporting similarly high levels of overall experience to Completing learners in two of three courses. This implies different underlying preferences or constraints for Auditing and Completing learners, and points to an op- portunity to design features to actively support these en- gagement trajectories. Auditors could be identified via self- report or based on a predictive model that should be devel- oped for the early detection of engagement patterns. For example, Auditing learners could be encouraged to focus on video-watching and not be shown potentially frustrating reminders about assessment completion. Moreover, instruc- tors could downplay the importance of assessments when outlining expectations for the course, in order to avoid dis- couraging learners from following this alternative engage- ment path. Another design strategy could be removing as- sessments altogether for Auditing learners. However, ev- idence from cognitive psychology suggests that testing not only assess learning but facilitates it [18], which implies that even though assessments do not fit with the engagement pro- file of Auditing learners, MOOC designers should not de- prive them of the option to engage in assessment activities that could serve to enhance their learning.  Compared to Auditing and Completing learners, Disen- gaging and Sampling learners almost universally report lower levels of overall experience. In the context of our conception of learning as a process of interactions with the learning  176    environment, we can think of these prototypical patterns of engagement as reflecting a trajectory of interactions that led, at some point, to the learner disengaging from the course. From the surveys, the most prominent reasons that learners across the three courses selected on a list of reasons for dis- engaging were: personal commitment(s), work conflict, and course workload. While the personal constraints reflected in the first two reasons may be unassailable, the three to- gether can be interpreted to mean that some of these learners may have been better served by a course that was offered at a slower pace or even entirely self-paced. Investigating points of disengagement is a ripe area for future work. More qualitative or survey-based data should be gathered on why learners choose to leave these courses. This data should be combined with more fine-grained analytics to develop a pre- dictive model for when learners are likely to disengage in the future and what category of disengagement their choice falls into.  Cross-cluster comparisons in survey responses and learn- ing processes allow us to develop hypotheses about the mech- anisms of how or why a learner may have stayed on a partic- ular trajectory. These hypotheses can be designed around and tested in future work. For example, forum activity is a behavioral measure excluded from the clustering algorithm that differentiates the trajectories and deepens our under- standing of the activities of highly engaged learners. Com- pleting learners exhibit the highest level of activity on the forum; notably, this rate is much higher than that of Dis- engaging learners, who are initially assessment-oriented and then disengage from the course. While to some extent this is a reflection of Completing learners high level of engagement with the course overall, we may hypothesize that participa- tion on the forum creates a positive feedback loop for some learners, as they are provided with social and informational inputs that help them stay on their trajectory towards com- pletion. This hypothesis can be tested using encouragement designs, such as reputation systems, or by leveraging social influence by displaying participation levels or contributions of other learners [17]. Platform designers should also con- sider building other community-oriented features to promote pro-social behavior, such as text or video chat, small-group projects, or facilitated discussions. Linking these commu- nity features more strongly to the content in the coursefor example, situated discussions linked to a point in a video or other resourcemay further promote learning. In addi- tion to being theory-driven, these designs and interventions should be based on future research that delves more deeply into the mechanisms of MOOC learners engagement on the forumincluding those learners who read the forum but do not contribute to itand how these interactions relate to their decisions in the course overall. Future research should examine the structure of the community in terms of the so- cial networks that develop, as well as the incentives to con- tribute and build trust among members. Another strand of research could explore how discourse on MOOC discussion boards facilitates the construction of knowledge [7].  5.3 Cross-Course Comparisons The clusters also act as a standard set of outcomes for  comparing the three courses. While each course adheres to a standard MOOC format, differences across courses in the distribution of learners in the clusters can bring into relief the content and instructional strategies of each course. For  example, the HS-level course stands out from the UG- and GS-level course with over half of the participants being Com- pleting learners or disengaging after being on that trajectory initially. This speaks to the wider accessibility of the entry- level course content, especially considering that the HS-level course has a far higher proportion of women enrolling, as well as double the number of active learners as the other two courses. Notably, the HS-level course also has a 26% higher average Streaming Index (SI) than the UG- and 40% higher than GS-level courses. This variation in SI may be partially due to the relative levels of difficulty of the courses. But an- other likely influence is that in-video exercises are only avail- able to those who stream the videos, and whereas the videos in the UG- and GS-level courses primarily feature multi- ple choice questions, the in-video exercises in the HS-level course tend to be short programming challenges. These pro- gramming challenges are likely to be fun and rewarding to participants, and additionally enhance learning by requiring learners to actively demonstrate their knowledge [4]. MOOC designers and instructors should be prompted by this obser- vation to continue to develop performance-based approaches to assessment. A future experiment could test the relative importance of these types of assessments for learning.  Another trend illuminated by comparing the HS-level course to the other courses is the result that females are relatively less frequently Completing learners in the GS-level course. This finding is consistent with research on stereotype threat, which shows that women tend to perform worse than equally skilled men on more challenging or frustrating quantitative task [28]. Among other explanations, it is theorized that this is because the feeling of challenge is likely to evoke anxiety that failing will confirm negative stereotypes about their group (that women are not good at quantitative tasks). Moreover, this effect is more likely to occur for individu- als who care highly about the domainas is the case with women who are enrolled in the GS-level course. Interven- tions demonstrated to counteract stereotype threat among women taking math tests include presenting the test as one where there are no gender differences associated with results one where everyone can achieveor a test that is described as designed to help you learn, not one that is diagnostic of your current skills [26]. Both of these devices could be used to frame assessments to counteract instances of stereotype threat in MOOCs.  Two trends in the characteristics of participants in the three MOOCs are particularly salient given the dominant themes in popular discourse about MOOCs. The first is why people choose to participate in MOOCs. Much commentary has focused on the role that MOOCs can play in credential- ing and opportunities for job (re)training. While acquiring new skills, along with the certification of those skills, is cer- tainly important to many participants, there are far more who are driven by the intellectual stimulation offered by the courses. MOOCs are evidently playing an important role in providing opportunities for engaging in lifelong learning outside of the confines of an institution, and can potentially serve as a powerful means of harnessing the cognitive sur- plus [24] that has emerged in a post-industrial age. Analo- gous to the case of learners who audit, designers and instruc- tors should be aware of the needs and goals of learners who are enrolling for personal enrichment, and consider how con- tent or instruction could be adapted to better satisfy them. Future research should explore these populations more thor-  177    oughly, turning to surveys, interviews or case studies as a source of contextually rich data about their needs and ex- periences.  The second trend concerns the promise that MOOCs hold for global access to education. Though there are many ex- ceptions, it is notable that the learners in all three courses tend to be well-educated professionals from high-HDI coun- tries. Moreover, the majority are male. These facts are partially an artefact of the technical nature of these courses. The awareness of MOOCs is also likely much higher among learners from the US, which dominates the enrollment of the three courses under analysis. But broadband access is likely to be a factor as well, as many learners in low- and medium-HDI countries are faced by intermittent, slow, or metered bandwidth that would make it a challenge to fully engage with video-heavy courses. MOOC designers should consider decreasing videos or offering only the audio version of the lecture, two strategies that would also have implica- tions for pedagogy and learning. The skew in geographical distribution is a clear call to action for those in the MOOC space who are focused on issues of access and equity, and explanations for this phenomenon should be pursued in or- der to develop more culturally sensitive and accommodating MOOCs.  6. CONCLUSION Learners in MOOCs who do not adhere to traditional  expectations, centered around regular assessment and cul- minating in a certificate of completion, count towards the high attrition rates that receive outsized media attention. Through our analysis we present a different framework for the conversation about MOOC engagement, which accounts for multiple types of student engagement and disengage- ment. We started out with the assumption that there are a small number of alternative patterns of interactions with MOOC content. Through our research we were able to extract, across all three classes studied, four prototypical learner trajectories; three of which would have been con- sidered noncompleting under a monolithic view of course completion. Using these patterns as a lens to more closely analyze learner behavior and backgrounds across the differ- ent trajectories, we were able to suggest research and design directions for future courses.  This work is one of the first applications of analytics tech- niques into the new wealth of learner data that is generated by MOOCsdatasets that we believe present exciting oppor- tunities for the learning analytics community. Though we were able to find high-level patterns, the vast amounts of information available should allow for the discovery of more subtle and deeper trends. A particularly rich area for fu- ture research is combining more fine-grained analytics with data on the noncognitive factors that inevitably influence the choices they make when moving through a MOOC. Mo- tivation, self-regulation, tenacity, attitudes towards the pro- cesses of learning, and feelings of confidence and acceptance are but some of many psychological factors that affect aca- demic performance [12, 10]. Along with other unobserved latent variables, these internal states are likely associated with choices that learners make about particular activities as well as with overall patterns of engagement with the course. Those factors that are found to be influential could inspire the design of tools, features, or interventions that are ei- ther broadly applicable or adapted to the needs of particu-  lar types of learners. Interventions can also be developed to directly target these factors, such as the promotion of micro- steps to simplify the learning process and increase learners ability to succeed [13], or interventions designed to promote a growth mindset among learners [9].  The large scale and virtual nature of MOOCs creates a fertile ground for experiments based on the hypotheses and design directions suggested by this paper. Modifications to subsequent instances of the same course would yield interest- ing insights, as would the continued comparison of multiple courses with different structures or approaches to instruc- tion. Other innovative designs of MOOC instruction, con- tent, or platform featuresbased on principles of the learning sciences or human-computer interactionshould likewise be subject to experimentation and evaluation. One potential design area is the development of simple cognitive tools [15], such as an integrated note-taking or concept-mapping system that would allow learners to actively interpret the course content, or task lists and calendar features for staying organized. Another is addressing learners prior knowledge in the field, which is widely understood to mediate learners encounters with new information and subsequent academic performance. Calibrating prior knowledge could aid in pro- viding adaptive content to learners, such as a finely-tuned hinting structure as part of assessment procedures [16], or a set of open educational resources linked to from within instruction on a particular topic. A third challenging design problem opens up in light of the increasing ubiquity of media multitasking [19], especially in an environment where learn- ers attention can be quickly compromised by attending to their social networking needs in the next browser tab.  A powerful promise of MOOCs is the unprecedented level of global access to a vast set of educational opportunities. We have the chance to design these new learning environ- ments both for learners who want a standard assessment- centric course and learners who have less structured moti- vations. Using a standard set of outcomes will allow re- searchers and designers across the MOOC space to develop a collective awareness of optimal approaches for meeting the needs of MOOC learners. The engagement trajectory model is one viable option for a high-level characterization of the effect of refinements and interventions in MOOCs.  7. ACKNOWLEDGMENTS We would like to thank Stanfords Office of the Vice Provost  for Online Learning, Daphne Koller, Clifford Nass, and Roy Pea for their support and guidance in the preparation of this paper. We are also indebted to our anonymous reviewers, the LAK program chairs, and the members of the Stanford Lytics Lab for their invaluable feedback.  8. REFERENCES [1] S. Amershi and C. Conati. Automatic recognition of  learner types in exploratory learning environments. Handbook of Educational Data Mining, page 213, 2010.  [2] P. Bahr. The birds eye view of community colleges: A behavioral typology of first-time students based on cluster analytic classification. Research in Higher Education, 51(8):724749, 2010.  [3] P. Black and D. Wiliam. Assessment and classroom learning. Assessment in education, 5(1):774, 1998.  178    [4] J. Bransford, A. Brown, and R. Cocking. How people learn: Brain, mind, experience, and school. National Academies Press, 2000.  [5] P. Brusilovsky and E. Millan. User models for adaptive hypermedia and adaptive educational systems. The adaptive web, pages 353, 2007.  [6] J. Cohen. Statistical power analysis for the behavioral sciences. Lawrence Erlbaum, 1988.  [7] B. De Wever, T. Schellens, M. Valcke, and H. Van Keer. Content analysis schemes to analyze transcripts of online asynchronous discussion groups: A review. Computers & Education, 46(1):628, 2006.  [8] S. Downes. New technology supporting informal learning. Journal of Emerging Technologies in Web Intelligence, 2(1):2733, 2010.  [9] C. Dweck. Mindset: The new psychology of success. Ballantine Books, 2007.  [10] C. Dweck, G. Walton, and G. Cohen. Academic tenacity: Mindset and skills that promote long-term learning. Gates Foundation. Seattle, WA: Bill & Melinda Gates Foundation, 2011.  [11] B. Everitt and T. Hothorn. A handbook of statistical analyses using R. Chapman & Hall/CRC, 2009.  [12] C. Farrington, M. Roderick, E. Allensworth, J. Nagaoka, T. Keyes, D. Johnson, and N. Beechum. Teaching adolescents to become learners: The role of noncognitive factors in shaping school performance: A critical literature review. Chicago: University of Chicago Consortium on Chicago School Research, 2012.  [13] B. Fogg. A behavior model for persuasive design. In Proceedings of the 4th international Conference on Persuasive Technology, page 40. ACM, 2009.  [14] S. Goldrick-Rab. Challenges and opportunities for improving community college student success. Review of Educational Research, 80(3):437469, 2010.  [15] B. Kim and T. Reeves. Reframing research on learning with technology: in search of the meaning of cognitive tools. Instructional Science, 35(3):207256, 2007.  [16] K. Koedinger, A. Corbett, et al. Cognitive tutors: Technology bringing learning science to the classroom. The Cambridge handbook of the learning sciences, pages 6178, 2006.  [17] R. Kraut, P. Resnick, S. Kiesler, Y. Ren, Y. Chen, M. Burke, N. Kittur, J. Riedl, and J. Konstan.  Building successful online communities: Evidence-based social design. The MIT Press, 2012.  [18] M. McDaniel, H. Roediger, and K. McDermott. Generalizing test-enhanced learning from the laboratory to the classroom. Psychonomic Bulletin & Review, 14(2):200206, 2007.  [19] E. Ophir, C. Nass, and A. Wagner. Cognitive control in media multitaskers. Proceedings of the National Academy of Sciences, 106(37):1558315587, 2009.  [20] C. Rodriguez. Moocs and the ai-stanford like courses: Two successful and distinct course formats for massive open online courses. Learning, 2012.  [21] H. Roediger III and J. Karpicke. Test-enhanced learning taking memory tests improves long-term retention. Psychological Science, 17(3):249255, 2006.  [22] P. J. Rousseeuw. Silhouettes: A graphical aid to the  interpretation and validation of cluster analysis. Journal of Computational and Applied Mathematics, 20(0):53  65, 1987.  [23] V. Saenz, D. Hatch, B. Bukoski, S. Kim, K. Lee, and P. Valdez. Community college student engagement patterns a typology revealed through exploratory cluster analysis. Community College Review, 39(3):235267, 2011.  [24] C. Shirky. Cognitive surplus: Creativity and generosity in a connected age. ePenguin, 2010.  [25] G. Siemens. Connectivism: A learning theory for the digital age. 2004.  [26] S. Spencer, C. Steele, and D. Quinn. Stereotype threat and womens math performance. Journal of Experimental Social Psychology, 35(1):428, 1999.  [27] G. Stahl, T. Koschmann, and D. Suthers. Computer-supported collaborative learning: An historical perspective. In R. K. Sawyer, editor, Cambridge handbook of the learning sciences, pages 409426. Cambridge, UK: Cambridge University Press, 2006.  [28] C. Steele, S. Spencer, and J. Aronson. Contending with group image: The psychology of stereotype and social identity threat. Advances in experimental social psychology, 34:379440, 2002.  [29] United Nations Development Programme. 2011 human development report. Retrieved from http://hdr.undp.org/en/media/HDR_2011_  Statistical_Tables.xls. (Accessed 18-Jan-2013).  179      "}
{"index":{"_id":"25"}}
{"datatype":"inproceedings","key":"Mirriahi:2013:PLR:2460296.2460331","author":"Mirriahi, Negin and Dawson, Shane","title":"The Pairing of Lecture Recording Data with Assessment Scores: A Method of Discovering Pedagogical Impact","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"180--184","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460331","doi":"10.1145/2460296.2460331","acmid":"2460331","publisher":"ACM","address":"New York, NY, USA","keywords":"data sharing, pedagogical adjustment/intervention","abstract":"Web technologies, such as lecture recordings, have the capacity to capture and store massive amounts of data from individuals' online behavior. Such data can provide insight into student learning processes and the relationship between online trace data and academic performance alerting educators to when intervention may be required or if their learning activities may need to be adjusted. This paper discusses how data captured from students' use of lecture recordings accessed through a Collaborative Lecture Annotation System (CLAS) when aggregated and correlated with assessment data can help educators evaluate the impact of the recordings on their students' learning. Such information can help inform and alert educators to when adjustments may be required to their pedagogical approach.","pdf":"The Pairing of Lecture Recording Data with Assessment  Scores: A Method of Discovering Pedagogical Impact    Negin Mirriahi   University of New South Wales  Sydney, NSW, Australia   negin.mirriahi@unsw.edu.au            Shane Dawson   University of South Australia  Adelaide, SA, Australia   shane.dawson@unisa.edu.au     ABSTRACT  Web technologies, such as lecture recordings, have the capacity to  capture and store massive amounts of data from individuals  online behavior. Such data can provide insight into student  learning processes and the relationship between online trace data  and academic performance alerting educators to when  intervention may be required or if their learning activities may  need to be adjusted. This paper discusses how data captured from  students use of lecture recordings accessed through a  Collaborative Lecture Annotation System (CLAS) when  aggregated and correlated with assessment data can help educators  evaluate the impact of the recordings on their students learning.  Such information can help inform and alert educators to when  adjustments may be required to their pedagogical approach.   Categories and Subject Descriptors  J.1 [Administrative Data Processing]: Education; K.3.1  [Computer Uses in Education]: Computer-assisted instruction  (CAI), Computer-managed instruction (CMI).   General Terms  Design, Human Factors   Keywords  Data Sharing, Pedagogical Adjustment/Intervention   1. INTRODUCTION  With the rise of attention in mainstream media to massive   online open courses (MOOCs), such as Coursera and MITx,  traditional higher education institutions are under increasing  pressure to consider alternative education delivery models that  provide flexible yet quality learning opportunities for their  students. Whether institutions implement distance education,  blended learning or flipped classroom, or a combination of  flexible delivery models, they require to varying degrees, the  adoption of web technologies. Such online technologies provide  opportunities for flexible learning and teaching not bounded by  time and place [3]. Learning management systems (LMS),  asynchronous discussion forums, blogs, wikis, and videos are just  a few of the web technologies available for facilitating learning  outside of the typical face-to-face classroom.    The adoption of educational technologies provides increased  flexible learning opportunities, alongside the capacity to capture  and store the interactions and behaviours students have with the  tools. This data exhaust can be further analysed and interpreted  for evaluating the impact of the course design. The methodology  and process of analyzing large sets of data available from existing  systems for the purpose of understanding and managing students  learning is part of the relatively new research area of learning  analytics [12]. Such analytics are necessary for evidence-based  decision-making and action to occur [8] since the data can inform  educators about their students learning and guide them in  modifying their pedagogical approach or intervening to help  students who may be struggling [4].    With the increased interest in providing flexible learning  opportunities, instructors have increasingly turned to developing  video or audio recordings of their lectures. These resources are  then made available online for students to review in their own  time and pace. While the lecture recordings provide review  opportunities for students, they could also reverse the classroom  experience by making them required out of class activities  allowing in-class time to be spent on more socio-constructivist  based activities. This has gained prominence in the term flipped  classroom. Often in large classes, there is minimal opportunity  for students to engage with one another or with the instructor and,  therefore, flipping lecture time with independent study activities  allows class time to be better spent on collaborative activities and  passive lectures to be delivered online to students as independent  study materials [5].    Similar to other web technologies, online lecture recordings can  provide tracking data that can help educators to understand how  students use and interact with the recordings for learning purposes  [1, 9] and subsequently share their experience with colleagues  who have not yet adopted this particular teaching strategy. As  innovators or relatively early adopters, these educators are often in  positions of influence in their academic departments having an  impact on their colleagues technology adoption decisions [6].  Data-informed pedagogy, therefore, can influence how academic  departments integrate recorded lectures with classroom  instruction. However to date, the investigation of how tracking  data can be used to help explain students learning processes and  inform educators of whether the lecture recordings contribute  towards students understanding of course content has been under  researched. This paper discusses the way that data captured from  students interactions with online lecture recordings can be  aggregated with students assessment scores to help educators  evaluate their pedagogical approach and adjust learning activities  or assessment strategies when required.   2. Literature on Lecture Recordings  Students are increasingly reliant on lecture recordings for   learning or reinforcing course content as part of their independent     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04...$15.00.     180    study as higher education institutions begin to offer flexible  learning opportunities and attempt to make use of face-to-face  class time for interactive activities. For example, The University  of Wisconsin-Madison redeveloped a large third year Computer  Science course by replacing two face-to-face class times per week  with video lecture recordings available for students to watch on  their own time and using class time for team problem solving  activities [2]. Survey results and the websites electronic records  showed that 75% of the students watched all or almost all of the  video lectures and 59% of the students felt the videos had a  positive effect on their learning experience. Similarly, an  Engineering course at the University of Wisconsin-Madison,  made lecture recordings as required homework activities allowing  class time for emphasizing difficult concepts, problems, and  quizzes [7]. Survey results showed that approximately 68% of the  students watched all the video lectures and 78% watched three  quarters of them with many of the students indicating that they  appreciated the ability to rewind to reinforce content or skip  forward when they were familiar with particular concepts.    While the evaluation of the use of video lectures primarily  focused on students self-reports on whether they viewed the  lecture recordings and how they felt it impacted their learning,  another study explored the relationship between Biology students  self reports of viewing the videos and their scores on assessments  [10]. Students self reports indicated that approximately 70-85%  of the students watched the video lectures prior to attending class  in preparation for their assessment questions using a personal  response system, namely clickers. Statistical results showed that  students who viewed the lecture videos prior to class performed  better on assessment questions that were designed to test lower- order cognitive skills. While this study evaluated how the lecture  videos impacted students mastery of content based on assessment  scores, the data regarding students actual use of the video highly  depended on their own self reports. However, with proper access  and tools, tracking data from online lecture recordings can be  captured and used to understand students interactions with the  recordings. While the studies at the University of Wisconsin- Madison focused mainly on students self reported data on their  use of the lecture recordings, other studies used tracking data to  investigate students behaviour patterns with lecture recording  technologies over an academic term [1, 9]. The study reported in  this paper further adds to previous literature by examining how  online tracking data aggregated with students assessment scores,  can help educators assess and adjust their pedagogical approach.   3. Study Purpose  This paper reports on a case study investigating how data   captured from students use of a lecture recordings can be  aggregated and correlated with students assessment scores to help  evaluate the educational impact of online lecture recordings and  inform instructors on how students are engaged with the learning  activity without having to rely on students self reports. Such  information can help instructors adjust their pedagogical approach  to align with their intended student learning outcomes. For  example, if aggregated data informs instructors that students were  performing well on their quizzes without having viewed the  lecture recordings, the instructors may wish to alter their quiz  questions to be more challenging. Alternatively, if the data shows  that students were generally performing poorly on the quizzes  despite viewing the recordings, instructors may wish to modify  their lecture recordings to address areas of misconception or  provide additional supplementary materials. Furthermore, if the  aggregated data does not show any significant statistical  correlation between students performance on quizzes and   accessing the lecture recordings, instructors may consider  reexamining the alignment between their course design, content,  and assessment.   4. Methods   The intent of this case study was to investigate how the data   captured from students use of lecture recordings could be  aggregated with their assessment scores to evaluate the impact of  the activity on students mastery of content and help instructors  assess their implemented pedagogical approach. At the  educational institution where this study took place, instructors had  access to a locally hosted video annotation tool called  Collaborative Lecture Annotation System (CLAS). A feature of  the in-house developed software is the easy sharing of videos or  audio recordings to the student cohort. As students review the  uploaded resources there is opportunity to comment or annotate  on the recordings for self-study or reflection purposes [11].  Students, in this study, were restricted from downloading the  recordings and had to use the CLAS interface to review the  streamed recorded lectures. Since CLAS was locally hosted,  tracking data captured on the database on students use of the  system was readily available. To protect students anonymity and  privacy, all student identifiers were obfuscated. Data such as  when students played or stopped a recording, the number of times  they accessed each recording, and the date and time of access are  captured in the CLAS database. In addition, data with respect to  students annotations and comments made on CLAS were also  captured.    This case study focused on the CLAS usage data pertaining to  the lecture recordings and relating to two quizzes designed to test  lower-order cognitive skills and understanding of the lecture  content. In consultation with instructors using CLAS, quiz scores  with obfuscated student identifiers were retrieved from the  learning management system (LMS) for the purpose of  investigating how the CLAS data coupled with students  performance on their assessments could be aggregated and  correlated in an informative way to help instructors examine and  reflect on their pedagogical strategies.     5. Findings  The dominant model for using CLAS in this study related to the   delivery of lecture content followed by a short answer assessment.  Based on prior literature [10], it was hypothesized that the greater  the portion of the related lecture recordings the students accessed,  the better they would perform on their assessment. The data most  pertinent for this study was anticipated to relate to the total  portion of each recording that every student accessed that directly  related to the quiz questions. For example, certain quizzes or  specific questions in a quiz were directly related to the content  presented in particular lecture recordings. Hence, knowing the  total portion of the related lecture recordings accessed by the  students correlated with how they performed on the quizzes can  help shed light on whether accessing the lecture recordings has  any effect on students mastery of content. In other words, it  would help instructors assess whether the recording content  appropriately matched the intended learning outcomes or if  modifications to either the assessment strategies or the way the  content is presented in the lecture recordings would be required  for future iterations of the course.    A script was developed to aggregate all the play and stop times  of each recording for every student in order to determine the total  portion of the lecture recording that each student accessed. While  students may have accessed certain portions of the recordings  more than once, the script removed this portion of the data since it   181    would only be useful if specific segments of the lecture recording  were relevant to quiz questions. For the quizzes used in this  particular study, the entire content of the lecture recording related  directly to the questions.    Once the script was developed and tested for accuracy, the total  portion of each lecture accessed by every student was correlated  with the students quiz score using statistical analysis software,  SPSS. However, prior to applying any statistical correlation tests,  the Kolmogorov-Smirnov test was conducted to determine  whether the data had non-normal or normal distribution based on  whether the test results were significant. The results showed that  the scores for quiz 1, D(133) = 0.20, p < .001, and for quiz 2,  D(131) = 0.25, p < .001, were both non-normal.    In addition, the CLAS data on the portion of the lecture  recordings accessed by the students were also tested to determine  their distribution using the Kolmogorov-Smirnov test. The results  showed that the data for the three lecture recordings relating to  quiz 1, D(133) = 0.18, p < .001, D(133) = 0.25, p < .001, and  D(133) = 0.28, p <.001, are all non-normal. Likewise, the results  showed that the data for the three lecture recordings relating to  quiz 2, D(131) = 0.26, p <.001, D(131) = 0.33, p < 0.001, and  D(131) = 0.26, p< 0.001, were also non-normal.    Since the Kolmogorov-Smirnov tests showed that the data had a  non-normal distribution, a non-parametric correlation, Spearmans  rho, was applied to determine any significant relationship between  students overall score on a quiz and the total portion of each  related lecture recording they accessed prior to the quiz.  Table 1  shows the results of a Spearmans rho correlation for Quiz 1  (N=133) in this study.      Table 1: Correlation between Portion of Lecture Recordings  Accessed and Quiz 1 Score           Recording 1 Recording 2 Recording 3   Spearmans rho .240* .312* .230*   *Indicates statistical significance (p < .01)     As shown in Table 1, there is a significant correlation between  how much of the three related lecture recordings students  accessed and how they performed on the quiz. Such information  would help the instructor assess whether students reviewing the  content in the lecture recordings has any impact on how they  answer the questions. In this case, it appears that since there is a  positive relationship between their quiz score and the total portion  of the related recordings they accessed, the activity seems to  benefit students understanding of lecture content and preparation  for the quiz.    However, in this particular case study, students were asked to  answer one randomly generated question based on one or more of  the three lecture recordings. Hence, students quiz scores were  also correlated with the total portion of the lecture recording they  accessed that specifically related to the question they answered on  their quiz as shown in Table 2.            Table 2: Correlation between Portion of Lecture Recordings  Accessed and Quiz 1 Score for Specific Questions          Recording 1 Recording 2 Recording 3   Spearmans rho   (Question 1)   .322  (N=31)       Spearmans rho   (Question 2)    .242  (N=33)   .298  (N=33)   Spearmans rho   (Question 3)   .488*  (N=37)       Spearmans rho   (Question 4)     .068  (N=32)   *Indicates statistical significance (p < .01)    As can be seen in Table 2, there is a significant positive  correlation between students who accessed Recording 1 and who  randomly received Question 3 on the quiz. In other words, the  greater the portion of Recording 1 the students accessed, the better  they performed on Question 3. This finding can be used to assist  the instructor in aligning their presented materials with student  performance and understanding of the discipline content as  assessed by the quiz question.    However, as the other correlations in Table 2 were not  significant, the instructor can infer that the recordings did not  directly impact students mastery of content. This information can  prompt the instructor to closely examine the other three quiz  questions and the way the content is presented in the related  lecture recordings to determine if changes are required for future  iterations of the course or if the lecture material is challenging and  requires supplemental activities to support students learning and  understanding.    Considering the results presented in Table 1 and 2 together, it  appears that although there was a significant positive relationship  between the total portion of recordings accessed by all students  and their quiz scores regardless of which question they answered,  when the Spearman rho correlation was applied to the specific  questions and related recordings, only the third question showed a  significant correlation. Hence, if specific questions in a quiz relate  to specific lecture recordings, both sets of correlations and data  would be useful for instructors to evaluate the learning activity  and assessment strategies and make pedagogical decisions as  required. Similar results were obtained when the Spearmans rho  correlation was applied to the overall score for quiz 2 (N=131) in  this case study and the portion of the related lecture recordings  students accessed as shown in Table 3.      Table 3: Correlation between Portion of Lecture Recordings  Accessed and Quiz 2 Score           Recording 1 Recording 2 Recording 3   Spearmans rho .391* .355* .439*   *Indicates statistical significance (p < .01)     182    Similar to the results shown in Table 1 for Quiz 1, there was a  significant positive correlation between the total portion of the  lecture recordings students accessed and their score on Quiz 2 as  shown in Table 3. In addition, for this second quiz, the correlation  coefficients are higher representing a greater relationship between  accessing lecture recordings and performing on the quiz. Hence,  these results can inform instructors that the lecture recordings  have some positive affect on students understanding or  reinforcement of content. Furthermore, since students received  one randomly generated question in this second quiz as they did in  the first quiz, Spearmans rho correlation was applied to their  second quiz score and the portion of the lecture recording they  accessed that specifically related to the question they answered as  presented in Table 4.     Table 4: Correlation between Portion of Recordings Accessed  and Quiz 2 Score for Specific Questions           Recording 1 Recording 2 Recording 3   Spearmans rho   (Question 1)     .457**  (N=30)   Spearmans rho   (Question 2)   .496*  (N=38)       Spearmans rho   (Question 3)   .410*  (N=40)   .381**  (N=40)      Spearmans rho   (Question 4)     .213  (N=23)   *Indicates statistical significance (p < .01)  **Indicates statistical significance (p < .05)     Unlike the results shown previously in Table 2, there were more  significant correlations observed in Quiz 2 as depicted in Table 4.  The Spearman rho measures applied to Quiz 2 show that for three  out of four randomly generated questions, there was a  considerable positive relationship between the total portions of the  related lecture recordings the students accessed and how they  performed on their specific quiz question. This helps the instructor  infer that the lecture recordings had a positive impact on students  understanding or reinforcement of material assessed by three of  the quiz questions. However, the non-significant correlation  between accessing the third lecture recording and students  performance on Question 4 alerts the instructor to reexamine this  specific question and the way the lecture content is presented in  the third recording and make changes as necessary the next time  the course is offered.      6.  Summary & Next Steps  While previous reports on lecture recordings have primarily   focused on showing how students use and perceive the value of  such learning activities from surveys and self-reports, this case  study further adds to the literature by demonstrating how tracking  data from students online interactions with lecture recordings  through CLAS can be statistically correlated with students quiz  scores to evaluate their pedagogical impact. Although, the data in  this case study was limited to students performance on two  quizzes with one randomly generated question each resulting in a   reduced sample size and thereby not generalizable to the broader  population, it suggests one way that analytics from online lecture  recordings can inform learning and teaching strategies. Future  studies can further explore the impact of lecture recordings on  students understanding of content by including an increased  sample size that would allow for better data reliability and  generalizability. In addition, since the tracking data used in this  study was focused on the total portion of a lecture recording  accessed, it does not consider students pause behaviours or  repetitive access. Such additional tracking data could offer more  insight on how students are using the lecture recordings in  preparation for assessment activities.    Furthermore, in this study, quiz scores were manually extracted  from the LMS and correlated with the relevant aggregated lecture  recording data identified by the database script. A logical next  step is to discover a way of directly connecting the tracking data  from students interactions with the lecture recordings stored on  the CLAS database with the quiz scores located in the LMS in  order to minimize manual intervention. Once the data aggregation  and correlation processes have been automated through  algorithms and associated database scripts, a dashboard or  intuitive user interface should be created for instructors to readily  access real-time aggregated data and results from statistical  correlations related to their particular lecture recordings and  assessments for their own reflective practice. In addition, such a  dashboard can be designed to alert instructors when a statistically  significant positive relationship between students assessment  scores and interactions with the lecture recordings is not  discovered so they can make informed decisions regarding their  pedagogical approach and make changes as necessary.    Overall, this case study begins to demonstrate one way that  online tracking data on students use of lecture recordings can be  paired with assessment scores to gain a better understanding of  how video or audio recordings can affect learning and assist  instructors in making data-informed pedagogical decisions.    7. REFERENCES  [1] Brooks, C., Epp, C. D., Logan, G., and Greer, J. 2011. The   who, what, when, and why of lecture capture. In Proceedings  of the 1st International Conference on Learning Analytics  and Knowledge (Banff, Canada, February 27 - March 1,  2011). LAK11. ACM, New York, NY, 9-17.    [2] Foertsch, J., Moses, G., Strikwerda, and Litzkow, M. 2002.  Reversing the lecture/homework paradigm using eTeach  web-based streaming video software. Journal of Engineering  Education, 91 (July 2002), 267-274.   [3] Garrison, D. R. and Kanuka, H. 2004. Blended learning:  Uncovering its transformative potential in higher education.  Internet High. Educ. 7, 2 (2nd Quarter 2004), 95-105.    [4] Greller, W. and Drachsler, H. 2012. Translating learning into  numbers: A generic framework for learning analytics. Educ.  Technol. Soc. 15, 3, 42-57.    [5] Houston, M. and Lin, L. 2012. Humanizing the classroom by  flipping the homework versus lecture equation. In  Proceedings of Society for Information Technology &  Teacher Education International Conference (Austin, Texas,  USA, March 5-10, 2012). AACE, Chesapeake, VA.    [6] Mirriahi, N., Dawson, S., and Hoven, D. 2012. Identifying  key actors for technology adoption in higher education: A  social network approach. In Proceedings of ascilite  Wellington, 2012 (Wellington, New Zealand, November 25- 28, 2012).    183    [7] Moses, G. and Litzkow, M. 2005. In-class active learning  and frequent assessment reform of nuclear reactor theory  course. In Frontiers in Education, 2005, Proceedings of the  35th Annual Conference (October 19-22, 2005). FIE05.    [8] Norris, D., Baer, L, Leonard, J., Pugliese, L., and Lefrere, P.  2008. Action analytics: Measuring and improving  performance that matters in higher education. Educause  Review. 43, 1 (Jan./Feb. 2008), 42-67.    [9] Phillips, R., Preston, G.,  Roberts, P., Cumming-Potvin, W.,  Herrington, J., Maor, D., and Gosper, M. 2010. Using  academic analytic tools to investigate studying behaviours in  technology-supported learning environments. In Proceedings   of ascilite Sydney, 2010 (Sydney, Australia, December 5-10,  2010).    [10] Prunuske, A. J., Batzli, J., Howell, E. and Miller, S. 2012.  Using online lectures to make time for active learning.  Genetics, 192, (September 2012), 67-72.    [11] Risko, E. F., Foulsham, T., Dawson, S., and Kingstone, A.  2012. The collaborative lecture annotation system (CLAS):  A new tool for distributed learning. IEEE Transactions on  Learning Technologies,    [12] Scull, W. R, Kendrick, D., Shearer, R., and Offerman, D.  2011. The landscape of quality assurance in distance  education. Continuing Higher Education Review. 75 (Fall  2011), 138-149.     184      "}
{"index":{"_id":"26"}}
{"datatype":"inproceedings","key":"Clow:2013:MFP:2460296.2460332","author":"Clow, Doug","title":"MOOCs and the Funnel of Participation","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"185--189","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460332","doi":"10.1145/2460296.2460332","acmid":"2460332","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, learning analytics, participation","abstract":"Massive Online Open Courses (MOOCs) are growing substantially in numbers, and also in interest from the educational community. MOOCs offer particular challenges for what is becoming accepted as mainstream practice in learning analytics. Partly for this reason, and partly because of the relative newness of MOOCs as a widespread phenomenon, there is not yet a substantial body of literature on the learning analytics of MOOCs. However, one clear finding is that drop-out/non-completion rates are substantially higher than in more traditional education. This paper explores these issues, and introduces the metaphor of a 'funnel of participation' to reconceptualise the steep drop-off in activity, and the pattern of steeply unequal participation, which appear to be characteristic of MOOCs and similar learning environments. Empirical data to support this funnel of participation are presented from three online learning sites: iSpot (observations of nature), Cloudworks ('a place to share, find and discuss learning and teaching ideas and experiences'), and openED 2.0, a MOOC on business and management that ran between 2010--2012. Implications of the funnel for MOOCs, formal education, and learning analytics practice are discussed. ","pdf":"MOOCs and the Funnel of Participation  Doug Clow   The Open University  Walton Hall, Milton Keynes  MK7 6AA, United Kingdom   +44 1908 654861  Doug.Clow@open.ac.uk            ABSTRACT  Massive Online Open Courses (MOOCs) are growing  substantially in numbers, and also in interest from the educational  community. MOOCs offer particular challenges for what is  becoming accepted as mainstream practice in learning analytics.   Partly for this reason, and partly because of the relative newness  of MOOCs as a widespread phenomenon, there is not yet a  substantial body of literature on the learning analytics of MOOCs.  However, one clear finding is that drop-out/non-completion rates  are substantially higher than in more traditional education.   This paper explores these issues, and introduces the metaphor of a  funnel of participation to reconceptualise the steep drop-off in  activity, and the pattern of steeply unequal participation, which  appear to be characteristic of MOOCs and similar learning  environments. Empirical data to support this funnel of  participation are presented from three online learning sites: iSpot  (observations of nature), Cloudworks (a place to share, find and  discuss learning and teaching ideas and experiences), and  openED 2.0, a MOOC on business and management that ran  between 2010-2012. Implications of the funnel for MOOCs,  formal education, and learning analytics practice are discussed.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning   General Terms  Algorithms, Management, Measurement, Performance, Design,  Economics, Human Factors, Theory, Legal Aspects   Keywords  Learning analytics, participation, MOOCs.   1. INTRODUCTION   A MOOC is an online course with the option of free and open  registration, a publicly shared curriculum, and open-ended  outcomes [38]. MOOCs have the potential to provoke major  shifts in educational practice [48] and are officially [...] the  higher education buzzword of 2012 [51]. There are two distinct  branches: the connectivist MOOCs (cMOOCs) inspired by   Downes, Siemens, Cormier, Groom et al, and the more recent,  more formal MOOCs (xMOOCs), including Udacity, MITx, EdX,  Coursea and Udemy [28].  The pedagogy of these branches are  quite distinct: cMOOCs are underpinned by connectivism [52,  33], a sophisticated and innovative reconceptualisation of what it  means to know and to learn, whereas xMOOCs are so far based  on a very old and out-dated behaviourist pedagogy, relying  primarily on information transmission [7]. This paper will use  MOOC as an umbrella term covering both branches, and will take  a broad view of Course to include any structured open, online  learning opportunity.   2. LEARNING ANALYTICS AND MOOCS  MOOCs  and particularly the cMOOCs closely associated with  many learning analytics figures  pose particular challenges for  learning analytics practice. Participation in a MOOC is emergent,  fragmented, diffuse, and diverse [38]; it seems unlikely that the  learning analytics process will be any less so.   Much learning analytics work presupposes a formal education  context. When learning analytics are most effective, they are an  integrated part of a whole system of learner support, which is hard  to deliver in a MOOC.   The foundational work on Signals at Purdue [4, 5, 9] is based  around a predictive model of likely completion. This is potentially  problematic in a MOOC context. Essa and Ayad [22] set out to  extend predictive modeling to accommodate the considerable  variability in learning contexts across different courses and  different institutions. Whether such efforts to encompass  diversity can include MOOCs is, at present, an open question. A  more profound problem with predictive modeling in MOOCs is  the lack of human resource to mediate the feedback, and the lack  of support available to learners who have come to know that they  are at risk. One useful design framework for learning analytics  [26] can be readily applied to a MOOC, but the terminology (e.g.  teachers and students) may need to be applied loosely.  On the other hand, some learning analytics technologies present  fewer issues in a non-formal context, such as recommendation  engines and other semantic technologies, content analytics [23],  social network analysis and visualisation (for an arresting example  applied to a MOOC, see [10]), and social learning analytics [8].   3. LEARNING ANALYTICS OF MOOCS  There has not yet been extensive published research on xMOOCs,  partly because they are so new, and partly because of their  proprietary nature. On the other hand, cMOOCs have been  researched in some depth, including a specific concern with  learning analytics.   PLENK2010 has received perhaps the most thorough treatment,  with mixed-methods approaches employing a range of qualitative  and quantitative sources, including Moodle data-mining, Twitter  metrics, content analysis, surveys, and interviews [25, 31]. Other  cMOOCs have received similar attention  e.g. CCK08 [24, 35]     Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.     185    and CCK11 [32]. The clear message from these studies is the  importance of methods beyond the simply mechanical /  quantitative.   Learning analytics is possible in the wider context of open, online  learning environments. Pham et al [42] explored two learning  blogospheres, where social network analysis and content  analysis yielded interesting results, including a steeply unequal  fat tailed distribution of posting frequency (which they  erroneously label a power law).  There are two significant points of difference in learning analytics  in MOOCs compared to formal education: one qualitative, and  one quantitative. The qualitative difference is the rationale behind  the course and the aspirations of its designers. In a cMOOC, the  designers are explicitly not intending to specify end points before  the course starts, so a learner who starts but does not complete  may well be seen as a success, depending on the reasons. The  quantitative difference is, as the old saw has it, one that is  sufficiently large to be a qualitative difference: the rate of drop- out is so very much larger in MOOCs. This idea is encapsulated in  the funnel of participation.   4. THE FUNNEL OF PARTICIPATION  The funnel of participation is inspired by the marketing funnel,  or purchase funnel, an idea in widespread use in marketing and  sales (see e.g. [29]). This attempts to model a customers journey  from initial awareness to a sale, typically in four stages:  Awareness  they have to know the product exists; Interest  they  have to want that sort of thing; Desire  they have to want the  specific product; Action  purchase. This model is not without  criticism. More recent thinking argues for more sophistication and  a focus on what the customer does after purchase [17, 41], but the  model remains a widely-used and useful structuring device. The  marketing funnel approach is used in higher education in  marketing departments, and in alumni fundraising, but is not  generally applied to student progress while enrolled.   In the marketing funnel, there is typically significant attrition in  numbers through the stages. A vast number of people need to  become aware that the product exists; a fraction of those will be  interested in that class of product; a fraction of those will form a  desire for the specific product; and, finally, a proportion of those  will make a purchase.    In formal education, despite concerns about drop-out rates, the  total attrition from enrolment/registration to graduation is  typically much lower. In a MOOC, the attrition rate is  significantly higher  approaching those seen in marketing. This is  the basis of the funnel of participation, as shown in Figure 1.     Figure 1: The funnel of participation   Here, the first step is Awareness: potential learners must know  that the MOOC exists. Next is Registration  only a small  proportion of those who are aware will want to sign up and  succeed in doing so. Then a fraction of those registered will go on   to engage in some Activity or other, and some of those will make  meaningful learning Progress. The drop off at each stage is large.   The simple funnel as presented here can be extended in  granularity. So, for instance, the Activity category can be split in  to simply making a contribution, and making more extensive or  higher-level contributions.    The funnel is intended to be applicable in a range of pedagogical  and theoretical contexts, from connectivism to a nave information  transmission model. It is also designed to be congruent with other,  broader conceptions of online participation, such as Communities  of Practice and legitimate peripheral participation [34, 55] Dron  and Andersons collective applications [20], Preece and  Shneidermans Reader-to-Leader Framework [43], and the Fairy  Rings of Participation model [14, 36].    There are also parallels with standard web marketing ideas around  conversion of visitors through to site-specific goals, such as site  registration and online purchases, and click-through and  conversion rates for online advertising.   It should be stressed, however, that the funnel of participation  does not presuppose a fixed outcome: it requires only a shared  form of registration, a shared form of activity, and some notion of  what it would mean to progress, however open-ended.    There are two key features of the funnel: steep drop-off from each  stage to the next, and steeply unequal patterns of participation.  These fat-tailed distributions (often mislabelled power laws)  are characteristic of most if not all online social activity. It has  been long noted as a feature of open networks [3, 44, 49], and is  also seen in formal education where the activity is online [47].   5. EMPIRICAL UNDERPINNING   Table 1: Summary analytics for three different learning sites,  from site opening to 7 Nov 2012. Visits and Unique visitors   from Google Analytics. Registrations and Contributors  (have made at least one contribution) from site databases.   The funnel of participation is underpinned by empirical data.  Three examples are presented here, from three entirely separate  learning websites. The first is iSpot (www.ispot.org.uk), a social  learning community aimed at helping beginners learn to identify  nature observations  [14, 37, 53]. The second is Cloudworks  (www.cloudworks.ac.uk), a professional learning community for  educators and educational researchers [15, 16]. The third is  openED (www.open-ed.eu), an open online course in business and  management aimed at postgraduate/practitioner level [1, 13].  While there is some overlap in the team behind these three sites  (including the author), the user communities are entirely distinct,  with only a handful of users present on more than one of them.    iSpot Cloudworks openED   Visits 1,100,000 275,000 30,000   Unique visitors 390,000  (35%)   165,000  (60%)   15,500  (52%)   Registrations 21,000  (5%)   5,239  (3%)   1,429  (9%)   Contributors 9,000  (43%)   1,750  (33%)   198  (14%)    Contributor rate  2.3% 1.0% 1.3%   186    Table 1 shows summary figures for participation on the three  sites. Two features leap out. First is the dramatic fall-off in each  step in greater involvement. The second is how similar the rates of  attrition are across the three sites.   Closer analysis of these three communities yields further  examples of the funnel, with steep drop-off, and highly unequally  distributed patterns of activity. For iSpot, this pattern has been  explored at length [14]; an updated analysis yielded no significant  differences. For Cloudworks, the funnel can be seen in the number  of users making different numbers of contributions (Table 2). For  openED, the pattern is explored in more detail in Section 6 of this  paper.   Table 2: Number of users who have made given numbers of  contributions to Cloudworks.    The funnel of participation has been observed on other MOOCs  and similar sites. For instance, PLENK2010 had 1,641  registrations, but about 40-60 individuals on average contributed  actively to the course on a regular basis [25], or 2.4-3.7%  close  to the overall contributor rate seen above. On CCK08, there were  2,200 participants [19]; there were 83 respondents to the post- course survey, of whom 15 said they had completed the entire  course (and 13 of those were studying formally for credit) [24].  Similarly, on Athabasca Landing, a social learning site for  (formal) students at Athabasca University, 78% of the content is  created by 21% of the users, and around 18-21% of the users are  active [45].   The funnel is also apparent  at least at the very coarsest level  in    reported completion rates for xMOOCs. These are variously said  to be less than 10% of registered students completing the course  [28] or generally between 10 and 20 percent [30].  The first  MITx course, Circuits and Electronics, attracted over 150,000  participants, but fewer than half look at the first problem set,  and only 7,157 passed, or about 5% [18]. Courseras first  Software Engineering course enrolled 50,000 students, of whom  3,500 passed, or 7% [40].   These rates are considerably lower than for conventional higher  education, where in the UK completion rates are over 90% for  highly selective high-status universities, and above 60% for  universities with a broader social mission. It is notable, however,  that rates for online and distance universities fall somewhere  between conventional HE and xMOOCs: University of Phoenix  completion rates are 31-36% for undergraduate-level degrees [54],  while completion rates for the Universitat Oberta de Catalunya  (UOC, Open University of Catalonia) have ranged between 33%  and 67% [27].   6. THE OPENED 2.0 COURSE  Having looked broadly at evidence of the funnel of participation,  this paper now looks more closely at one specific example: the  openED 2.0 course.   The openED 2.0 project explored a framework for collaborative,  open, multi-institutional development of a course, coupled with an  open, online model of delivery. Seven European organisations  worked together to create the course, largely based on existing  Open Educational Resources (OER). The main learning  environment was a customised version of Joomla, with additional  learning support provided by email, IRC chats, and Elluminate  conferencing. The course was presented three times between 2010   and 2012. The principles and rationale for openED and related  courses are articulated extensively elsewhere [39], and an account  of the approach to the design of the course has been published [1].  The projects deliverables included a full report of the evaluation  [13]. This section presents a further quantitative analysis of the  participation data.     Figure 2: Log plot of the number of visits to openED per user,   ranked by number of visits.   0  0.5  1  1.5  2  2.5  0 50 100 150 200 250  rank  lo g(  fo ru  m  p  os ts  )    Figure 3: Log plot of the number of forum posts on openED   per user, ranked by number of forum posts.  Visit data could be attributed with confidence to 691 individuals;  199 individuals made posts to the course forums. As can be seen  from Figures 2 and 3, the distribution of both visits and posts to  the forum was steeply unequal, or fat tailed. Neither, however,  follow a power law (see [11]). The visit data could be connected  to the forum data for 178 users; there was a clear correlation  between the two (R = 0.86, p < 0.0001), as would be expected.  These patterns of steeply-unequal participation and steep, staged  drop out fit the key characteristics of the funnel of participation.   7. DISCUSSION  The funnel of participation is a real, significant phenomenon in  MOOCs and related courses. Compared to formal learning, there  tends to be much higher rates of drop-out, and steeply unequal  patterns of participation. This is probably an almost-inevitable  consequence of any open, online activity: there is less initial  commitment, so the filtering happens at a later stage [38]; and the  well-attested tendency for steeply unequal patterns of  participation to emerge in online activity is manifest.   The phenomenon shows that MOOCs alone cannot replace  degrees or most other formal qualifications. The significant efforts   Contributions 0 1-5 5-9 10-49 50+   Users  3,489 1,322 192 192 44   187    that institutions put in to supporting their learners to reach a  commonality of learning outcome are necessary, and have a real  effect. As Siemens [50] argues, the long-term value for  universities is likely to lie in precisely those things that cannot be  cheaply duplicated through a MOOC.   Does it matter if MOOCs have high drop-out rates Some argue  that it is a positive sign of an exploratory phase [46]; Daniel [18]  points out that answers to this question create a sharp distinction  between the xMOOCs providers and other distance learning  institutions, with the xMOOCs observing that early drop-outs do  not add significantly to costs [30]. What constitutes drop-out and  completion can be a complex problem, particularly for online and  distance institutions, such as the UK Open University [6] and  UOC [27]: rates are highly sensitive to their precise definition,  and vary widely between courses. Is it drop-out, or non- continuation, or climb-out This is a long-standing issue for  distance educators [3], and is a bigger question in MOOCs,  because the phenomenon is so much larger. Where we have  indications of problems (e.g. the evidence that some learners find  cMOOCs confusing [24, 38]), we have a responsibility to do what  we can to address them.   Learning analytics offers great potential, but the choice of  intervention in a MOOC may be niew problematic. For example,  in a formal situation, a prediction of likely failure to complete is  instantly meaningful, relevant, and can be mediated by skilled  learning professionals, and the learner can be supported by a  range of existing resources and specialists.   The value in learning analytics comes from closing the loop  effectively to complete the Learning Analytics Cycle [12]. The  funnel of participation shows that this is a particular challenge for  MOOCs and similar open, online environments: they tend towards  steep drop-offs and highly unequal patterns of participation.  However, the experience of online and distance teaching  institutions, where the rates of drop-out fall somewhere between  conventional courses and MOOCs, suggests that it is possible to  mitigate the impact of the funnel. There is likely to be significant  value in further work to empirically explore and validate how  learning analytics can help learners in a MOOC context.   8. ACKNOWLEDGEMENTS  The author wishes to thank James Aczel for his invaluable  contribution to the creation and launch of the openED project, and  Simon Cross for his work on the project. Thanks are also due to  the project partners and the participants in the course. The  openED course was funded by the European Commission through  the Lifelong Learning Programme as project 505667-LLP-1-2009- 1-PT-KA3-KA3MP. This paper remains, however, the sole  responsibility of the author.   9. REFERENCES  [1] Aczel, J., Cross, S., Meiszner, A., Hardy, P., McAndrew, P.,   and Clow, D. 2011. Some issues affecting the sustainability  of open learning courses. EDEN 2011 Annual Conference,  Dublin, Ireland, 19-22 June 2011.   [2] Anderson, C. 2006. The Long Tail: How endless choice is  creating unlimited demand. London: Random House.   [3] Anderson, T. 2012. Interesting analysis of a c-MOOC.  http://terrya.edublogs.org/2012/07/25/interesting-network- analysis-of-a-c-mooc/   [4] Arnold, K. E. 2010. Signals: Applying academic analytics.  EDUCAUSE Quarterly, 33, 1.   [5] Arnold, K. E. & Pistilli, M. D. 2012. Course Signals at  Purdue: Using learning analytics to increase student  success. Proc. 2nd Int. Conf. on Learning Analytics &  Knowledge. New York: ACM.   [6] Ashby, A. 2004. Monitoring student retention in the Open  University: definition, measurement, interpretation and  action. Open Learning, 19, 1, 65-77.   [7] Bates, T. 2012. Whats right and whats wrong about  Coursera-style MOOCs.  http://www.tonybates.ca/2012/08/05/whats-right-and-whats- wrong-about-coursera-style-moocs/   [8] Buckingham Shum, S., & Ferguson, R. (2012). Social  Learning Analytics. Ed. Tech. & Society, 15, 3, 326.   [9] Campbell, J.P., and Oblinger, D.G. 2007. Academic  Analytics. EDUCAUSE Quarterly, October 2007.   [10] CBlissMath. 2012.  http://www.youtube.com/watchv=K_dIXNGVZnk   [11] Clauset, A., Shalizi, C.R., Newman, M.E.J. 2009. Power-law  distributions in empirical data. SIAM Review 51(4), 661-703.   [12] Clow, D. 2012. The Learning Analytics Cycle: Closing the  loop effectively. Proc. 2nd Int. Conf. on Learning Analytics  & Knowledge. New York: ACM.   [13] Clow, D., Cross, S., and McAndrew, P. (2012). Quality  Assurance  D7.3 Assessment Report. EU Deliverable:  openED, 505667-LLP-1-2009-1-PT-KA3-KA3MP.   [14] Clow, D., and Makriyannis, E. 2011. iSpot Analysed:  Participatory Learning and Reputation. Proc.1st Int. Conf. on  Learning Analytics and Knowledge, Banff, Alberta, Canada.   [15] Conole, G., Culver, J., Williams, P., Cross, S., Clark, P., and  Brasher, A. 2008. Cloudworks: social networking for  learning design. Proc. ascilite Melbourne 2008.   [16] Conole, G., and Culver, J. 2010. The design of Cloudworks:  Applying social networking practice to foster the exchange  of learning and teaching ideas and designs. Comput. Educ.,  54(3), 679 692.   [17] Court, D., Elzinga, D., Mulder, S., and Vetvik, O.J. 2009.  The consumer decision journey. McKinsey Quarterly, June  2009.  http://www.mckinseyquarterly.com/Media_Entertainment/Pu blishing/The_consumer_decision_journey_2373#   [18] Daniel, J. 2012. Making Sense of MOOCs: Musings in a  Maze of Myth, Paradox and Possibility.  http://sirjohn.ca/wordpress/wp- content/uploads/2012/08/120925MOOCspaper2.pdf   [19] Downes, S., and others. 2011. The MOOC Guide  https://sites.google.com/site/themoocguide/home   [20] Dron, J., and Anderson, T. 2009. On the Design of Collective  Applications. 2009 Int. Conf. on Computational Science and  Engineering, 368374.   [21] Esposito, A. 2012. Research ethics in emerging forms of  online learning: issues arising from a hypothetical study on a  MOOC. Electronic J. e-Learning 10, 3, 315-325.   [22] Essa, A., and Ayad, H. 2012. Student Success System: Risk  Analytics and Data Visualization using Ensembles of  Predictive Models. Proc. 2nd Int. Conf. on Learning  Analytics & Knowledge. New York: ACM.   188    [23] Ferguson, R. 2012. The state of learning analytics in 2012: a  review and future challenges. Technical Report No. KMI-12- 01, Knowledge Media Institute, The Open University  http://kmi. open. ac. uk/publications/techreport/kmi-12-01   [24] Fini, A. 2009. The Technological Dimension of a Massive  Open Online Course: The Case of the CCK08 Course Tools.  IRRODL, 10, 5.    [25] Fournier, H., Kop, R., and Sitla, H. 2011. The value of  learning analytics to networked learning on a personal  learning environment. Proc.1st Int. Conf. on Learning  Analytics and Knowledge, Banff, Alberta, Canada.   [26] Greller, W., and Drachsler, H. 2012. Translating Learning  into Numbers: A Generic Framework for Learning Analytics.  Ed. Tech. & Society, 15, 3, 4257.     [27] Grau-Valldosera, J. and Minguilln, J. 2011. Redefining  dropping out in online higher education: a case study from  the UOC. Proc.1st Int. Conf. on Learning Analytics and  Knowledge, Banff, Alberta, Canada.   [28] Hill, P. 2012. Four Barriers That MOOCs Must Overcome  To Build a Sustainable Model.  http://mfeldstein.com/four- barriers-that-moocs-must-overcome-to-become-sustainable- model   [29] Jobber, D. 1995. Principles and practice of marketing.  McGraw-Hill, Berkshire.   [30] Kolowich, S. 2012. How will MOOCs make money Inside  Higher Ed.  http://www.insidehighered.com/news/2012/06/11/experts- speculate-possible-business-models-mooc-providers   [31] Kop, R. 2011. The Challenges to Connectivist Learning on  Open Online Networks: Learning Experiences during a  Massive Open Online Course Analysis of Plenk2010.  IRRODL, 12, 3.   [32] Kop, R., Fournier, H., and Mak, J.S.F. 2011. A Pedagogy of  Abundance or a Pedagogy to Support Human Beings  Participant Support on Massive Open Online Courses.  IRRODL, 12, 7.   [33] Kop, R., and Hill, A. 2008. Connectivism: Learning theory  of the future or vestige of the past IRRODL, 9, 3.   [34] Lave, J., and Wenger, E. 1991 Situated Learning: Legitimate  Peripheral Participation. Cambridge University Press,  Cambridge.   [35] Mak, S.F.J, Williams, R., and Mackness, J. 2010. Blogs and  forums as communication and learning tools in a MOOC.   Proc. 7th Int. Conf. on Networked Learning. University of  Lancaster, UK.   [36] Makriyannis, E., and de Liddo, A., 2010. Fairy Rings of  Participation: The invisible network influencing participation  in online communities. Proc. 7th Conf. on Networked  Learning. University of Lancaster, UK.   [37] McAndrew, P., Scanlon, E., and Clow, D. 2010. An Open  Future for Higher Education. Educause Quarterly, 33(1).   [38] McAulay, A., Stewart, B., and Siemens, G. 2010. The  MOOC model for digital practice. University of Prince  Edward Island.  http://www.elearnspace.org/Articles/MOOC_Final.pdf   [39] Meiszner, A., 2011. The Why and How of Open Education.  UNU-MERIT, Maastricht, Netherlands. http://unu.edu/news/  book-the-why-and-how-of-open-education.html   [40] Meyer, R. 2012. What Its Like to Teach a MOOC (And  What the Hecks a MOOC). The Atlantic.  http://www.theatlantic.com/technology/archive/2012/07/what -its-like-to-teach-a-mooc-and-what-the-hecks-a- mooc/260000/ XXX   [41] Noble, S. 2010 Its Time To Bury The Marketing Funnel.  Forbes. http://www.forbes.com/2010/12/08/customer-life- cycle-leadership-cmo-network-funnel.html   [42] Pham, M.C., Derntl, M., Cao, Y., an Klamm, R. 2012.  Learning Analytics for Learning Blogospheres, In. E.  Popescu et al. (Eds.): ICWL 2012, LNCS 7558, 258267.   [43] Preece, J., and Shneiderman, B. 2009. The Reader-to-Leader  Framework: Motivating Technology-Mediated Social  Participation. AIS T. HCI. 1, 1, 13-32.   [44] Priedhorsky, R., Chen, J., Lam, S., Panciera, K., Terveen, L.,  and Riedl, J. (2007). Creating, destroying, and restoring  value in Wikipedia. 2007 Int. ACM Conf. on Supporting  Group Work, Sanibel Island, Florida, 259-268. New York:  ACM.   [45] Rahman, N. and Dron, J. 2012. Challenges and opportunities  for learning analytics when formal teaching meets social  spaces. Proc. 2nd Int. Conf. on Learning Analytics &  Knowledge. New York: ACM.   [46] Rosen, R.J. 2012. Overblown-Claims-of-Failure Watch: How  Not to Gauge the Success of Online Courses. The Atlantic.  http://www.theatlantic.com/technology/archive/2012/07/over blown-claims-of-failure-watch-how-not-to-gauge-the- success-of-online-courses/260159/   [47] Rosewell, J. and Hirst, T. 2008 Equability and dominance in  online forums.  CALRG Conference, 18-19 June 2008.  http://kn.open.ac.uk/public/document.cfmdocid=11627   [48] Sharples, M. et al. 2012. Innovating Pedagogy 2012: Open  University Innovation Report 1. The Open University,  Milton Keynes, UK.   [49] Shirky, C. 2003 Power Laws, Weblogs, and Inequality,  http://www.shirky.com/writings/powerlaw_weblog.html   [50] Siemens, G. 2011. Duplication theory of educational value.  http://www.elearnspace.org/blog/2011/09/15/duplication- theory-of-educational-value/   [51] Siemens, G. 2012. MOOCs are really a platform.  http://www.elearnspace.org/blog/2012/07/25/moocs-are- really-a-platform/   [52] Siemens, G., and Downes, S. 2008, 2009. Connectivism &  connected knowledge: CCK08, CCK09.  http://ltc.umanitoba.ca/connectivism/   [53] Silvertown, J. 2009. A new dawn for citizen science. Trends  in Ecology and Evolution 24, 9, 467-471.   [54] University of Phoenix. 2012. 2011 Academic Annual Report.  http://www.phoenix.edu/about_us/publications/academic- annual-report.html   [55] Wenger, E. 1998. Communities of Practice: Learning,  Meaning and Identity. Cambridge University Press,  Cambridge.   189      "}
{"index":{"_id":"27"}}
{"datatype":"inproceedings","key":"SaoPedro:2013:DKS:2460296.2460334","author":"Sao Pedro, Michael A. and Baker, Ryan S. J. D. and Gobert, Janice D.","title":"What Different Kinds of Stratification Can Reveal About the Generalizability of Data-mined Skill Assessment Models","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"190--194","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460334","doi":"10.1145/2460296.2460334","acmid":"2460334","publisher":"ACM","address":"New York, NY, USA","keywords":"automated inquiry assessment, educational data mining, generalizability, learning analytics, science inquiry, science microworlds, science simulations, user modeling, validation","abstract":"When validating assessment models built with data mining, generalization is typically tested at the student-level, where models are tested on new students. This approach, though, may fail to find cases where model performance suffers if other aspects of those cases relevant to prediction are not well represented. We explore this here by testing if scientific inquiry skill models built and validated for one science topic can predict skill demonstration for new students and a new science topic. Test cases were chosen using two methods: student-level stratification, and stratification based on the amount of trials ran during students' experimentation. We found that predictive performance of the models was different on each test set, revealing limitations that would have been missed from student-level validation alone.","pdf":"What Different Kinds of Stratification Can Reveal about the  Generalizability of Data-Mined Skill Assessment Models   Michael A. Sao Pedro  Worcester Polytechnic Institute   100 Institute Rd.  Worcester, MA 01609 USA   (508) 831-5000   mikesp@wpi.edu   Ryan S.J.d. Baker  Teachers College, Columbia  525 W. 120th St., Box 118  New York, NY 10027 USA   (212) 678-8329   ryan@educationaldatamining.org   Janice D. Gobert  Worcester Polytechnic Institute   100 Institute Rd.  Worcester, MA 01609 USA   (508) 831-5000   jgobert@wpi.edu      ABSTRACT  When validating assessment models built with data mining,   generalization is typically tested at the student-level, where   models are tested on new students. This approach, though, may   fail to find cases where model performance suffers if other aspects  of those cases relevant to prediction are not well represented. We   explore this here by testing if scientific inquiry skill models built   and validated for one science topic can predict skill demonstration   for new students and a new science topic. Test cases were chosen   using two methods: student-level stratification, and stratification  based on the amount of trials ran during students   experimentation. We found that predictive performance of the   models was different on each test set, revealing limitations that   would have been missed from student-level validation alone.   Categories and Subject Descriptors   H.2.8 [Database Applications]: Data Mining; J.1   [Administrative Data Processing]: Education; K.3.1 [Computer  Uses in Education]: Computer-assisted instruction (CAI),   Computer-managed instruction (CMI)   General Terms   Measurement, Reliability   Keywords  Science Microworlds, Science Simulations, Science Inquiry,  Automated Inquiry Assessment, Educational Data Mining,   Learning Analytics, Validation, Generalizability, User Modeling   1. INTRODUCTION  Data mining/learning analytics is a powerful approach for   building predictive models (detectors) that determine if a   student is engaging in a particular behavior (e.g. [1], [2]), and   models that assess whether students demonstrate somewhat ill-  defined skills within interactive learning environments (e.g. [3],  [4]). Building models using data mining makes validation of those   models easier, because processes like cross-validation exist to   estimate how well they will generalize to new students and tasks   not used to build them. Such estimates are important because they    can provide assurance that the behavior / assessment models will  correctly identify students who lack skill or engage in undesirable   learning behaviors, enabling the system to provide accurate, real-  time feedback [5]. Within open-ended interactive environments,   the estimates can also assure that models that detect skill  demonstration can be reused for new tasks/domains and students,   paving the way for reliable, scalable performance-based   assessment.   In educational domains, validation is often done at the student-  level, where models are built using one group of students data  and tested on new students whose data were not in model   construction [6], [1]. This ensures models will remain accurate   when applied to completely new students. It is possible, though,   that this method may fail to identify specific instances when   models do not predict adequately, particularly if some other aspect  of those cases, other than the student, is not taken into account.   We explore this topic here in the context of determining how well   two data-mined models that assess whether students demonstrate   scientific inquiry skills, built and validated for one science topic   [4], [7], can predict demonstration of those skills for a new  science topic and a new set of students. Few papers have tested   model effectiveness on different topics ([1] is an exception), but   validating at this level is essential if models will be used beyond   the topics in which they were originally developed.    In our approach, we first take this new topic and student sample,   and construct a test set stratified at the student level, where   students are equally represented in the test set. When doing this,   we find that there is an imbalance in the nature of behaviors   demonstrated by students. In particular, there is an imbalance in  the number of trials collected by students in this set, a factor   which could influence predictive performance of our models (cf.   [7]).  To address this, we construct a second test set, this time   stratifying over the number of trials, to ensure a greater balance in   student behaviors. We show that utilizing this different kind of  stratification can unveil a different performance profile than   conducting student-level validation alone, revealing new insights   on the predictive limitations of the models.   2. PRIOR WORK BUILDING INQUIRY  SKILL MODELS   In [4], [7], we developed data-mined models that assess students  demonstration of scientific inquiry process skills. Skills are   assessed as students conduct inquiry within Inq-ITS activities   (formerly known as Science Assistments [8]). This environment   aims to automatically assess, scaffold, track, and provide real-time   feedback on students scientific inquiry skills. Inquiry is assessed  as students explore within interactive simulations and use inquiry   support widgets that facilitate experimentation.   Inq-ITS activities are performance assessments of inquiry skills.   The actions students take within the simulation and work products   they create are the bases for assessment [8]. This paper focuses on   assessment of two process skills, designing controlled      Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit  or commercial advantage and that  copies bear this notice and the full citation on the first  page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,   requires prior specific permission and/or a fee.   LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   190    experiments and testing stated hypotheses. Briefly, students   design controlled experiments when they generate data that make   it possible to determine what the effects of independent variables  (factors) are on outcomes. They test stated hypotheses when they   generate data that can support or refute an explicitly stated   hypothesis. Since these are process skills, students are assessed   based on the actions they take while collecting data.   To build data mined assessment models, we employed text replay   tagging of log files [9] to generate labels from which, in part,   models were derived. A text replay is a chunk of student actions  in text format that contains information enabling a human coder to   identify what occurred in that sequence of actions. For our   domain, text replays leverage human judgment to identify whether   students demonstrate inquiry skill. These labels are then used as   ground truth for whether or not students demonstrate a skill, and   subsequently for building and validating detectors.   Prior to this paper, we have validated models for these skills for   one physical science topic, Phase Change, and one student sample   [4], [7]. One goal of this paper is to determine if these models can   be applied to a new Physical Science topic, Free Fall, and to a   new sample of students. As such, we first present a high-level  description of the text replay tagging process within the context of   assessing the two skills for Phase Change. A fuller description of   the distillation process appears in [4], and of the model   construction approach in [7].   2.1 Phase Change Activities  The Phase Change activities [8] aim to promote understanding   about the melting and boiling properties of ice. Students learn   about the topic by engaging in semi-structured scientific inquiry   activities with a simulation. First, students are given an explicit   goal to determine if one of four factors affects various measurable  outcomes (e.g. melting or boiling point). Students then formulate   hypotheses, collect and interpret data, and warrant their claims to   address the goal. As mentioned, we developed data-mined models   that infer whether students design controlled experiments and test   stated hypotheses [4], [7]. These skills are demonstrated when  students collect data to test their hypotheses in the experiment   task. As such, we describe this task in more detail.   In the experiment task, students are shown the Phase Change   simulation, graphs that track changes of the substances   temperature over time, and a table that captures all the data they   collected thus far. They experiment by collecting data that aim to  support or refute their hypotheses. For space reasons, we do not   include a visual of this interface (it can be seen in [8]), but it is   similar to the Free Fall interface shown in Figure 1. Students   collect data (trials) by changing the simulations variable values,   and running, pausing and resetting the simulation. Next, we  describe how these interactions were distilled and tagged with   skill demonstration.   2.2 Distilling Raw Logs and Building Clips  Interaction data was collected from 148 middle school students   who conducted inquiry within four Phase Change activities. All  students fine-grained actions were timestamped and recorded by   the system. These actions included: interactions with the inquiry   support widgets, interactions with the simulation including   changing simulation variable values and running/pausing/resetting   the simulation, and transitioning between inquiry tasks.   Contiguous sequences of these actions were then segmented into   clips. A clip contains all the actions necessary for a human coder  to identify (label) whether or not students demonstrate the inquiry   skills. For our domain, a clip has all actions taken while   formulating hypotheses (hypothesize actions) and collecting   data (experiment actions) in an activity. Clips are the grain-size   at which data collection skill is labeled, and in turn, student  performance is assessed. From there, models describing what it   means to demonstrate skill were trained and validated based on   (1) labels indicating whether students demonstrated skills within   clips, and (2) a set of features that summarize clips.   Humans determine whether or not students demonstrate skills   within clips, by labeling text replays of clips with one or more   tags (see [4] for an example of a text replay). For us, text replays  highlight hypothesis creation and experimentation processes. This   enables human to more easily identify skill demonstration. Clips   are tagged as designing controlled experiments, testing stated   hypotheses, both, or neither. In this prior work, we achieved   acceptable levels of inter-rater reliability in labeling clips [4].   With clip labels in place, features were defined next that  summarize clips. We defined a set of 79 features related to   students experimentation [4], [7]. Features include basic   aggregations of actions and domain-specific counts indicative of   skill demonstration. Examples include: number of trials run,   counts related collecting unconfounded data, a count for changing  the variable stated in a hypothesis, and number of simulation   pauses. With clip labels and summary features defined, we next   describe the model building process.   2.3 Extracting Models from the Data  To construct our models, we employed an approach that mixed  traditional data mining, iterative search, and domain expertise,   discussed in [7]. This procedure yielded two models, one for each   skill, that take as input a clip (note that each student contributes   multiple clips) and, by examining the clips feature values, predict   if a student demonstrated skill in that clip. Briefly, this procedure   worked as follows.    First, human coders tagged three sets of clips, a training set, a   validation set, and a held-out test set (more detailed descriptions   of these sets appears in [7]). Next, an initial candidate feature set   of promising predictors was selected by finding features that   correlated moderately with the skill labels from the training set.  This reduced the original 79 features to 11 candidate features.   Then, a manual backwards elimination search was performed to   find the optimum feature set yielding the best predicting model.   At each stage of the search, a feature with low theoretical support   was removed from the candidate feature set. Theoretical support  for a feature was determined by a domain expert based on theory   on features indicative (or not indicative) of skill demonstration.   Then, J48 decision trees were built from the candidate feature set   and training clips to yield a candidate model. The candidate   model was then tested against the validation set of clips to  determine how well it predicted and kept if it predicted better than   its predecessor. This process repeated until predictive   performance no longer improved for the validation clips. The   rationale for using decision trees is described in [4].   Predictive performance was measured using A [10] and Cohens   Kappa ( ). A' is the probability that when given two clips, one   labeled as demonstrating skill and one not, the model will  correctly identify which clip is which. A model with A' of 0.5   performs at chance, 1.0 indicates perfect performance.  measures   how well the model agrees with the human label; 0.0 indicates   chance-level performance, 1.0 indicates perfect performance.   Once optimal models were found, their final predictive   performance was measured against the held-out test set containing  clips not used in model construction. This step was needed to   191    better estimate true model goodness since the validation set was   used during model selection. In prior versions of these models [4],   cross-validation was conducted at the student-level, where  students were included in either the training or test folds,   validating that the models could generalize to new students.      Figure 1. Example Free Fall Activity   2.4 Prior Results and Next Steps  In [7], we found that both assessment models could predict skill   demonstration for unseen clips quite well. The designing   controlled experiments model was quite good at distinguishing a   clip in which the skill was demonstrated from a clip in which skill  was not (A = .94). It also matched the human coders labels   better than chance ( = .45). Similarly, the testing stated   hypotheses model performed very well, A = .91, = .70. These   findings meant that the data-mined models could adequately  identify skill demonstration in the Phase Change inquiry activities   for this sample of students.    Though their performance within the Phase Change microworld is   encouraging, these metrics do not provide a measure of their   generalizability to other science topics, because the models were   built solely from data for the Phase Change activities.   Furthermore, the model construction procedure in [7] used the  same students in the training/validation clip sets as in the test set.   Thus, we aim here to explore the generalizability of these models   to a new Physical Science topic, Free Fall. To do so, we collected   data from new students who conducted inquiry Free Fall   activities, tagged their resulting clips with skills, and re-measured  the models predictive performance. Using data from a different   science topic enables us to assess model transfer to different   topics [cf. 1]. Using new students also enables us to assess how   well these models will work for a broader range of students.   3. FREE FALL INQUIRY ACTIVITIES  The Free Fall activities in Inq-ITS (Figure 1) aim to foster   understanding about factors that influence the kinetic, potential   and mechanical energy of a ball when it is dropped. The two  factors students could change were the balls mass and starting   height. The look-and-feel and structure of these activities is   generally similar to Phase Change, but with some notable   differences. First, the layout of components on the screen was   redesigned to improve the organization of information and   instructions. Second, the number of factors the student could   manipulate was smaller (2 here versus 4 in Phase Change). Third,  students could only specify one hypothesis in total. Fourth,   students were shown three graphs in the experiment phase to   track each of the dependent measures over time. Finally, unlike   Phase Change, the table showing the results of students trials was   always visible. Next, we describe which clips were tagged to test  the generalizability of the models.   4. DATA SETS  We collected data from 292 eighth grade students interactions  with the Free Fall activities. None of these students were part of   the original data set used to construct the models. Students   attended three different schools in suburban Central   Massachusetts. All had prior experience conducting inquiry in   Inq-ITS for topics other than Free Fall. Students engaged in at  most five different Free Fall activities. As per the text replay   tagging process, clips were distilled to cull out student actions   relevant to hypothesizing and collecting data. In total, 1580 clips   were generated.   Since tagging all clips would be time consuming, we selected a  representative set of clips. One approach for selecting clips for the   test set is to apply student-level stratification when choosing clips   to code, so that each student is equally represented in the data set.   We note that this is distinct from student-level cross-validation,   where students are distributed to either training or test folds, e.g.  [6], [1]. Equally representing all students in a test set, and using   students different than those used for model construction provides   more assurance that such models will work for new students. In   our work, this stratification was performed as follows:    Student-stratified test set (291 clips): One clip per student was  randomly selected and tagged by a human coder. Only clips in   which a student ran the simulation at least once were   considered for selection. One student did not appear in this set,   because they had no clips with at least one run. In this set,  90.0% of the clips and 87.6% of the clips were tagged as   designing controlled experiments and testing stated   hypotheses, respectively.   During the clip selection process, we noticed that a   disproportionate number of clips had exactly 3 simulation runs.  As shown in Table 1, 70.4% of all clips distilled had 3 simulation   runs, and 74.6% in the student-level test set. Though these   percentages may reflect actual student behavior, it is possible that   some aspects of the models performance may not be captured by   stratifying solely in terms of the student. In particular, the models  performance may be impacted by different numbers of simulation   runs. This is important because we aspire to have models that   work for varying numbers of simulation runs, particularly since   we activate scaffolding in the live system after students run the   simulation [7]. To address this, we constructed a second test set  that ensures clips with a given number of simulation runs are   equally represented. This stratification is described below:    Run-stratified test set (245 clips): To generate a test set that  balances the number of runs per clip, we determined an  optimal number of clips to have per stratum. Given the   distribution in Table 1, we used runs = 5, 49 clips, as the base.   We then randomly select 49 clips for each stratum with   exactly 2 simulation runs, 3 runs, etc. The final stratum was   for clips with more than 5 runs. As in [7], we do not consider  clips with fewer than 2 simulation runs, because   demonstration of skill requires at least two trials to be   collected. In this set, 93.1% of the clips and 83.3% of the clips   192    were tagged as designing controlled experiments and testing   stated hypotheses, respectively. Students work could be   represented more than once in this test set.   We note it would be more optimal to stratify over both runs and   students, but too few clips would have been available for testing.   In the next section, we present our models predictive   performance against these two held-out test sets.   Table 1. Counts of Clips by Number of S imulation Runs      5. RESULTS  We estimate how well the two inquiry skill assessment models   built for one science topic, Phase Change, can predict skill  demonstration for another topic, Free Fall, and a new sample of   students. Generalizability is estimated by measuring how well the   models predict skill demonstration in two held-out test sets   containing clips pertaining to Free Fall activities. In the first test   set, clips were randomly chosen via student-level stratification.  Given our interest in understanding how well the models work at   finer grain-sizes [7] and the earlier finding that clips with exactly   3 simulation runs were over-represented, we constructed a second   test set. This set had clips randomly chosen to ensure a balanced   number of clips with a given number of simulation runs.   Performance is again measured using A' and Kappa ( ), though   we also report precision and recall for our models for   completeness. We focus on these metrics because they   compensate for successful classification by chance, which is   important given the imbalance in clip labels. Furthermore, as will  be shown below, most of the models precision and recall values   are near maximum, whereas the A and are more varied. Thus,   we believe A and  may better reflect models limitations.   5.1 Student Stratification Performance  As shown in Table 2, both models performed quite well at   predicting clips in the student stratified test set. The designing   controlled experiments model could distinguish a clip in which  skill was demonstrated from a clip which it was not at a rate of A   = 90%. It also highly agreed with the human coders ratings, =   .65. Performance for the testing stated hypotheses model was also   high, A = .91, = .62. These findings imply that the detectors  built for Phase Change generalize to another Physical Science   topic, Free Fall, and to an entirely new student sample, under   student-level stratification.    Recall this set has exactly one randomly chosen clip per student.   Furthermore, as shown in Table 1, a majority of these clips had  exactly 3 runs.  Though a majority of students may run exactly   three trials, providing credence to being able to use the detectors   as-is to assess students, the models performance may differ based   on the number of trials collected. We turn next to performance on   the run-level stratification test set.    5.2 Run Stratification Performance  As shown in Table 3, the performance profile on the run stratified   test set was different than on the student stratified test set. Though   the performance of the testing stated hypotheses model remained   high (A=.78, =.59), performance dropped for the designing  controlled experiments model, particularly for raw agreement with   labels (e.g. ) (A = .84, = .26). We inspected these results more   closely by recalculating the metrics for each stratum of 49 clips.   As shown in the bottom of Table 3, when model confidence is not   taken into account ( ), the designing controlled experiments  model had very low agreement with human labels for all run-  levels ( = .08 - .17) with the exception of clips with exactly 3   simulation runs ( = 1.00). The testing stated hypotheses model   fared better on agreeing with human labels on all strata ( = .40 -   .78) except for clips with exactly 4 simulation runs ( = .00).   When model confidence is taken into account (A), both models   could distinguish clips that demonstrated skill from those that did  not fairly well on each strata, with the exception of the designing   controlled experiments for at least 5 simulation runs (A >= .61).   Table 2. Overall Performance on Student-Stratified Test Set      In summary, both models performed well under student-level   validation. However, under run-level validation, the testing stated   hypotheses model remained strong while the designing controlled   experiments models performance suffered. In the next section,  we discuss the implications of these finding on generalizability.   6. DISCUSSION AND CONCLUSIONS  We investigated whether data-mined models that assess two  inquiry process skills for activities in one science topic [7] could   be reused as-is for assessing those same skills for a new topic and   new student sample. To explore this, we collected a new set of   student interactions for the topic, employed text replay tagging   [9], [4] in which student interactions (clips) were labeled by  humans with skill demonstration, and measured our models   ability to predict those labels. The overarching goal of this process   is to measure the degree to which these models can enable   scalable, reliable performance-based assessment of the inquiry   skills as students conduct inquiry within simulations [8].   Central to this work was choosing the clips to code that would   yield good estimates of model performance, since coding all   student interactions would be too laborious. One approach was to   represent the new students equally in the held-out test set. We   noticed that when we stratified this way there was an imbalance in  clips for an important kind of student interaction indicative of   skill, the number of times students ran the simulation. As such, we   constructed a different held-out test set that ensured an equal   representation over the number of simulation runs.   Under student-level stratification we found that the assessment  models of each skill performed quite well in this new domain and   new sample of students. These findings provide evidence that the   Simulation     Runs  # Clips Distilled   in Total  # Clips in   Student Strat.   Test Set  # Clips in            Run Strat.        Test Set  < 2 167 20 0  2 91 18 49  3 1112 217 49  4 102 15 49  5 49 10 49  > 5 59 11 49  Total: 1580 291 245  True N True Y True N True Y  Pred N 26 20 Pred N 21 7  Pred Y 3 242 Pred Y 15 248  * Pc = precision; Rc = recall  Testing Stated   Hypotheses  Pc = .99, Rc = .92 Pc = .94, Rc = .97  A' = .90, K = .65 A' = .91, K = .62  Designing Controlled   Experiments  193    models can be applied as-is to new topics without retraining [cf.   1]. Under run-level stratification, a different performance profile   for the models emerged.  The testing stated hypotheses assessment  model still maintained high performance providing even stronger   evidence of its generalizability. However, performance for the   designing controlled experiments detector decreased. This model   worked best for clips with exactly three simulation runs, the most   prominent kind of clip; performance on other clips was poorer.  Though performance was poorer, if the distribution of clips with   given numbers of runs (Table 1) is representative of the student   population we aim to assess, this model still can be used to assess   in the new topic. As a side note, we did examine why performance   was hindered (not presented in the results section). We found that  clips that were misclassified primarily fell under a branch of the   decision tree with features reflecting domain complexity  (the   number of variables changeable by the student). One possible way   to improve generalizability would be use ratio-based features (e.g.   percent of pairwise controlled trials over all possible pairs of  trials) instead of a raw counts [7] for handling domain complexity.   Table 3. Performance on Run-Stratified Test Set      This paper offers two contributions towards assessing the   generalizability of data-mined models used to assess students   skills. First, like prior work [1], [3] we measure the transferability    of models built for one task to a new task and new set of students.  In our case, we applied data mining to assess students inquiry   skills within physical science simulations. Though we have   increased evidence of models generalizability, we note that the   look-and-feel and task structure of the physical science activities   were generally similar. For other science domains like biology,  the nature of the experimentation process may differ; further   research is needed to determine if our models will generalize to   entirely new types of tasks and science domains  (cf. [8]).    Second, we showed how different kinds of stratification in such a  test set can reveal limitations on the performance of data mined   models. In particular, the ways in which a model will be used   should be considered when considering generalizability . In our   work, we aim for our models to be reusable to assess all students,   trigger scaffolding [8], and work regardless of how much data the  student collected [7]. Thus it was essential for us to consider   performance in the new simulation at the run-level since this is the   granularity at which we aim to assess student work and provide   scaffolding. Stratifying on other variables such as the total number   of student actions or the specific inquiry activity in question [cf. 1,   3] may reveal other differences in performance. Considering these   additional points may provide more evidence to the reusability of   data-mined models in different contexts or reveal limitations in  the models that can be addressed to improve performance in   specific cases.   7. ACKNOWLEDGEMENTS  This research is funded by the National Science Foundation (NSF-  DRL#0733286, NSF-DRL#1008649, and NSF-DGE#0742503)   and the U.S. Department of Education (R305A090170 and   R305A120778). Any opinions expressed are those of the authors   and do not necessarily reflect those of the funding agencies.   8. REFERENCES  [1] Baker, R.S.J.d, Corbett, A.T., Roll, I., and Koedinger, K.R.   Developing a Generalizable Detector of When Students  Game the System. User Modeling and User-Adapted   Interaction, 18, 3 (2008), 287-314.   [2] Blikstein, P. Using Learning Analytics to Assess Students'   Behavior in Open-Ended Programming Tasks. In   Proceedings of the 1st International Conference on Learning   Analytics and Knowledge (Banff, AB, CA 2011), 110-116.   [3] Ghazarian, A. and Noorhosseini, S. M. Automatic Detection   of Users' Skill Levels Using High-Frequency User Interface   Events. User Modeling and User-Adapted Interaction, 20, 2   (2010), 109-146.   [4] Sao Pedro, M.A., Baker, R.S.J.d., Gobert, J.D., Montalvo,   O., and Nakama, A. Leveraging Machine-Learned Detectors   of Systematic Inquiry Behavior to Estimate and Predict  Transfer of Inquiry Skill. User Modeling and User-Adapted   Interaction (2011), 1-39.   [5] Siemens, G. Learning Analytics: Envisioning a Research   Discipline and a Domain of Practice. In Proceedings of the   2nd International Conference on Learning Analytics and   Knowledge (Banff, AB, CA 2012), ACM, 4-8.   [6] Pardos, Z., Baker, R., Gowda, S., and Heffernan, N. The   Sum is Greater than the Parts: Ensembling Models of Student  Knowledge in Educational Software. SIGKDD Explorations,   13, 2 (2011), 37-44.   [7] Sao Pedro, M., Baker, R., and Gobert, J. Improving   Construct Validity Yields Better Models of Systematic   Inquiry, Even with Less Information. In Proceedings of the   20th Conference on User Modeling, Adaptation, and   Personalization (Montreal, QC, Canada 2012), 249-260.   [8] Gobert, J., Sao Pedro, M., Baker, R., Toto, E., and Montalvo,  O. Leveraging educational data mining for real time   performance assessment of scientific inquiry skills within   microworlds. Journal of Educational Data Mining, 4, 1   (2012), 111-143.   [9] Baker, R.S.J.d., Corbett, A.T., and Wagner, A.Z. Human   Classification of Low-Fidelity Replays of Student Actions. In   Proceedings of the Educational Data Mining Workshop held  at the 8th International Conference on Intelligent Tutoring   Systems, ITS 2006 (Jhongli, Taiwan 2006), 29-36.   [10] Hanley, J.A. and McNeil, B.J. The Meaning and Use of the   Area under a Receiver Operating Characteristic (ROC)   Curve. Radiology, 143 (1982), 29-36.      Pred N Pred N  Pred Y Pred Y  Runs A' K Pc Rc Runs A' K Pc Rc  2 1.00 .08 1.00 .18 2 .84 .71 .89 .94  3 1.00 1.00 1.00 1.00 3 .88 .40 .91 .98  4 $ .00 1.00 .98 4 .84 .00 .90 1.00  5 .66 .14 1.00 .66 5 .70 .51 .89 .98  >5 .61 .17 .97 .76 >5 .79 .78 .98 .98  * Pc = precision; Rc = recall  $ = A' could not be computed because only one class label was present  19  True Y  5  199  Testing Stated Hypotheses  True N True Y  16  1  60  168  Pc = .99, Rc = .74 Pc = .91, Rc = .98  A' = .78, K = .59  Designing Controlled   Experiments  A' = .84, K = .26  True N  22  194      "}
{"index":{"_id":"28"}}
{"datatype":"inproceedings","key":"Dimopoulos:2013:ASP:2460296.2460335","author":"Dimopoulos, Ioannis and Petropoulou, Ourania and Retalis, Symeon","title":"Assessing Students' Performance Using the Learning Analytics Enriched Rubrics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"195--199","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460335","doi":"10.1145/2460296.2460335","acmid":"2460335","publisher":"ACM","address":"New York, NY, USA","keywords":"enriched assessment rubrics, interaction analysis indicators, learning analytics, students' assessment performance","abstract":"The assessment of students' performance in e-learning environments is a challenging and demanding task for the teachers. Focusing on this challenge, a new assessment tool, called Learning Analytics Enriched Rubric (LAe-R) is presented in this paper. LAe-R is based on the concept of assessment rubrics which is a very popular assessment technique in education. LAe-R contains enriched criteria and grading levels that are associated to data extracted from the analysis of learners' interaction and learning behavior in an e-learning environment. LAe-R has been developed as a plug-in for the Moodle learning management system. Via an example, we will show how LAe-R can be used by teachers and students.","pdf":"Assessing Students Performance Using the Learning   Analytics Enriched Rubrics   Ioannis Dimopoulos  University of Piraeus   Department of Digital Systems   80 Karaoli & Dimitriou, 185 34   Piraeus, Greece  +302104142746    johndimopoulos@sch.gr     Ourania Petropoulou  University of Piraeus   Department of Digital Systems   80 Karaoli & Dimitriou, 185 34   Piraeus, Greece  +302104142746   rpetro@biomed.ntua.gr    Symeon Retalis  University of Piraeus   Department of Digital Systems   80 Karaoli & Dimitriou, 185 34   Piraeus, Greece  +302104142746   retal@unipi.gr          ABSTRACT  The assessment of students performance in e-learning   environments is a challenging and demanding task for the   teachers. Focusing on this challenge, a new assessment tool,   called Learning Analytics Enriched Rubric (LAe-R) is presented   in this paper. LAe-R is based on the concept of assessment rubrics   which is a very popular assessment technique in education. LAe-R   contains enriched criteria and grading levels that are associated   to data extracted from the analysis of learners interaction and   learning behavior in an e-learning environment. LAe-R has been   developed as a plug-in for the Moodle learning management   system. Via an example, we will show how LAe-R can be used by   teachers and students.    Categories and Subject Descriptors  J.1 [Administrative Data Processing]: Education; K.3.1   [Computer Uses in Education]: Collaborative Learning;   Distance Learning;     General Terms  Measurement, Performance, Design   Keywords  Learning Analytics, Enriched Assessment Rubrics, Students   Assessment Performance, Interaction Analysis Indicators   1. INTRODUCTION  During the last decades, teachers use learning management   systems (LMSs) at all levels of education for enriching the   traditional learning paradigm with more flexible and multimedia   rich learning experiences. Students are often engaged into   orchestrated learning activities which require the finding of   appropriate solutions to problems via inquiry and   experimentation, brainstorming, collaboration (synchronous or   asynchronous), social networking as well as the use of various   types of resources for self-paced learning [1]. According to   modern learning practices, learning should not be considered as   an isolated self-paced process. It should contain of a wide range   of interactions between students, between students and teachers   and between students and learning resources, that are described in   learning scripts [2, 3, 4].    One of the major challenges in these technologically and   pedagogically enhanced learning environments is that teachers   face serious difficulties to thoroughly capture, track, and analyze   the massive amounts of data gathered during an interactive   learning process in order to assess students performance [5, 6, 7].    To support teachers  in this complex and demanding task, new   assessment methods and tools based on educational data mining   or learning analytics have been proposed [8, 9, 10]. Nevertheless,   most of the data mining tools are too complex for educators to   use and their features go well beyond the scope of what an   educator might require [11]. So, current research studies in the   field of students assessment in e-learning environments indicate   the need for new assessment techniques and tools, integrated to   LMSs, capable of effectively supporting the teachers to   holistically assess both products of learning and the very complex   process of learning [12, 13].    This paper proposes an innovative assessment tool, called   Learning Analytics Enriched Rubric (LAe-R), which has been   developed as a Moodle plug-in. LAe-R allows teachers to easily   create enriched rubrics containing criteria and related grading   levels that are associated to data extracted from the analysis of   learners interaction and learning behavior in a Moodle course,   (i.e. number of post messages, times of accessing learning   material, assignments grades, etc.). LAe-R, using learning   analytics related to collaborative interactions, past grading   performance and inquiries of learning resources in Moodle, can   automatically calculate the score of the various levels per   criterion. The total rubric score is calculated as a sum of the   scores per each criterion (also weighted sum if required by the   teacher).   The rest of the paper is structured as follows: In the next section,   we briefly present the educational concept of the enriched rubrics   and the need to integrate them with learning analytics method. In   section 3 the Learning Analytics Enriched Rubric (LAe-R) is   presented in detail. Finally the conclusions and further research   are outlined.   (c) 2013 Association for Computing Machinery. ACM acknowledges that   this contribution was authored or co-authored by an employee, contractor   or affiliate of the national government of Greece. As such, the government   of Greece retains a nonexclusive, royalty-free right to publish or reproduce   this article, or to allow others to do so, for Government purposes only.   LAK '13, April 08 - 12 2013, Leuven, Belgium    Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00   195  mailto:johndimopoulos@sch.gr mailto:rpetro@biomed.ntua.gr mailto:retal@unipi.gr   2. ADDING LEARNING ANALYTICS   INDICATORS TO TRADITIONAL   ASSESSMENT RUBRICS     Assessment is a very important part of the learning process.   Teacher in a traditional classroom use paper-pencil assessment   which is method of grading students subject knowledge.   Moreover, thanks to their face-to-face interaction with the   students, they can have a good understanding of individual   students skills (e.g. real life problem solving skills, ability to   integrate knowledge across disciplines, communication and   collaboration skills) and talents [14]. Teachers often prefer to   make use of the assessment rubrics in order to reflect on and   evaluate student output, performance, and products. Rubrics are   scoring guides that formalize the evaluation process and provide   fair and clear results to students [15].    However, in eLearning environments where face-to-face is   lacking, there is a well justified need to equip teachers with new   techniques and tools for assessing students performance that will   not only take into consideration the students performance in   online tests and assignments but also several learning interaction   indicators [16]. Several sets of such learning interaction indicators   have been proposed to be considered as well, which are related to   students studying habits and active participation in group   discussions and group deliverables, e.g. total number of activities-  messages per student/group, proportion of writing-reading of   messages per student/group, density of social network, rate of   learning resources that are read per student/group, and so on [17,   18, 19, 11].    This is why a new assessment technique, called  Enriched   Rubrics, has been proposed.  Like a classic rubric, it consists   of a set of criteria and for each criterion, several descriptive levels   are provided. An enriched rubric contains some criteria and   related grading levels that are associated to data from the analysis   of learners interaction and learning behavior in an online course,   such as number of post messages, times of accessing learning   material, assignments grades and so on [20, 21].    However, for applying this technique, teachers need to use   specialized educational data retrieving tools that automatically   collect, extract and filter all the necessary data about students   learning interactions, which can be retrieved from the logfiles of   an e-learning platform. Such tools are GISMO [22], iPET [23],   Moodog [24], MOCLog [25] and others [26]. They help teachers   analyze data according to a set of indicators (e.g. number of   activities-messages per user/team, number of visited resources per   student, etc.) and produce reports in the form of simple statistical   tables, or visualizations such as bar charts, pie charts, etc.   According to these reports, a teacher can add scores to the various   rubric criteria.    Needless to say, that when teachers have to use several tools,   which had not been developed for their exact needs, they quickly   abandon the application of a more sophisticated assessment   process. So there is an imperative need for developing new   assessment tools capable of providing the teachers and the   students, the ability to effectively use the enriched rubrics   smoothly and quickly throughout the educational process via a   unified environment [27, 28].    Consequently, we developed an innovative Moodle plug-in, called   the Learning Analytics Enriched Rubric (LAe-R), which comes to   help teachers in applying the enriched rubrics assessment   technique. The tool allows teachers to easily create an enriched   rubric containing criteria and related grading levels that are   associated with indicators about learners interaction and learning   behavior in a Moodle course, (i.e. number of post messages, times   of accessing learning material, assignments grades, etc.). LAe-R,   is more analytically presented in the following sections.   3. THE LAe-R MOODLE PLUG-IN   3.1 LAe-R functionality   The LAe-R plug-in was created in order to be included in Moodle   (version 2.2+) as an advanced grading method. It is an enhanced   version of the existing rubric plug-in. A teacher can create an   enriched rubric (ER) from scratch, or create a new one from a   template or edit an existing one. When creating an ER, the teacher   adds the assessment criteria with their corresponding descriptions   and the grading levels (see Figure 1). At its current version, LAe-  R allows a teacher to add types of criteria that are associated to   learning analysis indicators: collaboration, grades to assignments,   study of learning resources.   In the case of collaboration, the teacher can use various log   file data for assessing students performance such as forum posts   (new or reply messages), chat messages, number of files attached   to forum post messages. For measuring students study behavior,   the number of students views upon selected learning recourses   can be considered. For taken into account students performance   in various assignments, the grading scores on selected   assignments can be used.  LAe-R also allows teachers not only to   calculate each students performance but also to measure a   students performance at each criterion in comparison to the   average score of the performance of all students (assessment   according to class scope).         Figure 1. Screenshot of how a teacher can specify an   assessment criterion in LAe-R   3.2 Assessing students performance with   LAe-R     LAe-R can automatically give a score at each criterion by   collecting the related data from the Moodle log files or data   entries from the selected course modules. The way that is done is   simple (see Figure 2). For each enriched criterion, a value   (benchmark) is calculated from the corresponding data. Then, the   appropriate rubric level gets picked. For example, if a student has   posted 4 reply messages to a forum (4 is the value/benchmark)   he/she will be at level 2 of that criterion which has been described   as the collaboration level where students post between 3 and 6   reply messages. As already mentioned, the score that a student   gets at a criterion might have to do with his/her performance in   comparison to other fellow students. This is called global scope   196    evaluation and uses percentages for all logical comparisons. For   example, a teacher might want to check if the number of one   students file submissions in specific forums is more than 50% of   students average, or students grade in a particular assignment is   30% over the fellow students average grade.      Figure 2. LAe-R automatic evaluation work flow      Of course, the teacher can review the assessment results and make   some extra (optional) comments before publishing the students   rubric. As shown in Figure 3, an enriched rubric presents the   assessment results in a very clear way, i.e. with explanations per   criterion.    3.3 Data retrieval to produce Learning   Analytics in LAe-R     At the current version of LAe-R, collection and process of data is   performed in various Moodle database tables according to   enrichment criteria in order to produce learning analytic (see   Table 1).   Table 1. Basic database querying characteristics   Enrichment   type   Collaboration   type   Database   Table   Condition /   Calculation   Collaboration  Simple   occurrences  log   action =    add post,   action =   talked   Collaboration  File   submissions   forum_   posts  attachment(s)   Collaboration Forum replies  forum_   posts  forum_id(s)   Collaboration  People   interacted   forum_   posts /  userid(s)   chat   messages   Study -  log  action =   view   Grade -   grade_   grades  finalgrade(s)       To produce learning analytics some specific principles have been   adopted:    Time stamps are embedded in database queries   providing that the corresponding options are enabled for   the LAe-R and respective dates are set in the particular   assignment.    For average scores estimation, the arithmetic mean of   values is calculated.    When calculating the students level of performance in   assignments/tests, all grades from the assignment course   modules are converted to 100 point scale for preserving   the consistency.             Figure 3. Evaluation results      4. EVALUATION & CONCLUDING   REMARKS  Before publishing LAe-R on the Moodle developers community,   extended tests were made in order to ensure that there are not   functionality problems. Also we performed an initial evaluation   study which concerned:   1. Code compatibility and compliance according to   Moodle coding styles and standards   2. Usability testing by experienced teachers   Regarding the coding style used, LAe-R was written according to   all coding guidelines specified by Moodle standards. All defined   classes, procedures and functions included in the code, are   197    compatible with the Moodle platform architecture. All the   significant changes which had been made from Moodle version   2.2 to 2.3 affecting crucial parts of the plugins functionality were   taken into account and minor adjustments were made so that LAe-  R is fully compatible to both versions. The plugin was   successfully approved by Moodle evaluators and was published.   Also, twelve (12) experienced primary and secondary education   teachers, who are now attending an MSc program in e-learning   and have excellent knowledge in Moodle, participated to a lab-  based usability inspection study. The goal of this project was to   evaluate the usability and acceptance of LAe-R by common users.   They were all given detail instructions for creating a course with   their own subject in Moodle, using an Inquiry Based Learning   technique, associating rubrics with LAe-R for evaluating students   and filling up a questionnaire at the end of the course regarding   the use of LAe-R.   Qualitative and quantitative data gathered from this project   consisted of a) the quality, competence and efficiency of the   learning scenarios created and used by the group of teachers, b)   the completeness and clarity of the created enriched rubrics that   derived from the LAe-R instances and c) the filled questionnaires   from all participating teachers. By processing these data, we come   to the following results.   The learning scenarios were well formed and proper for   implementing Inquiry Based Learning in an e-learning   environment according to educational standards. The majority of   enriched rubrics (11 out of 12) contained sufficient criteria to   evaluate the learning outcomes as well as the educational   interactions during the learning process. Regarding LAe-R, 91,7%   of the teachers stated that they felt comfortable using this tool   witch was easy for them to work with and 75% were satisfied by   its interface. According to LAe-Rs use from student evaluation,   91,7% declared that they found LAe-R very useful, 83,3% quick   to process and 75% stated that performing student evaluation   seemed effortless. Very good reviews were also noted for LAe-Rs   online instruction guide with detailed videos that helped a lot   during rubric creation and student evaluation, giving more   confidence during implementation.   Making use of the teachers feedback comments as well as the   notes of the research team, various enhancements of the tools can   be made, such as:     Add more criteria in an enriched rubric thus allowing a  teacher to assess more issues. An option is to embed   into LAe-R one of the existing typology of learning   interaction indicators and monitor teachers usage.      In order to calculate the value of a criterion, LAe-R  could use not only first level descriptive statistics by   inquiring data from the log files, but also more   sophisticated indicators like the centrality in a social   network analysis.    Make use of indicators that are not evaluation outcomes  to various formats.    Use more sophisticated and appealing ways to visualize  Learning Analytics indicators and distinct these   visualizations for students and teachers accordingly.    LAe-R seems a stable and very promising assessment tool that   could fill-in the gap in assessing students performance in   elearning environments using the wealth of interaction data.      5. ACKNOWLEDGEMENTS  This work has been partially supported by the SAILS project that   has received funding from the European Unions Seventh   Framework Programme (http://www.sails-project.eu/). Source of   funding: EU. FP7 Capacities Programme Science in Society. FP7   Grant Agreement N 289085 SAILS (CSA-SA_FP7-SCIENCE-  IN-SOCIETY-2011-1).      6. REFERENCES  [1] Lazakidou, G., & Retalis, S.  (2010). Using computer   supported collaborative learning strategies for helping   students acquire self-regulated problem-solving skills in   mathematics. Computers & Education 54(1), 3-13.    [2] Moore, G. (1989). Three types of interaction. The American   Journal of Distance Education, 3(2), 1-6.   [3] Dillenbourg, P., Jrvel, S., & Fischer, F. (2009). The   evolution of research on computer-supported collaborative   learning. Technology-Enhanced Learning, 3-19. Springer.   [4] Bliuc, A.-M., Ellis, R.A., & Goodyear, P. (2011). The role of   social identification as university student in learning:   relationships between students social identity, approaches to   learning, and academic achievement. Educational   Psychology, 31(5), 559-574.   [5] Quellmalz, E., & Kozma, R. (2003). Designing assessments   of learning with technology. Assessment in Education, 10(3),   389-406.   [6] Petropoulou, O., Lazakidou, G., Retalis, S., & Vrasidas, C.   (2007). Analysing interaction behaviour in network   supported collaborative learning environments: a holistic   approach. International Journal of Knowledge and Learning,   3(4/5), 450 - 464.   [7] Johnson, R., Penny, J., & Gordon, B. (2009). Assessing   performance: designing, scoring, and validating performance   tasks. Guilford Press.   [8] Mazza, R., & Dimitrova, V. (2007). CourseVis: A Graphical   Student Monitoring Tool for Facilitating Instructors in Web-  Based Distance Courses. International Journal in Human-  Computer Studies, 65(2), 125-139.   [9] Yang, D., Richardson, J., French, B., & Lehman, J. (2011).   The development of a content analysis model for assessing   students cognitive learning in asynchronous online   discussions. Educational Technology Research &   Development, 59(1), 43-70.   [10] Garca-Saiz, D., & Zorilla Pantalen, M.E. (2011). E-  learning web miner: A data mining application to help   instructors Involved in virtual courses. In M. Pechenizkiy et   al. (Eds.), Proceedings of the 3rd Conference on Educational   Data Mining 2011 (323-324). Eindhoven, The Netherlands.   [11] Romero, C. (2010). Educational Data Mining: A Review of   the State of the Art.  IEEE Transactions on Systems. Man,   and Cybernetics, 40(6), 601-618.   198  http://www.sails-project.eu/   [12] Strijbos, J. W. (2011). Assessment of (computer-supported)   collaborative learning. IEEE Transactions on Learning   Technologies, 4(1), 59-73.   [13] Dyckhoff, A. L., Zielke, D., Bltmann, M., Chatti, M. A., &   Schroeder, U. (2012). Design and Implementation of a   Learning Analytics Toolkit for Teachers. Educational   Technology & Society, 15(3), 5876.   [14] Carr, J. F., &. Harris, D. E. (2001). Succeeding with   standards linking curriculum, assessment, and action   planning. Alexandria, Virginia: Association for Supervision   and Curriculum Development.   [15] Oberg, C. (2009). Guiding classroom instruction through   performance assessment. Online Journal of Case Studies in   Accreditation and Assessment, 1, 111, ISSN: 19413386.   Retrieved from:   http://www.aabri.com/manuscripts/09257.pdf   [16] Daradoumis T., Martnez-Mons A., & Xhafa F. (2006). A   layered framework for evaluatingon-line collaborative   learning interactions. International Journal of Human-  Computer Studies, 64(7), 622-635.   [17] Barros, B., & Verdejo, M. F. (1999). An approach to analyse   collaboration when shared structured workspaces are used   for carrying out group learning processes. In S.P. Lajoie &   M. Vivet (Eds.), Artificial intelligence in education: Open   learning environments, 449-456. Amsterdam: IOS Press.   [18] Martnez, A., Dimitriadis, Y., De La Fuente, P. (2003).   Contributions to analysis of interactions for formative   evaluation in CSCL. In: Llamas M, Fernandez M J, Anido L   E (eds.) Computers and education: towards of lifelong   learning society, 227-238. Kluwer, The Netherlands.   [19] Ho, C., & Swan, K. (2007). Evaluating online conversation   in an asynchronous learning environment: An application of   Grices cooperative principle. Internet and Higher   Education, 10, 3-14.   [20] Petropoulou, O.,Vasilikopoulou, M., & Retalis, S. (2009).   Enriched Assessment Rubrics: A new medium for enabling   teachers easily assess students performance when   participating to complex interactive learning scenarios.   Operational Research International Journal, 11(2), 171-186.   [21] Petropoulou, O., Retalis, S., & Lazakidou, G. (2012).   Measuring Students Performance in e-Learning   Environments via Enriched Assessment Rubrics. In   Psaromiligkos, Spyridakos, Retalis(eds): Evaluation in e-  Learning. Nova Science Publishers, ISBN: 978-1-61942-  942-0.   [22] Mazza, R., & Botturi, L. (2007) Monitoring an Online   Course with the GISMO Tool: A Case Study. Journal of   Interactive Learning Research, 18(2), 251-265. Chesapeake,   VA: AACE.   [23] Saltz J., Hiltz S., Turoff M., & Passerini K. (2007).   Increasing participation in distance learning courses. IEEE   Internet Computing, 11(3), 36-44.   [24] Zhang, H., Almeroth, K., Knight, A., Bulger, M., & Mayer,   R. (2007). Moodog: Tracking students' Online Learning   Activities. In C. Montgomerie & J. Seale (Eds.), Proceedings   of World Conference on Educational Multimedia,   Hypermedia and Telecommunications, 4415-4422.   Chesapeake, VA: AACE.   [25] Mazza R., Bettoni M., Fare M., Mazzola L. (2012).   MOCLog  Monitoring Online Courses with log data. In   Retalis, S., and Dougiamas, M. (2012). 1st Moodle Research   Conference Proceedings. Online:   http://research.moodle.net/MoodleCon_Proceedings_progra  m   [26] Sharkey, M. (2011) Academic analytics landscape at the   University of Phoenix. In Proceedings of the 1st   International Conference on Learning Analytics and   Knowledge (LAK '11). ACM, New York, NY, USA, 122-  126.   [27] Petropoulou, O., Altanis, I., Retalis, S., Nicolaou, C. A.,   Kannas, C., Vasiliadou, M., & Pattis, I. (2010). Building a   tool to help teachers analyse learners' interactions in a   networked learning environment. Educational Media   International, 47(3), 231- 246.   [28] Gobert, J., Sao Pedro, M., Baker, R., Toto, E., & Montalvo,   O.  (2012). Leveraging Educational Data Mining for Real-  time Performance Assessment of Scientific Inquiry Skills   within Microworlds. Journal of Educational Data Mining,   Article 5, 4 (1), 153-185.                           199  http://www.aabri.com/manuscripts/09257.pdf http://research.moodle.net/MoodleCon_Proceedings_program http://research.moodle.net/MoodleCon_Proceedings_program     "}
{"index":{"_id":"29"}}
{"datatype":"inproceedings","key":"Segedy:2013:MAL:2460296.2460336","author":"Segedy, James R. and Loretz, Kirk M. and Biswas, Gautam","title":"Model-driven Assessment of Learners in Open-ended Learning Environments","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"200--204","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460336","doi":"10.1145/2460296.2460336","acmid":"2460336","publisher":"ACM","address":"New York, NY, USA","keywords":"adaptive scaffolding, model-driven assessment, open-ended learning environment, performance metrics","abstract":"Open-ended learning environments (OELEs) provide students with opportunities to take part in authentic and complex problem-solving tasks. However, many students struggle to succeed in such complex learning endeavors. Without support, these students often use system tools incorrectly and adopt suboptimal learning strategies. However, providing adaptive support to students in OELEs poses significant challenges, and relatively few OELEs provide students with adaptive support. This paper presents the initial development of a systematic approach for interpreting and evaluating learner behaviors in OELEs called model-driven assessments, which uses a model of the cognitive and metacognitive processes important for completing the open-ended learning task. The model provides a means for both classifying and assessing students' learning behaviors while using the system. An evaluation of the analysis technique is presented in the context of Betty's Brain, an OELE designed to help middle school students learn about science.","pdf":"Model-Driven Assessment of Learners in Open-Ended Learning Environments  James R. Segedy Department of EECS/ISIS  Vanderbilt University Nashville, TN 37203, U.S.A.  james.r.segedy @vanderbilt.edu  Kirk M. Loretz Department of EECS/ISIS  Vanderbilt University Nashville, TN 37203, U.S.A.  kirk.m.loretz @vanderbilt.edu  Gautam Biswas Department of EECS/ISIS  Vanderbilt University Nashville, TN 37203, U.S.A.  gautam.biswas @vanderbilt.edu  ABSTRACT Open-ended learning environments (OELEs) provide stu- dents with opportunities to take part in authentic and com- plex problem-solving tasks. However, many students strug- gle to succeed in such complex learning endeavors. Without support, these students often use system tools incorrectly and adopt suboptimal learning strategies. However, provid- ing adaptive support to students in OELEs poses significant challenges, and relatively few OELEs provide students with adaptive support. This paper presents the initial develop- ment of a systematic approach for interpreting and evaluat- ing learner behaviors in OELEs called model-driven assess- ments, which uses a model of the cognitive and metacog- nitive processes important for completing the open-ended learning task. The model provides a means for both classi- fying and assessing students learning behaviors while using the system. An evaluation of the analysis technique is pre- sented in the context of Bettys Brain, an OELE designed to help middle school students learn about science.  Keywords open-ended learning environment, model-driven assessment, adaptive scaffolding, performance metrics  1. INTRODUCTION Advances in computer technology have provided learning  science and learning technology researchers the affordances for designing computer-based learning environments that provide students with opportunities to take part in authen- tic, complex problem solving tasks. These environments, generically called open-ended learning environments (OE- LEs) [5, 9], provide students with a learning context and a set of tools for pursuing learning tasks. Examples of such en- vironments include hypermedia learning environments (e.g., [3]), modeling and simulation environments (e.g., [4]), and educational games featuring open worlds (e.g., [12]). While  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 13, April 08 - 12 2013, Leuven, Belgium Copyright 2013 ACM 978-1-4503-1785-6/13/04 ...$15.00.  OELEs may vary in the particular set of tools they pro- vide, they almost always include tools for: (i) seeking out and acquiring information, (ii) applying information to a problem-solving context, and (iii) evaluating the quality of the constructed solution [8, 13]. During evaluation, learn- ers have opportunities to reflect on their understanding of the information, their approach to generating solutions, and possible next steps for improving their solutions.  OELEs place high cognitive demands on learners [9]. To be successful, learners must understand how to execute: (i) cognitive processes for accessing and interpreting informa- tion, constructing problem solutions, and evaluating con- structed solutions; and (ii) metacognitive processes for co- ordinating the use of various cognitive processes and reflect- ing on the outcome of solution evaluations. This presents significant challenges to novice learners; they may have nei- ther the proficiency for using the systems tools nor the ex- perience and understanding necessary for regulating their learning behaviors. Not surprisingly, research has shown that novices often struggle to succeed in OELEs (e.g., [9, 11]). Without support, these learners typically use tools in- correctly and adopt suboptimal learning strategies [1, 14]. However, providing adaptive support to students in OELEs poses significant challenges (e.g., see [2]); it requires sys- tematic analysis techniques for diagnosing learners needs as they relate to one or more cognitive and metacognitive pro- cesses. In OELEs, such diagnoses involve assessing learners cognitive skill proficiency, interpreting their action sequences in terms of learning strategies, and evaluating their success in accomplishing their current tasks. The open-ended na- ture of OELEs combined with the longer term nature of the problems presented in such environments further exacer- bates the problem; successfully solving a problem presented in an OELE often requires many more steps than does a problem in a typical ITS, and OELEs allow students to make progress along several paths, each linked to a different ap- proach or different order for solving the components of the problem. Thus, interpreting and assessing students learning behavior is inherently complex; at any point in time, there may be a dozen or more correct next steps from which students may choose. This makes techniques like model- tracing, often employed in intelligent tutoring systems [7] difficult to implement, as the space of possible learning paths in an OELE quickly becomes intractable.  While several OELEs have been developed, relatively few perform systematic interpretations of how learners approach the learning task in order to provide adaptive support. In-  200    stead, these systems include non-adaptive scaffolded tools (e.g., lists of sub-goals or guiding questions) designed to provide support for learners who choose to use them. The main contribution of the present work is establishing a sys- tematic and theoretically-grounded approach for interpret- ing learner behaviors in OELEs called model-driven assess- ments (MDAs). MDAs interpret and assess students ac- tions using a model of relevant cognitive and metacognitive processes important for completing the learning task in an effective manner. We present our high-level interpretation framework and then use apply it to assess students cogni- tive skill levels in the context of Bettys Brain [10, 15], an OELE designed to help middle school students learn about science.  The remainder of this paper presents the Bettys Brain learning environment in more detail, including a description of the learning task and the cognitive and metacognitive model that drives the assessments. We then use the cog- nitive model to define a set of MDA metrics and use those metrics to perform post-hoc analysis with data from a recent study conducted with Bettys Brain.  2. OVERVIEW OF BETTYS BRAIN The Bettys Brain learning environment presents students  with the task of teaching a virtual agent, named Betty, about science topics by constructing a causal map that represents relevant science phenomena as a set of entities connected by directed links which represent causal relations. Once taught, Betty can use the map to answer causal questions and ex- plain those answers by reasoning through chains of links [10]. The goal for students is to teach Betty a causal map that matches a hidden, expert model of the domain.  As an OELE, Bettys Brain includes tools for acquiring information about a particular science topic, applying that information to a problem-solving task, and evaluating the quality of the constructed solution. Students can acquire domain knowledge by reading a set of hypertext resources, which include both high-level descriptions of scientific pro- cesses (e.g., shivering) and information pertaining to each concept that appears in the expert map (e.g., friction). As students read, they need to identify causal relationships, such as skeletal muscle contractions create friction in the body. Students can then apply the learned information by adding the concepts and causal links to their causal map (which teaches the information to Betty).  Students can evaluate their causal map by asking Betty to answer questions (e.g., if the hypothalamus response in- creases, what effect does it have on heat loss from the body ), take quizzes (which are sets of questions), and explain her answers. To answer questions, Betty uses qualitative rea- soning methods that operate through chains of links from the source concept to the target concept [10]. When Betty explains her answers, she illustrates her reasoning through text and animation; she simultaneously explains her think- ing (e.g., an increase in the hypothalamus response causes skin contraction to increase. This leads to...) and animates her explanation by highlighting concepts and links on the map as she mentions them.  After Betty answers a question, learners can ask Mr. Davis, another pedagogical agent in the system that serves as a mentor, to evaluate her answer. If Bettys answer and ex- planation match the expert model (i.e., both maps use the same set of causal links to answer the question), then Bettys  answer is correct. Learners can also have Betty take a quiz. Quiz questions are selected dynamically such that a set of questions is chosen (in proportion to the completeness of the map) for which Betty will generate correct answers. The rest of the quiz answers are incorrect, and they are chosen to direct the students attention to parts of the map with missing or incorrect links. When Betty is unable to answer a question correctly, the students can use that information to discover Bettys misunderstandings and correct them by identifying and modifying erroneous links in the causal map. Similarly, when Betty answers a question correctly, students know that each link that Betty uses to explain her answer to that question is also correct. When Betty answers all quiz questions correctly, students know that they have completed their task. To help students in keeping track of which links are correct and which are not, the system allows students annotate links as being correct.  3. MODEL-DRIVEN ASSESSMENTS As mentioned previously, MDAs are derived from a model  of the cognitive and metacognitive processes important for completing the learning task. Figure 1 presents our model for learning with OELEs. The cognitive portion of the model defines three classes of cognitive processes related to the tools often included in OELEs: information seeking/acquisition, solution construction, and solution assessment. The stu- dents tasks are facilitated by the system providing a set of system within Bettys Brain. These lists of associated sys- tem tools are the only portion of the model specific to the particular OELE under study. The metacognitive portion of the model, dealing primarily with metacognitive regula- tion, is divided into five key metacognitive processes: goal selection, planning, monitoring, control, and reflection.  Our approach leverages the cognitive and metacognitive model in interpreting students actions and behavior pat- terns (i.e., sequences of actions) in terms of the cognitive and metacognitive processes defined by the model (e.g., [14]). This paper focuses on using the cognitive portion of the model to interpret and assess how students use the system tools for solution construction and solution evaluation. As seen in the model, several tools in Bettys Brain correspond to at least one of the three types of cognitive processes. Thus, when a student uses one of these tools, the system can classify it according to the model. Once classified, the system can assess these actions according to their effective- ness. Actions in an OELE are considered effective if they move the learner closer to the task goal (teaching Betty the expert map). Effective solution construction behaviors im- prove the overall quality of Bettys causal model, and ef- fective solution evaluation behaviors produce and record in- formation about the correctness and completion of Bettys current causal model.  Together, measures of student actions along with mea- sures of the effectiveness of those actions provide the system with a set of measures for deciding whether or not to scaf- fold learners as they work: if a learner repeatedly utilizes tools ineffectively, then the system can offer feedback and scaffolding to support the learner in improving their under- standing of the cognitive skills involved in using those tools more effectively in the future.  201    Figure 1: Cognitive/Metacognitive Model of the Bettys Brain Learning Task  4. POST-HOC ANALYSIS USING MDA To illustrate the model-driven assessment approach, this  section presents a post-hoc analysis of data from a recent study conducted in a middle-school classroom using Bettys Brain. Fifty eighth-grade students from three middle Ten- nessee science classrooms, taught by the same teacher, par- ticipated in the study. Because use of Bettys Brain relies on students ability to independently read and understand the resources, the system is not suited to students with lim- ited English proficiency or cognitive-behavioral problems. Therefore, data from ESL and special education students were not analyzed. Similarly, we excluded the data of stu- dents who missed more than two class periods of work on the system. The final sample was forty students.  Students used the Bettys Brain system over five 45-minute class periods to learn about mammalian thermoregulation in cold temperatures. The expert map contained 13 concepts and 15 links representing cold detection and three bodily responses to cold: skin contraction, vasoconstriction, and shivering. The resources were 15 pages (1,981 words) with a Flesch-Kincaid reading grade level of 8.9. Further details of the study, including pre-post test results, have been reported in [6].  The data analysis focused on applying the MDA method- ology to assess students use of cognitive processes while working with Bettys Brain. In particular, this analysis fo- cused on students use of solution construction and solution evaluation tools. Solution evaluation tools were further di- vided into tools for model assessment (i.e., quizzes, ques- tion evaluations and explanations) and tools for recording progress (i.e., link annotations).  For each class of cognitive processes, two metrics were calculated for each student: (1) skill use, and (2) effective-  ness. Skill use is simply the average number of times one of the cognitive tools related to that process was used per minute. For solution construction, for example, a students skill use metric refers to the average number of causal map edits that student performed per minute on the system. For model assessment, the skill use metric refers to the average number of question evaluations, quizzes, and explanations performed per minute of using the system. For progress recording, the average number of causal link annotations created per minute serves as the measure of skill use.  Effectiveness was calculated as the percentage of actions that moved students closer to their task goal. For solution construction, effectiveness refers to the percentage of causal link additions, removals, and modifications (with respect to the total number of such edits) that improved the quality of Bettys causal map, where causal map quality is measured as the number of correct links minus the number of incorrect links in the map. For model assessment, effectiveness refers to the percentage of assessment actions that generated spe- cific information about the correctness of one or more causal links. For progress recording, effectiveness refers to the per- centage of link annotations created that correctly describe the annotated causal link.  In addition to assessing the group of students as a whole, an additional set of analyses were employed to compare stu- dents who were more and less successful in teaching Betty the correct causal model. Students were divided into three groups based on the quality of their causal maps at the end of the study. Students in the Low group taught Betty a map that achieved a score of 5 or below. Students in the Medium group taught Betty a map with a score of 6 to 10, and students in the High group taught Betty a map with a score of 11 to 15 (where 15 is the maximum possible score).  202    Table 1: Means (and Standard Deviations) of MDA Metrics for All Students  Actions/Min Effectiveness Solution Construction 0.439 (0.190) 0.525 (0.113)  Model Assessment 0.194 (0.1236) 0.370 (0.218) Progress Recording 0.012 (0.033) 0.161 (0.347)  Table 2: Means (and Standard Deviations) of As- sessment Actions  Actions/Min Effectiveness Question Eval 0.013 (0.019) 0.122 (0.280)  Explanation 0.056 (0.057) 0.023 (0.258) Quiz 0.147 (0.111) 0.391 (0.427)  These resulting Low, Medium, and High groups contained 18, 6, and 16 students, respectively. The behavior use and effectiveness metrics were computed for the Low and High groups in order to draw comparisons between those who were successful and those who were not.  5. RESULTS Results of the group level assessments are shown in Table  1. Students using the system regularly engaged in solution construction and model assessment activities. On average, students edited their causal maps once every 2.28 minutes and assessed their map once every 5.15 minutes. However, students rarely made explicit records of the results of their assessment activities; they performed progress recording ac- tions an average of once every 83.33 minutes.  Despite regularly engaging in solution construction and model assessment activities, the students in this study were not particularly effective in their endeavors. For solution construction, an average of just over half of their causal map edits improved the quality of their model. This suggests that students may have struggled to understand the causal relations described in the resources; alternatively, they may have edited their causal maps without first consulting the resources.  Students were even less effective in assessing Bettys un- derstanding of the science domain. On average, just over one third of their model assessment actions provided infor- mation about the correctness of one or more causal links. Moreover, these actions were largely limited to quizzes (see Table 2). This is striking, as quizzes can rarely provide cor- rectness information without being combined with Bettys explanations, which connect graded quiz answers to the sets of causal links that were used to generate those answers. It is important to note, however, that in some cases, students may have been able to infer which causal links generated an answer without requiring an explanation from Betty.  Together, these results suggest that students in this study struggled to use Bettys Brain in an effective manner. More often than not, their solution construction, model assess- ment, and progress recording activities did not help them move toward their task goal. These students may have ben- efited from more explicit feedback and scaffolding aimed at supporting their use of the cognitive and metacognitive pro- cesses important for success in Bettys Brain. However, despite these limitations, several students were successful (or close to successful) in teaching Betty the correct causal  Figure 2: MDA Metrics for High and Low Groups  model. To explore the differences between students who were successful and those who were not, the MDA metrics were calculated for the High and Low student groups iden- tified earlier. Because students rarely engaged in progress recording activities, the analysis focused only on solution construction and model assessment. These results appear in Figure 2.  The results of the comparative analysis show that the High group students more often used tools linked to solu- tion construction. Additionally, the High groups solution construction actions were more likely to be effective. Inde- pendent samples t-Tests conducted on these data showed a significant effect for actions per minute, t(32) = 2.67, p = .012, and effectiveness, t(32) = 5.99, p < .001. Figure 2 also shows that the High group performed slightly more model assessment actions/min than did the Low group; however, the Low groups model assessment actions were more likely to produce information about the correctness of one or more links. Independent samples t-Tests conducted on these data failed to reveal a significant effect for actions per minute, t(32) = 1.48, p = n.s., or effectiveness, t(32) = 1.77, p = n.s.  This set of results shows that the High group students superior performances were associated with a higher rate of causal map editing and more effective edits. However, their success was not associated with more effective model assessments. One possible interpretation is that these stu- dents were more effective than students in the Low group in identifying causal links during reading. However, even students who successfully completed the learning task might have benefited from feedback and support encouraging them to employ more effective strategies for assessing their map and recording the results of those assessments.  6. DISCUSSION AND CONCLUSIONS The results of applying an MDA methodology to the post-  hoc analysis of data from students using an open-ended learning environment called Bettys Brain provided valuable information about the effectiveness of actions that students took on the system. The main finding was that while stu- dents employed several learning behaviors related to solu- tion construction and evaluation, their use of these strate- gies was often sub-optimal. A large proportion (47.5%) of their causal map edits were incorrect, and an even larger proportion (63.0%) of their model assessment activities did not produce information related to the correctness of one or more causal links. Even students who were able to success-  203    fully teach Betty the correct causal map performed a large proportion of incorrect map edits (38.2%) and ineffective model assessment actions (68.9%). The results of this anal- ysis reveal that students using Bettys Brain in this study might have benefited from feedback targeted toward helping students employ more effective strategies as they worked to- ward completing the learning task. Moreover, they show the usefulness of the MDA methodology. By using just a subset of possible behavior and effectiveness metrics, the analysis provided information that could be used to describe how the learners in this study approached their open-ended learning task.  The MDA metrics used in this analysis focused on eval- uating students use of cognitive activities for solution con- struction and solution evaluation; each measured use of a system tool was judged as either being effective or ineffec- tive. However, MDA is not limited to assessing tool use on an OELE in isolation. Future work will focus on using the metacognitive portion of the task model to develop measures for assessing aspects of students metacognition, such as goal setting, planning, and monitoring. Additionally, we will be- gin incorporating these MDA metrics into Bettys Brain in order to identify opportunities for providing scaffolding and feedback to learners based on their needs. Once the decision to support students in using a particular type of cognitive or metacognitive process has been made, the pedagogical agents in the system will use what they learn from the MDA metrics to drive a discussion toward identifying exactly why the learner is having trouble. Ideally, the agents will be able to provide students with the scaffolding and support they need to gain a better understanding of both the cog- nitive and metacognitive strategies important for learning with OELEs. To that end, we have developed a set of tuto- rial activities for students to practice the cognitive processes important for success in teaching Betty, such as identify- ing causal relations from reading materials and identifying relevant link correctness information from quizzes. Hope- fully, this support will lead to improved student learning with Bettys Brain.  7. ACKNOWLEDGEMENTS This work has been supported by the National Science  Foundations IIS Award #0904387  8. REFERENCES [1] R. Azevedo and A. Hadwin. Scaffolding self-regulated  learning and metacognition - implications for the design of computer-based scaffolds. Instructional Science, 33(5-6):367379, 2005.  [2] R. Azevedo and M. Jacobson. Advances in scaffolding learning with hypertext and hypermedia: A summary and critical analysis. Educational Technology Research and Development, 56(1):93100, 200  [3] R. Azevedo, A. Johnson, A. Chauncey, and C. Burkett.  Self-regulated learning with metatutor: Advancing the science of learning with metacognitive tools. In M. Khine and I. Saleh, editors, New Science of Learning, pages 225247, 2010.  [4] C. Bravo, W. van Joolingen, and T. de Jong. Modeling and simulation in inquiry learning: Checking solutions and giving intelligent advice. Simulation, 82(11):769784, 2006.  [5] G. Clarebout and J. Elen. Advice on tool use in open learning environments. Journal of Educational Multimedia and Hypermedia, 17(1):8197, 2008.  [6] J. S. Kinnebrew, K. M. Loretz, and G. Biswas. A contextualized, differential sequence mining method to derive students learning behavior patterns. Journal of Educational Data Mining, in press.  [7] K. Koedinger and A. Corbett. Cognitive tutors: Technology bringing learning science to the classroom. In K. Sawyer, editor, The Cambridge Handbook of the Learning Sciences, pages 6178. Cambridge University Press, 2006.  [8] S. Lajoie and R. Azevedo. Teaching and learning in technology-rich environments. In P. Alexander and P. Winne, editors, Handbook of Educational Psychology, pages 803821. Erlbaum, Mahwah, NJ, second edition, 2006.  [9] S. Land. Cognitive requirements for learning with open-ended learning environments. Educational Technology Research and Development, 48(3):6178, 2000.  [10] K. Leelawong and G. Biswas. Designing learning by teaching agents: The bettys brain system. International Journal of Artificial Intelligence in Education, 18(3):181208, 2008.  [11] R. E. Mayer. Should there be a three-strikes rule against pure discovery learning American Psychologist, 59(1):1419, 2004.  [12] S. McQuiggan, J. Rowe, and J. Lester. Story-based learning: The impact of narrative on learning experiences and outcomes. In B. Woolf, E. Aimeur, R. Nkambou, and S. Lajoie, editors, Lecture Notes in Computer Science, volume 5091, pages 530539, 2008.  [13] R. Moreno and R. Mayer. Interactive multimodal learning environments. Educational Psychology Review, 19(3):309326, 2007.  [14] J. R. Segedy, J. S. Kinnebrew, and G. Biswas. Modeling learners cognitive and metacognitive strategies in an open-ended learning environment. In Advances in cognitive systems: Papers from the AAAI Fall Symposium, pages 297304. AAAI Press, 2011.  [15] J. R. Segedy, J. S. Kinnebrew, and G. Biswas. The effect of contextualized conversational feedback in a complex open-ended learning environment. Educational Technology Research and Development, 61(1):7189, 2013.  204      "}
{"index":{"_id":"30"}}
{"datatype":"inproceedings","key":"Tempelaar:2013:FAL:2460296.2460337","author":"Tempelaar, Dirk T. and Heck, Andr'e and Cuypers, Hans and van der Kooij, Henk and van de Vrie, Evert","title":"Formative Assessment and Learning Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"205--209","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460337","doi":"10.1145/2460296.2460337","acmid":"2460337","publisher":"ACM","address":"New York, NY, USA","keywords":"blended learning, formative assessment, learning analytics, learning dispositions, student profiles, test directed learning","abstract":"Learning analytics seeks to enhance the learning process through systematic measurements of learning related data, and informing learners and teachers of the results of these measurements, so as to support the control of the learning process. Learning analytics has various sources of information, two main types being intentional and learner activity related metadata [1]. This contribution aims to provide a practical application of Shum and Crick's theoretical framework [1] of a learning analytics infrastructure that combines learning dispositions data with data extracted from computer-based, formative assessments. The latter data component is derived from one of the educational projects of ONBETWIST, part of the SURF program 'Testing and Test Driven Learning'.","pdf":"Formative Assessment and Learning Analytics   Dirk T. Tempelaar   Maastricht University School of  Business & Economics,    Tongersestraat 53 - Room A2.20  6211 LM Maastricht   The Netherlands  D.Tempelaar@MaastrichtUniversity.nl     Andr Heck   University of Amsterdam,   Faculty of Science  Science Park 904   1098 XH Amsterdam  The Netherlands   A.J.P.Heck@uva.nl   Hans Cuypers  Eindhoven University of Technology,   Department of Mathematics and  Computing Science  5612 AZ Eindhoven   The Netherlands   hansc@win.tue.nl     Henk van der Kooij   Utrecht University,   Freudenthal Institute   P.O. Box 80125  3508 TC Utrecht  The Netherlands   h.vanderkooij@uu.nl     Evert van de Vrie  Open University Netherlands   Faculteit Informatica  P.O. Box 2960   6401 DL Heerlen  The Netherlands   Evert.vandeVrie@ou.nl       ABSTRACT  Learning analytics seeks to enhance the learning process through  systematic measurements of learning related data, and informing  learners and teachers of the results of these measurements, so as to  support the control of the learning process. Learning analytics has  various sources of information, two main types being intentional  and learner activity related metadata [1]. This contribution aims to  provide a practical application of Shum and Cricks theoretical  framework [1] of a learning analytics infrastructure that combines  learning dispositions data with data extracted from computer- based, formative assessments. The latter data component is  derived from one of the educational projects of ONBETWIST,  part of the SURF program Testing and Test Driven Learning'.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   Computer-assisted instruction (CAI).   General Terms  Measurement, Design.   Keywords  Blended Learning; Formative Assessment; Learning Analytics;  Learning Dispositions; Student Profiles; Test Directed Learning.   1. INTRODUCTION  The prime data source for most learning analytic applications is  data generated by learner activities, such as learner participation  in continuous, formative assessments. That information is  frequently supplemented by background data retrieved from   learning management systems and other concern systems, as for  example accounts of prior education. A combination with  intentionally collected data, such as self-report data stemming  from student responses to surveys, is however the exception rather  than the rule. In their theoretical contribution to LAK2012 [1],  Shum and Crick propose a learning analytics infrastructure that  combines learning activity generated data with learning  dispositions, values and attitudes measured through self-report  surveys and fed back to students and teachers through visual  analytics. Their proposal considers for example spider diagrams to  provide learners inside in their learning dispositions, values and  attitudes. In our empirical contribution, we aim to provide a  practical application of such an infrastructure based on combining  learning and learner data. In collecting learner data, we opted to  use a wide range of well validated self-report surveys firmly  rooted in current educational research, including learning styles,  learning motivation and engagement, and learning emotions.  Learner data were reported to both students and teachers using  visual analytics similar to those described in [1], so instead of  focusing on technology to feedback learner data, we will focus  here on the crucial role of the  richness of the profile of learner  dispositions, values and attitudes. Our second data source is  rooted in the instructional method of test-directed learning, and  brings about the second focus of this empirical study: to  demonstrate the crucial role of data derived from computer-based  formative assessments in designing effective learning analytic  infrastructures.   2. TEST-DIRECTED LEARNING  The classic function of testing is that of taking an aptitude test.  After completion of the learning process, we expect students to  demonstrate mastery of the subject. According to test tradition,  feedback resulting from such classic tests is no more than a grade,  and that feedback becomes available only after finishing all  learning. The alternative form of assessment, formative  assessment, has an entirely different function: that of informing  student and teacher. The information should help better shape the  teaching and learning and is especially useful when it becomes  available during or prior to the learning. Diagnostic testing is an  example of this, just as is practice testing. Because here the      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.     205    feedback that tests yield for learning constitutes the main function,  it is crucial that this information is readily available, preferably  even directly. At this point digital testing comes on the scene: it is  unthinkable to get feedback from formative assessments in time  without using computers. In the Netherlands, the development of  digital testing has accelerated over the past decade, partly by the  National Action Plan on e-Learning by SURF. This plan included  projects as the National Knowledge-Base Mathematics Skills  (NKBW I & II) and Intelligent feedback, which ran from 2006 to  2010 and delivered a multitude of test materials for digital,  formative assessments (www.nkbw.nl) to the mathematics  discipline. The projects under the current SURF program Testing  and Test Driven Learning [2] aim to complete this development  by integrating series of digital tests into a test-directed curriculum.  In the context of mathematics and statistics the project  ONBETWIST (www.onbetwist.org) achieves this goal [3].    3. LEARNING ANALYTICS  The broad goal of learning analytics is to apply the outcomes of  analyzing data gathered by monitoring and measuring the learning  process, as feedback to assist directing that same learning process.  Several alternative operationalizations are possible to support this.  In [4], six objectives are distinguished: predicting learner  performance and modeling learners, suggesting relevant learning  resources, increasing reflection and awareness, enhancing social  learning environments, detecting undesirable learner behaviors,  and detecting affects of learners. In the following sections  describing our approach, we will demonstrate that the  combination of self-report learner data with learning data from  test-directed instruction allows to contribute to at least five of  these objectives of applying learning analytics. Only social  interaction is restricted to learners being able to assess their  individual learning profiles in terms of a comparison of their own  strong and weak characteristics relative to the position of other  students. These profiles are based on both learner behavior,  including all undesirable aspects of it, and learner characteristics:  the dispositions, attitudes and values.  Learner profiles are used to  model different types of learners, and to predict learner  performance for each individual student. Since our instructional  format is of student-centered type, with the student, and not the  teacher, steering the learning process, it is crucial to feedback all  this information to learners themselves as to make them fully  aware of how to optimize their individual learning trajectories.   4. CASE STUDY: MATH AND STATS  Our empirical contribution focuses on freshmen education in  quantitative methods (mathematics and statistics) of the business  & economics school at Maastricht University, one of the  educational projects within ONBETWIST. All these projects  experiment with forms of test-directed learning, using a common  database of test materials (ONBETWIST database). We focus on  this specific project since it is unique for the integration of test- directed learning and the application of learning analytics. In  addition, it is directed at a large and diverse group of students,  which benefits the research design. The population of students  studied here consists of two cohorts of freshmen: 2010/2011 and  2011/2012, containing 1,832 students who in some way  participated in school activities (have been active in the digital  learning environment Blackboard). Besides BlackBoard, two  different digital learning environments for test-directed learning  were utilized: MyStatLab and ONBETWIST, by a large majority  of students: 1,743 and 1,682, respectively.   The diversity of the student population derives mainly from its  very international composition: only 34.8% took Dutch high  school, whereas all others were educated in international high  school systems. The largest group, 41.9% of the freshmen, were  educated according to the German Abitur system. High school  systems in Europe differ strongly, most particularly in the  teaching of mathematics and statistics. In that European palette  the Netherlands occupies a rather unique position, both in choice  of subjects (one of the few European systems with substantial  focus on statistics) and the chosen pedagogical approach. But  even beyond the Dutch position, there exist large differences, such  as between the Anglo-Saxon and German-oriented high school  systems. Therefore it is crucial that the first course offered to these  students is flexible and allows for individual learning paths. To  some extent, this is realized in offering optional, developmental  summer courses, but for the main part, this diversity issue needs  to be solved in the program itself. The digital environments for  test-directed learning play an important role in this.   5. TEST-DIRECTED LEARNING  For both sub-topics of the course, mathematics and statistics,  digital environments for test-directed learning are utilized. In  statistics, with the largest diversity in prior proficiency, we used  the commercial MyStatLab (MSL) environment. MSL is a generic  digital learning environment, developed by the publisher Pearson,  for learning statistics. It adapts to the specific choice of a textbook  from Pearson. Although MSL can be used as a learning  environment in the broad sense of the word (it contains, among  others, a digital version of the textbook), it is primarily an  environment for test-directed learning. Each step in the learning  process is initiated by submitting a question. Students are  encouraged to (try to) answer the question. If they do not master  (completely), the student can either ask for help to step by step  solve the problem (Help Me Solve This), or ask for a fully worked  example to show (View an Example). Next, a new version of the  problem loads (parameter based) to allow the student to  demonstrate their newly acquired mastery. In the investigated  courses, students work an average 19.2 hours in MSL, about a  quarter of the available time of 80 hours for learning statistics. In  this study, we use two different indicators for the intensity of use  of MSL: Stats#hours, the number of hours a student spent  practicing in the MSL environment, and StatsTestScore, the  average score for the practice questions, all chapters aggregated.   For the mathematics education, the  ONBETWIST database of  test items generated by the SURF project ONBETWIST was  applied. The ONBETWIST project has its own player that would  have made it possible to use these materials via server-based  computing. However, we opted to convert part of the content of  the ONBETWIST test database into BlackBoard item-pools and  subsequently organize access through the local UM Blackboard  environment, in order to accommodate the storage of all user  access data. We also opted to use items of multiple choice type  only, to prevent the use of formula editors by our students, with  parallel items stored in item pools from which a random version  was drawn in every practice or test attempt. The functionality of  the BlackBoard system realized this way is narrower than the  previously described MSL system: it is primarily a practicing and  testing functionality, in which the student can repeatedly test the  mastery of a specific math topic, rather than a true e-tutorial  supporting the learning itself. The variables signaling intensity of  practicing are Math#Tests, the number of practice tests a student  has tried out, and MathTestScore, the average score of those tests.   206    Because BlackBoard does not track time, it is not possible to  determine what part of the total learning time students spent in  practicing the BlackBoard math-tests, but because of the more   limited functionality, it seems plausible that this proportion is  lower than a quarter of the time spent on the course.   6. EDUCATIONAL PRACTICE  The educational system in which students learn mathematics and  statistics is best described as a blended system. The main  component is 'face-to-face: problem-based learning (PBL), in  small groups (14 students), coached by a content expert tutor.  Participation in these tutor groups is required, as for all courses  based on the Maastricht PBL system. Optional is the online  component of the blend: the use of the two test-directed learning  environments. The reason for having this component optional is at  one hand that this best fits the Maastricht educational model,  which is student-directed and places the responsibility for making  educational choices primarily with the student, and at the other  hand, the circumstance that not all students will benefit equally  from using these environments: due to the diversity in prior  knowledge, it is supposed to have less added value for students at  the high end. However, the use of test-directed environments is  stimulated by making bonus points available for good  performance in the quizzes. Quizzes are taken every two weeks  and consist of items that are drawn from item pools very similar to  the item pools applied in the two digital practice platforms. We  chose for this particular constellation, since it stimulates students  with little prior knowledge to make intensive use of the test  platforms. They realize that they fall behind other students in  writing the exam, and need to achieve a good bonus score both to  compensate, and to support their learning. The most direct way to  do so is to frequently practice in the MSL and BB environments.   The student-directed characteristic of the instructional model  requires first and foremost adequate information for students so  that they are able to monitor their study progress and their topic  mastery in absolute and relative sense. That provision of relevant  information starts the first day of the course when students take  two entry tests for mathematics and statistics, so as to make their  positions clear. Feedback from entry tests provide the first signals  of the importance of using the test platforms. Next, the digital  MSL- and BB-environments take over the monitor function:  students can at any time see their progress in preparing the next  quiz, get feedback on the performance in the already taken quizzes  and on the conduct of the practice sessions. The same information  is also available for the teachers. Although the primary  responsibility for directing the learning process is with the  student, the tutor acts complementary to that self-steering,  especially in situations where the tutor considers that a more  intense use of digital learning environments is desirable, given the  position of the student concerned. In this way, the application of  learning analytics shapes the instructional situation.   7. IMPACT TEST-DIRECTED LEARNING  To explore the role of test-directed learning, we investigated the  relationship between the intensity of use of the two test-directed  platforms and academic performance. Two indicators measure  academic performance: the exam containing a mathematics and  statistics part (MathExam and StatExam) and three quizzes for  both sub-topics, summed into a MathQuiz and StatQuiz score.  Before examining the relationship between practice and  performance, we corrected for differences in prior knowledge, in  two ways: by the level of prior mathematics education, and by the   student score in the math entry test. What prior education is  concerned: high school systems distinguish a basic level preparing  for the social sciences and an advanced level preparing for  sciences. An indicator variable is used for math at advanced level  (MathAdv) (which is true for one third of the students), with basic  level of math prior schooling being the reference group.  Moreover, the level of prior math knowledge is determined by the  day-one entry or diagnostic test, of which the score is labeled as  EntryTest, focusing on the mastery of basic algebraic skills.    One of the most straightforward ways to investigate the role of  test-directed learning on achievement is to use regression analyses  in which performance variables are explained by prior knowledge  and data on intensity of using the practice tests. These regressions  indicate that prior knowledge, both as type of prior schooling and  as score in the entry test, explains part of performance differences.  But the most important predictor of course performance is the  level that students gain in the test platforms. The number of  different tests students need to acquire that level, or the time they  need to practice to acquire that level, has a corrective effect, what  is intuitive: knowledge achieved through testing helps, but if a  student needs a lot of time or effort to reach that level, this signals  more problematic learning. An alternative demonstration of the  impact of using the test environments is obtained by dividing the  population of students into students with high and low mastery in  the entry test and high and low level of intensity of using the test  platforms, and comparing exam scores and pass/fail outcomes.  The fit resulting from these prediction models is very high. For  example, in a median split on performance in the math platform,  92% of the students with the better practice performance do pass,  against 59% in the students with lower practice performance.    7.1 LA: demographic characteristics  Having demonstrated that on average students benefit from the  opportunity of test-directed learning, the question arises whether  this is equally true for all students. This question asks for learning  analytics applications using data from other sources than the  learning environments to identify specific student groups most in  need for these practice environments. In this section of our  empirical study, we follow [1], [5] to investigate individual  differences in the intensity of using digital learning tools. As a  first step, we make use of data from the regular student  administration such as whether or not Dutch high school, whether  or not advanced prior math schooling, gender, nationality and  entry test score. Students with advanced prior schooling are better  at math, without incurring more need to practice. They are not  better at statistics, which corresponds to the fact that in programs  at advanced level, the focus is not on statistics but abstract math.  Dutch students make considerably less use of both test  environments and hence achieve a slightly lower score, benefiting  from a smoother transition than international students, but relying  just somewhat too much on that. Students with a high entry test  score do better in mathematics and a little better in statistics in the  test environments, without the need to exercise more. Finally,  there are modest gender effects, the strongest in the intensity of  exercising: female students are more active than male students.   7.2 LA: cultural differences  The remaining data from the student records of administrative  systems regard the nationality of students. Because cultural  differences in education has been given an increasingly important  role, and because the Maastricht student population makes it very  suitable through its strong international composition, the   207    nationality data are converted into so-called national culture  dimensions, based on the framework of Hofstede [6]. In that  framework, there are a number of cultural dimensions that refer to  values that are strongly nationally determined. In this study we  use six of these dimensions: Power Distance, Individualism versus  Collectivism, Masculinity versus Femininity, Uncertainty  Avoidance, Long-Term vs. Short-Term Orientation and  Indulgence vs. Restraint. Scores for each of these national  dimensions are assigned to the individual students. Correlating  these scores with the four indicators of practice tests intensity  result in several significant effects, all in line with Hofstede's  framework. The most significant effects are for students from a  masculine culture, where mutual competition is an important  driver in education, for students from a culture that value long- term over short-term and, somewhat in relation thereto, cultures  that value sobriety rather than enjoyment. In this, masculinity and  hedonism have a stronger impact on the intensity of exercising,  than on the proceeds of exercising, in contrast to long-term  orientation, that has about equal impact on both aspects.  Uncertainty avoidance contributes, as expected, to practicing,  albeit to a lesser extent and again primarily toward intensity of  exercising rather than its outcome. The roles of power distance  and individualism play a less salient role in learning, as expected.    7.3 LA: learning styles  Although the effects are smaller in size, learning data based on the  learning style model of Vermunt [7] exhibit a characteristic role.  Vermunts model distinguishes learning strategies (deep, step- wise, and concrete ways of processing learning topics), and  regulation strategies (self, external, and lack of regulation of  learning). Deep-learning students demonstrate no strong  relationship with test directed learning: they exercise slightly less,  but achieve a slightly better score. That is certainly not true for the  stepwise learning students. Especially for these students the  availability of practice tests seems to be meaningful: they practice  more often and longer than other students and achieve, especially  for statistics, a better score than the other students. These patterns  repeat themselves in the learning regulation variables that  characterize the two ways of learning: self-regulation being  characteristic for deep learning, external regulation as a feature for  stepwise learning. Indeed, the students whose learning behavior  has to be externally regulated, are those who benefit most from  the test environments: both in intensity and performance they  surpass the other students. A notable (but weak) pattern is finally  visible in learning behavior lacking regulation: these students tend  to practice more often and longer than the other students but  achieve in both subtopics lower performance levels. Apparently,  even the structure of the two test environments is incapable to  compensate the of lack of regulation for these student.   7.4 LA: (mal)adaptive thoughts & behaviors  Recent Anglo-Saxon literature on academic achievement and  dropout assigns an increasingly dominant role to the theoretical  model of Andrew Martin: the 'Motivation and Engagement  Wheel [8]. That model includes both behaviors and thoughts or  cognitions that play a role in learning. Both are then divided into  adaptive and mal-adaptive or obstructive forms. As a result, the  four quadrants are: adaptive behavior and adaptive thoughts (the  boosters), mal-adaptive behavior (the guzzlers) and obstructive  thoughts (the mufflers). In Figure 1, two panels depict the  relationships of adaptive and mal-adaptive thoughts and behaviors  with the usage data. The first panel documents adaptive thoughts     Figure 1. Role of (mal)adaptive thoughts and behaviors    Self-belief, Value of school and Learning focus, and adaptive  behaviors Planning, Study management and Perseverance. All  adaptive thoughts and all adaptive behaviors have a positive  impact on the willingness of students to use the test environments,  where the effect of the adaptive behavior dominates that of  cognitions. The mal-adaptive variables show a less uniform  picture. Because gender effects play a prominent role here, the  dummy variable female/male is added to the four data of use  intensity in the panel. From these additional correlations we  conclude that mal-adaptivity manifests itself differently in female  and male students: for female students primarily in the form of  limiting thoughts, especially fear and uncertainty, in male students  primarily as mal-adaptive behaviors: self-handicapping and  disengagement. That difference has a significant impact on  learning. Mal-adaptive behaviors negatively impact the use of the  test environments: all the correlations, both for use intensity and  performance, are negative. The effect of inhibiting mind, however,  is different: uncertainty and anxiety have a stimulating effect on  the use of the test environments rather than an inhibitory effect.  Combination of both effects provides a partial explanation for the  observed gender effects in the use of the test environments.   7.5 LA: learning emotions  Also of relatively recent date is research on the role of emotions in  learning. Leading in this research is Pekruns control-value theory  of learning emotions [9]. That theory indicates that emotions that  arise when learning are influenced by the feeling to be 'in control'  and something worthwhile to do. Pekruns model distinguishes  several emotions, and for this study we selected emotions that  contribute most strongly to student success or failure: the negative  emotions of Anxiety, Boredom and Hopelessness, the positive  emotion Enjoyment. Emotions are context-specific measured, for   208    example, Anxiety is defined in the context of learning  mathematics. Learning emotions are typically measured in the  middle of the course, unlike all other instruments that are taken in  the beginning of the course. Correlations can thus not be  interpreted within a cause-effect framework, as we can do for  most other variables. The most obvious association is that of  mutual influence: emotions will impact the use of the test  environments, but conversely experience gained in practicing, and  ideally the performance in practicing, will also determine learning  emotions. Associations we find all have predicted directions:  negative emotions demonstrate negative relationships to the use of  the test environments, positive emotion and feeling in control,  demonstrate positive relationships. It is striking that performance  in the test environment, especially for mathematics, is much  stronger associated with learning emotions than the intensity of  practicing in the test environments.   8. CONCLUSIONS  The intensive use of practice test environments makes a major  difference for academic performance. But in a student-centered  curriculum it is not sufficient when teachers are convinced of the  benefits that test-based learning in digital learning environments  entails. Students regulate their own learning process, making  themselves choices on how intensively they will exercise and  therefore, are the ones who need to become convinced of the  usefulness of these digital tools. In this, learning analytics can  play an important role: it provides a multitude of information that  the student can use to adapt the personal learning environment as  much as possible to the own strengths and weaknesses. For  example, in our experiment the students were informed about their  personal learning dispositions, attitudes and values, together with  information on how learning in general interferes with choices  they can make in composing their learning blend. At the same  time: the multitude of information available from learning  analytics is also the problem: that information requires individual  processing. Some information is more important for one student  than the other, requiring a personal selection of information to  take place. Learning analytics deployed within a system of  student-centered education thus has its own challenges.   The aim of this contribution extends beyond demonstrating the  practical importance of Shum and Cricks learning analytics  infrastructure. Additionally, this research provides many clues as  to what individualized information feedback could look alike. In  the learning blend described in this case study, the face-to-face  component PBL constitutes the main instructional method. The  digital component is intended as a supplementary learning tool,  primarily for students for whom the transition from secondary to  university education entails above average hurdles. Part of these  problems are of cognitive type: e.g. international students who  never received statistics education as part of their high school  mathematics program, or other freshmen who might have been  educated in certain topics, without achieving required proficiency  levels. For these kind of cognitive deficiencies, the digital test- directed environments proved to be an effective tool to  supplement PBL. But this applies not only to adjustment  problems resulting from knowledge backlogs. Students encounter  several types of adjustment problems where the digital tools  appear to be functional. The above addressed learning  dispositions are a good example: student-centered education  presupposes in fact deep, self-regulated learning, where many   students have little experience in this, and feel on more familiar  ground with step-wise, externally regulated learning. As the  analyses demonstrate: the digital test environments help in this  transformation. It also makes clear that the test environments are  instrumental for students with non-adaptive cognitions about  learning mathematics and statistics, such as anxiety. An outcome  that is intuitive: the individual practice sessions with  computerized feedback will for some students be a safer learning  environment than the PBL tutorial group sessions. Finally, the  learning analytics outcomes make also clear where the limits of  the potentials of digital practice are: for students with non- adaptive behaviors and negative learning emotions. If learning  involves boredom and provokes self-handicapping, even the  challenges of test-based learning will fall short.   9. ACKNOWLEDGEMENTS  The ONBETWIST project has been financed by SURF- foundation as part of the Testing and Test Driven Learning  program [2].    10. REFERENCES  [1] Buckingham, S. S. & Deakin, C. R. (2012). Learning   Dispositions and Transferable Competencies: Pedagogy,  Modelling and Learning Analytics. Proceedings LAK2012:  2nd International Conference on Learning Analytics &  Knowledge, pp. 92-101. ACM Press: New York   [2] SURF (2010). Programma Toetsing en Toetsgestuurd Leren.  http://www.surf.nl/nl/themas/innovatieinonderwijs/toetsen/D ocuments/Projectplan%20PROGRAMMA%20TOETSING% 20EN%20TOETSGESTUURD%20LEREN.pdf    [3] Tempelaar, D. T., Kuperus, B., Cuypers, H., Van der Kooij,  H., Van de Vrie, E. M., & Heck, A. (2012). The Role of  Digital, Formative Testing in e-Learning for Mathematics: A  Case Study in the Netherlands. In: Mathematical e- learning [online dossier]. Universities and Knowledge  Society Journal (RUSC), 9(1). UoC.    [4] Verbert, K., Manouselis, N., Drachsler, H., & Duval, E.  (2012). Dataset-Driven Research to Support Learning and  Knowledge Analytics. Educational Technology & Society,  15 (3), 133148.   [5] Whitmer, J., Fernandes, K., & Allen, W. R.. (2012).  Analytics in Progress: Technology Use, Student  Characteristics, and Student Achievement. EDUCAUSE  Review Online, July.    [6] Hofstede, G., Hofstede, G. J., & Minkov, M. (2010).  Cultures and organizations: Software of the mind. Revised  and expanded third edition. Maidenhead: McGraw-Hill.   [7] Vermunt, J. D. (1996). Leerstijlen en sturen van  leerprocessen in het Hoger Onderwijs. Amsterdam/Lisse:  Swets & Zeitlinger.   [8] Martin, A. J. (2007). Examining a multidimensional model  of student motivation and engagement using a construct  validation approach. British Journal of Educational  Psychology, 77, 413-440.    [9] Pekrun, R. (2006). The control-value theory of achievement  emotions: Assumptions, corollaries, and implications for  educational research and practice. Educational Psychology  Review, 18, 315-34.     209      "}
{"index":{"_id":"31"}}
{"datatype":"inproceedings","key":"Monroy:2013:SCL:2460296.2460339","author":"Monroy, Carlos and Rangel, Virginia Snodgrass and Whitaker, Reid","title":"STEMscopes: Contextualizing Learning Analytics in a K-12 Science Curriculum","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"210--219","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460339","doi":"10.1145/2460296.2460339","acmid":"2460339","publisher":"ACM","address":"New York, NY, USA","keywords":"STEM education, big data, learning analytics, online curriculum","Abstract":"In this paper, we discuss a scalable approach for integrating learning analytics into an online K-12 science curriculum. A description of the curriculum and the underlying pedagogical framework is followed by a discussion of the challenges to be tackled as part of this integration. We also include examples of data visualization based on real student and teacher data. With more than one million students and fifty thousand teachers using the curriculum, a massive and rich dataset is continuously updated. This repository depicts teacher and students usage of an inquiry-based science program, and offers exciting opportunities to leverage research to improve both teaching and learning. The growing dataset, with more than a hundred million items of activity in six months, also poses technical challenges such as data storage, complex aggregation and analysis with broader implications for pedagogy, big data, and learning.","pdf":"STEMscopes: Contextualizing Learning Analytics in a   K-12 Science Curriculum  Carlos Monroy   Center for Technology in Teaching  and Learning, Rice University   6100 Main St., MS 120  Houston, TX 77005  +1 713 348 5481   carlos.monroy@rice.edu   Virginia Snodgrass Rangel   Rice University Center for Digital   Learning and Scholarship  6100 Main St., MS 112   Houston, TX 77005  +1 713 348 5008   vsr@rice.edu   Reid Whitaker  Rice University Center for Digital   Learning and Scholarship  6100 Main St., MS 112   Houston, TX 77005  +1 713 348 3741   reid@rice.edu         ABSTRACT  In this paper, we discuss a scalable approach for integrating  learning analytics into an online K-12 science curriculum. A  description of the curriculum and the underlying pedagogical  framework is followed by a discussion of the challenges to be  tackled as part of this integration. We also include examples of  data visualization based on real student and teacher data. With  more than one million students and fifty thousand teachers using  the curriculum, a massive and rich dataset is continuously  updated. This repository depicts teacher and students usage of an  inquiry-based science program, and offers exciting opportunities  to leverage research to improve both teaching and learning. The  growing dataset, with more than a hundred million items of  activity in six months, also poses technical challenges such as data  storage, complex aggregation and analysis with broader  implications for pedagogy, big data, and learning.     Categories and Subject Descriptors  K.3.0 [Computers and Education]: General; H.4 [Information  Systems Applications]: Miscellaneous.   General Terms  Management, Design, Human Factors.   Keywords  Learning analytics, STEM education, online curriculum, big data.   1. INTRODUCTION  Computers and mobile devices offer new tools to support  learning, both inside and outside the classroom. This in turn opens  opportunities to understand and assess their impact on educators  and learners [42]. Indeed, a major advantage of online curricula  over traditional textbook-based curricula is the ability to capture  data about how teachers and students use these devices and  programs. In this vein, learning analytics is a relatively new, but  rapidly growing [17], discipline whose goal, according to the   Society of Learning Analytics, is understanding and optimizing  learning and the environments in which it occurs. Although most  learning analytics (LA) methods use data generated by the  interactions between learners and the Learning Management  Systems (LMS) they use [39, 57], in order to fully capture the  richness of learning, it is crucial to incorporate information  derived from hands-on activities, class discussions, and teachers  comments. This is of special relevance in science education where  students conduct real experiments and make observations not  captured by computers, or in other settings where students  complete activities using printed handouts. Achieving a holistic  approach to using learning analytics for curriculum improvement  is a challenge because it can be hard to make sense of and  contextualize the usage of data gathered by the LMS.   Ongoing research in educational data mining and learning  analytics suggests that harnessing analytics data can advance new  research methodologies in education, help educators better assess  their pedagogical practices, and devise innovative educational  methods, all with the goal of improving education [14, 38, 40, 47,  51, 57]. With the rapidly growing interest in and technical ability  to leverage these data in educational settings, there is a sense that  many recent educational technology and big data initiatives are  detached from what we know about learning and teaching [27,  46]. It is, therefore, imperative to ensure that the use of learning  analytics in the K-12 sector is grounded in relevant pedagogy and  that it incorporates the face-to-face interactive learning  experiences such as student inquiry, discussions, hands-on  activities, and so on, not captured by the digital artifacts. This type  of approach guarantees that data analyses in LA are not reduced to  the mere analysis of clicks and page visits [53].   The purpose of this paper, then, is to present a strategy to  incorporate learning analytics into one K-12 online science  curriculum and to discuss the challenges faced and options for  overcoming them. The strategy we have developed has two main  parts: the first is technical, and the second is qualitative. The  technical strategy comprises the architecture that enables our  analytics instruments and then the specific variables we are using  to answer questions about the curriculum. The second part, which  is qualitative, is more challenging to scale but necessary, we  argue, for designers and researchers to understand how the  curriculum is being used in classrooms. The qualitative strategy  has served to help us make sense of the analytics and to make  more meaningful recommendations for improvement.    We begin our discussion by first briefly describing the curriculum  examined here and its underlying pedagogical model. Then we  discuss the two parts of our strategy, including examples of   Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   210    findings from the work we have done to implement the strategy  and improve the curriculum and its data dashboard. As part of  this, we present some examples of visualization techniques we  recently incorporated into STEMscopes based on teacher  feedback. Finally, we end with a discussion about the future of  data analytics in K-12 education, with a focus on the challenges  we need to overcome and the potential for educational  improvement.   2. THE 5E+I/A PEDAGOGICAL  FRAMEWORK  STEMscopes is a K-12, comprehensive, online science curriculum  that is both aligned to state standards and grounded in the 5E  science inquiry model. It offers hands-on inquiry activities,  intervention and acceleration materials, and additional supporting  resources for teachers. The 5E pedagogical model is an inquiry- based learning approach that aims to harness learners prior  knowledge while eliciting interest in new phenomena [8, 57]. This  instructional model was created and refined by the Biological  Science Curriculum Study (BSCS). The 5E acronym is an  abbreviation of five steps that each begins with the letter E:  engagement, exploration, explanation, elaboration, and evaluation.  In the Engagement phase, students draw on prior knowledge to  raise their interest in and activate their prior knowledge of the new  content. During the Exploration phase, students take part in  activities and experiments that allow them to experience and learn  new concepts and skills. The Explanation phase requires students  to explain those new concepts and skills in their own words. New  experiences in the Elaboration phase question and deepen  students understanding of the new concepts. Finally, learners  understanding is assessed in the Evaluation phase.    STEMscopes adds two steps to the basic 5E model: one for  intervention with students struggling to master concepts, and one  for acceleration for those students who have mastered the  concepts and are ready to extend and apply their learning [57].  Our curriculum also offers accompanying hands-on activity kits  that teachers can use with students for conducting science  experiments and make observations, thereby offering a deeper  science education experience. The curriculum is organized in  grades from Kindergarten to 12th grade. Each grade is comprised  of units called scopes. There are nearly four hundred scopes in  total. Scopes in turn, are classified in different categories. The  total of scopes per category are listed in table 1.      Table 1. Number of scopes per topic in STEMscopes.   Topic name   # of   scopes    Formation of Fossil Fuels                     74   Safety                                        62   Heredity                                      58   Classifying Matter                            38   Uses of Energy                                33   One Dimensional Motion                       29  Physical and Chemical Changes and   Properties  25    Prokaryotic and Eukaryotic Cells             24   Whole Number Place Value                     12   Measurement Conversions                      4   Geometric Figures                             3   Probability and Predictions                   3   Relationships in Data                         2   Figure 1 provides an example of what the 5E model looks like in  STEMscopes. Specifically, it depicts a 4th grade unit entitled,  Changes from Heat. Components for each of the seven  (5E+I/A) steps are presented in the column on the left. They  include slideshows, interactive games, and assessments, among  others. Figure 2 shows three elements of a scope, or unit: 1) an  overview of a units content in the context of the 5E model, which  provides a snapshot of all the components a teacher can find for  that scope. It is intended for class planning purposes; 2) an  interactive game in the Explain phase that offers an engaging and  stimulating activity for the student to revisit what they have  learned; and 3) a student journal sheet that can be used by students  when completing hands-on activities as part of the Engage and  Explore steps.    Figure 1. Screen shot of a science unit titled,   Changes from Heat. The column on the left   presents elements grouped by each 5E phase   (Engage, Explore, Explain, Elaborate, and   Evaluate).      211       3. TECHNICAL ANALYTICS STRATEGY  In this section, we lay out our strategy for using LA in  STEMscopes. The strategy addresses both the technical and sense  making challenges we face in the context of K-12 education.   3.1 Architecture   When addressing learning analytics for STEMscopes, we face an  environment where data is constantly generated and stored as  teachers and students use our curriculum. Curating, storing and  processing the large amount of data amassed on a daily basis,  nearly a quarter of a million interactions with the curriculum each  day, demands techniques from the area of databases, data  warehouse, and big data. We have designed a distributed  environment that allows us to scale the storage and processing of  user-generated data. Accordingly, we employ three separate high- end computer servers that handle regular STEMscopes requests,  usage data storage, and analytics data processing. Although this  infrastructure is robust enough for our immediate needs, we will  use different big-data methodologies as we introduce more  sophisticated analyses such as machine learning, data mining  algorithms, simulations, and computational linguistics. For  example, we recently began experimenting with Hadoop [41, 52],  an open source framework that enables distributed high-intensive  computing for processing extremely large amounts of data.  Although this type of methodology might not be considered  important in Learning Analytics, current processing times for  aggregating data in STEMscopes suggest it will be part of our  infrastructure in the future. Data aggregation by week and month  is in the order of three to four hours. It is true that these processes  are not executed frequently. However, we need to plan a scalable  infrastructure as we begin calculating content- and user-based  models derived from data spanning several years, across multiple  schools and districts along with recent user generated data.  Among the practical benefits of this big data approach to our K-12  curriculum are to allow large clustering and machine learning  algorithms for supporting personalized learning.   3.2 STEMscopes Dataset  STEMscopes serves about 2,400 schools with more than 50,000  teachers and over one million students in the state of Texas.  STEMscopes currently serves 40% of the states school districts,  making it the most used science curriculum in the state. During   the 2011-2012 school year, we gathered more than one hundred  million data points generated by districts, schools, and users, both  teachers and students. We are able to identify the science topic  covered, grade level, specific step in the 5E+I/A process,  materials used, and the interim assessment results. LA allows for  multiple kinds of analyses including basic correlation analysis,  regression analysis, path analysis, user modeling, domain  modeling, learning analysis, curriculum evaluation, trend analysis,  and longitudinal analysis [7]. In this way, LA can shed light on  program effectiveness, on the ways in which a program can be  improved, and on ways the curriculum managers can better  support the teachers using the curriculum.    Because LA is based on the measurement and analysis of data  generated by end users, it has the potential to be a powerful tool  for educational improvement [16]. LA can leverage education  through improved educational decision-making [1, 19, 36, 37],  clearer goal setting [23], more timely and frequent feedback for  student and teachers [1, 20, 21, 35, 48], individualization of  learning [6, 15, 24, 32, 49, 58], and the generation of a richer set  of data on student behavior and learning [5, 33].    Yet, in order for LA to have a sustained and positive impact on K- 12 education, it must be implemented in ways that both recognize  and draw on the existing education research. Indeed, while LA has  an established presence in the business sector and even in higher  education, its foray into K-12 education is particularly new, and is  being driven in large part by the start-up community and venture  capital [11, 12]. Although there is nothing wrong with these  drivers, educators are unlikely to buy into new technology and big  data initiatives if they are not immediately accessible and relevant  to their daily work [34, 54, 56]. This paper addresses this gap in  the literature by discussing the strategy for integrating LA into a  K-12 science curriculum.   3.3 Pedagogical Considerations  The STEMscopes curriculum is grounded in the 5E+I/A inquiry  model described above. This framework underpins not only the  development of the curriculum, but also the analysis of all data.  Without theory, the data patterns at best have no meaning, and at  worst are given the wrong meaning [2, 31]. The 5E models  underlying theory of learning, constructivism, is used to answer  important questions about how the curriculum is actually  implemented in classrooms and how students learn best [8]. For  example, constructivism and the 5E model contend that students  learn best by exploring concepts through experience before  engaging in any formal explanation. STEMscopes, however, is  flexible and allows teachers to use materials from the curriculum  in any order, or skip some of them.    The need for flexibilitywhich allows teachers to adapt the  available resources to their students needsis in tension with  what constructivism and the 5E model say about learning. In our  analyses, we therefore have to balance the expectation that  teachers will follow the instructional model with the reality that  teachers adapt all of their resources. Together, pedagogical and  implementation considerations prompt us to ask questions such as:  Is the curriculum used to scaffold student learning What are  different patterns that users follow through the curriculum, and  how are these patterns of use related to student outcomes    3.4 Data Collection  There are two main purposes for collecting and analyzing  analytics data in a K-12 curriculum. The first is to improve the  curriculum, and the second is to evaluate the impact of the   1 2   3   Figure 2. Various elements for one science topic: 1) a partial   summary of the content of one unit, 2) an interactive game,   and 3) a student journal.   212    curriculum on student learning. The collection of analytics data is  guided by the pedagogical considerations outlined above 5E+A/I model helps us form questions that we can answer using  the analytics data.   Two overarching questions we currently are investigating with teacher use of curriculum (frequency of use, kind of use, and  the order in which teachers use the steps embedded in the  curriculum) and the time teachers spent accessing Together, the answers to these questions begin to paint a picture  of how teachers are using the curriculum to teach science  Examining teacher use of the curriculum is quite complex and has  required us to continually refine what we want to know.  most basic question about frequency of use has many possible  answers that require different approaches for calculat and to accommodate all of the possible ways to conceive us For instance, we can capture overall use for each teacher who has  an account, but this tells us nothing about how frequently they are  using what; it might simply be that they are signing in  Therefore, we also calculated how frequently teachers are using  individual units (we call them scopes), different steps of the  model, and other components within the curriculum (e.g. teacher  background material). We also have drilled  weighting these statistics by the number of grades a teacher  teaches or the number of students a teacher has. In this way, we  have begun to deal with questions of data normalization, which  we will discuss more below.   Teacher use also deals with questions of how using the curriculum. Of interest to us are which parts of the  curriculum are teachers using and whether they are using the steps  in order. As part of a three-year study, we analyzed a non sample of 134 mostly elementary schoolteachers from a large,  urban district in Texas (of which we are in year one) examined which steps the teachers in our sample found that for these teachers, the most commonly used step was  Explore. The results are portrayed in figure 3. As with other use  questions, there are many ways to drill down into this finding,  such as creating different weights in order to take into account  how many grades or students a teacher has.   Figure 3. Use of each of the 5E+I/A steps.  The collection of analytics data is  guided by the pedagogical considerations outlined abovethe  5E+A/I model helps us form questions that we can answer using   investigating deal  (frequency of use, kind of use, and   the order in which teachers use the steps embedded in the  accessing the curriculum.   Together, the answers to these questions begin to paint a picture  teachers are using the curriculum to teach science.   Examining teacher use of the curriculum is quite complex and has  required us to continually refine what we want to know. Even the   about frequency of use has many possible  calculating variables   to accommodate all of the possible ways to conceive usage.  For instance, we can capture overall use for each teacher who has  an account, but this tells us nothing about how frequently they are  using what; it might simply be that they are signing in and out.  Therefore, we also calculated how frequently teachers are using  individual units (we call them scopes), different steps of the  model, and other components within the curriculum (e.g. teacher    down further by  weighting these statistics by the number of grades a teacher  teaches or the number of students a teacher has. In this way, we  have begun to deal with questions of data normalization, which   how the teachers are  using the curriculum. Of interest to us are which parts of the  curriculum are teachers using and whether they are using the steps   , we analyzed a non-random  schoolteachers from a large,   (of which we are in year one). We  examined which steps the teachers in our sample used most. We  found that for these teachers, the most commonly used step was   . As with other use  questions, there are many ways to drill down into this finding,  such as creating different weights in order to take into account   Questions of time are similarly complex. how long a teacher is logged in, when the first and last login were,  how long a teacher spends on each page, and on how many  different days a teacher actually accessed the website.  example, we investigated teacher user activity for  teachers from one district over the course of several months. We  found a great deal of variation in actual use: the span of time  during which teachers actively use their curriculum accounts  varies in length (from first login to last login for the period of  interest, which ranged from a span of one day to a span of 183  days), as does the number of days on which teachers  in to access content or materials. These data are summarized in  table 2 below. Teacher activity ranged from a low of one day on  which teachers logged in, to a high of 107 separate days on which  teachers accessed the site. We also adjusted this figure to take into  account the number of grades the teachers teach; some teach only  one grade, while others teach science to multiple grades.  we calculated the percent of the total time span teachers were actively logged into the website; this statistic  ranged from 0% to 100% of the time.      Table 2: Teacher data on visits to      These kinds of data help the curriculum designers understand if  and how often the curriculum is being used. We also will be able  to relate these data to student learning outcomes, both those  measured by assessments embedded in the curriculum  students can take online) and by district and state standardized  assessments. The strength and direction of these relationships can  shed light on the efficacy and effectiveness of the curriculum.  their own, however, these data have little meaning relate them back to the theory behind the curriculum complement them with qualitative data  3.5 Visualizing Data  The final part of our LA strategy is the incorporation of various  visualization techniques to help teachers and admini sense of data. One of the biggest challenges facing educators and  analysts alike is turning data into information  56]. Visualization is an important tool that, in concert with strong  theories, can help make sense of data visualization techniques we use are heat maps and timelines design of these visual interfaces is driven by comments and  feedback received from the various stakeholders. As a way of  example, teachers who participated in the focus gro in section 4.2) indicated that knowing what parts of the  curriculum have not been used would help as reminder of what  needs to be taught. Similarly, color intensity would suggest  sections most and least used, driving teachers attention to th sections.    Use of each of the 5E+I/A steps.   are similarly complex. The analytics can tell us  how long a teacher is logged in, when the first and last login were,  how long a teacher spends on each page, and on how many  different days a teacher actually accessed the website. For   ed teacher user activity for a sample of  one district over the course of several months. We   found a great deal of variation in actual use: the span of time  during which teachers actively use their curriculum accounts   rst login to last login for the period of  , which ranged from a span of one day to a span of 183   ), as does the number of days on which teachers actually log  These data are summarized in   activity ranged from a low of one day on  which teachers logged in, to a high of 107 separate days on which   We also adjusted this figure to take into  account the number of grades the teachers teach; some teach only   ile others teach science to multiple grades. Finally,  we calculated the percent of the total time span (in days) that  teachers were actively logged into the website; this statistic   visits to STEMscopes   These kinds of data help the curriculum designers understand if  and how often the curriculum is being used. We also will be able  to relate these data to student learning outcomes, both those  measured by assessments embedded in the curriculum (which  students can take online) and by district and state standardized  assessments. The strength and direction of these relationships can  shed light on the efficacy and effectiveness of the curriculum. On   these data have little meaning if we do not  relate them back to the theory behind the curriculum and if do not  complement them with qualitative data.    The final part of our LA strategy is the incorporation of various  visualization techniques to help teachers and administrators make  sense of data. One of the biggest challenges facing educators and  analysts alike is turning data into information [22, 25, 28, 29, 55,   . Visualization is an important tool that, in concert with strong  theories, can help make sense of data [13]. Two specific   are heat maps and timelines. The  design of these visual interfaces is driven by comments and  feedback received from the various stakeholders. As a way of  example, teachers who participated in the focus group (discussed  in section 4.2) indicated that knowing what parts of the  curriculum have not been used would help as reminder of what  needs to be taught. Similarly, color intensity would suggest   , driving teachers attention to those   213    We have identified some cases where visualization will play an  essential role in understanding teaching practices. Covering all of  them is beyond the scope of this paper, but here we describe two  of them. At the teacher level, for example, we want to find out  whether sequences of use are similar across scopes for the same  teacher, or just limited to scopes with similar themes or concepts.  The former implies a common teaching practice, whereas the  latter, practices can be attributed to particular content properties.  Another case is one where usage patterns for a given scope are  consistent in a high number of teachers in one district but different  when compared to other districts. One can speculate that perhaps  teacher training might have an impact on teaching practices.        3.5.1 Timelines  Timelines are used to understand sequence and pacing. In  timelines, events are plotted on a graph, allowing users to see  when topics were introduced, how long certain themes were  covered, any overlap of topics, and the time elapsed between  events. For example, a timeline might be used to investigate the   impact of the time elapsed between the moment when content was  covered and the assessment of that content and students learning  outcomes [18, 30, 44]. Figure 4 offers a partial snapshot of a  teachers use of STEMscopes. The number of Density labels in  the display is indicative of a teacher working on that scope for a  certain period of time. The presence of two different scopes at the  right end of the display: Adaptations and Circuits and Electricity,  might signify a shift to a different theme at that point in time  during the school year. Time-based visualization addresses the  when materials are accessed and for how long.   3.5.2 Heat maps  Heat maps allow us to study the way in which the curriculum is  presented. Colors illustrate the intensity of use across the 5E steps.  Figure 5 depicts a partial view of curriculum use for 3rd and 4th  grades in one school district (grade level is indicated by the  numeric prefix in the TEKS column). Each row corresponds to  one scope. Blank cells indicate resources not available in  STEMscopes. This graph show high use for most scopes in third  grade and from scopes 4.5A to 4.6A, while low use can be seen  from scopes 3.8D to 4.4A. The most used steps are Engage and  Explore, while Acceleration is the least used. The top used scopes  are 3.8CD  Space and 4.6A  Forms of Energy. Conversely, 4.3C   Models, is the least accessed. This type of visualization offers a  quick overview of what has been taught and to what degree.   3.5.3 Visualizing Student Performance  At any given time during the school year, educators need  information about student progress. In addition to the students  overall performance information, more detailed progress data  augment teachers understanding of each student on different   Figure 5. Heat map depicting a partial list of STEMscopes content use for one district. Scopes included are for 3rd   and 4th grades (indicated by the numeric prefix on the TEKS column).    Time in hours   Figure 4. Use of themes by a teacher, separation   among events indicates time elapsed (pacing) between   them.   214    topics or even concepts. Figure 6 depicts a dashboard with  information about fourth graders. Columns show scopes grouped  by topic. Upward green arrows represent increase in grades from  the previous assessment and downward red arrows illustrate drop  in grades. Students are clustered by degree of intervention or  acceleration they require, represented by the colored cells (three  different groups). Students in the intervention group (yellow- colored cells) are the ones that although advancing in the class,  need a moderate amount of help. Those in the intensive  intervention group (pink-colored cells), require more help because  their poor performance. Conversely, students in the acceleration  group (green-colored cells) can be challenged with more advanced  materials. This dashboard helps teachers to better understand the  needs of the students and schedule the most appropriate resources  from the Acceleration or Intervention components in the  curriculum. This case illustrates the potential of analytics for  achieving a more personalized teaching experience.   3.6 Identifying Uncommon Patterns  Another area where analytics can help is in the identification of  non-canonical sequences of usage. The following example  illustrates this case. STEMscopes offers a variety of assessments  instruments that teachers assign to students. These assessments  are designed to measure students knowledge at three different  points in time as follows: Pre-assessment, at the beginning of the   unit, Progress Monitoring, half way in the unit, and Standard  Assessment, at the end of the unit.    The expectation is for students to complete these assessments in  the order previously described; however, we have found  occurrences where students either do not finish some or all of the  assessments, or cases in which students execute them in different  and unintended sequence. Table 3 shows assessments of students  in 8th grade for the scope 8.5A Atoms. The first three students in  the table completed only Pre-assessment. The next three students  did both Pre-assessment and Progress Monitoring. The following  twelve students completed all three assessments. In all these  cases, students followed the expected completion sequence. The  last four students in the table show unusual completion patterns.  Hop skipped the Progress-monitoring; Tal completed the  Standard Assessment before the Progress Monitoring. Cia did  only Progress Monitoring while Ken skipped Pre-assessment.  This type of information can be used in the identification of those  students that are missing activities, since that could signal a bigger  problem beyond the scope under study. In instances where a  teacher shows an uncommon pattern consistently regardless of the  scope or grade taught, might indicate lack of training or  understanding in how to administer the assessment instruments,  thus requiring additional training and ultimately improving his or  her teaching practices.       Figure 6. Partial list of 4th-grade student progress for all scopes grouped by topic. Red arrows   indicate grade drop from previous assessment, while green arrows show grade increase from   previous assessment.   215    Table 3. Sequence of completion of student   assessments in one class.   4. QUALITATIVE DATA STRATEGY In this section, we discuss the second part of our data analytics  strategy. We argue that collecting qualitative data and using them  to complement the analytics data are necessary for both  improvement and evaluation. It can be difficult to scale a high  quality qualitative data collection strategy, but it is difficult, if not  impossible, to really understand how the online curriculum is  being used without spending time observing its use in classrooms  and talking to teachers about their use.    The need for a qualitative component reflects a key way in which  K-12 LA is different from LA in higher education. Specifically, it  reflects the difference in access to technology: while institutions  of higher education tend to have up-to-date technology, including  computer labs, laptops for loan, reliable internet connections, and  even well-wired students, the same cannot be said for K schools. There is much variation, for example, among schools  based on socioeconomic differences or based on urbanicity [ Much research has documented that a lack of access to computers  and other technology is a major barrier to the implementation of  online or computer-assisted learning. It also, therefore, is a major  barrier to the collection of reliable analytics data. with the teachers directly, it is possible to uncover details about  the context and quality of implementation. Below, we describe the  methods we have adopted as part of our qualitative strategy:  survey, focus groups, and observations.   4.1 Survey  The research team created and administered a survey to over 700  STEMscopes teachers in a local district as part of the st mentioned above. The survey was created based on existing  surveys about technology and data use and contains 93 items.  While some of these items are specific to the STEMscopes  curriculum and its particular offerings, other items are  generalizable to other technology or curricula. The survey asks  teachers about the following concepts: teacher notions of data and  data use, how teachers use STEMscopes, attitudes toward  STEMscopes and the data dashboard, challenges to using  STEMscopes and the dashboard, and district and school  for using STEMscopes and the dashboard. The survey was piloted  and validated in the fall of 2012, and then administered for the  purpose of our ongoing study; 210 teachers completed surveys.   Student Test 1 Test 2   Tay Pre-assessment     Kim Pre-assessment     Mar Pre-assessment     Bia Pre-assessment Progress Monitoring    Esm Pre-assessment Progress Monitoring    Kat Pre-assessment Progress Monitoring    Lia Pre-assessment Progress Monitoring Standard Assessment   Chr Pre-assessment Progress Monitoring Standard Assessment   Jas Pre-assessment Progress Monitoring Standard Assessment   Ash Pre-assessment Progress Monitoring Standard Assessment   Bet Pre-assessment Progress Monitoring Standard Assessment   Eli Pre-assessment Progress Monitoring Standard Assessment   Fer Pre-assessment Progress Monitoring Standard Assessment   Ken Pre-assessment Progress Monitoring Standard Assessment   Kar Pre-assessment Progress Monitoring Standard Assessment   Lau Pre-assessment Progress Monitoring Standard Assessment   Seb Pre-assessment Progress Monitoring Standard Assessment   Cou Pre-assessment Progress Monitoring Standard Assessment   Hop Pre-assessment Standard Assessment    Tal Pre-assessment Standard Assessment Progress Monitoring   Cia Progress Monitoring     Ken Progress Monitoring Standard Assessment   Sequence of completion of student   STRATEGY  In this section, we discuss the second part of our data analytics   collecting qualitative data and using them  to complement the analytics data are necessary for both   It can be difficult to scale a high  quality qualitative data collection strategy, but it is difficult, if not   lly understand how the online curriculum is  being used without spending time observing its use in classrooms   The need for a qualitative component reflects a key way in which  ducation. Specifically, it   reflects the difference in access to technology: while institutions  date technology, including   computer labs, laptops for loan, reliable internet connections, and  he same cannot be said for K-12   schools. There is much variation, for example, among schools  based on socioeconomic differences or based on urbanicity [36].  Much research has documented that a lack of access to computers   rier to the implementation of  assisted learning. It also, therefore, is a major   barrier to the collection of reliable analytics data. By interacting  with the teachers directly, it is possible to uncover details about   ity of implementation. Below, we describe the  methods we have adopted as part of our qualitative strategy:   The research team created and administered a survey to over 700  STEMscopes teachers in a local district as part of the study  mentioned above. The survey was created based on existing  surveys about technology and data use and contains 93 items.  While some of these items are specific to the STEMscopes  curriculum and its particular offerings, other items are   her technology or curricula. The survey asks  the following concepts: teacher notions of data and   , attitudes toward  and the data dashboard, challenges to using   district and school supports  The survey was piloted   , and then administered for the  purpose of our ongoing study; 210 teachers completed surveys.    And while there were many interesti discussing what we learned about teachers access to technology  in their schools and classrooms (see table  teachers (89%) have computers in their classrooms, but almost  half (47%) of those teachers have only of those is for the teachers exclusive use, meaning that students  have access to one to two computers for their own activities.  Another 45% of teachers have four to six computers in their  classrooms. The implication of these fin activities available to the students online is not likely; it simply is  not feasible for the vast majority of the teachers who responded to  the survey.   4.2 Focus Groups and Observations The research team also has conducted several focus groups and  observed over ten days of instruction to understand how teachers  useand want to usethe curriculum and data dashboard. example, in the summer of 2012, the research team conducted a  focus group with thirteen teachers and two science specialists to  gather feedback on the design of the data dashboard, which allows  teachers to view their students and assign work.  expressed interest in an option that would curriculum usage, in addition to seeing progress. Since the summer focus group, these elements have  been added to the dashboard and are in use currently 3).   The focus group participants also suggested the creation of a  feedback mechanism where they could write comments about  their experience using curriculum resources and sharing them with  other teachers. We believe that this mechanism will tremendously  improve our analytics strategy considering the nearly fifty  thousand teachers that presently use our curriculum. However,  analyzing the content of these open-ended comments requires the  adoption of techniques from natural language processing (NLP),  ontologies, digital libraries, and information retrieval.  Through classroom observations, we have seen how teachers and  students interactand do not interact curriculum. Observations, though time because they allow you to see and then compare and classify interactions. Teachers often unwittingly ove they use the curriculum, and observations can cut through the  problems of self-reporting and perceptions.   In the three classrooms we observed,  about the context and quality of implementation. For instance,  there were no more than two computers available to students, and  most of the teachers either printed materials from the website, or  projected the website onto a whiteboard. This finding can help  explain variation in teacher use. Similarly, we observed that whil  Test 3  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Standard Assessment  Progress Monitoring  Table 4: Access to computers  And while there were many interesting findings, it is worth  discussing what we learned about teachers access to technology  in their schools and classrooms (see table 4). A large majority of  teachers (89%) have computers in their classrooms, but almost  half (47%) of those teachers have only one to three. Usually one  of those is for the teachers exclusive use, meaning that students  have access to one to two computers for their own activities.  Another 45% of teachers have four to six computers in their  classrooms. The implication of these findings is that full use of  activities available to the students online is not likely; it simply is  not feasible for the vast majority of the teachers who responded to   and Observations  team also has conducted several focus groups and   observed over ten days of instruction to understand how teachers  the curriculum and data dashboard. For   n the summer of 2012, the research team conducted a  rteen teachers and two science specialists to   gather feedback on the design of the data dashboard, which allows  teachers to view their students and assign work. Participants   that would depict their own  ddition to seeing metrics on student   Since the summer focus group, these elements have  been added to the dashboard and are in use currently (see section   The focus group participants also suggested the creation of a  k mechanism where they could write comments about   their experience using curriculum resources and sharing them with  other teachers. We believe that this mechanism will tremendously  improve our analytics strategy considering the nearly fifty   rs that presently use our curriculum. However,  ended comments requires the   adoption of techniques from natural language processing (NLP),  ontologies, digital libraries, and information retrieval.   ations, we have seen how teachers and  and do not interactwith the online   Observations, though time-consuming, are important  and then compare and classify   interactions. Teachers often unwittingly over-report how often  they use the curriculum, and observations can cut through the   reporting and perceptions.    n the three classrooms we observed, we learned a great deal  about the context and quality of implementation. For instance,   were no more than two computers available to students, and  the teachers either printed materials from the website, or   the website onto a whiteboard. This finding can help  Similarly, we observed that while   : Access to computers.   216    some teachers followed the curriculum very carefully, others used  fewer of the activities and instead incorporated other available  resources. In talking with teachers, we also learned that because of  the dearth of computers, teachers often share accounts, which can  skew the data: one teachers user account may actually contain  data on several teachers use. We also observed that in two  classrooms, the students used their teachers authentication  information to log in, which can help explain outliers.   By visualizing how elements in the curriculum are used (and not  used), we have identified three implications for our learning  analytics strategy going forward. First, we will examine how  patterns of teacher curriculum usage relate to student outcomes in  science. These data can serve as validating the 5E+I/A  pedagogical model that underpins STEMscopes. Second, we can  investigate further why there are so many different usage patterns  by administering surveys and asking teachers more targeted  questions, including about their access to technology and training.  Finally, analysis of these data can shed light onto ways to improve  the user interface, such as how content and activities are written  and presented to the teachers and students.   5. CONCLUSION AND FUTURE WORK  LA has the potential to bring important changes to K-12 education  if implemented with a coherent, contextualized strategy, such as  the one briefly described in this paper. The STEMscopes LA  strategy has shed light on interesting finding and trends, such as  the fact that teachers use the Explore step the most and do not  have access to many computers in their classrooms. But it also has  raised new questions and highlighted several challenges facing the  LA community as it integrates further into the K-12 education  setting.   In some ways, the strategy we have developed creates as many  new questions as it answers. This point, of course, simply  highlights the complex nature of K-12 education. Two important  lessons that we have learned and that can inform similar efforts by  other researchers to incorporate LA at the K-12 level include the  need to drill down into variables to account for the various  dimensions of use and time, the need to undertake qualitative  research, and to talk to and observe teachers.   Future LA work that STEMscopes will undertake includes  examining use patterns and relating the various dimensions of use  to student assessments. These, no doubt, will present the team  with fresh challenges as the complexity of task becomes apparent.  There also are several hurdles that the LA community will have to  overcome or address to ensure that these new data collection and  analysis tools do, in fact, fulfill their promise. First, schools must  address the gap in access to technology. While some schools have  achieved one-to-one computing, most schools are not even close  to this goal, and this has profound implications for our ability to  collect reliable analytics data. Our data revealed that teachers and  students often share accounts, and that students are limited in the  activities they can complete online. This, in turn, means that the  analytics data we collect may not be reliable: for some teachers,  the usage analytics may accurately portray their use, while for  others, they may not.    A second and equally important challenge is time. Teachers often  do not have time to incorporate technology and online activities  into their regular instruction. It is time-consuming to plan for this  incorporation, especially because it often requires the teacher to  learn new software or strategies, and because it requires the  teacher to create new classroom and management routines. The  result of this hurdle is that the computer systems that should be   collecting analytics data are not fully implementedor are not  implemented at all, which degrades the quality of data we can  collect.   Though not a panacea for the challenges facing K-12, a strong LA  strategy can help educators make sense of and leverage the large  amounts of data generated by the increased use of mobile devices,  computers, and other technology in classrooms. With an improved  understanding of data, students can learn from their own mistakes.  Teachers can improve their lessons, work with students who need  extra help and provide extensions for those ready to move on.  Analysts can improve the curriculum and the resources available  to teachers and students. The strategy described in this paper was  developed to analyze detailed usage patterns of the online science  curriculum, STEMscopes. This approach enables analysis of large  amounts of data and opens the possibilities to complex  undertakings such as the creation of personalized learning  environments and targeted professional development programs for  educators.   6. ACKNOWLEDGMENTS  We would like to thank STEMscopes editorial and production  teams that make possible our science curriculum. Specials thanks  go to the teachers and science specialists that participated in the  focus group. Finally, thanks to Dan Hoyt and Lizzie Bell for  reviewing this manuscript.   7. REFERENCES  [1] Arnold, K. 2010. Signals: Applying Academic Analytics,   EDUCAUSE Quarterly 33, 1.  http://www.educause.edu/EDUCAUSE+Quarterly/EDUCAU SEQuarterlyMagazineVolum/  SignalsApplyingAcademicAnalyti/199385   [2] Atkinson, M. and Wiley, D. 2011. Learning analytics as  interpretive practice: applying Westerman to educational  intervention. Proceedings of the 1st International Conference  on Learning Analytics and Knowledge. ACM Press, New  York. 117-121.   [3] Baker, R., Corbett, A., Koedinger, K., and Roll, I. 2006.  Generalizing Detection of Gaming the System Across a  Tutoring Curriculum. In Proceedings of the 8th International  Conference on Intelligent Tutoring Systems. Berlin,  Heidelberg: Springer-Verlag, 402411.   [4] Baker, R., Corbett, A., Koedinger, K., and Wagner, A. 2004.  Off-Task Behavior in the Cognitive Tutor Classroom: When  Students Game the System. In Proceedings of the SIGCHI  Conference on Human Factors in Computing Systems (CHI  '04). New York, NY: ACM, 383390.   [5] Baker, S. and Yacef, K. 2009. The State of Educational Data  Mining in 2009: A Review and Future Versions. Journal of  Educational Data Mining, 1(1), 3-17.   [6] Beck, J. and Mostow, J. 2008. How who should practice:  Using learning decomposition to evaluate the efficacy of  different types of practice for different types of students. In  Proceedings of the 9th International Conference on   Intelligent Tutoring Systems, 353-362.   [7] Bienkowski, M., Feng, M., and Means, B. 2012. Enhancing  Teaching and Learning through Educational Data Mining  and Learning Analytics: An Issue Brief. Washington, DC:  SRI International.   [8] Bybee, R., Taylor, J., Gardner, A., Van Scotter, P., Powell,  J., Westbrook, A., and Landes, N. 2006. The BSCS 5E   217    instructional model: Origins and effectiveness. Science.  Colorado Springs.  http://science.education.nih.gov/houseofreps.nsf/b82d55fa13 8783c2852572c9004f5566/$FILE/Appendix%20D.pdf   [9] Chipman, S. 2010. Applications in Education and Training:  A Force Behind the Development of Cognitive Science.  Topics in Cognitive Science, 2(3), 386-397.   [10] Cogburn, D., Ramnarine-Rieks, A., Espinoza, F., and  Levinson, N. 2009. Learning Across Borders: Socio- Technical Strategies for Globally Distributed Teaching and  Learning. Proceedings of the 2nd International Conference  of Education, Research and Innovation, 1930-1941.   [11] Culatta, R. 2012. From Innovation Clusters to Datapalooza:  Accelerating Innovation in Educational Technology.  November 1, 2012:  https://www.educause.edu/ero/article/innovation-clusters- datapalooza-accelerating-innovation-educational-technology.   [12] DeSantis, N. 2012. A Boom Time for Education Start-Ups:  Despite Recession Investors See Technology Companies  Internet Moment. The Chronicle of Higher Education.  March 18, 2012: http://chronicle.com/article/A-Boom-Time- for-Education/131229/.   [13] Duval, E. 2011. Attention Please! Learning Analytics for  Visualization and Recommendation. Proceedings of LAK11:  1st International Conference on Learning Analytics and   Knowledge.   [14] Elias, T. 2011. Learning analytics: Definitions, processes,  and potential.    [15] Farzan, R. 2004. Adaptive socio-recommender system for  open-corpus e-learning. In doctoral consortium of the third  international conference on adaptive hypermedia and   adaptive web-based systems.   [16] Ferguson, R. 2012. The State of Learning Analytics in 2012:  A Review and Future Challenges. Technical Report KMI-12- 01. http://kmi.open.ac.uk/publications/pdf/kmi-12-01.pdf     [17] First International Conference on Learning Analytics and  Knowledge. Baniff, Alberta, Canada. February 27-March 1,  2011. https://tekri.athabascau.ca/analytics/   [18] Glenberg, A. 1976. Monotonic and non-monotonic lag  effects in paired-associate and recognition memory  paradigms. Journal of Verbal Learning and Verbal Behavior,  15, 1-16.   [19] Goldstein, P. and Katz, R. 2005. Academic Analytics: The  Uses of Management Information and Technology in Higher  Education, ECAR Research Study Volume 8.  http://www.educause.edu/ers0508   [20] Ha, S., Bae, S., and Park, S. 2000. Web mining for distance  education. In IEEE international conference on management  of innovation and technology, 715719).   [21] Hamalainen, W., Suhonen, J., Sutinen, E., and Toivonen, H.  2004. Data mining in personalizing distance education  courses. In World conference on open learning and distance  education.   [22] Heer, J. and Shneiderman, B. 2012. Interactive Dynamics for  Visual Analysis. Communications of the ACM, 55(4), 45-54.   [23] Hendricks, M., Plantz, M., and Pritchard, K. 2008.  Measuring outcomes of United Way- funded programs:  Expectations and reality. In J.G. Carman & K.A. Fredricks   (Eds.), Nonprofits and evaluation. New Directions for  Evaluation, 119, 1335.   [24] Heraud, J., France, L., and Mille, A. 2004. Pixed: an its that  guides students with the help of learners interaction log. In  International conference on intelligent tutoring systems   (workshop analyzing student tutor interaction logs to   improve educational outcomes), Maceio, 5764.   [25] Ingram, D., Seashore Louis, K., and Schroeder, R. 2004.  Accountability policies and teacher decision making:  Barriers to the use of data to improve practice. Teachers  College Record. 106(6), 1258-1287.   [26] Jagadish, H.  2012. Big Data: Its Not Just the Analytics.  May 2012 ACM SIGMOD Blog.  http://wp.sigmod.org/p=430.   [27] Junco, R. 2012. Most ed-tech startups suck! Heres where  they go wrong. http://venturebeat.com/2012/10/28/most-ed- tech-startups-suck-heres-where-theyre-going-wrong/.   [28] Kerr, K. A., Marsh, J. A., Ikemoto, G. S., Darilek, H., and  Barney, H. 2006. Strategies to promote data use for  instructional improvement: Actions, outcomes, and lessons  from three urban districts. American Journal of Education,  112(4), 496-520.   [29] Lachat, M. 2005. Practices that support data use in urban  high schools. Journal of Education for Students Placed at  Risk. 10(3), 333-349.   [30] Landauer, T.K. and Bjork, R. A. 1978. Optimum rehearsal  patterns and name learning. In M. M. Gruneberg, P. E.  Morris, & R. N. Sykes (Eds.), Practical aspects of memory,  625-632. London: Academic Press.   [31] Long, P.  and Siemens G. 2011. Penetrating the Fog:  Analytics in Learning and Education. EDUCAUSE Review,  46(5). http://www.educause.edu/ero/article/penetrating-fog- analytics-learning-and-education   [32] Lu, J. 2004. Personalized e-learning material recommender  system. In International conference on information  technology for application, 374379.   [33] Mazza R., and Dimitrova, V. 2004. Visualising student  tracking data to support instructors in web-based distance  education, Wide Web conference on Alternate track papers  & posters. New York, NY, USA: ACM Press, 154-161.  http://www.iw3c2.org/WWW2004/docs/2p154.pdf   [34] Means, B., Padilla, C., and Gallagher, L. 2010. Use of  Education Data at the Local Level: From Accountability to  Instructional Improvement. Washington, DC: U.S.  Department of Education, Office of Planning, Evaluation and  Policy Development.   [35] Merceron, A., and Yacef, K. 2005. Tada-ed for educational  data mining. Interactive Multimedia Electronic Journal of  Computer-Enhanced Learning, 7(1), 267287.   [36] Mossberger, K., Tolbert, C. J., and Gilbert, M. 2006. Race,  place, and informational technology. Urban Affairs Review,  41(5), 583-620.   [37] Norris, D., Baer, L., Leonard, J., Pugliese, L. and Lefrere, P.  2008. Action Analytics: Measuring and Improving  Performance That Matters in Higher Education,  EDUCAUSE Review 43(1).  http://www.educause.edu/EDUCAUSE+Review/EDUCAUS  218    EReviewMagazineVolume43  /ActionAnalyticsMeasuringandImp/162422   [38] Oblinger, D. and Campbell, J. 2007. Academic Analytics,  EDUCAUSE White Paper.  http://www.educause.edu/ir/library/pdf/PUB6101.pdf   [39] Pahl, C. 2004. Data mining technology for the evaluation of  learning content interaction. International Journal on E- Learning., 3(4).   [40] Pardo, A. and Delgado, C. 2011. Stepping out of the box.  Towards analytics outside the Learning Management  System, 163-167. In International Conference on Learning  Analytics.   [41] Pavlik, P., Cen, H., and Koedinger, K. 2009. Learning  Factors Transfer Analysis: Using Learning Curve Analysis to  Automatically Generate Domain Models. Proceedings of the  2nd International Conference on Educational Data Mining,  121-30.   [42] Pavlo, A., Paulson, E., and Rasin, A. 2009. A Comparison of  Approaches to Large-Scale Data Analysis. In Proceedings of  the 2009 ACM SIGMOD Conference.   [43] President's Council of Advisors on Science and Technology,  Prepare and Inspire: K-12 Education in Science, Technology,  Engineering, and Math (STEM) for America's Future  (Executive Office of the President, Washington, DC, 2010);  www.whitehouse.gov/sites/default/files/microsites/ostp/pcast -stem-ed-final.pdf.   [44] Reif, F. 2008. Applying Cognitive Science to Education:  Thinking and Learning in Scientific and Other Complex  Domains. MIT Press. Cambridge, MA.   [45] Roediger, H. and Karpicke, J. 2010. Intricacies of spaced  retrieval: A resolution. In A. S. Benjamin (Ed.), Successful  remembering and successful forgetting: Essays in honor of   Robert A. Bjork. New York: Psychology Press.   [46] Romero C. and Ventura, S. 2010. Educational Data Mining:  A Review of the State of the Art. IEEE Transactions on  Systems, Man and Cybernetics, Part C: Applications and   Reviews 40 (6): 601618.   [47] Siemens, G. 2012. Learning Analytics: Envisioning a  Research Discipline and a Domain of Practice. Proceedings  of the 2nd International Conference on Learning Analytics &   Knowledge.   [48] Suthers, D., Ravi, V., Medina, R., Joseph, S., and Dwyer, N.  2008. Beyond threaded discussion: representational guidance  in asynchronous collaborative learning environments.  Computers & Education, 50, 1103-1127.   [49] Talavera, L., and Gaudioso, E. 2004. Mining student data to  characterize similar behavior groups in unstructured  collaboration spaces. In Workshop on artificial intelligence  in CSCL. 16th European conference on artificial   intelligence, 1723.   [50] Tang, T., and McCalla, G. 2002. Student modeling for a  web-based learning environment: A data mining approach. In  Eighteenth national conference on artificial intelligence,  Menlo Park, CA, USA, 967 968.   [51] Tytler R. and Prain, V. 2009. A Framework for Re-thinking  Learning in Science from Recent Cognitive Science  Perspectives. International Journal of Science Education.  32(15), 2055-2078.   [52] Vatrapu, R., Tplovs, C., Fujita, N., and Bull, S. 2011.  Towards a visual analytics for teachers' dynamic diagnostic  pedagogical decision-making. Proceedings of the IEEE, 93- 98.   [53] Venner, J. 2009. Pro Hadoop. Apress. New York.   [54] Watters, A. 2012. Learning Analytics: Lots of Education  Data... Now What  http://hackeducation.com/2012/05/04/learning-analytics- lak12/   [55] Wayman, J. C. 2005. Involving Teachers in Data-Driven  Decision Making: Using Compuer Data Systems to Support  Teacher Inquiry and Reflection. Journal of Education for  Students Placed at Risk. 10(3).   [56] Wayman, J. C., and Cho, V. 2009. Preparing educators to  effectively use student data systems. In T. Kowalski (Ed.),  Handbook of data-based decision making in education, 89- 104.). New York: Taylor & Francis.   [57] Wayman, J., Cho, V., Jimerson, J., and Spikes, D. 2012.  District-wide effects on data use in the classroom. Education  Policy Analysis Archives, 20(25).   [58] Whitaker, J. R. 2012. Responding to the need for  intervention: Six easy steps prime students for mastery of  science concepts. Science and Children, 50(4), 75-79.      219      "}
{"index":{"_id":"32"}}
{"datatype":"inproceedings","key":"Dyckhoff:2013:SAR:2460296.2460340","author":"Dyckhoff, A. L. and Lukarov, V. and Muslim, A. and Chatti, M. A. and Schroeder, U.","title":"Supporting Action Research with Learning Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"220--229","numpages":"10","url":"http://doi.acm.org/10.1145/2460296.2460340","doi":"10.1145/2460296.2460340","acmid":"2460340","publisher":"ACM","address":"New York, NY, USA","keywords":"action research, impact analysis, indicators, learning analytics","abstract":"Learning analytics tools should be useful, i.e., they should be usable and provide the functionality for reaching the goals attributed to learning analytics. This paper seeks to unite learning analytics and action research. Based on this, we investigate how the multitude of questions that arise during technology-enhanced teaching and learning systematically can be mapped to sets of indicators. We examine, which questions are not yet supported and propose concepts of indicators that have a high potential of positively influencing teachers' didactical considerations. Our investigation shows that many questions of teachers cannot be answered with currently available research tools. Furthermore, few learning analytics studies report about measuring impact. We describe which effects learning analytics should have on teaching and discuss how this could be evaluated..","pdf":"Supporting Action Research with Learning Analytics  A.L.Dyckhoff, V. Lukarov, A. Muslim, M.A. Chatti, U. Schroeder   Learning Technologies Research Group, RWTH Aachen University  52056 Aachen, Germany   {dyckhoff, chatti,schroeder}@informatik.rwth-aachen.de,   muslim@cil.rwth-aachen.de, and vlatko.lukarov@rwth-aachen.de         ABSTRACT  Learning analytics tools should be useful, i.e., they should be  usable and provide the functionality for reaching the goals  attributed to learning analytics. This paper seeks to unite learning  analytics and action research. Based on this, we investigate how  the multitude of questions that arise during technology-enhanced  teaching and learning systematically can be mapped to sets of  indicators. We examine, which questions are not yet supported  and propose concepts of indicators that have a high potential of  positively influencing teachers didactical considerations. Our  investigation shows that many questions of teachers cannot be  answered with currently available research tools. Furthermore,  few learning analytics studies report about measuring impact. We  describe which effects learning analytics should have on teaching  and discuss how this could be evaluated.    Categories and Subject Descriptors  J.1 [Computer Applications]: Administrative Data Processing   Education.   General Terms  Documentation, Design, Human Factors, Theory   Keywords  Learning Analytics, Action Research, Indicators, Impact Analysis   1. INTRODUCTION  Teaching is a dynamic activity, which should constantly be  monitored and adjusted to the demands of changing social  contexts and needs of the learners, to ensure high quality. This  implies that teachers need to be aware about teaching and learning  processes. Moreover, they should constantly analyze, self-reflect,  regulate and update their didactical methods and the learning  resources they provide to their students.    Reflection has been defined as a conscious activity, exploring  ones experience, in order to gain new insight and appreciation [7].  It can foster learning, if reflection is embedded in a cyclical  process of active experimentation, where concrete experience  forms the basis for observation and reflection [36]. This is also  true for action research. Action research is a method for reflective  teaching practice that enables and guides teachers to investigate  and evaluate their work [2]. Hinchey defines action research as  a  process of systematic inquiry, usually cyclical, conducted by those   inside a community rather than outside experts; its goal is to  identify action that will generate improvement the researchers  believe important  [31]. We believe that learning analytics tools  could initiate and support action research, if they are designed to  meet action research requirements, as also discussed in [16].  The field of learning analytics (LA) has been defined in several  ways; see [21, 24, 33, 53]. In the context of our research, we  understand learning analytics as the development and exploration  of methods and tools for visual analysis and pattern recognition in  educational data to permit institutions, teachers, and students to  iteratively reflect on learning processes and, thus, call for the  optimization of learning designs [39, 40] on the on hand and aid  the improvement of learning on the other [14, 15].    Although the goals behind action research and LA are very  similar, a difference can be seen in the initial trigger of related  study projects. While action research projects usually start with a  research question that arises from teaching practice [2], learning  analytics projects often evolve based on observations made with  regard to already collected data. Action research projects,  therefore, also often use qualitative methods to generate a holistic  picture of the learning situation, while learning analytics are  mostly based on quantitative methods (more details in table 3).    In terms of LA, the creation of indicators has been controlled by  and based on the data available in learning environments. Hence,  the indicators might solely represent information that depends on  the data sources used, e.g., by just making data visible that has  been unseen, unnoticed, and therefore unactionable [13]. An  important principle of action research is first to think about the  questions that have to be answered before deciding about the  methods and data sources [31]. Asking questions independently  and putting aside the fact whether the necessary data is available  or not, could provide more insightful information about the  requirements analysis. This will improve the design of future LA  tools and learning environments.   The following assumptions and research questions let to the  investigations at hand:   Q1. Indicator-Question-Mapping: Important questions of  teachers remain unanswered by LA tools: Current LA  Implementations fail to answer several important  questions of teachers, e.g., concerning correlations of  student profile data with other sources. Which questions  cannot be mapped to the available (sets of) indicators  Which indicators could deliver what kind of  enlightenment   Q2. Teacher-Data-Indicators: There is a need for including  teacher data into indicators: Current indicators do NOT  explicitly relate teaching and teaching activities to  student learning. Are there tools that explicitly correlate  teacher data with student data How should teacher data  be correlated      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK13, April 0812, 2013, Leuven, Belgium.  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.        220    Q3. Missing Impact Analysis: Current LA research focuses  on supporting teachers and students in their tasks, but  fails to prove the impact of LA tools on stakeholders  behaviors. How could LA impact teaching How could  this impact be evaluated   The remainder of this paper proceeds as follows: Section 2  (Methods) gives an overview on our research procedure and  materials. Section 3 (Categorization of Indicators) structures the  materials and the used related works. This is the foundation for  section 4 (Analysis and Discussion). This analysis constitutes the  consolidation of the previously described data, and serves as the  basis for answering our research questions. We discuss our  findings and limitations of the investigation. Finally, section 5  (Conclusion) summarizes the main accomplishments of our study  and gives an outlook on future tasks.   2. METHODS  We conducted our research in several steps. First, we used the  results of a former qualitative meta-analysis, which investigated  what kind of questions teachers ask while performing action  research in their technology-enhanced learning (TEL) scenarios  [22]. A summary of this study is provided below in section 2.1.  Furthermore, we collected publications on LA tools and available  indicators (section 2.2). For systematic reasons, two researchers  from our research group independently developed a categorization  scheme for the list of 198 collected indicators (section 3). The  categories were merged into one scheme, which is presented in  section 3. This categorization scheme helped us to get an  overview of what is currently present and available in the research  field. It supported the further analysis of LA tools and indicators.  Last, the same two researchers performed and analyzed the  mapping of teachers questions to sets of available indicators  independently from each other and a second time together. The  consolidation of our findings helped us to find questions that did  not yet have corresponding indicators.    2.1 Teachers Questions  Dyckhoff structures the collected questions by the purposes and  methods used to investigate them [22]. Table 1 shows the six  resulting categories, including examples.   Questions of category (A) mainly serve exploratory purposes.  They can be investigated by qualitative methods, such as  interviews with students or qualitative and quantitative surveys. If  the learning environment offers rating features, these can be used  as well. Questions of category (B) could be investigated by using  quantifiable data from surveys and/or log files. Several of the  questions fit into more than one category, due to the nature of the  categorization. For instance, category (E) is a collection of  questions investigating correlations, proportions and comparisons  by combining data from different sources, like (A) and (B).  Questions of the categories (C) and (D) seek to enlighten teachers  on how diverse resources and activities might aid different groups  of students in learning. Finally, category (F) refers to questions  concerned with all kinds of performance measures [22].   2.2 Learning Analytics Tools  We included the following LA projects in our study on indicators:  LOCO-Analyst [1, 4], TADA-Ed [46], Data Model to Ease  Analysis and Mining [38], Student Inspector [50], MATEP [56 58], CourseVis [43, 45], GISMO [44], Course Signals [3], Check  My Activity [25], Moodog [54, 55], TrAVis [41, 42], Moodle  Mining Tool [48], EDM Vis [34], AAT [29], Teacher ADVisor  [37], E-learning Web Miner [26], ARGUNAUT [30], Biometrics- based Student Attendance Module [27], CAMera and Zeitgeist- Dashboard [51, 52], Student Activity Meter [28], Discussion  Interaction Analysis System (DIAS) [811], CoSyLMSAnalytics  [49], Network Visualization Resource and SNAPP [5, 6, 17, 18],  i-Bee [47], iHelp [12], and Participation Tool [32]. We limited our  analysis on available indicators to these projects because they  have been presented in literature as state-of-the-art LA-tools,  which can already be used by their intended target users.    (A) Qualitative evaluation     How do students like/rate/value  specific learning offerings   How difficult/easy is it to use the  learning offering   Why do students appreciate the  learning offering   (B) Quantitative measures of use /  attendance   When and how long are student  accessing specific learning offerings  (during a day)    How often do students use a learning  environment (per week)   Are there specific learning offerings  that are NOT used at all   (C) Differentiation between groups of  students   By which properties can students be  grouped   Do native speakers have fewer  problems with learning offerings than  non-native speakers   How is the acceptance of specific  learning offerings differing according  to user properties (e.g. previous  knowledge)   (D) Differentiation between learning  offerings   Are students using specific learning  materials (e.g. lecture recordings) in  addition or alternatively to  attendance   Will the access of specific learning  offerings increase if lectures and  exercises on the same topic are  scheduled during the same week   (E) Data consolidation / Correlation     How many (percent of the) learning  modules are student viewing   Which didactical activities facilitate  continuous learning   How do learning offerings have to be  provided and combined to with  support to increase usage   (F) Effects on Performance     How do those low achieving students  profit by continuous learning with e- test compared to those who have not  yet used the e-tests    Is the performance in e-tests  somehow related to exam grades      Table 1. Classification and examples of questions asked by teachers in TEL (cp. [22])   221    3. CATEGORIZATION OF INDICATORS  On basis of our literature review, we developed a categorization  scheme with the purpose to structure the indicators. Table 2 shows  that five categories are related to the perspectives and six  categories are related to the data sources that the indicators  implement. The concept of perspective refers to the point of view  a user might have on the same data. Our categorization  distinguishes the perspectives into: individual student, group,  course, content, and teacher. The user might, e.g., be interested in  the usage behavior of a single student in relation to the whole  group. Another perspective might be the content, e.g., how a  specific resource was used. These examples show that the same  data can be used to answer different questions and be in line with  different perspectives. Note that our categorization of perspectives  is similar to the work of Dimitracopoulou [20], who distinguishes  four general points of view of interaction analysis indicators:  individual, group, community and society. During our research, it  became apparent that a high percentage of indicators rely on basic  usage data, generated by students while using a single e-learning   system. This is clearly not enough for creating a holistic picture of  the students learning process. Hence, we additionally chose a  categorization scheme related to data sources (student generated  data, context/local data, academic profile, evaluation,  performance, or course meta-data) to make sure that those  indicators that include or even combine different data sources  become more visible within our analysis.   In the following two subsections we will introduce each category  of table 2 by describing how teachers and students can use the  related indicators and give concrete indicator examples from the  literature. Note that all indicators relate to more than one category.   3.1 Perspective Categories  Individual Student: This category refers to indicators that are  supposed to inspire an individual students self-reflection on  his/her learning and academic success. Indicators process data and  visualize information about a single student. For instance, the  indicators could measure how active a student is. Activity is  derived from the students generated data, and it can be divided                                                                                            Categorization                   Indicator Examples   Pe rs  pe ct  iv e   In di  vi du  al  S  tu de  nt    G ro  up    C ou  rs e   C on  te nt     Te ac  he r   D at  a  so  ur ce     St ud  en t-G  en er  at ed   D at  a   C on  te xt  /L oc  al  D  at a   A ca  de m  ic  P  ro fil  e   Ev al  ua tio  n   C ou  rs e-  re l.   Pe rf  or m  an ce     C ou  rs e   M et  a- D  at a   Number of content pages viewed per student [54]      X          X       Number of threads started per student [42, 45, 55]  X     X       Number of assignments submitted per student [41, 54] X     X    X   Relation between keywords and students [47] X     X       Student risk group/status [3] X     X  X  X   Social network derived from email exchange data per student [51] X      X      Keyword analysis derived from email exchange data per student [51] X      X      Resources watched (read) in a session per student (via webcam) [27] X     X X      Students gender [26, 58] X  X     X     Students academic level [58] X  X     X     Students tags of learning resources [51] X   X     X    Work amount (message dimension per user) per course phase [49] X          X  Number of participants per group [25, 41]  X    X       Number of files per group [41]  X    X       Advice to groups concerning uncommunicative behavior [37]  X    X       Global accesses to the course [45]   X   X       Resources that have NOT been accessed (weekly, daily, hourly) [55]   X   X       Learning paths analysis [51, 58]   X   X       SNA: actors degree centrality per course phase [49]   X   X   X    Students comprehension of topics (based on annotations) [1]   X          Avg. overall quiz score [1]   X       X   Clusters of students who made a (specific) mistake [46, 50]   X       X   Advice to the teacher concerning excellent and weak students [37]   X       X   Avg. number of contributions per course phase [49]   X   X     X  Avg. number of incorrect answers per question in a quiz [1]    X      X   Number of unique users per resource (weekly, daily, hourly) [55]    X  X       Resources frequently used together (forum, mail, etc.) [27]    X  X   X    Top5 pages/resources [58]    X  X        Table 2. For illustration, this table shows a sample of the overall 198 indicators. Each indicator was categorized according to  its perspective and related data sources as state in its respective publications.   222    into receptive and active activities: Receptive activities are  accessing or reading resources, listening to audio, or watching  lecture videos. Active participation is based on contributions on  the forums, discussion participation, editing wiki pages, uploading  assignments, as well as sharing resources. The indicators also  include summarized data from other students. This way, a student  can compare his/her learning progress and success with the rest of  the class. Most of the indicators of the individual student category  can also support teachers in monitoring and assessing a single  students learning process for the purpose of tutoring. They can be  used for quick glances, or in-depth research on how a specific  student is doing in a course. Sophisticated systems might even  recommend learning activities, predict a students success or give  advice. Examples of indicators are: Number of content pages  viewed per student [58], Number of threads started per student  [41, 45, 55], Number of messages read on forum per student [11,  41, 45, 58], Number of messages read by the user in relation to  number of messages available [12], Content currently read by one  or more students [12], Relation between keywords and students  [47], Frequency a student used each keyword [47], Student risk  group/status [3], Advice to uncommunicative advanced students  to help others [37].   Group: This category includes combinations of indicators  dedicated to groups of students for the purpose of self-reflecting  on their groups behavior, learning, and academic success.  Furthermore, items of this category can help the teacher to  monitor and analyze each groups activities, perform inter-group  comparisons, or show the level of collaboration among students in  a group. Examples include: Number of participants per group [25,  41], Avg. number of posts per group [11], Number of messages  quoted per group [41], Number of files per group [41], Avg.  thread depths/weight [11], Relative activity regarding number of  posts, number of types, number of initiated threads per group [11],  Advice to groups concerning uncommunicative behavior [37].   Course: The course category is a collection of indicators that  helps the teacher to monitor and analyze the overall course data. A  course follow-up could also provide indicators for the teacher to  review and compare several courses of the same type. Examples  include: Global accesses to the course [45], Number of distinct  users [58], Avg. visit duration [58], Learning paths analysis [51,  58], Resources that have NOT been accessed (weekly, daily,  hourly) [55], SNA: actors degree centrality per course phase [49],  Learner isolation/students with limited connectivity [17, 18]. We  did not find indicator related to course follow-up.   Content: This set of indicators serves the purpose to present the  students interactions with the content of a course. It explicitly  takes the perspective of a resource, lesson, quiz, etc. Examples  include: Number of unique users per resource (weekly, daily,  hourly) [55], Number of revisits per lesson/quiz [1], View counts  per resource (weekly, daily, hourly) [51, 55], Resources  frequently used together (forum, mail, etc.) in each learning  session [26], Top5 pages/resources [58].   Teacher: This class refers to indicators that allow the teacher to  reflect upon his/her teaching, with the purpose of improving it.  These indicators explicitly process teacher-generated data and  correlate it with the student-generated data. We only found few  indicators that corresponds to this category: Sociogram of  interaction between teacher and participant [17, 18].   3.2 Data Source Categories  Student-Generated Data: This set of indicators presents the  students presence online, their usage behavior. These basic   indicators take into consideration session information, number of  visits, hits, duration, and other basic log data available. Other data  generated by students are for example, messages posted to a  discussion forum, files uploaded, etc. Other examples of  indicators include: Course access by student per date [45], Overall  time spent per student (weekly, daily, hourly) [26, 55, 58], Trends  in students activity (based on time spent) [28].   Context/Local Data: These indicators use data that surround the  student, such as, local or mobile data, and influence the learning  context. These indicators might consider the location of the  learner, co-learners nearby, calendar information, email exchange,  homework assignments, deadlines, or exam dates. Furthermore,  they might even use biometric data from webcams or fingerprints.  Examples include: Social network derived from email exchange  data per student [51], Keyword analysis derived from email  exchange data per student [51], Resources watched (read) in a  session per student (via webcam) [27].    Academic Profile: These indicators consider the students  academic profile. In the academic profile demographic  characteristics of the student are included beside other data on  academic progress. The demographic profile includes, e.g.,  gender, age, and mother language. The academic profile could  include field of study, previous knowledge (as in finished courses  and prerequisites), and grades from assignments, quizzes, exams,  and mistakes. Examples are: Students gender [26, 58], Students  academic level [58], Students age [26, 58]. We could not  explicitly find indicators that correlate this data to other sources,  besides: Student risk group/status [3].   Evaluation Data: This is a set of indicators, which consider the  evaluation data provided by students, such as, survey question  answers, annotations, tags of resources, ratings, course  evaluations, one-minute feedback, or questionnaires. Examples  include: Students comprehension of the studied topics (based on  annotations) [1], Students tags of learning resources [51].   Course-related Performance: This category refers to indicators  that use performance-related data, like grades from assignments,  quizzes, and exams, or attempts per quiz questions, mistakes  made, etc. Examples of indicators are: Avg. overall quiz score [1],  Avg. number of incorrect answers per question in a quiz [1],  Mistakes that often come together (a-priori): if students make  mistake A, followed by mistake B, then later they make mistake C  [46], Clusters of students who made a (specific) mistake [46, 50],  Number of assignments submitted per student [41, 58], Advice to  the teacher concerning excellent and weak students relative to the  whole class [37].   Course Meta-Data: Lastly, this class refers to indicators that take  into consideration the course structure, course goals, events,  resources, resources allocation, teaching concept, and the rate of  their implementation. Examples include: Work amount (message  dimension per user) per course phase [49], Avg. number of  contributions per course phase [49].   4. ANALYSIS AND DISCUSSION  In the following, we discuss our findings. Our discussion is  structured into three subsections, which are related to the three  research questions Q1-Q3 mentioned in section 1. A fourth  subsection refers to limitations of our study.   4.1 Indicator-Question-Mapping (Q1)  Our analysis showed that current LA implementations still fail to  answer several important questions of teachers. In the first step of   223    the research activity, we took the list of tools (see, section 2.2)  and identified about 200 indicators. The second step of the  research was to introduce the research questions (see, table 1) to  the indicators (table 2). The third step of our research process was  to map the available indicators to the corresponding questions by  crosschecking them. The idea behind this mapping was that each  indicator provides certain pieces of information that indicate how  a question can be answered. With the help of this question- indicator-mapping, we were able to identify which questions the  indicators could support fully or in part.    Most of the questions from category B (table 1) can be answered.  They are concerned mainly with quantitative measures on the  basis of student generated usage data. Several questions ask if,  when, and how often students are learning online (accessing the  learning environment). Diverse indicators can be used to answer  these questions, such as the time spent online using the system or  a specific resource (weekly, daily, hourly, per student or on  average), the visit duration, and the number of sessions [1, 26, 55,  58]. Since such log data is just an indicator for access of  resources, researchers have also developed means for indicating  that a student is reading online by using biometric data generated  by webcams [27].   There are some other questions that could be answered as well.  But it was not easy to find clear sets of indicators that correspond  to them. During the mapping activity, we faced the difficulty of  missing documentation about which indicators serve which  purpose in which context. This made our work rather subjective,  depending on our individual interpretation of each indicator.   A few questions of category (B) cannot yet be answered, because  they do not yet use specific student generated data, like mobile  sensor data. Examples from this set of questions are: Are  students using specific learning offerings at home or mobile or  How often do students attend lectures   It was not surprising for us that almost all questions of category  A, which are concerned with the satisfaction and preferences of  students, cannot yet be answered sufficiently. For example, we  cannot calculate indicators on how students value specific  learning offerings, or how satisfied the students are with specific  learning offerings. Learning analytics tools would need to  automatically collect and include, e.g., data from surveys and  ratings, in order to support these questions.  The same is true for even more complex questions. Some  indicators could be used, but the resulting interpretations are not  satisfying. One group of questions that cannot be related to any of  the available indicators requires data correlation and combination  from different data sources. Examples of these questions are: Are  students using specific learning materials (e.g. lecture recordings)  in addition or alternatively to attendance or How do learning  offerings have to be provided and combined to with support to  increase usage or Which teaching activities increase learning  activities (e.g. attendance in online discussions) LA indicators  within the tools investigated in the study at hand have not  explicitly addressed these questions. This might be due to data  privacy issues. On the one hand, the more data sources we  combine for indicator calculations, the more we will learn about  the whole learning process. On the other hand, users privacy  concerns legitimately increase. Since there are diverse contexts of  teaching and learning, individual requirements have to be taken  into account, when searching for compromise solutions. An  overview on legal, ethical and related management issues  surrounding analytics in the context of teaching, learning and  research has been provided by [35].    Furthermore, there is a set of complex questions, which require  data from official student academic records/profiles. Examples for  these questions are How do those low achieving students profit  by continuous learning with e-test compared to those who have  not yet used the e-tests or Is the performance in e-tests  somehow related to exam grades or Do native speakers have  less problems with the learning offering than non-native  speakers From our findings, we concluded that the most  valuable questions still remain unanswered. Their related  indicators require the usage of qualitative data, official academic  profile data, teacher data, and their combinations/correlations.    4.2 Teacher-Data-Indicators (Q2)  We did not find tools or indicators that explicitly collect and  present teacher data. Indicators that come closest to correlating  teacher data, are indicators related to course phases (see, [49]),  which relate to course events, or sociograms, which show  interactions between teachers and students [17,18].    There is some teacher data already stored in databases, such as  activity logs. But other data is missing in most systems, like data  on the quality of course materials. Missing data concerns meta- data or information on events and activities outside the learning  environment (e.g. lectures). LA tools should explicitly collect,  analyze and present teacher data. We need to correlate this data  with student behavior to show how and which teaching activities  and events, have impact on student behavior and student learning.  If teachers had indicators about their activities and online  presence, they might be inspired and motivated to be more active  in the online learning environment. Hence, their presence in  discussions might stimulate students likewise to participate more  actively and motivate them to share knowledge and ideas.   In order to identify possible indicators that will present teachers  data, we conducted a brainstorming session. We took into  consideration the activities and duties teachers have while  teaching a course. These teachers activities vary from grading an  assignment or exam, preparation of learning offerings (lecture  slides, notes), or providing and combining lecture resources  online, writing emails, participating in online forums, etc. From  these activities, we identified example indicators. Some of them  are: Time spent grading each assignment/quiz/exam, Which  learning offerings are preferably used to prepare or reinforce  lecture topics, How do learning offerings have to be provided  and combined with support to increased usage, Usage diagram  of the course, and showing specific milestones/events of the  semester., Simple intervention of teacher contacting a student,  and does the behavior of the student changes, Does teacher  participation in online discussions, inspires students to be more  proactive and present more in the discussion forums    The following three use cases are supposed to provide ideas of  how teachers could use indicators that are based on teacher data  and how helpful the indicator could be for a certain task.    Indicator use case 1: Teacher Forum Participation Indicator  After the collection and analysis of students and teachers forum  interaction data, the results are presented to the teachers, to show  them their patterns of participation and forum presence. For  example, it would be interesting to see whether in the periods  when the teacher was more active and more present on the forum,  students online presence and activities also increased.   Indicator Use Case 2: Teacher Correspondence Indicator  For this indicator, the system collects data and information about  teachers personal correspondence with each of his students. The  teacher notices that several students have low participation in the   224    online discussions, and their assignment submission results are  low. She decides to contact the students in order to investigate the  reasons. After this intervention, the teacher will be curious to see  whether the students behavior has changed for the better and  whether her intervention was correlated to this.  Indicator Use Case 3:  Average Assignments Grading Time   The teachers publish, read and grade assignments online in the  LMS. The data of the time spent on grading each assignment is  logged by the system. From the dashboard, the teacher can see  whether the time spent on each assignment is evenly distributed.  If there are outliers from the standard deviation of the time spent,  the teachers could be inclined to self-reflect why they were  spending more time on certain assignments, while spending less  time on other tasks.  Of course teacher-data-indicators also raise privacy issues, as a  single teachers data cannot be anonymized. One of several  possible ways to solve this issue could be personalization, i.e., to  present their own data only to the teachers themselves.   4.3 Missing Impact Analysis (Q3)  Regarding our last question, the following paragraphs discuss how  LA could impact teaching and how this could be evaluated.    The impact evaluation method described below is based on our  assumption that learning analytics (LA) support action research.   Table 3, which is based upon [14, 24, 31], opposes key  characteristics of action research to LA to show similarities as  well as differences of both approaches.     Action research is a well-known method for improvement of  teaching and professional development [31]. The LA tools  considered in our analysis (see, section 2.2) try to achieve  common purposes. Many studies aim at increasing awareness and  reflection about the learning process; fostering improvement  activities at best. Learning analytics are supposed to help teachers   to reflect and draw conclusions on the quality of their learning  content, pedagogical practice, and quality of interactions among  students (see, [1, 29, 46, 50, 58]), while students are to be  stimulated to self-reflect their learning behavior based on  monitoring their own usage and interaction behavior as well as  having comparative information at hand (see, [25, 32, 47, 51]).   As presented in table 4, these overall goals have been detailed and  formulated in several ways in the LA literature.  They can be  divided into goals that   a. explicitly inform the design of learning analytics tools  b. involve a behavioral reaction of the teacher  c. involve a behavioral reaction of the student   How does an LA tool, which was carefully designed according to  the above-mentioned requirements, influence the behavior of its  users, and therefore, how does it influence practical learning  situations Strikingly, there are very few publications reporting  about findings related to the behavioral reactions of (b.) teachers  and (c.) students, i.e., few studies measure the impact of using  learning analytics tools. What are the effects of the usage of LA  How do LA systems influence practical learning situations How  does a specific indicator help the user to reflect and change his  behavior Which behavioral changes are visible to us How could  we measure them    Evaluative research on LA tools focuses on functionality,  usability issues (indicator design, comprehensibility, terminology)  and perceived usefulness of specific indicators [1, 28, 43, 50].  Furthermore, possibly due to novelty reasons several projects  have not yet published data about conducting reliable case studies  or evaluations results at all (e.g., [26, 29, 48, 51]). Some have  conducted rather small evaluations that limit generalization of  conclusions; e.g., [56] tested MATEP with one teacher. Although  [41] mentioned that their experimental study aimed at measuring    Action research Learning analytics   Goals Professional development, finding answers to practical  questions, improvement of teaching, and social justice   Monitoring, analysis, prediction, intervention, assessment, feedback,  adaption, personalization, recommendation, reflection and self reflection   Process  cycle   Develop a question  formulate research plan  collect data   analyze  develop and implement action plan  record  project in writing  share   Data gathering (select, capture)  information processing (aggregate,  report)  knowledge application (use, refine) and sharing   Driving  factor   Human-driven: activities are centered around the person  (group), who conduct the project   Data-driven: process is based on large amounts of data that promise to  reveal new information   Advantages Individual, perfectly fitting to a specific scenario, answers  exactly the questions a teacher asks, open for all questions:  What do I want to learn about my teaching Methods of  data collection can be adjusted creatively/accordingly   Standardized, general, suited for several scenarios, possibility to provide  approved data analysis/visualization by research experts, developed for  and well suited for TEL or distance learning, data privacy issues can be  handled centrally   Drawbacks     Limited by time-constraints, teachers workload and action  research know-how, data analysis error-prone due to human  error, not optimized for TEL, data privacy and permissions  need to be handled by the teachers themselves   Limited by missing data, often restricted to quantitative data collection  methods, interpretation difficulties, danger to answer only questions  nobody is interested in, focused on questions like: What does the data tell  us Specific question cannot be studied, if data is missing.   Mode Manually Automatically   Impact Effecting reflection, motivation and teaching activities Assumed influence on users behaviors and reflective practice    Context  knowledge   Knowledge about individual teaching situation (e.g.,  motives, teaching history, reasons)   Only data on teaching activities that have been recorded (e.g. log files,  teacher journals, IMS learning design)   Instance Single instance (or rather courses by one teacher) Multiple instances of the same scenario possible   Methods All kinds of qualitative and quantitative methods (e.g.,  surveys, interviews, video recording)   Limited to quantitative data collection methods (mostly data that can be  logged automatically on different devices)   Table 3. Comparison of key characteristics of action research and learning analytics.   225    impact on the learning situation, they could not make conclusions  because of a low participation rate. Bratitsis and Dimitracopoulou  [10] give some evidence for the effects of using learning analytics  on teachers and students behavior. They concluded that  discussion analysis indicators helped to increase students activity,  which might affect the learning process by leading to more  effective discussions and critical thinking.   Current LA systems try to support teachers and students in their  tasks. So far, none of the tools that we have analyzed has a strong  proof for a beneficial impact on either teachers, or students, or  both. However, these tools should not only be usable and  interesting to use, but also useful in the context of the goals:  awareness, self-reflection, improvement of teaching, and  improvement of learning. Measuring the impact of learning  analytics tools is a challenging task. One of the main problems is  to find enough participants for evaluations. Therefore, we suggest  using a combination of qualitative and quantitative methods. In  the following paragraphs, we sketch one of several possible way  how to measure impact. The following approach could be  described as design-based research with a focus on uncovering  action research activities.    The first step of measuring the impact of Learning Analytics tools  is to make the tools available to the users. These should be a  representable group of non-expert teachers and students. To be  able to draw conclusions about the impact of LA usage, we need  to know the current status in a classroom, and set this status as a  reference point. How is the teachers learning design, and the  students learning process In which way does he/she interact  with the systems and students What is going well/not-so-well  How are the participants addressing problems without the support  of LA How is the performance of students Answers to these  questions will be used later as a reference point on to compare and  measure whether the presence of LA tools has certain impacts on  the behavior of both, teachers and students. This can be achieved  by doing qualitative and quantitative evaluation on teachers and  students.    Conducting interviews with teachers, or providing them with  questionnaires will also help to grasp their motivation and  qualitative sides of teachers status. As for the students side,  online questionnaires in the LMS at the beginning of the semester  could provide information and comprehend their mindset  concerning their activities and interactions in their learning  process. For quantitative analysis, students log data and   performance data from previous semesters (if existing) could be  extracted, and analyzed to serve as comparison basis with the  collected data of the ongoing evaluating time period, which could  also record teachers interactions with the LA tool. How did users  use the LA tool Can we find patterns of usage How do they  react upon the data, e.g., does a teacher write an email to his/her  students after viewing an indicator    After we have created a clear picture of the current state, we have  to identify, which activities are likely to be improved by the usage  of the LA tool/indicators. This is where we pose our hypotheses  how the usage of the tool will improve the behavior of students  and teachers, their activities, and the learning process. It is  important to explicitly distinguish between the beginning state of  the learning process, and the predicted state of the system. This  will support the impact claimed of the tool use and explicitly  show in which direction the possible effects and influence of the  tool on both teachers and students go.   The final step of the impact evaluation is to conduct interviews  and questionnaires with both teachers and students. These will  help to take into consideration their personal feelings and opinions  after using the tool in a given time period. Did a specific indicator  puzzle them Did they reflect upon data shown by the indicators  How did they react Did they revise their action plan   The results of these interviews should be compared to the results  of the questionnaires/interviews before the beginning of the  impact evaluation. The quantitative analysis should consist of  comparing the analysis of the current performance and log data,  with the analysis of the data of the previous semesters. Results of  these two comparisons are combined to create and present a list of  changes in the activities of both teachers and students, changes in  their behavior, as well as changes in the learning process and  resulting performance. To finalize this step, the hypothesized  changes are compared with the actual changes. This way, we can  draw conclusions on how the hypothesized impact relates to the  actual impact of the learning analytics tool.    This proposed impact evaluation method has its own limitations.  The process itself needs long periods of time (at least one  semester including the exam phase). It requires a lot of effort and  active participation from researchers and participants in the  learning process (teachers and students). The analysis of the  qualitative data from the questionnaires and interviews is always  prone to personal interpretations and biased while making  conclusions. Therefore, it is necessary that several domain experts   a.  Learning analytics are supposed to b.  Educators are supposed to c.  Students are supposed to    track user activities   capture the interaction of students with   resources / the interactions among students    gather data of different systems   provide educators / students with   feedback/information on students activities   provide an overview   highlight important aspects of data   provide different perspectives   offer possibilities for (peer) comparison    draw the users attention to interesting   correlations   pinpoint problematic issues   establish an early warning system   provide decision support    monitor learning process / way of learning /  students effort    explore student data / get to know students  strategies    identify difficulties   discover patterns   find early indicators for success / poor   marks / drop-out    draw conclusions about usefulness of   certain learning materials and success  factors    become aware / reflect / self-reflect   better understand effectiveness of learning   environments   intervene / supervise / advice / assist   improve teaching / resources / environment    monitor own activities / interactions /  learning process    compare own behavior with the whole  group / high performing students    become aware   reflect / self-reflect   improve discussion participation / learning   behavior / performance   become better learners   learn     Table 4. Goals of learning analytics concerning tools, educators, and students.   226    interpret the qualitative data, and compare their findings. On the  one hand, it will be a great challenge to predict and identify the  key differences between the teaching and learning states before  and after using the LA tool. Depended on the number of  participants of the study and the collected data, it might not be  possible to come to clear conclusions about the impact of LA  tools. On the other hand, the suggested evaluation process delivers  lots of information that can be used to iteratively inform future  designs of LA tools, which might better support action research.  Furthermore, as one reviewer of this paper wrote, LA is not yet  part of the routine practice in teaching and learning. With more  practice of LA in everyday life, other ways might appear to  measure the impact of LA.   4.4 Limitations of our Study  We based our analysis on a former qualitative meta-analysis,  which investigated what kind of questions teachers ask while  performing action research in their technology-enhanced learning  (TEL) scenarios. The meta-analysis was limited to case studies  described in the conference proceedings of the German eLearning  conference DeLFI [22]. This conference could be considered as  representative for technology application in Germany at the time  of writing. Future research could take into account questions from  a more international context. These findings will probably differ  from the teachers questions we based our study on, showing that  specific contexts need specific approaches. It can also be assumed  that the issues of education will change in the future, together with  the teaching and learning scenarios. We conclude that practical  LA designs should always consider and constantly update the  concrete scenarios and take into account the characteristics of  each target groups, which they are supposed to address.   We based our analysis of indicators on 27 LA tools (section 2.2).  There are many more studies on indicators; in particular there are  more findings on higher-level indicators in the EDM field that  will be able to solve complex questions of teachers in future LA  tools. The challenge is, how to make them usable.   Another limitation of our approach lies in the subjectivity of the  personal interpretation of the questions and the indicators. For  objectivity reasons, we tried to overcome this limitation, by  conducting the indicator collection, categorization and question- indicator-mapping separately by two researchers. The process of  creating the mapping was not easy. Questions and indicators did  not really fit. In order to provide teachers with insight and  enlightening information about their students, LA researchers  need to involve teachers interests in LA development projects  and design and evaluate sophisticated indicators that are able to  answer their questions.   Another reason for the difficulties we faced was the absence of  clear and explicit guidelines explaining which questions the  presented indicators are trying to answer. If we, as researchers of  the LA field, cannot clearly identify which questions each  indicator answers, how can teachers and non-experts understand  the purpose of an indicator How can a teacher find the fitting  indicators to his or her own individual questions How can  teachers be prevented from false interpretations This calls for the  necessity, that LA researchers should always provide, accessible  guidelines of how to interpret each newly developed indicators,  and give hints about which questions can be answered by which  indicators in which learning context. At the same time, they also  have to provide information about the limitations of each  indicator. These arguments are especially meaningful for EDM- based indicators.   5. CONCLUSION  Learning Analytics tools should be an integral part of TEL. The  tools aim at having an impact on teachers and students. But the  impact has not been evaluated. The concern we are raising is that  LA tools should not only be usable, but also useful in the context  of the goals we want to achieve.    Creating diversified sets of indicators, according to the roles of  and interests of users may be a promising idea [19]. An action  research project may involve an expert researcher or mentor, who  provides information, but leaves decisions to the practitioners of  the field [31]. LA tools could take this position by informing and  supporting action research tasks, such as choosing research  questions and following a research plan, allowing the manual  collection of data that otherwise would not be available (e.g.,  course attendance information), or recommending appropriate  indicators for individual interests.    Our investigation showed that the currently available research  tools do not yet answer many questions of teachers. But  appropriate and significant indicators would be important for  initiating action research. At present, we have a set of indicators  that mainly answer questions concerned with usage analysis.  Complex questions that relate to qualitative analysis and data  correlation from diverse sources cannot yet be answered with the  existing indicators. This complies with findings from the field of  interaction analysis in 2005, when most tools produced indicators  of low interpretative value, e.g. percentage of participation or  answers to messages [19]. The causes for these shortcomings are  insufficient involvement of teachers in the design and  development of indicators, absence of rating data/features, non- used student academic profile data, and absence of specific  student generated data (mobile, data usage from different  devices), as well as missing data correlation and combination  from different data sources. Furthermore, teachers data and  indicators based on this teachers data are not easily visible in the  current publication on implementations of Learning Analytics  tools. And also, against this background of data consolidation, we  need best practice examples to deal with data privacy issues.   Future learning environments (no matter if LMS or PLE) will  probably have rating features. Data collected by these functions  should be used for LA tools. Furthermore, in order to fill in the  gap between what the indicators do, and what the teachers  actually need, researchers should actively involve teachers in the  design and implementation of indicators. The identification of  their requirements, as also noted by [19], will ensure that the  implemented indicators will be tailored according to teacher  needs. It is also noteworthy to mention that researchers should  always provide guidelines for the teachers on how to interpret the  indicators. It is imperative to give hints on which teachers  questions can be answered with which sets of indicators. Also, LA  researchers should clearly state the limitations of their newly  developed indicator to prevent misinterpretations. Lastly, there is  a necessity for creation of evaluation tools to measure the impact  and effects of LA on the learning process. We need proofing  mechanisms that will support and reassure the goals concerning  increased awareness and reflections, and improved teaching  processes. To conclude, we need better indicators that will serve  answering the teachers questions, we need to collect and correlate  data from different sources to support these new indicators, and  we need to evaluate how these indicators impact teaching and  learning processes.   We discussed how LA tools could impact teaching and suggested  how we could we measure this impact to initiate a discussion   227    about this in the research community. Our own next steps are the  enhancement of an existing LA tool, namely eLAT [23] and the  evaluation of its impact.   We would like to encourage the community of LAK researchers  to discuss our findings to advance the field of LA and inform the  development of improved and personalized tools that provide  high-level indicators, which have a high potential of initiating  reflective thinking processes.   6. REFERENCES  [1] Ali, L., Hatala, M., Gaevi, D. and Jovanovi, J. 2012.   A qualitative evaluation of evolution of a learning  analytics tool. COMPUT EDUC. 58, 1 (Jan. 2012), 470 489.   [2] Altrichter, H., Posch, P. and Somekh, B. 1996. Teachers  investigate their work: An introduction to the methods of  action research. Routledge.   [3] Arnold, K. 2010. Signals: Applying Academic Analytics.  EDUCAUSE Quaterly. 33, 1 (2010).   [4] Asadi, M., Jovanovi, J., Gasevic, D. and Hatala, M.  2011. A Quantitative Evaluation of LOCO-Analyst: A  tool for Raising Educators Awareness in Online  Learning Environments. (2011), 117.   [5] Bakharia, A., Heathcote, E. and Dawson, S. 2009. Social  networks adapting pedagogical practice: SNAPP. Proc.  of ascilite (2009), 4951.   [6] Borgatti, S.P. 2002. NetDraw: Graph visualization  software. Harvard: Analytic Technologies. (2002).   [7] Boud, D., Keogh, R. and Walker, D. eds. 1985.  Reflection: Turning Experience Into Learning. Routledge  Chapman & Hall.   [8] Bratitsis, T. and Dimitracopoulou, A. 2005. Data  Recording and Usage Interaction Analysis in  Asynchronous Discussions: Proc. of the Workshop on  Usage Analysis in Learning Systems (AIED05) (2005),  916.   [9] Bratitsis, T. and Dimitracopoulou, A. 2008. Interaction  Analysis as a Multi-Support Approach of Social  Computing for Learning, in the Collaborative Era:  Lessons Learned by Using the DIAS System. Proc. of the  8th Int. Conf. on Advanced Learning Technologies  (ICALT08) (2008), 536538.   [10] Bratitsis, T. and Dimitracopoulou, A. 2008.  Interpretation Issues in Monitoring and Analyzing Group  Interactions in. INT J E-COLLABORATION. 4, 1 (2008),  2040.   [11] Bratitsis, T. and Dimitracopoulou, A. 2006. Monitoring  and Analyzing Group Interactions in Asynchronous  Discussions with the DIAS system. 12th Int. Workshop  on Groupware, GRIWG2006, Groupware: Design,  Implementation and Use (2006), 5461.   [12] Brooks, C., Panesar, R. and Greer, J. 2006. Awareness  and Collaboration in the iHelp Courses Content  Management System. Proc. of the 1st Europ. Conf. on  Technology Enhanced Learning (EC-TEL 2006) (2006),  3444.   [13] Cator, K. and Adams, B. 2012. Enhancing Teaching and  Learning Through Educational Data Mining and  Learning Analytics: An Issue Brief.   [14] Chatti, M.A., Dyckhoff, A.L., Schroeder, U. and Ths,  H. 2012. A reference model for learning analytics.  IJTEL. 4, 5/6 (2012), 318331.   [15] Chatti, M.A., Dyckhoff, A.L., Schroeder, U. and Ths,  H. 2012. Forschungsfeld Learning Analytics. I-Com. 11,  1 (Mar. 2012), 2225.   [16] Clow, D. 2012. The Learning Analytics Cycle: Closing  the loop effectively. Proc. of the 2nd International Conf.  on Learning Analytics and Knowledge (LAK12) (2012),  134138.   [17] Dawson, S. 2010. Seeing the learning community: An  exploration of the development of a resource for  monitoring online student networking. BRIT J EDUC  TECHNOL. 41, 5 (Sep. 2010), 736752.   [18] Dawson, S., Bakharia, A. and Heathcote, E. 2010.  SNAPP: Realising the affordances of real-time SNA  within networked learning environments. Proc. of the 7th  Int. Conf. on Networked Learning (2010), 125133.   [19] Dimitracopoulou, A. 2008. Computer based Interaction  Analysis Supporting Self-regulation: Achievements and  Prospects of an Emerging Research Direction. TICL.  (2008).   [20] Dimitracopoulou, A. 2005. State of the art of Interaction  Analysis for Metacognitive Support & Diagnosis. IA.  JEIRP Deliverable D.31.1.1. Kaleidoscope NoE. (2005).   [21] Duval, E. and Verbert, K. 2012. Learning Analytics.  eleed. 8, (2012).   [22] Dyckhoff, A.L. 2011. Implications for Learning  Analytics Tools: A Meta-Analysis of Applied Research  Questions. IJCISIM. 3, (2011), 594601.   [23] Dyckhoff, A.L., Zielke, D., Bltmann, M., Chatti, M.A.  and Schroeder, U. 2012. Design and Implementation of a  Learning Analytics Toolkit for Teachers. EDUC  TECHNOL SOC. 15, 3 (2012), 5876.   [24] Elias, T. 2011. Learning Analytics: Definitions,  Processes and Potential.   [25] Fritz, J. 2011. Classroom walls that talk: Using online  course activity data of successful students to raise self- awareness of underperforming peers. The Internet and  Higher Education. 14, 2 (Mar. 2011), 8997.   [26] Garca-Saiz, D. and Pantalen, M.E.Z. 2011. E-learning  Web Miner: A data mining application to help instructors  involved in virtual courses. Proc. of the 4th Int. Conf. on  Educational Data Mining (EDM11) (2011), 323324.   [27] Gonzlez Agulla, E., Alba Castro, J.L., Argones Ra, E.  and Anido Rifn, L. 2009. Realistic Measurement of  Student Attendance in LMS Using Biometrics. Proc. of  the Int. Symposium on Engineering Education and  Educational Technologies (EEET09) (2009), 57.   [28] Govaerts, S., Verbert, K., Duval, E. and Pardo, A. 2012.  The student activity meter for awareness and self- reflection. Proc. of the ACM SIGCHI Conf. on Human  Factors in Computing Systems (CHI EA12) (May.  2012), 869884.   [29] Graf, S., Ives, C., Rahman, N. and Ferri, A. 2011. AAT:  A Tool for Accessing and Analysing Students  Behaviour. Proc. of the 1st Int. Conf. on Learning  Analytics and Knowledge (LAK11) (2011), 174179.   228    [30] De Groot, R., Drachman, R., Hever, R.R., Schwarz, B.B.,  Hoppe, U., Harrer, A., De Laat, M., Wegerif, R.,  McLaren, B.M. and Baurens, B. 2007. Computer  Supported Moderation of E-Discussions: the  ARGUNAUT Approach. Proc. of the Conf. on Computer  Supported Collaborative Learning 2007 (CSCL07)  (2007), 165167.   [31] Hinchey, P.H. 2008. Action Research Primer. Peter Lang  Publishing, Inc.   [32] Janssen, J., Erkens, G., Kanselaar, G. and Jaspers, J.  2007. Visualization of participation: Does it contribute to  successful computer-supported collaborative learning  COMPUT EDUC. 49, 4 (Dec. 2007), 10371065.   [33] Johnson, L., Adams, S. and Cummins, M. 2012. NMC  Horizon Report: 2012 Higher Education Edition.   [34] Johnson, M.W., Eagle, M.J., Joseph, L. and Barnes, T.  2011. The EDM Vis Tool. Proc. of the 4th Int. Conf. on  Educational Data Mining (EDM11) (2011), 349350.   [35] Kay, D., Korn, N. and Oppenheim, C. 2012. Legal , Risk  and Ethical Aspects of Analytics in Higher Education.  JISC CETIS Analytics Series. 1, 6 (2012), 130.   [36] Kolb, D. 1984. Experiential learning: experience as the  source of learning and development. Prentice Hall.   [37] Kosba, E., Dimitrova, V. and Boyle, R. 2005. Using  Tracking Data to Build Student and Group Models and  Generate Advice in Web-Based Distance Learning  Environments.   [38] Krger, A., Merceron, A. and Wolf, B. 2010. A Data  Model to Ease Analysis and Mining of Educational Data.  Proc. of the 3rd Int. Conf. on Educational Data Mining  (EDM2012) (2010), 131140.   [39] Lockyer, L. and Dawson, S. 2011. Learning designs and  learning analytics. Proc. of the 1st Int. Conf. on Learning  Analytics and Knowledge (LAK11) (2011), 153156.   [40] Lockyer, L. and Dawson, S. 2012. Where Learning  Analytics Meets Learning Design. Proc. of the 2nd  International Conf. on Learning Analytics and  Knowledge (LAK12) (2012), 1415.   [41] May, M., George, S. and Prvt, P. 2011. TrAVis to  enhance online tutoring and learning activities: Real-time  visualization of students tracking data. Interactive  Technology and Smart Education. 8, 1 (2011), 5269.   [42] May, M., George, S. and Prvt, P. 2011. TrAVis to  Enhance Students Self-monitoring in Online Learning  Supported by Computer-Mediated Communication  Tools. IJCISIM. 3, (2011), 623634.   [43] Mazza, R. 2006. Evaluating information visualization  applications with focus groups. Proc. of the 2006 AVI  workshop on BEyond time and errors: novel evaluation  methods for information visualization (BELIV06) (May.  2006), 16.   [44] Mazza, R. and Botturi, L. 2007. Monitoring an Online  Course With the GISMO Tool: A Case Study. JILR. 18,  2 (2007), 251265.   [45] Mazza, R. and Dimitrova, V. 2007. CourseVis: A  graphical student monitoring tool for supporting  instructors in web-based distance courses. INT J HUM- COMPUT ST. 65, 2 (Feb. 2007), 125139.   [46] Merceron, A. and Yacef, K. 2005. TADA-Ed for  Educational Data Mining. IME of CEL. 7, 1 (2005).   [47] Mochizuki, T., Kato, H., Fujitani, S., Yaegashi, K.,  Hisamatsu, S., Nagata, T., Nakahara, J., Nishimori, T.  and Suzuki, M. 2007. Promotion of Self-Assessment for  Learners in Online Discussion Using the Visualization  Software. User-Centered Design of Online Learning  Communities. N. Lambropoulos and Z. Panayiotis, eds.  Information Science Publishing. 365386.   [48] Pedraza Perez, R., Romero, C. and Ventura, S. 2011. A  Java desktop tool for mining Moodle data. Proc. of the  4th Int. Conf. on Educational Data Mining (EDM11)  (2011), 319320.   [49] Petropoulou, O., Retalis, S., Siassiakos, K., Karamouzis,  S. and T., K. 2007. Helping Educators Analyse  Interactions within Networked Learning Communities: A  Framework and the AnalyticsTool System. Proc. of the  6th Int. Conf. on Networked Learning (2007), 317324.   [50] Scheuer, O. and Zinn, C. 2007. How did the e-learning  session go The Student Inspector. Proc. of the Conf. on  Artificial Intelligence in Education (AIED07) (Jun.  2007), 487494.   [51] Schmitz, H.-C., Scheffel, M., Friedrich, M., Jahn, M.,  Niemann, K. and Wolpers, M. 2009. CAMera for PLE.  Proc. of the 4th Europ. Conf. on Technology Enhanced  Learning (EC-TEL09) (Oct. 2009), 507520.   [52] Schmitz, H.-C., Wolpers, M., Kirschenmann, U. and  Niemann, K. 2009. Contextualized Attention Metadata.  Human Attention in Digital Environments. C. Roda, ed.  Cambridge University Press. 186209.   [53] Siemens, G. 2010. What are Learning Analytics   [54] Zhang, H. and Almeroth, K. 2010. Moodog: Tracking  Student Activity in Online Course Management Systems.  JILR. 21, 3 (2010), 407429.   [55] Zhang, H., Almeroth, K., Knight, A., Bulger, M. and  Mayer, R. 2007. Moodog: Tracking Students Online  Learning Activities. Proc. of World Conf. on Educational  Multimedia, Hypermedia and Telecommunications (ED- Media07) (2007), 44154422.   [56] Zorrilla, M., Garca, D. and lvarez, E. 2010. A decision  support system to improve e-learning environments.  Proc. of the EDBT/ICDT Workshops (2010), 18.   [57] Zorrilla, M.E., Marn, D. and lvarez, E. 2007. Towards  Virtual Course Evaluation Using Web Intelligence. Proc.  of the 11th Int. Conf. on Computer Aided Systems Theory  (EUROCAST07) (2007), 392399.   [58] Zorrilla, M.E. and lvarez, E. 2008. MATEP:  Monitoring and Analysis Tool for E-Learning Platforms.  Proc. of the 8th IEEE International Conf. on Advanced  Learning Technologies (ICALT08) (Jul. 2008), 611613.       229      "}
{"index":{"_id":"33"}}
{"datatype":"inproceedings","key":"Camilleri:2013:CSI:2460296.2460341","author":"Camilleri, Vanessa and de Freitas, Sara and Montebello, Matthew and McDonagh-Smith, Paul","title":"A Case Study Inside Virtual Worlds: Use of Analytics for Immersive Spaces","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"230--234","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460341","doi":"10.1145/2460296.2460341","acmid":"2460341","publisher":"ACM","address":"New York, NY, USA","keywords":"corporate training, data analytics, higher education, pre-service teachers, virtual worlds","abstract":"In this paper we describe some case studies of the use of virtual worlds in corporate training as well as Higher Education. In particular for Higher Education we describe how the Virtual World constructed using the platform Avaya Live Engage, is used as an immersive environment with pre-service teachers, who are undergoing a 1-year teacher training program, and how the data analytics collected in-world is being used to monitor and direct content development. We focus our studies on the initial hypothesis that 3D immersive environments are highly engaging and offer an experience that goes beyond the 'traditional' online education. We want to combine different analysis methods to be able to get empirical evidence showing the students' engagement with the 3D space in ways that can help us in the design of the learning experience accompanying the learners in their journey. In this paper we describe the research methods we use for the study, and give an overview of the information we can collect from the in-world analytics. We also propose how these analytics can be used for a predictive model with the intention of refocusing the virtual world experience to match learner needs.","pdf":"A case study inside Virtual Worlds:   use of analytics for immersive spaces   Vanessa Camilleri  University of Malta   Faculty of Education  Malta   00356 23403413  vanessa.camilleri@um.edu.mt        Sara de Freitas  Serious Games Institute  University of Coventry   UK  0044 24 7615 8208   SFreitas@cad.coventry.ac.uk     Paul McDonagh-Smith  AvayaLive Engage   Avaya House, Guildford  UK   0044 1483 309291  paulmcsm@avaya.com   Matthew Montebello  University of Malta   Faculty of ICT  Malta   00356 23402132  matthew.montebello@um.edu.mt        ABSTRACT  In this paper we describe some case studies of the use of virtual  worlds in corporate training as well as Higher Education. In  particular for Higher Education we describe how the Virtual  World constructed using the platform Avaya Live Engage, is used  as an immersive environment with pre-service teachers, who are  undergoing a 1-year teacher training program, and how the data  analytics collected in-world is being used to monitor and direct  content development. We focus our studies on the initial  hypothesis that 3D immersive environments are highly engaging  and offer an experience that goes beyond the traditional online  education. We want to combine different analysis methods to be  able to get empirical evidence showing the students engagement  with the 3D space in ways that can help us in the design of the  learning experience accompanying the learners in their journey. In  this paper we describe the research methods we use for the study,  and give an overview of the information we can collect from the  in-world analytics. We also propose how these analytics can be  used for a predictive model with the intention of refocusing the  virtual world experience to match learner needs.    General Terms  Measurement, Documentation, Design, Experimentation, Human  Factors.   Keywords  Virtual Worlds, Pre-service teachers, Higher Education, Corporate  Training, data analytics.   1. INTRODUCTION  Virtual Worlds (VWs) have been described as 3D representations   of real world communication structures, various interactions and  enterprise (Castronova, 2007). The author describes the way the  masses have embraced this medium as an exodus, whereby people  not only flock to the VWs and Virtual Realities, whether they are  immersed in games, or in any other form of activity held in the  cyberspace with motivations that extend beyond the fun element.  The short-term and long-term behavioral effects that VWs and  Virtual Reality have on people have also been at the center of  recent research studies (Blascovich & Bailenson, 2011). This  investigation aims to study the impacts of the 3D immersive space  on learner engagement manifested by changes in behavior. For  this investigation and in the case studies we are presenting we  work with AvayaLive Engage. The 3D platform is currently  being used by a number of leading enterprises to deliver  innovative corporate training experiences at all stages of the  employee learning cycle; from recruitment through on-boarding to  assigning new skills, leadership development and retention  activities. Learning analytics on the other hand may be considered  an exercise in educational data-mining, that uses traces left as  breadcrumbs by the users, in this case referred to as learners, to  be able to understand what the learners are doing and how they  are navigating through their learning experiences (Pardo &  Delgado Kloos, 2011). We describe learner engagement both in  terms of content presented as well as using the critical-democratic  perspective (Portelli & McMahon, 2004). For the latter, the  authors argue that a dynamic environment does not necessarily  imply critical-democratic engagement. Critical democracy in an  individual is described as the learners state of being that goes  beyond the physical classroom/university structure. This indicates  learning which is manifested over a much longer term than any  course duration. A number of universities and higher education  institutions are finding an increased need in moving/shifting their  courses towards the online environment. Administration in such  institutions as well as in corporate training, are using big data  analytics to provide numbers showing the learner/trainee  engagement with the content, as well as with the peers and/or  academic staff members. However Portelli & McMahon (2004)  argue that such levels of engagement might not run as deep into  the formation of a critical being, as is much needed in todays  society. A critical individual possesses skills such as critical  inquiry that does away with the traditional notions of education  and views learners as responsible, autonomous learners.       Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04...$15.00   230    In our study, we focus on this aspect of engagement as we design  different immersive learning experiences, inside a virtual world.  In our paper we discuss two case studies involving higher  education and corporate training respectively. We shall also give  an overview of the modalities for learning that have been chosen  for the two case studies and the dashboard that is currently being  used to obtain data revealing our learners immersive journey.  Section 2 of this paper will discuss some of the background into  virtual worlds and learning analytics whilst section 3 will describe  the methodology used in two distinct case studies involving  higher education and corporate training. Section 4 will give an  overview of some of the results that are obtained using the virtual  world analytics dashboard whilst section 5 will discuss the  possible implications of collecting big data for learning inside  immersive spaces and point towards possible future research in  the area especially for learning recommendation systems  integrated inside 3D immersive spaces.    2. BACKGROUND  In this section we shall be discussing some background into 3D  immersive spaces for learning, including some of the archetypes  that they support and how these have been shown to be conducive  to a deeper engagement level. We shall also be discussing some  of the assessment strategies that are used inside immersive spaces,  and the inferences that can arise from the assessment results. In  the second part of this section we shall discuss learning analytics  in terms of the different approaches that can be chosen, the  strengths and challenges of using data to trace learning paths as  well as the usefulness both in the corporate as well as in higher  education settings.    2.1 Immersion for learning  Immersion has been attributed a number of characteristics and  definitions in various contexts. Dede & Barab (2009) view  immersion in education as focusing more on the learner  experience and less on the tools used. This may also be in  agreement with a number of other researchers in the field. Calleja  (2011) views immersion, albeit from the gamer experience as  similar in that immersion does not simply depend on the high  fidelity of graphics and audio that is involved. Although these  certainly may help render experiences, more life-like much more  can be done in the design aspect to render experiences  authentic. Salen & Zimmerman (2004) emphasize that game  design should primarily focus around game mechanics that  stimulate engagement for true immersion to occur. Carpenter  (2009) describes the experiences in VWs as the connections,  which the users nurture as they interact with the 3D environment  and with the other personalities inhabiting the world. This applies  to both social virtual worlds as well as to game worlds, where the  individual acquires a second identity embodied in the persona of  an avatar. Dede (2009) describes how symbolic immersion can  have the power of stimulating psychological associations that  would be difficult to explain. For example, the author mentions  how fighting a terrifying monster can induce fear despite the fact  that the individual is physically located in the safety of their room.  Dede goes on to discuss how immersion in a digital environment  can augment the learning experience by facilitating an  environment sustaining multiple perspectives, situated learning  (also referred to as authentic experiences (Freedman, 2011)) as  well as transfer.    2.1.1 Archetypes in Virtual Worlds  The different archetypes inside virtual worlds are responsible for  different learning experiences that can be supported. Multiple   perspectives most often arise from the ability of the individuals  who are participative inside the 3D experience to interact and  collaborate. Messinger et al. (2009) describe the concept of a  shared workspace that brings together the people inside a 3D  immersive space in a way that they can collaborate, and share  their ideas. The difference between the 3D space, and any other  environment that might be used to bring people together in a spirit  of collaboration, is also found in the embodiment of the avatar and  the narrative or diegesis (de Freitas & Oliver, 2006) that is  constructed over their shared life experiences in a manner that  cannot be reconstructed over any other environment. Situated  Learning or authentic experience is another archetype that is  supported inside 3D immersive spaces. Galarneau (2005)  illustrates how the activities by avatars inside virtual worlds most  often mirror real life situations rendering the way they solve  problems more authentic. Freedman (2011) defines authentic as  learning which is contextualized into real life practices. Therefore  the setting and the context is of vital importance to 3D immersive  space design. Transfer of the learning experience inside the world  occurs through a number of interaction processes, including the  interactions between avatars, between the world objects and  beyond the world as the avatars keep digging deeper beyond the  3D space (Camilleri & Montebello, 2008).    2.1.2 Assessment in Virtual Worlds  Assessing learner engagement in 3D spaces is one of the  important metrics when considering uses of virtual worlds for  education. Whereas a number of assessment models in online or  traditional environments might require the assessment of lower  level knowledge transfer, research has indicated that virtual  worlds and 3D spaces can support different forms of assessment.  One such model is the exploratory model (de Freitas & Oliver,  2006) which features an open-ended design in learning (de  Freitas, Rebolledo-Mendez, Liarokapis, Magoulas, &  Poulovassilis, 2010). In this kind of learning, the learners are  responsible for tracing the path to their own learning as learning  in turn is much dependent on social interactions in-world.  Therefore the assessment in this kind of model is built around an  evaluative exercise of structured activities, feedback and the  learners reactions to the feedback. However there is still a gap in  the research as to how the assessment of the learning experience  can mirror the learning objectives set for that specific context. Our  future study intends to use learning analytics to be able to provide  data filling in that gap in research.    2.2 Learning Analytics  Siemens (2010) defines learning analytics as the use of  intelligent data, learner-produced data, and analysis models to  discover information and social connections, and to predict and  advise on learning. In this context data is collected from users, in  this case learners and harvested in a way that can make more  sense in the general learning objectives. In terms of assessment,  West (2012) describes how data mining and analytic software can  facilitate real-time assessment of learning experiences as  predictions can be drawn on the basis of patterns of behavior that  each learner traces in his/her individual online pathways. Within  the same report, the author presents a number of research findings  from other researchers, as they reveal how different data from the  different learning domains that is collected, harvested and  analyzed can be interpreted in multiple ways to facilitate and  support learning. In one case example, involving pre-service  teachers, engagement assessment is carried out on the basis of  pupil interactions in an online forum. Results were considered  very effective in the reflective teaching process. Bienkowski,  Feng, & Means (2012) differentiate between educational data   231    mining and learning analytics in terms of development and  application of tools for pattern identification and prediction of  learner experiences. de Laat et al. (2007) focus on Social Network  Analysis (SNA) as they investigate the role of socialization in  human-computer interaction during computer-supported  collaborative learning. This type of learning finds its theoretical  roots in the social-constructivist paradigm, which sees learning  happening within community practices. This type of community  setting is one of the important building blocks in the dynamics of  social and game-oriented 3D immersive spaces and is one that we  focus our study on. Further research work has also been carried  out on SNA for the analysis of interaction patterns in online  course, with the aim of overcoming at least one of the challenges  most often associated to the e-environment  a sense of isolation  that most often learners tend to report (Ferguson, 2012). SNA also  brought with it the advent of visualization of data spread across  the large connection nodes. These types of visual data analytics  help individuals make sense out of the emergent data patterns  whilst at the same time the real-time visualization of how each of  the individuals stand across a continuum of learning directions.    2.2.1 Data Visualization  This notion of data visualization is also discussed in detail in  Duval (2011). The openness with which data visualization occurs,  facilitates a sense-making process as the learners, and the teachers  go through the learning pathways. The author also combines the  visualization of data with the influence that online social networks  may have on the individual users behavior. The way that the  effectiveness of this visualization of data can be measured is in  the learners behavior as this gradually shapes up to different, new  tendencies. Data visualization is also described in terms of the  prediction of failures in learning, especially when a closed  monolithic framework, such as an LMS is used to collect and  trace data from. The difficulty with using an LMS to collect data  is that the data which is visualized is not always tantamount to the  amount of learning that goes on and that other sources may be  contributing to. This is one of the challenges mentioned by Pardo  & Delgado Kloos (2011) as they attempt to investigate the use of  virtual machines to collect large sets of data triggered by events  not necessarily set inside a Learning Management System. What  many researchers agree upon, is that the user of visualization of  data can be ultimately be used not just for the prediction of  failures or successes, but also for the possibility of personalization  and adaptation of the various learning instances to match with the  learner needs.    3. METHODOLOGY  3.1 Higher Education: a pre-service teacher  experience  For our investigation we have chosen a cohort of pre-service  teachers, following a 1-year teacher-training course at the Faculty  of Education, University of Malta. Our target research question is  How can we measure the engagement of our learners with the 3D  immersive space Our second question targets the behavioral  intention of our learners to keep up with the teaching and learning  practices they go through themselves during their in-world  residence. To be able to give a measure and assess their  engagement we are utilizing the dashboard from the Avaya Live  Engage platform that gives us information on the virtual presence  of our learners in-world and about the learners interactions with  the other avatars as well as with the in-world objects. The pre- service teachers are enrolled in a semester-long course, that is  delivered entirely in-world. All the content has been purposely  designed to meet the requirements of pre-service teacher training   in the domain of learning technologies. Therefore pre-service  teachers are immersed in an environment which targets the use of  and application of learning technologies in teaching and learning  practices within the classroom. The study-methodology that is  being used involves a quasi-experimental design, using a pre-  post-test method, for giving a measure to the effectiveness of the  3D immersive world using the technology acceptance model  (TAM). Beyond data collected using pre- and post-test survey  methods, we also collect the breadcrumbs which learners leave  as they go through the immersive environment using an  exploratory method of learning, whilst combining collaborative  practices with project-based learning. The data that is generated  from the learners pathways inside the virtual world is used to  understand not only the level of engagement of the learners with  the immersive environment but use it to predict the behavioral  intention to integrate learning technologies during their real  classroom practices. The assessment of learning inside the virtual  world is based upon structured activities that are carried out in- world, as well as interactions that occur during the in-world  residence. The real-time data that is collected over the virtual  world dashboard helps to identify the level of interactions  amongst avatars, the objects inside the world that the learner  avatars interact with most, and the activities that the learners do  inside the world.    3.2 Corporate training: immersion-on-the job  TELUS, a leading Canadian telecommunications service provider  has transformed its new employee training and on-boarding  programme with TELUS Collaboration House using AvayaLive  Engage. The program it is currently offering its new employees a  more cost-effective, efficient means to bring new employees into  the company and get them excited and productive early in the  employee lifecycle as part of its national growth strategy.  Therefore the 3D immersive space is basically being used to  relieve the cumbersome task of scheduling and significant  investments in time, travel and facilities. Since June 2011, more  than 1000 new TELUS employees have been trained and on- boarded through the Collaboration House. During the training  held in-world they go through set targets to get up and running  within the company. AvayaLive Engage gives enterprises the  possibility to set up training through formal, scheduled learning  events and sessions as well as informal meetings held amongst  company employees to facilitate accidental collaboration,  especially in contexts when employees may not have other  opportunities for meeting up. Collected data shows that  satisfaction rates amongst employees have soared.    4. RESULTS  The dashboard of the Avaya Live Engage platform gives us access  to information related to the visitor history, the duration of each  avatars residence, the conversation of the avatars and which part  of the virtual world, the resident avatars spend more time in. In  corporate training, important results that need to be displayed are  real time indicators of attendance, participation and engagement  and the reduction the corporate/training and education sectors  over-reliance on lag indicators in assessment. Analytics are also  used to assess the achievements of avatars, such as who interacts  most, who converts most opportunities, or who is the best  connector. The figures below show some examples of  preliminary data that is being collected throughout our  investigation.    232       Figure 1 - An overview of the dashboard from Avaya Live  Engage  Further to the overall dashboard additional data can be collected  using the volume usage relating to the users interaction with the  content inside the virtual world layout as well as the common user  paths that are traced as the in-world avatars traverse the virtual  world.      Figure 2 - Common User Paths tracing learner activities  inside the virtual world  The conversation history per authenticated avatar is also traced  giving an overview of the learners visits in relation to the  amounts of conversation minutes which they sustain inside the  world.      Figure 3 - A visualization of the data collected for the  conversation minutes versus the amounts of minutes that  participants spend in-world   5. IMPLICATIONS   Learning Analytics are increasingly showing that through the  collection and harvesting of big data we can understand more  about the learning pathways being taken by the learners and   through adaptive techniques we can modify our methods and  approaches to teaching in a way which would benefit the learners  most (Bienkowski, Feng, & Means, 2012). It has also been shown  that the visualization of data, by both learners and teachers can  indeed motivate learners to take actions in a way that they monitor  and moderate attention levels (Duval, 2011). The dashboard we  are using carries implications in corporate training that help  administrators and managers to understand the benefits and value  of the 3D immersive experience. The analytics help them estimate  the amount of money saved by hosting events and sessions  virtually rather than physically. In addition the analytics show the  flow of the learning experience, times of day when accessed,  length of time individuals stayed in the level, and whether the  trainees are active/passive. In higher education, we focus on  collecting data that can help us focus on user behavior modeling  and thus monitor the type of actions that can lead us to predict to  user engagement with the content that the virtual world presents  and with the virtual world connections that are established in- world. Some of the questions we are trying to focus on include:  which learning objects in-world are more effective for promoting  learning Does the use of an immersive space enable a greater  access to learning What modalities of learning can stimulate  learner engagement The data that we collect include learner  actions however we aim to use other data gathered from pre-and  post-test surveys, as well as from focus group sessions to be able  to combine the multiple sources of data and provide an integrated  picture of the learners chosen learning paths across the 3D  immersive space. We want to understand the level and depth of  engagement that learners inside the immersive environment reach  and whether this leads to behavior modeling and change that is  longer term. The limitations and challenges that we are currently  facing include the size of the population participating, and the  amounts of data that we are collecting which may be considered  to be small, albeit it being the entire cohort of pre-service  teachers, and the entire group employed on the workforce of the  company. We are also aware that although the data collected may  give indications about certain directions, the learners and the  teachers may themselves have different perceptions about the  learning that is occurring. Our future research lies in how learner  progress can be monitored in terms of his/her journey inside the  immersive space. Since inside the immersive spaces learning does  not follow a structured pathway, it is very difficult for the learners  to establish their position on a learning continuum. At this  moment in time, the dashboard that we are working with, can  deliver important information to course coordinators and  organisers however this information might not be considered  interpretable for the learners/course participants. Therefore our  future research lies in designing and implementing a model for  interpreting analytics by aligning a learning continuum with the  set world objectives. The information gathered from the data  collected in-world is then used to trace a learning path covered by  the learner. This path is also made available to the learner, so that  beyond the display of statistical data about where he/she visited  and how much he/she conversed, the data is translated into  attainment targets. These attainment targets may also equate to the  measurement of knowledge, competence and skills as part of the  overall behavioral changes that might lead us to predict the level  of engagement by the learners over a period of time.    6. CONCLUSION  In conclusion we have to say that the initial data that we are  collecting over the 3D virtual world is quite encouraging. The  data is showing that the majority of learners have already  established a working presence inside the 3D immersive space   233    that goes beyond our expectations. In corporate training the  response is also extremely satisfactory with reports of increased  employee collaboration that extend beyond the face-to-face  meetings. In the case of higher education pre-service teachers we  find that from the initial findings, there is an enthusiasm for this  modality for learning that is as yet rather unparalleled when  compared to courses taught by traditional face-to-face methods.  The data analytics also show that the participants are not  compartmentalizing their learning times into set day time, but they  log in at disparate times, have meetings late at night, and work  through weekends without any undue pushing from our end. We  believe that we are still at the initial stages for exploring further  research in the exploitation of learning analytics for immersive  spaces. The directions in which we want to take our research lie  not only in terms of prediction models, but also in the ways of  presenting meaningful information to our learners. At the focus of  our interest for both higher education and corporate training, are  the learners who design their own learning pathways inside a 3D  immersive world, using data in a way that can trace a meaningful  connection in a learning environment.    7. ACKNOWLEDGMENTS  Our thanks go to AvayaLive Engage offering constant support  throughout our investigations.    8. REFERENCES     [1] Bienkowski, M., Feng, M., & Means, B. (2012).  Enhancing Teaching and Learning Through  Educational Data Mining and Learning Analytics: An  Issue Brief . US Department of Education, Office of  Educational Technology. US: US Department of  Education.   [2] Calleja, G. (2011). In-Game: from immersion to  incorporation. London, UK: MIT Press.   [3] Camilleri, V., & Montebello, M. (2008). SLAVE   Second Life Assistant in a Virtual Learning  Environment. RELIVE08  Researching Learning in  Virtual Environments. Milton-Keyes: The Open  University.   [4] Carpenter, S. (2009). Virtual Worlds as educational  experience: Living and learning in interesting times.  Journal of Virtual Worlds Research , 2 (1), 3-4.   [5] de Freitas, S., & Oliver, M. (2006). How can  exploratory learning with games and simulations within  the curriculum be most effectively evaluated  Computers & Education (46), 249-264.   [6] de Freitas, S., Rebolledo-Mendez, G., Liarokapis, F.,  Magoulas, G., & Poulovassilis, A. (2010). Learning as  immersive experiences: Using the four-dimensional  framework for designing and evaluating immersive  learning experiences in a virtual world. British Journal  of Educational Technology , 41 (1), 69-85.   [7] de Laat, M., Lally, V., Lipponen, L., & Simons, R.  (2007). Investigating patterns of interaction in  networked learning and computer-supported  collaborative learning: A role for Social Network  Analysis. Computer-Supported Collaborative Learning .   [8] Dede, C. (2009). Immersive Interfaces for Engagement  and Learning . Science , 323, 66-69.   [9] Dede, C., & Barab, S. (2009). Emerging Technologies  for Learning Science: A Time of Rapid Advances.  Journal of Scientific Educational Technology , 18, 301 304.   [10] Duval, E. (2011). Attention Please! Learning Analytics  for Visualization and Recommendation. Learning  Analytics and Knowledge (pp. 9-17). Banff, Canada:  ACM.   [11] Ferguson, R. (2012). The State of Learning Analytics in  2012: A Review and Future Challenges. Knowledge  Media Institute. UK: The Open University.   [12] Freedman, T. (2011). Authentic Learning and ICT.  Retrieved June 2011, from ICT in Education:  http://www.ictineducation.org/home- page/2011/6/16/authentic-learning-and-ict.html   [13] Galarneau, L. (2005). Games and Simulations for  Transformative Learning. Games, Learning & Society  Conference. Madison, Wisconsin.   [14] Messinger, P. R., Stroulia, E., Lyons, K., Bone, M., &  Hiu, R. H. (2009). Virtual worlds  past, present, and  future: New directions in social computing. Decision  Support Systems , 47 , 204-228.   [15] Pardo, A., & Delgado Kloos, C. (2011). Stepping out of  the box. Towards analytics outside the Learning  Management System. Learning Analytics & Knowledge,  LAK (pp. 163-167). Banff, Canada: ACM.   [16] Portelli, J., & McMahon, B. (2004). Why Critical- Democratic Engagement . Journal of Maltese  Education Research , 2 (2), 39-45.   [17] Salen, K., & Zimmerman, E. (2004). Rules of Play:  Game Design Fundamentals. USA: MIT Press.   [18] Siemens, G. (2010). What are Learning Analytics  Retrieved 2012, from ELEARNSPACE:  http://www.elearnspace.org/blog/2010/08/25/what-are- learning-analytics/   [19] West, D. (2012). Big Data for Education: Data Mining,  Data Analytics, and Web Dashboards . Governance  Studies. Brookings, US: Reuters.               234      "}
{"index":{"_id":"34"}}
{"datatype":"inproceedings","key":"Lonn:2013:ICL:2460296.2460343","author":"Lonn, Steven and Aguilar, Stephen and Teasley, Stephanie D.","title":"Issues, Challenges, and Lessons Learned when Scaling Up a Learning Analytics Intervention","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"235--239","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460343","doi":"10.1145/2460296.2460343","acmid":"2460343","publisher":"ACM","address":"New York, NY, USA","keywords":"design-research, higher education, learning analytics, scale","abstract":"This paper describes an intra-institutional partnership between a research team and a technology service group that was established to facilitate the scaling up of a learning analytics intervention. Our discussion focuses on the benefits and challenges that arose from this partnership in order to provide useful information for similar partnerships developed to support scaling up learning analytics interventions.","pdf":"Issues, Challenges, and Lessons Learned When Scaling  up a Learning Analytics Intervention     Steven Lonn   University of Michigan  USE Lab, Digital Media Commons   1401B Duderstadt Ctr, 2281 Bonisteel   Ann Arbor, MI 48109-2094 USA   +1 (734) 615-4333  slonn@umich.edu   Stephen Aguilar  University of Michigan   School of Education & USE Lab  Suite 4215, 610 E University Ave   Ann Arbor, MI 48109-1259 USA   +1 (734) 764-8416  aguilars@umich.edu  Stephanie D. Teasley  University of Michigan   School of Information & USE Lab  4384 North Quad, 105 S. State St.    Ann Arbor, MI 48109-1285 USA   +1 (734) 763-8124  steasley@umich.edu     ABSTRACT  This paper describes an intra-institutional partnership between a  research team and a technology service group that was established  to facilitate the scaling up of a learning analytics intervention. Our  discussion focuses on the benefits and challenges that arose from  this partnership in order to provide useful information for similar  partnerships developed to support scaling up learning analytics  interventions.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3  [Computer Uses in Education] - General   General Terms  Management, Measurement, Performance, Design.   Keywords  Learning Analytics, Design-Research, Scale, Higher Education.   1 INTRODUCTION  Over the past few decades, most of the new learning technology  innovations in higher education have been tested in relatively  small, localized settings [1]. Given the personal and financial  costs of transferring these technologies to new contexts, and  providing training as well as hardware, it is (unfortunately) very  difficult for these technologies to scale beyond the initial scope of  the research project. Ideally, recent innovations in learning  analytics should be able to overcome these challenges and make  the leap from the focused and particular to the broad and general.  At their core, these technologies leverage rich and massive data  sets (i.e.,  big data ) that have the potential to generalize across  disciplines and individual learners. Purdue University, for  example, has scaled their Course Signals innovation to over 100  courses thus far, providing formative grade feedback to more than  23,000 students [2].   Our prior research mined Learning Management System (LMS)  data to better understand the influence of students in-system  behaviors on educational practices [3]. Building on this research,   we developed an Early Warning System to support just-in-time  decision-making around students' academic performance for the  academic advisors within two specific learning communities [4].  As we worked with academic advisors to identify what data they  required to increase and inform the academic support they  provided, we intentionally developed processes and displays that  could be adapted later to serve the needs of other academic  advisors and potentially other classes of users as well. To achieve  our vision of a robust system that facilitated these support  activities, we needed to identify a way to automate the data  extraction and transformation processes within an online  environment.   Although our research team has access to the LMS system logs  and the student information system (SIS), we do not maintain  these systems directly nor do we have the necessary infrastructure  within our research lab to host an institutional web service. Given  this, we concluded that it made sense to partner with an internal  organization tasked with building, running, and supporting the  highly technical infrastructureat a broad scale. The universitys   Information and Technology (IT) Service was the ideal partner  because they had the personnel and technical capacity to address  database design, storage, load testing, documentation, user  support, and other issues that would inevitably arise when  building a system meant to be widely used. We benefitted from an  existing relationship with IT based on the development and prior  research of the LMS that we were able to leverage and extend for  this project. Moreover, IT has been included and is committed to  supporting the institutional decisions pertaining to the emergent  technologies and processes related to learning analytics.   IT personnel, moreover, had an established track record of  maintaining and linking databases within the SIS and related  databases (e.g., the universitys data warehouse). Our product  would require access to such databases since the nature of the  analytics project specified that connections to various student  records databases would be necessary. Specifically, our design  required that student demographic and course history information  would eventually flow into an analytics database. IT personnel  had the capacity to devote time and resources to a new project,  and we were fortunate to partner with them so that we did not  need to hire outside expertise.   Through our partnership with IT, we gained the capacity to extend  our work to other academic advisors and to further investigate  how, when, and why student performance, effort, and  demographic data can inform engagement with students who are  in need of academic assistance. Long-term, the partnership will  allow our system to scale in size as well as sustainability, meaning   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK13, 8-12 April 2013, Leuven, Belgium.  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00   235    that operational staff can manage the system without the  involvement of the researchers who developed the initial design.   Intra-university partnerships can be an important and critical step  in the scalability for learning analytics innovations; yet these  partnerships are not free from challenges and complex issues.  Scholars have identified learning sciences research, in particular,  as being well positioned to unpack the different processes and  interconnected issues that permeate partnerships established for  the purpose of bringing technology innovations to broad scale [5].  This paper is therefore intended to share the benefits and  drawbacks of such partnerships and to provide a case study that  can inform the broader learning analytics community.    2 BACKGROUND  We developed an Early Warning System, Student Explorer [4], to  satisfy a tangible, small-scale problem for the academic advisors  in the M-STEM and M-BIO Academies. These academies provide  an integrated student development program for first- and second- year undergraduate engineering and biology students [6]. The  advisors in these academies required a way to track student  performance that would allow timely intervention during the  semester, rather than relying on final course grades.  Our Student  Explorer system provides the program advisors with weekly  updates on their students academic performance and effort.   These updates are presented through dashboards that provide  easily interpretable presentations of data that allow the advisors to  identify students who are in need of immediate support. Our long- term goal is to scale this functionality broadly so that the   feedback loops  between learners, teachers, and academic  advisors can be reduced in time and effort [7].  Our design-based research program is a collaborative effort  among the researchers and practitioners who are also our target  users [8]. By beginning with a relatively small set of users (the  academic advisors) and the related set of data (students'  performance and effort), we believed that we could improve the  data processes and displays at a relatively low cost in terms of  time and effort, while shaping user expectations for continuous  iterative development [9].    2.1 Initial Design of Student Explorer  In order to support the academic advisors and their decisions  about student engagement, we manually queried information from  the LMS Gradebook and Assignments tools in order to track  students' performance. We also used LMS course website login  events to act as a proxy for student  engagement  that is  consistent across all LMS websites. Two linked dashboards (an  overall summary screen and individual student/course  combination details) included figures displaying students'  developing grades and use of the LMS in comparison to their  peers, students' performances on specific assignments, and week- to-week classifications of students (Figure 1, left). The  institutional LMS' data structure is similar to popular LMS  platforms (e.g., BlackBoard, Moodle) and could, in theory, be  adapted and deployed using data from those systems.   Based on specific combinations of students' grades and course site  login frequency, Student Explorer displays whether advisors  should take one of three actions:  encourage  students to keep  doing well,  explore  students' progress in more detail, or  immediately  engage  students to assess possible academic  difficulties. These classifications are generated using three rules:  (1) whether a student's percent of points earned is at or above the  thresholds of 85%, 75%, or 65%; (2) whether a student is 10% or   5% below the course average in percent of points earned; and (3)  whether a student is below the 25th percentile in number of logins.  Absolute and relative percentages of points earned are given  predominance in our categorization algorithm while each students'   effort  or percentile rank for course logins is used to classify  students close to the boundaries of the first two rules.   We opted to design the initial implementation of Student Explorer  within Microsoft Excel, which allowed us to present the data in a  relatively sophisticated fashion with minimal coding or technical  development. After manually extracting and transforming the data  into the spreadsheet displays, we distributed the Excel file to the  academic advisors on a weekly schedule. While this procedure  did, for the first time, allow advisors to view and act on student  performance and effort data, the processing time required  effectively made the included data approximately 6-7 days old by  the time the advisors received the information.    2.2 IT's Selection of BusinessObjects  Before we began developing dashboards, our preliminary  exploration of how to conduct Extract, Transform, and Load  (ETL) processes for the LMS and SIS data and translate those  processes into user-facing dashboards included an investigation of  a variety of tools, including Tableau and Pentaho. Such analytics- specific tools are important in order to effectively scale learning  analytics systems and achieve long-term sustainability [10]. IT  personnel also reviewed the functionality of Pentaho, but in order  to move the project forward, they suggested that the existing  institutional practice of building an Oracle data structure with  ETL processing power would suffice. Layering over that database  infrastructure (i.e., the data  universe ), our IT partners suggested  that BusinessObjects software could be utilized as a way to  replicate most of the functionality of the Excel spreadsheets  previously developed by our research team. Collectively, the  driving belief was that this solution offered the fastest way to  explore the potential of online analytics-powered dashboards with  the least effort and at relatively low financial cost.   Before the partnership was established, however, IT explored the  feasibility of our project and placed it in their timeline of  priorities. This formal process was expected, and once we were  folded into their workflow, our partnership with IT allowed us to  leverage their existing infrastructure, staff skills, and institutional  software licenses of Oracle and BusinessObjects. The affordances  created by this arrangement allowed us to adapt our pilot system  of Student Explorer and prepare it for a larger audience, while IT  managers and technical staff gained experience working in the  emergent learning analytics domain [7]. This freed up our team  members considerablywe no longer had to spend 5-7 hours per  week (on average) manually conducting an ETL process. Instead,  these processes were automated against the LMS production and  archive servers, and relevant external data (e.g., grading data that  resided outside of the LMS) was included as well.    3 CHALLENGES  As with any partnership, various challenges can arise due to  unforeseen circumstances as well as different priorities and  sensibilitiesand our partnership with IT was no exception. In  the sections that follow, we detail some of these issues.   3.1 Usability Gaps: BusinessObjects  BusinessObjects is a software tool designed to allow users to use a  Graphical User Interface (GUI) to identify columns and related  criteria in relational databases in order to construct reports. These   236    reports are generated in a manner that is reminiscent of Microsoft  Excels pivot table feature, and produce similar deliverables. The  current iteration of Student Explorer simply automates this  process. It should be noted, however, that BusinessObjects is not  designed with the intention to produce analytics dashboards; it is  designed primarily to build tables from database queries.   The IT team has adapted BusinessObjects reports to duplicate and  in some cases, expand the functionality of the original Excel  spreadsheets our team had previously handcrafted for academic  advisors (Figure 1, right). In one example of the expanded  functionality, the advisor can now click on any given week in a  students performance history and see a detailed view of the  students course performance snapshot as of that date. This allows  the advisor to identify any changes on key assessments (e.g., a  grade that got changed on a midterm exam).    However, there are several limitations to BusinessObjects in terms  of user interface. The IT design team could not, for example,  replicate the behavior of our Excel sheets where clicking on the  course name opened the student detail report in a new tab. Instead,  BusinessObjects could only be used to create two separate  reportsthis requires the opening of a new browser window or  tab. Consequently, when a advisor is using the system to look at  data from multiple students, the potential for confusion and  frustration with the multiple windows and tabs open (none of  which return to the summary page) may prove cumbersome.    Additional quirks about the BusinessObjects interface include the  user interface for advancing through multiple pages being  available only though a small button located at the top of the  window and not at the bottom. Therefore, this button is easy to  miss, especially when advisors scroll to the end of the page.  Timeout limits from computers accessing the system through  wireless connections has also proven to be a difficult challenge to  both understand and resolve, and has led to unforeseen  complications that will be need to be addressed as the project  scales up in size.   3.2 Calculation Gaps: Errors in Manipulating  Gradebook Data   One technical challenge in scaling up an analytics application is  that undergraduate courses often involve careful manipulation of  grade curves, optional assignments, extra credit, weighting, and  other nuanced ways in which students are assessed. When   instructors assessment decisions are translated into the LMS  Gradebook tool, the resulting data appears to Student Explorer in  unexpected ways. These edge cases are easily identifiable by the  advisors who are very familiar with specific courses and  instructors, but an automated system cannot differentiate between  edge cases and standard cases as accurately.   For example, in a high enrollment Chemistry course, there are  three subsections of students that share one overall LMS site, and  certain assignments (e.g., lab work) apply only to individual  sections while other assessments (e.g., the midterm) apply to all  students. This leads to gaps in the data (appearing as null values  or zeros) that can be caught and re-coded by a knowledgeable  human coder. When our team conducted the ETL process into  Excel, we were able to hand-code the variations in this course so  that the student data could be parsed and aggregated properly by  the system.    When IT automated the ETL process in the online version of  Student Explorer, they encountered the same challenge but the  system could not hand-code the variations. We arrived at a  stopgap solution which involved only counting entered zero  values against the student and ignoring all null values. It should be  noted, however, this solution masks the issue of students who do  not turn in assignments, are absent from class, etc., so some  information is treated inaccurately. This troublesome automated  treatment of certain types of data risks making the tool less useful  overall for the end users, and might ultimately impede the kinds of  learning and teaching interventions that the original design made  possible.   In a related issue, our research team discovered after working with  the academic advisors that some instructors were using their LMS  Gradebook to record assessment grades, but chose to remove  those scores from the automatic calculation of the course grade  (we uncovered a variety of reasons for this behavior including  manual calculation of grades, extra credit, and grade curving). In  the first (manual) iteration of our system we were able to adjust  our database query to include these grades and display them for  the academic advisors, while also retaining their exemption from  the overall calculation of formative course performance (i.e.,  points earned divided by points possible). After several months of  investigation and testing, the IT team was able to create an  additional column in the student detail view indicating whether  the individual item was included in the overall class grade.   Figure 1. Screen Shots of Student Summary Data Displays in Student Explorer.    Excel  version (Left) and  BusinessObjects  version (Right) Examples.   237    Our conclusion from these coding issues is that while  BusinessObjects is a reasonable cost-savings solution, it may not  be nimble enough to be responsive to idiosyncratic cases like the  one outlined above. This may end up being an irreconcilable  challenge to the ability of the system to scale beyond the M- STEM community of users. Nonetheless, the academic advisors  who have used the system have been appreciative of the ability to  access the system online and view weekly data from any computer  with an Internet connection.    3.3 Access Gaps: Two-Factor Authentication  In order to better secure sensitive data in the institutional data  warehouse, such as Family Educational Rights and Privacy Act  (FERPA)-protected information, our institution has implemented  a two-factor authentication system that utilizes a  username/password combination as well as a 6-digit random  number stored on a remote keychain assigned to an authorized  individual. Since BusinessObjects is the adopted interface used to  access data of this sort, it can only be accessed by using the two- factor process outlined above.    Two-factor identification access is not automatically granted to all  university staff and must be approved at the management level.  Initially, two of the four M-STEM and M-BIO advisors did not  have this level of access. Getting the approvals, multiple  authorizations, and the physical keychain device took time and  energy on the part of the advisors. This authentication process  may prove to be a barrier against scalability, as faculty and  undergraduates do not routinely have this kind of access. In order  to deliver this kind of dashboard to faculty and/or students, it will  be necessary to find a technical alternative in the future to be able  to deliver the data displays to these users.   3.4 Performance Gaps: Impact on Enterprise  Systems     One of the primary reasons we decided to partner with IT is that  Student Explorer, by necessity, interacts with both archival as  well as production LMS data. As we investigated the possibilities  of scaling up our pilot design, IT made sure to include load testing  against the production servers as part of the project timeline in  order to mitigate the risk of an adverse system event.    The extraction of the Gradebook, Assignments, and login data had  a marginal impact on LMS performance in our manual ETL  processes, largely due to our limitation to only extract data for the  courses in which M-STEM and M-BIO students were enrolled.  However, as IT developed the analytics data universe, they  decided to include all Gradebook and Assignments data for the  current term to accommodate an eventual comprehensive scale for  systems like Student Explorer that would access the databases. In  October 2012, this larger extraction caused a system failure and  unintended shutdown of the production LMS when the servers ran  out of allocated memory. While the root cause of this shutdown  was quickly rectified, this event highlights the challenges of  learning analytics solutions that utilize interrelated systems and  processes to produce timely data displays for end users.   3.5 Automatization Gaps: Manual  Maintenance of Cohort and Advisor  Information   Although critical activities (such as ETL processing) were  automatized by the IT team during the evolution of our pilot  project, not all components were automatized. For example,  importing which student cohorts (e.g., M-STEM and M-BIO   students) are included in Student Explorer continues to be a  manual process in the current IT-maintained data universe. First,  students to be included are identified by the individual programs  using admissions metrics from the data warehouse, which are  forwarded to IT using spreadsheet files. These files are then  uploaded into the database, thus populating a student table. The  ETL process then uses the imported lists to match students to their  courses and LMS sites. In order to scale this solution in future  iterations, the system will have to be able to identify all known  groups of students and their corresponding academic and/or  programmatic advisor, or we will have to find a way to manually  enter this information in an efficient manner.   Finally, many (but not all) students have their assigned academic  advisor listed in the data warehouse, but any program affiliation,  including the growing number of learning communities and  special mentoring programs, like M-STEM and M-BIO, are often  missing from this central database. These kinds of gaps in student  data can impede scaling of analytics systems, particularly data that  serves to group and sort students in order to present relevant  analytics to the desired end user.    4 DISCUSSION  The iterative and fluid nature of design research projects often  leads to sporadic and non-linear modes of development for new  innovations when the work culture of collaborators are dissimilar.  For example, for faculty, a casual conversation with a colleague  may lead to a new insight and generate ideas to test in a pilot  project. By contrast, most institutional technology departments are  often run like a for-profit business, where products and processes  are managed in strict pipelines and outcomes are measured in  number of users served and terabytes used. These divergent  cultures and work processes can be reconciled, but the process of  doing so can also lead to misaligned incentives and development  delays.  Overall, our research team's partnership with IT allowed  us to explore and innovate new approaches to leverage learning  analytics to provide a pathway to bringing them to scale.  However, there were several times when our organizational work  processes did not align.   First, IT's implementation of our project was delayed due to  existing projects (e.g., LMS hardware replacement) that were a  higher priority for the institution. Later, the IT version of Student  Explorer was locked in terms of additional data inclusion and  query changes due to the stringent requirements guidelines by  which IT projects are organized. Because IT operates with  multiple projects prioritized in their long-term pipeline, a delay in  their deliverables is potentially very costly. In the current term, IT  is approaching the BusinessObjects delivery of analytics-powered  displays as a high priority pilot project. To that end, our research  team is routinely contacted to help address technical problems that  are necessary to solve in order to scale the system more broadly.    While using BusinessObjects may not be the ultimate solution for  scaling-up Student Explorer, IT's commitment to addressing  usability and technical challenges has produced a powerful and  collaborative partnership that can serve as a model for future  learning analytics projects at our institution and beyond. A  summary of the challenges described in this paper is provided  below (Table 1). We also suggest possible solutions that other  learning analytics projects should consider and explore when  establishing partnerships between researchers and technologists.       238    Table 1. Summary of Challenges and Possible Solutions   Challenge(s) Example(s) Possible Solution(s)   Institutional  Governance  and Resources   Openness of IT  organization to  external  partnerships.  Availability of  site-licensed  software solutions.   Partnership needs to  benefit both the IT  organization and the  researchers in some  way. Keep the lines of  communication open.   Usability Gaps   Scaled software  solution may  behave differently  or have different  features than the  pilot version.   Flexibility and  additional training.  Modifying the large- scale software to  approximate the pilot  version.   Calculation  Gaps   Edge cases in  terms of online  Gradebook  structures, LMS  use, etc.   Hand coding, nimble  tweaks, etc. Ultimately,  may lead to change in  large-scale software  solution.   Automatization  Gaps   Persistent manual  processes (e.g.,  lists of student  cohorts) required  for large-scale  solution.   Automatic processes  must be created and/or  modified in  conjunction with new  fields in the data  warehouse.   Access Gaps   Two-factor  authentication that  not all intended  end users possess.   May require a work- around, if possible, a  change in the way the  data is displayed, or a  change in software.   Performance  Gaps   Scaled ETL  process may have  adverse effects on  production systems  (e.g., LMS).   Accurate (as close as  possible) load testing in  combination with peak  load scenarios on the  production systems.     The partnership between our research team and IT is one example  of how focused learning analytics innovations can be scaled  within an institution. Appropriate skills from both partner  organizations have been applied to allow for growth in terms of  size and functionality, as well as broader and more generalizable  research investigations. Recognition of the challenges and  processes highlighted in this paper may be useful to other  institutions who are planning to scale learning analytics  innovations either within their own infrastructure or by utilizing  an outside vendor.    Furthermore, it is important to note that our learning analytics  innovation, Student Explorer, utilized student data that is captured  and maintained within our home institution. There may be  additional challenges when the source data resides with an  external vendor, such as whether there is access to non-aggregate  data that can be reliably integrated with the institutional data  warehouse. Institutional leaders need to be mindful of these issues  and challenges when negotiating new contracts educational  technology vendorsthese contracts need to acknowledge not  only the service level agreement, but a data access agreement as  well.   As learning analytics solutions serve to break down the technical  barriers between  silos  of data, it is important to recognize that  technical, cultural, and process-oriented challenges may be  unavoidable as different groups of professionals work together   toward a common goals of improving teaching and learning in  education. Addressing these challenges directly between partners  with complimentary expertise has the potential to lead to more  successful and broadly applicable learning analytics solutions  overall.   5 AKNOWLEDGEMENTS  We would like to acknowledge Andrew Krumm and Joseph  Waddington for their contributions to the original design of  Student Explorer. We also thank Gierad Laput, Amine Boudalia,  and SungJin Nam for their work in the USE lab, and to the  continued participation and feedback from our partners in the M- STEM and M-BIO Academies. Finally, many thanks to Daniel  Kiskis and his fellow analytics team members from IT.   6 REFERENCES    [1] Fishman, B. J. (2005). Adapting innovations to particular  contexts of use: A collaborative framework. In C. Dede, J. P.  Honan, & L. C. Peters (Eds), Scaling up success: Lessons  learned from technology-based educational improvement  (pp. 48-66). San Francisco: Jossey-Bass.   [2] Arnold, K. E. & Pistilli, M. D. (2012). Course signals at  Purdue: Using learning analytics to increase student success.  Paper presented at The 2nd International Conference on  Learning Analytics and Knowledge. Vancouver, BC, Canada.   [3] Lonn, S., Teasley, S. D., & Krumm, A. E. (2011). Who needs  to do what where: Using learning management systems on  residential vs. commuter campuses. Computers & Education,  56(3), 642-649. doi:10.1016/j.compedu.2010.10.006   [4] Lonn, S., Krumm, A. E., Waddington, R. J., and Teasley, S.  D. (2012). Bridging the gap from knowledge to action:  Putting analytics in the hands of academic advisors. Paper  presented at The 2nd International Conference on Learning  Analytics and Knowledge. Vancouver, BC, Canada.   [5] Roschelle, J., Bakia, M., Toyama, Y. & Patton, C. (2011):  Eight issues for learning scientists about education and the  economy. Journal of the Learning Sciences, 20(1), 3-49.   [6] Davis, C. S., St. John, E., Koch, D. & Meadows, G. (2010).  Making academic progress: The University of Michigan  STEM academy. Proceedings of the joint WEPAN/NAMEPA  Conference, Baltimore, Maryland.   [7] Clow, D. (2012). The learning analytics cycle: Closing the  loop effectively. Paper presented at The 2nd International  Conference on Learning Analytics and Knowledge.  Vancouver, BC, Canada.    [8] Cobb, P., Confrey, J., diSessa, A., Lehrer, R., & Schauble, L.  (2003). Design experiments in educational research.  Educational Researcher, 32(1), 9-13, 35-37.   [9] Bienkowski, M., Feng, M., and Means, B. (2012). Enhancing  teaching and learning through educational data mining and  learning analytics: An issue brief. Report submitted to the  Office of Educational Technology, U.S. Department of  Education.    [10] van Barneveld, A., Arnold, K. E., & Campbell, J.P. (2012).  Analytics in higher education: Establishing a common  language. Boulder, CO: EDUCAUSE Learning Initiative.   239      "}
{"index":{"_id":"35"}}
{"datatype":"inproceedings","key":"Prinsloo:2013:EPF:2460296.2460344","author":"Prinsloo, Paul and Slade, Sharon","title":"An Evaluation of Policy Frameworks for Addressing Ethical Considerations in Learning Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"240--244","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460344","doi":"10.1145/2460296.2460344","acmid":"2460344","publisher":"ACM","address":"New York, NY, USA","keywords":"distance learning, ethics, learning analytics, policy","abstract":"Higher education institutions have collected and analysed student data for years, with their focus largely on reporting and management needs. A range of institutional policies exist which broadly set out the purposes for which data will be used and how data will be protected. The growing advent of learning analytics has seen the uses to which student data is put expanding rapidly. Generally though the policies setting out institutional use of student data have not kept pace with this change. Institutional policy frameworks should provide not only an enabling environment for the optimal and ethical harvesting and use of data, but also clarify: who benefits and under what conditions, establish conditions for consent and the de-identification of data, and address issues of vulnerability and harm. A directed content analysis of the policy frameworks of two large distance education institutions shows that current policy frameworks do not facilitate the provision of an enabling environment for learning analytics to fulfil its promise. ","pdf":"An evaluation of policy frameworks for addressing ethical  considerations in learning analytics   Paul Prinsloo  University of South Africa   TVW4-69, P O Box 392, Unisa  0003, South Africa  +27 12 429 3683   prinsp@unisa.ac.za   Sharon Slade  Open University    Foxcombe Hall, Boars Hill  Oxford, UK   +44 1865 327000  sharon.slade@open.ac.uk      ABSTRACT  Higher education institutions have collected and analysed student  data for years, with their focus largely on reporting and  management needs. A range of institutional policies exist which  broadly set out the purposes for which data will be used and how  data will be protected. The growing advent of learning analytics  has seen the uses to which student data is put expanding rapidly.  Generally though the policies setting out institutional use of  student data have not kept pace with this change.   Institutional policy frameworks should provide not only an  enabling environment for the optimal and ethical harvesting and  use of data, but also clarify: who benefits and under what  conditions, establish conditions for consent and the de- identification of data, and address issues of vulnerability and  harm. A directed content analysis of the policy frameworks of two  large distance education institutions shows that current policy  frameworks do not facilitate the provision of an enabling  environment for learning analytics to fulfil its promise.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education - Distance learning, K.7.4 [The Computing Profession]:  Professional Ethics  Codes of ethics     General Terms  Management, Documentation, Security, Legal Aspects.   Keywords  learning analytics, ethics, distance learning, policy   1. INTRODUCTION  The majority of institutions have long employed academic  analytics for reporting, operational and financial decision-making,  and quality assurance purposes [1,2]. However, while learning  analytics is heralded as one of the key trends expected to  significantly impact on the shape of higher education within the  next few years [3,4,1], many institutional policy frameworks fail  to fully reflect the use and ethical implications of learning  analytics. The increasing digitisation of learning has resulted in  the availability of real-time data on an unprecedented scale,   creating new opportunities for harvesting digital trails of students  (non)engagement [5,1,6]. Knowing more about students learning  processes and trajectories allows the potential for higher education  institutions to offer personalised and customised curricula,  assessment and support to improve learning and retention [1,7].   Realising the promise of learning analytics will require  institutions to align their policies with national and international  legislative frameworks; to consider the ethical issues inherent in  the harvesting, use and dissemination of data and to ensure an  enabling environment for adequate resourcing and integration of  institutional support.   This paper will analyse the existing policy frameworks of two  large distance education institutions according to a set of  considerations developed by Slade and Prinsloo [8]. The Open  University in the UK (OU) operates largely within a developed  world and the University of South Africa (Unisa) within a  developing world context, each with considerably different  student profiles, business architectures and programme  qualification mixes. However, both are specialist open distance  learning (ODL) institutions with huge student numbers.  Furthermore, both grapple with balancing the tensions inherent in  the massification of higher education and openness, and their  commitment to quality provision, accreditation and planning  interventions to address concerns about throughput rates.    2. LEARNING ANALYTICS AS MORAL  PRACTICE - A SOCIO-CRITICAL  APPROACH  Defining and addressing ethical issues in learning analytics  depend on a number of epistemological and ideological  assumptions. Our own epistemological and ideological framework  falls within the broad scope of a socio-critical approach which  entails being critically aware of the way our cultural, political,  social, physical and economic contexts and power-relationships  shape our responses to the ethical dilemmas and issues in learning  analytics Choosing a specific socio-critical approach allows, inter  alia, engagement with both the potential and challenges of  learning analytics and recognises the unequal power-relations  between students and the institution.    Analysing a diverse range of literature, Slade and Prinsloo [8]  propose a number of considerations from which institutions can  develop context-specific and appropriate guidelines and policy  frameworks. These considerations are:     Who benefits and under what conditions    Conditions for consent, de-identification and opting out    Vulnerability and harm    Collection, analyses, access to and storage of data    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   240    3. METHODOLOGY  This paper entailed a two-stage qualitative research design. The  first stage applies a directed content analysis approach [9].   Institutional policies from both the OU and Unisa were identified  and cross-analysed to defamiliarise and ensure validity and  reliability. Trustworthiness was ensured by member-checking.   Neither institution has policies covering the analysis and use of  data with the explicit purpose of understanding, predicting and  influencing student learning. Both have specific policies relating  to the ethical use of student data for research. All other policies or  frameworks with reference to monitoring, surveillance, privacy  and security of data, etc., in the two institutions were identified  and reviewed for cues or guidelines specifically pertaining to the  use, analysis, dissemination and storage of educational data.  The second stage of the research design involved the evaluation of  the respective policies of the OU and Unisa against the set of  considerations [8] discussed above.    4. AN OVERVIEW OF CURRENT POLICY  FRAMEWORKS AT UNISA AND THE OU  There is currently no single policy at either institution which  covers, per se, the notion of learning analytics. In order to  establish potential gaps or guidelines pertaining to proposed  considerations [8], all policies of both institutions relating to data,  teaching and learning and research were scrutinized.    4.1 Who Benefits and Under What  Conditions  While authors such as Kruse and Pongsajapan [10] propose a  student-centric approach to learning analytics, Slade and  Prinsloo [8] state that it is crucial that all key stakeholders in  learning analytics should benefit from learning analytics. Such an  approach not only involves students, but everyone involved in  delivering and supporting learning.    However, we acknowledge that students are a major stakeholder  and should be seen as active agents in defining the purpose and  scope of collected data, as well as conditions for its use (e.g. de- identification). While there is some information accepted to fall  within the normal scope of the registration agreement between the  student and the institution, other categories of information will  require informed consent and an active commitment from students  to ensure the correctness and currency of data.    4.1.1 Unisa  While the different policies and guidelines pertaining to  conducting research using student data address issues of privacy,  informed consent, vulnerability, harm and benefit, there is no  indication in any Unisa policy or guideline document  communicating the harvesting and the conditions of harvesting of  data from students to the student community. References are  provided for Unisa documents in the public domain.   The Guidelines for Conducting Research Involving Unisa  Staff, Students or Data [11] explicitly excludes institutional  research or authorised routine data gathering activity, necessary  for the efficient administration and operation of Unisa (p.1).  When students register, they declare that the information they  provide is correct and current.   The benefits of harvesting and analysing educational data are not  explicitly highlighted in any Unisa policy or guideline document.  Students are also not provided with information on how their data  is used, by whom and under which conditions.    4.1.2 Open University  The majority of OU policy documents may be accessed through  the OU website [12]. The OU Student Community Charter  includes a number of overarching principles which aim to  establish a shared responsibility between University and student.  The Charter sets out the Universitys intention to anticipate and  respond positively to different needs and circumstances by  providing support that is appropriate for each individual learner  and their subject area of study.  The Retention of Student Data and Records policy makes clear  that some data will be depersonalised and retained for uses  relating to management, development and research.   The policy states that personal data owned by the university may  be shared with third parties; and conversely personal data owned  by other organisations may be shared with the OU. Where the  third party is acting as a university agent, the university remains  the data controller. The third party must adhere to the universitys  student data retention and security policies. Where the third party  takes ownership of OU student data (e.g. government agencies,  sponsors, etc.), the third party becomes the data controller. The  data is then subject to that third partys data retention policies.    The alumni office has an objective to both keep alumni up to date  with University activities and to pursue donation prospects. The  policy setting out the Retention of Alumni Data and Records  requires alumni contact details and contact history (e.g. donations  made) to be held indefinitely.   4.2 Conditions for Consent, De-identification  of Data and Opting Out  Considering conditions for consent, de-identification of data and  the option to opt-out of the collection of certain types of data  refers to the notion of informed consent and transparency, (e.g.  information regarding the uses to which their data might be put,  algorithms used to analyse data, etc.)    4.2.1 Unisa  While there are guidelines which include explicit conditions for  research on Unisa students, there is no guidance in any policy  where students are informed either that data will be harvested and  used, or which data will be harvested. The Interception and  Surveillance Policy states that Unisa may monitor or track,  although the policy applies specifically to the monitoring and  surveillance of employees. The Data Privacy Policy defines  students as consumers, that is, any natural person who enters ...  into an electronic transaction with a supplier as the end user of the  goods or services offered by that supplier; e.g. students [p.1]. The  purpose of the Data Privacy Policy is to protect the privacy of  privacy subjects; provide guidelines for the collection, use,  disclosure and maintenance of personal information by Unisa;  limit Unisas possible liability for privacy infringement; and  educate users on privacy and related rights (p. 2).    This policy also stresses that Users may only collect personal  information on privacy subjects if such information is necessary  for business purposes of Unisa, or when the privacy subject has  given permission that his/her personal information be collected by  or on behalf of the Unisa (p. 2). This implies that the collection,  analysis and use of student data is within the legal parameters of  the business purposes and is therefore legitimate. The collection  of information to inform the business of Unisa is confirmed by the  Guidelines for conducting research involving Unisa staff,  students or data [11] which states that while the purpose of this  document is to provide guidelines for acquiring permission to do   241    research that involves Unisa staff, students and/or data (p.1),  these guidelines do not apply to [the Department of Institutional  Statistics and Analysis] DISA research approved by the Vice  Principal: Research and Innovation, or to duly authorised routine  data gathering activity, which is necessary for the efficient  administration and operation of Unisa (p. 2).   None of the existing policies therefore mandates the university to  explicitly inform students that their behaviour may be monitored  or surveilled, or provides students with the opportunity to opt out  of these actions. Students may opt out as objects of research, and  the Policy on Research Ethics [13] makes informed consent and  anonymity non-negotiable (unless the latter is waived by the  participant him or herself). The Students Charter on Rights  and Responsibilities [14] makes no mention of data privacy,  access to own personal data, or to ensuring the correctness and  currency of personal data (whether from the perspective of the  institution or as a specific responsibility of students).    4.2.2 Open University  Within the Data Protection policy, students are informed that  some information, including the information you give us about  your ethnic background or a disability, may be used by the  University to identify students who require additional support or  specific services. We consider disclosure of this information as  explicit consent to use this information for this purpose (p. 1).    The OU Terms and conditions governing the use of software,  tools and content document sets out that material produced and  uploaded to the OU LMS may be used by The Open University  on an irrevocable and perpetual basis and may be incorporated  into module material and other content (p.3).   4.3 Vulnerability and Harm  Vulnerability and harm are defined as implicit or explicit  discrimination (whereby a student receives, or does not receive,  support based on what might be considered to be a random  personal characteristic), the consequences of labelling (on student  identity and behaviours) and the validity of regarding student  groups based on assumptions made about shared characteristics.    4.3.1 Unisa  If we consider that issues of privacy are directly linked to notions  of vulnerability and harm, there are several policies and guidelines  dealing with the protection of data and the prevention of harm.   The Unisa Information Security Policy prescribes a three-tier  classification system for information: namely confidential,  internal and public use. The purpose of the policy is to protect  Unisa's corporate data and information and any client, employee  or student information within its custody or safekeeping by  safeguarding its confidentiality, integrity and availability (p. 4).   Personal information is very broadly defined in the Data Privacy  Policy as encompassing a variety of data including (but not  limited to) information relating to the race, gender, sex,  pregnancy, marital status, national, ethnic or social origin, colour,  sexual orientation, age, physical or mental health, disability,  religion, culture, language and birth of the individual; information  relating to the education or the medical, criminal or employment  history of the individual or information relating to financial  transactions involving the individual; and correspondence sent by  the individual that is implicitly or explicitly of a private or  confidential nature or further correspondence that would reveal  the contents of the original correspondence.  Much of the data  above might be used within a learning analytics profiling model.  This then raises the interesting ethical issue regarding the use of   such data to personalise or customise the learning experience  without the explicit consent or refusal of consent by students.    Interestingly, the correctness of data provided to students by the  institution and vice versa can play a huge role in the scope and  permanence of vulnerability and harm. The Unisa Students  Charter on Rights and Responsibilities [14] is silent on matters  of data privacy, and on the students responsibility to ensure that  data provided to the institution are correct. The Students  Disciplinary Code [15] includes a statement under the description  of misconduct concerning the provision of materially false  information about the University (p. 4), but excludes provision of  materially false or incorrect personal data to the institution.    4.3.2 Open University  The OUs Information, Advice and Guidance Policy (IAG)  highlights OU objectives to empower students to achieve their  study goals and to develop independence in their decision-making  by providing timely and targeted IAG to students at key points  along the student journey that recognises and is responsive to  diverse and distinct need by ensuring online information and  advice is personalised, accessible, accurate, up to date and applies  innovative technology. It aims to provide a service which  respects the needs of the individual student and is in their best  interests. The policy does not discuss whether the support is in  the best interests of the individual student or the wider cohort to  which a student belongs, nor who makes decisions regarding best  interests. The lack of clarity on this point suggests the potential  for a dual role for the student which appears currently absent in  practice. Nor is it clear whether there are further guidelines which  determine the point at what tailored IAG becomes untenable in  terms of available (and affordable) resource.   The IAG policy explicitly recognises the diversity of student  backgrounds and educational experience, and flags that the  service delivered will be targeted to the specific needs of  enquirers and students at different stages of their student journey.  In this way, there is a tentative attempt at least to be transparent  about differential levels of service, about assumptions made about  student groupings which may relate to their shared characteristics,  and to recognise that these profiles may change over time.    Students are informed, when they first register that they must  notify the University within a reasonable time if they change their  personal details. When informing the University of a disability  which might affect their studies, students must provide further  evidence as required. Similarly, if any module requires the student  to meet specific conditions, there is a responsibility to inform the  University should those conditions no longer be met.  The OUs  Fraud Response Policy makes it clear that any student  intentionally and dishonestly making a false representation or  dishonestly failing to disclose information is considered to have  committed fraud for gain (where gain is assumed to extend  beyond the purely financial). The Code of Practice for Student  Discipline also includes as unacceptable conduct knowingly  making a false statement or fraudulently providing information (at  registration or when asking for a particular service).   4.4 Collection, Analyses, Access to and  Storage of Data  This aspect relates to information about sites (both inside and  outside of the LMS) used to gather information and to give  informed consent regarding the scope and rights of the institution  to harvest, analyse and use data from such sources. Students  should be told which information is integral to official   242    institutional business, which information may be harvested with  or without consent, and how they and others can help to ensure  the correctness, currency and appropriateness of data. Students  should be informed about uses of their data at registration.    4.4.1 Unisa  This consideration is covered extensively by the policy framework  at Unisa. The collection, analyses, access to and storage of student  data is not explicitly mentioned but implied within the broader  guidelines on data privacy. For example, the Data Privacy Policy  focuses explicitly on the institutions responsibility to safeguard  staff and operational data rather than a duty for students to respect  the data of fellow students or staff. The Information Security  Policy proposes a three-tier classification system for information:  namely confidential, internal and public use. The purpose of the  policy is to protect Unisa's corporate data and information and  any client, employee or student information within its custody or  safekeeping by safeguarding its confidentiality, integrity and  availability (p.4). The same principles are addressed in the  Information Sensitivity Classification Policy [16] which allows  that a single lapse in information security can have significant  long-term consequences and that Unisa unduly risks loss of  student relationships, loss of public confidence, internal  operational disruption, excessive costs and competitive  disadvantage (p.1). This policy states also that its intention is to  consistently protect confidential information regardless of its  form, the technology used to process it, who handles it, its  location, and the stage in its information lifecycle.  Under Access  control the principle of need to know is established, meaning  that information must not be disclosed to any person who does  not have a legitimate business need for the information (p.2).   The Records Management Policy [17] defines record as  recorded information, regardless of format or medium, which has  been created, received, used, accessed and maintained by Unisa  (and/or predecessors) as evidence and information in pursuance of  its legal obligations or in the transaction of business. Included are  e-mail, electronic records and records other than correspondence  (p.1). Access to records is governed by the sensitivity  classifications allocated to record series and detailed in the  Information Sensitivity Classification Policy. [16] Access to  records by employees or third parties is dealt with in accordance.   4.4.2 Open University  The OU has a clear Data Protection Policy establishing that  student records are created and maintained over significant  periods of time. The student record includes data collected at  registration and throughout the student journey. The data  controller is clearly given as The Open University, although it is  acknowledged that external service providers may process  personal information under strict contractual confidentiality  obligations. Students are informed that personal information is  used to:    process applications;    provide services (including providing certain online facilities   and/or services and sending information about current and  future study opportunities with the University);     conduct research to help plan and improve university services;    produce statistical information for publication;    provide information about students to others, in line with legal   and government requirements. The OU will transfer personal  information outside of the European Economic Area only  when necessary safeguards have been secured by contract.     allow others to provide services to students and alumni  This policy states that data may be transferred within the  University on a need-to-know basis to facilitate the provision of  academic and other services to students. There is no clear  guidance given regarding these academic or other services, nor  any explanation of what may define a need to know basis.   Within the Terms and conditions governing the use of  software, tools and content document, students are advised to  check the terms and conditions and privacy policy of any other  (external) website you visit. Within the University LMS, students  are informed via the Data Protection policy that Cookies are used  so that we can easily recognise you when you return to our  websites and, as a result, will enable us to provide you with a  better service. We may also track user traffic patterns in order to  determine the effectiveness of our website. Information obtained  from these cookies will not be used for marketing purposes or  released to third parties.(p.4). Students may opt not to receive  cookies while browsing the Universitys website, but would not  then be able to access password-protected sites.   The Open University Data Protection Policy states that  Information is protected from unauthorised access and we are  confident no one will be able to access your personal information  unlawfully. Any personal information transmitted from a  students browser to the OU web service, or from the service to  the students browser, is encrypted (as long as the students web  browser supports the Secure Sockets Layer (SSL)). Students are  explicitly warned that internet email is not always secure and that  they take responsibility for information. Students are also made  aware that they may access personalised stored data, as well as an  overview of those stakeholders granted access to specific datasets.    The Freedom of Information Code of Practice limits public  access to information to non-personal recorded information.  However, the students rights to privacy under the Data Protection  Act 1998 outweigh the rights of other individuals to access their  information under the Freedom of Information Act 2000 except  under certain public interest exemptions.   The Retention of Student Data and Records Policy sets out the  conditions under which student data is maintained and covers all  student data (relating to an identifiable individual), information,  records and content relating to university business created by  university staff or students. The UKs Data Protection Act 1998  requires that student records are retained only as long as is  necessary, and should be accurate and up-to-date. As a student can  continue to study modules for many years, the deletion of certain  information after a set time with a requirement for the student to  re-submit up-to-date information ensures compliance with this  principle. The policy states both that There is an expectation by  students, employers and Government agencies  that Universities  should retain a permanent core record of student names, the  modules and qualifications studied and their outcomes (p.3), and  that there are records and data which need to be retained whilst a  student might continue to study with the OU.    On completion of their qualification, students become OU alumni.  The policy on the Retention of Alumni Data & Records  requires that alumni contact details continue to be held in student  record systems to ensure a single instance of accurate information.   Students are also advised under the guidance document Using  Social Networking Tools that they should take care to avoid  activity that infringes another person's privacy (e.g. by posting  their contact details without permission).   243    5. SUMMARY OVERVIEW AND  DISCUSSION  Having reviewed the policies of both Unisa and the OU against  the proposed considerations [8], it seems clear that the  institutions current policy frameworks largely focus on academic  analytics and research with an emphasis on data governance, data  security and privacy issues. Since learning analytics is more  concerned with learning data at course and departmental level [1],  both institutions policy frameworks appear to lack explicit  guidance for the questions, issues and ethical challenges to  institutionalise learning analytics.    While both institutions have classification systems for  categorising information, neither makes a distinction between the  different layers of information harvested from students in terms of  consent and opportunities to opt-out. It is accepted that there are  certain types of information and analyses (e.g. cohort analyses)  that fall within the legitimate scope of business of higher  education. There is though an urgent need to approach personal  data differently when it is used to categorise learners as at-risk, in  need of special support or on different learning trajectories.  Currently both institutions employ relatively crude and  incomplete data sets to customise learning and support. Both  institutions have ample policies and guidelines to protect data and  to ensure that data is governed according to national and  international legislation.    It is clear from the existing policy frameworks of both that the  definition and scope, harvesting and analyses of data is an  imbalanced and non-transparent affair.    6. CONCLUSION  Educational data mining is established practice in higher  education and the increasing digitisation of education,  technological advances, the changing nature and availability of  data have huge potential for learning analytics to contribute to our  understanding of the different variables impacting on the  effectiveness of learning, student success and retention.  Most higher education institutions have existing policy  frameworks in response to (inter)national legislative contexts to  regulate and govern intellectual property, safeguard data privacy,  and regulate access to data. These policy frameworks may not  always be sufficient to address the specific ethical challenges in  the harvest and analysis of big data in learning analytics.    Approaching learning analytics from a socio-critical perspective  [8] suggests gaps in the policy frameworks of two large distance  education institutions: the OU and Unisa. Both these institutions  policy frameworks offer extensive protection and regulation with  regard to data privacy and protection.    This brief review of both institutions policy frameworks  highlights the irregularity of learning analytics where the  institution is the only role-player with decision-making power,  determining the scope, definition and use of educational data  without the input of other stakeholders.   This research indicates that some higher education institutions  policy frameworks may no longer be sufficient to address the  ethical issues in realising the potential of learning analytics.     7. ACKNOWLEDGEMENTS  The authors would like to acknowledge the OU and Unisa for  their support, and Aimee Rhead for her preparation work.   8. REFERENCES  [1] Long, P. & Siemens, G. 2011. Penetrating the fog: Analytics   in learning and education. EDUCAUSE Review  September/October, 31-40.   [2] Van Barneveld, A., Arnold, K.E., & Campbell, J.P. 2012.  Analytics in higher education: establishing a common  language. ELI paper 1: 2012. EDUCAUSE. Retrieved from  http://net.educause.edu/ir/library/pdf/ELI3026.pdf   [3] Booth, M. 2012. Learning analytics: the new black.  EDUCAUSE Review, July/August, 52-53.   [4] Johnson, L., Adams, S., & Cummins, M. 2012. The NMC  Horizon Report: 2012 Higher education Edition. Austin,  Texas: The New Media Consortium.   [5] Oblinger, D.G. 2012. Lets talk analytics. EDUCAUSE  Review, July/August, 10-13.   [6] Siemens, G. 2011. Learning analytics: envisioning a research  discipline and a domain of practice. Paper presented at  LAK12, Vancouver. Retrieved from  http://learninganalytics.net/LAK_12_keynote_Siemens.pdf   [7] Subotzky, S., & Prinsloo, P. 2011. Turning the tide: a socio- critical model and framework for improving student success  in open distance learning at the University of South Africa.  Distance Education, 32(2), 177-193.   [8] Slade, S., & Prinsloo, P. 2013. Learning Analytics: Ethical  Issues and Dilemmas. Manuscript accepted for publication,  American Behavioral Scientist.   [9] Hsiu-Fang Hsieh, H-F., & Shannon, S.E. 2005. Three  approaches to qualitative content analysis. Qualitative Health  Research, 15, 1277-1288. DOI: 10.1177/1049732305276687   [10] Kruse, A., & Pongsajapan, R. 2012. Student-centered  learning analytics. Retrieved from  https://cndls.georgetown.edu/m/documents/thoughtpaper- krusepongsajapan.pdf   [11] Unisa. 2012. Guidelines for conducting research involving  Unisa staff, students or data.  http://heda.unisa.ac.za/filearchive/Guidelines_Research%20I nvolving_UNISAstaff.pdf   [12] Open University Essential documents for students  http://www8.open.ac.uk/students/essential-documents/   [13] Unisa. 2012. Policy on research ethics.  http://cm.unisa.ac.za/contents/departments/res_policies/docs/ ResearchEthicsPolicy_apprvCounc_21Sept07.pdf   [14] Unisa. 2007. Students charter on rights and responsibilities.  http://cm.unisa.ac.za/contents/departments/studentaff_policie s/docs/StudentCharter_apprvCounc_30Nov07.pdf   [15] Unisa. 2007. Students disciplinary code.  https://my.unisa.ac.za/tool/a87dd927-a9e0-4b59-0012- 5ab7d72ca660/contents/courses/docs/StudentDisciplinaryCo de_apprvCounc_26Jan08.pdf    [16] Unisa. 2007. Information sensitivity classification policy.  http://cm.unisa.ac.za/contents/departments/corp_policies/doc s/InfoSensitivClassificAnnexA_apprvCounc_21Sept07.pdf   [17] Unisa. 2007. Records management policy.  http://cm.unisa.ac.za/contents/departments/corp_policies/doc s/RecordsManagementPolicy_ManCom_15May07.pdf     244      "}
{"index":{"_id":"36"}}
{"datatype":"inproceedings","key":"Niemann:2013:ASU:2460296.2460345","author":"Niemann, Katja and Wolpers, Martin and Stoitsis, Giannis and Chinis, Georgios and Manouselis, Nikos","title":"Aggregating Social and Usage Datasets for Learning Analytics: Data-oriented Challenges","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"245--249","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460345","doi":"10.1145/2460296.2460345","acmid":"2460345","publisher":"ACM","address":"New York, NY, USA","keywords":"data-driven analysis, dataset, education, experimental investigation, usage data formats","abstract":"Recent work has studied real-life social and usage datasets from educational applications, highlighting the opportunity to combine or merge them. It is expected that being able to put together different datasets from various applications will make it possible to support learning analytics of a much larger scale and across different contexts. We examine how this can be achieved from a practical perspective by carrying out a study that focuses on three real datasets. More specifically, we combine social data that has been collected from the users of three learning portals and reflect on how they should be handled. We start by studying the data types and formats that these portals use to represent and store social and usage data. Then we develop crosswalks between the different schemas, so that merged versions of the source datasets may be created. The results of this bottom-up, hands-on investigation reveal several interesting issues that need to be overcome before aggregated sets of social and usage data can be actually used to support learning analytics research or services.","pdf":"Aggregating Social and Usage Datasets for Learning Analytics: Data-oriented Challenges  Katja Niemann, Martin Wolpers Fraunhofer Institute for Applied Information  Technology Schloss Birlinghoven, Sankt Augustin, Germany  {katja.niemann, martin.wolpers}@fit.fraunhofer.de  Giannis Stoitsis, Georgios Chinis, Nikos Manouselis  Agro-Know Technologies 17 Grammou str., Vrilissia  15235, Athens, Greece {stoitsis, gchinis, nikosm}@agroknow.gr  ABSTRACT Recent work has studied real-life social and usage datasets from educational applications, highlighting the opportunity to combine or merge them. It is expected that being able to put together different datasets from various applications will make it possible to support learning analytics of a much larger scale and across different contexts. We examine how this can be achieved from a practical perspective by carrying out a study that focuses on three real datasets. More specif- ically, we combine social data that has been collected from the users of three learning portals and reflect on how they should be handled. We start by studying the data types and formats that these portals use to represent and store social and usage data. Then we develop crosswalks between the different schemas, so that merged versions of the source datasets may be created. The results of this bottom-up, hands-on investigation reveal several interesting issues that need to be overcome before aggregated sets of social and us- age data can be actually used to support learning analytics research or services.  Categories and Subject Descriptors H.3.3 [Information Storage and Retrieval]: Information Search and Retrievalinformation filtering  General Terms Algorithms, Measurement, Performance, Experimentation.  Keywords Dataset, data-driven analysis, experimental investigation, education, usage data formats  1. INTRODUCTION Social and usage data about learning resources are be-  ing collected within various applications such as educational  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 13 April 08 - 12 2013, Leuven, Belgium Copyright 2013 ACM 978-1-4503-1785-6/13/04 ...$15.00.  web portals and learning management systems. They in- clude information on the types of activities that users per- form over the resources (usage data) as well as information that the users provide as annotations to the resources (social data). For instance, attention or usage metadata represent the activities of users and their usage of data objects in the applications.  In learning analytics, such social or usage data provide the basis for a number of different learning support systems [1, 2, 3, 6]. For example, based on an analysis of usage data, irregularities of learning behaviour of students can be identified [4] and the results of corrective activities by the teacher can be monitored. Another example of the success- ful application of analysing usage data in learning settings is the reflection and comparison of learning activities among students of a learning group. A further example of success- ful use of usage data are personalized recommender systems that base their learner support on the analysis of social data [8].  A representation of implicit or explicit information from the users on how they have used or experienced a particu- lar resource is valuable for any type of service that would like to provide advanced services based on user data. This information can be in several forms. For example, in the case of collaborative filtering systems it can be explicit (e.g. how a resource was rated) or implicit (e.g. if a resource has been accessed, downloaded, or bookmarked). In the case of content-based recommender systems it can be tex- tual reviews or simple tags (keywords) that users provide on resources. Additional information is required if the desired support is going to be adapted for specific users and specific items.  A number of widely adopted data representation formats for social and usage data from educational applications have already evolved.  In this paper, we examine some of the practical require- ments on what is achievable by transforming and combining existing social and usage datasets. We particularly examine datasets coming from three specific portals: Organic.Edunet1, MACE2, and Learning Resource Exchange3. Our aim is to see how data coming from the two latter portals may be cross walked and transformed into the social data schema used by Organic.Edunet, and then explore what would be possible by the combined, aggregated dataset.  1http://www.organic-edunet.eu 2http://portal.mace-project.eu 3http://lreforschools.eun.org  245    2. BACKGROUND The context in which this study is taking place is the  European initiative titled Open Discovery Space4 (ODS), a large-scale pilot project that tries to increase the adoption of digital learning resources by European schools. Part of its activities includes setting up a back end infrastructure that will allow existing learning portals and repositories to be connected, in order to allow the exchange of information and therefore accelerate the sharing, adoption, usage, and re-purposing of the already existing educational content.  ODS will not only interconnect different applications to al- low exchange of metadata about content resources, but also tries to set up a mechanism for the exchange and re-use of so- cial and usage information among the different applications. This mechanism is actually a social and usage data aggrega- tion layer that will try to collect, store, process and expose such data across all applications that will join the network. The aggregation layer is expected to offer to these applica- tions (such as learning portals that are already running or will be built on top of the infrastructure) the ability to pro- vide enhanced visualization and social navigation features based on the social and usage data. Additionally, parts of this data can be used to support cross-portal services, such as federated recommendation of resources, people and learn- ing paths, based on the social activity around resources and enabling large-scale learning analytics.  As a first exploratory investigation of how social and us- age data may be aggregated within Open Discovery Space, we decided to study a very specific and well-defined sce- nario: how social data (that is ratings, tags, and comments) generated in the MACE (Metadata for Architectural Con- tents in Europe) portal and in the LRE (Learning Resource Exchange) portal could be used to enhance the recommen- dation services of the Organic.Edunet portal. We start with an overview of the use social and usage metadata formats to then define specific transformations for the CAM instances collected in MACE and the NSDL instances collected in LRE into Organic.Edunet format. Thereafter, we visualise the properties of the new merged dataset and compare them to the original ones.  3. USAGE DATA FORMATS  3.1 Contextualized Attention Metadata The CAM schema [5] was defined as an extension of At-  tention.XML5 which is an early approach to capturing and storing attention metadata for single users. In the current CAM version6, the focus has moved from the user and the data object to the event itself. This is due to the insight that not every event has a fixed set of attributes. Addition- ally, only the basic information about an event is stored, e.g. the event type and the time stamp. All other information, e.g. metadata describing users or documents involved in the event, are linked. This way, each entity and also each ses- sion can be described in a different and suitable way and no information is duplicated.  Fig. 1 shows the complete CAM schema. The main ele- ment of a CAM instance is the event entry which comprises  4http://www.opendiscoveryspace.eu/ 5http://tantek.com/presentations/2005/01/- attentionxml.html 6https://sites.google.com/site/camschema/  Figure 1: CAM Schema  its id, the event type, the timestamp, and a sharing level ref- erence. The sharing level reference points to a description of the specific sharing level which describes the privacy related issues of the event. Depending on the event, various entities with different roles can be involved, e.g. when sending an e-mail, there is a person with the role sender, at least one person with the role receiver and a document with the role e-mail. Each event can be conducted in a session.  The current CAM schema does not have fixed bindings so far. The information can be stored in XML, RDF, JSON or in a relational database, depending on the purpose of capturing the data. See Fig. 4 in section 4 for an exemplary CAM instance in XML format.  3.2 NSDL Paradata The NSDL Paradata format was defined to capture aggre-  gated usage data about a resource (e.g. downloaded, favour- ited, rated) which is designated by audience, subject or edu- cation level7. In contrast to the CAM schema, this format is not event, but object-centric. Each data object has exactly one NSDL Paradata record, which is identified by a recor- dId and must contain the URL of the resource to which the paradata record applies (usageDataResourceURL). A record can also contain the title and a description of the record, the title of the resource, and any additional XML element.  Figure 2: Simplified excerpt of the NSDL schema  The most important element is the usageDataSummary, that comprises all available usage statistics/information about  7https://wiki.ucar.edu/display/nsdldocs/comm para  246    a resource using five different types of values. An Inte- ger/Float value represents the number of times certain ac- tions have been performed on the resource. A String value is a textual value that has been associated to the resource. A RatingType value is the numerical average that represents the judging of a resource on a numerical scale. A Vote- Type value represents the number of positive and negative responses to a resource. A RankType value represents the standing of a resource in a hierarchy.  Please see Fig. 6 in chapter 4 for an exemplary NSDL instance. For an example using all types, please see the exemplary NSDL Paradata instance for the learning object The Capacity of the Planets8.  3.3 Organic.Edunet Format The Organic.Edunet format was defined in the Organic.Edu-  net project to store social data provision activities, e.g. tag- ging, rating, and commenting. It is designed in an extend- able way, so other user activities, e.g. downloading could be easily stored as well.  Figure 3: Organic.Edunet schema  Similarly to the NSDL format, it is not event, but object- centric (see. Fig. 3). This means all social data provision activities for one object are stored in a single commSocial- Data instance which is identified by its recordId, linked to the respective object by the socialDateResourceUrl and con- tains the name of the socialDataProvider. For each user that added social data to the object the commSocialData instance holds one socialDataSummary instance that com- prises all activities of the respective user which is identi- fied by the userId. The activities are represented by string and rating instances, each of these instances holds the date- Time of the event and its type, e.g. tag or comment for string and star for rating instances. Additionally, string instances hold a language attribute and rating instances hold attributes to store the minumin (min) and maximum (max) possible rating values, a dim attribute to store the dimen- sion of the rating to enable multi-dimensional ratings, e.g. relevance to a topic, quality of metadata and usefulness and a total attribute that is used if the users average rating of several dimensions is stored.  8http://ns.nsdl.org/ncs/comm para/1.00/- records/planets.xml  Please see Fig. 5 and Fig. 7 in chapter 4 for exemplary Organic.Edunet instances. Similarly to the CAM schema, the Organic.Edunet format doesnt contain any detailed in- formation about the user or the object but links and ids to not duplicate any information. However, it contains all in- formation about the social data activity that is represented.  4. APPLICATION SPECIFIC MAPPINGS As each format has been created with a specific purpose in  mind, no one-size-fits-all mapping among all three formats is possible. In contrast, mapping can only be defined for specific application scenarios.  4.1 Transforming CAM to Organic.Edunet The MACE CAM instances reference the user and the  object by an id, however, the event values, i.e. rating, tag, comment and competence, can be directly put into the CAM instances when they dont contain any further information and, thus, dont hold an own metadata instance that can be referenced.  Figure 4: MACE CAM instance for a tag  See Fig. 4 for a MACE CAM instance representing a tag- ging activity. The transformation of MACE CAM instances to Organic.Edunet takes place for the three event types ad- dRating, addTag, and addComment. Fig. 5 shows the same tagging event in Organic.Edunet format.  Figure 5: Organic.Edunet instance for MACE data  When transforming the instances from CAM to Orga- nic.Edunet, for each learning object contained in the CAM instances one commSocialData element is created with the value of the CAM entity with the role item as Social- DataResourceURL. Additionally, an automatically generated recordId is added, while the value of the DataProvider can be set manually. For each user that rated, tagged or com- mented this object, a SocialDataSummary is added to the  247    commSocialData instance containing the value of the CAM entity with the role user as userId.  Then, each CAM event is added as string or rating in- stance to the respective Organic.Edunet commSocialData instance. For the CAM event types addTag and addCom- ment a string instance is added with the type tagged or commented. For the CAM event type addRatinga rating instance with type star is added, the min and max values can be set automatically as they are similar for all MACE CAM instances. Both, the string and rating instances hold the CAMtimestamp as dateTime attribute, its important to mention that a transformation of the date format is needed here as well.  4.2 Transforming NSDL to Organic.Edunet Fig. 6 shows a LRE instance in NSDL format. Similar  to the Organic.Edunet format, each object is represented by one XML instance that comprises all events the object was involved in. However, the Organic.Edunet format was de- signed to store single events while NSDL stores aggregated events without a link to the user. The NSDL instance in Fig. 6 represents 153 rating events conducted by (anonymous) educators. Fig. 7 shows these events as Organic.Edunet instance, please note that the rating events that were ag- gregated in NSDL are represented as one single event in Organic.Edunet. In other settings, it might be useful to add an event as often as it occurred (total attribute), e.g. when tags are weighted in a tag cloud.  Figure 6: LRE NSDL instance  5. DATASET ANALYSIS Developing the mappings between the schemas used in  the three studied portals allows us to develop a set of data transformation components that could ingest data coming from MACE and LRE and crosswalk them into a database storing data using the Organic.Edunet schema. To better understand how the transformed datasets may be used to facilitate research and development of new algorithms and services for the Organic.Edunet portal, we carry out an anal- ysis that engages different techniques to examine and visu- alise the dataset properties in order to understand how easily they can be used for experimentation.  Figure 7: Organic.Edunet instance for LRE data  5.1 Method, Materials and Tools In the following, we only focus on the aggregating of the  numerical data (the ratings) and create two aggregated data- sets: Aggregate A comprises the datasets that include unique user identification information (i.e. Organic.Edunet and MACE) and Aggregate B comprises all three datasets but without storing any user information. Table 1 provides an overview of the datasets.  Table 1: Overview of the MACE, LRE, Or- ganic.Edunet (OE), Aggregate A and B datasets  MACE LRE OE A B # users 76 n/a 162 238 238 # items 429 1007 569 998 2005  # ratings 532 989 978 1510 2499 rating  dimensions 1 1 3 1 1  The properties of the examined datasets include: 1) The Skewness which measures the asymmetry of an item (item skewness) or user (user skewness) rating frequency (popular- ity) distribution. This metric determines whether the mass of the distribution is concentrated on the right side of the mean or the left side of the mean, i.e., representing nega- tive or positive skewness. 2) The Gini which measures the concentration (or inequality) of an item (item Gini) or user (user Gini) rating frequency distribution. A value of 0 rep- resents total equality (all items are equally popular), and a value of 1 represents maximal inequality (a few popular items have all the ratings).  5.2 Results We calculate all mentioned data properties for each one  of the five studied datasets, namely the Organic.Edunet, MACE, LRE, Aggregate A and Aggregate B. For Orga- nic.Edunet, we observe that 67% of the total rated items have received only one rating and only 4% of the items have received more than 6 ratings. Similarly, only two items in the MACE dataset received more than five ratings and 95% of the items have received less than 3 ratings. The distri- bution of ratings per items for LRE shows that almost 99% of the items have received less than two ratings. Although there are differences between the datasets, they generally  248    seem to be rather sparse ones. Comparing the distributions of the Organic.Edunet and  MACE ratings, we note that in Organic.Edunet the distri- bution is more concentrated around few ratings. To further investigate this, we also calculate the user-related statistics for these two datasets.  Table 2 illustrates the values of User Skewness and User Gini for all datasets, for Organic.Edunet, it includes the val- ues for the average rating (Organic.Edunet offers a multi- criteria rating). The high value of Item Skewness in the Organic.Edunet data confirms the observation that the dis- tribution of ratings per item is concentrated around a few ratings. Table 2 also indicates that the MACE distribution is characterized by a larger equality compared to the Orga- nic.Edunet case and this is evident from the rating distribu- tion. For the LRE dataset it can be observed that there is a high value of item skewness. This is also evident from the distribution of ratings per item where we can observe that the vast majority of items have received only one rating. The Aggregate A dataset follows the distribution pattern of the Organic.Edunet dataset. This may be due to the fact that the number of ratings provided in Organic.Edunet is almost double in size of that from MACE and thus the aggregated dataset tends to adopt the properties of this dataset. The dataset Aggregate B shows a higher item skewness than the OE and MACE datasets which is still lower than the item skewness of the LRE dataset.  Table 2: Data characteristics of the MACE, LRE, Organic.Edunet (OE), Aggregate A and B datasets  MACE LRE OE A B User Skewness 61.08 - 54.80 57.57 n/a  User Gini 0.65 - 0.63 0.63 n/a Item Skewness 2.58 9.40 3.85 3.88 5.47  Item Gini 0.17 0.13 0.33 0.28 0.24  According to the results of this analysis we can conclude that the aggregation of different datasets had no significant effect on the data properties. Aggregating social data from different learning portals can provide useful insights about the popularity of the items but should be supported by the adoption of unified resources identifiers. It is critical to sup- port aggregation by an identification service that will anal- yse the items URLs, compare the different items and will create a unique identifier for each item in the aggregation.  6. CONCLUSION Recent studies around dataset-driven research on learning  analytics [3, 7] have identified the potential of using real-life social and usage datasets to support research and develop- ment. They have also outlined the potential of aggregating and combining datasets from different sources, in order to facilitate large-scale and cross-context tasks.  In this paper we investigated on a particular case study that we worked on in the context of the networked infrastruc- ture of ODS. It is expected that being able to put together different datasets from various applications will make it pos- sible to support learning analytics of a much larger scale and across different contexts. We tried to understand how easy it is to aggregate social data from different learning portals and what are the practical challenges that emerge.  The overview of the used social and usage data formats showed that theres no one-size-fits-all application, however, the adopted approach to defining the mappings between dif- ferent data formats is not efficient and scalable. A more suitable solution would be to have a common data model for ODS and map the existing formats into it; the number of required mappings in that case would be far less. In this way, the data from the different portals could be combined and analysed.  In our next steps, we will take the semantic differences and mappings between domains into account and deal with resource/user disambiguation. Furthermore, we will carry out experiments with more datasets to examine how ser- vices such as recommender systems can benefit from such aggregations.  7. ACKNOWLEDGMENTS The work presented in this paper has been funded with  support by the European Commission and more specifically the grant agreement no 297229 (Open Discovery Space) of the Information and Communication Technologies Policy Support Programme (CIP PSP). The authors would like to thank David Massart and the European Schoolnet for shar- ing an instance of the LRE dataset with them.  8. REFERENCES [1] Elias, T. Learning analytics - definitions, processes  and potential, http://learninganalytics.net/learning- analyticsdefinitionsprocessespotential.pdf, 2012.  [2] Ferguson, R. The state of learning analytics in 2012: A review and future challenges. Tech. Rep. Technical Report KMI-12-01, Knowledge Media Institute, The Open University, UK, 2011.  [3] Massart, D., and Shulman, E. Interaction data exchange. D-Lib Magazine 21, 5/6 (2013).  [4] Scheffel, M., Niemann, K., Leony, D., Pardo, A., Schmitz, H.-C., Wolpers, M., and Delgado Kloos, C. Key action extraction for learning analytics. In EC-TEL (2012), A. Ravenscroft, S. N. Lindstaedt, C. Delgado Kloos, and D. Hernandez Leo, Eds., vol. 7563 of Lecture Notes in Computer Science, Springer, pp. 320333.  [5] Schmitz, H.-C., Wolpers, M., Kirschenmann, U., and Niemann, K. Dynamic ambient paradigms. In Paradigm Gems 2, A. Doe, Ed. Addison Wesley, 2005, pp. 223233.  [6] Shane, D. Analytics to literacies: Emergent learning analytics to evaluate new literacies. Workshop on New Media, New Literacies, and New Forms of Learning, 2011.  [7] Verbert, K., Manouselis, N., Drachsler, H., and Duval, E. Dataset-driven research to support learning and knowledge analytics. Educational Technology & Society 15, 3 (2012), 133148.  [8] Verbert, K., Manouselis, N., Ochoa, X., Wolpers, M., Drachsler, H., Bosnic, I., and Duval, E. Context-aware recommender systems for learning: A survey and future challenges. IEEE Transactions on Learning Technologies 99, PrePrints (2012).  249      "}
{"index":{"_id":"37"}}
{"datatype":"inproceedings","key":"Holman:2013:GWL:2460296.2460350","author":"Holman, Caitlin and Aguilar, Stephen and Fishman, Barry","title":"GradeCraft: What Can We Learn from a Game-inspired Learning Management System?","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"260--264","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460350","doi":"10.1145/2460296.2460350","acmid":"2460350","publisher":"ACM","address":"New York, NY, USA","keywords":"game-inspired instruction, gamification, learning analytics, syllabus design","abstract":"The gamification of courses (i.e., designing courses that leverage motivational mechanisms found in videogames) is a movement that is gaining traction in educational research communities and universities. Two game-inspired courses were developed at a high-enrollment public university in an effort to increase student engagement, and to provide students with more personalized learning experiences. We designed a learning management system, GradeCraft, to foreground the affordances of these grading systems, and to enhance the game-like experience for students. Along with serving as a translation layer for the grading systems of these courses, GradeCraft is also designed with an eye towards learning analytics, and captures information that can be described as student process data. Currently this data includes what types of assignments students choose to complete; how students assign percentage weights to their chosen assignments; how often and how accurately students check or model their course grades; and how successfully assignments are completed by students individually and the class as a whole across a structured grading rubric. We hope GradeCraft will give instructors new insight into student engagement, and provide data-driven ideas about how to tailor courses to student needs.","pdf":"GradeCraft: What Can We Learn From a Game-Inspired  Learning Management System   Caitlin Holman  University of Michigan   School of Information   3330G North Quad, 105 S. State St.   Ann Arbor, MI 48109-1285  +1 (734) 644-3674   cholma@umich.edu   Stephen Aguilar  University of Michigan   School of Education & USE Lab  Suite 4215, 610 E University Ave   Ann Arbor, MI 48109-1259 USA   +1 (734) 764-8416  aguilars@umich.edu  Barry Fishman  University of Michigan   School of Education   Suite 4215, 610 E University Ave   Ann Arbor, MI 48109-1259 USA   +1 (734) 647-9572  fishman@umich.edu     ABSTRACT  The gamification of courses (i.e., designing courses that  leverage motivational mechanisms found in videogames) is a  movement that is gaining traction in educational research  communities and universities. Two game-inspired courses were  developed at a high-enrollment public university in an effort to  increase student engagement, and to provide students with more  personalized learning experiences. We designed a learning  management system, GradeCraft, to foreground the affordances of  these grading systems, and to enhance the game-like experience  for students. Along with serving as a translation layer for the  grading systems of these courses, GradeCraft is also designed  with an eye towards learning analytics, and captures information  that can be described as student process data. Currently this data  includes what types of assignments students choose to complete;  how students assign percentage weights to their chosen  assignments; how often and how accurately students check or  model their course grades; and how successfully assignments are  completed by students individually and the class as a whole across  a structured grading rubric. We hope GradeCraft will give  instructors new insight into student engagement, and provide data- driven ideas about how to tailor courses to student needs.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   collaborative learning, computer-assisted instruction,  computer-managed instruction   General Terms  Management, Measurement, Design, Theory   Keywords  Learning analytics, syllabus design, game-inspired instruction,  gamification   1. INTRODUCTION  In our experience, discussions around learning analytics often  begin with the tongue-in-cheek question: Wheres the learning  This question points to a real concern within the field, namely,  that there can be an overemphasis on the analytics (i.e., predictive   models, data mining to detect patterns, etc.) and far too little  emphasis on applying this knowledge to contexts where student  outcomes can be improved (i.e., the learning). Systems like  Purdues Course Signals represent a step in the right direction  because they begin to tackle the open question of wheres the  learning by providing students with grade feedback that helps  them adjust their strategies within a given course to be more  successful [2]. We see this work as valuable and worthy of future  development, but our approach towards learning analytics is  slightly different.   Instead of working within established university courses, we have  sought to reify the complex grading systems of non-traditional  game-inspired courses and have used the opportunity to capture  student data that speaks to the process through which they reach  course outcomes. In so doing we deliberately sit at the nexus of  learning analytics and gamification. While these data are  generally descriptive in nature at this point, we see this as a  subject for iterative design; as courses evolve, so too will our  understanding of the data that we need to collect and analyze.  Why game-inspired, however Good games typically inspire  players to spend large amounts of time and effort achieving in- game success. Well-designed games succeed because they tap into  our deep-seated desire to learn and be engaged. They give players  multiple ways to succeed, maximize choice, and mitigate the cost  of failure [4]. Educators over the last decade have been inspired  by the depth of content learned and the high-intensity effort that  gamers choose to put in when engaged in a good game [6].  Similarities that commonly exist between games and school  include well-defined goals at the outset, the establishment of  specific challenges to be conquered, requiring practice to succeed,  and using assessments to gauge whether material has been  properly learned. These parallels led to the question of whether  school itself could be made into a good game.   We are currently seeing the first wave of game-inspired courses at  our university. These courses aim to increase student choice while  mitigating the negative impact of failure. Gameful elements we  have observed include: using points and incremental levels instead  of grades; awarding badges to recognize achievements and skill- acquisition; allowing students to redo certain assignments as many  times as necessary to succeed; and giving students the ability to  decide what types of assignments they will take on and how much  those assignments will be worth. Thus far our research indicates  that the frame shift of these grading systems is motivating and  encourages students to complete more work within the course [3].    Given the complexities and choices inherent in a game-inspired  grading system, it can be difficult for students to quickly and  intuitively grasp the options available to them, and also tricky for  instructors to manage. To alleviate these issues we developed     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   260    GradeCraft, a learning management system designed for the  purpose of foregrounding the affordances of gameful grading  systems. This design brief describes the current application and  the nascent Learning Analytics potential of GradeCraft.    1.1 Case Study: Videogames & Learning  GradeCraft was initially developed to support an elective class on  Videogames & Learning at a high-enrollment American public  university. The goal of this course is to examine the learning and  motivational theories that operate withinand inform the design  ofvideogames. Rather than treat this content abstractly, the  course grading system is infused with gameful design principles.  The course has required assignments in addition to a series of  optional assignments. While completing all of the assignments is  possible, it involves more work than is necessary to do well in the  course. The variety of assignment options is an example of giving  students multiple routes to successa key principle that makes  videogames motivating [4].    These assignments are divided into three categories: Grinding  assignments are characterized as necessary for learning the  content, but are not always as engaging as other assignments,  including attendance, weekly reading reactions, blogging, and  team activities; Learning From Playing A Game assignments  center on students reflecting and commenting on a commercial  videogame they have chosen to play throughout the termtheir  game textand are required for all students; Boss Battle  assignments are longer, more complex, and require a certain level  of content mastery to complete successfully. As a result, Boss  Battle assignments occur near the end of the term.    Optional assignments in this course may be used as a pathway to  exceed the courses main requirements (because a student wants  an A+, perhaps) or as a way to regain points that were lost as a  result of being absent from class, missing a reading reaction, or  simply performing poorly on a required assignment. This latter  path is an example of how a game-inspired grading system can  mitigate the cost of localized failure in coursework, thereby  allowing students the opportunity to self-correct, complete the  stated learning goals, and succeed in the overall course.   Badges were introduced in the third iteration of the course to  highlight specific skills the instructor wanted students to learn, but  earning them did not contribute value towards the final grade.    1.2 Case Study: Intro. to Political Theory  GradeCraft is also currently being used in a political science  course at the same university. The professor, hoping to increase  student motivation and encourage mastery-driven learning,  designed the following gameful grading system:   Forty percent of a students final grade is traditional in that it  consists of a core set of requirements: attending lectures, writing  weekly reading reactions, and attending teaching assistant led  discussion sections. These requirements are designed to provide a  core set of content knowledge and a common course experience.  The remaining sixty percent of a students grade is determined by  two student-driven decisions:   First, students must choose what types of assignments will make  up the remaining sixty percent of their grade. There are four  assignment types to choose from during the term: traditional  essays, an open-ended group project, an open-ended individual  project, and contributing to the class blog. Students are  encouraged to work on two of the four assignment types, but are  allowed to select any number.   Second, students are given the freedom to determine how each of  the four assignment types is weighted. This decision is  operationalized by giving students six points to spend on any  assignment type they wish. These points determine the weight of  each assignment they select and have no inherent valuethey are  only meant to simplify the process of determining how the  assignments they choose to do are weighted. A student can, for  example, assign all six points to academic essays. Doing so  amounts to the student choosing to focus only on writing essays.    Finally, students are awarded badges that either recognize or  incentivize certain behaviors. These badges are valued at up to  twenty-five percent of the students grade.    The remainder of this design brief will explore how these choice- driven grading systems are reified through the GradeCrafts  student and instructor interfaces as well as explore the  implications for the kind of data collected by GradeCraft.   2. GRADECRAFT  GradeCraft allows for three types of users: students, teaching  assistants, and instructors. For the purposes of this paper we will  collapse the teaching assistant and instructor roles, as they are  functionally similar. At its core GradeCraft is a comprehensive  dashboard that allows students to see their course performance in  a single view, much like the dashboard of a videogame. The  instructor dashboard displays a summary of students, sections, and  the overall class performance across a variety of metrics.   We will illustrate the analytics displayed through a series of  hypothetical situations, beginning with reviewing what a student  sees in GradeCraft.   2.1 The Student Perspective  Upon logging into GradeCraft, a student sees their current score, a  chart of the points they have earned so far, and a chart of the  points that are available to earn throughout the entire course.       Figure 1: Student Dashboard    The dashboard shows students up-to-date grade information for  their course. These descriptions are intended to be both  informative and empowering because students have a summary  view of their performance, and see a decomposed view of  performance by assignment type (Figure 1). This information can  be buried in traditional courses, and its inclusion reflects a growth  model of learningstudents are shown that their current progress  can be altered through effort and forward planning for future  assignments.    On their dashboard, students can also see which badges they have  earned, their progress towards completing unearned badges, and  which badges their classmates have earned. They can notify  instructors that they have completed a specific task on the path to  earning a badge.   261      Figure 2: Badges for understanding course content     Course objectives are also displayed on a student's dashboard,  allowing them to visualize their progress towards these  overarching goals. This helps orient students to the greater  purpose of the course, rather than just their grades.       Figure 3: Visualizing Learning Objectives     To that end, one goal we have is for GradeCraft to encourage  students to be less concerned with grades and more focused on  mastering skills by completing assignments and earning badges.  To this end, we have chosen not to display course grades on the  student dashboard by default. However, we recognize that we are  working within a dominant educational paradigm where grade  information is highly sought after, so students can click the   Check My Final Grade  button in order to see how their current  score would translate into a final course grade. This information is  recorded, and allows instructors to know which students are  focused on this outcome. Such information could inform future  course design or pedagogical strategies.        Figure 4: Checking Their Grade     Students can also check how their current score compares to the  class average by clicking  How Am I Doing  (Figure 5). This  displays a box-and-whisker plot intended to help students gauge  their performance against the rest of the class. We opted not to use  leaderboards because they can be de-motivating to low-ranked  students/players.       Figure 5: Checking Class Standing      We believe giving students meaningful choices (ones that have an  impact on how they will play the game ) is crucial to designing a  good game [5][7]. Students in the political science course  mentioned above are able to select which assignments they will do  and how much of their grade those assignments will be worth. But  how do they make these complex calculations We built a grade  prediction tool to help them in this process. When the predictor is  first loaded, the bar chart fills with any points they've already  earned, broken down by assignment type.        Figure 6: The Grade Predictor     The student can page through each assignment type, selecting how  many assignments they plan to do, how well they believe they will  score on them, and if applicable, how they would like to weight  that assignment type. Students are thus able to model their  performance, taking more responsibility for their choices than is  typical in college syllabi. GradeCraft captures these modeling  instances, allowing instructors and researchers to begin to  understand the process behind how students make course  engagement decisions.    The predictor stays in sync with the student throughout the  semester, showing them their current score at any given moment,  and allowing them to assess what work must be done to earn a  particular grade. The predictor also acts as a resource, displaying  links to materials and tools recommended by the instructor to help  students complete the assignment.   2.2 The Instructor Perspective  When initially setting up a course in GradeCraft, instructors can  declare the overarching course objectives. As they create  assignment types and badges, each one can be tagged with the  relevant course objectives.   Instructors can then visualize exactly how their objectives are  distributed across the entire course per activity, and in relation to  the grading scheme. This helps instructors to see if their learning  objectives are fully represented in the course structure or if there  are elements that need clearer application.        Figure 7: Learning Objectives Assignment Breakdown     262    Instructors can create badges to encourage the development of  particular attributes, skills, or actions that they feel are important  for their students to have or do. Each badge has a set of criteria  that must be accomplished in order for a student to earn it.  Instructors can easily view which badges have been earned, how  often, and when. They can also see which badges students are  working on, which criteria have been marked complete, and  which are proving more difficult for students to achieve.        Figure 8: Badge Analytics     Standard access datalogin count, page views, resources  accessedprovide the basic framework within which instructors  can first begin to investigate student engagement. Instructors can  view an interactive table displaying student and class statistics, on  these metrics. While this data is possible to collect in all learning  management systems, displaying these metrics for instructors use  is not frequently taken advantage of. Making this data plainly  available to instructors allows them to have a richer understanding  of how their students are choosing to engageor disengage, as  the case may bewith their course. With this information  instructors can intervene as necessary to improve student  outcomes.      Figure 9: Student Activity     When approaching the mid-point of the semester, a professor  viewing the engagement charts could, for example, sort students  by their attendance record and schedule conversations with  students who have missed a high percentage of classes.  Alternatively, an instructor might notice that a student who had  been performing well in the course suddenly stopped attending  and has not been turning in assignments. This acts as an early  warning system, suggesting to the instructor that something has  changed and that they should contact the student while it is still  relatively easy for the student to recover from the situation.   Students also need feedback regarding their class performance and  guidance as to what else they should be doing. GradeCraft gives  students direct access to analytics that can help answer these  questions, and also provides instructors with further material to  support conversations with students regarding what additional  work they can do in the course. Instructors can see each student's  dashboard view, visualize how well they have completed the  comprehensive learning objectives, and check where the student  has ranked in completing each assignment.    In discussing with students what their specific path through the  course is, instructors can use the predictor tool to keep track of the  choices a student has made, and make recommendations regarding  future work. GradeCraft logs the choices students make in the  predictor and what final grade these selections ultimately produce.  Instructors can see students predicted final grade charted over  time, and drill down to explore which specific assignments a  student was considering doing, and how well they expected to do  on any particular assignment.        Figure 10: Self-Predicted Final Grades Over Time     If instructors know when students deviate negatively (through  missing assignments or lower than expected scores) from their  original intentions on an assignment, they can predict earlier in  the course timeline which students may be in need of support.  This is an improvement on current early warning systems, which  rely on comparing a students behavior to previous classes or  current classmates. The unit of analysis in GradeCraft is the  course itself, rather than the academic histories students bring  with them. These histories cannot fully capture the nuance of a  student performing differently than they themselves had intended.   GradeCraft allows instructors to visualize which assignment types  students choose to complete, and how much weight they decide to  assign them. Mapping these choices back to students final grades  will help us investigate if students know their own skills and  choose to weight them higher, or if students weight things so as to  reduce the risk of working on assignment types they are less  familiar with. Given that one of the long-term goals of game- inspired grading systems is to encourage students to explore new  types of activities and broaden their skill sets, tracking this  relationship will be key to understanding if the frame change is  successful in achieving this goal. We need to understand how  students perceive risk-taking in relation to assignment type  selection, and how to incentivize this behavior to achieve the best  learning outcomes.   263    Clearly-designed rubrics have the advantage of helping students  understand explicitly what is expected of them and how they  should direct their efforts, provide a more concrete avenue for  instructor feedback, and reduce bias in grading [1]. To support  these goals, we designed an interactive grading rubric, allowing  instructors to set categories, enter criteria and scoring guidelines,  and then select how students had fared on each criterion. These  assessments build to a final score on the assignment.    From these selections instructors can then explore how well  students complete their work from the perspective of these  criteria. An instructor might create the category of Writing Skill  in a long-form essay rubric, with the criteria of Spelling &  Grammar, Clarity of Thought, and Supported by Research.  When the grading has been completed, the instructor can then see  how students individually, as a section, and as a class did on each  criteria. Different levels of success would have implications for  student, section, and class interventions, explanations, and greater  understanding of what the class as a whole needs to be taught in  order to succeed.        Figure 11: Class Criterion Score for Poster Assignment      3. FUTURE WORK  To date the development of GradeCraft has been focused on the  construction of the core LMS functionality and the gathering of  data to power basic learning analytics. Our next task is to take the  analytic information on student behavior and return it to the  instructor and the student in more meaningful, nuanced ways.  Currently an instructor can load the profile of any student to check  their raw activity statistics. However, especially in large courses,  this effort is both unwieldy to perform and a simplistic analysis at  best. Therefore, we intend to build a tool that allows the instructor  to establish a series of metrics that singly, or in combination,  create grounds for identifying students of concern, i.e., those  with specific characteristics that suggest the desirability of review  and potential contact by the instructor.  In keeping with the gameful nature of the system, we also intend  to build a health bar for students to review course progress. This  tool could be set to mirror the instructors early warning system  metrics, or be based on entirely separate criteria. For instance, an  instructor might choose to have the students health bar be fifty  percent reflective of attendance and fifty percent based on their  performance across course learning objectives. Absence from  class would be marked by diminished health, which could be  regained by doing other optional activities like blogging. Poor  performance on learning objectives, as assessed via the interactive  grading rubrics, would also decrease a students health. Revising   and resubmitting the assignment in question, or completing an  alternative assignment to show improvement on that learning  objective, would serve to boost the students health bar.    4. CONCLUSIONS  GradeCraft gives educators access to new types of analytics that  are focused on learning because of the multiple points at which  students interact with the system. Rather than simply drilling  down into a letter grade and examining the parts that constitute  it, GradeCraft collects data that speaks to process and decisions. It  captures which assignments students choose to complete, how  students weight those assignments, how students did in regards to  completing specific assignment rubrics, and which badges were  awarded throughout the course. Each of these decisions is  captured, and it is our hope that the resulting data will yield  valuable insights about student behavior within game-inspired  courses. It is our hope that GradeCraft foregrounds the  affordances of the game-inspired grading systems in such a way  as to make complicated decisions clear, while also yielding data  that speaks to student processes, as opposed to simply reporting  student outcomes.    5. ACKNOWLEDGMENTS  We would like to acknowledge Mika Lavaque-Manty for his  contributions to the design of GradeCraft, as well as the USELab  at the University of Michigan for their feedback and support.    6. REFERENCES  [1] Andrade, Heidi Goodrich. Teaching with Rubrics: The Good,   the Bad, and the Ugly. College Teaching, Vol. 53, No. 1  (Winter, 2005). 27-30.    [2] Arnold, K. E. & Pistilli, M. D. Course signals at Purdue:  Using learning analytics to increase student success. Paper  presented at The 2nd International Conference on Learning  Analytics and Knowledge, (Vancouver, BC, Canada, 2012),  ACM Press, in press.   [3] Fishman, B & Aguilar, S. Gaming the Class: Using a Game- based Grading System to Get Students to Work Harder... and  Like It. in Games + Learning + Society Conference 8.0,  (Madison, WI, USA, 2012), ETC Press, 124-130.   [4] Gee, J. P. What videogames have to teach us about learning  and literacy. Palgrave Macmillan, New York, 2003.   [5] Malone, T. W., & Lepper, M. R. Making learning fun: A  taxonomy of intrinsic motivations for learning. In R. E. Snow  & M. J. Farr (Eds.), Aptitude, learning, and instruction:  Cognitive and affective process analysis, Volume 3. 223-253.    [6] Prensky, Mark. The motivation of gameplay: The real  twenty-first century learning revolution, On the Horizon,  Volume 10 (1). 5-11.   [7] Schell, J. The art of game design: a book of lenses. Morgan  Kaufmann Publishers, San Francisco, 2008.           264      "}
{"index":{"_id":"38"}}
{"datatype":"inproceedings","key":"Raca:2013:SAC:2460296.2460351","author":"Raca, Mirko and Dillenbourg, Pierre","title":"System for Assessing Classroom Attention","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"265--269","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460351","doi":"10.1145/2460296.2460351","acmid":"2460351","publisher":"ACM","address":"New York, NY, USA","keywords":"attention, behavioural observation, classroom orchestration, computer vision, orchestration","abstract":"In this paper we give a preview of our system for automatically evaluating attention in the classroom. We demonstrate our current behaviour metrics and preliminary observations on how they reflect the reactions of people to the given lecture. We also introduce foundations of our hypothesis on peripheral awareness of students during lectures.","pdf":"System for Assessing Classroom Attention  Mirko Raca CRAFT  Ecole Polytechnique Federale de Lausanne RLC D1 740, Station 20 Lausanne, Switzerland mirko.raca@epfl.ch  Pierre Dillenbourg CRAFT  Ecole Polytechnique Federale de Lausanne RLC D1 740, Station 20 Lausanne, Switzerland  pierre.dillenbourg@epfl.ch  ABSTRACT In this paper we give a preview of our system for automati- cally evaluating attention in the classroom. We demonstrate our current behaviour metrics and preliminary observations on how they reflect the reactions of people to the given lec- ture. We also introduce foundations of our hypothesis on peripheral awareness of students during lectures.  Categories and Subject Descriptors K.3.1 [Computer Uses in Education]: Computer-assisted instruction (CAI); K.3.m [Computers and Education]: Metricsperformance measures, Miscellaneous  General Terms Measurement, Design, Orchestration  Keywords Attention, classroom orchestration, computer vision, behavioural observation  1. INTRODUCTION Learning analytics (LA) started off from the need to gov-  ern educational decisions in an informed way[5][9]. Many existing papers give advice on how to conduct each of the five stages of LA (capture, report, predict, act, refine) [14] by taking information systems such as CMS (Class Manage- ment System) and SIS (Student Information Systems) as the base of approach.  In this paper we explore possibilities of implementing more unintrusive means for assessing the progress of learners, in environment without digitally quantified inputs. We turn to the most basic (and most common) learning scenario - a teacher in a classroom.  The aim is to develop a system which can monitor atten- tion in the classroom during the lecture and indicate to the lecturer drops in concentration. This will allow us to find  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Copyright 2013 ACM 978-1-4503-1785-6/13/04 ...$15.00.  parts of the lecture which were not received with high at- tention by the audience. In order to have minimal overhead on the existing learning process, we are reducing our inputs to video observations of the students in the classroom. To conserve their privacy, we do not try to connect the images to the identity of the student nor to we try to track a person outside of a single lecture. The goal is not to provide reports on individual students but to give an overall picture of the classroom attention during the lecture.  As the work of Campbell and Oblinger [14] noticed, our intervention needs to be well timed in order for the teach- ing staff to correct their approach in accordance with the observations of any analytical system. For this reason, our end goal has two possibilities: a real-time reporting system which acts as the indicator of current attention of the stu- dents in the class, and summary report of each class which can be presented to the teacher after a finished period of teaching.  In this short paper we introduce our preliminary results and design concept in order to start a discussion with the community. By following the five steps of learning analyt- ics, we will present our methodology for collecting data; our tools for visualization; our preliminary results on reporting and analysis; and ideas for developing the model and pre- sentation of data.  2. THEORETICAL BACKGROUND In the teaching community there is a growing interest in  classroom orchestration ([7][4][8][18][6][10]) as a theory on how to conduct the classroom learning process. The main instrument of action is the teacher [7]. The our work is focused on the factors of awareness and control of what is going on in the classroom, in order to allow the teacher to organize the learning period to the maximum benefit of the listeners.  It is easy for a lecturer to neglect the reception of the topic by the audience, even more so for a novice one. A good teacher has the flexibility to react upon the reception and adjust the course of lecture as part of a reflection in action [16].  With this work, we are taking the existing teacher-centric approach and expanding it with our observations about stu- dents. There are propositions on how long can the students maintain attention in a given lecture [19]. Our assumption at the moment is that even though classroom learning is not a group activity, there are lateral connections between the participants by which they are affecting each other. We take some concepts from the theory of group work. For instance it  265    Figure 1: Snapshot of the tool for video synchroni- sation, annotation and a sample of our recording of a class  is noticed in [15] that contribution of non-conforming group members is perceived as of higher importance then the work of conforming members. We tend to see similar behaviour in reaction to non-conforming audience members paying atten- tion and we are interested to see how this influences other participants. We aim to capture spatial propagations of at- tention or distraction among students, which we currently call distraction ripples. The idea is that even though the teacher is the dominant influence, students are not isolated from their surrounding. Our approach can be considered as a counterpart to the research conducted on teachers move- ment [13] which we already see as a big influence on at- tention of students [7]. We have also seen how peripheral awareness can be used to improve classroom interaction [4], and we aim to achieve similar reactions with less intrusive input methods.  3. STATE OF RESEARCH After establishing the theoretical base of our research we  present our activities, tools and cues for analysing classroom attention. We are classifying our activities in accordance with the five steps of learning analytics [14].  3.1 Capture Our main source of data are captured videos of classrooms.  This is consistent with our determination of making a min- imal impact on the classroom existing ecosystem, since the needed modification consists of inserting observational cam- eras, which are a passive way of collecting data and require no interaction on the subjects side. We are motivated to take this approach with the maturing of computer vision technologies [11] and gradual demystification of human per- ception [12].  Our setup for collecting data consists of a system of 3 to 4 cameras, of which one is observing the lecturer and the others are used for capturing audience reactions. On Figure 1 we see one of the recordings in the video synchro- nisation and annotation tool. This allows us to collect, for every recorded lecture, events such as changes of slides, their duration, amount of questions-answers, annotate person lo- cations in the classroom and other properties.  In addition to this, on a limited number of classes, we also  Figure 2: Spatial display of activities in class. Top represents the front of the class. Size of red circles shows reported level of attention. The blue rings indicate subjects that marked that they took notes during that period of the lecture.  collect self-reported levels of attention and actions by means of questionnaires. At four moments during the class, the students are asked to stop and report their level of attention and activities they were doing during the period, whether positive or negative. As noted in Section 2, we take interest into spatial distribution of student actions, and note the location of each subject in the classroom. A sample of our data and visualization can be seen on Figure 2.  In our visualisation tool, we are able to choose a set of attributes that we want to display (Figure 2), in order to explore how the activities are changing over space and time (at each of the 4 different interruptions). The reason why we did not use one of the existing tools such as Gephi [2] or Cytoscape [1] is that we have a predetermined layout of the graph, with small number of nodes and big variability of display styles which we would like to dynamically change during a presentation. For this reason, we hope that our solution, based on Processing library [3], will provide better visualisations for the intended domain, while it has no in- tention to compete with the above mentioned tools for other purposes.  3.2 Report In the exploratory phase of the research we are testing  different visual indicators of attention during the lecture. We are currently focused on two aspects - quantifying body motion and estimation of gaze direction.  Gaze direction has proven to be high importance cue for human interaction, as it is extensively written in Chapter 6 of [12]. For our usage, we can not assume that we will have high enough resolution of data to estimate the precise point of observation. We are aiming to reach rough estimation, and separation of 3 distinct directions: i) the teacher/slides, ii) notebook/bench and iii) other directions, which should be reachable even given the low resolution of current input if we take into consideration position of the entire head, similar to [17].  We tested our motion metrics by annotating regions in which each student resides and measuring the amount of movement inside of it. Directions of movement in the nine measured sub-regions for each student can be seen on Fig- ure 3. The current approach is based on estimating optical flow between 2 images by usage of iterative Lucas-Kanade method [20], which we found to be a good combination of speed and robustness.  We find it encouraging that even without localizing the  266    Figure 3: Motion detected of sub-regions of a single person. Blue lines which are more visible indicate higher amount of movement in that region. Ori- entation of the lines indicate the direction of the movement.  Figure 4: Movement of a single person; at the top of the timeline we display annotated events dur- ing the class (slide changes (blue), periods of an- swering questions (red), slide animations (green)). The red curve represents the normalised amount of movement in intervals of 10 seconds. The amount of movement can allow us to tell the difference be- tween changes in pose/major body movement (a,b) and writing activity (c)  movement (in which sub-section the motion was detected), we can estimate the difference between body-posture shifts, writing action and being still. We try to show importance of these measures and correlation with events during the class by overlaying our annotations of classroom events in Figure 4.  We would like to note that these visualisations are for primary purpose of evaluating whether our selected cues have the basic potential to provide us with meaningful infor- mations about students activity. Represented in this form, data can become overwhelming, as demonstrated in Figure 5. Each student has an individual set of habits, motions and postures, a set of attributes which renders the data unusable for a simple statistical processing, if we consider isolated in- dividuals.  3.3 Predict We start from the idea that the students are reacting ei-  ther to a stimulus which is coming from the teacher, or they are reacting to movement of their peers (mimicking attentive  behaviour). The assumption is that the class that is paying attention will have higher synchronisation in actions, e.g. higher probability that they will start writing down impor- tant information when they hear it. A classroom with low attention would either i) stay passive or ii) react in a more sporadic manner. The idea of this kind of measurement is attractive since its independent of the topic and individuals behaviour.  We can already see higher coordination in activities at Figure 5 at the four points of the class when the students were asked to fill out the attention form (the four periods of filling out the questionnaire are marked with red lines at the top of the time-line). The rest of the time, students tend to behave in an uncorrelated fashion, depending on their personal level of attention.  Another goal for prediction is to use machine learning techniques to process the data of individual students. We have already made the setup for collecting data which can be used for supervised machine-learning, by handing out the questionnaires. The amount of data needed for creating a valuable training set is too big for the current phase of the project, so we set this as our potential long-term goal.  3.4 Act In accordance with the principles of classroom orchestra-  tion [7], the acting upon the information is left to the lec- turer. Two stages of our process will encourage reflection on action by presenting an digest analysis of the classrooms attention to the teacher after the class and reflection in action with a indicator of current attention level to the teacher.  As it is seen in previous work [4] the display of information during the class must be kept in a minimal level in order not to become too distracting. With this in mind, an indication to the professor on the amount of attention in the previous 5 minutes would suffice as an signal whether a concept needs to be repeated or the pace of the lecture needs to be changed. We do not intend to provide concrete advices on how to change the lecture.  3.5 Refine As stated in the previous section, our main goal is to see  refinement in the mental model of the teacher. As we are in the exploratory phase of the project, our metrics and technologies will pass trough several iterations before we can conclude their relevance in the final framework.  4. CONCLUSIONS We presented our current state of research. Our cues for  analysing the attention during the lecture seem promising, and the goal of this paper is to discuss the feasibility of the approach with a broader community. Our immediate goal is to continue the work on the technical pipeline of the process in order to increase robustness and quality of the final result.  In the time of massive exploitation of virtual learning en- vironments and new learning concepts, we find that there is still room for improvement of the typical learning sce- nario. New vision technologies are enabling us to provide relevant statistical analysis of learning situations in a unob- trusive way, and provide the teachers with tools to deliver better lectures on the spot. The task is more challenging with the given variety of scenarios, but we think that by  267    Figure 5: Movement analysis of several persons and associated video with color-coded markers  setting modest goals and using low-demanding technologies we could produce significant benefit for the practise of ev- eryday lecture.  5. ACKNOWLEDGMENTS This work has been sponsored by the ProDoc SNF Grant,  project PDFMP1 135108. We would also like to thank all the participants of our experiments.  References [1] Cytoscape - an open source platform for  complex network analysis and visualization, http://www.cytoscape.org/.  [2] Gephi - open graph viz platform, https://gephi.org/.  [3] Processing, http://processing.org/.  [4] H. S. Alavi, P. Dillenbourg, and F. Kaplan. Dis- tributed awareness for class orchestration. In Learning In The Synergy Of Multiple Disciplines, Proceedings, pages 211225. 4th European Conference on Technol- ogy Enhanced Learning, 2009.  [5] J. Campbell, P. DeBlois, and D. Oblinger. Academic analytics: A new tool for a new era. Educause Review, 42(4):40, 2007.  [6] C. DiGiano and C. Patton. Orchestrating handhelds in the classroom with sris classsync(tm). Computer Sup- port for Collaborative Learning, pages 706707, 2002.  [7] P. Dillenbourg and P. Jermann. Technology for class- room orchestration. New Science of Learning, pages 525552, 2010.  [8] P. Dillenbourg, G. Zufferey, H. Alavi, P. Jermann, S. Do-Lenhand, Q. Bonnard, S. Cuendet, and F. Ka- plan. Classroom orchestration: The third circle of us- ability. In International Conference on Computer Sup- ported Collaborative Learning Proceedings, pages 510 517. 9th International Conference on Computer Sup- ported Collaborative Learning, 2011.  [9] T. Elias. Learning analytics: Definitions, processes and potential. Retrieved February, 9:2012, 2011.  [10] F. Fischer, C. Wecker, J. Schrader, P. Gerjets, and F. W. Hesse. Use-inspired basic research on the or-  268    chestration of cognition, instruction and technology in the classroom. EARLI, 2005.  [11] D. A. Forsyth and J. Ponce. Computer Vision: A Mod- ern Approach. Prentice Hall, 2011.  [12] R. B. A. Jr, N. Ambady, K. Nakayama, and S. Shimojo. The Science of Social Vision. Oxford Series in Visual Cognition, 2010.  [13] F. Lim, K. L. OHalloran, and A. Podlasov. Spatial pedagogy: mapping meanings in the use of classroom space. pages 235251, May 2012.  [14] D. Oblinger and J. Campbell. Academic analytics, ed- ucause white paper. Retrieved October, 20:2011, 2007.  [15] C. Ridgeway. Conformity, group-oriented motivation and status attainment in small groups. Social psychol- ogy, pages 175188, 1978.  [16] D. Schon. The reflective practitioner: How professionals think in action, volume 5126. Basic Books, 1984.  [17] R. Stiefelhagen. Tracking focus of attention in meet- ings. In Multimodal Interfaces, 2002. Proceedings. Fourth IEEE International Conference on, pages 273 280. IEEE, 2002.  [18] C. A. Tomlinson. The Differentiated Classroom: Re- sponding to the Needs of All Learner. Pearson, 1999.  [19] K. Wilson and J. H. Korn. Attention during lectures: Beyond ten minutes. Teaching of Psychology, pages 85 89, December 2007.  [20] J.-Y. Bouguet. Pyramidal Implementation of the Affine Lucas-Kanade Feature Tracker Description of the algo- rithm Inter corp. 2010  269      "}
{"index":{"_id":"39"}}
{"datatype":"inproceedings","key":"Slotta:2013:OCI:2460296.2460352","author":"Slotta, James D. and Tissenbaum, Mike and Lui, Michelle","title":"Orchestrating of Complex Inquiry: Three Roles for Learning Analytics in a Smart Classroom Infrastructure","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"270--274","numpages":"5","url":"http://doi.acm.org/10.1145/2460296.2460352","doi":"10.1145/2460296.2460352","acmid":"2460352","publisher":"ACM","address":"New York, NY, USA","keywords":"ambient display, intelligent agents, orchestration, smart classrooms","pdf":"Orchestrating of complex inquiry: Three roles for learning  analytics in a smart classroom infrastructure  James D. Slotta   OISE/University of Toronto   252 Bloor Street West,   Toronto, Ontario   jslotta@gmail.com   Mike Tissenbaum  OISE/University of Toronto   252 Bloor Street West,   Toronto, Ontario   mike.tissenbaum@utoronto.ca   Michelle Lui  OISE/University of Toronto   252 Bloor Street West,   Toronto, Ontario   michelle.lui@utoronto.ca       ABSTRACT  This paper presents our research of a pedagogical model known as  Knowledge Community and Inquiry (KCI), focusing on our design  of a technological infrastructure for the orchestration of the  complex CSCL scripts that characterize KCI curricula. We first  introduce the KCI model including some basic design principles,  and describe its dependency on real time learning analytics. Next,  we describe our technology, known as SAIL Smart Space (S3),  which provides scaffolding and analytic support of sequenced  interactions amongst people, materials, tools and environments. We  outline the critical role of the teacher in our designs and describe  how S3 supports their active role in orchestration. Finally we  outline two implementations of KCI/S3 and the role of learning  analytics, in supporting dynamic collective visualizations, real time  orchestrational logic, and ambient displays.   Categories and Subject Descriptors  K.3.1 Computer Uses in Education: Collaborative learning    General Terms  Design   Keywords  orchestration, smart classrooms, intelligent agents, ambient display   1. INTRODUCTION  This paper presents recent work from Encore Lab  (http://encorelab.org) in researching a pedagogical model known as  Knowledge Community and Inquiry (KCI), and a supporting  technology infrastructure for smart classrooms.  KCI describes a  class of pedagogical designs where students engage in carefully  scripted inquiry and collaborative knowledge construction to  achieve science learning goals [17, 18]. KCI curriculum includes  complex forms of interaction such as aggregated observations, (i.e.,  by the classroom community), and scripted collaborations within  and between groups (e.g., via jigsaw). Individuals are scaffolded in  all aspects of the curriculum, as they engage in reflection, critique,  discussion, or design activities performed individually, in groups,  or as a wider learning community. Our designs rely on a  sophisticated technology infrastructure that supports the  orchestration of KCI curriculum through real-time coordination of   devices, tools and materials, student group and role assignments  across multiple contexts: in school, at home, and in field-based  activities.   This paper begins by reviewing KCI, and our open source software  framework, then presents two recent KCI curricula with a focus on  their learning analytic features. Specifically, we describe our  application of real time data mining of student contributions, using  intelligent agents and a messaging software architecture, to inform:  (1) the dynamic, emergent representation of aggregated student  contributions, which provides a crucial reference and input for  ongoing classroom activities and (2) the orchestration of complex  inquiry scripts, including pedagogical logic that is often left  unbound (i.e., with variable assignments that can only be fixed  after student data becomes available to analyze), and (3) the  changing of state for ambient representations of individual, group  and collective progress during the activity, which also contributes  greatly to the orchestration of KCI scripts.    2. KNOWLEDGE COMMUNITY AND  INQUIRY  KCI curriculum is developed through a sustained co-design effort  including teachers, researchers, technology developers, and  interaction designers [12]. Students work in parallel, building on  one anothers contributions to develop a community knowledge  base that is indexed to the content domain learning goals (e.g., in a  science topic) [15, 16]. In KCI, inquiry activities are designed to  engage students individually and in small groups where they make  use of their community knowledge base as a resource for carefully  scripted inquiry designs.  The designed curriculum constitutes a  script that depends upon real-time executive decisions or  assignments, and involves student-contributed content, social  tagging, ubiquitous computing, and immersive environments. The  script is orchestrated by the teacher, who in turn is enabled by a  smart classroom learning environment, that includes ambient  displays and spatial mapping of activities.   Recent KCI designs have included tangible and embodied  interactions with the environment (e.g., spatially dependent  interactions within the classroom, or user contributed observations  from the students neighborhood), scaffolded by tablet computers  that track student location and guide data collection activities. The  teachers role is that of an expert collaborator or mentor,  responding to student ideas as they emerge, and orchestrating the  pedagogical flow of activities. Teachers are not just a guide on the  side  an image or assignment that often confuses teachers and  leaves them sidelined by overly scaffolded learning designs with  no explicit role for the teacher. Rather, KCI specifies an interaction  script that includes a clear and consequential role for the teacher in  orchestrating the design.    Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial  advantage and that copies bear this notice and the full citation on the  first page. Copyrights for components of this work owned by others  than ACM must be honored. Abstracting with credit is permitted. To  copy otherwise, to republish, to post on servers or to redistribute to  lists, requires prior specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   270    In a recent KCI activity designed for high school physics, we  engaged students in solving ill structured problems in a smart  classroom setting, where they worked collaboratively at various  stations within the room, inputting their tags and votes on personal  tablet computers, then collaborating to negotiate a consensus. After  setting up the problems for solution, the group submitted their  readiness, and the teacher was notified (on his own tablet) that the  group was ready for a debriefing. The teacher then walked over to  that groups station and consulted with them about their problem  set-up. If they had done a sufficiently detailed and accurate job, the  teacher touched a go ahead button on his tablet, and the student  tablets were refreshed with new tasks for the next activity in the  script. If the teacher felt that the students needed to refine their set- up, he could have them work on it some more and, when ready, re- alert him to review their work again.   The example above illustrates the current focus of our research,  with regard to scaffolding technologies and learning analytics.  Working closely with the classroom teacher, we develop  substantive curricula  typically whole semester or multi-week  according to the KCI model [16].  Following the designed script,  we then develop all materials and scaffolding for activities at the  individual, small group and whole class levels. Internet-based  software collects student contributions, and intelligent agents  perform real-time data mining to compile an aggregated  representation of student contributions presented on a Smartboard  at the front of the room. Students respond to this emergent view,  perhaps identifying cells in a table where they disagree or need  more data. Teachers can be prompted on their own personal tablets  for a variety of interactions, and ambient displays provide  secondary channels of information to support orchestration.    Studies of KCI in smart classroom environments have included  many technical challenges and innovations, but two in particular  seem relevant to the issue of learning analytics and knowledge.  First, there is a challenge of dynamically representing students  individual and collective knowledge contributions.  How should  we capture the running aggregation of student observations In  table form or as a tag graph There are numerous ways to display  any given aggregate; how do we choose the right one The  aggregate representation (AR) that we require is one that reveals  progress and productive patterns, promoting discourse and  productive inquiry decisions amongst students and teachers.  Second, there is a challenge of orchestrating the often unbound  pedagogical script.  That is, the script may specify that it needs all  students to be sorted into four groups, according to the four most  common social tags, but cannot specify, a priori, what those tags  will be, nor which students will be best suited to what groups.   Only by actively monitoring student-contributed data in real time  can the system scaffold such complex and open specifications of  inquiry scripts.   3. SMART CLASSROOM TECHNOLOGY   Our recent work has focused on scripting and orchestration of KCI  learning designs [3, 5]. The script provides a specification of  pedagogical roles, goals, materials, activities, outcomes, and  various dependencies that might affect the sequence, structure or  content of the learning experience. The script can be seen as  specifying the structured interactions that occur between learners  and any materials or environments, and with peers and teachers.  These interactions would typically be scaffolded [7] to support  constructivist learning and productive exchange or discourse [19].  Enactment of the script  in classrooms, museums, at home, in the  field, or any combination or settings  has been referred to as a  process of orchestration [3, 5], where the teacher, technology,   materials, and even the physical classroom (walls, furniture) play a  role in coordinating conditions and activities of the script.    Technological environments can provide orchestration support for  the coordination of materials, tools, environments and structured  interactions.  Moving beyond the realm of computer-based learning  environments such as WISE [14], or Quest Atlantis [1]  investigators are now incorporating new forms of interactions that  include ubiquitous computing embedded within furniture [2,10],  walls [8, 9, 11], and even floors [4, 6]. Our group has developed an  infrastructure for smart classrooms called the Scalable Architecture  for Interactive Learning (SAIL) that employs learning analytic  techniques to allow students physical interactions and spatial  positioning within the room to play a strong role in scripting and  orchestration. Using intelligent agents and real time data mining,  the room can be given an awareness of its occupants: who is in  the room, what activities or conditions they are assigned to, what  tasks have been completed, and even student models that are  dynamically constructed and queried.   At present, SAIL includes: (1) a student management layer, (2) a  content management system for the storage and retrieval of student  work, (3) a messaging server architecture for connecting devices  and software in real-time, (4) a visualization layer that displays  aggregated student work and materials, and (5) an intelligent  software agent architecture that can aid the complex distribution of  materials, tracking and movement of students, and prompting of  teachers based on predefined criteria or factors emerging from  student interactions.  See Figure.   Sections below describe two recent KCI designs, each consisting of  a complex script that was co-designed and orchestrated by the  teacher, with scaffolding from a SAIL-based smart classroom. For  each of the two examples, we focus on two elements that are  important to learning analytics: (1) our use of symbolic  representations of the design (2) the role of the technology  environment in orchestration (including ambient displays, spatial  features, and data mining).   4. EVOROOM: IMMERSIVE SIMULATION  FOR COLLECTIVE INQUIRY IN BIOLOGY  EvoRoom is a room-sized simulation of a rainforest environment  designed for a smart classroom in a high school biology curriculum  in topics of biodiversity and evolution. The immersive experience,  in which our smart classroom was transformed through large-wall     Figure 1:  SAIL Architecture   271    projections into a dynamic, interactive rainforest, was designed as  part of a two-month KCI curriculum enacted by the teacher [8].  Within the room, students are engaged in collaborative inquiry,  taking on the role of field researchers making observations of the  simulations via their tablet computers. The rainforest itself was  dynamic, rendered in Flash, with animated flora and fauna.  The  rainforest also evolved through 200 million years, with distinct  versions of background, flora and fauna developed for 9 different  time increments. Students observed assigned species over time,  working in groups to make connections between different species.  Collectively, students determined how various mechanisms of  evolution were involved in shaping present day biodiversity [8].   4.1 Emergent Aggregate Representation    A central challenge to our research is as follows: given a roomful  of students who are immersed in such an environment, how can  they be engaged collectively in a constructive learning experience  How can their interactions with the media (e.g., comparing the  simulations of 150 MYA with those of 100 MYA) be aggregated  across all student observations to reveal important patterns that lead  to insight about complex biology topics such as natural selection,  genetic drift or sexual selection We came up with the idea of a  real-time, emergent cladogram: a representation common to  evolutionary biology that shows the species descendancy relations  over time.  Typically, these diagrams start off quite simple, with  species radiating as time moves forward.     We engaged students, via their tablet computers, in making discrete  observations of whether certain species were present at various  times, and  if not  which species that actually was present might  be their assigned species predecessor.  Over one session, students  created a total of 157 such relationships. This would have been  unwieldy without real time data analytics which created a running  assembly of a cladogram, resulting in a kind of social puzzle (a  form of playification that engaged students in completing the  whole picture  see Figure: Evoroom).  The result was an activity  where students felt they were engaged meaningfully as a  community within the environment, creating an aggregate  representation that the teacher was able to use for purposes of a  concluding discussion about evolutionary processes.     4.2 Technology scaffolds for orchestration  Once students entered the EvoRoom simulation, all student  experience was orchestrated by the teacher and facilitated S3  technology. Because the students had been engaged in related  activities throughout the broader curriculum design (e.g.,  researching a specialization species), they entered not as  complete novices to the rainforest, but with some knowledge and  specific expertise. We designed the experience to build on their  prior expertise, using intelligent agents to assign students to  different regions of the room and observation tasks. Their personal  tablet gave them instructions of where to go and what to observe.  To confirm that students were at the assigned location within the  room, we employed two-dimensional QR codes, which students  were prompted to photograph with their tablets. The teacher was  able to coordinate the discussion by referring to the aggregate  representations at the front of the room.    We developed an Instructors tablet that allowed the teacher to  control all student tablets in the room and to advance the room  through various evolutionary time periods. Intelligent agents  tracked students real-time activities, sending the data to a central  database that housed curriculum materials and the products of  student interactions. As part of the activity, a sorting agent  assigned students different organisms to look for. When students  scanned QR codes at the different stations, the agents recognized  their location and sent further, contextualized instruction to the  tablets. The simulation files were also a part of this network and  were controlled with a custom tablet application, allowing the  teacher to manage the time spent in each portion of the activity,  controlling the pedagogical flow within the room.   4.3 Ambient Feedback  Two Smartboards at the front of the room provided ambient  feedback as students progressed through the activity. Observations  captured on students tablet computers were aggregated and  displayed for each step of the activity. For example, as students  noted their assigned species potential ancestors at different time  periods, everyones answers were placed in the interactive  cladogram - allowing teacher and student manipulation and serving  a locus of discussions.     Figure 2: Evoroom. Large screen projections around the room (above) display the immersive  simulation, together with audio tracks of natural rainforest sounds, transform a smart classroom into a  rainforest in Southeast Asia.  Students are engaged collectively, collaboratively and individually in a  series of carefully scripted activities, such as to make observations of whether individual species are  present in the environment.  This results in a dynamic aggregated representation (right) of an emergent  cladogram that guides student activity and leads to productive discussions. The cladogram shows  species evolutionary relationships emerging over time, from bottom to top, with color coded boxes  representing plants, animals and insects. Real time data mining of student contributions was employed,  with each student observation interpreted by an intelligent agent (via subscription to an XMPP  messaging channel), then handled by integrating it into the structured representation.   272    5. PHYSICS LEARNING ACROSS  CONTEXTS & ENVIRONMENTS (PLACE)  This project infused a traditional high school physics course with a  persistent knowledge base that captured students social tags and  reflections about homework problems, as well as their own  contributions of everyday phenomena (i.e., photos and videos) and  constructed problems [21]. Students engaged in carefully scripted  activities during three physics units on force, motion, and energy,  producing a dynamic, filterable web of relevant content, indexed to  the 13 major principles of the course units. For example, students  regularly answered multi-choice qualitative problems as  homework, tagging them in terms of the relevant principles, and  adding reflections. The teacher used this information (patterns of  student replies, as well as reflections) to lead productive  discussions in class. Students were also required to add physics  examples into the web (i.e., found on the Internet or photographed  in their everyday lives), tagging them to the relevant physics  principles, and to review, discuss, and refine the contribution of  their peers. The resulting web provided a rich array of physics  connections across principles, illustrating the coherent nature of the  domain, and providing a dynamic resource for further inquiry.   5.1 Aggregate Representations  As the PLACE Web matured (i.e., gained content and social  information) it provided a resource for subsequent student inquiry,  including a smart classroom activity where students applied the  various problems, principles, reflections and equations from their  web to collectively analyze selected Hollywood movie clips. Their  objective was to decide whether or not the clip violated the laws of  physics, and how the clip could be set up as a physics problem.  This served as a culminating activity, engaging students with the  principles in new ways, with an embodied aspect of physically  moving ideas and people around the room.    Intelligent agents conducted real time data mining as students  constructed aggregate representations of their emerging solutions to  the ill-defined problems. For example, students were asked to tag  the Hollywood clip with equations, drawn from physics problems  that they had seen during previous weeks.  At first, all equations  tagged by students were shown on the screen (using XMPP  messaging/agents that allowed students to send the equations   from their tablets onto the shared display). Those problems were  provided to students by agents  doing a real time query of the  PLACE.Web knowledge base to find problems that had been  tagged with the same principles as those applied to the video clips.  On observing this aggregate display of equations, students were  scaffolded to form a consensus about which equations would be  their final set to be passed on to the next group who would use  them to try to set up the problem.  These forms of dynamic shared  displays were developed for several stages of the step-wise problem  solving process, allowing students to become familiar with the  emergent representation and consensus process.   5.2 Technology scaffolds for orchestration  As with the EvoRoom project, the S3 technology helped the teacher  to orchestrate our PLACE curriculum design, reducing his load to  one of responding to patterns he observed or requests made by  students. We improved upon the orchestrational technology used in  EvoRoom, developing a teacher tablet to show real-time updates of  all student groups and broker the flow of activities in the room,  (i.e., requiring a teacher action on the tablet before a group could  progress to the next step) [20]. Intelligent agents also played a more  sophisticated role in PLACE, performing pedagogical operations  such as grouping students according to their choices made in the  immediately preceding activity or supplying students with physics  problems that they had rated differently from others.    A set of intelligent agents orchestrated the flow of materials and  students in the smartroom. The agents responded to emergent  patterns in the data, making orchestration decisions on-the-fly,  and providing teachers and students with timely information. Three  agents are of relevance to our analysis of orchestration: (1) The  Sorting Agent sorted students into groups and assigned room  locations. The sorting was based on patterns that only emerged  during enactment. For example, students were sorted into groups  for Step two based on their tagging choices at made during Step  one. (2) The Consensus Agent requiring consensus to be achieved  among group members before they could progress to the next step;  (3) The Bucket Agent coordinated the distribution of materials,  providing one item at a time until the bucket was empty.    5.3 Ambient Feedback  A large Smartboard screen at the front of the room (i.e., not one of  the 4 Hollywood video stations) provided a persistent, passive  representation of the state of individual, small group, and whole  class progression through each step of the smart classroom activity.     Figure: PLACE.Web.  Students work collaboratively and  collectively in the smart classroom to set up and solve ill- defined Hollywood physics problems.     Figure: Ambient Display. A large projected display at the fornt  of the room helped in orchestration of complex activities.   273    A large projected display dynamically updated student location  assignments within the room, and tracked the timing of each  activity, using three color codes (a large color band around the  whole board that reflected how much time was remaining: green  (plenty of time remaining), yellow (try to finish up soon), and  red (you should be finished now). The ambient display provided  feedback to students and teacher alike, ensuring timely enactment  of the script, which involved assignments of students to four  different stations, in a complex sequence, all in a 70 minute period.   6. Conclusion   Over the past several years, our research group has advanced our  technology framework, in pursuit of a means to coordinate complex  collaborative inquiry designs that are not fully specified at their  outset [13].  That is, they depend upon aspects of the student  contributions or interactions, which determine sequences, grouping,  or distribution of material.  In recent years, many of our designs  have centered on a collective construction, whether it be a wiki, or  aggregate representation that emerges in real time. We have begun  to develop symbolic representations that formalize such  dependencies, not necessarily with the aim of achieving machine  readable designs (although such would not be out of the question)  but rather to support the discourse of science, in our own design  process and in sharing those designs in our publications.  Too long  constrained by the absence of such formalism, we are now  progressing toward a pedagogical logic that, when accompanied by  intelligent agents and smart classroom infrastructure, could allow  the investigation a wide range of inquiry designs and interaction  paradigms.   7. References  [1] Barab, S, A., & Dede, C. 2007. Games and immersive   participatory simulations for science education: An emerging  type of curricula. Journal of Science Education and  Technology, 16(1), 1-3.   [2] Dillenbourg, P., Huang, J., & Cherubini, M. 2008. Interactive  artifacts and furniture supporting collaborative work and  learning. Springer Verlag.   [3] Dillenbourg, P., & Jermann, P. 2007. Designing Integrative  Scripts. In F. Fischer, I. Kollar, H. Mandl & J. M. Haake  (Eds.), Scripting Computer-Supported Collaborative Learning  (Vol. 6, 275301). Boston, MA: Springer.   [4] JohnsonGlenberg, M., Birchfield, D., Koziupa, T., Savio- Ramos,C. & Cruse, J. 2012. Seeing It versus Doing It: Lessons  from Mixed Reality STEM Education. In Abrahamson, D.  (Chair), Youre It! Body, Action, and Object in STEM  Learning. Proceedings of the 11th International Conference of  the Learning Sciences (ICLS) - Volume 2, (pp. 99-109). ISLS.    [5] Kollar, I., Fischer, F., & Slotta, J. D. 2007. Internal and  external scripts in computer-supported collaborative inquiry  learning. Learning & Instruction, 17(6), 708-721.   [6] Lindgren, R., Aakre, A., & Moshell, J. M. 2012. Youre the  Asteroid! Body-Based Metaphors in a Mixed Reality  Simulation of Planetary Astronomy. In Abrahamson, D.  (Chair), Youre It! Body, Action, and Object in STEM  Learning. Proceedings of the 11th International Conference of  the Learning Sciences (ICLS) - Volume 2, (pp. 99-109). ISLS.    [7] Linn, M. C., & Eylon, B.-S. 2006. Science education:  Integrating views of learning and instruction. In P. A.  Alexander, & P. H. Winne (Eds.), Handbook of educational  psychology (pp. 511544). Mahwah, NJ: LEA   [8] Lui, M., & Slotta, J. D. 2012. Designing Immersive  Environments for Collective Inquiry. Proceedings of the 10th   International Conference of the Learning Sciences (ICLS  2012) - Volume 2 (pp. 12-14). ISLS.   [9] Lui, M., Tissenbaum, M., & Slotta, J. D. 2011. Scripting  collaborative learning in smart classrooms: Towards building  knowledge communities. Proceedings of the 9th International  Conference on Computer-Supported Collaborative Learning  (CSCL)  Volume 1, (pp. 430-437). ISLS.   [10] Mercier, E., McNaughton, J., Higgins, S. & Burd, E. 2012,  Orchestrating Learning in the Multi-touch Classroom:  Developing Appropriate Tools, in van Aalst, J., et al., eds,  Short Papers, Symposia and Abstracts 2: The Future of  Learning: 10th International Conference of the Learning  Sciences (ICLS 2012).   [11] Moher, T. 2008. Learning and participation in a persistent  whole-classroom seismology simulation. Proceedings  International Conference of the Learning Sciences (ICLS  2008), Vol. 2 (Utrecht, Netherlands, June 2008), 82-90.    [12] Roschelle, J. & Penuel, W.R. 2006. Co-Design of Innovations  with Teachers: Definition and Dynamics. Proceedings of the  7th International Conference on Learning Sciences,  Bloomington, IN, pp. 606-612.   [13] Slotta, J. D. 2010. Evolving the classrooms of the future: The  interplay of pedagogy, technology and community. In K.  Mkitalo-Siegl, F. Kaplan, J. Zottmann & F. Fischer (Eds.).  Classroom of the Future. Orchestrating collaborative spaces.  (215-242). Rotterdam: Sense.   [14] Slotta, J. D., & Linn, M. C. 2009. WISE Science: Web-based  Inquiry in the Classroom. New York: Teachers College Press.   [15] Slotta J. D. and Najafi H. 2010, Knowledge Communities in  the Classroom. In: P. Peterson, E. Baker, B. McGaw, (Eds.),  International Encyclopedia of Education. volume 8, pp. 189- 196. Oxford: Elsevier.    [16] Slotta, J. D., & Najafi, H. 2012.  Knowledge Community and  Inquiry  Wikibased collaboration in a high school science  course as a knowledge community.  In N. Lavigne, (Ed.),  Emerging Technologies for the Classroom:  A Learning  Sciences Perspective. Pp. 93-112.  New York. Springer   [17] Slotta, J. D., & Peters, V. L. 2008. A blended model for  knowledge communities: Embedding scaffolded inquiry.  International Perspectives in the Learning Sciences: Cre8ing a  learning world. Proceedings of the 8th International  Conference for the Learning Sciences (pp. 343-350). ISLS.   [18] Slotta, J. D., Tissenbaum, M. & Lui, M. 2011. SAIL Smart  Space: Orchestrating Collective Inquiry for Knowledge  Communities. Proceedings of the 9th International Computer- Supported Collaborative Learning Conference, Hong Kong.  Vol 3, pp. 1082-1083.    [19] Slotta, J.D., Tissenbaum, M., Lui, M., & Zukowski, M.  2012.  Smart Classrooms for Knowledge Communities: EPIC  Technology Environment.  Proceedings of the 10th   International Conference of the Learning Sciences - Volume 2:  64-71. ISLS.   [20] Tissenbaum, M., Lui, M. & Slotta, J.D. 2012. Smart  Classrooms for Knowledge Communities: Learning Across  Contexts in Secondary Science. Paper presented at the annual  meeting of the American Educational Research Association  (AERA), Vancouver, BC.   [21] Tissenbaum, M., Lui, M., & Slotta, J.D. 2011. Orchestrating  Collaborative Science Curriculum Across Formal and  Informal Contexts. Proceedings of the 9th International  Computer-Supported Collaborative Learning Conference,  Hong Kong. Vol 3, pp 1143-114  274      "}
{"index":{"_id":"40"}}
{"datatype":"inproceedings","key":"Baer:2013:CTS:2460296.2460354","author":"Baer, Linda L. and Duin, Ann Hill and Norris, Donald and Brodnick, Robert","title":"Crafting Transformative Strategies for Personalized Learning/Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"275--277","numpages":"3","url":"http://doi.acm.org/10.1145/2460296.2460354","doi":"10.1145/2460296.2460354","acmid":"2460354","publisher":"ACM","address":"New York, NY, USA","keywords":"academic analytics, collaboration, learning analytics, personalized learning, strategic planning","abstract":"Personalized learning environments and learning analytics hold the promise to transform learning experiences, enhance and accelerate student success, and open up student learning to resources and experiences from outside individual institutions. To achieve their potential, personalized learning projects must move beyond individual, stand-alone projects or innovations to reshaping the institutional experience. Learning science must connect with learning pedagogy and design. Learners and institutions must have access to tools and resources that assist in customizing student progress and supplemental learning needs. Teachers and faculty must be empowered to provide teaching and learning environments that allow individual students to thrive. All this will require unique partnerships and collaborations within and across institutions, incorporating the best learning science findings and bridging with public and private entities developing the learning and analytic tools to support personalized learning. Crafting a strategy to embrace and sustain the transformative power of personalized learning systems will require strong leadership and clear planning models to align with institutional planning and future investments. ","pdf":"Crafting Transformative Strategies for Personalized  Learning/Analytics      Linda L. Baer  Minnesota State University, Mankato   315 Wigley Administration  Mankato, MN  56001   1-507-389-1333  linda.baer@mnsu.edu      Donald Norris  Strategic Initiatives, Inc.   12209 Jonathons Glen Way  Hendon, VA  20170   1-703-450-5255  dmn@strategicinitiatives.com   Ann Hill Duin  University of Minnesota  315 Pillsbury Drive SE   Minneapolis, MN  55455  1-612-625-9259   ahduin@umn.edu   Robert Brodnick  Strategic Initiatives, Inc.   12209 Jonathons Glen Way  Hendon, VA  20170   1-703-450-5255  robert@strategicinitiatives.com      ABSTRACT  Personalized learning environments and learning analytics hold  the promise to transform learning experiences, enhance and  accelerate student success, and open up student learning to  resources and experiences from outside individual institutions.  To  achieve their potential, personalized learning projects must move  beyond individual, stand-alone projects or innovations to  reshaping the institutional experience.     Learning science must connect with learning pedagogy and  design.  Learners and institutions must have access to tools and  resources that assist in customizing student progress and  supplemental learning needs. Teachers and faculty must be  empowered to provide teaching and learning environments that  allow individual students to thrive.  All this will require unique  partnerships and collaborations within and across institutions,  incorporating the best learning science findings and bridging with  public and private entities developing the learning and analytic  tools to support personalized learning.    Crafting a strategy to embrace and sustain the transformative  power of personalized learning systems will require strong  leadership and clear planning models to align with institutional  planning and future investments.   General Terms  Measurement   Keywords  Academic analytics, learning analytics, personalized learning,  collaboration, strategic planning.   1. INTRODUCTION  Learning science researchers and practitioners provide extensive  evidence that learning can be improved, unleashed and  revolutionized.  Thomas and Brown [1] have stated that learning  is happening all around us, not just in the classroom.  What  happens to learning when we move from the stable infrastructure  of the twentieth century to the fluid infrastructure of the twenty- first century where technology is constantly creating and  responding to change     The growing digital, networked infrastructure is amplifying our  ability to access and use nearly unlimited resources and incredible  instruments to connect learners with learning. This new type of  learning requires us to shift our thinking about education from  traditional forms to augment learning at every facet and stage of  life. Thomas and Brown call it the arc of life learning.   2. PERSONALIZED LEARNING  The U. S. Department of Education [2] reports that:   Personalization refers to instruction that is paced to learning  needs [i.e. individualized], tailored to learning preferences [i.e.  differentiated], and tailored to the specific interests of different  learners. In an environment that is fully personalized, the learning  objectives and content as well as the method and pace may all  vary.   2.1 Technology and Personalized Learning  Products  Technological advances now provide personalized products,  services and user experiences, though education has only  scratched the surface on the potential to personalize the learner  experience.  Personalization is affecting many aspects of our lives  today in terms of customer data and virtual communities.   Technology can bring an improved understanding of performance  levels, learning styles and learning preferences and the capacity  for instructional strategies and content to better meet learning  needs.  Tools now can enable a customized approach to learning  as adaptive and personalized as a private one on one lesson.  Content is smart, feedback is adaptive to the individual learner   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK'13, Apr 08-12 2013, Leuven, Belgium  ACM 978-1-4503-1785-6/13/04.   275    progression, and learning paths are personalized to each  individual student.  In the Smart Sparrow adaptive learning  platform [3], the focus is on promoting learning by doing, being  intelligent and adaptive, and empowering teachers. Using the  Knewton product, the online content is continuously adapted for  each individual student. Knewton now includes a partnership with  Pearsons MyLabs and Mastering software, which are now used  by about 10 million students [4].   2.2 Role of Educators  Teachers must actively engage with this new environment as  adaptive authoring tools will be required to change content and  analytics will be required to inform learners, teachers and advisors  about how students are learning and collaborative environments.   Teachers now work with professional developers and designers to  make truly interactive and learner rich content.    Personalized learning requires that teachers and students have  real-time access to meaningful data to better facilitate each  student's experience. This also requires sophisticated data and  assessment systems, which track, illustrate, and translate the data  in an on-going way into specific information that informs the  student and teacher, as well as the instructional tools, content, and  learning approach best suited for the student.     2.3 Transformation of Learning  The promise of transforming learning experiences, enhancing and  accelerating student success resides within the personalized  learning environments, learning analytics, as well as opening up  student learning to resources and experiences from outside  individual institutions.     To achieve their potential, personalized learning projects must  move beyond individual, stand-alone projects or innovations to  reshaping the institutional experience.  This will require a  strategic understanding of the transformative promise of  personalization to maximize student learning potential.    Personalized, open learning initiatives have been developing in  higher education for some time.  The launch and sustainability of  personalized learning requires new collaborations and  partnerships from the learning sciences, brain research, learning  pedagogy, instructional design and delivery.  In addition, creative  partnerships are evolving with higher education learning projects  and technology vendors. Carnegie Mellons Open Learning  Initiative, the Next Gen Learning projected funded by The Bill &  Melinda Gates Foundation, and major projects funded by the EU  have gathered attention.  Major educational publishers such as  Pearson, McGraw-Hill, Wiley & Sons and Cengage Learning  have long been transposing their textbook content on to dynamic  online platforms that are equipped to collect data from students  that are interacting with it. Huge instructional software vendors  such as Blackboard, Ellucian, and D2L have invested in analytics  tools that aim to predict student success based on data logged by  their client universities enterprise software systems.     Knewtons partnership with Arizona State University has received  considerable attention in the US, and other companies are  emerging worldwide. The University has established a Learning  Science Institute and ASU Online, the massive online arm that the  university co-runs with Pearson, the for-profit education  company. It is a place... where ideas and university research  become new technology and commercial enterprises,A place  where the future is invented.[4]   Smart Sparrow Adaptive eLearning PlatformTM [5] is an  entrepreneurial toolset that includes adaptive tutorials, virtual  labs, smart courses, serious games and simulations.  The Platform  is a web-based suite of tools that let teachers create rich,  interactive and adaptive learning experiences. We know that  hands on learning leads to the best possible education outcome  and fosters student engagement and the Adaptive eLearning  Platform is particularly suited for this mode of learning. Because  Smart Sparrow is intelligent, it adapts to students as they learn,  giving them instant feedback and delivering lessons and activities  at different levels of difficulty depending on how a student is  performing.    2.4 Collaborations and Federated Solutions  Optimizing student success is imperative for expanding higher  education worldwide. Intelligent investments are needed to garner  wide support and can be justified through return on investment  (ROI) or even better, value on investment (VOI).  Moreover,  improving performance, productivity, and institutional  effectiveness are the new gold standards for institutional  leadership in the 21st century. Personalized learning systems and  associated analytics are critical to both student success and  institutional effectiveness. This connects with federated solution  providers, adaptive learning, and the development of creative  collaborations.   3. THE PANEL DISCUSSION  Using a set of four framing questions, panelists will explore and  debate the facets of how to encourage the engaged use of  personalized learning/analytics and how to craft strategies for  using PL/A to transform the institution.   3.1 How might personalized/adaptive learning  move beyond pilots to potentially  transformative technology  Personalized, adaptive learning and learning sciences, and the  analytics embedded in them, have the potential to transform  learning. They accelerate learning, the acquisition of  competences, and the capacity to demonstrate mastery. They  enhance student success. They also have the potential to span  organizational boundaries, open up institution-centric learning,  and create connected learning across all settings.  Given this  potential, why such reticence to move beyond prototypes to  deployments at scale across institutions Panelists will explore  and debate how to engage institutional stakeholders and craft  strategies.   3.2 How should institutions leverage this  emerging marketplace of homegrown and  solution provider options  Personalized, open learning initiatives have been developing  across higher education for some time.  Examples include  Carnegie Mellons Open Learning Initiative and the Next Gen  Learning projected funded by The Bill & Melinda Gates  Foundation. Projects funded by the EU include the Responsive  Open Learning Environments and the Generic Responsive  Adaptive Personalized Learning Environment (GRAPPLE)  focused on learning support.    Moreover, technology vendors have been rolling out a wide range  of personalized learning solutions that are available as hosted  solutions.  Knewtons partnership with Arizona State University  has received considerable attention in the US, and other  companies are emerging worldwide, including Smart Sparrow.    276    Might the killer app for higher education come when personalized  learning and leaning sciences are embedded in MOOCs and this  offering is made available globally   This wide emerging and evolving marketplace of personalized  learning options is available to institutions and individual  educators. How should institutional leaders consider these options  as they craft partnerships and strategic plans for personalized  learning Will educators embrace these institutional plans or go it  alone   3.3 How might institutions nurture their  commitments to personalized  learning/analytics  Some institutions have found a way to foster an institution-wide  commitment to personalized learning through active engagement  with educators.  For example, University of Minnesota Rochester,  led by a visionary Chancellor starting a new campus, has  implemented an intelligent system for education and assessment  of learning.  Building a new campus provided the opportunity for  establishing a pervasive culture of behaviors in support of risk  taking, measurement, and continuous improvement, and  embedded predictive analytics in a system called iSEAL. The  challenge confronting higher education is to introduce these  innovations, at scale, across higher education. Panelists will  explore and debate the challenges of nurturing commitments to  personalized learning and analytics, whether that be as part of  starting a new campus or of working with established campuses.   3.4 How might institutions align personalized  learning/analytics with institutional strategy  Personalized, adaptive learning indeed can be transformative to  individual institutions.  It might also be the ingredient that can   turn MOOCs into a killer app.  How can institutions transform  their strategic planning to accommodate this potential by aligning  strategy for PLE/Analytics with institutional strategies  How can  institutions advance innovations in personalized learning/analytics  and build/acquire the organizational capacity to do so      The exponential growth in marketplace options can lead to  distrust among educators as institutional leaders explore platforms  and engage vendor partners. How might institutional leadership  build trust that leads to sustained effort This perspective will  focus on how to embed PLE/analytics in institutional strategic  planning, crafting strategy, advancing innovation and committing  to organizational develop, and leveraging both internal and  external partnerships and collaborations.   4. REFERENCES  [1] Thomas, D. and Brown, J. S. 2011. A New Culture of   Learning. Douglas Thomas and John Seely Brown.  [2] National Education Technology Plan.   http://www.ed.gov/technology/netp-2010  [3] https://www.smartsparrow.com/  [4] Kolowich, S. 2013. The new intelligence. Inside Higher   Education.  http://www.insidehighered.com/news/2013/01/25/arizona-st- and-knewtons-grand-experiment-adaptive-learning    [5] https://www.smartsparrow.com/adaptive-elearning/          277      "}
{"index":{"_id":"41"}}
{"datatype":"inproceedings","key":"BuckinghamShum:2013:EDS:2460296.2460355","author":"Buckingham Shum, Simon and Hawksey, Martin and Baker, Ryan S. J. D. and Jeffery, Naomi and Behrens, John T. and Pea, Roy","title":"Educational Data Scientists: A Scarce Breed","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"278--281","numpages":"4","url":"http://doi.acm.org/10.1145/2460296.2460355","doi":"10.1145/2460296.2460355","acmid":"2460355","publisher":"ACM","address":"New York, NY, USA","keywords":"data science, educational data mining, educational data scientist, learning analytics","abstract":"The Educational Data Scientist is currently a poorly understood, rarely sighted breed. Reports vary: some are known to be largely nocturnal, solitary creatures, while others have been reported to display highly social behaviour in broad daylight. What are their primary habits? How do they see the world? What ecological niches do they occupy now, and will predicted seismic shifts transform the landscape in their favour? What survival skills do they need when running into other breeds? Will their numbers grow, and how might they evolve? In this panel, the conference will hear and debate not only broad perspectives on the terrain, but will have been exposed to some real life specimens, and caught glimpses of the future ecosystem. ","pdf":"Educational Data Scientists: A Scarce Breed   Martin Hawksey   Jisc CETIS   c/o University of Strathclyde  16 Richmond Street, Glasgow    G1 1XQ, UK   martin.hawksey@strath.ac.uk   Naomi Jeffery  The Open University in Scotland   10 Drumsheugh Gardens,  Edinburgh    EH3 7QJ, UK   naomi.jeffery@open.ac.uk   Roy Pea  H-STAR Institute   Stanford University  450 Serra Mall, Building 160,   Stanford CA 94305-2055, USA   roypea@stanford.edu    ABSTRACT  The Educational Data Scientist is currently a poorly understood,  rarely sighted breed. Reports vary: some are known to be largely  nocturnal, solitary creatures, while others have been reported to  display highly social behaviour in broad daylight. What are their  primary habits How do they see the world What ecological  niches do they occupy now, and will predicted seismic shifts  transform the landscape in their favour What survival skills do  they need when running into other breeds Will their numbers  grow, and how might they evolve In this panel, the conference  will hear and debate not only broad perspectives on the terrain,  but will have been exposed to some real life specimens, and  caught glimpses of the future ecosystem.    Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education]   General Terms  Measurement, Documentation, Human Factors, Theory    Keywords  Learning Analytics; Educational Data Mining; Data Science;  Educational Data Scientist   1. INTRODUCTION  While the learning analytics and educational data mining research  communities are tackling the question of what data can tell us   about learners, relatively little attention has been paid, to date, to  the specific mindset, skillset and career trajectory of the people  who wield these tools. Within business and government, Data  Scientists are heralded as the new, scarce breed. A widely cited  report from McKinsey Global Institute identified a widening  talent gap in the workforce [1], while Harvard Business Review  [2] declared Data Scientist to be sexiest job in the 21st century!   The U.S. Department of Education contextualized this talent  vacuum within the educational ecosystem as follows:   Interdisciplinary teams of experts in educational data mining,  learning analytics, and visual analytics should collaborate to  design and implement research and evidence projects. Higher  education institutions should create new interdisciplinary  graduate programs to develop data scientists who embody  these same areas of expertise. [3]   Clearly, the LAK community has every interest in shaping this  mindset and skillset: as MOOCs, conventional courses, and the  Learning Analytics Summer Institutes1 begin to bear fruit,  graduates need careers, and research teams will be recruiting back  from this pool of people as they build industrial track records.    So, over to our panellists, who bring a wealth of academic and  hands-on experience to help move this debate forward.   2. WE NEED MORE EDUCATION DATA  SCIENTISTS   I should probably track the number of phone calls and emails I get  each month from companies that want to hire an education data  scientist (sometimes called an educational data miner, or learning  analytics engineer). I get a lot of emails. I try to figure out what  the company needs (sometimes they dont know), and recommend  a colleague, or post-doc, or student. I dont have enough  recommendations to go around. Ive had companies ask me if they                                                                       1  Learning Analytics Summer Institutes:   http://www.solaresearch.org/events/lasi    Permission to make digital or hard copies of part or all of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies  bear this notice and the full citation on the first page. Copyrights  for components of this work owned by others than ACM must be  honored. Abstracting with credit is permitted. To copy otherwise, to  republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK '13, April 08 - 12 2013, Leuven, Belgium  Copyright 2013 ACM 978-1-4503-1785-6/13/04...$15.00.   Simon Buckingham Shum   Knowledge Media Institute   The Open University  Walton Hall   Milton Keynes, MK7 6AA, UK   s.buckingham.shum  @gmail.com   Ryan S.J.d. Baker  Teachers College    Columbia University  525 W. 120th St., Box 118  New York, NY 10027, USA   baker2@exchange.tc.  columbia.edu   John T. Behrens  Center for Digital Data,    Analytics, & Adaptive Learning  Pearson   400 Center Ridge Drive  Austin, TX 78753   John.Behrens@pearson.com       can hire someone without experience, and then have me train that  person.    We need more graduate programs for education data scientists. A  couple of graduate programs are already out there that focus on  this: Carnegie Mellon University, with their new M.S. in Learning  Science and Engineering; Worcester Polytechnic Institute, with  their Ph.D. and M.S. in Learning Sciences and Technologies; and  there are a lot of academic labs that produce graduates trained in  this area even without an official program.   One of the tricky things for training a cadre of education data  scientists  in my opinion  is that data is not just data.  Different kinds of data have different characteristics and  affordances; different grain-sizes of analysis; different features  that are key to engineer; different algorithms that just tend to  work well. Less-specialized data science and data mining  graduate programs can be very good, and if they have good  education data scientists teaching in them, theyll naturally  support the development of this cadre. But where theres not  scale, we need resources to support teaching. No one researcher is  going to know about all the types of data an education data  scientist needs to know about (Im very much not a text miner,  myself  just to give one example). Its way too early for  standardized curricula, but sharing resources, creating textbooks,  and talking to each other will definitely help. Summer institutes  like the PSLC Summer School and the Learning Analytics  Summer Institute will help. MOOCs (and hopefully multiple  competing MOOCs) like the LAK13 MOOC on Learning  Analytics will help.  To the future: an education data scientist (or several) in every  company, well-trained, knowing what has already been tried and  is awesome, and ready to try new and more awesome things.  Bio: Ryan Shaun Joazeiro de Baker is the Julius and Rosa  Sachs Distinguished Lecturer at Teachers College, Columbia  University. He earned his Ph.D. in Human-Computer Interaction  from Carnegie Mellon Universitys School of Computer Science.  Dr. Baker was previously Assistant Professor of Psychology and  the Learning Sciences at Worcester Polytechnic Institute. He  previously served as the first Technical Director of the Pittsburgh  Science of Learning Center DataShop, the largest public  repository for data on the interaction between learners and  educational software. He researches student engagement, affect,  meta-cognitive behavior, and robust learning in educational  software, including in intelligent tutors, simulations, microworlds,  and serious games, and his lab developed the first automated  detectors of gaming the system, off-task behavior, and preparation  for future learning in these contexts. He is currently serving as the  founding President of the International Educational Data Mining  Society, and as Associate Editor of the Journal of Educational  Data Mining. http://www.columbia.edu/~rsb2162     3. HOW DO YOU BECOME AN  EDUCATIONAL DATA SCIENTIST   In 1999 I was finishing my undergraduate in Structural  Engineering. Whilst there were moments of interest such as using  finite element analysis to produce strength visualizations from  structural simulations, the subject largely left me cold. By 2001 I  returned to academia to do postgrad in multimedia and interactive  systems. A notable incident in this period was when the  programme leader distributed the transcript for the previous  semesters grades in a Microsoft Excel spreadsheet. Data were  anonymized by using student matriculation numbers. Few  students appeared to pay attention to his data, I did not. Not only   was I analyzing where I ranked in the class, I was predicting my  final grade based on previous performance and all by using  formula a formatting available in Excel. My curiosity also allowed  me to decode the student matriculation numbers. By formatting  the data and pasting it into our student webmail client the check  names feature conveniently reconciled matriculation number to  student name. Now not only could I see my own performance, I  could also now look my competition in the eye. I graduated with  distinction and with a university medal.   This example underlines a number of the qualities I believe are  important in becoming an educational data scientist. As noted by  Stephen Brobst in a talk at Teradata Universe Conference in 2012:     A data scientist  is someone who: wants to know what the  question should be; embodies a combination of curiosity, data  gathering skills, statistical and modelling expertise and strong  communication skills.  The working environment for a data  scientist should allow them to self-provision data, rather than  having to rely on what is formally supported in the  organisation, to enable them to be inquisitive and creative. [4]   We all have varying degrees of curiosity, inquisition and  creativity. The key in my evolution towards educational data  science has been the professional and personal opportunity (aka  pushing pixels at 1am) to explore ideas and concepts, doing this  allowing me to refine my skills in data collection and processing,  evolve recipes to allow others to taste the sweetness of data  science, and develop an understanding of my craft.    Bio:  Martin Hawksey is an advisor at the Centre for Educational  Technology and Interoperability Standards (CETIS), a national  advisory and innovation centre funded by JISC, supporting the  UK Higher and Post-16 Education sectors on educational  technology and standards. He is a master of Google Sheets and a  number of his templates are used internationally within education  and business for collecting and analyzing data. His most notable  example is the Twitter Archiving Google Spreadsheet (TAGS)  which archives Twitter search terms for analysis and  visualization. His current research is primarily around aggregating  and analyzing secondary data sources from Massive Open Online  Courses (MOOCs). He is a supporter of open educational  practices and his work can be followed on his blog, MASHe, at  http://mashe.hawksey.info    4. REMEMBER OUR PURPOSE  How do you define an educational data scientist  The answer  varies considerably by post and by practitioner.  The role is also  rapidly evolving as new software developments chase the  demands of the research community.  For me, data science is a  combination of statistics, computer science and information  design, but as my role connects with so many communities and  professions, good communication and collaboration skills are  essential.  Many definitions specify working with big data but I for one  dont work exclusively with big data  only when its necessary  and appropriate.  One important basis for working with big data is  pattern recognition in noisy data, which is a very visual  and, for  now, primarily human  skill.  As a statistician I am determined  that in manipulating and visualising data we remain honest and  realistic, hence shifting focus to smaller, more definite, data  sources can be both refreshing and an important re-grounding.   The educational data scientist must dive from Learning Analytics  (LA) into Educational Data Mining (EDM) and resurface:  exploring the real world, proposing meaningful measures,  modelling the data, visualising the output, sharing the technique     and automating the process.  Sharing new ideas and techniques  takes enormous confidence and as many education data scientists  are  like myself  not academics, promoting our work as  individuals does not always come naturally or easily.  Working  with academic researchers who are more comfortable on the front  line can be a relief as long as credit is shared.  Keeping track of new LA publications (both formal and informal)  is extremely important because an understanding of how to  measure learning must come before any exploratory analysis of  educational data.  New pedagogies and technologies can render  traditional analysis methods and conventional wisdom on learner  behaviour patterns obsolete, hence the educational data scientist  should always be scanning for new developments and considering  how they may impact on (and ideally improve) the learner  experience.   In the near future I think one focus of educational data science  will be on developing animated, multi-dimensional techniques for  mining and visualising data.  Continuing issues will be data  privacy and ethics, the danger of viewing learners as merely the  incidental constructs of their own data identity, and the perfidious  and ultimately ruinous double-standard that your data should be  shared but my data must be protected.  An exciting new frontier  will be developing progressively more meaningful measures of  learning as we engage with new data and are able to expand our  quantitative conception of success beyond the course, and into  individual learner aims for their career and life as a whole.    Bio: Naomi Jefferys academic background is in statistics and  computer science.  Shes a designer at heart and is working  towards another degree in design and innovation.  She has more  ideas than time and more crafts than space.  http://statisticiana.wordpress.com      5. BE A DATA ANALYST AND A  PHILOSOPHER/ SCIENTIST  The science and art of data analysis concerns the process of  learning from quantitative records of experience. By its very  nature it exists in relation to people. Tukey & Wilk [4].     Though a famous and successful statistician, Tukey wanted to  create a field that dealt with all data, even when it came in such  poor shape that it was not amenable to statistical analysis.  He  called it data analysis and created the field called Exploratory  Data Analysis.      My undergraduate degree was in Psychology and Philosophy.  I  thought if I knew the logic of how we know things (epistemology)  and understood the human lens through which all perception and  thought occurs (psychology) I would have the fundamental layers  of knowing from which to acquire more knowledge. After serving  as a social worker and studying special education, I sought my  Ph.D in Educational Psychology with a cognate called  Measurement, Statistics & Methodological Studies.  I would  approach it as applied epistemology: How do we learn from data   When I discovered Tukeys writings I knew I had found the right  place.  I conducted psychological studies on perception of  statistical graphics and wrote about the logical foundations of data  analysis.  When I wrote such a chapter called Data and Data  Analysis [6] people told me it was a silly title  data wasnt a  subject, its only a piece of the background to other sciences.  Philosophy is concerned with understanding meaning and the  application of logic.  The philosopher asks What do we mean by  data What do we mean by analysis If data are symbols that  point to elements in the world, what kind of logic do we need to  understand that linkage Like very good scientists, philosophers  question the obvious.  Such questioning may not be essential for  what you do today, but it may open the door to do new ways of  thinking you never imagined.      The successful learning analyst will avoid two common errors:  Failure to understand the context and failure to become intimately  familiar with the data.  The first error is caused by lack of  contextual knowledge. Studying the learning sciences, education,  and related disciplines will help.  The second is error is caused by  a substitution of complex statistical or computational models for  detailed mental models.  We only build computational models or  display to help our mental models.  Question the assumptions of  your work deeply. It is important that analysts understand their  work is about revelation or unveiling the reality of the world.   It is a special (at times prophetic) role in society and should be  taken very seriously.   Do not think of data science as a set of techniques but as a  collection of viewpoints (epistemic positions) and habits of mind.   To undertake good visualization we need to know the techniques  of data display, but also the psychology of perception, the  anthropology of semiotics, the mathematics of fluctuation and the  philosophy and art of aesthetic engagement. We will always need  good technical analysts, but we need them to be (or at least  understand) scientists, philosophers & artists as well.   Bio: John Behrens is Vice President and leader of the Center for  Digital Data, Analytics, and Adaptive Learning at Pearson.  He  brings cognitive, statistical, computational and philosophical  lenses to designing, deploying and analyzing data-intensive  learning systems including instructional, assessment, and game  environments.   He continues to write and speak about data and  data analysis and the implications of the digital revolution for  methodology. He is in love with data and the worlds they reflect.   Previously, John led product research and development in the  Cisco Networking Academies. Serving 10,000 schools in 160  countries, John oversaw the first large scale use of Evidence  Centered Design to drive the integrated use of curriculum,  simulations, and gaming for on-line instruction and assessment.   His work in Cisco certifications made simulation based  assessment standard practice in the IT certification exam industry.   Prior to Cisco, John was a tenured associate professor of  Psychology in Education at Arizona State University.  http://researchnetwork.pearson.com/digital-data-analytics-and- adaptive-learning      6.   WHAT ARE KEY COMPETENCY  DOMAINS FOR THE EDUCATION  DATA SCIENTIST   When I was responsible for designing and launching the first  Learning Sciences doctoral program in the world in 1992 at  Northwestern University with my colleagues in education,  psychology, and computer science, we put considerable thought  into how to frame the areas of scholarship and inquiry that our  students needed to master to make the advances we felt the  opportunities warranted.   What we developed at the time was a tripartite conceptualization  of the Learning Sciences: Cognition  constructing scientific  models of the structures and processes of learning and teaching by  which organized knowledge, skills and understanding are  acquired; Environments [now Context]  examining the social,  organizational and cultural dynamics of learning and teaching  situations, including schools and out-of-school settings, such as  homes, museums and corporations; and Architectures [now  Design]  building environments for learning and teaching,  incorporating multimedia, artificial intelligence, computer  networks and innovative curriculum and classroom activity  structures. Each of our students tended to focus on going deep in  one of these to make their contributions, but we insisted that they  learn enough about the others to team productively. We also  recognized that in principle and in practice, these distinctions  between Cognition, Context and Design were as much about  figure and ground than about hard and fast: embodied minds  interact and learn in contexts involving designed socio-technical  tools. And twenty years on, these distinctions are becoming even  more indissociable in an increasingly hyperconnected world.   These three domains of competencies were prescient framings.  Northwestern University revisited this scheme twenty years later  and decided after critical reflection to keep to it, since it had  served its graduates, faculty, and building the field of Learning  Sciences well. At Stanford, we launched our Learning Sciences  and Technology Design program in 2001. Today there are 40 or  so postgraduate degree programs around the world in the Learning  Sciences, and the 2nd edition of the Cambridge Handbook of the  Learning Sciences, first published in 2006, is in the works. There  is a growing International Society for the Learning Sciences  (ISLS.org), and two archival journals that are among the highest  impact journals in the education field: The Journal of the  Learning Sciences, and the International Journal of Computer- Supported Collaborative Learning.   What is the relevancy of this history for what an education data  scientist needs to know and be able to do  I argue that the  education data scientist, too, needs to have an understanding of  the cognitive, contextual and design aspects of the transactions  that generate educational data and the interdisciplinary sciences  that will contribute to an understanding of it  if he or she is to ask  generative research questions that will advance the sciences and  practices of education and learning.   Whats new with education data science A great deal: The  foundational roles of developments in statistical computing for  data analytics; the centrality of state-of-the-art interactive data  visualization for exploratory data analysis; the vital roles of  machine learning; more powerful and distributed computing  architectures that enable sense-making and prediction with big  data; deeper understanding of the links between types of research  questions, education data types, and effective LA and EDM  methodologies to suite the questions and data; greater need than   ever for critical questioning of assumptions at every level, and for  balancing learning data openness and transparency with ethical  considerations. The technical and mathematical wizardry  underlying these advances still needs theory and science from the  Cognition, Context and Design domains. For example, the social  and environmental contexts of technology-mediated learning need  to be sensed and become part of our multimodal learning  analytics agenda, as they are central to the environments which  learners are experiencing when educational data is collected.  In  short, an education data scientist will have to be an active listener  and open collaborator because no one will know and be able to do  everything that will produce the highest quality work.    We should unite in a clarion call for universities, governments,  philanthropists, and industry to all contribute to accelerating the  field of education data science, and work collectively to attract the  best minds of our generation to tackling problems and providing  compelling solutions that would enable global personalized  learning for all.   Bio. At Stanford University Roy Pea serves as David Jacks  Professor of Education and the Learning Sciences (and, by  courtesy, Computer Science), co-founder and Director of the H- STAR Institute (Human Sciences and Technologies Advanced  Research), and founder and Director of the PhD program in  Learning Sciences and Technology Design. He was on the  founding board of ISLS and served as President, 2004-2005. His  work in the learning sciences focuses on advancing theories,  findings, tools, practices and interdisciplinary field-building for  technology-enhanced learning of complex domains. He co-leads  the LIFE Center (http://life-slc.org) whose studies seek to  inform better bridging of the sciences of learning for informal and  formal environments. He has been long been inspired by the  Engelbartian quest for augmenting human intellect and  performance by co-evolving human-computer systems, and  learning analytics seems poised as a breakthrough area toward  that long-term vision. Recently, he joined with Stanford's Vice  Provost for Online Learning, John Mitchell, to serve as faculty co- director of an interdisciplinary student-initiated Lytics Lab,  devoted to advancing learning analytics science, theory and tools,  with special attention to improving MOOCs.  http://www.stanford.edu/~roypea    6. REFERENCES  1. Big Data: The Next Frontier for Innovation, Competition,   and Productivity. McKinsey Global Institute.  http://bit.ly/McKinseyBigDataReport     2. Data Scientist: The Sexiest Job of the 21st Century. Harvard  Business Review Magazine: http://hbr.org/2012/10/data- scientist-the-sexiest-job-of-the-21st-century/ar/1   3. Expanding Evidence Approaches for Learning in a Digital  World. Office of Educ. Technology, U.S. Dept. Education:  http://www.ed.gov/edblogs/technology/evidence-framework   4. Cooper, C.: Analytics and Big Data - Reflections from the  Teradata Universe Conference 2012. Blog post (Apr. 27,  2012): http://bit.ly/CooperTeradataBlog    5. Tukey, J. W. and Wilk, M. B. (1966). Data analysis and  statistics: An expository overview. AFIPS Conf. Proc. 1966  Fall Joint Comp. Conf. 29, pp. 695709   6. Behrens, J. T., & Smith, M. L. (1996).  Data and data  analysis. In D. C. Berliner, & R. C. Calfee (Eds.), Handbook  of Educational Psychology, pp. 945-989.  New York:  MacMillan.     "}
{"index":{"_id":"42"}}
{"datatype":"inproceedings","key":"BuckinghamShum:2013:DSI:2460296.2460357","author":"Buckingham Shum, Simon and de Laat, Maarten and De Liddo, Anna and Ferguson, Rebecca and Kirschner, Paul and Ravenscroft, Andrew and S'andor, 'Agnes and Whitelock, Denise","title":"DCLA13: 1st International Workshop on Discourse-Centric Learning Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"282--282","numpages":"1","url":"http://doi.acm.org/10.1145/2460296.2460357","doi":"10.1145/2460296.2460357","acmid":"2460357","publisher":"ACM","address":"New York, NY, USA","keywords":"argumentation, deliberation, dialogue, discourse, learning analytics, visualization","abstract":"This workshop anticipates that an important class of learning analytic will emerge at the intersection of research into learning dynamics, online discussion platforms, and computational linguistics. Written discourse is arguably the primary class of data that can give us insights into deeper learning and higher order qualities such as critical thinking, argumentation, mastery of complex ideas, empathy, collaboration and interpersonal skills. Moreover, the ability to write in a scholarly manner is a core competence, often taking the form of discourse with oneself and the literature. Computational linguistics research has developed a rich array of tools for machine interpretation of human discourse, but work to develop these tools in the context of learning is at a relatively early stage. Moreover, there is a significant difference between designing tools to assist researchers in discourse analysis, and their deployment on platforms to provide meaningful analytics for the learners and educators who are conducting that discourse. This workshop aims to catalyse ideas and build community connections among those who want to shape this field. ","pdf":"DCLA13: 1st International Workshop on   Discourse-Centric Learning Analytics   Simon Buckingham Shum1, Maarten de Laat2, Anna De Liddo1, Rebecca Ferguson1,   Paul Kirschner2, Andrew Ravenscroft3, gnes Sndor4, Denise Whitelock1     1 The Open University   Knowledge Media Institute &   Institute for Educational Technology   Walton Hall, Milton Keynes, MK7 6AA, UK   {firstname.lastname}@open.ac.uk   2 Open Universiteit NL  LooK & Centre for Learning Sciences and Technologies    6401 DL Heerlen  The Netherlands   {firstname.lastname}@ou.nl     3 University of East London  Cass School of Education and Communities  Stratford Campus, Romford Road, Stratford   London E15 4LZ, UK   a.ravenscroft@uel.ac.uk   4 Parsing & Semantics Group  Xerox Research Centre Europe   6 chemin Maupertuis, F-38240 Meylan  France   agnes.sandor@xrce.xerox.com      ABSTRACT  This workshop anticipates that an important class of learning  analytic will emerge at the intersection of research into learning  dynamics, online discussion platforms, and computational  linguistics. Written discourse is arguably the primary class of data  that can give us insights into deeper learning and higher order  qualities such as critical thinking, argumentation, mastery of  complex ideas, empathy, collaboration and interpersonal skills.  Moreover, the ability to write in a scholarly manner is a core  competence, often taking the form of discourse with oneself and  the literature. Computational linguistics research has developed a  rich array of tools for machine interpretation of human discourse,  but work to develop these tools in the context of learning is at a  relatively early stage. Moreover, there is a significant difference  between designing tools to assist researchers in discourse analysis,  and their deployment on platforms to provide meaningful  analytics for the learners and educators who are conducting that  discourse. This workshop aims to catalyse ideas and build  community connections among those who want to shape this field.   Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education     General Terms  Design   Keywords  Learning Analytics, Discourse, Dialogue, Deliberation,  Argumentation, Visualization   WORKSHOP CONCEPT  Written discourse is a major class of data that learners produce in  online environments, arguably the primary class of data that can  give us insights into deeper learning and higher order qualities  such as critical thinking, argumentation, mastery of complex  ideas, empathy, collaboration and interpersonal skills. It is central  to the collaborative and social learning that takes place online and  there is a correspondingly significant literature on discourse  analysis for online learning/CSCL. Computer-supported discourse  ranges from unconstrained forms of interaction to more structured  forms, where structuring refers to the constraints that the user  interface places on how one contributes (eg flat commenting,  threaded forums, blogging, argument mapping...).    Computational linguistics research in academia and business (eg.  IBMs Watson) has developed a rich array of automated tools for  machine interpretation of human discourse, but work to develop  these tools in the context of learning is at a relatively early stage.  Moreover, there is a significant difference between the use of such  tools to assist researchers in discourse analysis, and their  deployment on platforms in order to provide meaningful analytics  for learners and educators.  We argue, therefore, that an important class of learning analytic  will emerge at the intersection of research into learning dynamics,  deliberation platforms, and computational linguistics. What will  make these learning analytics, as opposed to research that sits in  any of the above categories, will be their use to generate  information displays that help learners and/or educators to  understand where significant discourse patterns are happening and  that support interventions to improve discourse for learning.   It is hoped that this workshop will provide the opportunity to frge  productive new connections between researchers from the above  fields, in order to ensure that Learning Analytics research is  informed by the best possible minds.    http://www.solaresearch.org/events/lak/lak13/dcla13         Copyright is held by the author/owner(s).    LAK '13, Apr 08-12 2013, Leuven, Belgium  ACM 978-1-4503-1785-6/13/04.   282  mailto:a.ravenscroft@uel.ac.uk mailto:agnes.sandor@xrce.xerox.com http://www.solaresearch.org/events/lak/lak13/dcla13     "}
{"index":{"_id":"43"}}
{"datatype":"inproceedings","key":"Giannakos:2013:AVL:2460296.2460358","author":"Giannakos, Michail N. and Chorianopoulos, Konstantinos and Ronchetti, Marco and Szegedi, Peter and Teasley, Stephanie D.","title":"Analytics on Video-based Learning","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"283--284","numpages":"2","url":"http://doi.acm.org/10.1145/2460296.2460358","doi":"10.1145/2460296.2460358","acmid":"2460358","publisher":"ACM","address":"New York, NY, USA","keywords":"MOOCs, interaction design, learning analytics, video based learning","abstract":"The International Workshop on Analytics on Video-based Learning (WAVe2013) aims to connect research efforts on Video-based Learning with Learning Analytics to create visionary ideas and foster synergies between the two fields. The main objective of WAVe is to build a research community around the topical area of Analytics on video-based learning. In particular, WAVe aims to develop a critical discussion about the next generation of analytics employed on video learning tools, the form of these analytics and the way they can be analyzed in order to help us to better understand and improve the value of video-based learning. WAVe is based on the rationale that combining and analyzing learners' interactions with other available data obtained from learners, new avenues for research on video-based learning have emerged. ","pdf":"Analytics on Video-Based Learning  Michail N. Giannakos  1 *, Konstantinos Chorianopoulos  2 , Marco Ronchetti  3 , Peter Szegedi  4 ,   Stephanie D. Teasley 5    1 Dept. of Computer and Information Science, Norwegian University of Science and Technology, Trondheim, Norway   2 Department of Informatics, Ionian University, Corfu, Greece   3 Dipartimento di Ingegneria e Scienza dellInformazione, Universit di Trento, Povo di Trento, Italy   4 Trans European Research and Education Network Association (TERENA), Amsterdam, The Netherlands   5 School of Information, University of Michigan, Ann Arbor, MI, USA   michail.giannakos@idi.ntnu.no      ABSTRACT  The International Workshop on Analytics on Video-based   Learning (WAVe2013) aims to connect research efforts on Video-  based Learning with Learning Analytics to create visionary ideas   and foster synergies between the two fields. The main objective of   WAVe is to build a research community around the topical area of   Analytics on video-based learning. In particular, WAVe aims to   develop a critical discussion about the next generation of analytics   employed on video learning tools, the form of these analytics and   the way they can be analyzed in order to help us to better   understand and improve the value of video-based learning. WAVe   is based on the rationale that combining and analyzing learners'   interactions with other available data obtained from learners, new   avenues for research on video-based learning have emerged.   Categories and Subject Descriptors  K.3.1 [Computer Uses in Education] Computer-assisted   instruction (CAI), Distance learning; J.1 [Administrative Data   Processing] Education   General Terms  Measurement, Design, Experimentation, Human Factors,   Keywords  Video Based Learning, MOOCs, Learning Analytics, Interaction   Design.   1. BACKGROUND OF THE WORKSHOP  With the widespread adoption of video-based learning systems   such as Khan Academy and edX, new research in the area of   Learning Analytics has emerged. Even new for-profit companies,   such as Coursera and Udacity, have started offering forms of   instruction that are primarily video-based. To date, universities   across the globe (Stanford, Oxford, MIT and some 800 other   schools) offer video lectures on topics from Algebra to Zoology.   The use of video for learning has become widely employed in the   past years [3]. Many universities and digital libraries have   incorporated video into their instructional materials. Massive   Online Open Courses (MOOCs) are becoming an increasingly   important part of education. For instance, students access   academic content via digital libraries, discuss with tutors by email   and attend courses from their home. In order to support video   learning, various technological tools have been developed. For   example, Matterhorn and Centra are just few of them. These tools   provide an easy way for a learner who has missed a lecture to   catch up, but also enable other, especially slow learners, to review   difficult concepts.   Many instructors in higher education are implementing video   lectures in a variety of ways, such as broadcasting lectures in real   time, augmenting recordings of in-class lectures with face-to-face   meetings for review purposes, and delivering lecture recordings   before class to flip the classroom and provide hands-on   activities during class time. Other uses include showing videos   that demonstrate course topics and providing supplementary video   learning materials for self-study.   Millions of learners enjoy video streaming from different   platforms (e.g., YouTube) on a diverse number of terminals (TV,   desktop, smart phone, tablets) and create billions of simple   interactions. This amount of learning activity might be converted   via analytics into useful information [1, 5] for the benefit of all   video learners. As the number of learners' watching videos on   Web-based systems increases, more and more interactions have   the potential to be gathered. Capturing, sharing and analyzing   these interactions (datasets) can clearly provide scholars and   educators with valuable information [7]. In addition, the   combination of learner profiles with content metadata provide   opportunities for adding value to learning analytics obtained from   video based learning.   To explore the future of video-based technologies for teaching   and learning, we aim to build a research community around this   topical area, to brainstorm about what the next generation of   video-based learning tools might look like, what kind of data can   be collected, and how these data can help us to better understand   and improve the value of video-based learning.   Existing empirical research [e.g. 2, 3, 4, 6] has begun to identify   the educational advantages and disadvantages of video-based   learning. However, there still remain many essential unexplored   aspects of video-based learning and the related challenges and   opportunities; such as, how to use all the data obtained from the   learner, how to combine data from different sources, and so on.   WAVe aims to support this research endeavor through an   analytics approach to video-based learning. In particular, the   objective of this workshop is to bring together researchers,     Copyright is held by the author/owner(s).   LAK '13, Apr 08-12 2013, Leuven, Belgium   ACM 978-1-4503-1785-6/13/04.      * This work was carried out during the tenure of an ERCIM  Alain   Bensoussan  Fellowship programme. The research leading to these results   has received funding from the European Union Seventh Framework   Programme (FP7/2007-2013) under grant agreement no 246016.   283    designers, teachers, practitioners and policy makers who are   interested in how to do research on the use of any form of video   technology for supporting learning. This workshop will provide   an opportunity for these individuals to come together, discuss   current and future research directions, and build a community of   people interested in this area.   By taking into account learners' interactions and many other   datasuch as students' demographic characteristics of gender,   ethnicity, English-language skills, prior background knowledge,   their success rate in each section, their emotional states, the speed   at which they submit their answers, which video lectures seemed   to help which students best in which sections, etc. new avenues   for research in the intersection of video-based learning and   analytics are now possible.   2. WORKSHOP OBJECTIVES  The workshop will be an interactive, engaging experience that   will motivate participants to get involved and engage in fruitful   discussions on the topic of Video-Based Learning and the   potential benefits of Analytics. To do so, it will combine several   activities. First, highly recognized keynote speakers will open the   workshop. Then the workshop organizers will give the   participants the opportunity to be engaged into creative and   motivating discussions about the key issues related to analytics on   video-based learning.   One of our main objectives is to bring together researchers who   are interested on Learning Analytics and their application on   video-based learning. Specifically, WAVe aims to provide an   environment where participants will get opportunities to: develop   their research skills; increase their knowledge base; collaborate   with others in their own and complementary research areas; and   discuss their own work. In particular, guiding questions and   themes include:    What might next generation of analytics enhanced video   learning tools look like    What kind of data can be collected from video-based   learning tools     How these data can help us to better understand and   improve the value of video-based learning   3. ABOUT THE FACILITATORS  Michail N. Giannakos is an ERCIM/Marie Curie Fellow in the   Department of Computer and Information Science at Norwegian   University of Science and Technology (NTNU) and a Visiting   Richard T. Cheng Fellow in the Center for Real-Time Computing,   Virginia, USA. Giannakos is interested in how people learn in the   presence of technology and each other. Since 2010, he is a   member of the IFIP Working Group 3.1 on Informatics and ICT   in Secondary Education.   Konstantinos Chorianopoulos is Lecturer in the Department of   Informatics at the Ionian University, Corfu, Greece. He has been a   post-doctoral Marie Curie Fellow from 2006 to 2011. In 2002, he   founded UITV.INFO, which is a newsletter (currently a   discussion-group) and web portal for interactive television   research resources (papers, theses), news and events. He is serving   on the steering committee of the European Interactive TV   organization and on the editorial boards of the following journals:   Computers in Entertainment (ACM), Entertainment Computing   (Elsevier), Journal of Virtual Reality and Broadcasting.   Pter Szegedi is Project Development Officer of Trans European   Research and Education Network Association (TERENA). He is   also secretary of TF-Storage, TF-Media, TF-NOC, and the GLIF   Technical Working Group as well as coordinator of the End-to-  End Network Provisioning issues and the Video & Web   Conferencing activities and looks after the NRENum.net service.   He participated in the EC funded projects such as FP7-  FEDERICA (leader of NA2 & JRA2 activities) & IST-MUPBED.   Marco Ronchetti is a CS professor at the Department of   Information Engineering and Computer Science at the University   of Trento, Italy. He is author of more than hundred peer-reviewed   research articles in several international journals and conferences,   and over the last ten years his interests have focused in the area of   Educational Technology, especially in the area of video-supported   learning. He has been a director of the Master in Technologies for   System Integration and e-Government.   Stephanie D. Teasley is a research professor at the School of   Information and the director of the USE Lab at the University   Library, whose mission is to investigate how instructional   technologies and digital media are used to innovate teaching,   learning, and collaboration. Teasleys research utilizes Learning   Analytics to categorize and simplify the vast amount of data on   student engagement and learning available in the campus   Learning Management System.  She is on the Executive Board of   the Society for Learning Analytics Research (SoLAR).   4. ACKNOWLEDGMENTS  We would like to thank Dr. George Siemens and Dr. David Geerts   for accepting our invitation to give keynote presentations, the   workshop Program Committee members for accepting our   invitation and the workshop chairs for their constructive   comments and their helpful assistance.   5. REFERENCES  [1] Chorianopoulos, K., Leftheriotis, I. & Gkonela, C. 2011.   SocialSkip: pragmatic understanding within web video. In   Proceedings of EuroITV11, 2528.   [2] Giannakos, M. N. & Vlamos, P. 2013, Using webcasts in   education: Evaluation of its effectiveness. British Journal of   Educational Technology. doi:10.1111/j.1467-  8535.2012.01309.x   [3] Giannakos, M. N. 2013. Exploring the research on video   learning: A review of the literature. British Journal of   Educational Technology.   [4] Lonn, S. & Teasley, S. D. 2009. Podcasting in higher   education: What are the implications for teaching and   learning The Internet and Higher Education, 12, 88-92.   [5] Mertens, R., Farzan, R., & Brusilovsky, P. 2006. Social   navigation in web lectures. In Proceedings of HYPERTEXT   '06, 41-44.   [6] Ronchetti, M. 2010. Using video lectures to make teaching   more interactive, International Journal of Emerging   Technologies in Learning (iJET), 5 (2), 45-48.   [7] Verbert, K., et al., 2011. Dataset-driven Research for   Improving Recommender Systems for Learning. In   Proceedings of LAK '11, 44-53.   284      "}
{"index":{"_id":"44"}}
{"datatype":"inproceedings","key":"Vatrapu:2013:SIW:2460296.2460360","author":"Vatrapu, Ravi and Reimann, Peter and Halb, Wolfgang and Bull, Susan","title":"Second International Workshop on Teaching Analytics","booktitle":"Proceedings of the Third International Conference on Learning Analytics and Knowledge","series":"LAK '13","year":"2013","isbn":"978-1-4503-1785-6","location":"Leuven, Belgium","pages":"287--289","numpages":"3","url":"http://doi.acm.org/10.1145/2460296.2460360","doi":"10.1145/2460296.2460360","acmid":"2460360","publisher":"ACM","address":"New York, NY, USA","keywords":"affordances, computer supported collaborative learning (CSCL), learning analytics, open learner models representational guidance, teaching analytics","abstract":"Teaching Analytics is conceived as a subfield of learning analytics that focuses on the design, development, evaluation, and education of visual analytics methods and tools for teachers in primary, secondary, and tertiary educational settings. The Second International Workshop on Teaching Analytics (IWTA) 2013 seeks to bring together researchers and practitioners in the fields of education, learning sciences, learning analytics, and visual analytics to investigate the design, development, use, evaluation, and impact of visual analytical methods and tools for teachers' dynamic diagnostic decision-making in real-world settings.","pdf":"Second International Workshop on Teaching Analytics     Ravi Vatrapu1, 2, Peter Reimann3, Wolfgang Halb4, and Susan Bull5  1Computational Social Science Laboratory (CSSL), ITM, Copenhagen Business School, Denmark   2Norwegian School of Information Technology (NITH), Norway  3MTO Psychologische Forschung und Beratung, Germany   4 Joanneum Research, Austria  5Electronic, Electrical and Computer Engineering, University of Birmingham, United Kingdom     vatrapu@cbs.dk, preimann.undefined@gmail.com, wolfgang.halb@joanneum.at, s.bull@bham.ac.uk        ABSTRACT  Teaching Analytics is conceived as a subfield of learning  analytics that focuses on the design, development,  evaluation, and education of visual analytics methods and  tools for teachers in primary, secondary, and tertiary  educational settings. The Second International Workshop  on Teaching Analytics (IWTA) 2013 seeks to bring  together researchers and practitioners in the fields of  education, learning sciences, learning analytics, and visual  analytics to investigate the design, development, use,  evaluation, and impact of visual analytical methods and  tools for teachers dynamic diagnostic decision-making in  real-world settings.   ACM Classification Keywords  H.5.3 Group and Organization Interfaces: Theory and  models, Asynchronous interaction Collaborative computing,  Evaluation/methodology; H.1.2 User/Machine Systems:  Software Psychology.   Author Keywords  Learning analytics, teaching analytics, computer supported  collaborative learning (CSCL), open learner models  representational guidance, affordances   GENERAL TERMS  Design, Human Factors, Theory      INTRODUCTION  The core problem that this workshop series on Teaching  Analytics addresses is that in comparison with most other  professionals that work in dynamically changing  environments, presently teachers often do not get the  information they need for decision making in a timely  fashion and in a meaningful and actionable format.  Teaching Analytics is conceived as a subfield of learning  analytics that focuses on the design, development,  evaluation, and education of visual analytics methods and  tools for teachers in primary, secondary, and tertiary  educational settings.  Teachers professional practices with  visual analytics methods and tools are a central concern of  teaching analytics. Teaching analytics methods and tools  aim to develop innovative solutions to assist and augment  teachers dynamic diagnostic decision-making in the  classrooms of the 21st century [1-3]. An example usage  scenario (but not limited to) is the use of teaching analytics  methods and tools in high-performance classrooms that are  characterized by 1:1 computing, high cognitive density, and  big data.    Building on the NEXT-TELL project and the first  international workshop on teaching analytics (TAPTA- 2012) held at EC-TEL 2012 (http://www.next tell.eu/tapta/), the current workshop (IWTA-2013)  explores methods and tools to support teachers  professional vision in classrooms. This workshops  ambitious objective is to jumpstart a new learning analytics  research stream on teaching analytics by bringing together  learning scientists in different sub-fields such as CSCL,  ITS, EDM, researchers in Visual Analytics, and data  scientists working with Big Data in public institutions  and private enterprises. In addition to researchers, we are  also targeting teachers at primary, secondary, and tertiary  levels of education to be involved as co-designers and  discussants.   As mentioned earlier, the first workshop on teaching  analytics (TAPTA-2012, http://www.next tell.eu/tapta/), was held at EC-TEL 2012. The workshop   Permission to make digital or hard copies of part or all of this work  for personal or classroom use is granted without fee provided that  copies are not made or distributed for profit or commercial advantage  and that copies bear this notice and the full citation on the first page.  Copyrights for components of this work owned by others than ACM  must be honored.   Abstracting with credit is permitted. To copy otherwise, to republish,  to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK '13, April 08 - 12 2013, Leuven, Belgium   Copyright 2013 ACM 978-1-4503-1785-6/13/04$15.00.   287    participants included researchers as well as teachers. Topics  presented and discussed at the first workshop included  semiology for teaching analytics, multi-domain perspective  on data collection and analysis, ontological engineering and  scripting applications, open learner models, browser-based  learning management system extensions, and investigation  tools for teacher training. Teachers from the Strmstad  Gymnasium, Sweden were invited to present their  observations and reflections on the pedagogical practices in  their 1:1 laptop school. Workshop proceedings are available  at http://ceur-ws.org/Vol-894/.   The key lessons learnt from the first workshop were the  need to further focus the workshop on teachers current  pedagogical and analytical practices, expand the scope to  include real-time teaching scenarios in the actual classroom,  and explicitly emphasize the use of visual analytics for  technology enhanced formative assessment. Based on the  above, we invite researchers and teachers to participate in  the IWTA-2013. Workshop submissions can be in the form  of position papers, tool demonstrations, conceptual  sketches, and/or thought experiments.    Please note workshop submissions should be concerned  with teachers use of visual analytics methods and tools in  some shape or form. No requirements are placed on  contexts such as high-performance classrooms or big data  volumes or real-time decision-making.    TOPICS  IWTA-2013 workshop topics include but are not limited to  the following:    Historical and contemporary practices in notations,  representations, and  visualizations of classroom  activities and student learning    Theories and methods for designing and evaluating  new notations, representations, and visualizations  for teaching analytics    Cognitive dimensions of notations for learning and  teaching purposes    Visual analytics for activity tracking   Visual analytics for knowledge tracking   Visual analytics for formative vs. summative   evaluations   Visual analytics for technology enhanced   formative assessment   Engendering  teachers professional vision in high-  performance classrooms   Synthesis of visual analytics oriented theory and   practice in the Learning Sciences, e-Learning,  Computer Supported Collaborative Learning  (CSCL), Intelligent Tutoring Systems (ITS),  Educational Data Mining, and Learning Analytics.    Technical architectures and technological  infrastructure for teaching analytics    Linked Data for teaching analytics   Instrumentation of classrooms and ecosystems for   learning and teaching   Critical perspectives on teaching analytics   WORKSHOP FORMAT   According to Kensing and Madsen (1991, p. 157),    A Future Workshop is divided into three phases:  the Critique, the Fantasy, and the Implementation  phase. Essentially the Critique phase is designed to  draw out specific issues about current work  practice; the Fantasy phase allows participants the  freedom to imagine what if the workplace could  be different; and the Implementation phase focuses  on what resources would be needed to make  realistic changes. These phases are surrounded by  preparation and follow-up periods.   We plan to use 75 minutes to jump start the Critique and  Fantasy phases of the Future Workshop and then the  interactive sessions #1 and #2 will be used to structure the  workshop contributions on all three phases of the future  workshop.   9.00  09:15 Introductions   9.15  10:30 Future Workshop on Teaching Analytics  (Critique and Fantasy Phases)   10.30  11.00  Coffee break   10.30  12.00  Interactive Session #1    (Critique, Fantasy, and Implementation Phases)    Position papers, tool demonstrations, conceptual  sketches, or thought experiments.    12.00  13.00 Lunch break   13.00  14.30  Interactive Session #2    (Critique, Fantasy, and Implementation Phases)     Position papers, tool demonstrations,  conceptual sketches, or thought experiments.   14:30-15:00: Coffee break   15:00-16:00: Plenary Discussion Session   16:00-16:30: Conclusion and Next Steps   19:00-22:00: Workshop Group Dinner   SELECTION PROCESS  4-6 page position papers/1-2 page tool demonstration/1- page conceptual sketches/1-paragraph thought experiments  were sought through a Call for Papers that was widely  distributed.    WORKSHOP PROCEEDINGS   The proceedings are published online in the CEUR series of  workshop proceedings. If there is enough interest, we will  pursue a journal special issue.    288    SHORT PROFILES OF WORKSHOP CHAIRS  Ravi Vatrapu is a professor of human computer interaction  at the Department of IT Management of the Copenhagen  Business School, adjunct professor of applied computing at  the Norwegian School of Information Technology, and  director of the Computational Social Science Laboratory  (CSSL). He holds a Doctor of Philosophy (PhD) degree in  Communication and Information Sciences from the  University of Hawaii at Manoa, a Master of Science (M.Sc)  in Computer Science and Applications from Virginia Tech,  and a Bachelor of Technology in Computer Science and  Systems Engineering from Andhra University. Vatrapus  basic research program is to conduct theory-based empirical  studies of socio-technical affordances and develop an  empirically-informed theory of technological  intersubjectivity. His applied research areas are Social  Media Management & Technology enhanced Learning. His  current research projects are on social business, teaching  analytics, and comparative informatics.   Peter Reimanns is a professor of education at the  University of Sydney. His research areas has been cognitive  learning research with a focus on educational computing,  multimedia-based and knowledge-based learning  environments, e-learning, and the development of  evaluation and assessment methods for the effectiveness of  computer-based technologies. Current research activities  comprise among other issues the analysis of individual and  group problem solving/learning processes and possible  support by means of ICT, and analysis of the use of mobile  IT in informal learning settings (outdoors, in museums,  etc.). Currently, he spends his time half time between  Australia, working in the Faculty of Education and as senior  researcher in the CoCo Research Centre, and the other half  time in Europe as Scientific Coordinator of Next-Tell, a  large research project funded by the European Commission  in the area of educational technology.                                          Wolfgang Halb finished 2005 his bachelor study of   Softwareentwicklung und Wissensmanagement   and passed his study  Softwareentwicklung-Wirtschaft   2007 with distinction at the University of Technology in  Graz. After numerous internships at home and abroad he  works since 2007 at the Institute of Information Systems as  a scientific assistant and started working on his PhD. His  research interests are in Linked Data and he leads the  activity capture and tracking work in the NEXt-TELL EU  project  Susan Bull is a Senior Lecturer at Electronic, Electrical and  Computer Engineering of the University of Birmingham,  UK. Her research interests are in Open Learner Models,  Artificial Intelligence in Education, Learner Reflection and  Metacognition, Misconceptions   ACKNOWLEDGEMENTS  This work is partially supported by the NEXT-TELL - Next  Generation Teaching, Education and Learning for Life  integrated project co-funded by the European Union under  the ICT theme of the 7th Framework Programme for R&D  (FP7). This document does not represent the opinion of the  EC and the EC is not responsible for any use that might be  made of its content.   REFERENCES  Vatrapu, R., Reimann, P. and Hussain, A. Towards Teaching  Analytics: Repertory Grids for Formative Assessment. In  Proc. International Conference of the Learning Sciences  (ICLS) 2012 (2012).  Vatrapu, R., Tanveer, U. and Hussain, A. Towards teaching  analytics: communication and negotiation tool (CoNeTo). In  Proc. Proceedings of the 7th Nordic Conference on Human- Computer Interaction: Making Sense Through Design, ACM  (2012), 775-776.  Vatrapu, R., Teplovs, C., Fujita, N. and Bull, S. Towards  Visual Analytics for Teachers' Dynamic Diagnostic  Pedagogical Decision-Making. Paper presented at the 1st  International Conference on Learning Analytics &  Knowledge (LAK 2011), Banff, Canada. (2011).         289      "}
