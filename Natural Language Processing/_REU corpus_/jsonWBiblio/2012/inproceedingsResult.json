{"index":{"_id":"1"}}
{"datatype":"inproceedings","key":"Borner:2012:VAS:2330601.2330604","author":"Borner, Katy","title":"Visual Analytics in Support of Education","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"2--3","numpages":"2","url":"http://doi.acm.org/10.1145/2330601.2330604","doi":"10.1145/2330601.2330604","acmid":"2330604","publisher":"ACM","address":"New York, NY, USA","keywords":"data mining, education, information visualization, macroscopes, visual analytics","Abstract":"The amount of data about us and our world is increasing rapidly, and the capability to analyze large data sets---so-called big data---becomes a key basis of competition, underpinning new waves of productivity growth and innovation. The big data phenomenon is fueled by cheap sensors and high-throughput simulation models, the increasing volume and detail of information captured by enterprises, the rise of multimedia, social media, and the Internet. It exists from social media to cell biology offering unparalleled opportunities to document the inner workings of many complex systems [1]. Research by MGI and McKinsey's Business Technology Office argues that there will be a shortage of talent necessary for organizations to take advantage of big data. By 2018, the United States alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions [2]. In everyday life, people deal with large amounts of data regularly: online search engines provide access to millions of web sites almost instantly; consumer sites offer literally thousands of purchase options seamlessly; and social media sites let you create and benefit from extensive social networks. In bestselling books like Freakonomics, Super Crunchers and The Numerati, authors illuminate how more and more decisions in health care, politics, education, and other sectors utilize big data and data analysis [3]. The texts highlight the growing need for specialists and every-day citizens to be able to understand and interpret data. Whether it is a table of nutritional information, a graph of stock prices, or a chart comparing health care plans, the skills of understanding and interpreting data are necessary to navigate successfully through daily life. This talk starts with a review of visual analytics projects that aim to increase our understanding of how people learn, increase the efficacy of learning environments, or support decision making in education [4]. The second part of the talk provides a theoretical framework for the design of effective data analysis workflows and insightful visualizations. It also introduces plug-and-play macroscope tools [5], see also http://cishell.org, that were designed for different research communities and are used by more than 120,000 users from 40+ countries to design and benefit from visualizations of complex data. The talk concludes with a discussion of challenges that arise when visual analytics tools are introduced to classrooms and informal science education","pdf":"Visual Analytics in Support of Education  Katy Brner   Cyberinfrastructure for Network Science Center  School of Library and Information Science   Indiana University  1320 E. 10th Street, Bloomington   IN 47405  USA   +1 812-855-3256   katy@indiana.edu      ABSTRACT  The amount of data about us and our world is increasing rapidly,   and the capability to analyze large data setsso-called big   databecomes a key basis of competition, underpinning new   waves of productivity growth and innovation. The big data   phenomenon is fueled by cheap sensors and high-throughput   simulation models, the increasing volume and detail of   information captured by enterprises, the rise of multimedia,   social media, and the Internet. It exists from social media to cell   biology offering unparalleled opportunities to document the inner   workings of many complex systems [1].  Research by MGI and   McKinsey's Business Technology Office argues that there will be   a shortage of talent necessary for organizations to take advantage   of big data. By 2018, the United States alone could face a   shortage of 140,000 to 190,000 people with deep analytical skills   as well as 1.5 million managers and analysts with the know-how   to use the analysis of big data to make effective decisions [2].    In everyday life, people deal with large amounts of data   regularly: online search engines provide access to millions of   web sites almost instantly; consumer sites offer literally   thousands of purchase options seamlessly; and social media sites   let you create and benefit from extensive social networks.      In bestselling books like Freakonomics, Super Crunchers and   The Numerati, authors illuminate how more and more decisions   in health care, politics, education, and other sectors utilize big   data and data analysis [3]. The texts highlight the growing need   for specialists and every-day citizens to be able to understand   and interpret data. Whether it is a table of nutritional   information, a graph of stock prices, or a chart comparing health   care plans, the skills of understanding and interpreting data are   necessary to navigate successfully through daily life.   This talk starts with a review of visual analytics projects that aim   to increase our understanding of how people learn, increase the   efficacy of learning environments, or support decision making in   education [4]. The second part of the talk provides a theoretical   framework for the design of effective data analysis workflows   and insightful visualizations. It also introduces plug-and-play   macroscope tools [5], see also http://cishell.org, that were   designed for different research communities and are used by   more than 120,000 users from 40+ countries to design and   benefit from visualizations of complex data.      The talk concludes with a discussion of challenges that arise   when visual analytics tools are introduced to classrooms and   informal science education.    Keywords  data mining, information visualization, visual analytics,   education, macroscopes   BIOGRAPHY  Katy Brner is the Victor H. Yngve Professor of Information   Science at the School of Library and Information Science,   Adjunct Professor at the School of Informatics and Computing,   Adjunct Professor at the Department of Statistics in the College   of Arts and Sciences, Core Faculty of Cognitive Science,   Research Affiliate of the Biocomplexity Institute, Fellow of the   Center for Research on Learning and Technology, Member of the   Advanced Visualization Laboratory, and Founding Director of   the Cyberinfrastructure for Network Science Center at Indiana   University.  She is a curator of the Places & Spaces: Mapping   Science exhibit. Her research focuses on the development of data   analysis and visualization techniques for information access,   understanding, and management. She is particularly interested in   the study of the structure and evolution of scientific disciplines;   the analysis and visualization of online activity; and the   development of cyberinfrastructures for large scale scientific   collaboration and computation.  She is the co-editor of the   Springer book on Visual Interfaces to Digital Libraries and of a   special issue of PNAS on Mapping Knowledge Domains   (2004).  Her new book Atlas of Science: Visualizing What We   Know by MIT Press was published in 2010. She holds a MS in   Electrical Engineering from the University of Technology in   Leipzig, 1991 and a Ph.D. in Computer Science from the   University of Kaiserslautern, 1997.   ACKNOWLEDGMENTS  This work benefits from discussions and collaboration with   Adam V. Maltese, Indiana University; Joe E. Heimlich, Center of   Science and Industry, Columbus; Stephen Uzzo, New York Hall   of Science; Paul Martin, Science Museum of Minnesota; Sasha   Palmquist, Institute for Learning Innovation.      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK12, 29 April May 2, 2012, Vancouver, BC, Country.  Copyright 2012 ACM 978-1-4503-1111-3/12/04 $10.00.   2    REFERENCES  [1] Barabsi, A.-L. 2011. The network takeover. Nature   Physics. 8: 14-16.   [2] The McKinsey Global Institute. 2011. Big data: The next   frontier for innovation, competition, and productivity.   McKinsey & Company.   http://www.mckinsey.com/Insights/MGI/Research/Technolo  gy_and_Innovation/Big_data_The_next_frontier_for_innova  tion   [3] Thomas, J.J. and Cook, K.A. (Eds.). 2005.  Illuminating the   Path: The Research and Development Agenda for Visual   Analytics. IEEE Press.   [4] Brner, K. 2010. Atlas of Science: Visualizing What We   Know. Cambridge, MA: MIT Press.    [5] Brner, K. 2011. Plug-and-Play Macroscopes.   Communications of the ACM, 54(3), 60-69.         3      "}
{"index":{"_id":"2"}}
{"datatype":"inproceedings","key":"Siemens:2012:LAE:2330601.2330605","author":"Siemens, George","title":"Learning Analytics: Envisioning a Research Discipline and a Domain of Practice","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"4--8","numpages":"5","url":"http://doi.acm.org/10.1145/2330601.2330605","doi":"10.1145/2330601.2330605","acmid":"2330605","publisher":"ACM","address":"New York, NY, USA","keywords":"collaboration, data integration, ethics, learning analytics, practice, research, theory","Abstract":"Learning analytics are rapidly being implemented in different educational settings, often without the guidance of a research base. Vendors incorporate analytics practices, models, and algorithms from datamining, business intelligence, and the emerging big data fields. Researchers, in contrast, have built up a substantial base of techniques for analyzing discourse, social networks, sentiments, predictive models, and in semantic content (i.e., intelligent curriculum). In spite of the currently limited knowledge exchange and dialogue between researchers, vendors, and practitioners, existing learning analytics implementations indicate significant potential for generating novel insight into learning and vital educational practices. This paper presents an integrated and holistic vision for advancing learning analytics as a research discipline and a domain of practices. Potential areas of collaboration and overlap are presented with the intent of increasing the impact of analytics on teaching, learning, and the education system.","pdf":"Learning Analytics: Envisioning a Research Discipline and  a Domain of Practice   George Siemens  Technology Enhanced Knowledge Research Institute    Athabasca University  Edmonton, AB T5J 3S8   780-421-5841   gsiemens@athabascau.ca     ABSTRACT  Learning analytics are rapidly being implemented in different  educational settings, often without the guidance of a research  base. Vendors incorporate analytics practices, models, and  algorithms from datamining, business intelligence, and the  emerging big data fields. Researchers, in contrast, have built up  a substantial base of techniques for analyzing discourse, social  networks, sentiments, predictive models, and in semantic content   (i.e., intelligent curriculum). In spite of the currently limited  knowledge exchange and dialogue between researchers, vendors,  and practitioners, existing learning analytics implementations  indicate significant potential for generating novel insight into  learning and vital educational practices. This paper presents an  integrated and holistic vision for advancing learning analytics as a  research discipline and a domain of practices. Potential areas of  collaboration and overlap are presented with the intent of  increasing the impact of analytics on teaching, learning, and the  education system.     Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed   instruction (CMI), Distance learning   General Terms  Algorithms, human factors   Key Words  Learning Analytics, Theory, Research, Practice, Collaboration,  Ethics, Data Integration   1. INTRODUCTION  Learning analytics (LA) is a young and developing concept.  Reflection is warranted on how to position early developments for  long-term viability and positive impact of LA on learning and  teaching. Of critical importance is increased dialogue between   researchers and practitioners in order to guide the development of  new tools and techniques for analytics.    It is uncertain at this stage whether LA will develop as a distinct  field of study or whether analytics techniques will be subsumed  into existing research fields. Regardless of the long-term  trajectory of LA, a research base is already rapidly developing.  The Learning Analytics and Knowledge conference registrations  doubled from 2011 to 2012 (from 99 to over 200) and  submissions for review increased from 38 to 90. Numerous  special issues of academic journals (see http://www.ifets.info/ and  http://sloanconsortium.org/publications/jaln_main) indicate that  LA is gaining interest in different research fields. Additional  indicators of LAs continued growth can be found in government  reports [1] and numerous EDUCAUSE papers [2].     The first international LA conference in Banff in 2011, Learning  Analytics and Knowledge (LAK), emphasized the importance of  bridging computer sciences and social sciences:     Advances in knowledge modeling and representation,  the semantic web, data mining, analytics, and open data  form a foundation for new models of knowledge  development and analysis. The technical complexity of  this nascent field is paralleled by a transition within the  full spectrum of learning (education, work place  learning, informal learning) to social, networked  learning. These technical, pedagogical, and social  domains must be brought into dialogue with each other  to ensure that interventions and organizational systems  serve the needs of all stakeholders. [3]     To date, this social and technical connection has been largely  positive, but needs continued focus to advance LAs impact on  learning. With the significant increase in interest in data and  analytics, as indicated by conferences, journals, grant funding  opportunities, and growing vendor base, educators and researchers  have an opportunity to influence the development of analytics in  education.     LA is a sprawling term, at times referring to complex predictive  models and at other times to routine tasks such as classroom  allocation and energy conservation. The Society for Learning  Analytics (SoLAR, http://solaresearch.org/) emphasizes the  learner in its definition: Learning analytics is the measurement,  collection, analysis and reporting of data about learners and their  contexts, for purposes of understanding and optimizing learning  and the environments in which it occurs [4].        Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.   LAK12, 29 April May 2, 2012, Vancouver, BC, Country.  Copyright 2012 ACM 978-1-4503-1111-3/12/04 $10.00.   4    This definition includes techniques such as predictive modeling,  building learner profiles, personalized and adaptive learning,  optimizing learner success, early interventions, social network  analysis, concept analysis, and sentiment analysis. For a more  detailed overview of the development and approaches of LA,  Ferguson has a comprehensive account [5].    The International Educational Datamining Society (IEDMS,  http://www.educationaldatamining.org) community is progressing  on a parallel path to SoLAR in developing techniques and  approaches to understand the learning process through analytics  and data [6]. IEDMS (EDM at the time) initiated a workshop  series in 2005 and in 2008 hosted its first international conference.  Together, SoLAR and IEDMS seek to discover new insights into  learning and new tools and techniques so that those insights  impact the activity or practitioners in primary, secondary, and  higher education, as well as corporate learning.    2. The Gap  The research and practice gap, prominent in numerous fields  including corporate finance [7] and social sciences [8], is evident  within LA. The work of researchers often sits in isolation from  that of vendors and of end users or practitioners. This gap is  challenging as it reflects a broken cycle of communication and  interaction between empirical research and how those findings are  translated into practice.     Researchers, especially in technology fields, are not found only in  university laboratories. Many software companies invest heavily  in research, creating and ensuring protection of their intellectual  property (IP). Researchers in the LA domain explore learning  through the lens of data and analytics and share findings with  peers through publications and conferences so that other  researchers can build on discoveries. Sharing and disseminating  findings, algorithms, and new tools through scholarly discourse is  vital for innovation.     The practitioners who use the tools and techniques of researchers  and vendors are educators, designers, data scientists, trainers,  managers, and university administrators. The translation of  research into practice occurs through the knowledge development  and teaching roles of universities and through the risk-taking  activities of corporations as they develop products and services.  The university sector is a vital contributor to the knowledge  economy, providing states, provinces, and nations with  competitive advantages [9]. Researchers activities contribute to  the development of tools and techniques that influence corporate  activity. Vendors serve as a bridge between researchers and  practitioners as they translate research findings into software or  product offerings. Data are constantly generated. Collection and  analysis in LA do not have a natural beginning or conclusion.  Each analysis activity potentially feeds into practice, and each  practitioner act can serve as a new data point. Early indications  from vendors who are developing analytics software suggest that  findings will be treated as proprietary and will not be made  available to other researchers. The growing prominence of  protected IP can hinder iterative and rapid improvements to LA  techniques.     3. Why is this an Important Conversation  LA researchers risk losing relevance in the rapidly developing  world of corporate learning analytics companies (i.e., corporate  networks that integrate content companies with companies that  offer adaptation and personalization platforms). Researchers  should recognize that much of the innovation in LA happening in  the vendor space. Unfortunately, many vendor-driven innovations  are closed and do not meet the basic needs of researchers: open,  testable, accessible, and improvable algorithms and tools.    A few tools, such as R (http://www.r-project.org/), have been  developed with openness in mind (even though they were not  developed as LA tools). Researchers have already acquiesced  significant ground in building an open analytics platform to the  vendor community. A healthy vendor community is vital for LA  to make an impact, but researchers need access to their tools and  data in order to validate and test findings. How are various factors  weighted in an algorithm Are the concepts being analyzed the  right ones Can researchers adjust the algorithms of vendor tools  to conduct experiments of other factors that might impact  learning    Researchers require transparency and the ability to expose their  work to scrutiny of the broader community. Practitioners need  tools that are easy to use and that provide a positive end-user  experience. This will often require that much of the technical  functionality of analytics be hidden as an end-user layer guides  practitioners in how to interact with the data. Vendors develop  and commercialize tools and services, informed by research, that  allow for broad deployment. Practitioners, in their use of tools and  techniques, can inform both researchers and vendors.     Researchers seek to understand LA from multiple perspectives:  learners, institutions, and effectiveness. Corporations do not share  this obligation. Context influences the nature and type of  analytics. Understanding and disseminating analytics practices  and algorithms will assist researchers in building better models.  When these models are open, customization based on context is  possible. Increased dialogue between researchers, vendors, and  practitioners will unlikely solve the research-practice gap, but it  will raise awareness of the needs of each entity and generate a  sense of the important role that each plays.   4. Holistic and Integrated Research/ Practice   Relationships  The U.S. Department of Education has stated that the next 5  years will bring an increase in models for collaboration between  learning system designers, researchers, and educators [1]. Such  models would include participation from numerous stakeholders  in the analytics process: learners, faculty, departments, institution,  researchers, corporations, and government funders. A holistic  view of LA includes a broad spectrum of educational activity,  including the full student experience: pre-enrolment in university,  learning design, teaching/learning, assessment, and evaluation.     It is possibly futile to layout the direction needed in an emerging  field. The momentum in LA is significant and rapid changes are  difficult to track and it is even more difficult to trace their  trajectory. Yet in spite of the uncertainty around analytics in  education, a few considerations are important for researchers,  practitioners, and vendors in order to position LA for long-term   5    success. To advance as a field, LA researchers and practitioners  need to address the following: (a) development of new tools,  techniques, and people; (b) data: openness, ethics, and scope; (c)  target of analytics activity; and (d) connections to related fields  and practitioners.   4.1. Development of tools, techniques, and   people  Three areas of development are needed to drive the adoption of  analytics in education: new tools and techniques, the practitioner  experience, and the development of analytics researchers.    Analytics tools and techniques that focus on the social  pedagogical aspect of learning are required. Numerous techniques  have been developed outside of the education system, often from  business intelligence research. In other instances, the tools used  for analysis have not scaled with the increase in data size or  sophistication of analytics models. For example, discourse  analysis has a long history [10] in educational research. However,  dramatic increases in the size of discourse data sets, such as those  generated in large online courses, can overwhelm manual coding.  In response, automated analysis of discourse [11] builds on  existing models while scaling to accommodate the analysis of  larger data sets.     Some analytics techniques, such as early warning systems [12,  13], attention metadata [14], recommender systems [15], tutoring  and learner models [16], and network analysis [17], are already in  use in education. A few papers in LAK11 presented analytics  approaches that emphasized newer techniques, such as  participatory learning and reputation mechanisms [18],  recommender systems improvement [19], and cultural  considerations in analytics [20]. Beyond these, however, there are  limited first-generation LA techniques. The lack of defined  identity of LA tools and techniques with an explicit learning focus  is reflected in how analytics are described in papers and  conference venues: Its like Shazam, or Its like Amazon or  Netflix, or Its like Facebook friend recommendations. This is  not to criticize appropriating techniques from other fields for use  in learning. Instead, it is a reflection that LA-specific approaches  are still emerging and more research is required.     The second aspect of development, the practitioner experience,  focuses on the end user experiences of analytics tools and  techniques. Researchers and vendors present practitioners with  tools that are too complex. First-generation LA tools involve  researchers actively engaged with practitioners, providing  oversight, guidance, and support. As LA begin to inform more of  the education system, such as curriculum design, advising  learners, and pedagogical practice, practitioners using the tools  will have varying technical skillsets. Intuitive and easy-to-use  tools are important in involving greater numbers of educators. The  next generation of tools must be designed to serve a dual purpose:  context-sensitive help and guidance for non-technical users and an  accessible technical layer that allows more advanced users to  interact directly with data and to tweak and adjust analysis  models.    The final area of development centers on the capacity of  practitioners and researchers. Practitioner skills and knowledge  are being developed through traditional educational programs. A   few universities, such as North Caroline State University  (http://analytics.ncsu.edu/) and Northwestern  (http://www.analytics.northwestern.edu/), have started offering  masters programs in analytics. These programs are focused on  business intelligence, but many techniques are transferrable to  education. The capacity for analytics deployment requires the  development analysts and practitioners at masters and doctoral  levels. Certificate programs for university leaders and  administrators are not yet available. Professional development  programs are anticipated to address this need in the near future.    For researchers, capacity for LA research is being created in  various fields: computer science, statistics, programming, network  analysis, and psychology of education. In order to bring these  fields in dialogue with each other, research labs will need to be  developed. A distributed online research lab has been proposed to  help develop analytics students and researchers:  http://www.solaresearch.org/lab/. If the current trajectory of LA  development continues, it is reasonable to expect traditional  research labs to emerge that serve a similar role of bringing  specialized analytics fields in relation to each other.    4.2. Data: Openness, ethics, scope  As analytics derives from data, it is not surprising that many  outstanding concerns in LA centre on data. Foundational issues of  data quality, ethics of use, scope of analytics activity, data  standards, and integration data sets will continue to occupy a large  part of the conversation. Additionally, big data [21] raises the  prospect of new research methods [22]. It is conceivable that  future research models, especially in complex domains like  education, will be based on analytics rather than existing research  models. Conceivably, an explosion in learning sciences research  will result.    Ethics, learner rights, and data ownership are prominent topics.  Early attempts at clarifying data ownership recognize the need for  learner control [23]. Mismanagement of the messaging around  ethics in analytics can result in learner, and even broader public,  pushback to LA as a field. Analytics researchers, practitioners,  vendors, and educational systems have a responsibility  communicate clearly and transparently the scope and role of an  LA deployment. A proactive stance of transparency and  recognition of potential learner and educator unease of analytics  may be helpful in preventing backlash.    Human-contributed feedback or corrective options are also  required. When systems develop profiles and models of learners,  anomalies and errors can be expected. Recommender systems, for  example, may provide personalized content to a learner based on  learners who share similar profiles. This content may not be  helpful to a specific learner. End users, when given options of  correcting or teaching recommender systems, can improve  personalization.     The data silos that exist in universities, schools, and organizations  present a profound challenge for both researchers and  practitioners. Data integration from multiple sources can improve  the accuracy of a learner profile and subsequent adaptation and  personalization of content. Sharing data across silos addresses a  weakness in existing LA activity: data is too centric to learning  management systems (LMS) and student information systems  (SIS). Learner activity captured in these two systems comprises   6    only a fraction of the learning process. The inclusion of data from  other sources, such as mobiles, sensors, physical world data,  advising, and the use of university resources such as libraries and  tutors, will result in a more complete learner profile. New data  sets create exponentially to building learner profiles: LMS data,  combined with SIS data and the social media profile of a learner,  affords analytics opportunities that far exceed single data points.     Ideally, an integrated analytics system would allow data and  analytics layering: using multiple data sets and analytics  techniques in a single interface for visualizing and presenting data  to practitioners [24]. When these analytics tools are learner facing,  learners can gain insight into their habits and the impact of their  learning activities, thereby improving their self-awareness and  self-regulation.     Two final considerations regarding data include: semantic data  and real-time analytics. Extending analytics to include the role  linked data and semantic technologies will enable better  relationships between social and computing systems [25].  Semantic content (i.e., intelligent curriculum) will enable  computers to provide personalized content to learners. Learner  activity and their evolving profile can be constantly matched with  the knowledge architecture of a particular domain and learning  resources provided to fill any knowledge gaps.    Secondly, once analytics are conducted in (near) real-time,  learners will receive notification of conceptual errors earlier than  they currently do when the educators marks exams or essays. For  educators, real-time analytics and visualization will identify  challenges facing different learners based on concept  comprehension (as a result of lectures, labs, or simulations) or  through sentiment analysis (i.e., self-confidence) of discourse.   4.3. Target of analytics activity  Analytics are frequently cast as primarily technical or statistical  activities. A transition in analytics from a technical orientation to  one that emphasizes sensemaking, decision-making, and action  [26] is required to increase interest among educators and  administrators. This transition in no way minimizes the technical  roots of analytics; instead, it recasts the discussion to target  problems that are relevant to practitioners instead of researchers.    A second needed transition is one that moves LA research and  implementation from at-risk identification to an emphasis on  learner success and optimization [27, 28]. Identifying at-risk  learners has been, and will continue to be, an important  deliverable for LA. College dropouts are a concern facing  universities and society (and obviously the learners). However,  identifying at-risk learners is a small aspect of what analytics can  do to improve education. Through social network analysis and  content recommender systems, automated marking, improved  learner self-awareness, and real-time feedback for educators, the  learning process can be significantly optimized.    4.4. Connections to related fields and   practitioners  Improving connections with related fields of research, such as  machine learning, educational data mining, learning sciences,  psychology of learning, and statistics, is vital. The pieces that  define analytics are scattered across these fields. Working with   and sharing distributed knowledge is challenging but important  [29].    In addition to connecting various research domains, LA must  consider how it interacts with education systems, leaders and  other stakeholders. It is necessary to promote realism around what  learning analytics are and what they are able to accomplish.  Resolving, or at least suitably responding to, concerns about the  inability of data to capture complex social processes such as  learning are also required. In his presentation in SoLARs open  online course, Campbell [30] emphasizes the limitations of  analytics to measure complex processes such as learning.  Nuanced and thoughtful messaging should address both the hype  and buzz around analytics as well as voices that discount LA are  nothing new.     Research organizations and industry associations are the likely  agents to serve this society-facing role. For example, both SoLAR  and IEDMS are well positioned in this regard. Association  publications are still needed that target administrators and policy  makers. These reports could include annual state of the industry  analysis to communicate how the LA ecosystem is evolving in  terms of analytics adoption, implementation models, and the  vendor community.   5. Conclusion  Theoretically, LA has potential to dramatically impact the existing  models of education and to generate new insights into what works  and what does not work in teaching and learning. The results are  potentially transformative to all levels of todays education  system. For example, as models of personalization and adaptation  of learning develop, do we still need a course model in higher  education Are schools and universities allocated resources in  those areas that make the biggest impact How will we learn in a  networked, distributed, mobile, and analytics-driven system    Answering these questions through research, and then translating  those findings into practice, requires an evaluation of the current  state of LA and the challenges that need to be addressed. These  challenges currently involve the development of new tools,  techniques, and people; resolving data concerns such openness,  ethics, and the scope of data being captured; enlarging and  transitioning the target of analytics activity; and improving  connections to related fields. The task is significant and difficult,  but well worth embracing given the large potential benefit of an  integrated and holistic LA researcher-practitioner model.     6. REFERENCES  [1] M. Bienkowski, M. Feng, and B. Means. Enhancing teaching   and learning through educational datamining and learning   analytics (Draft): page 45, 2012. Available from  <http://evidenceframework.org/wp- content/uploads/2012/04/EDM-LA-Brief- Draft_4_10_12c.pdf> [accessed April 15, 2012]   [2] EDUCAUSE: Learning analytics: 69 resources, 2012.  Available from  <http://www.educause.edu/Resources/Browse/LearningAnal ytics/39193> [accessed April 15, 2012]   [3] G. Siemens. About: Learning Analytics & Knowledge:  February 27-March 1, 2011 in Banff, Alberta: para. 4. July   7    22, 2010. Available from  <https://tekri.athabascau.ca/analytics/> [accessed April 15,  2012]   [4] Society for Learning Analytics Research. About, 2012.  Available from http://www.solaresearch.org/about/ [accessed  April 15, 2012]   [5] R. Ferguson. The state of learning analytics in 2012: A  review and future challenges, 2012. Available from  <http://kmi.open.ac.uk/publications/pdf/kmi-12-01.pdf>  [accessed April 15, 2012]   [6] R. S. J. Baker and K. Yacef. The state of educational data  mining in 2009: A review and future visions, 2009. Available  from  <http://www.educationaldatamining.org/JEDM/images/articl es/vol1/issue1/JEDMVol1Issue1_BakerYacef.pdf> [accessed  April 15, 2012]   [7] E. A. Trahan and L. J. Gitman. Bridging the theory-practice  gap in corporate finance: A survey of Chief financial  officers. Quarterly review of economics and Finance, 35: 73- 87, January 1995.   [8] P. Lather. Research as praxis. Harvard Educational Review,  56(3): 257-278, Fall 1986.    [9] D. F. Shaffer and D. J. Wright. Higher education. A new  paradigm for economic development: How higher education   institutions are working to revitalize their regional and state   economies, March 2010. Available from  <http://www.rockinst.org/pdf/education/2010-03-18- A_New_Paradigm.pdf> [accessed April 15, 2012]   [10] G. Brown and G. Yule. Discourse analysis. London,  England; Cambridge University Press, 1983. Available from  <http://www.cambridge.org/gb/knowledge/isbn/item1130157 /site_locale=en_GB> [accessed April 15, 2012]   [11] A. De Liddo, S. Buckingham Shum, S., I. Quinto, M.  Bachler, and L. Cannavacciuolo. Discourse-centric learning  analytics.  <http://dl.acm.org/citation.cfmid=2090120&CFID=822691 74&CFTOKEN=35344405> [accessed April 1, 2012]   [12] J. P. Campbell, W. B. Collins, C. Finnegan, and K. Gage.  Academic analytics: Using the CMS as an early warning   system. Paper presented at WebCT Impact 2006. Chicago,  IL, July 2006.   [13] L. P. Macfadyen and S. Dawson. Mining LMS data to  develop an early warning system for educators: A proof of  concept. Computers & Education, 54(2): 588-599, February  2010.   [14] E. Duvall. Attention please!: learning analytics for  visualization and recommendation. In Proceedings of the 1st  International Conference on Learning Analytics and   Knowledge: pages 9-17, March 2011.  doi:10.1145/2090116.2090118   [15] Y. H. Cho, J. K. Kim and S. H. Kim. A personalized  recommender system based on web usage mining and  decision tree induction. Expert Systems with Applications,  23(3): 329-42, September 2002.   [16] P. Brusilovsky. Adaptive hypermedia: From intelligent  tutoring systems to web-based education. User Modeling and  User-Adapted Interaction, 11(1-2):87-110, 2001.    [17] M. Newman. Networks: An introduction. New York, NY:  Oxford University Press, 2010.   [18] D. Clow and E. Makriyanis (2011) iSpot analysed:  Participatory learning and reputation. In Proceedings of the  1st International Conference on Learning Analytics and   Knowledge: pages 34-43, March 2011.  doi:10.1145/2090116.2090121   [19] K. Verbert, H. Drachsler, N. Manouselis, M. Wolpers, R.  Vuorikari and E. Duval. Dataset-driven research for  improving recommender systems for learning. In  Proceedings of the 1st International Conference Learning   Analytics & Knowledge: pages 44-53, March 2011. Available  from  <http://dl.acm.org/citation.cfmid=2090122&CFID=774120 43&CFTOKEN=45426814> [accessed April 3, 2012]   [20] R. Vatrapu. Cultural considerations in learning analytics. In  Proceedings of the 1st International Conference on Learning   Analytics and Knowledge: pages 127-133, March 2011.  doi:10.1145/2090116.2090136    [21] F. X. Diebold. Big data dynamic factor models for  macroeconimic measurement and forecasting [online], 2000.  Available from  <http://www.ssc.upenn.edu/~fdiebold/papers/paper40/temp- wc.PDF> [accessed June 4, 2011]   [22] C. Anderson. The end of theory: The data deluge makes the  scientific method obsolete, June 2008. Available from  <http://www.wired.com/science/discoveries/magazine/16- 07/pb_theory> [accessed April 15, 2012]   [23] Office of Science and Technology Policy, Executive Office  of the President. Unlocking the power of education data for  all Americans [Fact sheet], January 19, 2012. Available from  <http://www.whitehouse.gov/sites/default/files/microsites/ost p/ed_data_commitments_1-19-12.pdf> [accessed April 4,  2012]   [24] D. Suthers and D. Rosen. A unified framework for multi- level analysis of distributed learning. In Proceedings of the  1st International Conference on Learning Analytics and   Knowledge: pages 64-74, March 2011.  doi:10.1145/2090116.2090124   [25] J. Hendler and T. Berners-Lee. From the semantic web to  social machines: A research challenge for AI on the world   wide web, November 2009.  doi.org/10.1016/j.artint.2009.11.010 [accessed April 15,  2012]   [26] G. Siemens. Sensemaking: Beyond analytics as a technical  activity. April 11, 2012. Available from  <http://www.educause.edu/ELI124/Program/GS01>  [accessed April 15, 2012]   [27] A. W. Astin. Degree attainment rates at American colleges  and universities: Effects of race, gender, and institutional  type. Los Angeles, CA: Higher Education Research Institute,  University of California, 1996.   [28] V. Tinto. Leaving college: Rethinking the causes and cures  of student attrition, 2nd edition. Chicago, IL: The University  of Chicago Press, 1993.    [29] G. Stasser and W. Titus. Pooling of unshared information in  group decision making: Biased information sampling during  discussion. Journal of Personality and Social Psychology,  48(6): 1467-1478, June 1985. Available from  http://www.citeulike.org/group/2546/article/1398512  [accessed April 15, 2012]   [30] G. Campbell. Here I stand. Presentation to Learning and  Knowledge Analytics Open online course, 2012  (http://lak12.wikispaces.com/). Session recording available  from <https://sas.elluminate.com/p.jnlppsid=2012-03- 01.1231.M.0728C08DFE8BF0EB7323E19A1BC114.vcr&si d=2008104>    8      "}
{"index":{"_id":"3"}}
{"datatype":"inproceedings","key":"Drachsler:2012:SIW:2330601.2330607","author":"Drachsler, Hendrik and Dietze, Stefan and Greller, Wolfgang and D'Aquin, Mathieu and Jovanovic, Jelena and Pardo, Abelardo and Reinhardt, Wolfgang and Verbert, Katrien","title":"1st International Workshop on Learning Analytics and Linked Data","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"9--10","numpages":"2","url":"http://doi.acm.org/10.1145/2330601.2330607","doi":"10.1145/2330601.2330607","acmid":"2330607","publisher":"ACM","address":"New York, NY, USA","keywords":"educational datasets, ethics, learning analytics, linked data, privacy","Abstract":"The main objective of the 1st International Workshop on Learning Analytics and Linked Data (#LALD2012) is to connect the research efforts on Linked Data and Learning Analytics in order to create visionary ideas and foster synergies between the two young research fields. Therefore, the workshop will collect, explore, and present datasets, technologies and applications for Technology Enhanced Learning (TEL) to discuss Learning Analytics approaches that make use of educational data or Linked Data sources. During the workshop, an overview of available educational datasets and related initiatives will be given. The participants will have the opportunity to present their own research with respect to educational datasets, technologies and applications and discuss major challenges to collect, reuse, and share these datasets.","pdf":"1st International Workshop on Learning Analytics and  Linked Data   Hendrik Drachsler  Open University of the Netherlands   Valkenburgerweg 177  6419AT Heerlen, Netherlands   +31 45 576-21-74   hendrik.drachsler@ou.nl   Stefan Dietze  L3S Research Centre   Appelstrae 9A  30167 Hannover, Germany   +49 511 762-17-705   dietze@l3s.de   Wolfgang Greller  Open University of the Netherlands   Valkenburgerweg 177  6419AT Heerlen, Netherlands   +31 45 576-21-74   wolfgang.greller@ou.nl   DAquin, Mathieu; The Open University, UK; m.daquin@open.ac.uk  Jovanovic, Jelena; University of Belgrade, Serbia; jeljov@gmail.com   Pardo, Abelardo; University of Madrid, Spain; abel@it.uc3m.es  Reinhardt, Wolfgang, University of Paderborn, Germany; wolle@upb.de   Verbert, Katrien; K.U.Leuven, Belgium; katrien.verbert@gmail.com  ABSTRACT  The main objective of the 1st International Workshop on Learning  Analytics and Linked Data (#LALD2012) is to connect the  research efforts on Linked Data and Learning Analytics in order  to create visionary ideas and foster synergies between the two  young research fields. Therefore, the workshop will collect,  explore, and present datasets, technologies and applications for  Technology Enhanced Learning (TEL) to discuss Learning  Analytics approaches that make use of educational data or Linked  Data sources. During the workshop, an overview of available  educational datasets and related initiatives will be given. The  participants will have the opportunity to present their own  research with respect to educational datasets, technologies and  applications and discuss major challenges to collect, reuse, and  share these datasets.    Categories and Subject Descriptors  E.1 [Data Structures] Distributed data structures; E.2 [Data   Storage Representations] Linked representations; J.1   [Administrative Data Processing] Education; H.1.1   [Information Systems] Models and principles, Systems and   Information Theory; J.4 [Social and behavioural sciences].   General Terms  Algorithms, Measurement, Design, Standardization, Human  Factors, Theory, Legal Aspects.   Keywords  Linked data, Learning analytics, Educational datasets, privacy,  ethics.   1. WORKSHOP BACKGROUND  In Technology Enhanced Learning (TEL), a multitude of datasets  exists that offer new opportunities for teaching and learning. The  available datasets can be roughly distinguished between (a) Open   Web data and (b) Personal Learning data originating from  different learning environments.   Open Web data covers educational data publicly available on the  Web, such as Linked Open Data (LOD) published by institutions  about their courses and other resources; examples include (but are  not limited to), e.g. The Open University (UK), the National  Research Council (CNR, Italy), Southampton University (UK), or  the mEducator Linked Educational Resources. It also includes the  emergence of Linked Data based metadata schemas and TEL- related datasets. The main driver for the adoption of the LOD  approach in education is the enrichment of the learning content  and the learning experience by making use of various connected  data sources.   Personal Learning data from different learning environments  originate from tracking learners interactions with different tools  and resources. The main driver for analyzing these data is the  vision of personalized learning that offers potential to create more  effective learning experiences through new possibilities for the  prediction and reflection of individual learning processes.   To this end, Learning Analytics can be seen as an approach,  which brings together two different views: (i) the external view  on publicly available Web data and (ii) an internal view on  personal learner data, e.g. data about individual learning activities  and histories. Learning Analytics aims at combining these two in  a smart and innovative way in order to enable advanced  educational services, such as recommendation of suitable learning  resources to individual learners.    To enable synergies and alignment of those efforts, communities  like the Special Interest Group (SIG) dataTEL of the European  Association of Technology Enhanced Learning (EATEL) and the  LinkedEducation.org open platform, emerged very recently. The  SIG dataTEL aims at advancing data-driven TEL research and to  develop a body of knowledge about personalization derived from  analyzing and visualizing personal data sourced from learning  environments. Connecting information extracted from such  personal tracking data with the Web of (Linked Open) Data offers  interesting perspectives to enrich learning processes with suitable  resources available on the Web.   The main objective of the LALD workshop is to connect the  research efforts on Linked Data and Learning Analytics to create  visionary ideas about how the synergy of a Web of Data and     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.   9    Learning Analytics can transform and support TEL processes and  applications. Therefore, the workshop will explore, collect and  review datasets for TEL to discuss Learning Analytics  approaches, which make use of the Web of Data. During the  workshop, an overview of available educational datasets will be  given. The participants will have the opportunity to present their  own datasets or dataset descriptions, show their data products and  tools, and discuss major challenges to collect, use and share  educational datasets and their products. Different promising  initiatives and solutions for the above mentioned challenges will  be presented.   2. WORKSHOP OBJECTIVES  The main objective of the LALD workshop is to foster a research  network around educational data issues and Learning Analytics.  The workshop will contribute to the following main challenges of  the SIG dataTEL and the Linked Education platform:   1. Educational Datasets:    Evaluating, promoting, creating and clustering of educational  datasets, schemas and vocabularies.    Feasibility of standardization of educational datasets to enable  exchange and interoperability.    Facilitating the sharing of educational datasets among TEL  researchers in general, and researchers in the Learning  Analytics field, in particular.    2. Data Technologies:    What technologies are available for the exploration of  educational datasets, i.e., for filtering, interlinking, exposing,  adapting, converting and visualizing educational datasets    What are the real-world applications that show a measureable  impact of Learning Analytics and thus successfully promote  the field to target groups    Which tools are available to use and exploit educational  Linked Open Data    Which innovative TEL applications make large-scale use of  the available open Web data   3. Evaluation of Technologies and Datasets:    Fostering standardized evaluation methods for Learning  Analytics.    Discuss the need of data competitions similar to TREC and  CLEF to compare TEL research and guide people in  evaluating and comparing their results.   4. Privacy and Ethics:    Contributing to policies on ethical implications of using the  educational data for learning analytics (privacy and legal  protection rights)    Suggesting guidelines for the anonymization and sharing of  educational data for Learning Analytics research.    3. WORKSHOP FACILITATORS  The workshop will be organized jointly by the Linked Education  initiative (http://linkededucation.org) and the SIG dataTEL  (http://bit.ly/datatel) of EATEL. Both, the Linked Education and  the SIG dataTEL aim at advancing data-driven research in TEL.  The main goals are to promote the re-use of public Web data, to   foster the cooperation between different Learning Analytics  research units and to act as a representative to other relevant  communities.   Both initiatives can look back on an annual workshop series at  different conferences:    RecSysTEL workshop with the 1st dataTEL marketplace  jointly at the EC-TEL and the ACM Recommender Systems  conference in Barcelona (http://bit.ly/datatel10).     dataTEL11 at the 3rd  Alpine Rendezvous conference in La  Clusaz, France (http://bit.ly/datatel_arv11).     Linked Learning workshop at the 8th Extended Semantic Web  Conference (http://purl.org/linkedlearning).   4. REFERENCES  [1] Dietze, S., Gugliotta, A., Domingue, J., (2008). Supporting   Interoperability and Context-Awareness in E-Learning  through Situation-driven Learning Processes. Special Issue  on Web-based Learning of International Journal of Distance  Education Technologies (JDET), 2008.    [2] Dietze, S., dAquin, M., Gaevi, D., Sicilia, M-A. (Eds.),  Proceedings of the 1st International Workshop on eLearning  Approaches for the Linked Data Age held at the 8th  Extended Semantic Web Conference, ESWC 2011, CEUR  Vol. 717, 2011.    [3] Drachsler, H., Bogers, T., Vuorikari, R., Verbert, K., Duval,  E., Manouselis, N., Beham, G., Lindstaedt, S., Stern, H.,  Friedrich, M., Wolpers, M. (2010). Issues and  Considerations regarding Sharable Data Sets for  Recommender Systems in Technology Enhanced Learning.  Elsevier Procedia Computer Science, 1, 2, pp. 2849 - 2858.   [4] Dovrolis, N., Stefanut, T., Dietze, S., Yu, H.Q., Valentine, C.  and Kaldoudi, E., Semantic Annotation and Linking of  Medical Educational Resources, 5th European Conference of  the International Federation for Medical and Biological  Engineering (IFMBE MBEC), Budapest, 2011.   [5] Kaldoudi, E., Dovrolis, N., Dietze, S., Information  Organization on the Internet based on Heterogeneous Social  Networks, 29th ACM International Conference on Design of  Communication (ACM SIGDOC11), Pisa, 2011.    [6] Manouselis, N., Drachsler, H., Vuorikari, R., Hummel, H.  and Koper, R. (2010). Recommender Systems in Technology  Enhanced Learning. In Kantor, P.B.; Ricci, F.; Rokach, L.;  Shapira, B. (Eds.) 1st Recommender Systems Handbook,  Berlin: Springer (2011).   [7] Mitsopoulou, E., Taibi, D., Giordano, D., Dietze, S., Yu, H.  Q., Bamidis, P., Bratsas, C., Woodham, L., Connecting  Medical Educational Resources to the Linked Data Cloud:  the mEducator RDF Schema, Store and API, Proceedings of  Linked Learning 2011: the 1st International Workshop on  eLearning Approaches for the Linked Data Age, CEUR-Vol  717, 2011.   [8] Verbert, K., Drachsler, H., Manouselis, N., Wolpers, M.,  Vuorikari, R., & Duval, E. (2011). Dataset-driven Research  for Improving Recommender Systems for Learning. 1st  International Conference Learning Analytics & Knowledge.  February, 27 - March, 1, 2011, Banff, Alberta, Canada.   10      "}
{"index":{"_id":"4"}}
{"datatype":"inproceedings","key":"Suthers:2012:CLM:2330601.2330608","author":"Suthers, Daniel D. and Hoppe, H. Ulrich and de Laat, Maarten and Shum, Simon Buckingham","title":"Connecting Levels and Methods of Analysis in Networked Learning Communities","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"11--13","numpages":"3","url":"http://doi.acm.org/10.1145/2330601.2330608","doi":"10.1145/2330601.2330608","acmid":"2330608","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative and networked learning, socio-technical networks","Abstract":"This paper describes the rationale behind a workshop on using data-intensive computational methods of analysis for empirical-analytical studies of collaborative and networked learning, with a particular focus on how learning takes place in the technically-mediated interplay between individual, small group and collective levels of agency. This workshop is primarily designed for researchers interested in empirical-analytical studies using data-intensive computational methods of analysis (including social-network analysis, log-file analysis, data mining, video analysis).","pdf":"Connecting Levels and Methods of Analysis   in Networked Learning Communities   Daniel D. Suthers   University of Hawaii  Honolulu, USA  1.808.956.3890   suthers@hawaii.edu   H. Ulrich Hoppe   University of Duisburg- Essen, Germany   49.203.379.3553   hoppe@collide.info   Maarten de Laat   Open University   Heerlen, NL    31.45.576.2961   maarten.delaat  @ou.nl   Simon Buckingham Shum   Open University   Milton Keynes, UK  44.1908.655723   s.buckingham.shum  @open.ac.uk   ABSTRACT  This paper describes the rationale behind a workshop on using   data-intensive computational methods of analysis for empirical-  analytical studies of collaborative and networked learning, with   a particular focus on how learning takes place in the   technically-mediated interplay between individual, small group   and collective levels of agency. This workshop is primarily   designed for researchers interested in empirical-analytical   studies using data-intensive computational methods of analysis   (including social-network analysis, log-file analysis, data   mining, video analysis).       Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1   [Computer Uses in Education] Collaborative learning   General Terms  Measurement, Experimentation, Human Factors   Keywords  Collaborative and networked learning, socio-technical   networks,    1. LEARNING AT MULTIPLE LEVELS  Technological and societal developments over the past few   decades have led to the emergence and confluence of multiple   conceptions of learning beyond what was possible at scale in a   pre-networked age. Studies of collaborative learning originating   in educational and cognitive psychology first conceived of   social settings as providing stimuli and conditions for   individual learning, but progressed towards more collective   forms of learning agency [5]. Led by the seminal work of   Roschelle and Teasley [19], a productive line of research and   practice focuses on understanding how small groups maintain a   joint conception of a problem, taking the group as the unit of   learning agency [16, 10]. At the community level, we find   concepts of learning as legitimate peripheral participation in   the practices of a community [12]; a process that also constructs   personal identity, entwining individual learning with group   practices that themselves can change; and knowledge   building: the deliberate effort of a community to increase its   cultural capital.   The growth of online learning and increasing percentages of   working or geographically distributed students enrolled in   educational institutions makes the understanding of formal and   informal learning in online settings more pressing [1].   Furthermore, Web 2.0 technologies and their associated   practices have increased the prominence of networked   individualism [2], leading to greater emphasis on networked   learning [3]. As Jones, Dirckinck-Holmfeld, & Lindstrom [8]   point out, this conception of learning emphasizes the power of   weak ties to enable individuals to access large networks of   resources [6], and thereby challenges the emphasis on strong   ties implied by both theories of collaborative learning in closely   interacting small groups [15] and the shared identity of   communities of practice [20].    Rather than viewing these and other conceptions of learning as   competing alternatives, we recognize that todays learners   participate in multiple forms of learning that take place   simultaneously at different scales and that influence each other   in constitutive and contextual relationships. Networked   learning succeeds when there is positive synergy between   individual activity and accrual of collective value that can be   drawn upon further by individuals [9]. Sociotechnical systems   intensify pathways of influence between local times and places   and support the emergence of global social phenomena from the   myriad of associations between many actors and artifacts [11].   The information and communication technologies (ICT)   underlying such systems enable us, as was never before   possible, to observe these processes through the electronic   traces left by participants [3, 18]. Multiple levels and forms of   learning phenomena are manifest in these traces in different   ways, leading researchers to combine different analytic   techniques. For example, Harrer and colleagues [7] and   Martinez and colleagues [13, 14] have combined quantitative   and qualitative content analysis with social network analysis;   De Laat, Lally, Lipponen & Simons [4] investigate patterns of   interaction using content analysis, critical event recall, and   social network analysis; and Suthers and colleagues [18, 17]   have developed an analytic hierarchy that transforms log files   into representations of action, interaction, associations, and ties   for multilevel analysis of online learning. The present   workshop is concerned with further developing the analytic   tools needed to address multiple levels of learning embedded in   networks.        Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK12,  29 April  2 May 2012,  Vancouver,  BC,  Canada.,  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   11    2. OBJECTIVES OF THE WORKSHOP  The workshop continues a series of workshops on a   multivocality theme, in which multivocal dialogue takes   place between analysts who address multiple scales of groups   and learning communities at multiple granularities of analysis,   with multiple methods of analysis and theoretical foundations.    The workshop on Connecting Levels of Learning in Networked   Communities, held at the 9th International Conference on   Computer Supported Collaborative Learning in Hong Kong,   was convened to address the theoretical and methodological   challenges of studying multi-level learning phenomena. The   workshop organizers perceive a need to connect (bridge   between, coordinate, etc.) analyses of how learning takes place   locally via individual activity and in small group settings and   global learning phenomena such as the learning of a community   or how local activity produces resources for learning in other   locales. Concomitant with these questions of the relationships   between local and network level learning phenomena come   questions of how theories at these levels articulate with each   other or explain phenomena across levels, and how methods for   local analysis, such as microanalytic or process oriented   methods and content analysis can be coordinated with methods   for qualitative case comparisons and methods for global   analysis, such as social network analysis.    The present workshop stresses how integrating multiple   methods of analysis reaches to practical questions of integrating   computational tools. Especially the interaction between certain   analysis methods and techniques and certain models and   approaches to theory building is of high interest. Guiding   questions and themes include:     How to detect emergent phenomena and patterns in traces   of collective/collaborative learning activities by using a   plurality of methods How do we interconnect these   methods    What theoretical perspectives are relevant to bridging   levels and methods of analysis    What practical techniques such as different types of   triangulation or visualization can help to connect different   levels and approaches of analysis    What are the prospects of technical integration of analysis   tools through a kind of open analysis workbench (open   architecture, adequate GUI metaphors)    REFERENCES   [1] I. E. Allen and J. Seaman, Growing by Degrees: Online   Education in the United States, 2005, Alfred P. Sloan   Foundation, Needham, MA, 2005.   [2] M. Castells, The Internet Galaxy: Reflections on the   Internet, Business, and Society, Oxford University Press,   2001.   [3] M. de Laat, Networked Learning, Politie Academie,   Apeldoorn, 2006.   [4] M. de Laat, V. Lally, L. Lipponen and R.-J. Simons,   Investigating patterns of interaction in networked learning   and computer-supported collaborative learning: A role for   Social Network Analysis, International Journal of   Computer Supported Collaborative Learning, 2 (2007), pp.   87-103.   [5] P. Dillenbourg, M. Baker, A. Blayne and C. O'Malley, The   evolution of research on collaborative learning, in H.   Spada and P. Reimann, eds., Learning in Humans and   Machine: Towards an interdisciplinary learning science,   Elsevier, Oxford, 1996, pp. 189-211.   [6] M. S. Granovetter, The strength of weak ties, American   Journal of Sociology, 78 (1973), pp. 1360-1380.   [7] A. Harrer, N. Malzahn, S. Zeini and H. U. Hoppe,   Combining Social Network Analysis with semantic   relations to support the evolution of a scientific   community, in C. Chinn, G. Erkens and S. Puntambekar,   eds., Mice, Minds, and Society  The Computer   Supported Collaborative Learning (CSCL) Conference   2007, International Society of the Learning Sciences,   2007, pp. 267-276.   [8] C. Jones, L. Dirckinck-Holmfeld and B. Lindstrom, A   relational, indirect, meso-level approach to CSCL design   in the next decade, International Journal of Computer-  Supported Collaborative Learning, 1 (2006), pp. 35-56.   [9] S. Joseph, V. Lid and D. D. Suthers, Transcendent   Communities, in C. Chinn, G. Erkens and S.   Puntambekar, eds., The Computer Supported   Collaborative Learning (CSCL) Conference 2007,   International Society of the Learning Sciences, New   Brunswick, 2007, pp. 317-319.   [10] T. Koschmann, A. Zemel, M. Conlee-Stevens, N. Young,   J. Robbs and A. Barnhart, How do people learn: Member's   methods and communicative mediation, in R. Bromme, F.   W. Hesse and H. Spada, eds., Barriers and Biases in   Computer-Mediated Knowledge Communication (and how   they may be overcome), Kluwer Academic Press,   Amsterdam, 2005, pp. 265-294.   [11] B. Latour, Reassembing the Social: An Introduction to   Actor-Network-Theory, Oxford University Press, New   York, 2005.   [12] J. Lave and E. Wenger, Situated Learning: Legitimate   Peripheral Participation, Cambridge University Press,   Cambridge, 1991.   [13] A. Martinez, Y. Dimitriadis, E. Gomez-Sanchez, B.   Rubia-Avi, I. Jorrin-Abellan and J. A. Marcos, Studying   participation networks in collaboration using mixed   methods, International Journal of Computer-Supported   Collaborative Learning, 1 (2006), pp. 383-408.   [14] A. Martinez, Y. Dimitriadis, B. Rubia-Avi, E. Gomez-  Sanchez and P. de la Fuente, Combining qualitative   evaluation and social network analysis for the study of   classroom social interactions, Computers & Education, 41   (2003), pp. 353-368.   [15] R. Slavin, An Introduction to Cooperative Learning,   Cooperative Learning: Theory Research and Practice,   Allyn and Bacon, Boston, 1990, pp. 1-46.   [16] G. Stahl, Group Cognition: Computer Support for   Collaborative Knowledge Building, MIT Press,   Cambridge, MA, 2006.   12    [17] D. D. Suthers, N. Dwyer, R. Medina and R. Vatrapu, A   framework for conceptualizing, representing, and   analyzing distributed interaction, International Journal of   Computer Supported Collaborative Learning, 5 (2010), pp.   5-42.   [18] D. D. Suthers and D. Rosen, A unified framework for   multi-level analysis of distributed learning Proceedings of   the First International Conference on Learning Analytics &   Knowledge, Banff, Alberta, February 27-March 1, 2011,   2011.   [19] S. D. Teasley and J. Roschelle, Constructing a joint   problem space: The computer as a tool for sharing   knowledge, in S. P. Lajoie and S. J. Derry, eds.,   Computers as Cognitive Tools, Lawrence Erlbaum   Associates, Hillsdale, NJ, 1993, pp. 229-258.   [20] E. Wenger, Communities of Practice: Learning, Meaning   and Identity, Cambridge University Press, Cambridge,   1998.     13      "}
{"index":{"_id":"5"}}
{"datatype":"inproceedings","key":"Lockyer:2012:LAM:2330601.2330609","author":"Lockyer, Lori and Dawson, Shane","title":"Where Learning Analytics Meets Learning Design","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"14--15","numpages":"2","url":"http://doi.acm.org/10.1145/2330601.2330609","doi":"10.1145/2330601.2330609","acmid":"2330609","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, learning design, pedagogical models, social network analysis, university teaching","Abstract":"The wealth of data available through student management systems and eLearning systems has the potential to provide faculty with important, just-in-time information that may allow them to positively intervene with struggling students and/or enhance the learning experience during the delivery of a course. This information might also facilitate post-delivery review and reflection for faculty who wish to revise course design and content. But to be effective, this data needs to be appropriate to the context or pedagogical intent of the course -- this is where learning analytics meets learning design.","pdf":"Where Learning Analytics Meets Learning Design  (workshop summary)   Lori Lockyer  University of Wollongong   Faculty of Education  Wollongong, NSW Australia   +61242215511   llockyer@uow.edu.au   Shane Dawson  University of British Columbia   Faculty of Arts  Vancouver, BC, Canada   +1 604 8220978   shane.dawson@ubc.ca      ABSTRACT   The wealth of data available through student management   systems and eLearning systems has the potential to provide   faculty with important, just-in-time information that may allow   them to positively intervene with struggling students and/or   enhance the learning experience during the delivery of a course.    This information might also facilitate post-delivery review and   reflection for faculty who wish to revise course design and   content.  But to be effective, this data needs to be appropriate to   the context or pedagogical intent of the course  this is where   learning analytics meets learning design.    Categories and Subject Descriptors  H.1 Models and Principles   General Terms  Measurement, Design, Human Factors    Keywords   learning design, pedagogical models, learning analytics, social   network analysis, university teaching   1. BACKGROUND  The field of learning analytics is concerned with integrating the   big data sets available in the higher education context in order   to better understand student engagement, progression and   achievement.  Student information systems capture demographic   and academic background of students as well as their academic   plans and achievements.  Learning Management and library   systems capture aspects of students daily online academic (and   sometimes social) life  what resources they access, which   teachers and peers they interact with, which learning activities   they engage in, what assessment tasks they complete.   Traditionally, universities have used this data retrospectively    and often on an adhoc basis  to evaluate courses, consider   resource allocation, identify institutional performance issues.    The analysis tools and modeling being developed in the field of   learning analytics allows university educators and administrators   opportunities to draw upon this data in order to actively intervene   in the academic process and provide a tailored and more   personalized education experience for students.  For struggling   students, this might mean early identification and support that   has the potential to limit attrition.  Analytics that support   identification of high achieving students might allow universities   to provide options for further challenges and opportunities.     Numerous examples of the use of learning analytics to date have   focused on institutional use.  However, there is great potential in   learning analytics for faculty delivering individual courses.    These significant data sets and the tools and models to integrate   and analyse them may provide important information for faculty   to adapt a course and/or direct support to individual student or   groups of students during the academic semester.  Learning   analytics also provides additional and more sophisticated   information, then student satisfaction surveys, to help faculty   review and revise courses after the academic semester is   complete.   To be most useful for faculty during and post course delivery,   learning analytics need to match the teaching and learning intent   of the course.  The possible data is endless, but faculty will not   use endless amounts of data.  Specific, analyzed, and   appropriately presented data has the potential to be useful and   effective.  But what learning analytics are useful with what types   of courses  This is where learning analytics meets learning   design.   The field of learning design has developed for more than a   decade now as a way of documenting and sharing examples of   teaching and learning practice  particularly in higher education.1    Learning designs have been represented in various forms2,3 but   all essentially describe the sequence learning activities, resources   and supports that a faculty member constructs for students over   part of or the entire academic semester.  In essence, learning   designs capture teaching and learning intent.  And, they may be a   useful tool in attempting to identify the specific learning   analytics that might best support faculty based on that teaching   and learning intent.   This workshop bring the two fields of learning design and   learning analytics together as a way forward in establishing   useful analytic models for course delivery and review.   2. Workshop objectives and plan  In this workshop, participants    - further developed their understanding of learning  design and learning analytics   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00      14    - explored how learning analytics that can support  faculty during course delivery and for course review   - identified the types of learning analytics that can be  useful for different learning designs        This workshop involved:   - brief overview presentations about learning design and  learning analytics   - participant brainstorm session about what learning  analytics might support faculty during (1) course   delivery and/or (2) course review   - participant groups working with specific designs from  the Learning Designs Project4 to identify relevant   learning analytics    - particular focus on social network analysis to support5,6   communication and collaboration learning designs      3. ABOUT THE FACILITATORS  Professor Lori Lockyer teaches and researchers in educational   technology in the Faculty of Education at the University of   Wollongong in Australia.  Loris research and development work   has focused on how online technology can support teaching and   learning particularly to foster collaborative learning.   Over the   past decade Lori has been investigating how Learning Designs   can support university faculty to effectively integrate technology   into their teaching.   Dr Shane Dawson is the Director of Arts Instructional Support   and Information Technology at the University of British   Columbia. His research centres on the application of quantitative   data derived from student online activity to inform teaching and   learning practice. Shane's research demonstrates the use of   student online interaction and network data to provide lead   indicators of learning support, sense of community, course   satisfaction, learning dispositions and creative capacity.   Shane and Lori have been working with collaborators to   understand how social network analysis supports effective   teaching in higher education and are extending this work to   investigate the integration of learning analytics with learning   design.   4. REFERENCES  Relevant references and resources for this workshop:   [1] Lockyer, L., Bennett, S., Agostinho, S., & Harper, B.   (Eds) (2009). Handbook of Research in Learning   Designs and Learning Objects. Hershey, New York: IGI   Global.    [2] Conole, G. (2010). An overview of design representations. In L.  Dirckinck-Holmfeld, V. Hodgson, C. Jones, M. d. Laat, D.   McConnell & T. Ryberg (Eds.), Proceedings of the 7th   International Conference on Networked Learning (pp. 482-489).   [3] Agostinho, S. (2009). Learning design representations to   document, model, and share teaching practice. Handbook of   Learning Design and Learning Objects: Issues, Applications,   and Technologies, 1, 1-19.   [4] Learning Designs Project  http://www.learningdesigns.uow.edu.au/   [5]  Seeing  networks: visualising and evaluating student  learning networks project and related resources   http://research.uow.edu.au/learningnetworks/seeing/ab  out/index.html   [6] Dawson, S., Bakharia, A., Lockyer, L., & Heathcote, E.  (2011) Seeing networks: visualising and evaluating   student learning networks Final Report 2011.   Canberra: Australian Learning and Teaching Council   Ltd. Available URL   http://research.uow.edu.au/content/groups/public/@we  b/@learnnet/documents/doc/uow115678.pdf      15      "}
{"index":{"_id":"6"}}
{"datatype":"inproceedings","key":"Slade:2012:LAH:2330601.2330610","author":"Slade, Sharon and Galpin, Fenella","title":"Learning Analytics and Higher Education: Ethical Perspectives","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"16--17","numpages":"2","url":"http://doi.acm.org/10.1145/2330601.2330610","doi":"10.1145/2330601.2330610","acmid":"2330610","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, open university (OU), student walk","Abstract":"Take two students who were enrolled on the same higher education course, both of whom were identified as likely to benefit from additional support and tailoring of their learning experience. Three years later, one student has gone on to gain a good degree and is now making great progress in her career. The other student, whose background and learning needs appeared similar, scraped through the experience, has recently been eased out of her organization and is unemployed. To what extent were decisions taken by their tutors and institution about the design of their learning experiences, responsible for these two very different outcomes?","pdf":"Learning Analytics and Higher Education:   Ethical Perspectives   Sharon Slade  Open University   Foxcombe Hall, Boars Hills  Oxford, UK   +44(0)1865 486250   s.slade@open.ac.uk      Fenella Galpin  Open University   Foxcombe Hall, Boars Hills  Oxford, UK   +44(0) 1865 486284   f.a.v.galpin@open.ac.uk      Take two students who were enrolled on the same higher  education course, both of whom were identified as likely to  benefit from additional support and tailoring of their learning  experience.  Three years later, one student has gone on to gain a  good degree and is now making great progress in her career.  The  other student, whose background and learning needs appeared  similar, scraped through the experience, has recently been eased  out of her organization and is unemployed.  To what extent were  decisions taken by their tutors and institution about the design of  their learning experiences, responsible for these two very different  outcomes     **********************   The vast potential of Learning Analytics in student support and  engagement cannot be denied  a different and appropriate  learning experience tailored for each student taking a module  sounds just what learning designers would wish for  but the  ethical issues and their impact on stakeholders needs to be  carefully thought through.   With increasing competition from the private sector and reduced  funding in the public sector, many higher education (HE)  institutions are giving much more attention to retention and  progression of students throughout their studies.  At the same time  HE institutions are making increasing use of online course  delivery as part of their standard offering.  Add to this the  explosion in electronic data which it is now possible to collect,  and the potential for Learning Analytics is clear.        Categories and Subject Descriptors   K3.1 [Computers and Education]: Computer Uses in Education   distance learning.   General Terms  Management, Measurement, Performance, Design, Human  Factors   Keywords  Learning Analytics, Open University (OU), student walk,   Analytics have been used in the corporate sector for years, where   clear benefits can be seen.  A recent McKinsey report [1] proposes  that the potential for a retailer embracing  big data (the  enormous trails of data each of us generates during our daily  lives)  can lead to a 60% increase in its operating margin.   Amazon, for example, has been using customer data for some  time to tailor their offerings accordingly, and this is generally  welcomed, or at least accepted, by customers.     HE Institutions are similarly confident that Learning Analytics is  capable of facilitating clearer and simpler means of understanding  and driving student engagement and performance, and thus  enhancing retention and progression.  But in contrast to the use of  analytics in the corporate sector, Learning Analytics is potentially  a much more emotive topic, as it raises issues of success and  failure, personal prospects and labels that can stick for life.  In his  report on the first International Conference on Learning Analytics  and Knowledge (LAK11), held in Banff, Canada, in 2011, Brown  [2] reports that speakers and delegates were in agreement that  Learning Analytics raises deep and complex privacy issues akin  to eavesdropping: the ethical concerns are clear to see.     HE institutions have, of course, been analyzing data to some  extent for many years.  Public examinations at school level, for  example, are completely accepted as indicators of student  selection for courses and institutions.  Academic analytics,  however, in which progress is measured and the approach to  supporting the student is adapted as a programme progresses, is a  new phenomenon.     The assumption that its acceptable to collect and make use of  data on individuals or groups by monitoring patterns of behaviour  is increasingly coming under the spotlight.  Facebook, for  example, has recently been in the news for using photographs of  members without their permission to endorse the commercial  products of third parties to Friends .   Earlier on, we pointed out that it would be wonderful if learning  could be tailored for each individual student.  This is usually  impractical with large groups and limited teaching resource and so  grouping of data to create a limited range of semi-standardized  learning offerings has been the obvious simplification.  Van Wel  and Royakker [3] ask what rights the student has to remain an  individual.  They express concerns about de-individualisation,  where students are judged and treated according to group  characteristcs rather than their own specific capabilities and traits.   They show, too, that the use of group characteristics can lead to  non-distributed group profiles, where not every member of the  group necessarily shares every one of the characteristics of that  group.  Personalisation of service is seen as positive, but if its  achieved by creating non-distributed group profiles, then it can  lead to de-individualised discrimination.      Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy   otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00      16    Another big question mark hangs over whether users should even  know that information is being collected about them and if so,  how it is being used.  But if they do know, and are being given a  less than positive label  albeit in the interest of student support  and concern  what long term effect might that have  If students  are given the opportunity to withhold information about  themselves, the data set becomes weaker and therefore has less  validity and use.     The potential upsides of data collection and analysis are manifold.   Harvard, for example, are experimenting with Classroom click  streams [4] to group students, based on Learning Analytics.   Rather than asking students to discuss an issue with whoever may  be sitting next to them in the classroom, a computer chooses who  they should talk to, based on their study profile to date.  At the  Open University, and as reported elsewhere in these Proceedings,   we have found that targeted support to students has led to tangible  improvements in retention and progression.     Will Learning Analytics fulfil its promise to make courses and  institutions more accessible and appropriate, or will students be  steered away from concepts and methods that might lower their  scores  When tutors have access to substantial but selective data  will it still be possible for them to deal with their students fairly  and objectively  Perhaps a disservice is being done to students if  they are constantly directed down the right course, and have the  right materials put in front of them.  Is the institution by so  doing hampering the students ability to learn how to learn in the  long run  How will they be prepared for the real world, if  everything has been pre selected for them  How will students  learn to make their own choices  And what chance is there for  serendipity  for happy accidents or pleasant surprises   Learning Analytics will never be a panacea.  If students are not  progressing satisfactorily there are many factors to be taken into  account.  But if it can be harnessed to best effect and taking due  account of ethical considerations, surely Learning Analytics can  be a powerful tool in enhancing the students experience and  increasing his or her chances of long term success     WORKSHOP OUTLINE   A wide range of issues and consequences relating to the use of  Learning Analytics will be explored from different stakeholder  perspectives in this practical half day workshop.  The workshop  will engage participants in an exploration of some of the ethical  complexities that are introduced by using Learning Analytics to  categorise and predict student cohorts and behaviours.   Scenarios  and data sets will be used to bring out the issues for different  stakeholder groups.  Work in small groups will surface ethical  issues around Learning Analytics and identify possible ways  forward to address the issues raised.  A plenary session will share  and record group outputs, which will then form the basis for  further online discussion and potential future research.   Issues covered may include:   Might the use of Learning Analytics present insurmountable  ethical challenges    How and why do ethical decisions get made   How to bring together potentially conflicting views from those  involved in the Learning Analytics process  tutors, students,  administrators/management.   Is Learning Analytics about standardization or personalized  tailoring   Does Learning Analytics drive the behaviour of a HE institution  and what role does the culture of the institution play in Learning  Analytics How far should the responsibilities of an institution go  to support identified cohorts   Why might tutors be in favour or not of support driven by a  Learning Analytics approach   How might Learning Analytics facilitate or suppress access to  additional student support   What are the effects on student behaviour of labeling student  groups   Who are the real beneficiaries of this approach  the student or the  institution   ABOUT THE FACILITATORS   Fenella Galpin and Sharon Slade are senior lecturers in the  Faculty of Business and Law at the Open University in the UK.  They work to support both tutors and students on Open University  distance learning modules and programmes. The Open  Universitys move toward curriculum-based support means they  are at the forefront of the practical and operational use of  Learning Analytics in the University. They have significant  experience and involvement in running staff development  workshops and other sessions for OU tutors, staff and students.   Their research interests encompass online delivery learning and  tutoring, online learning communities and Learning Analytics.   REFERENCES  [1] McKinsey Global Institute. 2011. Data: The next frontier for   innovation, competition and productivity.  McKinsey &  Company   [2] Brown, M. 2011. Learning Analytics: the coming 3rd wave.  http://net.educause.edu/ir/library/pdf/ELIB1101.pdf.  Accessed 14 October 2011    [3] van Wel, L., & Royakkers, L. 2004. Ethical issues in web  data mining. Ethics and Information Technology, 6, 129-140   [4] Parry, M. 2011. Colleges mine data to tailor students  experience. Technology  The Chronicle of Higher  Education      17      "}
{"index":{"_id":"7"}}
{"datatype":"inproceedings","key":"Baker:2012:EDM:2330601.2330613","author":"Baker, Ryan S. J. d. and Duval, Erik and Stamper, John and Wiley, David and Shum, Simon Buckingham","title":"Educational Data Mining Meets Learning Analytics","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"20--20","numpages":"1","url":"http://doi.acm.org/10.1145/2330601.2330613","doi":"10.1145/2330601.2330613","acmid":"2330613","publisher":"ACM","address":"New York, NY, USA","keywords":"data mining, education, learning analytics","Abstract":"W This panel is proposed as a means of promoting mutual learning and continued dialogue between the Educational Data Mining and Learning Analytics communities. EDM has been developing as a community for longer than the LAK conference, so what if anything makes the LAK community different, and where is the common ground?","pdf":"Panel: Educational Data Mining meets Learning Analytics     Ryan S.J.d. Baker  Dept. Social Science and Policy   Studies  Worcester Polytechnic Institute    Worcester, MA USA   +1 508-831-5355      rsbaker@wpi.edu   Erik Duval  Dept. Computerwetenschappen  Katholieke Universiteit Leuven   Celestijnenlaan 200A - bus 2402  3001 Heverlee   Belgi  +32 1632-7700   erik.duval@cs.kuleuven.be   John Stamper  Human-Computer Interaction   Institute  Carnegie Mellon University   5000 Forbes Ave  Pittsburgh, PA 15213, USA   +1 412-268-9690   john@stamper.org      David Wiley  Dept. Instructional Psychology &   Technology  Brigham Young University   Provo, UT 84602, USA  +1 801-422-7071   david.wiley@byu.edu   Simon Buckingham Shum (Panel Chair)  Knowledge Media Institute   The Open University  Walton Hall   Milton Keynes, MK7 6AA, UK  +44-1908-655723   s.buckingham.shum@gmail.com    ABSTRACT  This panel continues the dialogue between the Educational Data  Mining and Learning Analytics communities. EDM has been  developing as a community for longer than the LAK conference,  so what if anything makes the LAK community different, and  where is the common ground Is LAK just reinventing the wheel,  or adding some important new spokes To push the metaphor, are  LAKs wheels fit for the new learning terrain without EDM In  any case, what do we need in addition to wheels to go places Is  EDM narrower but deeper, best suited for stable, well  understood domains in which domain knowledge and user  cognition can be formally modelled, but at considerable expense  Is EDM also more mathematical, while LA is more qualitative,  socially oriented, and interested in open, social learning in the  wild where far less can be known about users or learning  objectives Or are these just myths and stereotypes waiting to be  debunked Two representatives from each community (LAK:  Duval & Wiley; EDM: Baker & Stamper) will present a brief  position, outlined in this paper, in which they set out what it is  that excites them about their home discipline and community,  and how they see the relationships between the fields. The issue  will then be opened up for conference delegates to debate what  could or should be future trajectories for the fields.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1   [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI)  General Terms  Design, Human Factors, Theory   Keywords  learning analytics; educational data mining       1. EDUCATIONAL DATA MINING  1.1 Ryan Baker  I will discuss a vision of what the educational data mining  community has to offer to the science and practice of education,  focusing on the positive things that LAK can learn from EDM,  and the positive things that EDM can learn from LAK. As such, I  believe that the future of EDM and LAK should be as best friends,  rather than as frenemies (or worse), as is so often seen when two  research communities occupy similar spaces in their scope of  scientific inquiry.   My belief is that one of the key contributions that EDM makes is  the advancement of rigorous positions on how to verify that  models produced through data mining and analytics are valid and  generalizable. The migration of some of these standards and  approaches to the LAK community may be useful to LAK  researchers in specific cases.   In addition, the EDM community's focus on comparing different  modeling methods, towards discovering when specific models and  frameworks are appropriate, is producing knowledge that would  be beneficial to researchers in the LAK community.   At the same time, I believe that many of the problems being  attacked in LAK and the methods for attacking these problems are  unique and highly advanced, and that EDM would benefit from   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12: 2nd International Conference on Learning Analytics &  Knowledge, 29 April  2 May 2012, Vancouver, BC, Canada   Copyright 2012 ACM 1-58113-000-0/00/0010$10.00.       learning more about these problems and approaches. I also believe  that LAKs attention to the needs and concerns of various  stakeholder groups is exemplary, and that EDM research would  benefit strongly from learning how LAK researchers are  addressing those issues.   Furthermore, in some areas EDM research and LAK research are  making advances that could be integrated, to the benefit of both  communities. For example, much EDM research now leverages  human judgment to support data mining, suggesting that the  combination of LAK research in leveraging human judgment with  EDM expertise in data mining may produce positive results  beyond the current capacities of either community.   Finally, I will argue vigorously (but in a friendly way) against the  proposition that EDM is narrower but deeper, best suited for  stable, well understood domains. In my opinion, both EDM and  LAK are highly suited for any learning domain, and exemplary  research in both communities is targeted at extending the  phenomena and settings in which learning and learners can be  studied.  Bio: Ryan S.J.d. Baker is Assistant Professor of Psychology and  the Learning Sciences at Worcester Polytechnic Institute. His  research, at the intersection of educational data mining/learning  analytics and human-computer interaction, focuses on modeling  and studying students learning, engagement, and affect. He was  elected founding President of the International Educational Data  Mining Society in 2011, and is Associate Editor of the Journal of  Educational Data Mining. He graduated from Carnegie Mellon  University in 2005, with a Ph.D. in Human-Computer Interaction.  He received the Best Paper Award at the Intelligent Tutoring  Systems Conference in 2006, and received the Best Oral  Presentation Award at the Intelligent Tutoring Systems  Conference in 2010. http://users.wpi.edu/~rsbaker      1.2 John Stamper  In the past, I was always preaching: the data flood is coming,  but today that has changed to: the data flood is here! Traditional  methods of data analysis have not kept pace with the amount of  data that can be collected and is being collected from educational  environments today. Many others have also seen this trend which  is one of the main reasons that the Educational Data Mining and  Learning Analytics communities have begun to grow as fast as  they have in the last couple of years.   One of my roles is the Technical Director of the Pittsburgh  Science of Learning Center DataShop1, which has become a large  repository of log data collected from a variety of educational  systems, most notably the cognitive tutors that have been  developed at Carnegie Mellon University and Carnegie Learning,  Inc. The datasets in DataShop are composed of fine-grained data,  with student actions recorded roughly every 20 seconds. As of  March 2012, DataShop contains over 300 datasets which are  comprised of over 70 million student actions and 190,000 student  hours of data. Over time, we have seen a shift in the types of data  collected. Originally, most of the datasets were from experimental  studies performed in a classroom and generally lasted days or  weeks. More often now, the data coming in has a much longer  time frame lasting months, semesters, or entire years. In addition  to study data, we are now receiving course data that does not  represent any preset experiment but is collected in hopes that                                                                        1 https://pslcdatashop.web.cmu.edu/about   2 http://www.teleurope.eu/pg/podcasts/playg=140221    researchers can use the data to understand learning and improve  the courses where the data was derived.    In 2010, DataShop hosted the KDD Cup Challenge, which asked  participants to predict student performance on mathematical  problems from logs of student interaction data similar to the type  stored in DataShop. One major difference was that the size of the  datasets for the competition was larger than the entire DataShop  repository at that time. The size did seem to provide major hurdles  for researchers in the competition  especially those from the  learning sciences. To effectively use these large datasets to make  discoveries, both the EDM and LAK communities need to  develop or find the tools and algorithms to handle the size and  robustness of these data.   For the most part, the goals of the EDM and LAK communities  overlap extensively, but there are subtle differences that I see  between the two communities. The most fundamental is where the  research is focused. The EDM community tends to use data to  understand how and when learning occurs. The focus is on the  process. One key area is building predictive models to explain and  detect aspects of learning. The LAK community tends to focus on  the learner, and using data to explore how the learner interaction  with technology affects individual learning. Again, the difference  is subtle, and both are needed to improve the effectiveness of  educational technology, which is the goal of both communities.   Bio: John Stamper is a member of the research faculty at the  Human-Computer Interaction Institute at Carnegie Mellon  University. He is also the Technical Director of the Pittsburgh  Science of Learning Center DataShop. His primary areas of  research include Educational Data Mining and Intelligent  Tutoring Systems. As Technical Director, John oversees the  DataShop, which is an open data repository and set of associated  visualization and analysis tools for researchers in the learning  sciences. John received his PhD in Information Technology from  the University of North Carolina at Charlotte, holds an MBA from  the University of Cincinnati, and a BS in Systems Analysis from  Miami University.  Prior to returning to academia, John spent over  ten years in the software industry including working with several  start-ups.  He is a Microsoft Certified Systems Engineer (MCSE)  and a Microsoft Certified Database Administrator (MCDBA).  John was the co-chair of the 2010 KDD Cup Competition, titled  Educational Data Mining Challenge, which centered on  improving assessment of student learning via data mining.  http://www.hcii.cmu.edu/people/faculty/john-stamper      2. LEARNING ANALYTICS  2.1 Erik Duval  In my view, Learning Analytics is about collecting traces that  learners leave behind and using those traces to improve learning.  Educational Data Mining can process the traces algorithmically  and point out patterns or compute indicators. My personal interest  is more in using the traces in order to empower learners to be  better learners.  My team focuses on building dashboards that visualize the traces  in ways that help learners or teachers to steer the learning process.  I like this approach because it focuses on helping people rather  than on automating the process. It is inspired by a modest  computing approach2 where the technology is used to support  what we want people to be good at (being aware of what is going                                                                       2 http://www.teleurope.eu/pg/podcasts/playg=140221      on, making decisions, ) by leveraging what computers are good  at (repetitive, boring tasks).   Of course, capturing meaningful learning traces is something that  both we and the EDM community struggle with. Translating those  traces into visual representations and feedback that support  learning is another challenge: the danger of presenting  meaningless eye candy or networks that confuse rather than help  is all too real.  Both our work and that of the EDM community is also difficult to  evaluate: we can (and do!) evaluate usability and usefulness, but  assessing real learning impact is hard  both on a practical,  logistical level (as it requires longitudinal studies) as well as on a  more methodological level (as impact is messy and it is difficult  to isolate the effect of the intervention that we want to evaluate).   In both these areas, we may be able to make better progress by  exchanging our experiences. There is also an opportunity to  combine both approaches: for instance, we can use visualization  techniques to help people understand what data mining algorithms  come up with and why. In that way, work on visualization can  help to increase understanding of and trust in what the EDM  community achieves.   Bio: Erik is professor of computer science and chairs the research  unit on human-computer interaction, at KU Leuven, the  University of Leuven in Belgium. His research focuses on novel  ways to interact with information, through information  visualization, mobile information devices and multi-touch  displays. Typical application areas are technology enhanced  learning, interaction with music and research2.0. Erik teaches  courses on Human-Computer Interaction, Multimedia, problem  solving and design. He is a member of the informatics section of  the Academia Europeae and co-founded two spin-offs on  personalized smart interaction with music and scientific output, as  well as the not-for-profit ARIADNE Foundation that promotes  share and reuse of learning material.  http://erikduval.wordpress.com/about     2.2 David Wiley  As part of his 2 sigma work, Bloom (1984)3 challenged  educational researchers to devise practical methods  methods  that the average teacher or school faculty can learn in a brief  period of time and use with little more cost or time than  conventional instruction  that would help learners reach their  academic potential. My personal interest in learning analytics lies  in its ability to answer extremely practical and socially responsive  questions such as, What is the most effective thing a teacher  could do with her next 30 minutes and What is the most  effective experience a learner could choose next                                                                         3 http://en.wikipedia.org/wiki/Bloom's_2_Sigma_Problem    In my view, learning analytics as a term simply describes the  extremely interdisciplinary endeavor of providing this pragmatic  support for learning.   On the back end learning analytics combines knowledge and  techniques from data mining and psychometrics to leverage both  behavioral data and data about academic performance. From this  perspective learning analytics is a synthesis of techniques like  Nave Bayes, Rasch modeling, collaborative filtering, and item  response theory. Both data mining and psychometrics possess a  rich set of tools that are applicable to the problems we want to  solve using learning analytics.   On the front end learning analytics combines knowledge and  techniques from data visualization and UI/UX to empower  ordinary teachers or learners with little or no training to bring the  full power of data to bear on their learning-related decisions.  Data-related tools still look too much like the Your Product in  the famous StuffThatHappens comic.4 We typically fail to  acknowledge that the work involved in achieving Google or  Apple-like simplicity in the front end design of learning analytics  tools will require at least as much effort and attention as will  solving back end problems.   Learning analytics, then, is a consumer of the knowledge created  by the educational data mining community and depends on this  and the work of numerous other fields in order to bring the full  promise of technology (in this case, the data-enabled promises) to  ordinary learners and teachers everywhere.    Bio: Dr. David Wiley is Associate Professor of Instructional  Psychology and Technology, and Associate Director of the Center  for the Improvement of Teacher Education and  Schooling at Brigham Young University, where he directs  the Open Education Group. David is currently Senior Fellow for  Open Education at the National Center for Research in Advanced  Information and Digital Technologies (Digital Promise) and a  Peery Social Entrepreneurship Research Fellow in BYUs  Marriott School of Business. Previously, David was a recipient of  the National Science Foundations CAREER grant.  http://opencontent.org/blog                                                                           4 http://devio.wordpress.com/2012/02/19/user-interface-design/      "}
{"index":{"_id":"8"}}
{"datatype":"inproceedings","key":"Graf:2012:BDG:2330601.2330614","author":"Graf, Sabine and Ives, Cindy and Lockyer, Lori and Hobson, Paul and Clow, Doug","title":"Building a Data Governance Model for Learning Analytics","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"21--22","numpages":"2","url":"http://doi.acm.org/10.1145/2330601.2330614","doi":"10.1145/2330601.2330614","acmid":"2330614","publisher":"ACM","address":"New York, NY, USA","keywords":"ethics, learning analytics","Abstract":"This international panel presentation aims to explore and discuss the issues that emerge when an educational institution decides to develop learning analytics initiatives. While learning analytics may provide data that lead to improvements in the quality of teaching and learning design, and therefore has the potential to enhance the overall quality of education, the successful development and implementation of tools and processes for learning analytics are complex and problematic. In this panel, data governance considerations will be discussed from organizational, ethical, learning design, and technical points of view.","pdf":"LAK 2012 Panel   Building a Data Governance Model for Learning Analytics  Sabine Graf   Computing and Information Systems  Athabasca University, Canada   sabineg@athabascau.ca                     Cindy Ives  Acting Associate Vice President (Learning  Resources), Athabasca University, Canada    cindyi@athabascau.ca      Lori Lockyer   Faculty of Education, University   of Wollongong   Paul Hobson   Director, Enterprise   Architecture, Information  Technology, University of   British Columbia    Doug Clow   Institute of Educational  Technology, The Open   University UK    PANEL SUMMARY   This international panel presentation aims to explore and   discuss the issues that emerge when an educational institution   decides to develop learning analytics initiatives. While learning   analytics may provide data that lead to improvements in the   quality of teaching and learning design, and therefore has the   potential to enhance the overall quality of education, the   successful development and implementation of tools and   processes for learning analytics are complex and problematic.   In this panel, data governance considerations will be discussed   from organizational, ethical, learning design, and technical   points of view.   Categories & Subject Descriptors    J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,   Computer-assisted instruction (CAI), Computer-managed   instruction (CMI), Distance learning   General Terms   Management.   Keywords    Learning Analytics, Ethics.   1. LIST OF PANELISTS  Chair: Cindy Ives, Acting Associate Vice President   (Learning Resources), Athabasca University   Discussants:     Sabine Graf, Assistant Professor, Computing and  Information Systems, Athabasca University    Lori Lockyer, Professor, Faculty of Education,  University of Wollongong    Paul Hobson, Director, Enterprise Architecture,  Information Technology, The University of British   Columbia    Doug Clow, Lecturer, Interactive Media  Development, Institute of Educational Technology,   The Open University UK   Format:   After a brief introduction to the topic and the presenters, each   discussant will offer an opening statement about their   perspective on the factors affecting learning analytics projects   with which they are familiar. Each will then address the   following specific questions related to data governance:    Who owns the data that are being analyzed Students,  instructors, administrators, learning designers What   permissions are necessary for data access What   documentation is required    To what extent do student and instructor privacy  concerns determine the nature and scope of a learning   analytics project What consultations are necessary   with stakeholders    Should the analysis of learning and teaching data be  considered as research To what extent should   learning analytics projects be subject to the   guidelines and controls of research ethics boards    For student facing analytics projects such as  dashboards and identification of learners at risk,   when does responsible facilitation of learning cross   the line to be seen as an intrusion on learners   privacy    From a technical point of view, what types of data can  be tracked and used in learning analytics How does   this correlate with the types of data that should be   tracked in order to inform appropriate conclusions   Audience input will be encouraged with a view to engaging an   open discussion of these and other issues.     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   21    2. TOPIC OF THE PANEL  This panel brings together representatives from four   universities from different countries and with distinct mandates   to discuss factors to be considered during the implementation of   a learning analytics initiative. Indeed, we will propose that the   issues are best discussed broadly across the institution in   advance of a learning analytics project, and should be framed in   the context of data governance.   The promise of analytics for higher education institutions has   been well articulated for several years  [3], but researchers have   noted that actual applications are limited  [6].  Recent initiatives    [2] in organizational level academic analytics focusing on   student retention, such as at the American Public University   System  [5], and on providing learners with dashboards to chart   their progress through online courses, such as the Course   Signals program (Purdue University, 2009), have great   potential to inform systemic improvements to online learning   experiences. Other approaches emphasize extracting evidence   of student behaviors inside learning systems, with a view to   informing iterative improvements in course design  [4]. This   methodology is analogous to the immediate and informal   feedback of face-to-face instruction, and has the added potential   benefit of making teaching and learning practice more   transparent, leading to the design of qualitatively different   online learning environments.   Methodological considerations are critical when considering   data analysis projects  [1]. Issues of data quality,   interpretability, ethics and privacy need to be understood and   addressed in order to draw valid and reliable inferences, before   applying this new knowledge in practical ways. The skills to   interpret data are not trivial. Neither are the skills to design   and develop tools for identifying and accessing data of interest.   As well, in order for learning analytics tools to contribute to the   improvement of pedagogical practices, evaluation measures and   feedback mechanisms are needed, where the results of the   analytic tool are measured in terms of whether and how much   they benefit instructors and course designers, and whether and   how much they lead to improved learning designs.   Furthermore, such new or revised teaching and learning design   practices must be evaluated with respect to whether they really   enhance student learning. Users of data also need mechanisms   that communicate whether the implemented learning analytics   techniques are useful for them and what further information   would help them to improve the overall quality of their   practice.   A wide variety of factors affect decision making about data   analysis in general, and learning analytics in particular, among   them data security and access, governance policies and   procedures, data volumes and visualization, data accuracy and   comprehensiveness, as well as user needs for information,   strategies for units and levels of analysis, and feedback   mechanisms for evaluating usefulness and interpretability. The   complexity of the relationships among organizational processes,   analytic tools and interpretive methods suggests careful design   and planning of academic analytics projects to ensure   appropriate, ethical and useful benefits.   3. REFERENCES  [1]. Boyd, D. (2010, April). Privacy and Publicity in the   Context of Big Data. WWW. Raleigh, NC. Retrieved   February 17, 2012 from   http://www.danah.org/papers/talks/2010/WWW2010.html    [2]. Brown, M. (2011). Learning Analytics: The Coming Third   Wave. In Educause Learning Initiative Brief. Retrieved   January 6, 2010, from   http://www.educause.edu/Resources/LearningAnalyticsTh  eComingThir/227287   [3]. Goldstein, P. J., & Katz, R. N. (2005).  Academic  analytics: The uses of management information and   technology in higher education. ECAR Research Study (vol   8). Retrieved January 22, 2012,from   http:/www.educause.edu/ers0508   [4]. Graf, S., Ives, C., Rahman, N., & Ferri, A. (2011). AAT   A tool for accessing and analysing students behaviour   data in learning systems. In Proceedings of the   International Conference on Learning Analytics and   Knowledge (LAK2011) (pp. 174-179). ACM Press,   February, Banff, Canada.    [5]. IBM (2012). Using data to boost student engagement and   retention. In Campus Technology. Retrieved January 22,   2012, from   http://campustechnology.com/whitepapers/2011/12/ibm_us  ing-data-to-boost-student-engagement-and-retention.aspx    [6]. Romero, C., & Ventura, S. (2007). Educational data   mining: A survey from 1995 to 2005. Expert Systems with   Applications, 33(1), 135-146.               22      "}
{"index":{"_id":"9"}}
{"datatype":"inproceedings","key":"Ferguson:2012:SLA:2330601.2330616","author":"Ferguson, Rebecca and Shum, Simon Buckingham","title":"Social Learning Analytics: Five Approaches","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"23--33","numpages":"11","url":"http://doi.acm.org/10.1145/2330601.2330616","doi":"10.1145/2330601.2330616","acmid":"2330616","publisher":"ACM","address":"New York, NY, USA","keywords":"21st century skills, SocialLearn, discourse analytics, educational assessment, learning analytics, learning how to learn, social learning, social learning analytics, transferable skills","Abstract":"This paper proposes that Social Learning Analytics (SLA) can be usefully thought of as a subset of learning analytics approaches. SLA focuses on how learners build knowledge together in their cultural and social settings. In the context of online social learning, it takes into account both formal and informal educational environments, including networks and communities. The paper introduces the broad rationale for SLA by reviewing some of the key drivers that make social learning so important today. Five forms of SLA are identified, including those which are inherently social, and others which have social dimensions. The paper goes on to describe early work towards implementing these analytics on SocialLearn, an online learning space in use at the UK's Open University, and the challenges that this is raising. This work takes an iterative approach to analytics, encouraging learners to respond to and help to shape not only the analytics but also their associated recommendations.","pdf":"Social Learning Analytics: Five Approaches     Rebecca Ferguson  Institute of Educational Technology   The Open University  Milton Keynes, MK7 6AA, UK   +44-1908-654956   r.m.ferguson@open.ac.uk   Simon Buckingham Shum  Knowledge Media Institute   The Open University  Milton Keynes, MK7 6AA, UK   +44-1908-655723   s.buckingham.shum@gmail.com     ABSTRACT  This paper proposes that Social Learning Analytics (SLA) can be   usefully thought of as a subset of learning analytics approaches.   SLA focuses on how learners build knowledge together in their   cultural and social settings. In the context of online social   learning, it takes into account both formal and informal   educational environments, including networks and communities.   The paper introduces the broad rationale for SLA by reviewing   some of the key drivers that make social learning so important   today. Five forms of SLA are identified, including those which   are inherently social, and others which have social dimensions.   The paper goes on to describe early work towards implementing   these analytics on SocialLearn, an online learning space in use at   the UKs Open University, and the challenges that this is raising.   This work takes an iterative approach to analytics, encouraging   learners to respond to and help to shape not only the analytics but   also their associated recommendations.    Categories and Subject Descriptors   K.3.1 [Computers and Education]: Computer Uses in   Education   collaborative learning, distance learning.   General Terms  Measurement, Design.   Keywords  social learning; learning analytics; discourse analytics; learning   how to learn; transferable skills; 21st century skills; educational   assessment; social learning analytics; SocialLearn   1. INTRODUCTION  The field of learning analytics has its roots in the appropriation   of business intelligence concepts by educational institutions: the   earlir terms academic analytics [1] and action analytics [2]   refer respectively to the capture and report of data by educational   administrators, and to the need for benchmarking to increase the   effectiveness of educational institutions. Learning analytics shift   the perspective from that of the institution gathering data about   learners in order to inform organisational objectives, to that of   providing new tools for the learner and teacher, drawing on   experience from the learning sciences with the intention of   understanding and optimizing not only learning but also the   environments in which it takes place.   As part of this shift to learner-centred design, we propose that   Social Learning Analytics (SLA) can be usefully thought of as a   subset of learning analytics, which draws on the substantial body   of work evidencing that new skills and ideas are not solely   individual achievements, but are developed, carried forward, and   passed on through interaction and collaboration. A socio-cultural   strand of educational research demonstrates how language is   itself one of the primary tools through which learners construct   meaning, and its use is influenced by the aims, feelings and   relationships of their users, all of which shift according to   context [3] (as will be seen, discourse and context are two foci of   the SLA we propose). Another strand of research emphasises that   learning cannot be understood by focusing solely on the   cognition, development or behaviour of individual learners;   neither can it be understood without reference to its situated   nature [4, 5].    As groups engage in joint activities, their success is related to a   combination of individual knowledge and skills, environment,   use of tools, and ability to work together. Understanding learning   in these settings requires us to pay attention to group processes   of knowledge construction  how sets of people learn together   using tools in different settings. The focus must be not only on   learners, but also on their tools and contexts.   Viewing learning analytics from a social perspective highlights   types of analytic that can be employed to make sense of learner   activity in a social setting. This does not require the development   of a completely new set of tools; this paper cites numerous   examples of related work in context. Instead, it groups a range of   pre-existing and new tools and approaches to form the basis of a   coherent set. In doing so, it identifies ways in which analytics   may be developed and implemented in order to identify social   behaviours and patterns that signify effective process in learning   environments. The aim is to use analytics not only to identify   these but also to render them both visible and actionable.   The paper is organized as follows. We introduce the broad   rationale for focusing on social learning (2), which is then   developed specific to several forms of analytic which are   inherently social (3), or which have social dimensions (4). We   then describe progress towards the implementation of these      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not  made or distributed for profit or commercial advantage and that copies bear  this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK12-29 April  2 May 2012, Vancouver, BC, Canada   Copyright 2012 ACM 978-1-4503-1111-3/12/04   $10.00        23    analytics in a social learning space (5), consider some of the   challenges that we are encountering (6), before concluding (7).   2. WHY SOCIAL LEARNING ANALYTICS  The focus on SLA reflects shifts in the broader cultural,   technological and business landscapes, which together are   reshaping the educational landscape platforms. We see these as a   set of drivers for the growing importance of online social   learning, and hence, for SLA.   2.1 Social media  No review of the forces shaping the educational landscape can   ignore the digital revolution. Only very recently have we had the   right infrastructural ingredients to provide almost ubiquitous   internet access in wealthy countries and mobile access in many   more. In addition, we now have user interfaces that have evolved   through intensive use, digital familiarity from an early age,   standards enabling interoperability and commerce across diverse   platforms, and scalable computing architectures capable of   servicing billions of real-time users, and mining that data. With   the rise of very large social websites such as Facebook, YouTube   and Twitter, plus the thousands of smaller versions and niche   applications for specific tasks and communities, we have   witnessed a revolution in the way that people think about online   interaction and publishing. Such social media platforms facilitate   the publishing, indexing and tracking of user-generated media,   provide simple to learn collaboration spaces, and enable a new   set of social gestures that are becoming ubiquitous, and   expected by the current generations: friending, following,   messaging, microblogging, liking, rating, etc.   Potential implication: As ubiquitous access to social networks   become a critical part of learners online identity, and an   expected part of learning platforms, social learning analytics   should provide tools to provoke learning-centric reflection on   how interpersonal relationships and interactions reflect   learning, or can be more effectively used to advance learning.   2.2 Open/free content and data   There has been a huge shift in expectations about access to   digital content. Learners expect increasingly to find reasonable   quality information on the Web for free, to the point where they   often feel aggrieved when confronted by a request for money, and   will seek free avenues first. Within education, the Open   Educational Resource (OER) movement has been a powerful   vehicle for making institutions aware of the value of making   quality learning material available, not only for free, but in   formats that promote remixing, in an effort to reap the benefits   seen in the open source software movement. This has by no   means proven to be a simple matter, since educational staff,   materials and institutions are different in important respects from   open source programmers, source code and developer networks,   but OER has made huge progress, and is gaining visibility at the   highest levels of educational policy.   Free and open learning resources are mirrored by efforts within   the open and linked data communities to make data open to   machine processing as well as human interpretation. This   requires both the shift in mindset by data owners (which OER   has had to effect within education), as well as the laying of   technological infrastructure to make it possible to publish data in   useful formats.    Potential implication: A consequence of the information   overload that now confronts learners is the need for more   effective filtering and navigation, and it is here that social   networks are playing an increasing role, as a means to maximize   the increasingly scarce resource at a learners disposal: focused   attention. SLA should augment learners capacities to build   effective social learning networks.   2.3 Society increasingly values participation  Technology is always appropriated to serve what people believe   to be their needs and values. Beyond what we can observe for   ourselves informally, there is a significant body of research   indicating that the period in which we find ourselves is moving   towards a set of values mirrored in the affordances of social   media. In 1997, the World Values Survey covered 43 societies,   representing 70% of the worlds population. Inglehart [6] argued   that the shift to postmaterialism (a finding from earlier surveys)   was confirmed and he offered a new postmodernization   framework. He suggested that modernization helped society   move from poverty to economic security, and that the success of   this move led to a shift in what people want out of life. In   postmodernity, as he used the term, people value autonomy and   diversity over authority, hierarchy, and conformity. According to   Inglehart, postmodern values bring declining confidence in   religious, political, and even scientific authority; they also bring   a growing mass desire for participation and self-expression. We   find these results interesting: on the one hand it is easy to   recognise this shift in wealthy nations, but this shift seems also   to be reflected even in the less developed regions surveyed,   where poverty is still clearly a daily reality.    Another perspective on the shift in social value is the view that,   since 1991, we have lived in the knowledge age  a period in   which knowledge, rather than labour, land or capital, has been a   key wealth-generating resource [7]. This shift has occurred   within a period when constant change in society has ben the   norm, and it is therefore increasingly difficult to tell which   specific knowledge and skills will be required in the future [8].   These changes have prompted an interest in knowledge-age   skills that will allow learners to become both confident and   competent designers of their own learning goals [9]. Accounts of   knowledge-age skills vary, but they can be broadly categorized as   relating to learning, management, people, information,   research/enquiry, citizenship, values/attributes and preparation   for the world of work [10]. From one viewpoint they are   important because employers are looking for problem-solvers,   people who take responsibility and make decisions and are   flexible, adaptable and willing to learn new skills [11, p5].   More broadly, knowledge-age skills are related not just to an   economic imperative but to a desire and a right to know, an   extension of educational opportunities, and a responsibility to   realize a cosmopolitan understanding of universal rights and   acting on that understanding to effect a greater sense of   community [12, p111].    Potential implication: Research evidencing the growing desire in   many societies for civic participation and self-expression   provides another driver for social learning. Another is within   education, with the perceived need to move away from a   curriculum based on a central canon of information, towards   learning that develops skills and competencies for coping with   24    complexity and novel challenges. SLA should augment learners   capacities to assess themselves on 21 st  Century skills.   2.4 Innovation depends on social connection  The conditions for online social learning are also related to the   pressing need for effective innovation in organisational life. In a   succinct synthesis of the literature, Hagel, et al. [13] argue that   social learning is the only way in which organisations can cope   with the unprecedented turbulence they now face. They invoke   the concept of pull as an umbrella term to signal some   fundamental shifts in the ways in which we catalyse learning and   innovation, and argue that the world is changing so rapidly that   useful knowledge/understanding (in contrast to data or   information) is rarely well codified, indexed or formalized, while   socially transmitted knowledge is growing in importance as a   source of timely, trustworthy insight. This leads them to   highlight quality of interpersonal relationships, tacit knowing,   discourse and personal passion as key capacities to foster, as we   move in business from mere transactional relationships, to   building and sustaining more meaningful relationships.    Potential implication: These business trends serve as another   driver for social learning, and invite opportunities for SLA to   augment personal and collective capacities by investigating how   we can make visible representations of quality of interpersonal   relationships, tacit knowing, discourse and personal passion.   2.5 Summary  We have reviewed some of the tectonic forces reshaping the   learning landscape. These are signals that many futures   analysts and horizon scanning reports on learning technology   have highlighted as significant. If taken together, these are   shaping a radically new context for learning, then by extension,   learning analytics must be reframed accordingly to place online   social interaction and the social construction of knowledge at the   heart of their models.    We now introduce five categories of analytic whose foci are   driven by the implications of the drivers reviewed above. The   first two categories are inherently social, while the other three   can be socialized, ie. usefully applied in social settings:    social network analytics  interpersonal relationships  define social platforms    discourse analytics language is a primary tool for  knowledge negotiation and construction    content analytics  user-generated content is one of  the defining characteristics of Web 2.0    disposition analytics  intrinsic motivation to learn is  a defining feature of online social media, and lies at the   heart of engaged learning, and innovation    context analytics  mobile computing is transforming  access to both people and content.   We do not present these five categories as an exhaustive   taxonomy, since this would normally be driven by, for instance,   a specific pedagogical theory or technological framework, in   order to motivate the category distinctions. We are not grounding   our work in a single theory of social learning, nor do we think   that a techno-centric taxonomy is helpful. The social learning   platform and analytics we are developing is in response to the   spectrum of drivers reviewed above, drawing on diverse   pedagogical and technological underpinnings which will be   introduced with each category.   3. INHERENTLY SOCIAL TYPES OF  LEARNING ANALYTIC   3.1 Social learning network analytics  Networked learning uses ICT to promote connections between   learners, tutors, communities and resources [14]. These networks   consist of actors  both people and resources  and the relations   between them. A tie describes the relationship between these   actors and can be classified as strong or weak, depending on its   frequency, quality or importance [15]. People make use of weak   ties with people they trust when accessing new knowledge or   engaging in informal learning, and go on to make increasing use   of strong ties with trusted individuals as they deepen and embed   their knowledge [16].   Social network analysis investigates ties, relations, roles and   network formations, and a social learning network analysis is   concerned with how these are developed and maintained to   support learning [15]. Because of its focus on the development of   relationships, and its view that technology forms part of this   process, this type of analysis offers the possibility of identifying   interventions that are likely to increase a networks potential to   support the learning of its actors.   Social network analysis can be approached from the perspective   of an individual or of the entire network. An egocentric approach   may identify the people who support an individuals learning, the   origin of conflicts in understanding, and some of the contextual   factors that influence learning. On the other hand, a whole-  network view provides insight into the interests and practices of   a set of people, identifying elements that hold the network   together [17]. It also has the potential to help with the   identification of groupings within a network that can support   learning, for example communities and affinity groups [18, 19].   As social network analysis is developed and refined in the   context of social learning, it has the potential to be combined   with other types of social learning analytic in order to define   what counts as a learning tie and thus to identify interactions   which promote the learning process. It also has the potential to   be extended in order take more account of socio-material   networks, identifying and, where appropriate, strengthening and   developing indirect relationships between people which are   characterised by the ways in which they interact with the same   objects of knowledge [20].   3.2 Social learning discourse analytics  The ties between learners in a network are typically established   or strengthened by their use of dialogue. These interactions can   be studied using the various forms of discourse analysis that offer   ways of understanding the large amounts of text generated in   online courses and conferences. For example, Schrire [21] used  discourse analysis to understand the relationship between the   interactive, cognitive and discourse dimensions of online   interaction, examining initiation, response and follow-up (IRF)   exchanges. More recently, Lapadat [22] has applied discourse   analysis to asynchronous discussions between students and   tutors, showing how groups of learners create and maintain   community and coherence through the use of discursive devices.   25    Corpus linguistics, the study of language based on examples of   real-life use, is a method of discourse analysis that relies heavily   on electronic tools and computer processing power. This method   employs software to facilitate quantitative investigation of vast   corpora including millions of words of both speech and text [23].    Educational success and failure have been related to the quality   of learners educational dialogue [24]. Social learning discourse   analytics can be employed to analyse, and potentially to   influence, dialogue quality. The ways in which learners engage   in dialogue indicate how they engage with the ideas of others,   how they relate those ideas to their personal understanding and   how they explain their own point of view. Mercer and his   colleagues distinguished three social modes of thinking used by   groups of learners in face-to-face environments: disputational,   cumulative and exploratory talk [25-28]. Disputational dialogue   is characterised by disagreement and individual decisions; in   cumulative dialogue speakers build on the contributions of others   without critiquing or challenging them. Educators typically   consider exploratory dialogue the most desirable because it   involves speakers explaining their reasoning, challenging ideas,   evaluating evidence and developing understanding together.   Learning analytics researchers have built on this work to provide   insight into textual discourse in online learning [29, 30],   providing a bridge to the world of social learning analytics.   A related approach to social learning discourse analytics employs   a structured deliberation/argument mapping platform to study   what learners are paying attention to, what they focus on, which   viewpoints they take up, how learning topics are distributed   amongst participants, how learners are linked by semantic   relationships such as support and challenge, and how learners   react to different ideas and contributions [31]. This approach to   overlaying discourse network models on social network models   exemplifies what makes social learning analytics distinctive   from generic social network analytics, which examine topology   but take no account of the quality of stakeholder interactions.   4. SOCIALIZED LEARNING ANALYTICS   In this section, we consider three kinds of analytic, which   although meaningful for an isolated learner who is making no use   of interpersonal connections or social media platforms, take on   significant new dimensions in the context of social learning.   4.1 Social learning content analytics  Content analytics is used here as a broad heading for the   various automated methods used to examine, index and filter   online media assets for learners. (Note that this not identical to   content analysis, which is concerned with description of the   latent and/or manifest elements of communication [32].) These   analytics may be used to provide recommendations of resources   tailored to the needs of an individual or a group of learners. This   is a very fast-moving field, and the state of the art in textual and   video information retrieval tools is displayed annually in   competitions such as the Text Retrieval Conference [see 33 for a   review].   One example is visual similarity search, which uses features of   images in order to find material that is visually related, thus   supporting navigation of educational materials in a variety of   ways, including identifying the source of an image, finding items   that offer different ways of understanding a concept, or finding   other content in which a given image or movie frame is used   [34].    This takes on a social learning aspect when it makes use of the   tags, ratings and other data supplied by learners. An example is   iSpot, a citizen science social media site that helps learners to   identify anything in the natural world [35]. When a photo is first   uploaded to the site, it usually has little to connect it with other   information. The addition of a possible identification by another   user ties the image to other sets of data held externally. In the   case of iSpot, this analysis is not solely based on the by-products   of interaction; each users reputation within the network is used   to weight the data that they add. This suggests one way in which   content analytics can be combined with social network analytics   to support learning.   Other approaches to content analytics are more closely aligned   with content analysis. These involve examination of the latent   elements that can be identified within transcripts of exchanges   between people learning together online. This method has been   used to investigate a variety of issues related to online social   learning, including collaborative learning, presence and online   cooperation [36]. These latent elements of interpersonal   exchanges could also support sentiment analysis, revealing   learners emotions such as happiness and frustration.    It is also possible to draw on the manifest information about user   activity and behaviour that is provided by tools such as Google   Analytics and userfly.com as well as by the tools built into   virtual learning environments (VLEs) such as Moodle and   Blackboard. This is the approach taken by LOCO-Analyst, which   uses content analysis to establish and investigate semantic   relations between different learning resources and to provide   feedback for content authors and teachers that can help them to   improve their online courses [37].    4.2 Social learning disposition analytics  Learners who are prepared to learn and are open to new ideas   have the potential to make good use of these resources and tools.   A well established research programme has identified,   theoretically, empirically and statistically, a seven-dimensional   model of learning dispositions, termed learning power [38].   These dispositions can be used to render visible the complex   mixture of experience, motivation and intelligences that make up   an individuals capacity for lifelong learning and influence   responses to learning opportunities [39]. Learning dispositions   are not learning styles, which have been critiqued on a variety   of grounds, including lack of contextual awareness [40]. In   contrast, an important characteristic of learning dispositions is   that they have been validated as varying according to a range of   variables [41]. As detailed in [41], a learning analytics platform   and visual analytic has been developed to model and assess such   dispositions and transferable skills. This visual analytic is used   to reflect back to learners their self-perception on these   dimensions, providing an explicit language for describing   dispositions, catalysing changes in their engagement, activities   and approach to learning.   From a social learning perspective, three elements of disposition   analytics are particularly important. Firstly, they draw learners   attention to the importance of relationships and interdependence   as one of the seven key learning dispositions. Secondly, they can   be used to support learners as they reflect on their ways of   26    perceiving, processing and reacting to learning interactions.   Finally, they play a central role in an extended mentoring   relationship. This type of relationship has an important role in   online social learning, especially when learning is informal and   not teacher-led. Mentors motivate, encourage, challenge and   counsel learners, and can also provide opportunities to rehearse   arguments and increase understanding [42, 43].    4.3 Social learning context analytics   All these types of social learning analytic can be applied in a   wide variety of contexts, including formal settings such as   universities, informal contexts in which learners choose both the   process and the goal of their learning [44] and in the many   situations in which learners are using mobile devices [45]. In   some cases, many learners are simultaneously engaged in the   same activity, and in other cases learning takes place in   asynchronous environments, where the assumption is that is that   learners will be participating at different times [30]. They may be   learning alone, in a network, in an affinity group, in communities   of inquiry, communities of interest or communities of practice   [18, 46-48].   Context analytics are the analytic tools that expose, make use   of or seek to understand these contexts. These analytics may be   used alone, or may be employed as higher-level tools, pulling   together data produced by other analytics. For example, if   network analysis indicates that student Rebecca is on the edge of   a community, and dispositions analysis shows that she is   currently working on her collaboration skills, then a context-  focused recommendation might suggest that she could join a   teamwork skills group and use analytics visualizations to monitor   her position within the group. Several weeks later, she might be   prompted to reflect on her collaboration skills and to rate the   group. She might receive this prompt directly from the system, or   the system could recommend her teacher, mentor or group leader   to engage with her and to make the recommendation.   5. DESIGN & IMPLEMENTATION  Having identified different types of social learning analytic, the   challenge is to employ these to analyse learners behaviour and   to offer visualizations and recommendations that can be shown to   spark and support learning. This section focuses on progress   towards the implementation of these analytics in a social learning   space developed by The Open University, a UK-based university   with a strong emphasis on open and distance learning.    SocialLearn is a social media space tuned for learning. It has   been designed to support online social learning by helping users   to clarify their intention, to ground their learning and to engage   in learning conversations [49]. The systems architecture   includes a Recommendation Engine, a pipeline designed to   process data and output it in a form for analysis by SocialLearn   recommendation web services.   The second element of SocialLearns architecture is the Identity   Server that supplies, with the learners informed consent, data to   the Recommendation Engine. These data include learners   profiles and activities within SocialLearn, selected elements of   their activity at The Open University, and selected elements of   their activity and interactions on social media sites such as   LinkedIn, Twitter and sites employing OpenID. The SocialLearn   Analytics and Delivery Channels depend on the Identity Server to   maintain a unified user profile.   The final element of SocialLearns architecture is the SL   Delivery Channel, which includes sites for both input and output.   Data are collected, with the learners informed consent, from use   of the SocialLearn website, from use of the SocialLearn   backpack (browser toolbar) while elsewhere on the web, from   use of SocialLearn applications embedded on external sites and,   where applicable, from calls on the SocialLearn application   programming interface (API). At the same time, data that has   been analysed by the Recommendation Engine may be presented   to learners in the form of recommendations or visualizations   available on the SocialLearn website, via the SocialLearn   backpack, within embedded applications or by ways of calls on   the API. Reactions to these visualizations and recommendations,   together with options for feedback by learners, make this an   iterative process because these responses are fed back via the SL   Delivery Channel and influence subsequent output.   The architecture is designed to be flexible, so that new   algorithms can be added at any time, and analytics can be trialed,   developed, combined or set aside without disruption.   Sections 4.1-4.6 describe progress towards the implementation of   different types of social learning analytic, including work carried   out at The Open University and elsewhere that supports the   development of social learning analytics, recommendations and   visualizations. Work on some types of social learning analytic is   still at the stage of planning how work carried out elsewhere   might be adapted. In other cases, mock-ups and wireframes are   in place or pilot studies are underway.   5.1 Implementing social learning network  analytics   In the case of social learning network analytics, the SocialLearn   team is considering the possibilities offered by SNAPP (Social   Networks Adapting Pedagogical Practice), a freely available   network visualization tool that analyses forum contributions and   presents them as a network diagram. Its architects identify uses   for such diagrams from the point of view of teachers, including:    identifying disconnected students    identifying key information brokers within a class    indicating the extent to which a learning community is  developing within a class [50]   In the case of SocialLearn, the intention is to deploy social   learning network analytics to exploit data in the Identity Server,   in order to support both individual and group recommendations.   For example, individuals might see:    One of your key search terms is learning analytics. This  has been mentioned five times in the Future Developments   thread. View thread    John Smith has been identified as a key information broker  in your network, View Johns most recent posts    Ten people you have replied to list social learning as a key  search term. Add this to your key search terms   In learning groups that are not formally led by a teacher,   members may share responsibility for welcoming newcomers,   engaging all members and encouraging meaningful participation.   Social learning network feedback for a group or moderator will   seek to use what is known about effective group structure and   dynamics and feed this back for reflection [51]. For example:   27     Research shows that effective learning groups tend to be  structured like this <network diagram> whereas your group   currently looks like this <group diagram>.   5.2 Implementing social learning discourse  analytics   In order to support meaningful participation, SocialLearn is   developing two sets of discourse analytics  the first based on the   work of Neil Mercer and his colleagues around exploratory   dialogue [27], and the second building on development of   Contested Collective Intelligence and Concept Mapping to   scaffold structured deliberation and argument mapping [52].   Key characteristics of exploratory dialogue include challenge,   evaluation, reasoning and extension. Initial research suggests that   these are signaled in forum interaction by key words and phrases   [29]. For example: alternative, but if and I dont believe   suggest challenge; good point, important and how much   suggest evaluation; next step, its like and relates to suggest   extension, and does that mean, my understanding and take   your point suggest reasoning. Figure 1 shows how a   visualization of these elements of dialogue could be presented to   learners, together with recommendations.   The coloured shapes in Figure 1 indicate comparative levels of   use of different types of dialogue. In this case, indicators of   reasoning, evaluation and extension appeared several times   within the learners discussion and are represented by green   squares. Only one challenge was detected, and this lower level is   represented by a yellow circle. The final sentence, Positive   challenges suggests ways of increasing indicators of   exploratory dialogue.      Figure 1: Visualization of learners use of indicators of   exploratory dialogue   This example focuses on a single learner. A group version of the   visualization could be used to represent the dialogue of a group   or a thread, with the aim of achieving a more widespread shift in   the quality of the learning dialogue.   Explicit semantic networks provide a computational system with   a more meaningful understanding of the relationships between   ideas than natural language. Following the established   methodological value of Concept Mapping [53], the mapping of   issues, ideas and arguments extends this to make explicit the   presence of more than one perspective and the lines of reasoning   associated with each.    In a comprehensive review of computer-supported argumentation   [54], Scheuer et al concluded that studies have demonstrated   more relevant claims and arguments disagreeing and   rebutting other positions more frequently and engaging in   argumentation of a higher formal quality. However, appropriate   tools need to be part of an effective learning design:   The overall pedagogical setup, including sequencing of   activities, distributions of roles, instruction on how to use   diagramming tools, usage of additional external   communication tools, and collaboration design, has an   influence on learning outcomes [54]   On this basis, Cohere is being developed to interoperate with   SocialLearn. As preliminary results show [31], this holds the   promise of giving the platform access to proxy indicators of   participants attitudes towards the topic under discussion, and of   the roles they play within the group (e.g. forging meaningful   links between peers contributions, or a tendency to challenge   others). This provides the representational basis to automate   recommendations that encourage new approaches to a given   subject, either by providing links to resources that challenge or   extend learners point of view, or by providing links to other   groups talking about the same subject or resources but in   different ways.   5.3 Implementing social learning content  analytics   When viewing online resources, SocialLearns Backpack  a   toolbar of apps and resources  can be used on any Internet site.    The Backpack currently includes the basic components of social   learning content analytics. Clicking on the Backpacks light bulb   icon provides the option of viewing the keywords, hotlinks or   images connected with the open web page (as in the large box on   the right of Figure 2). This information about images can be   combined with visual similarity search to identify and   recommend other resources that make use of these images, for   instance:      Figure 2: The SocialLearn Backpack open at the foot of a    BBC News page, showing a list of images on the page    This image appears to be The Mona Lisa, and is used  twice in this Renaissance 101 lecture webcast [view]    This image appears to be Steve Jobs, and is used in the  following blogs by academics in Design faculties   [view]   As SocialLearn develops, it will be possible to refine these   recommendations, based on the number of users following or   recommending links or on the relevance of key words on a site to   28    the key words associated with individual learners or groups of   learners. In the case of learners who are developing the learning   dispositions of resilience and critical curiosity  (the desire and   capacity to be taken out of ones comfort zone, and to dig beneath   the surface), these analytics could recommend online resources   that both stretch a learner to a new level and which are rated as   rewarding investigative skills.   5.4 Implementing social learning disposition  analytics   Theoretical and empirical evidence in the learning sciences   substantiates the view that deep engagement in learning is a   function of a complex combination of learners identities,   dispositions, values, attitudes and skills. When these are fragile,   learners struggle to achieve their potential in conventional   assessments, and critically, are not prepared for the novelty and   complexity of the challenges they will meet in the workplace,   and the many other spheres of life which require personal   qualities such as resilience, critical thinking and collaboration   skills.  As detailed in an accompanying LAK paper [41], learning   dispositions can be modelled as a multi-dimensional construct   called Learning Power, currently assessed by learner self-report   via a web questionnaire called ELLI (Effective Lifelong Learning   Inventory), whose data warehouse platform supports a range of   analytics. ELLI has been extensively validated, and is now being   piloted within The Open University [55]. ELLI generates a spider   diagram visual analytic which is used to support self-reflection   and change. Figure 3 suggests how these meta-cognitive   processes could be supported within SocialLearn.   The ELLI profile generated by completing the self-report   questionnaire appears at the top of Figure 3. In this case, the   learner saw herself as fairly strong on changing and learning,   learning relationships and meaning making, but her resilience   was low at that point. The central text indicates that, within a   mentored discussion, she agreed that she would work on this   area. Working to develop resilience involves accepting that   learning can be hard for everyone, taking on a challenge and   persisting even when the outcome and the way ahead are   uncertain. The ELLI Spider at the foot of Figure 3 visualizes her   progress since the mentored discussion. Red triangles would   indicate no activity on a dimension, yellow squares signal some   activity and green circles indicate the learner has been very   active in an area. The ELLI Spider is fed by self-report data (for   example, within a learning blog [56]) and by data about activity   and interactions that is processed within the Recommendation   Engine and provided via the Identity Server and the Delivery   Channel.    We are now operationalising the dimensions against candidate   activity traces that could signify them. For example, indicators of   growing resilience that could be fed back to the learner,   rendering the Recommendation Engines rationale transparent:    You seem to be making progress in building your  learning resilience. Last time you declared yourself   Stuck on a path you did not return to it. This time you   returned to Step 3 on Photosynthesis 101 after a week   and, after requesting help, solved the problem.         Figure 3: ELLI Profile (top) visualizing results of most   recent self-report questionnaire. ELLI Spider and summary   (bottom) highlighting recent work on dispositions   5.5 Implementing social learning context  analytics   Within SocialLearn, dispositions analysis and subsequent activity   are among the data items that feed into the Identity Server and   Delivery Channel, Together, the server and channel provide data   about a learners current context, including goals, activities,   group membership and learning roles. A future SocialLearn app   will make use of this data, adding geolocation to the mix  to   produce recommendations tailored to the learner.   Figure 4 shows a mock-up of the SocialLearn app, currently   under development, which will recommend and provide access to   learning materials in response to search terms. The app will   allow resources to be rated and recommended to individuals or to   groups. If users choose to make their location data available, this   can be used to influence recommendations, For example, if   Simon is working on Climate Change the app might suggest a   podcast on coastal defences when he visits a seaside resort, and   could provide a map showing a local site where Simon would be   able to view the effects of erosion.   29       Figure 4: Mock-up produced by designer of SocialLearn app   making use of social learning context analytics   5.6 Different dashboard views  Because SocialLearn is designed to work in a wide variety of   contexts, users are likely to move between roles while using it.   At some points they will be learners, at others mentors or   teachers and at others group leaders or administrators. In many   cases this will involve tailoring recommendations and   visualizations to take into account these different roles. The   intention is to provide different dashboard views of analytics.   Figure 5 (below) shows what an individual learners dashboard   could look like  providing Kris Mann with an overview of   different analytics and recommendations. If Kris were mentoring   someone, he would also have agreed access to elements of their   dashboard and could rate the system-generated recommendations   and add his own. As a teacher, he would need an overview of his   pupils analytics and recommendations, with clear visualizations   and teacher recommendations helping him to find his way   through these. In the role of group leader or administrator he   would need an overall view of group activity and dialogue,   without needing a breakdown of individual learners activities.   The challenge is to provide sufficient dashboard options to meet   users needs without overburdening them with possibilities.                   Figure 5: Mock-up of SocialLearn dashboard for an individual learner   30    6.  CHALLENGES AND POSSIBILITIES  All the social learning analytics described here are currently   under development. An initial and ongoing challenge is to gain   learners informed consent to their data being used to support   these analytics. Data harvesting on websites generally goes   unnoticed, it is often only by looking at the list of cookies stored   on our computer that we realise how much information about our   activity is being gathered, analysed, bought and sold. In the   context of education, analytics are likely to include sensitive   information about identity, status, background and achievements.   Ethical use therefore involves making users aware of the data   that is being collected, how it is being used and who has access   to it. This is difficult to do clearly and concisely, giving users   sufficient privacy options without overwhelming them.   At this early stage, a second challenge is to experiment with and   refine these analytics, while continuing to provide a supportive   experience for learners. In the case of social learning discourse   analytics, for example, the correlation of words and phrases with   elements of exploratory dialogue needs to be investigated in   more detail. In the case of social learning dispositions analytics,   we need to be clear which levels of activity should prompt colour   changes in the visualization, signaling a move from low to high   levels of activity. Each area of social learning analytics requires   further research in order to optimise support for learners.A final   challenge is to ensure that these analytics, recommendations and   visualizations spark and support learning. There is a danger that   learners could be overwhelmed and discouraged by the amount of   information presented to them, confused by being presented with   too many visualizations, or misled by system-generated analytics.   The SocialLearn research programme therefore works in the case   of each analytic from a theory of how learning can be triggered or   improved. It then develops an appropriate analytic and monitors   what happens when it is implemented, looking not only for the   predicted positive changes, but also for any significant changes.   At this stage, the challenge is to improve on or refine the analytic   and how it is presented to learners.    7. CONCLUSION   Social learning analytics make use of data generated by learners   online activity in order to identify behaviours and patterns within   the learning environment that signify effective process. The   intention is to make these visible to learners, to learning groups   and to teachers, together with recommendations that spark and   support learning. In order to do this, these analytics make use of   data generated when learners are socially engaged. This   engagement includes both direct interaction  particularly   dialogue  and indirect interaction, when learners leave behind   ratings, recommendations or other activity traces that can   influence the actions of others. Another important source of data   consists of users responses to these analytics and their   associated visualizations and recommendations   At present, we are focusing on the five broad categories of social   learning analytic described in this paper: network analytics,   discourse analytics, content analytics, dispositions analytics and   context analytics. The Open University is currently developing   these within SocialLearn, which provides a technical architecture   enabling different analytics and recommenders to be deployed.   Their initial deployment in 2011-12 is part of a research   programme at the university, focused on the effective use of   social learning analytics through evaluation of both their use and   their effects. In addition, the research programme is beginning to   address some of the important challenges relating to the ethical   use of data to support learning.   8. ACKNOWLEDGMENTS  We gratefully acknowledge The Open University for making this   work possible by resourcing the SocialLearn project.   9. REFERENCES  [1] Campbell, J. P., DeBlois, P. B. and Oblinger, D. G.,   Academic Analytics: A New Tool for a New Era.   EDUCAUSE Review, 42, 4 (July/August), (2007), 40-57.   [2] Norris, D., Baer, L., Leonard, J., Pugliese, L. and Lefrere,   P., Action analytics: measuring and improving performance   that matters in higher education. EDUCAUSE Review, 43,   1, (2008).   [3] Wells, G. and Claxton, G., Sociocultural perspectives on   the future of education. In: G. Wells and G. Claxton (Eds.),   Learning for Life in the 21st Century. Blackwell, Oxford,   2002.   [4] Wertsch, J. V., Voices of the Mind: A Sociocultural   Approach to Mediated Action. Harvester Wheatsheaf,   London, 1991.   [5] Gee, J. P., Thinking, learning and reading:  the situated   sociocultural mind. In: D. Kirshner and J. A. Whitson   (Eds.), Situated cognition: social, semiotic and   psychological perspectives. Lawrence Erlbaum Associates,   London, 1997.   [6] Inglehart, R., Modernization and Postmodernization.   Cultural, Economic, and Political Change in 43 Societies.   Princeton University Press, New Jersey, 1997.   [7] Savage, C. M., Fifth Generation Management: Co-  Creating through Virtual Enterprising, Dynamic Teaming,   and Knowledge Networking. Butterworth-Heinemann,   1996.   [8] Lyotard, J. F., The Postmodern Condition. Manchester   University Press, manchester, 1979.   [9] Claxton, G., Education for the learning age: a sociocultural   approach to learning to learn. In: G. Wells and G. Claxton   (Eds.), Learning for Life in the 21st Century. Blackwell,   Oxford, 2002.   [10] Futurelab, Developing and Accrediting Personal Skills and   Competencies. 2007. http://archive.futurelab.org.uk   [11] Qualifications & Curriculum Authority, Futures: Meeting   the Challenge. 2007. http://dera.ioe.ac.uk/5529   [12] Willinsky, J., Just say know Schooling the knowledge   society. Educational Theory, 55, 1, (2005), 97-111.   [13] Hagel, J., Seely Brown, J. and Davison, L., The Power of   Pull. Basic Books, New York, 2010.   [14] Jones, C. and Steeples, C., Perspectives and issues in   networked learning. In: C. Steeples and C. Jones (Eds.),   Networked Learning: Perspectives and Issues. Centre for   Studies in Advanced Learning Technology, Lancaster,   2003.   31    [15] Granovetter, M. S., The strength of weak ties. The   American Journal of Sociology, 78, 6, (1973), 1360-1380.   [16] Levin, D. Z. and Cross, R., The strength of weak ties you   can trust: the mediating role of trust in effective knowledge   transfer. Management Science, 50, 11, (2004), 1477-1490.   [17] Haythornthwaite, C. and de Laat, M., Social networks and   learning networks: using social network perspectives to   understand social learning. In: 7th International   Conference on Networked Learning (Aalborg, Denmark (3-  4 May), 3-4 May, 2010)   [18] Gee, J. P., Situated Language and Learning: A Critique of   Traditional Schooling. Routledge, New York, 2004.   [19] Goodfellow, R., Virtual Learning Communities: A Report   for the National College for School Leadership. 2003.   http://kn.open.ac.uk/public/document.cfmdocid=2778   [20] Cetina, K. K., Sociality with Objects: Social Relations in   Postsocial Knowledge Societies Theory, Culture & Society,   14, 4, (1997), 1-30.   [21] Schrire, S., Interaction and cognition in asynchronous   computer conferencing. Instructional Science, 32, 6,   (2004), 475-502.   [22] Lapadat, J. C., Discourse devices used to establish   community, increase coherence, and negotiate agreement in   an online university course. The Journal of Distance   Education, 21, 3, (2007), 59-92.   [23] O'Halloran, K., Investigating argumentatio in reading   groups: combining manual qualitative coding and   automated corpus analysis tools. Applied Linguistics, 32, 2,   (2011), 172-196.   [24] Mercer, N., Sociocultural discourse analysis: analysing   classroom talk as a social mode of thinking. Journal of   Applied Linguistics, 1, 2, (2004), 137-168.   [25] Mercer, N., The Guided Construction of Knowledge: Talk   amongst Teachers and Learners. Multilingual Matters Ltd,   Clevedon, 1995.   [26] Mercer, N., Words & Minds: How We Use Language To   Think Together. Routledge, London, 2000.   [27] Mercer, N. and Littleton, K., Dialogue and the   Development of Children's Thinking. Routledge, London   and New York, 2007.   [28] Mercer, N. and Wegerif, R., Is 'exploratory talk' productive   talk In: P. Light and K. Littleton (Eds.), Learning with   Computers: Analysing Productive Interaction. Routledge,   London and New York, 1999.   [29] Ferguson, R. and Buckingham Shum, S., Learning analytics   to identify exploratory dialogue within synchronous text   chat. Proc. 1st International Conference on Learning   Analytics and Knowledge (Banff, 27 Mar-1 Apr, 2011).   ACM: NY.   [30] Ferguson, R., The Construction of Shared Knowledge   through Asynchronous Dialogue. PhD, The Open   University, Milton Keynes. http://oro.open.ac.uk/19908   2009.   [31] De Liddo, A., Buckingham Shum, S., Quinto, I., Bachler,   M. and Cannavacciuolo, L., Discourse-centric learning   analytics. Proc. 1st International Conference on Learning   Analytics and Knowledge (Banff, 27 Mar-1 Apr, 2011).   ACM: NY.   [32] Potter, W. J. and Levine-Donnerstein, D., Rethinking   validity and reliability in content analysis. Journal of   Applied Communication Research, 27, 3, (1999), 258-285.   [33] Little, S., Llorente, A. and Rger, S., An overview of   evaluation campaigns in multimedia retrieval. In: H.   Mller, P. Clough, T. Deselaers and B. Caputo (Eds.),   ImageCLEF: Experimental Evaluation in Visual   Information Retrieval. Springer-Verlag, Berlin/Heidelberg,   2010.   [34] Little, S., Ferguson, R. and Rutger, S., Navigating and   Discovering Educational Materials through Visual   Similarity Search. Proc. World Conference on Educational   Multimedia, Hypermedia and Telecommunications   (EDMEDIA) (Lisbon, Portugal, 27 June - 1 July, 2011).   [35] Clow, D. and Makriyannis, E., iSpot Analysed:   Participatory Learning and Reputation. Proc. 1st   International Conference on Learning Analytics and   Knowledge (Banff, 27 Mar-1 Apr, 2011). ACM: NY.   [36] de Wever, B., Schellens, T., Vallcke, M. and van Keer, H.,   Content analysis schemes to analyze transcripts of online   asynchronous discussion groups: a review. Computers &   Education, 46, 1, (2006), 6-28.   [37] Jovanovic, J., Gaevic, D., Brooks, C., Devedic, V.,   Hatala, M., Eap, T. and Richards, G., LOCO-Analyst:   semantic web technologies in learning content usage   analysis. International Journal of Continuing Engineering   Education and Life Long Learning 18, 1, (2008), 54-76.   [38] Deakin Crick, R., Learning how to learn: the dynamic   assessment of learning power. The Curriculum Journal, 18,   2, (2007), 135-153.   [39] Deakin Crick, R., Broadfoot, P. and Claxton, G.,   Developing an effective lifelong learning inventory: the   ELLI project. Assessment in Education: Principles, Policy   & Practice, 11, 3, (2004), 247-272.   [40] Coffield, F., Moseley, D., Hall, E. and Ecclestone, K.,   Should We Be Using Learning Styles What Research Has   To Say to Practice. Learning and Skills Research Centre,   London, 2004.    [41] Buckingham Shum, S. and Deakin Crick, R., Learning   Dispositions and Transferable Competencies: Pedagogy,   Modelling and Learning Analytics. Proc. 2nd International   Conference on Learning Analytics & Knowledge   (Vancouver, 29 Apr-2 May, 2012). ACM Press: NY.   [42] Anderson, E. M. and Shannon, A. L., Towards a   conceptualisation of mentoring. In: T. Kerry and A. S.   Mayes (Eds.), Issues in mentoring. Routledge, London,   1995.   [43] Ferguson, R., The Integration of Interaction on Distance-  Learning Courses. MSc (RMet) Dissertation, The Open   University, Milton Keynes, 2005.   [44] Vavoula, G., KLeOS: A Knowledge and Learning   Organisation System in Support of Lifelong Learning. PhD,   University of Birmingham, Birmingham, 2004.   32    [45] Sharples, M., Taylor, J. and Vavoula, G., Towards a   Theory of Mobile Learning. In: mLearn 2005 conference   (Cape Town, South Africa, 2005).   [46] Jones, A. and Preece, J., Online communities for teachers   and lifelong learners: a framework for comparing   similarities and identifying differences in communities of   practice and communities of interest. International Journal   of Learning Technology, 2, 2-3, (2006), 112-137.   [47] Lipman, M., Thinking in Education. Cambridge University   Press, Cambridge, 2003.   [48] Wenger, E., Communities of Practice: Learning, Meaning   and Identity. Cambridge University Press, Cambridge,   1998.   [49] Ferguson, R. and Buckingham Shum, S., Towards a social   learning space for open educational resources. In: A.   Okada, T. Connolly and P. Scott (Eds.), Collaborative   Learning 2.0 - Open Educational Resources. IGI Global,   Hershey, PA, 2012, in press.   [50] Bakharia, A., Heathcote, E. and Dawson, S., Social   networks adapting pedagogical practice: SNAPP. In: Same   Places, Different Spaces. ascilite 2009 (Auckland, 2009).   [51] Haythornthwaite, C., Learning relations and networks in   web-based communities. International Journal of Web   Based Communities, 4, 2, (2008), 140-158.   [52] De Liddo, A., Sndor, A. and Buckingham Shum, S.,   Contested Collective Intelligence: Rationale, Technologies,   and a Human-Machine Annotation Study. Computer   Supported Cooperative Work, (2012, in press).   [53] Novak, J. D., Learning, creating, and using knowledge:   concept maps as facilitative tools in schools and   corporations. Lawrence Erlbaum Associates, Mahwah: NJ,   1998.   [54] Scheuer, O., Loll, F., Pinkwart, N. and McLaren, B. M.,   Computer-supported argumentation: A review of the state   of the art. International Journal of Computer-Supported   Collaborative Argumentation, 5, 1, (2010), 43-102.   [55] Edwards, C., Embedding reflective thinking on approaches   to learning - moving from pilot study to developing   institutional good practice. In: 16th Annual Conference of   the Education, Learning Styles, Individual Differences   Network (29 June-1 July) (Antwerp, 2011)   [56] Ferguson, R., Buckingham Shum, S. and Deakin Crick, R.,   EnquiryBlogger  Using Widgets To Support Awareness   and Reflection in a PLE Setting. In: 1st Workshop on   Awareness and Reflection in Personal Learning   Environments in conjunction with the PLE Conference   2011. July 11 (Southampton, United Kingdom, 2011)         33      "}
{"index":{"_id":"10"}}
{"datatype":"inproceedings","key":"Suthers:2012:MCS:2330601.2330618","author":"Suthers, Dan and Chu, Kar-Hai","title":"Multi-mediated Community Structure in a Socio-technical Network","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"43--53","numpages":"11","url":"http://doi.acm.org/10.1145/2330601.2330618","doi":"10.1145/2330601.2330618","acmid":"2330618","publisher":"ACM","address":"New York, NY, USA","keywords":"community structure, distributed learning, networked learning, social network analysis, socio-technical networks","Abstract":"Digital environments for networked learning and professional networks may not comprise one community: identification of clusters of affiliated groups of participants that potentially constitute embedded communities is an empirical matter, and one of interest to managers of large learning and professional networks. Also, these socio-technical networks are typically multi-mediated, in that they offer multiple means of participation, each with their own interactional affordances. Different communities may be using the multiple media in different ways. We have developed an analytic framework for extracting events from log files and representing interaction and affiliations at different granularities as needed for analysis. In this paper we show how bimodal networks of actors and media artifacts can be constructed in which directed arcs relate actors to the artifacts they read, write or edit, and how the resulting graphs can be used to detect community structures that extend across different media. We illustrate these ideas with a study that characterizes community structure within the Tapped In network of educational professionals, and how the associations between members of this network are distributed across media (chat rooms, discussion forums and file sharing).","pdf":"Multi-mediated Community Structure    in a Socio-Technical Network  Dan Suthers   Information and Computer Sciences  University of Hawaii at Manoa   Honolulu, HI 96822  1.808.956.3890   suthers@hawaii.edu   Kar-Hai Chu  Communication and Information Sciences   University of Hawaii at Manoa  Honolulu, HI 96822   karhai@hawaii.edu     ABSTRACT  Digital environments for networked learning and professional  networks may not comprise one community: identification of  clusters of affiliated groups of participants that potentially  constitute embedded communities is an empirical matter, and one  of interest to managers of large learning and professional  networks. Also, these socio-technical networks are typically  multi-mediated, in that they offer multiple means of participation,  each with their own interactional affordances. Different  communities may be using the multiple media in different ways.  We have developed an analytic framework for extracting events  from log files and representing interaction and affiliations at  different granularities as needed for analysis. In this paper we  show how bimodal networks of actors and media artifacts can be  constructed in which directed arcs relate actors to the artifacts  they read, write or edit, and how the resulting graphs can be used  to detect community structures that extend across different media.  We illustrate these ideas with a study that characterizes  community structure within the Tapped In network of educational  professionals, and how the associations between members of this  network are distributed across media (chat rooms, discussion  forums and file sharing).   Categories and Subject Descriptors  K.4.3 [Computers and Society]: Organizational Impacts   computer supported collaborative work    Keywords  Socio-technical networks, community structure, distributed  learning, networked learning, social network analysis.   1. INTRODUCTION  Learning in university settings, professional communities, and  virtual organizations [3, 5, 12] is increasingly technologically  embedded, with the rapid adoption of information and  communication technologies in support of online, distributed,  and networked learning and knowledge creation activities [2,  10, 37], and their blending with face-to-face venues [15]. A  related trend is towards open learning communities. In corporate  or other work settings, professional learning communities may   cross team contexts rather than being isolated in work teams [38].  The sharing of resources in these networks benefits both the  individual users within these networks and their collectives [18,  37], and the network and socio-technical infrastructures in of  themselves constitute a form of socio-technical capital [16, 28].  We will refer to these various technologically embedded social  networks as socio-technical networks [21]. A fundamental  question in all of these settings is how learning and other  enhancements of knowledge, skill and capital take place through  the interplay between individual and collective agency. Such a  question demands analyses that connect learning activity in  specific times and places with the larger socio-technical network  contexts in which they take place. A related analytic challenge is  that the granularity at which events are recorded may not match  analytic needs. Addressing these analytic challenges by  connecting levels of analysis is one objective of our developing  analytic approach [34].   Many digital environments for networked learning and  professional networks are multi-mediated, in that they offer  multiple means of participation, each with their own interactional  and social affordances  (e.g., asynchronous discussion forums,  quasi-synchronous chats, and file sharing). Thus, there are  different mediational means through which members may be  affiliated. Licoppe and Smoreda [25] found that the choice of  technologies by which people share personal news or keep in  touch with each other both reflects and reaffirms the nature of the  relationship between the interlocutors. The present line of  research applies this idea at a collective rather than dyadic level:  communities are embedded within and make use of technological  media for interaction in ways that reflect and reaffirm their  community nature. We address the question of community  identification as an empirical matter, discovering clusters of  affiliated groups of participants in socio-technical networks rather  than assuming that the network constitutes one community or that  that prior or external communities are replicated within the socio- technical system. We then examine how the discovered clusters  correspond to participants organizational affiliations and  projects.   The fact that learning and knowledge creation activities in these  networked environments are often distributed across multiple  media and sites leads to a second analytic challenge. The  networked learning environments we study offer mixtures of  threaded discussion, synchronous chats, wikis, whiteboards,  profiles, and resource sharing. Events in these media may be  logged in different formats and databases, disassociating actions  that for participants were part of a single unified activity. This  disassociation is exacerbated when activity is distributed across  multiple virtual sites or spread over time, and by the need to work  at higher levels of description alluded to earlier in the first      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, April 29-May 2, 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.      43    analytic challenge. To address both of these analytic challenges,  we developed an analytic hierarchy that support bridging between  local analysis of sequential activity and global analysis of  mediated affiliations and ties, and associated representations that  abstracts from media-specific transcript representations [34]. This  hierarchy includes an intermediate representation, the  associogram, that supports structural network analysis while  preserving information about mediation and direction of  interaction.    The concerns discussed above are particularly salient in the  professional learning network we are studying, SRI's Tapped In  professional network of educators [14, 30]. The network has  consisted of both organizational tenants and individuals who  come for their own enrichment. There are multiple forms of  participation and mediational means by which participant  associate with each other. This paper reports an analysis of  community structure within the Tapped In network, and how this  structure is distributed across media (chat rooms, discussion  forums and file sharing). The paper also illustrates an application  of our analytic framework. Affiliation networks of actors and  media artifacts were constructed in which directed arcs relate  actors to the artifacts they read, write or edit. Analysis of these  networks exposes community clusters within the network and  how they are distributed differently across media types.   The remainder of this paper begins with a description of Tapped  In. We then summarize the analytic approach mentioned above.  After describing how the Tapped In data was prepared for  analysis in the associogram representation, we provide empirical  results. We characterize the overall network studied, and then  focus on the top six largest sub-networks detected by a  community detection (modularity partitioning) algorithm.  These sub-networks are described in terms of what we know  about the most active participants in each network, showing that  sub-networks derived on a purely structural basis correspond to  clusters of institutional affiliations and purposeful collective  activity in Tapped In. The paper ends with discussion of what this  tells us about Tapped In, and what this tells us about analyzing  distributed activity in socio-technical networks.   2. TAPPED IN  The study examines participant interaction in SRI Internationals  Tapped In (tappedin.org), an international (albeit mostly US)  network of educators engaged in diverse forms of informal and  formal professional development and peer support [14, 30].  Cumulatively, Tapped In has hosted the content and activities of  more than 150,000 education professionals (over 20,000 per year  in our study period) in thousands of user-created spaces that  contain threaded discussions, shared files and URLs, text chats,  an event calendar, and other tools to support collaborative work.  More than 50 tenant organizations, including education  agencies and institutions of higher education, have used Tapped  In to meet the needs of students and faculty with online courses,  workshops, seminars, mentoring programs, and other  collaborative activities. Also, approximately 40-60 community- wide activities per month were explicitly designed by Tapped In  volunteer members to help connect members. (Volunteers drive  the majority of Tapped In activity.) Extensive data collection   capabilities underlying the system captured the activity of all  members and groups. SRI colleagues provided eight years of  anonymized data to us. Out of this data, we selected a period of  peak usage that occurred from September 2005 through May 2007  for analysis in this study.   Because Tapped In is populated with members of multiple tenant  organizations as well as unaffiliated members, it is best seen as a  network of education professionals rather than a single  community. Members may move freely between most forms of  participation. The question of what communities (or clusters)  exist in this network is a matter for empirical investigation. We  approach this question in terms of the artifact-mediated  associations found between members.   3. ANALYTIC APPROACH  Our prior research is methodologically eclectic, taking insights  from multiple traditions. At a fine granularity, rich descriptions  and the unpacking of the sequential structure of situated activity  leads to insights into the experiences of participants and the  methods by which they accomplish their objectives. Yet  microanalytic approaches do not capture emergent social  structures that are constructed by yet influence local activity. At  the other extreme, social network data and analytic techniques can  uncover social regularities, but binary ties obscure the situated  interactions that constitute and sustain these ties.  Both levels of  analysis are needed, and we also see the need for bridges between  the two levels. There is a dialectic in socio-technical networks in  which the local and the global influence and indeed constitute  each other. These influences are mediated by cultural artifacts as  well as by human actors [20]. Latours [23] version of Actor- Network Theory  inspires us to trace out the artifact-mediated  pathways of influence between actors, acknowledging that non- human entities or actants are influential in the network as well  as human actors. We use the term association for these  mediated pathways, preferring it to relationship or tie because  the latter have socio-emotive connotations, while we include in  our scope of consideration the diversity of other ways in which  people interact with or influence each other [10, 18], or even  potentially may do so in the future [31].    Motivated in this manner, we have developed an abstract  transcript representation called the Entity-Event-Contingency  (EEC) graph that provides a unified analytic artifact [33]; and an  analytic hierarchy derived from the EEC that supports multiple  levels of analysis [34]. To construct the EEC, log files are  abstracted to collections of events (actors taking actions on media  objects), supported by a domain model describing the  participating entities (actors, actions and digital objects or  artifacts). Other information is also recorded, such as time and  virtual location. To support sequential analysis of interaction,  directed graphs that record observed relationships between events  called contingencies are constructed [33]. The present study does  not use contingency graphs, but rather abstracts further to  associograms, two-mode directed graphs that record how  associations between actors are mediated by their creation and  modification of and access to digital artifacts.   44    3.1 Associograms  Associograms are like affiliation networks, but they relate actors  to mediating artifacts and can be directed: arcs are directed from  actors to the artifacts they read, and from artifacts to the actors  who wrote or edited the artifact (Figure 1). The arcs represent a  form of dependency: they may be reversed to indicate direction of  information flow. Existing social network analytic techniques for  affiliation network analysis may be applied to associograms, or  transitive closure can be used to convert associograms to  sociograms to which other existing techniques can apply [36].  The results of network analysis can then be interpreted by  reference to the other levels of analysis. Thus associograms  bridge between interaction data and network analysis.    Associograms offer a different viewpoint on a network than  sociograms. An associogram tie links an actor to an artifact;  therefore, an artifact always mediates an association between two  actors. Another way to visualize this is to expand a sociogram tie,  identify the means by two actors have interacted, and then  explicitly display the mediating factors. Because digital artifacts  afford different types of interactions, associations between actors  need not be the result of direct exchange or transmission of  information. While the tie in a sociogram might represent a  relationship (e.g. family member) or metric that defines the  relationship between the actors (e.g. frequency of contact), an  association defines the relationship in terms of the distribution of  affiliations across a set of mediating artifacts. This enables study  of how users connect with others within a socio-technical system,  and whether the different media types affect the relationships and  structures that are formed.   3.2 Data Preparation   We now transition to discussion of aspects of method specific to  our analysis of Tapped In. Preparation of the data required many  months of work, and was greatly facilitated by SRI colleagues.  We parsed and filtered logs of user activity involving three  different types of digital artifacts: files, asynchronous threaded  discussion forums, and quasi-synchronous chat rooms. The  relevant interactions consist of users accessing (reading and  downloading) or contributing (posting and uploading) to one of  these three artifact types. Private chats were excluded from our  analysis, as we are focusing on observable public behavior. All  activity in the K-12 (student) campus was excluded, as our  research (and human subjects permission) focuses on the  professional community. Guest accounts were also filtered, as  different rotating individuals use these. File access was filtered to  include only intentional access such as clicking on a link to a file  such as a Word document, Powerpoint presentation, etc.  (excluding downloads incidental to entering a room). Data from  several different EEC and Tapped In database tables, including  old system backups that provided missing data, were consolidated  to identify all interactions for each artifact type. The resulting  associograms were exported to VNA format so various network  analysis software tools could load them. Attributes for the  different entities were included with vertices to help with later  analyses, and the arcs (directed edges) were weighted according  to the number of times the given actor-artifact affiliation was  observed, as discussed below.    We constructed an associogram in which vertices represented  actors,  files, discussions, and chat rooms. A file vertex represents  a single file, a discussion vertex represents an entire threaded  discussion, and a chat vertex represents a chat room. The   direction of the arcs (directed edges) is a form of state  dependency: the source of an arc is dependent on the target of the  arc in that information or content has potentially moved from the  latter to the former. A file vertex points to an actor if the actor  created (uploaded) that file; an actor vertex points to a file vertex  if the actor has downloaded the file. A discussion vertex points to  an actor vertex if the actor has posted a message in the  corresponding discussion forum; an actor vertex points to a  discussion vertex if the actor has accessed messages in the  discussion forum (having loaded the discussion page). A chat  room points to an actor if the actor posted a chat contribution in  that chat room while someone else was present; an actor vertex  points to a chat vertex if the actor was present in the room when  another actor posted a chat contribution. Finally, each of these  arcs is weighted according to the number of time the events just  described were seen. For example, if an actor's arc to a chat room  has a weight of 375 then the actor was present for 375  contributions made by other actors.    3.3 Data Analysis  We used the Gephi [6] software package to analyze the  associogram for our data. Gephi is an open-source tool that  provides a suite of network metrics and interactive visualizations.  At this writing, Gephi is still in the beta stage of development  (version 0.8), and it is not as mature as Pajek or UCINet in terms  of available sociometrics. However, unlike these older software  tools (which use matrix representations of graphs), Gephi uses an  adjacency list representation of graphs, so is able to handle very  large graphs. Gephi provides several of the more recent layout  algorithms for large graphs and a recent community detection      Figure 1. Constructing an associogram from events. For  example, the event of P2 writing message m1 is represented as  a dependency of m1 on P2, and the event of P1 reading m1 is  represented as a dependency of P1 on m1. The cycle between  P1 and P2 indicates an interactional relationship, while P3 is a  consumer of artifacts created by P1 and P2.   45    algorithm with good properties (discussed below). Once our graph  was partitioned into candidate communities, we used Gephi to  visualize the partitioning in various ways. We also used Gephi's  Data Laboratory to inspect members of the partitions. The suite  of metrics available in Gephi was run to generate basic  descriptive statistics. We also exported spreadsheets of various  partitions to compute sums of unweighted and weighted in-degree  and out-degree in order to interpret the distribution of  communities across artifact types. When needed, we accessed our  original database to look up additional information on the entities  involved or inspected the live Tapped In environment.    3.3.1 Data Visualization  We applied Gephis implementation of the OpenOrd [26] layout  algorithm to visualize the network. OpenOrd is a force-directed  algorithm that scales well to support very large networks.  OpenOrd is based on Frutcherman-Reingold, an O(n2) force- directed layout algorithm, but resolves two of the latters defects:  Frutcherman-Reingold is too slow for large graphs, and may  obscure global structure. OpenOrd uses a multilevel approach,  where a sequence of successively coarser graphs is constructed by  clustering vertices according to edge weights and distances and  representing each cluster with a single vertex in the coarser graph.  Layout is computed with the coarsest graph obtained and then the  constituent vertices are placed in the location of their cluster to  initiate the next finer iteration of layout. Additionally, edge  cutting is used to prevent long edges from unduly pulling together  vertices that are best displayed in distinct global structures.  Several phases are used in which parameters are varied to resolve  tradeoffs between initial clustering, expansion, tightening of  clusters, and fine-tuning, The OpenOrd algorithm is very fast and  exposes structure in large graphs.     3.3.2 Community Detection  In the network analysis literature, community refers to clusters  of mutually associated vertices under graph-theoretic definitions  rather than to the sociological concept, and community  detection refers to finding sub-graphs that constitute such  structural clusters. However, a good graph-theoretic definition  should capture the intuition that individuals in a sociological  community are more closely associated with each other than they  are with individuals outside of their community. In this paper, we  understand community as empirically associated actants for  whom it is also possible to identify some focus of their shared  activity. This sense of community is much looser than the  traditional gemeinschaft [35], and does not make claims about  participants own identities [11]. We believe that a looser  definition is appropriate for the networked age [10, 37]. We use  graph theoretic terms (e.g., partition) when discussing  algorithmic results that are candidates for interpretation as a  community, and usually reserve community for when we are  entering into the realm of such interpretation by trying to identify  what a cluster has in common, except when referring to the larger  endeavor of community detection.      A variety of graph-theoretic definitions of communities are  available, and there are multiple algorithms for each. Algorithms  based on the modularity metric are widely used. The modularity  metric compares the density of weighted links inside partitions to  weighted links crossing between partitions, ranging from 1 (high  modularity) to -1 (no modularity). A partitioning of a graph into  highly modular partitions  (a.k.a. modularity classes in Gephi)  defines nonoverlapping communities. Finding the best possible  partition under a modularity metric is computationally hard   (impractical to compute on large networks) [9]. Blondel et al. [7]  offer a fast approximation algorithm that gives good results. On  each pass, each vertex is initially placed into its own separate  partition, and then the algorithm examines the neighbors of each  vertex to see whether moving the vertex to a neighbors partition  can increase modularity. This process iterates over vertices until  no vertices move between partitions. Then each partition so  constructed is collapsed into a single vertex, with the edges to  other partitions merged, summing their weights. The algorithm  then repeats on the collapsed vertices until there is no further  change in partition membership. At this point, a local maximum  has been reached that is not guaranteed to be the most optimal  partition under the modularity metric, but has been shown to be a  good approximation.    4. EMPIRICAL RESULTS  In this section we summarize metrics for the overall socio- technical network studied: all activity within Tapped In  surrounding chats, discussions and files (filtered as discussed  previously) for the period from 9/2005 to 5/2007. We also  summarize the metrics for media-specific networks. Then we  present the modular partitions found by the Blondel et al.  algorithm and interpret the several largest partitions as  communities. We do so by examining who the top actants (actors  and artifacts) are, what their affiliations and/or intended purpose  seems to be, and how their activity distributes across media.  These results are used to illustrate the utility of this kind of  analysis.    4.1 Overall Network Metrics   In the combined network, there are 40,490 vertices (a.k.a entities  in the EEC, or actants in the world). These comprise 19,842  Actors (49.00%), 12,037 Discussions (29.73%), 5,862 Files  (14.48%), and 2,749 Chat Rooms (6.79%). The combined  network has 229,072 edges. The presence of an edge represents  the existence of a person-artifact association. Each edge may  represent one or more events involving that person and artifact.  Edges are weighted according to the number of events. The sum  of weights on all edges gives us the number of events  encompassed by the total analysis. The sum of weighted degree in  the entire network is 20,431,944, which constitutes the number of  events (as we have defined them) analyzed.    The weighted out-degree gives the number of write events for  the given artifact (the arc indicates that the state of the artifact  depends on the target actor), so the sum of weighted out-degree  across all artifacts of a given type gives the number of 'write'  events for that artifact type. Similarly, the weighted in-degree  gives the number of read events for an artifact (the arc indicates  that the actor has accessed the artifact), so the sum gives the total   Table 1. Weighted degrees (events)     Weighted In-  Degree   Weighted Out-  Degree   Row   Totals   Chat Rooms 12,220,792 2,512,887 14,733,679   Discussions 5,592,946 45,085 5,638,031   Files 54,372 5,862 60,234   Artifact   Totals 17,868,110 2,563,834 20,431,944   Actors 2,563,834 17,868,110 20,431,944   46    number of such events for an artifact type. The sums shown in  Table 1 indicate how activity distributes across artifact types, with  the caveat that the units are different activities. There are over  twice as many events in the chat rooms as in discussions, and few  file events, which reflects typical frequencies with which one  might interact with each of these artifact types. These results  illustrate how the directed, weighted, and multimodal properties  of associograms provide information about mediated activity not  available in simple sociograms.      Some further graph metrics help characterize this network in  comparison to other networks. The density is less than .001 (does  not display any significant digits in Gephi), so this is a sparse  graph. The network diameter is 17, and the average path length is  4.398, smaller than the 6 degrees of separation found in other  networks [e.g., 24]. This result is especially remarkable given that  the associogram is a bimodal graph in which an artifact must be  present between every person connected. If the graph were  collapsed to direct actor-actor ties the average path length would  be about half, so this network is more closely knit than typical  networks, although others have found similar results [4].    The close connectivity is partly due to associations via a single  artifact, the Tapped In Reception (R1), which most users pass  through after logging in. This room has a normalized betweeness  centrality [8] of 0.665, and has by far the highest degree  (2,511,057 weighted or 18,810 unweighted) of any actant. If this  mediating artifact is deleted from the graph, average path length  goes up to 6.02, or about 3 if intermediate artifacts are removed.  We return to the relevance of R1 shortly.    4.2 Modular Partitions  The Blondel et al. algorithm [7] constructs 171 partitions with an  modularity score of 0.817. The combined network is visualized in  Figure 2. Color represents the modular partitions generated by the  algorithm. Figure 3 shows the six largest partitions by number of  vertices. Henceforth we refer to these six partitions as partitions  1-6. The overall visualization in Figure 2 shows a strong central  cluster, as do several of visualizations of the six largest partitions  in Figure 3. The fact that these central clusters are co-located in  the OpenOrd layout (which places more strongly related vertices  near each other) indicates that there is overlap between the cores  of these potential communities.    The visualization also juxtaposes vertices for several important  actants on top of each other. In order to separate the important  actants and see the relationships between them, Figure 4 shows  only those actants of unweighted degree greater than 282, with  vertex size scaled by weighted degree, and using a radial layout  with a non-overlapping filter. Several of the major actants are  labeled anonymously, and will be discussed below. A, B, C, and  D are actors, D1 is a discussion forum, and R# indicate chat  rooms. The colors indicate partitions, as in Figures 2-3. (If color  is not visible, of the numbered vertices, R1, R3, R4, and R5 are in  partition 1; A, B, C, D, and R6 are in partition 2; R8 is in partition  3; D1 in partition 4; and R2 in partition 6.)    Figure 4 shows that there are strong relationships between  partition 1 and the other networks, particularly partition 2. We  were concerned that the large degree of R1, the Tapped In  Reception chat room around which partition 1 forms, might skew  the community analysis. Its degree and weighted degrees are a  factor of 10 larger than the next largest actant. This may be due to  the fact that R1 is the default room into which anyone without a   specific tenant affiliation is placed when logging in, and an  association will form if anyone chats while they are there. The  help desk is located in R1, and help desk volunteers often greet  others who enter this room. R1 plays an important role in the  functioning of Tapped In, but from the standpoint of community  analysis R1 presents a dilemma. On the one hand, associations via  the reception are a valid community phenomenon: conversations  between friends may take place there, and R1 may provide the      Figure 2. Combined Associogram for Actors, Chat   Rooms, Discussion Forums and Files. Colors indicate high- modularity partitions (candidate communities) found. Vertex  side indicates weighted degree.                  Figure 3. The six largest partitions. Top left: partition 1;  top right: partition 2; bottom left: partitions 3 (brown, left  side) and 4 (blue, right side); bottom right: partitions 5  (lighter tan, upper half) and 6 (reddish brown, lower half).    47    setting for serendipitous meetings between persons who might not  otherwise have met. On the other hand, many of these  associations form without the actor having made a deliberate  choice to be there. Anyone in the public campus who has not  changed their home room setting will enter this room when  logging in and will automatically be associated via R1 with  anyone else who has chatted there. The dilemma is exacerbated  by a limitation of the Blondel et al. community detection  algorithm (and many other community detection algorithms): it  does not allow vertices to lie in multiple communities. Thus, any  actant placed into the modularity class surrounding R1 will not be  available for membership in other modularity class, potentially  obscuring their intentional participation elsewhere.   In order to assess the risk of superficial associations via R1  obscuring more purposeful communities, we conducted an  analysis with R1 (and all of its associations) deleted. As expected,  the large partition centered on R1 (partition 1) disappeared and  partition 2 became the largest partition, absorbing many of the  important actants formerly in partition 1. The interpretation of this  new collapsed partition was very clear, and the other largest  partitions still produced similar interpretations as communities.  However, we decided to leave R1 in for this final analysis, for  two reasons: First, many actors were orphaned by the removal of  R1: 2178 isolates appear. Second, we felt that we should grapple  with interpreting the partitions resulting from the unedited data, as  we may learn something about both the Tapped In network and  what a community detection algorithm on an associograms can  show us. As it turns out, the partition (#1) forming around R1 has  a useful interpretation that can be distinguished from partition 2.      4.3 Example Interpretations of Partitions  In this section we discuss our interpretation of the highest  modularity partitions discovered to illustrate the method. Each of  the networks is interpreted for what kind of human network it  represents by examining what is known about the actants (actors  and artifacts) that are ranked highest by degree within the  associated partition, indicating their importance to the activity of  that partition.    4.3.1 Partition 1  The largest modular partition has 8452 vertices (20.87% of the  total graph) 29698 edges (12.96% of the total edges). There are  6953 actors, 673 chat rooms, 495 discussions, and 331 files in this  partition. We illustrate how this partition is interpreted by  examining top ranked actants and the distribution of activity  across media.    4.3.1.1 Top actants by degree  Recall that unweighted degree is a measure of how many other  actants a given actant has been associated with, and weighted  degree is a measure of how many contact events there have been,  i.e., a measure of level of activity.    The top 20 actants by unweighted degree in this partition are all  rooms. Out-degree indicates the number of persons chatting in  each room, and in-degree indicates the number of persons hearing  someone chat in each room. Total degree counts ingoing and  outgoing associations, so will count someone who both chatted  and heard a chat twice. The top ranked actants are:   R1, the Tapped In Reception: 7173 out, 11,637 in, 18,810 total.     R4, the public room for Tapped Ins After School Online (ASO)  events: 898 out, 986 in, 1884 total.    R10, the Floor Lobby for the Tapped In Groups floor: 489 out,  601 in, 1090 total. All groups are on the third floor; this is  the lobby through which you enter that floor.    R5, the personal office of Actor E, a faculty member focusing on  teachers use of technology, and the active owner of a  Tapped In group on teacher education: 313 out, 319 in, 632  total.    R9, ComfyConf, a public conference room available for use by  Tapped In members: 285 out, 310 in, 595 total.    R12, the personal office of Actor B, an important volunteer to be  described in the next section: 289 out, 296 in, 585 total.    The top 20 weighted degree vertices in this community are also  all rooms. Weighted out-degree indicates the number of chat  events in each room; weighted in-degree indicates the number of  events of someone being present when someone else chatted  (loosely, hearing events); and the total can be taken as a  measure of total level of chat interaction in the rooms:    R1, the Tapped In Reception: 549,572 out (chatting events)  1,961,485 in (hearing events), 2,511,057 total. This room  hosts 12.29% of all chat events in the network.    R3, the personal office of Actor F, an educational researcher:  24,477 out, 407,429 in, 431,906 total.     R4, the ASO Public Room: 51618 out, 253,611 in, 305,229 total.    R5, the personal office of Actor E, 19448 out, 213474 in, 232922  total.    R11, a group room owned by Actor B: 10872 out, 172479 in,  183351 total.    R9, the ComfyConf: 16785 out, 106596 in, 123381 total.       Figure 4. Radial layout of high degree vertices (degree >   282; Gephi does not currently filter by weighted degree).   Vertex size is weighted degree. Color is modularity class.   48    Most of the rooms on both top-20 lists are public, reception or  group rooms. Some these are owned by groups with diverse  purposes (Art, Blogging, Math, Portfolios, Robotics, Writing),  making it difficult to identify a specific purpose or activity for  this partition. But the very fact of this diversity and consideration  of other information leads to a clearer interpretation of this  partition. Many of the highest ranked rooms (R1, R4, R9 and  R10) are owned by Tapped In, and explicitly function either to  welcome newcomers and route them to their destinations or as  venues for public events open to all. The personal offices  involved mostly belong to people who keep open office hours to  help others. Also, consider the structural fact that when R1 is  deleted, 2178 actor vertices (5.3% of all actants and 10.98% of all  actors) become isolated, so were only linked to R1. Of these,  1519 are in partition 1, meaning that 17.97% of actors in partition  1 are there solely because of their association via R1.   Furthermore, the distribution of degrees is highly skewed: the  average is 9.31 (4.62 for actors), the median 3, and the mode 2.  Thus this partition consists of many actors who have weak  affiliations within Tapped In, but are bound together by their  mutual association with the major entry points and centers of  activity for new members or those unaffiliated with tenant  organizations: the reception, rooms in which public events take  place, and offices of volunteers, as well as some public group  rooms for popular topics. We interpret this partition to represent  not a separate community with its own purpose or activity, but  rather a large network in which more specific communities  represented by other partitions are embedded. It may also reflect a  phase of participation in which new members are becoming  familiar with Tapped In, after which they may or may not deepen  their participation in specific groups. (Temporal analysis is a topic  for further research.)    4.3.1.2 Media distribution   The degrees for each artifact type give us a general overview of  the distribution of activity in this partition. Unweighted degree  summarizes the number of other actants a given actant has some  affiliation with. Weighted degree is a better overall measure of  level of activity bearing on or emanating from an actant, keeping  in mind that events involving chats, discussions and files each are  of a different nature.    Examining the degrees in Table 2, we can see that the bulk of  events involving these 6953 actors are overwhelmingly chat- based. This is consistent with the fact that the top actants are chat  rooms, and with the interpretation that this partition gathers  together activity related to newcomers and chat sessions popular  with participants not strongly active in tenant organizations.    Table 2. Media Associations in Partition 1   (6953 Actors) Unweighted Weighted    In Out In Out   Chat Rooms (673) 22,325 16,952  4,901,04 2   1,067,66 7   Discussions (495) 3,275 1,317 139,268 1,317   Files (331) 2,393 331 4,199 331      4.3.2 Partition 2   The next largest sub-network (modularity class) has 5826 vertices  (14.39% of the actants) and 20459 edges (8.93% of events), with   2485 actors, 782 chat rooms, 1828 discussions, and 731 files.  Henceforth we will report the top five actants by weighted degree  to indicate where the activity lies, and then add those that are in  the top five by unweighted degree to ensure we also include  actants with high connectivity. For brevity we now report only  total degree in parentheses. The top five weighted degree actants  in this partition are as follows:    Actor A (weighted 499,998; unweighted 2,355), having a  Normal account type and not a tenant of any organization,  nor affiliated with SRI, is the most active account in the  system. The level of activity of this actor and of Actor B/B  (see below) highlights the importance of committed  volunteers in this community.    Actor B (weighted 370,604; unweighted 2,892), a volunteer who  was given Facilitator status and paid for some (but not all) of  her activities. Account B is the second most active account in  the system. The real-world actor was given a second account   B (121,259; 238), so that she could facilitate two events at  once. Account B is a member of a tenant organization for a  partnership of multiple school districts during the time our  data was gathered. Taken together, the real word actor B/B  is as active as Actor A.    R13 (441,600; 229), the personal office of a faculty member in  the school of education at a community college. R13 does  not appear in Figure 4 due to the lower unweighted degree.    Actor C (336,002; 305), a middle school technology support staff  and a Tapped In help desk volunteer.    R6 (221,078; 369), an educational technology group room owned  by a university education faculty member.    The top five actants by unweighted degree include Actors A and  B, and the following:    R14 (48,116; 2,355), the personal office of Actor A. This office  has nickname Online Support and is described as offering  sustained online support.    R15 (31,847; 625), a resource room for a primary school center,   owned by Actor G, a primary school teacher and Tapped In  help desk volunteer who is involved in many groups.      Actor D (166,568; 583), a middle school technology teacher who  became a Tapped In help desk volunteer during the period of  this study.    Most of the actors on this list are help desk volunteers. This  suggests that partition 2 has some overlap in function with  partition 1: helping support the broader Tapped In community.  But scanning ranked lists of chat rooms and discussions, a striking  difference emerges: while partition 1 has a mixture of personal  offices and group rooms, the top ranked chats and discussions in  partition 2 are overwhelmingly group rooms. The topics are  diverse, including assessment, climate change, librarianship,  online teaching and learning (two groups), music, special  education, teachers in training, the WWW (two groups), and a  system-wide Tapped In festival. We found that all of these chat  rooms have one thing in common: they are the site of regular  (repeating) and public After School Online (ASO) events  announced in the calendar. Furthermore, actors A and B are  known for facilitating or supporting the facilitators of many ASO  events (i.e., were present in those rooms when the chatting took  place). These facts suggest that partition 2 is the broad public  community associated with After School Online, arguably the   49    largest and most significant activity in Tapped In. We  acknowledge that some relevant rooms such as R4, the After  School Online room, R9, the ComfyConf room, and R10, the  floor lobby for Tapped In groups, are in partition 1. However, all  of these actants ended up in the same partition in the other  analysis conducted without R1.    Examining the distribution of activity across media in Table 3,  most of the events are chat-based, which makes sense given that  ASO events are chat based. However, although there is less than  half the number of actors as in partition 1, both discussion activity  and file sharing are double those in partition 1. These facts are  consistent with the interpretation that partition 1 focuses more on  brief chat interactions such as when persons enter R1 and are  given help, while partition 2 includes topic focused public groups,  some of which include asynchronous discussion as well as  scheduled chats. Help desk volunteers are involved in both of  these activities: they are represented by the rooms where help is  given in partition 1, and by themselves as actants participating in  various group rooms in partition 2.   Table 3. Media Associations in Partition 2   (2485 Actors) Unweighted Weighted    In Out In Out   Chat Rooms (673) 4,862 4,674  1,198,11 7   289,666   Discussions (495) 7,228 3,654 270,870 3,654   Files (331) 2,841 731 4,747 731     The remaining partitions will be described much more briefly due  to limitations of space, but sufficiently to illustrate how  modularity partitioning on associograms identifies clearly  interpretable communities once the densely connected core has  been taken care of.    4.3.3 Partition 3   The next largest modularity class has 2565 actants (6.33% of  total), including 851 actors, 103 chat rooms, 1286 discussion  forums, and 325 files. Interpretation of this sub-network is much  more straightforward. Sorted by degree, two rooms followed by  40 actors have the highest degree, and all 42 of these actants have  tenant affiliation with a public school district of a city in the  midwestern US. The top ranked actors by degree are all teachers  at various levels in this public school system. When sorted by  weighted degree, there is a mixture of rooms, discussions and  actors, the majority of which are again associated with this public  school system or with a related teacher education center.    These results suggest that there was formal involvement of this  school system during 2005-2007. We checked this interpretation  out with our SRI colleagues. According to Judi Fusco (an SRI  researcher on the Tapped In project; personal communication  October 2011), Tapped In began working with this school system  in 2002 to help them support new teachers online with a  communities-of-practice model in which cadres of new teachers  worked with an experienced teacher facilitator [22, 29]. The  program began with about 175 new teachers and 23 facilitators.  The tenant organization is still active and has 704 people working  in their online program this year.    The weighted degree figures for this partition suggest that this  community relies on all media. The chat (272,865 in, 226,561  out) and discussion (355,656 in, 5,976 out) figures are healthy for   this number of actors, with the asymmetry suggesting that  discussions were used more for dissemination. This network has  1/8 the actors of partition 1, but similar figures for file sharing  (5,042 weighted), indicating a proportionally greater importance  of file sharing.   4.3.4 Partition 4  The fourth largest modularity class has 1630 actants (4.03%),  including 857 actors, 26 chat rooms, 605 discussion forums and  142 files. The top actants involve a state professional  development center (anonymously abbreviated SPD), and a  national professional development center (NPD), both located in  the southern US. Top actants by weighted degree include a  discussion on 21st century learning in the SPD group room (D1 in  Figure 4); a chat room owned by a teacher educator; an NPD  actor with facilitator status who is a university faculty member, a  middle school teacher in the same state as SPD, and another  discussion in a SPD group room Adding top five unweighted  degree actants, we have three actors, all being secondary school  teachers located in the SPD state. Of the top 50 ranked  discussions, 35 of them are affiliated with the SPD tenant. Of the  remaining 15, the Tapped In Reception hosts 12. This is an online  professional development effort involving persons in multiple  southern states but with the greatest activity focused in one state.  It is possibly a collaboration between the state PD center and the  national PD center. Media figures show that this is primarily a  discussion-based effort, so it was conducted asynchronously.    4.3.5 Partition 5  This modularity class has 1251 actants (3.09%), including 112  actors, 35 chat rooms, 1006 discussion forums and 98 files, and  4219 edges (1.84%). Top ranked actants by weighted degree  include a room for final task collaboration and a room that  appears to be for a course on using the internet in K-12 schools.   Both are owned by Actor H, a language arts high school teacher  in the Midwest, who is the third highest weighted actant. The next  two actants are the office of a language arts teacher and the  account of a middle school language teacher, both in the same  state as Actor H. Sorting by unweighted degree adds three more  actors, all teachers in the same Midwestern region. The media  usage distribution for these 857 actors is weighted on the chats  (104,666 in, 35,111 out). There are low figures for the 2313  discussionsmost have only one posting and a few reads. This  appears to be a series of events run by Actor H.    4.3.6 Partition 6   Finally, the sixth largest modularity class has 1037 vertices  (2.56%) and 6858 edges (2.99%), with 729 actors, 153 chat  rooms, 71 discussions, and 220 files. The affiliation with a  university on the west coast (well call it WU) is clear. The top  five actants by weighted degree include four personal offices of  WU faculty members in education, and one group room owned by  a researcher at WU and SRI. Sorting by unweighted degree we  add one more WU faculty member office, and two WU public  rooms. Almost all of the 100 top ranked actors by weighted  degree have WU as their tenant affiliation. The interaction is  clearly chat based (sum of weighted degree 4,519,127 compared  to only 27,789 for discussions). Heavy use is made of personal  offices: 41 of top 50 chats are in personal offices. Asynchronous  discussions play a lesser role, but are affiliated with a nearby  public school system, suggesting involvement of this university  with local public schools. Clearly, this cluster is associated with  WU teacher education, possibly in collaboration with local public   50    schools. Judi Fusco confirms that WU was one of the major  Tapped In tenants during this time period. They have Masters and  EdD programs, and apparently had advanced students doing  professional development with teachers in the schools.    4.4 Small Communities  The visualization of Figure 2 shows that there are also many small  peripheral networks embedded in the larger network. We  inspected some of the small clusters shown in Figure 5, and found  a diversity of actors and topics. (1) One centers around a group  room for educational technology students at a liberal arts college  with multiple campuses and online programs. This room is owned  by an elementary school teacher who is also teaching online for  this institution. (2) A cluster of actors is connected by several  discussions, all owned by a high school language arts teacher in  the Midwest. (3) Another room owned by an education faculty  member at a west coast state college is surrounded by many  actors. Their user IDs are distributed in a manner suggesting  clustered creation over a number of years. Sampling the actors we  find a consistent pattern, with accounts being created around the  same time in 2004, another cluster in 2005, and another in 2006.   All of the sampled actors are educators in nearby school districts,  ranging from elementary through high school and including math,  science, technology and social studies. It appears that the state  college actor owning this room is running a recurring course or  program. These small clusters attest to how Tapped In enables  participants to create hundreds of small communities embedded in  and synergizing with the larger Tapped In network.    5. DISCUSSION  These communities were detected based purely on structural  characteristics. Although our database contains other information  such as institutional affiliation, job role, geographic location, and  demographics for actors and descriptions for discussions and  rooms, none of this information was used in constructing the  partition. We only used such descriptive information associated  with top-ranked actants in interpreting the partitions after they  have been constructed. The fact that we can find a clear  interpretation based on the characteristics of actants in each  partition attests to the power of the structural method for finding  partitions that have some external validity.    A caveat: one should not conclude that all actants in these  partitions are engaged in the activity identified by our  interpretations. Modularity optimizationmaximizing intra-group  links while minimizing inter-group linksis appealing as a  graph-theoretic definition having some correspondence to our  understanding of community, but does not capture traditional  aspects of community having to do with shared geography,  identity or purpose. Each partition discussed above has up to a  few hundred highly connected actants, followed by a long tail of  other actants that have some weaker or peripheral association with  the network partition. The institutional affiliations, job roles, etc.  of the core actants ranked highly by degree give us an idea of the  activity that resulted in closer affiliations among these actants, but  many other actants who did not necessarily participate in the  identified activity but are weakly associated with the network will  be placed in the corresponding partition by the modularity  optimization algorithm if they do not have stronger associations  elsewhere. The same point applies to participation of artifacts.  Thus, for example, we will sometimes see very large numbers of   discussions involved in a partition, but this does not mean that all  of these discussions were devoted to the identified activity.    Yet, this caveat does not diminish the power of the analytic  technique. On the contrary, it shows that we can find both the  core purpose of a partition and the extent to which it involves  others not directly identifying with this core purpose. Also, such  expanded structures illustrate the synergistic power of embedding  task-specific activities within a larger transcendent community  [19, 32], a point on which Tapped In was clearly successful.      A limitation of the analysis is the use of a non-overlapping  community detection algorithm. Clearly, actors and artifacts may  play a role in multiple communities, but many community  detection algorithms force each vertex into one community.  Recently, various algorithms for overlapping community  detection have been proposed, including clusters of k-cliques [27],  vertex splitting [17], and approaches that find communities of  edges [1, 13]. Ongoing work is evaluating these algorithms with  respect to associograms. K-cliques do not apply to bipartite  graphs because there are no triads or higher order cliques in a  bipartite graph. Edge communities look most promising from both  empirical and theoretical standpoints: the results reported in [1]  are strong, and the approach is consistent with our theoretical  position that relationships between entities are primary rather than  entities in isolation (see for example [33, 34]). Once we find an  overlapping community detection algorithm that is suitable for  associograms, we will redo this work to study how actants  (including TI-Reception and highly active volunteer facilitators)  may bridge between communities.   6. Conclusions   These results demonstrate the vibrancy of Tapped In, in which  multiple tenant individuals and organizations support their own  instrumental purposes and also lead to the emergence of a larger  encompassing socio-technical network that is not in the same  sense a community but that constitutes the synergistic value of  the networkwhat we have termed a transcendent community  [19]. As can be see in Figure 2 and its decomposition into sub- networks in Figure 3 and our interpretations above, tenant  organizations drive significant activity in Tapped In, but they are  entangled with participants from other organizations and the  larger public sphere of the volunteer-based After School Online  series.       Figure 5. Closeup of small peripheral sub-networks,   many of which constitute topic-focused communities.   51    For the purposes of this paper, these results also demonstrate our  method. Social network analysis and its use of the classic  sociogram generally take a high level view of user relationships,  but do not elucidate how those relationships are formed or  mediated. Various micro-analytic methods provide detailed  information for different forms of interactions, but do not expose  structures of the larger network. This study illustrates a middle  level of analysis: associations between participants that take place  through digital media. Associograms capture valuable  information, abstracting enough for aggregate structure analysis  across multiple media while preserving some information on the  quantity and directionality of interaction. A community  detection algorithm applied to the Tapped In associogram found  actual communities without applying any knowledge of  participant affiliation or other demographics. This further  illustrates the importance of considering activity across all  available mediational means for interaction, not just limiting  research to (for example) chats or discussion alone.    7. ACKNOWLEDGMENTS  This work was supported by NSF Award 0943147. The views  expressed herein do not necessarily represent the views of NSF.  The authors thank SRI and Patti Schank for providing us with  access to the anonymized data, Judi Fusco for her help with our  interpretations, and Nathan Dwyer and Devan Rosen for their  prior collaborations on this work.   8. REFERENCES    [1] Y.-Y. Ahn, J. P. Bagrow and S. Lehmann, Link communities reveal   multiscale complexity in networks, Nature, 466 (2010), pp. 761-765.  [2] I. E. Allen and J. Seaman, Growing by Degrees: Online Education in   the United States, 2005, Alfred P. Sloan Foundation, Needham, MA,  2005.   [3] T. Anderson, ed., The Theory and Practice of Online Learning,  Second Edition, Athabasca University Press, Edmonton, Canada,  2008.   [4] R. Bakhshandeh, M. Samadi, Z. Azimifar and J. Schaeffer, Degrees of  separation in social networks, in D. Borrajo, M. Likhachev and C. L.  Lopez, eds., Proceedings of the Fourth Annual Symposium on  Combinatorial Search (SoCS-2011), Castell de Cardona, Barcelona,  Spain, July 1516, 2011., AAAI, Menlo Park, 2011, pp. 18-23.   [5] S. A. Barab, R. Kling and J. H. Gray, Designing for Virtual  Communities in the Service of Learning, Cambridge University Press,  New York, 2004.   [6] M. Bastian, S. Heymann and M. Jacomy, Gephi: An open source  software for exploring and manipulating networks, International  AAAI Conference on Weblogs and Social Media, 2009.   [7] V. D. Blondel, J.-L. Guillaume, R. Lambiotte and E. Lefebvre, Fast  unfolding of communities in large networks, Journal of Statistical  Mechanics: Theory and Experiment, http://dx.doi.org/10.1088/1742- 5468/2008/10/P10008 (2008).   [8] U. Brandes, A faster algorithm for betweenness centrality, Journal of  Mathematical Sociology, 25 (2001), pp. 163-177.   [9] U. Brandes, D. Delling, M. Gaertler, R. Grke, M. Hoefer, Z.  Nikoloski and D. Wagner, On modularity clustering, IEEE  Transactions on Knowledge and Data Engineering, 20 (2008), pp.  172-188.   [10] M. Castells, The Internet Galaxy: Reflections on the Internet,  Business, and Society, Oxford University Press, 2001.   [11] A. P. Cohen, The Symbolic Construction of Community, Routledge,  New York, 1985.   [12] J. N. Cummings, T. Finholt, I. Foster, C. Kesselman and K. A.  Lawrence, Beyond Being There: A Blueprint for Advancing the  Design, Development and Evaluation of Virtual Organizations  (Final   report from workshops on building effective virtual organizations)  2008.   [13] T. S. Evans and R. Lambiotte, Line graphs of weighted networks for  overlapping communities, arXiv:0912.4389v2 [physics.data-an],  2010.   [14] U. Farooq, P. Schank, A. Harris, J. Fusco and M. Schlager, Sustaining  a community computing infrastructure for online teacher professional  development: A Case Study of Designing Tapped In, Computer  Supported Cooperative Work, 16 (2007), pp. 397-429.   [15] D. R. Garrison and H. Kanuka, Blended learning: Uncovering its  transformative potential in higher education, The Internet and Higher  Education, 7 (2004), pp. 95-105.   [16] M. S. Granovetter, The strength of weak ties, American Journal of  Sociology, 78 (1973), pp. 1360-1380.   [17] S. Gregory, Finding overlapping communities using disjoint  community detection algorithms, Complex Networks, 207 (2009), pp.  47-61.   [18] C. Haythornthwaite, Crowds and communities: Light and  heavyweight models of peer production, Proceedings of the 42nd  Hawaii International Conference on the System Sciences (HICSS- 42), January 5-8, 2009, Waikoloa, Hawaii (CD-ROM), Institute of  Electrical and Electronics Engineers, Inc. (IEEE), New Brunswick,  2009.   [19] S. Joseph, V. Lid and D. D. Suthers, Transcendent Communities, in C.  Chinn, G. Erkens and S. Puntambekar, eds., The Computer Supported  Collaborative Learning (CSCL) Conference 2007, International  Society of the Learning Sciences, New Brunswick, 2007, pp. 317-319.   [20] V. Kaptelinin and B. A. Nardi, Acting With Technology: Activity  Theory and Interaction Design, MIT Press, Cambridge, 2006.   [21] R. Kling, Learning about information technologies and social change:  The contribution of social informatics, The Information Society, 16  (2000), pp. 217-232.   [22] M. Koch and J. Fusco, Designing for growth: Enabling communities  of practice to develop and extend their work online, in C. Kimble and  P. Hildreth, eds., Communities of Practice: Creating Learning  Environments for Educators, Volume 2, Information Age Publishing,  North Carolina, 2008, pp. 1-23.   [23] B. Latour, Reassembing the Social: An Introduction to Actor- Network-Theory, Oxford University Press, New York, 2005.   [24] J. Leskovec and E. Horvitz, Planetary-scale views on an instant- messaging network, Proceeding of the 17th international conference  on World Wide Web (WWW '08), ACM. arXiv:0803.0939, New  York, 2007.   [25] C. Licoppe and Z. Smoreda, Are social networks technologically  embedded How networks are changing today with changes in  communication technology, Social Networks, 27 (2005), pp. 317-335.   [26] S. Martin, W. M. Brown, R. Klavans and K. Boyack, OpenOrd: An  Open-Source Toolbox for Large Graph Layout, SPIE Conference on  Visualization and Data Analysis (VDA), 2011.   [27] G. Palla, I. Dernyi, I. Farkas and T. Vicsek, Uncovering the  overlapping community structure of complex networks in nature and  society, Nature, 435 (2005), pp. 814-818.   [28] P. Resnick, Beyond Bowling Together: SocioTechnical Capital, in J.  M. Carroll, ed., Human-Computer Interaction in the New Millennium,  ACM Press, Upper Saddle River, NJ, 2002, pp. 647-672.   [29] M. Schlager, J. Fusco, M. Koch, V. Crawford and M. Phillips,  Designing equity and diversity into online strategies to support new  teachers, NECC 2003, Seattle, WA, 2003, July.   [30] M. Schlager, J. Fusco and P. Schank, Evolution of an Online  Education Community of Practice, in K. Renninger and W. Shumar,  eds., Cambridge University Press, Building Virtual Communities,  2002, pp. 129-158.   [31] M. Smith, C. Giraud-Carrier and N. Purser, Implicit affinity networks  and social capital, Information Technology Management, 10 (2009),  pp. 123-134.   [32] D. D. Suthers and K.-H. Chu, Identifying mediators of socio-technical  capital in a networked learning environment, in L. Dirckinck- Holmfeld, V. Hodgson, C. Jones, M. de Laat, D. McConnell and T.  Ryberg, eds., Proceedings of the 7th International Conference on  Networked Learning, Aalborg, Denmark, 2010, pp. 387-395.   52    [33] D. D. Suthers, N. Dwyer, R. Medina and R. Vatrapu, A framework for  conceptualizing, representing, and analyzing distributed interaction,  International Journal of Computer Supported Collaborative Learning,  5 (2010), pp. 5-42.   [34] D. D. Suthers and D. Rosen, A unified framework for multi-level  analysis of distributed learning Proceedings of the First International  Conference on Learning Analytics & Knowledge, Banff, Alberta,  February 27-March 1, 2011, 2011.   [35] F. Tnnies, Community and Civil Society (J. Harris & M. Hollis,  Trans. from Gemeinschaft und Gesellschaft, 1887) Cambridge  University Press, Cambridge, United Kingdom, 2001.   [36] S. Wasserman and K. Faust, Social Network Analysis: Methods and  Applications, Cambridge University Press, New York, 1994.   [37] B. Wellman, A. Quan-Haase, J. Boase, W. Chen, K. Hampton, I. Diaz  and K. Miyata, The  social affordances of the internet for networked  individualism, Journal of Computer-Mediated Communication, 8  (2003).   [38] E. Wenger, R. A. McDermott and W. Snyder, Cultivating  Communities of Practice: A Guide to Managing Knowledge, Harvard  Business School Press, Boston, Mass., 2002.        53      "}
{"index":{"_id":"11"}}
{"datatype":"inproceedings","key":"Rahman:2012:COL:2330601.2330619","author":"Rahman, Nazim and Dron, Jon","title":"Challenges and Opportunities for Learning Analytics when Formal Teaching Meets Social Spaces","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"54--58","numpages":"5","url":"http://doi.acm.org/10.1145/2330601.2330619","doi":"10.1145/2330601.2330619","acmid":"2330619","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics, formal learning, informal learning, social systems","Abstract":"Social networking is revolutionizing the world in ways few imagined just a few years ago. The power of social networking technology can also be leveraged to improve education and enhance the instructor and learner experience. Unlike conventional learning management systems, social software environments such as Athabasca Landing provide a persistent space and are flexible enough to support social and learner-led methods of informal, non-formal, and formal learning. Analytics can be used to effectively track and measure personal progress and help uncover extra-curricular factor affecting learner success such as network formation and growth. The paper reports on an attempt to explore this problem through analysis of student behaviour on the Athabasca Landing site within the context of a course. Its findings, explanation, and potential implications are listed. Effects of social learning on learners, based on the learner's behaviour before, during, and after the course are described and discussed. Finally, features of an open source tool created for this analysis, LASSIE is presented.","pdf":"Challenges and Opportunities for Learning Analytics when  formal teaching meets social spaces   Nazim Rahman  Technology Enhanced Knowledge Research Institute   Athabasca University, Canada  +1 780 421 2535   nrahman@athabascau.ca   Jon Dron  School of Computing and Information Systems   Athabasca University, Canada     jond@athabascau.ca       ABSTRACT  Social networking is revolutionizing the world in ways few  imagined just a few years ago. The power of social networking  technology can also be leveraged to improve education and  enhance the instructor and learner experience. Unlike  conventional learning management systems, social software  environments such as Athabasca Landing provide a persistent  space and are flexible enough to support social and learner-led  methods of informal, non-formal, and formal learning. Analytics  can be used to effectively track and measure personal progress  and help uncover extra-curricular factor affecting learner success  such as network formation and growth. The paper reports on an  attempt to explore this problem through analysis of student  behaviour on the Athabasca Landing site within the context of a  course. Its findings, explanation, and potential implications are  listed. Effects of social learning on learners, based on the learner's  behaviour before, during, and after the course are described and  discussed. Finally, features of an open source tool created for this  analysis, LASSIE is presented.    Categories and Subject Descriptors  D.3.3 [Programming Languages]: PHP   General Terms  Management, Measurement, Documentation, Experimentation,  Human Factors   Keywords  Social systems, formal learning, informal learning, analytics    1. INTRODUCTION  This paper describes the rationale and design of LASSIE  (Learning Analytics for Social Systems in Institutional Education)  an open source analytics tool for Athabasca Landing, a social site  implemented at Athabasca University, which supports both formal  and informal learning as well as many other social and practical  applications. The paper begins by exploring salient differences  between the Landing and a learning management system. We go  on to discuss LASSIEs objectives, design decisions, architecture  and functionality, followed by descriptions of its data extraction,  graphing, trend discovery, and statistical features. We describe the   value of the system and discuss its limitations and future plans for  development.    1.1 Learning Spaces  Learning management systems (LMSs) such as Moodle  (www.moodle.org), BlackBoard (www.blackboard.com), and  Desire2Learn (www.design2learn.com) have, historically, tended  to model and replicate traditional classroom and institutional  processes and consequently tended to embed institutional  processes and forms such as courses, formal assessments,  timetables, classes and hierarchies of control. They do not lend  themselves well to different, more learner-centric approaches and  are the cause of increasing dissatisfaction among educators  (e.g.[1-4]). In recent years, alternatives that go beyond the LMS  have begun to enter the field, with potential for greater learner  control and rich tools for media creation and sharing. (e.g. [5-16]).  These can present new challenges for learning analytics (LA) as  less formal structure in a learning space leads to less easily  analysable forms of data.  Social media are soft technologies from  which emerge patterns and usages that are not part of the hard  design of the system but that are overlaid on top of it.  Without  clearly demarcated courses, lessons, learning outcomes and so on,  there is a need for a flexible analytics toolset that can be adapted  constantly to cater for many different structures of data.   Athabasca University (AU) is an entirely distance-based  university. To support its distributed population, we have built   Athabasca Landing, an Elgg-based beyond-the-LMS social  system. The Landing connects AU staff and students who are  distributed over a vast geographical region, providing a soft space  with rich tools and a comprehensive infrastructure for sharing and  connecting with others, including blogs, wikis, bookmark sharing,  photo sharing, event scheduling, group formation, social tagging,  file sharing, podcasting, video sharing, profile creation, social  networking and much more. It is more of a social construction-kit  than a purpose-driven space. It supports many social forms  including explicit groups,  social networks, and set-based  categorizations. It is as much as possible owned, shaped and  controlled by the people who use it. It is a persistent space not  defined by course or program temporal boundaries. There is no  prescribed way of structuring courses in Landing. Most Landing  users are not using it to support or attend a specific course and  have joined it to communicate and network with other users, and  those who are taking courses may engage with the site in many  other ways. Thus the activity of a user taking a course on Landing  is neither course specific nor is his or her course activity restricted  to the course.    1.2 Learning Analytics  LMSs are, typically, course-centric and follow a rigid structure,  thereby focusing and delimiting the scope of analytics. Courses  have defined durations and students are only active within those     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April May 2, 2012, Vancouver, BC, Country.  Copyright 2012 ACM 978-1-4503-1111-3/12/04 $10.00.      54    time frames and can only interact within the confines of the  course structure.    Social learning environments such as the Landing provide  enormous flexibility to instructors and learners alike and the  analytics of this data can potentially explain the influence of  extra-curricular activities on learner success such as the influence  of a learners social network and extra-course activities. However,  the blurred edges and flexible nature of the system presents a  problem for those attempting to analyse how it is used. There is  no strict definition of a course, course structure, a diverse set of  learning objects can be used in innovative ways, and the instructor  may choose to use the social space partially or fully for his course.    Analytics applied to course delivery on the Landing poses  numerous challenges when compared with that using a traditional  LMS but also provides unique opportunities not possible in the  formally bounded space. Students activity can be analyzed inside  and outside the context of the course so, for example, friends  network of a student and its correlation to student success can be  analyzed and, in principle, it should be possible to observe  diffusion of knowledge and connections beyond a fixed set of  formal learning transactions.    1.3 Related Work  Data from a database can be extracted in several different contexts  and tools have been proposed for different sorts of data extraction.  [16, 27]. DeLeS [27] offers useful analytics but the analytics is  limited to the functions programmed in the system. AAT [25] is  not only limited to pre-programmed analytics queries but also  allows users to specify data, create queries, and design reports  using a graphical user-interface. Neither DeLeS not AAT is  designed to work with social learning environments.   SNA software (Social network analysis) is frequently used to  analyse social systems data and many studies have been  conducting on social systems using SNA. SNA numerically or  visually describe features of a network for the purpose of  quantitative or qualitative analysis. SNAs predate social  networking and most are designed to analyse a network rather  than a social network. Cytoscape (www.cytoscape.org) is a  powerful network graphing and analysis tool that has been used in  anything from analysing social networks and web networks to  biological networks. Sonivis (www.sonivis.org) is geared more  toward wiki spaces. Recently, SNAs geared towards social  networking such as SocNetV [23], SNA-network [24], and Statnet  (www.statnet.org) have also appeared on the market but they only  provide analysis from a network perspective. To date, we have not  found a software tool designed to analyse formal learning within a  social network, leading to the decision to develop LASSIE.   2. LASSIE  LASSIE, available as a standalone software tool and as an Elgg  plugin (with limited features), enables the analysis of user  behaviour within and outside the context of formal learning.  LASSIE is capable of extracting data, graphing it, and performing  statistical calculations. It could be used to view data, trends, and  correlate data to facilitate interpretation. Data are displayed in  sortable tables and can also be downloaded in CSV format for  further analysis using tools such as spreadsheet software, SNAs or  R-language statistical packages.  It can be used to observe activity  of an individual user, a group of people, or the overall activity in  different time slices. Furthermore, it supports time slicing,  allowing users to view data from specific periods of interest.  LASSIE also provides a REST-based web service support for  future integration with SNAs and other software tools.   Initially, LASSIE was designed as an Elgg plugin.. To cater for  more data sources, LASSIEs development was forked into a  standalone application while maintaining the Elgg plugin as a cut- down version for simpler real-time and ad hoc queries. Figure 1  shows a screenshot of the standalone application.      Figure 1. Screenshot of LASSIE. LASSIE provides a   wizard-based user-friendly GUI with plenty of   documentation.   To cater for difficulties in defining course boundaries and user  activity related to courses, LASSIE allows users to define these  boundaries themselves. For example, a user may define course  activity to be all activity in a group and/or all activity by a group  of persons within a time period, and/or to filter according to  specified tags, or relationships with others such as the teacher.  LASSIE provides powerful and customizable reporting to  empower users to make informed and intelligent interpretations of  the data. Definitions of course, user activity, and learning objects  can be specified for a single analysis or globally re-used.   3. RESULTS AND DISCUSSIONS  Analytics of social systems is data rich, noisy, influenced by many  external factors and is therefore often difficult to interpret. For  example, loss of activity or significant reduction in activity of a  user for an extended period could be due to changes in personal  life, tragedies, changing priorities, or due to a technical hiccup in  the system.    LASSIE provides analytics in the form of raw, numerical, and  visual data for users, entities, entity groups, analytics of the entire  site and correlation between any of these.  Interpretation of data is  left to humans.    The system allows detailed and summarized tracking of user  activity on Landing and the usage of different content types over a  period of time. For example, Figure 2 shows a timeline of the  collective activity of a group of users who took a course. The two  high peaks correspond to first and last week of the course. As  expected, there is high level of activity at the start and end of the  course with some variation in the middle. The activity during the  course is higher than the activity before or after the course.  Activity on Landing continues to be higher after the course than  before the course but only with selected tools such as blogs. Not  surprisingly, spikes in activity in this course correspond to  assessments and teacher-specified activities.    LASSIE continues to provide very useful and insightful statistics  about the Landing, its content, and how it is used. Discussing all  the statistics and their interpretations is not possible in this paper,  but we provide a few examples to illustrate the kind of analyses  that are enabled by the system.   Analyses show that a small subset of students who were not active  before a course become active users during the course and remain  active users past the course end date. Students who do not  participate in extracurricular activities outside their courses tend   55    to become inactive upon termination of the course. Course designs  that favour social engagement will usually favour those who like  to engage socially and the time on task that results from this may  be the cause of greater motivation or the result of it.           Figure 2. Collective blog, file and page posting activity   before after and during the course. Two spikes   corresponds to first and last assignments of the course.     Figure 3 shows correlation between membership increase and  addition of new pages to Landing. It also shows a constant  decrease in the average content posted per user. Around 18  21  percent of the users are active users. It is interesting to observe,  however, that different tools encourage quite different patterns of  engagement: wiki pages, for instance, are intended to be used for  collaborative content development while blogs are used for  discussions on different topics.    An active user is difficult to define. Many users become highly  active or completely inactive for different periods of time. Many  users are active readers and viewers but hardly post anything on  the site. For the purpose of this discussion, an active user is  someone who posts at least one content entry per month.    Significant numbers of users continue to have a one-way  relationship with Landing. They read and view regularly but do  not participate with comments, votes, postings, or messages.      Figure 3. Most of the pages are posted by a smaller group   of users. As the number of members increases, this   becomes more apparent. Decimal average was multiplied   by 1000 to show the line in the graph.   Users have tool and content preferences. Most users tend to use  one or two tools (e.g. bookmarks, pages) most often. They tend to  participate more in courses where their preferred tools part of the  course structure.   Group activity spikes are generally reflected in activity of users  and spikes in activities of users are generally reflected in one or  more groups.   78% of Landing content was created by 12% of the users. 91% of  them are either directly linked or one link apart. A user following  someone or in a shared group is a direct link. If a users direct link  is linked to another user that he is not linked to, then the user is  one link distance apart. Generally, users who follow many other  users tend to be more active than people who follow fewer users.    Many different networks can be constructed from social systems.  Network of people following, network by groups, and content- posting network are some examples.  Since most networks are  based on some kind of activity, active users tend to be part of  larger networks. Many networks such as networks of followers  and followed users are scale-free networks. Degree distribution of  scale-free networks follows the Power Law. If a node is removed  from the network at random, there would be no significant change  in connectivity, however, if the a hub node is removed, the loss of  connectivity would be significant. Since the Landing project  started, only one relatively inactive member has closed the  account and three were banned, making it difficult to observe  Power Law properties. However, LASSIE allows us to step back  in time, remove a node and observe the effects. The findings  confirmed the assumption that the networks are scale-free and that  their degree distribution is governed by Power Law.   Median and least active users in a course tend to become inactive  after course completion. Active users tend to remain active in  Landing after course completion date. Users participating in extra- curricular activities on the Landing during a course tend to  continue participating in those activities past the course  completion date.    LASSIE provides valuable information about users over a  timeline. Currently, the meaning behind the details in a timeline  could be anyones guess, however, as the correlation capabilities  of LASSIE improves and predictive modeling capabilities are  added, it would be possible to identify in course and  extracurricular behavior which leads to success in a course.  Furthermore, this information would be highly valuable in  counseling learners and designing courses that would facilitate  learner success.   The ultimate goal of a learning analytics software system is to be  capable of clearly identifying activities and behaviour, which  would lead to greater levels of success in a course. Such a  software tool has yet to be invented for traditional or digital  classrooms. Currently LASSIE provides invaluable information  and tools to help us interpret the data. Many interesting  correlations have been uncovered, however, caution has been  taken not to jump to conclusions without further investigation,  which requires adding additional capabilities to the software tool.    Offering a course on a social system is not sufficient to improve  learner success. The course structure needs to take the course  material and user preferences into account. This cannot be  accomplished without analytics software that can show how users  are behaving in a social system and how they are responding to  course structure and their online social environment. LASSIE  provides this information but it is up to us to interpret the  analytics data accurately.   0 5 10 15 20 25 30 35 40 45  1 -S e p -1 0  1 -O ct -1 0  1 -N o v -1 0  1 -D e c- 1 0  1 -J a n -1 1  1 -F e b -1 1  1 -M a r- 1 1  blogs  files  pages  56    4. CONCLUSION  In this paper, the LASSIE analytics software tool has been  introduced and some examples provided of its use. It is a  powerful, user-friendly and flexible tool designed to analyse  learner and user activity in a social networking system.   LASSIE is sufficiently flexible to allow course designers and  instructors to explore beyond the course group area, allowing  them to evaluate the performance of courses, course structures, as  well as learner responses to new learning objects. It allows them  to better understand their students and their activities. This  information serves to help learning designers and instructors  adapt, extend, and revise the course material and activities to  achieve the pedagogical goals.   As it develops, the tool becomes softer, more flexible, more  capable of connecting disparate data. The softness and flexibility  of the tool, like the Landing itself, allows many creative and  unforeseeable uses. This means that greater care must be taken not  to jump to conclusions without cross-checking correlations and  their possible interpretations.    Our next major challenge is to extend LASSIE to be able to work  more effectively with the inverse set of those activities that are not  course related yet which lead to learning.   Analysis of a soft system, in which many of the technical and  organisational processes are not embodied in software but in  external systems and the minds of the users of that software  requires a slightly different approach from that used for systems  where goals, needs, methods and processes are clearer. Softer  technologies need softer analytics LASSIE, like the eponymous  sheepdog famed in Hollywood movies, has begun to herd and  make sense of the information we need.   5. REFERENCES  [1] Jon Dron,  2006. Any color you like, as long as its   Blackboard. In Proceedings of World Conference on E- Learning in Corporate, Government, Healthcare, and Higher  Education, (T. Reeves & S. Yamashita , eds.) 2772-2779.  VA: AACE   [2] Mona Singh, and Dawn Brooks. 1999. Topics in  Computational Molecular Biology.  http://www.cs.princeton.edu/~mona/Lecture/lecture-10- profiles.pdf.   [3] Mart Laanpere, H Poldoja, and Kaido Kikkas. 2004. The  Second Thoughts about Pedagogical Neutrality of LMS. In  Fourth IEEE International Conference on Advanced  Learning Technologies, 807-809.   [4] George Siemens. 2004. Learning Management Systems: The  wrong place to start learning.  http://www.elearnspace.org/Articles/lms.htm.   [5] Franoise Blin, and Morag Munro. 2008. Why hasnt  technology disrupted academics teaching practices  Understanding resistance to change through the lens of  activity theory. Computers & Education 50 (2): 475-490.   [6] Jon Dron and Terry Anderson. 2009. Lost in social space:  Information retrieval issues in Web 1.5. Journal of Digital  Information 10 (2): 475-490.   [7] Stephen Downes. 2009. Places to Go: Connectivism &  Connective Knowledge.  http://innovateonline.info/pdf/vol5_issue1/Places_to_Go- __Connectivism_&_Connective_Knowledge.pdf   [8] Neil Ford, Melissa Bowden, and Jill Beard. 2011. Learning  together: using social media to foster collaboration in higher  education. In Cutting-edge Technologies in Higher  Education, 105-126. Emerald Group Publishing Limited.   [9] Rebecca Galley, and H Poldoja. 2010. Cloudworks as a  pedagogical wrapper for lams sequences: supporting the  sharing of ideas across professional boundaries and  facilitating collaborative design, evaluation and critical. In  Proceedings of the 2010 European LAMS & Learning  Design Conference. Oxford, UK.  http://www.tewtjournal.org/VOL%2011/ISSUE%201/volum e_11_issue_01-05_article_03.pdf.   [10] Andrew Francesco Chiarella,. 2009. Enabling the collective  to assist the individual: A self-organising systems approach  to social software and the creation of collaborative text  signals. MCGILL UNIVERSITY.  http://gradworks.umi.com/NR/66/NR66585.html.   [11] Nicole B. Ellison, Charles Steinfield, and Cliff Lampe. 2007.  The Benefits of Facebook Friends: Social Capital and  College Students Use Online Social Network Sites. Journal  of Computer-Mediated Communication 12 (4): 1143-1168.   [12] Ralf Klamma, Mohamed Amine Chatti, Erik Duval, et al.  2006. Social Software for Professional Learning: Examples  and Research Issues. In Advanced Learning Technologies,  2006. Sixth International Conference on, 912 - 915.   [13] Jon Dron. 2006. Social Software and the Emergence of  Control. In Proceedings of the Sixth IEEE International  Conference on Advanced Learning Technologies. IEEE  Computer Society Washington, DC, USA.   [14] Ulises Mejias. 2005. A Nomads Guide to Learning and  Social Software. Commonwealth of Australia.  http://knowledgetree.flexiblelearning.net.au/edition07/html/l a_mejias.html.   [15] Stephen Downes., 2007. Learning networks in practice.  Emerging Technologies for Learning (David Ley, ed). 2007.  http://  www.downes.ca/files/Learning_Networks_In_Practice.pdf.   [16] Hendrik Drachsler. 2009. Navigation Support for Learners in  Informal Learning Networks. The Netherlands: Open  University of the Netherlands.  http://dspace.ou.nl/handle/1820/2084.   [17] Rita Kop. 2011. The challenges to connectivist learning on  open online networks: Learning experiences during a  massive open online course. The International Review of  Research in Open and Distance Learning 12 (3).  http://www.you-learn.org/papers.html.   [18] Charles M. Reigeluth, Carr-Chellman, A.A.  (ed). 209AD.  Instructional-Design Theories and Models: A New Paradigm  of Instructional Theory. Vol. III. New York.   [19] Sabine Graf, Cindy Ives, Nazim Rahman, and Arnold Ferri.  2011. AAT  A Tool for Accessing and Analysing Students  Behaviour Data in Learning Systems. In 1st International  Conference on Learning Analytics and Knowledge.   [20] Riccardo Mazza, and Christian Milani. 2005. Exploring  usage analysis in learning systems: gaining insights from  visualisations. In Workshop on Usage Analysis in Learning  Systems at the International Conference on Artificial  Intelligence in Education. AIED. Springer.   57    [21] Sabine Graf, Kinshuk, and Tzu-Chien Liu. Supporting  Teachers in Identifying Students Learning Styles Learning  Management Systems: An Automatic Student Modelling  Approach. Educational Technology & Society 12 (4): 3-14.   [22] SocNetV. http://socnetv.sourceforge.net.   [23] SNA-network. http://search.cpan.org/~obradovic/SNA- Network-0.13      58      "}
{"index":{"_id":"12"}}
{"datatype":"inproceedings","key":"Zhuhadar:2012:CLR:2330601.2330621","author":"Zhuhadar, Leyla and Yang, Rong","title":"Cyberlearners and Learning Resources","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"65--68","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330621","doi":"10.1145/2330601.2330621","acmid":"2330621","publisher":"ACM","address":"New York, NY, USA","keywords":"e-learning, learning analytics, modularity, social network analysis, social recommender system, social web, visual analytics","Abstract":"The discovery of community structure in real world networks has transformed the way we explore large systems. We propose a visual method to extract communities of cyberlearners in a large interconnected network consisting of cyberlearners and learning resources. The method used is heuristic and is based on visual clustering and a modularity measure. Each cluster of users is considered as a subset of the community of learners sharing a similar domain of interest. Accordingly, a recommender system is proposed to predict and recommend learning resources to cyberlearners within the same community. Experiments on real, dynamic data reveal the structure of community in the network. Our approach used the optimal discovered structure based on the modularity value to design a recommender system.","pdf":"Cyberlearners and Learning Resources  Leyla Zhuhadar Dept. of Computer Engineering and Computer  Science Knowledge Discovery and Web Mining Lab  University of Louisville, USA leyla.zhuhadar@wku.edu  Rong Yang Dept. of Mathematics and Computer Science  Western Kentucky University, USA rong.yang@wku.edu  ABSTRACT The discovery of community structure in real world net- works has transformed the way we explore large systems. We propose a visual method to extract communities of cy- berlearners1 in a large interconnected network consisting of cyberlearners and learning resources. The method used is heuristic and is based on visual clustering and a modularity measure. Each cluster of users is considered as a subset of the community of learners sharing a similar domain of in- terest. Accordingly, a recommender system is proposed to predict and recommend learning resources to cyberlearners within the same community. Experiments on real, dynamic data reveal the structure of community in the network. Our approach used the optimal discovered structure based on the modularity value to design a recommender system.  Keywords Learning Analytics, E-learning, Social Web, Social Network Analysis, Visual Analytics, Modularity, Social Recommender System.  1. INTRODUCTION Nearly 20 years ago, it seemed that the Worldwide Web  (WWW) played a considerable role in facilitating the way people share information. Today, it is obvious that the Web is not only about sharing information, but it is a place where people create, share, interact and learn. If we view the Web purely, like a natural phenomenon, and if we study, say, the behaviors of learners using the social media networks to learn how people learn, this could answer fundamental ques- tions about human behaviors and the impact of the Internet on the social process.  Using analytics to discover hidden patterns in big data  1The term cyberlearner centers on the idea of using on- line learning tools in an educational setting. This term was coined for the first time by the National Science Foundation. For more information, refer to http://cyberlearning.sri.com.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK12, 29 April - 2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM 978-1-4503-1111-3/12/04 ... $10.00.  started a long time ago in fields such as business intelligence and predictive marketing. Nevertheless, recently, big organi- zations, such as EDUCAUSE2 and the Bill & Melinda Gates Foundation3, are playing an essential role in bridging higher education and data analytics [17].  2. BACKGROUND AND RELATED WORK The term Learning Analytics occurred the first time in  Berk [3], but it was more related to business intelligence as stated in Shum et al.[22]. Noticeably, over the last two years, more research related to learning analytics occurred. The following summary does not exhaust the vast amount of research done in this area, but it gives a short summary. For example, Gasevic et al.[11] experimented with LOCO- Analyst4 on two masters level courses for learning analyt- ics with feedback; their goal was to discover the semantic knowledge based on community and accordingly update the learning process of this community. Johnson et al.[14] men- tioned some learning analytics tools that have emerged in higher education: i) Northern Arizona University uses an academic early alert and retention system5 to improve stu- dents retention and success; ii) Purdue University utilizes Signals6, an analytical data mining tool that identify stu- dents who need help; iii) Ball State University designed a visualized collaborative writing system to help better eval- uate student performance; iv) the University of Wollongong in Australia uses SNAPP7 to visualize data from discus- sion boards to find patterns in students behaviors. The last example is the closest to our system. The similarity be- tween both systems is twofold: goal and method. The goal is to collect data about students to better understand their behaviors and accordingly provide a better understanding on how to help these students. There is a major difference, however between our approach and theirs; in our system this help comes as a recommendation based on similar a commu- nity of learners. We consider the similarity in both context and in communities in social networks; whereas, SNAPP is based only on context. Both systems use visualization as a method to discover patterns. As we know, our cognitive  2http://www.educause.edu/ 3http://nextgenlearning.org 4LOCO-Analyst is an educational tool that provides feed- back to teachers about learners in a specific course, the aim of this system is to improve content and structure of online courses: http://jelenajovanovic.net/LOCO-Analyst 5http://www4.nau.edu/ua/GPS/student/ 6http://www.itap.purdue.edu/tlt/signals/ 7http://research.uow.edu.au/learningnetworks  65    system is not able to deal with vast amounts of informa- tion [6]. The importance of communities in social networks has long been recognized. In 1999, Kleinberg et al.[16] used the concept of a bipartite core to identify Web communi- ties. Flake et al.[8] introduced one of the most attractive definitions for a community, both because of its intuitive appeal and its computational simplicity. There, a commu- nity is defined as a set of web pages in which each member page has more links (in either direction) inside the commu- nity than outside. The exact proportion of inside to outside links can be varied as required. Girvan and Newman [12, 19] devised a method for community determination based on be- tweenness centrality by generalizing it to edges and finding communities by deleting edges from the network in order of decreasing betweenness (and recalculating the between- ness between deletions). Modularity, introduced in Newman and Girvan [20], has become a standard method for measur- ing the success of community decomposition in a network. This measure was turned into a fast and effective commu- nity identification mechanism by using a greedy algorithm to approximately optimize the modularity values. Even this fast algorithm was improved in Clauset et al.[5]. For a recent and much more comprehensive survey see Fortunato [9].  3. METHODOLOGY  3.1 Data Collection The data set used in this study is part of HyperMany-  Medias (HMM) Logfile. We extracted and used only the last six months from the Logfile. In our previous research Zhuhadar et al.[23], we found that i) the profiles of our users are evolving (users interests change over time; i.e., a user might register for courses in chemistry, but after three months, the same users switch to courses in biology) and ii) our platform is an evolving domain (new courses are added to the platform each semester). Accordingly, we provide recommendations based on this dynamic change of students interests. Also, we argue that building a dynamic recommender system based on a social network needs to be scalable to accommodate current and new users. If we con- sidered using the whole Logfile which consists of activities of 750,000 users so far, the time needed to extract recom- mendations from the best candidates in the Logfile, on the fly, would be impractical. The data set used in this research consists of users logs during the following period (2/1/2011- 8/1/2011). Each entry has the following fields: user name, visited resource, number of visits. The number of visits is used as a Weighted Degree in the graph. The more the user (learner) visited a learning resource, the closer the learner is considered to the hub (learning resource); therefore, users who are close to hubs are considered as authorities in that specific domain. Our assumption is built upon the theory of reinforced learning. A very old concept that was introduced in 1913, by Ebbinghaus, in [7]. Ebbinghaus found that if learners are introduced to a problem over many trials, an exponential learning curve is produced. Finally, we visu- alized our Logfile using a graph analysis tool called Gephi [2].  3.2 Evaluation Method (Categorization Cri- teria and Determining the Energy Levels)  As we discussed in section 2, there is a variety of measures and methods for finding communities in social networks. In  this research, a modified version of the modularity measure proposed by Blondel et al. [4] is utilized to compare the quality of clusters (Equation 1) for measuring the success of a community decomposition of a network. This mea- sure is considered fast and effective to identify communities by using a greedy algorithm to approximately optimize the modularity values.  Q = 1  2m   [Aij   (kikj)  2m ]d(ci, cj), (1)  where Aij represents the weight of the edge between i and j, ki =   j Aij is the sum of the weights of the edges  attached to vertex i, ci is the community to which vertex i is assigned, the d-function d(u, v) is 1 if u = v and 0 otherwise and m =   ij Aij .  3.3 Detecting Communities In this section, we identify the methods used to detect  communities of similar users from our extracted Logfile. First, we defined a set of various force laws to recognize commu- nities in the network structure. Once the communities have been recognized, we ensured that each community had its own energy state which determines the relevance level of that particular community within its range of proximity. We used force directed methods to discover the similarity between users. Three types of methods were used: (i) the Yifan Hu Algorithm [13], (ii) the Fruchterman and Reingold Algorithm [10], and finally, the Force Atlas 2 Algorithm [18].  4. EXPERIMENTAL ANALYSIS We deployed the three algorithms (i) Yi Fan Hu Algo-  rithm, (ii) Fruchterman and Reingold Algorithm, and (iii) Force Atlas 2 Algorithm on HyperManyMedias Logfile ex- tracted during the period of (2/1/2011- 8/1/2011). Af- ter filtering out some data based on the conditions (users visits>= 10 & length of accumulated sessions>= 30 min- utes) and deleting outliers, our network consisted of 8, 510 Nodes (# of users) and 23, 079 Edges (# of edges between users and learning resources). Each edge connects a user (learner) to a learning resource. In this small portion of the Logfile, the number of learning resources is  10, 000 learning objects.  First, we noticed that the Yifan Hu Algorithm [10] proved to be efficient. It seems to overcome the localized nature of the Kernighan-Lin Algorithm [15] and also the local minima of the Fruchterman and Reingolds Algorithm [10]. We also deployed the Force Atlas 2 Algorithm, calculated the mod- ularity for each force directed method, and visualized the network.  4.1 Force Directed Method (Fruchterman Rein- gold Algorithm)  We used three parameters in the Fruchterman Reingolds [10] force directed method: (i) Area (which defines the num- ber of nodes in the graph); (ii) Gravity (it works to attract all nodes to the center to avoid dispersion of disconnected components); and (iii) Speed (convergence speed). We ran 20 trials and the best results obtained in trial 6 had a mod- ularity of 0.606 and number of communities (clusters) of 14. Accordingly, we present the social network structure in Fig- ure 1.  66    Figure 1: Social network structure for HMMs Log- file (Fruchterman Reingold)  Figure 2: Social network structure (Force Atlas 2)  4.2 Force Directed Method (Force Atlas 2 Al- gorithm)  The Force Atlas 2 Algorithm [18] uses a classic force- vector, similar to the Fruchterman Rheingold. This algo- rithm benefits from Barnes-Hut optimization techniques [1] and its own repulsive and tolerance levels [1]. We ran 20 trials and the best results obtained in trial 18 had a modu- larity of 0.610 and number of communities (clusters) of 14. Accordingly, we present the the social network structure in Figure 2. We noticed that we got the best results in this method when there is (i) a little repulsive force given by Scaling and (ii) a higher attractive force given by Gravity.  4.3 Force Directed Method (Yi fan Hu Algo- rithm)  The Yifan Hu Algorithm [13] overcomes local minima by using Barnes and Huts [1] octree technique which approxi- mates the short-and-long range force efficiently. It uses the  Table 1: Evaluation of the three Force-directed methods  Algorithm Modularity # of Communities  Fruchterman and Rheingold 0.606 14  Force Atlas 2 0.610 14  Yi Fan Hu 0.607 15  Figure 3: Social network structure (Yi-fan Hu)  adaptive cooling schemes and general repulsive force models to develop the set of forces to be applied on the data set for formation of the communities. We ran 20 trials and the best results obtained in trial 12, had a modularity of 0.607 and the number of communities (clusters) of 15. Accordingly, we present the the social network structure in Figure 3.  4.4 Discovering the Best Communities Struc- ture in HMMs Logfile Social Network  To summarize, by running multi-trials, we discovered the best combinations of parameters for each algorithm, as shown in Table 1. However, we noticed a slight difference in the results that could be inferred as follows: We found more clusters using the Yi fan Hu Algorithm; whereas, we obtained a better modularity measure using the Force Atlas 2 algorithm. Therefore, we decided to use the Force Atlas 2 algorithm for clustering. Figure 2 presents the notion of detecting communities of users in the Social Web.  4.5 Designing a Social Recommender System We considered that providing recommendations to a learner  based on similarity metrics between the users and him/her and extracted from the social network would answer this question. We propose adding a social recommender system to HMM repository where recommendations provided to a user (learner) is based on detecting triangles in the commu- nity [21], refer to Figure .  Figure 4: Adding personalized recommendations to a user profile based on three users (Triangle)  67    5. CONCLUSIONS We consider our research is different than any other re-  search that has been done on detecting community using graph-based methods for the following reasons: i) our re- search is an applied study on a real platform visited by thousands of users on a daily basis; ii) we used data collected from HyperManyMedias Logfile to discover communities in social networks; iii) finally, we proposed the triads concept for recommendations, keeping in mind that our current ex- periments are based on triangles of nodes. In our future work, we plan to experiment and compare the results based on learners feedback. In addition, we plan to complete our evaluation of the visual recommender system, using objec- tive metrics as well as user testing.  6. REFERENCES [1] Barnes, J., and Hut, P. A hierarchical 0 (n log iv)  force-calculation algorithm. Nature 324 (1986), 4.  [2] Bastian, M., Heymann, S., and Jacomy, M. Gephi: An open source software for exploring and manipulating networks. In International AAAI Conference on Weblogs and Social Media (2009), pp. 361362.  [3] Berk, J. The state of learning analytics. report for american society for training and development.  [4] Blondel, V., Guillaume, J., Lambiotte, R., and Lefebvre, E. Fast unfolding of communities in large networks. Journal of Statistical Mechanics: Theory and Experiment 2008 (2008), P10008.  [5] Clauset, A., Newman, M., and Moore, C. Finding community structure in very large networks. Physical Review E 70, 6 (2004), 066111.  [6] Dillon, T., Chang, E., et al. Overview of cognitive visualisation. In Digital Ecosystems and Technologies Conference (DEST), 2011 Proceedings of the 5th IEEE International Conference on (2011), IEEE, pp. 138142.  [7] Ebbinghaus, H. Memory: A contribution to experimental psychology. Teachers College, Columbia University, 1913.  [8] Flake, G., Lawrence, S., Giles, C., and Coetzee, F. Self-organization and identification of web communities. Computer 35, 3 (2002), 6670.  [9] Fortunato, S. Community detection in graphs. Physics Reports 486, 3-5 (2010), 75174.  [10] Fruchterman, T., and Reingold, E. Graph drawing by force-directed placement. Software- Practice and Experience 21, 11 (1991), 11291164.  [11] Gasevic, D., Zouaq, A., Torniai, C., Jovanovic, J., and Hatala, M. An approach to folksonomy-based ontology maintenance for learning environments. IEEE Transactions on Learning Technologies, 99 (2011), 11.  [12] Girvan, M., and Newman, M. Community structure in social and biological networks. Proceedings of the National Academy of Sciences 99, 12 (2002), 7821.  [13] Hu, Y. Efficient, high quality force directed graph drawing. Mathematica Journal 10, 1 (2005), 3771.  [14] Johnson, L., Smith, R., Willis, H., Levine, A., and Haywood, K. The 2011 horizon report.  [15] Kernighan, B., and Lin, S. An efficient heuristic procedure for partitioning graphs. Bell System  Technical Journal 49, 2 (1970), 291307.  [16] Kleinberg, J., Kumar, R., Raghavan, P., Rajagopalan, S., and Tomkins, A. The web as a graph: Measurements, models, and methods. In Proceedings of the 5th Annual International Conference on Computing and Combinatorics (1999), Springer-Verlag, pp. 117.  [17] Long, P., and Siemens, G. Penetrating the fog: Analytics in learning and education. EDUCAUSE Review 46, 5 (Sept. 2011), 3032+.  [18] Mathieu Jacomy, Sebastien Heymann, T. V. M. B. Force atlas 2, a graph layout algorithm for handy network visualization.  [19] Newman, M., and Girvan, M. Mixing patterns and community structure in networks. Statistical Mechanics of Complex Networks (2003), 6687.  [20] Newman, M., and Girvan, M. Finding and evaluating community structure in networks. Physical review E 69, 2 (2004), 026113.  [21] Serrour, B., Arenas, A., and Gomez, S. Detecting communities of triangles in complex networks using spectral optimization. Computer Communications (2010).  [22] Shum, S. B., and Ferguson, R. Social Learning Analytics.  [23] Zhuhadar, L., Nasraoui, O., and Wyatt, R. Dual representation of the semantic user profile for personalized web search in an evolving domain. In Proceedings of the AAAI 2009 Spring Symposium on Social Semantic Web, Where Web, vol. 2, pp. 8489.  68      "}
{"index":{"_id":"13"}}
{"datatype":"inproceedings","key":"Cambridge:2012:FST:2330601.2330622","author":"Cambridge, Darren and Perez-Lopez, Kathleen","title":"First Steps Towards a Social Learning Analytics for Online Communities of Practice for Educators","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"69--72","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330622","doi":"10.1145/2330601.2330622","acmid":"2330622","publisher":"ACM","address":"New York, NY, USA","keywords":"education, learning analytics, online communities of practice, paradata, professional development, social network analysis","Abstract":"Learning analytics has the potential to provide actionable insights for managers of online communities of practice. Because the purposes of such communities and the patterns of activity that might further them are diverse, a wider range of methods may be needed than in formal educational settings. This paper describes the proposed learning analytics approach of the U. S. Department of Education's Connected Educatorsproject, and presents preliminary applications of social network analysis to the National Science Teachers Association Learning Center as an illustration.","pdf":"First Steps Towards a Social Learning Analytics for Online  Communities of Practice for Educators   Darren Cambridge  American Institutes for Research  1000 Thomas Jefferson St NW  Washington, DC, 20007, USA   +1-202-403-6924   dcambridge@air.org   Kathleen Perez-Lopez  American Institutes for Research  1000 Thomas Jefferson St NW  Washington, DC, 20007, USA   +1-202-403-5651   kperez-lopez@air.org       ABSTRACT  Learning analytics has the potential to provide actionable insights  for managers of online communities of practice. Because the  purposes of such communities and the patterns of activity that  might further them are diverse, a wider range of methods may be  needed than in formal educational settings. This paper describes  the proposed learning analytics approach of the U.S. Department  of Educations Connected Educatorsproject, and presents  preliminary applications of social network analysis to the National  Science Teachers Association Learning Center as an illustration.    Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning   General Terms  Measurement, Human Factors   Keywords  Learning Analytics, Online Communities of Practice, Professional  Development, Social Network Analysis, Education, Paradata    1. INTRODUCTION  This paper provides an overview of research and development  work being conducted by the American Institutes for Research  through the Connected Educators project, under contract from the  United States Department of Educations Office of Educational  Technology. In accordance with the National Educational  Technology Plan, Connected Educators intends to help educators  at the school, district, and state level across the U.S. become more  connected with resources and with each other in order to enhance  their professional effectiveness [13].    Online communities of practice (OCoPs) are a key means for  helping educators connect. The projects initial environment scan  yielded considerable evidence that online communities of practice  are becoming increasingly prevalent and effective means for  professional learning in education [9]. The research synthesized in  our initial report has shown that OCoPs can help educators access,   share, and create knowledge and develop professional identity in  ways that go beyond what is possible through face-to-face  engagement alone.     This research also shows that effective leadership and moderation  is key to the success of OCoPs in supporting these activities [1, 6,  7]. In small OCoPs, a community manager may be able to read all  of the member-contributed content and discussion and come to  know most or all of the participants. Once an OCoP reaches a  certain scale, however, this coverage becomes impossible.  Division of labor is one approach to being responsive to the  emerging dynamics of community activity and relationships, but it  is also helpful for the manager to have a systematically generated,  holistic picture of what is going on. Learning analytics may help  provide that picture, drawing on the considerable volume of data  exhaust generated by online community activity. Beyond basic  Web analytics, these data are largely an untapped resource for  practitioners.    COCP is eager to explore ways in which pioneering work in the  field can be applied to OCoPs for educators. However, most  learning analytics work of which we are aware has so far focused  on structured learning experiences for students, such as semester- long online courses or discussions on a blog. Learning within  OCoPs likely differs from learning in these contexts in several  ways. First, the learning experience is not time bounded. Second,  participation is usually voluntary, with individual participants  coming to the community with different needs that correspond  with a range of styles of engagement [10]. Third, the primary  motivation for engaging with OCoPs is often solving problems of  practice rather than learning for its own sake. Finally, experience  has shown that the value OCoPs offer is multi-dimensional, that  causal relationships between different types of value are  challenging to establish, and that understanding of the success of  the shared enterprise may change over time [15]. All these  differences suggest that it may be more difficult to determine  which outcomes of collective activity are valuable and what  patterns of activity are more significant in OCoPs than in formal  educational settings.    Learning analytics for OCoPs may, therefore, need to be more  exploratory than for formal educational experiences. Learning  analytics should help community managers see a range of  potentially notable patterns rather than simply tracking progress  towards pre-defined indicators of success. Precisely because the  range of potentially valuable patterns of content and activity are  so diverse, learning analytics is likely to be powerful in enabling  community managers and moderators to invest their expert  interpretive attention more efficiently.       Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   69    2. PLANNED APPROACH  Making visible the wide range of patterns of activity that are  potentially actionable likely requires multiple methods. Initially,  we plan to utilize learning analytics in two of the five categories  proposed by Buckingham Shum and Ferguson [3], social learning  network analysis and social learning context analysis. Social  network analysis is our chosen method in the first category, and  we are employing pre-hypothesis narrative analysis in the second.  In future work, we hope to also explore content and discourse  analysis.    2.1 Social Network Analysis  Our work with social network analysis (SNA) is furthest along,  and an example of its application is presented in the next section.  SNA has been used in previous research on OCoPs in education,  but primarily to analyze discrete discussions or friend networks  [2, 8]. In contrast, we are using SNA to explore as wide a range of  relationships between community participants and content objects  as is possible for a given community, beginning from whole- network maps such as those presented in the next section and only  narrowing the scope of the analysis when potentially significant  patterns or individuals have been identified.    By representing OCoP platform usage data as a unimodal network  of relationships between individual community membersthe  approach common in learning analytics applications of SNA, such  as Social Networks Adapting Pedagogical Practice (SNAPP)  [5]we are seeking to identify community members who are  particularly significant to the health of the community, either  because they are highly influential, as signified by metrics such as  eigenvector centrality, or because they frequently connect  subgroups, as signaled by high betweenness centrality. These  individuals may merit additional support or recognition from the  community managers.    In addition, we are visualizing the data as bimodal social network  diagrams that explore the relationships between people and  content objects, as illustrated in the next section. This alternative  representation allows us to see patterns in subgroup activity that  might not otherwise be detectable. For selected individuals  identified as significant in either representation, we will create  egocentric maps of their usage. If permission can be obtained, we  will also create egocentric maps of their usage of Facebook and  Twitter to obtain a fuller picture of how they connect with other  professionals online. Patterns in this usage may suggest strategies  individuals can use to increase the impact of their participation, as  well as criteria for identifying potentially effective participants to  recruit into the OCoP.    2.2 Pre-Hypothesis Narrative Analysis  Analysis of usage data can yield a rich representation of the  dynamic of online activity, but understanding the impact of that  activity on offline professional practice can be furthered through  the collection of self-report data. To collect and analyze self- report data, we are employing Snowdens [12] narrative analysis  approach to identify patterns in the context of OCoP members  experiences. In this method, narrative fragmentsbrief stories are collected online using the CognitiveEdge SenseMaker  Collector software, which also asks the respondent a series of  closed survey questions about the context of the narrative, called  filters. Community members recording and classifying narratives  are given unique identifiers, making it possible to add their  centrality metrics from SNA.     Snowden calls this approach pre-hypothesis narrative research  because it is designed to help analysts identify emergent   patternsweak signals in the terminology of complex systems  theorythat might be missed if the impact of their cognitive  biasesin this case, the beliefs they hold about what aspects of  community context are significant enough to attend toare not  minimized. Techniques for minimizing bias include using  prompting questions that encourage both strongly positive and  negative stories, developing filter questions that mask the desired  outcomes, and soliciting stories from a large, diverse group of  respondents.    Most significantly, analysts begin analysis by using the  SenseMaker Explorers powerful visualization capabilities to look  for patterns in the quantitative filter question data (including, in  our case, SNA metrics) prior to analyzing the content of the  narratives themselves. This abstraction helps to reduce the  influence of the analysts interpretive predilections. Once a  pattern is identified, researchers can drill down into the content of  the narratives associated with it, which can become the subject of  content analysis. This method of selecting samples of stories to  analyze has the added benefit of making narrative analysis more  time efficient as a formative evaluation strategy.   2.3 Content and Discourse Analysis   Eventually, we also hope to systematically analyze the content of  members contributions to the communities, such as discussion  posts and blog entries, as well as the narratives collected from  them. We are particularly interested in extracting significant  semantic concepts using automated tools such as Open Calais.  Such automated analysis has the potential to help community  managers discover emerging topics of interest to the community  that could be incorporated into its editorial calendar.    We are also evaluating tools for discourse analysis. These tools  have the potential to help community managers understand what  styles of discourse are most often associated with which  community outcomes, enabling them to encourage the style most  likely to help the community achieve its purpose.    3. CURRENT WORK  At present, we have begun SNA work in three OCoPs. Here, we  present some very preliminary findings on the National Science  Teachers Association Learning Center (NSTA LC) to illustrate  our direction in working with these communities. The NSTA LC,  launched in April 2008 through the efforts of Dr. Al Byers and his  colleagues, aims to support science teachers in increasing their  knowledge of science and of pedagogy [4]. It provides a rich  source of (mostly free) learning materials and experiences for  science teachers. The NSTA LC also hosts an online community  through its Community Forums. NSTA members can initiate  topics within any of a number of forums, or post to existing  topics.       Figure 1: One year of 6978 posts made by 492 NSTA members    to 557 topics within 21 forums.   70    We received NSTA LC forum posts for the full year, from  9/24/2010 to 9/28/2011.  As a preliminary analysis of this forum  activity, we used NodeXL [11], an open-source template for  Microsoft Excel, to create bimodal network diagrams of the  6,978 posts made by 492 members to 557 topics within 21 forums  during that time. Figure 1 is a low-resolution depiction of the  patterns of these posts as edges between member nodes (black  triangles along the left) and topic nodes (diamonds, along the  right, colored according to their forums; 13 forums were private,  labeled PrvF<n>). Member nodes are sized according to the  number of different topics to which they posted; topic nodes are  sized according to the number of posts made to them. Topic nodes  are grouped by forum; within their forum group they are placed  left to right by number of posts made to them. The member nodes  and the forum groups are ordered top to bottom by their total  number of posts. Edge color (black to yellow) and opacity are  logarithmically proportional to the number of times a member  posted to the topic. A frequency analysis (not shown) indicates  greatly skewed distributions within the post data: a few forums  received most of the posts; a few members initiated most of the  topics and made most of the posts; and a few topics received half  the posts. The figure captures this, with dense dark edges in the  upper portion of the network, between the relatively few members  and topics.   However, the distribution of edges is not smooth from the upper  portions of the network to the lower, and there appear to be  different concentrations of edges in some regions. There might be  some interesting activity by members in those regions, but with  this static view, it is difficult to see what that could be.    To tease out this information, we separated the data into 5  contiguous periods, Q1Q5, each containing one-fifth of the  posts, and created network diagrams for each period. These   diagrams are shown in Figure 2. Nodes are placed exactly as in  Figure 1 (i.e., according to total annual number of posts), but their  sizes are relative to the number of posts made during the quintile.  Likewise, edge color and opacity are relative to the data in the  quintile.    During the initial period, Q1, the activity is mostly by a very few  active members, and there is very little activity in the lower part  of the figure. During Q2 there are many new members, but their  posting activity is fairly light. In Q3 something interesting  develops: very heavy posting to the private forum Prv18 (pink,  mid-diagram), mostly by moderately active posters, but also from  a number of new members. The PrvF18 posts all but disappear in  Q4, but the mid-active members remain somewhat active during  this period. By Q5, they are posting quite heavily, and now to the  more standard forums of Life Science, Earth and Space Science,  and Physical Science. We need to examine these data further, but  it is possible that this time series of network views has highlighted  something that could prove useful to community managers. It  might show how time-bounded activity targeted at some subgroup  could be leveraged into more sustained and general engagement.   The next steps in our analysis will include using the topic initiator  information to create initiator-topic and member-initiator  networks; transforming the bimodal data to create unimodal,  member-member diagrams; obtaining additional member  information, such as which online seminars they are attending,  who is a member of a cohort (a district-wide NSTA LC  professional development plan), and the points and badges they  have attained for their activity in the Learning Center. We will  also examine the social network analysis metrics produced by  NodeXL, such as the centrality measures discussed in the previous  section.          Q1                                               Q2                     Q3                                                                                   Q4            Q5   Figure 2. Network diagrams for five quintiles. Nodes are placed exactly as in Figure 1 (i.e., according to total annual number of  posts), but their sizes are relative to the number of posts made during the quintile period. Likewise, edge color and opacity are   relative to the data in the quintile.   71    4. FUTURE WORK  Overall, our work with learning analytics has two goals. The  first is a traditional goal of research, to increase our knowledge  of how OCoPs work and how to use them effectively. Our  second, and perhaps ultimately more important, goal is to give  community leaders and participants tools and techniques that  can help them make better choices about leadership of and  participation in such communities as part of their routine  professional practice. By the conclusion of the Connected  Educators project, we hope to offer tools that community  managers themselves can use to analyze usage data analogous to  what Social Networks Adapting Pedagogical Practice (SNAPP)  offers for teachers using learning management systems.   The NSTA example illustrates both goals. The general pattern of  participation in a time-bounded subgroup transforming into  more general sustained participation, should it also be observed  elsewhere within the NSTA LC and in other OCoPs, may help  us understand one way that individuals become persistently  engaged in OCoPs. The specific pattern of the PrvF18 forum  contributors becoming active in other popular forums may help  NSTA managers identify other subgroups within the current  membership that could be supported in making the same  transition. Connected Educators might offer managers a set of  NodeXL data providers, settings files, and macros that make  such identification less complicated.    Although our current primary focus is on learning analytics in  the service of effective management and moderation of OCoPs  for educators, we share Buckingham Shum and Fergusons [3]  conviction that learning analytics should also be put into the  service of helping individual learners. Learning analytics has the  potential to guide individual educators in choosing how to  connect with others online in support of their professional goals.  For this potential to be realized, however, data about online  participation and resource use need to be shared across  community and network contexts.    We are encouraged that the Learning Registryan open,  distributed infrastructure for learning resource sharing and  discovery that launched in November 2011enables sharing not  just metadata but also paradata about resources [14]. Paradata  capture how resources are used and evaluated, representing the  contexts of learning. If privacy issues can be addressed, in the  future the usage and self-report data we are analyzing within  OCoPs could be shared across them as paradata via an  infrastructure similar to the Learning Registry. Enabling  educators to use learning analytics to examine distributed  records of OCoP engagement at scale could help educators  connect with each other much more powerfully and efficiently  than is possible today.      5. REFERENCES  [1] Babinski, L. M., Jones, B. D., and DeWert, M. H. 2001.   The roles of facilitators and peers in an online support  community for first-year teachers. Journal of Educational  Psychological Consultation, 12, 2, 151169.   [2] Bonsignore, E., Hansen, D., Galyardt, A. et al. 2010. The  power of social networking for professional development.  in Gray, T. and Silver-Pacuilla, H. eds. Breakthrough  Teaching and Learning: How Educational Assistive  Technologies are Driving Innovation. Springer, New York,  2552.    [3] Buckingham Shum, S., and Ferguson, R. 2011. Social  Learning Analytics. Technical Report KMI-11-01.  Knowledge Media Institute, Open University, UK.  Retrieved November 11, 2011 from  http://kmi.open.ac.uk/publications/pdf/kmi-11-01.pdf.    [4] Byers, A., Sherman, G., Chadwick, K., and Mendez, F.  2011. Online PD: Applying What Research Says For  Effective Learning. Retrieved November 11, 2011 from  National Science Teachers Association:  http://learningcenter.nsta.org/research/OnlinePD_Applying _What_Reseach_Says_RDC.pdf.    [5] Dawson, S. 2009. Seeing the learning community: An  exploration of the development of a resource for  monitoring online student networking. British Journal of  Educational Technology, 41, 5, 736752.   [6] Farooq, U., Schank, P., Harris, A., Fusco, J., and Schlager,  M. 2007. Sustaining a community computing infrastructure  for online teacher professional development: A case study  of designing Tapped In. Computer Supported Cooperative  Work, 16, 4, 397429.   [7] Gareis, C. R., and Nussbaum-Beach, S. 2007.  Electronically mentoring to develop accomplished  professional teachers. Journal of Personnel Evaluation in  Education, 20, 3, 227246.   [8] Hansen, D. L., Shneiderman, B., and Smith, M. A. 2011.  Analyzing Social Media Networks with NodeXL: Insights  from a Connected World. Morgan Kaufmann, Burlington,  MA.   [9] Office of Educational Technology. 2010. Connect and  Inspire: Online Communities of Practice in Education.  Retrieved November 11, 2011 from Connected Educators:  http://connectededucators.org/report/.    [10] Preece, J., and Shneiderman, B. 2009. The reader-to-leader  framework: Motivating technology-mediated social  participation. AIS Transactions on Human-Computer  Interaction, 1, 1, 1332.   [11] Social Media Research Foundation. 2011. NodeXL:  Network Overview, Discovery, and Exploration for Excel.  Retrieved November 11, 2011 from  http://nodexl.codeplex.com/.    [12] Snowden, D. 2010. Narrative Research. Retrieved  November 11, 2011 from Cognitive Edge:  http://www.cognitive- edge.com/articledetails.phparticleid=64.    [13] U.S. Department of Education. 2010. Transforming  American Education: Learning Powered by Technology.  Office of Educational Technology, U.S. Department of  Education, Washington, DC. Retrieved November 11, 2011  from the U.S. Department of Education:  http://www.ed.gov/technology/netp-2010.    [14] Department of Education and U.S. Department of Defense.  2011. The Learning Registry. Retrieved November 11,  2011 from: http://www.learningregistry.org/.   [15] Wenger, E., Tayner, B., and de Laat, M. 2011. Promoting  and Assessing Value Creation in Communities and  Networks: A Conceptual Framework. Ruud de Moor  Centrum, Amsterdam.      72      "}
{"index":{"_id":"14"}}
{"datatype":"inproceedings","key":"Schon:2012:JLM:2330601.2330624","author":"Schon, Martin and Ebner, Martin and Kothmeier, Georg","title":"It's Just About Learning the Multiplication Table","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"73--81","numpages":"9","url":"http://doi.acm.org/10.1145/2330601.2330624","doi":"10.1145/2330601.2330624","acmid":"2330624","publisher":"ACM","address":"New York, NY, USA","keywords":"algorithm, degree of competence, learning analytics, learning curve, multiplication table, school children","Abstract":"One of the first and basic mathematical knowledge of school children is the multiplication table. At the age of 8 to 10 each child has to learn by training step by step, or more scientifically, by using a behavioristic learning concept. Due to this fact it can be mentioned that we know very well about the pedagogical approach, but on the other side there is rather less knowledge about the increase of step-by-step knowledge of the school children. In this publication we present some data documenting the fluctuation in the process of acquiring the multiplication tables. We report the development of an algorithm which is able to adapt the given tasks out of a given pool to unknown pupils. For this purpose a web-based application for learning the multiplication table was developed and then tested by children. Afterwards so-called learning curves of each child were drawn and analyzed by the research team as well as teachers carrying out interesting outcomes. Learning itself is maybe not as predictable as we know from pedagogical experiences, it is a very individualized process of the learners themselves. It can be summarized that the algorithm itself as well as the learning curves are very useful for studying the learning success. Therefore it can be concluded that learning analytics will become an important step for teachers and learners of tomorrow","pdf":"It's Just About Learning the Multiplication Table  Martin Schn   LifeLongLearning  Graz University of Technology   Mandellstrae 13, A-8010 Graz  +433168734931      martin.schoen@tugraz.at       Martin Ebner  Social Learning   Graz University of Technology  Mnzgrabenstrae 35A/I, A-8010   Graz  +433168738540   martin.ebner@tugraz.at   Georg Kothmeier  Social Learning   Graz University of Technology  Mnzgrabenstrae 35A/I, A-8010   Graz  +433168738540   georg.kothmeier@student.  tugraz.at         ABSTRACT  One of the first and basic mathematical knowledge of school  children is the multiplication table. At the age of 8 to 10 each  child has to learn by training step by step, or more scientifically,  by using a behavioristic learning concept. Due to this fact it can  be mentioned that we know very well about the pedagogical  approach, but on the other side there is rather less knowledge  about the increase of step-by-step knowledge of the school  children.   In this publication we present some data documenting the  fluctuation in the process of acquiring the multiplication tables.  We report the development of an algorithm which is able to adapt  the given tasks out of a given pool to unknown pupils. For this  purpose a web-based application for learning the multiplication  table was developed and then tested by children. Afterwards so- called learning curves of each child were drawn and analyzed by  the research team as well as teachers carrying out interesting  outcomes. Learning itself is maybe not as predictable as we know  from pedagogical experiences, it is a very individualized process  of the learners themselves.   It can be summarized that the algorithm itself as well as the  learning curves are very useful for studying the learning success.  Therefore it can be concluded that learning analytics will become  an important step for teachers and learners of tomorrow.    General Terms  Algorithms, Measurement, Documentation, Performance,  Experimentation.   Keywords  multiplication table, learning analytics, learning curve, degree of  competence, algorithm, school children   1. Introduction  Phil Long and George Siemens stated [11] that the most dramatic  factor shaping the future of higher education is something that we  cant actually touch or see: big data and analytics. In other words  if a lot of learning data is measured and analyzed, new insights   into the learning process will be given and therefore new  didactical approaches will help to improve our learning behaviors   not only in higher education but in the whole educational  system. Of course the main question addressed by Erik Duval [2]  is about what exactly should be measured to get a deeper  understanding of how learning takes place.   In this publication we like to answer the following research  question: How can we improve learning the multiplication table  with the help of an individualized learning program and a follow- up data analyzing   First of all, it must be mentioned that learning the multiplication  table is a central subject of mathematics in primary school. Young  children gradually develop an intuitive and practical arithmetic  that they use to solve problems successfully and confidently in  their everyday life. But each child is different and some of the  basic knowledge about mathematics is acquired even before  children start school [8] [14]. Considering current didactical  knowledge about learning math tells us, that there are cultural  differences, some obviously based on language implications [3].  Each child needs a general linguistic competence before starting  to operate with numbers. Some publications point out that  mathematic is the first non-native language [9]. For example,  understanding the simple expression multiply is implicitly not  only a mathematical problem but also a linguistic one. Many  children who carry out the algorithms correctly do not really  understand reasons for crucial aspects of the procedure [6].    The most common and traditional way to learn the multiplication  table is drill and practice without taking care of any linguistic  preconditions. Multiplication Tables in 21 days [12] is one of  many offers of teach the kids online or to use traditional materials.   From the perspective of special education it is well-known that  this way of teaching and learning math is problematic and leads to  disorders and blockades on the learners side. The defective  number module hypothesis indicates a genetic defect, a more  physiological cause of basic brain functionality [1]. Teachers  gladly accepted such a biological explanation as a relief for their  problems with a child. According to current neurological oriented  research results, but also based on a very old educational tradition,  teaching should use tactile, optical and acoustic processing  methods in every single case  intensively when they observe  deviations from the mainstream. It is important to allow  multisensory math to be taught to children with special  educational needs. Kendall [7] offers some practical suggestions  of how to achieve this. Furthermore face-to-face teaching has to  bear in mind also interlinking between different assignments. But  the daily practice is dominated by pure row learning of the  multiplication table [5].      Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy   otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   73    It has to be borne in mind that learning per se is an active  individual process on the learners themselves. The competence  perfect handling the multiplication table differs enormously in  the needed learning time and resulting learning effort  from  weeks to years.   It is obvious that the simple problem learning the multiplication  table is not as trivial as it seems to be at first glance. The goal of  our research is to develop a web-based system that assists learners  as well as teachers. Furthermore it should provide teachers with  an overview about the current learning process of their pupils. The  research group (consisting of educators, e-learning experts, an  educational scientist and IT-developers) specified their  requirements as follows:    The system should estimate the competence grade of  the learner    The system should provide appropriate exercises  according to the competence grade of the learner.  Nevertheless the exercises should tend to be  challenging.    The system should ensure that already well-done  exercises are repeated and practiced. After succeeding a  problem the probability for a repeated display decreases  in two levels (analogous to a Leitner System [10])    In general the system should be motivating and show  that learning can be fun.     The system should record and safe fine-grained data of  all done exercises, test results and the current  competence grade of the learner in order to prepare the  next sessions in an adequate way.    Thereby it should unburden the teachers from that task.    In contrast to many other approaches to adopt learners in this  research work there are no probabilistic strategies for estimating  competencies used. Even more the goal is to generate a complete  table to inform learners as well as teachers about their problems in  every single task. And for sure teachers want to know something  about the process, the used time and the up and down of the  learning curves.    Similar research work in the area of Intelligent Tutoring Systems  (ITS) [4] [13] pointed out that such an approach can raise many  problems for daily-life-situations of an actual classroom. In other  words, our research work concentrates on a particular and small  learning problem, but likes to ensure the learning success.   Finally, the implemented intelligent algorithm is not only useable  for the specific problem of learning the multiplication table. Any  problem that is presented in a set of flashcards could be easily  transferred to a system with this functionality.    2. Technical details about the application  Beforehand some technical details need to be mentioned that were  defined by the project team:   The application is written in PHP1 5. The interface is based on  HTML2 and CSS3. Because of the different render engines of the  browsers the CSS-Framework YAML4 is used. Carrying out a                                                                 1 PHP: Hypertext Preprocessor  2 Hypertext Markup Language  3 Cascading Style Sheets  4 Yet Another Multicolumn Layout (http://www.yaml.de/; last   visited October 2011)   solid software development the application is programmed with  the help of Zend Framework5.    The main approach is to store the data of each learner to analyze  their learning process.   3. Algorithm  The approach of this research study is to develop an algorithm  that allows an individual learning by providing exercises in  dependence on the learners knowledge. Therefore the main task  is to consider which exercise is provided next. On the one side,  the exercise should be challenging on the other side it must be  adequate; not too difficult but not too easy at all. Therefore the  core task of the program is the new designed algorithm. This  algorithm decides which question is chosen to be next. The  general idea is to always find the best learnable question in  reference to Vygotskys [15] idea of  the zone of proximal  development  for the user. Therefore two parameters are  important:    Difficulty of the exercise: First of all each learning  exercise has to get a difficulty rank. If the task  difficulties are already known from a preliminary  investigation (for example a pedagogical study), they  should be used in that way. Otherwise a hierarchy of  estimated difficulties must be assumed according to  specific pedagogical experiences. The difficulty ranks  should then be converted to a scale with values between  0 and 1. This means an easy question gets a high  success probability value (close to 1) and a more  difficult one gets a low success probability value (close  to 0).    Degree of competence: To keep track of the learning  progress of a user the degree of competence is  calculated (0 ... 1), called learning rate. The calculation  of this value depends on the performance of each  learner and is calculated in real time.     When we started developing this program we used  degree of competence in the sense of ratio of items,  which have been solved to the sum of items which has  been offered. During the further process we found out  that it is more useful to have a less volatile variable for  determining the level of the next question and two  others (learning rate 1 & 2) for describing the success  and analyses.   3.1 Degree of competence   The basic idea of a degree of competence (DOC) is to indicate  which question is difficult and which is easy in relation to the  current users knowledge. It must be considered, that for the first  session we dont know anything about the competence level of the  student so far. With every answered question we get a more and  more reliable estimation of the real competence.    The main idea is that each user has their specific degree of  competence equaling a value between 0 and 1. 1 means the user is  advanced and every item could be selected as a suitable task or  question, 0.5 indicates an intermediate user and 0 is a beginner. In  general the degree of competence reflects the current knowledge.  Questions graded below the individual degree of competence  are solved and known by the learner; questions above are  questions to be learned. When the next learning item is chosen it  must be avoided that the task is neither too difficult (learners will                                                                5 http://framework.zend.com/ (last visited October 2011)   74    get frustrated) nor too easy (learners will become bored). To  fulfill that demand we have to assess the questions given to be  homogenous compared to traditional tasks. The amount of data  collected by the application helps us to discuss for example  whether this homogeneity is really given, and whether the  students achieve the learning goals in typical, well known and  logical or different and more individualized learning paths.    We use the idea of a so-called extended learning area to  describe the pool of which learners get the next tasks from. By  definition the whole learning area is a combination of the actual  learning area (known by the learner) and the extended learning  area (unknown by the learner). In our pilot study we defined the  extended learning area to be 25% above the learners degree of  competence. Of course this parameter can be adjusted by teachers.  In a situation where the teacher anticipates a low level of  competence in this domain, at the beginning of learning the tables,  this value must be kept down. Over the time, for example when  learners are starting to learn the multi-digit multiplication, it could  be become higher.    Figure 1 visualizes the whole learning area sectioned by the  degree of competence into an area of known questions (actual  learning area) and one of future questions (extended learning area)  limited by the extended degree of competence.       Figure 1 Degree of competences      In the initial test phase, we found out that these settings are  complicated, especially during the first session when the students  have only solved some few problems. A computer program is  restricted in perceiving students interaction only towards solved  or unsolved learning questions. A teacher perceives of course,  many other and different signals (nervousness, facial expression,  comments on the task ) that may affect the decision which task  is to be given next. With other words especially an improper pre- assessment of learners knowledge can be frustrating by keeping  learners too long overstrained or bored. Furthermore it must be  avoided, that the tasks difficulty escalates too quickly and  questions therefore become inadequate. In the following chapter  the design of the pretest is pointed out.      3.2 Pretest  At the beginning, before the learning session, each learner will be  asked two questions, starting with a moderate one; if the answer is  correct; a more difficult one will be presented - the estimated  degree of competence is set around 0,75. If the answer is incorrect  an easier one will be provided (around 0,25). Figure 2 points out  the flow diagram to get the estimated degree of competence in the  beginning. For example if the first answer is correct and the  second one false, the estimated learning rate results 0,50.    Finally to get adequate questions (easy  medium  difficult) the  following categories are defined    Easy: A question with a probability for a correct answer  of 0,78  0,69     Medium: A question with a probability success value of  0,54  0,47    Difficult: A question with a probability success value of  0,20  0,13   Figure 2 Pretest  estimated degree of competence      3.3 Answer classification  Another issue that must be addressed in our context is the  classification of well-known learning problems. From a  pedagogical point of view it must be stated that presenting a right  solution is not a satisfactory indicator. Especially learning the  multiplication table is learning by drill & practice which means if  a learner practices one problem more often they will get more  confident that they can manage this item. Therefore also the  developed algorithm must ensure that a task is well-known.  This issue is addressed by establishing a well-known parameter.   The answers of the learners are marked with 0, 1 or 2:    0 means a wrong answer    1 shows that the user knew the correct answer once    2 indicates that the student had two consecutive correct  answers (this means a question is well known)   Questions that have been signed with 2 were set back to 0  immediately in case the student failed.    After the pilot stage it was decided to set the well-known  parameter always to 2 when the degree of competence of the  learner is 0,3 higher than the difficulty of the task. We wanted to  reduce the frequency of getting too easy tasks for good students.  Each time a student solves a problem, the result will be stored by  the application. We calculate a learning rate 2 to be the ratio of  the sum of solved items signed with 2 and the number of items.  We also compute a learning rate 1. This ratio is based on the  sum of all solved items, e.g. signed with 1 or 2.    75    It is to remark, that these two levels of knowledge and the set  procedures have the functionality of a Leitner System [10]. The  results of psychology of memory give the recommendation that  learned tasks should be used again and again, but there is only a  decreasing frequency necessary to avoid forgetting the right  solutions.   3.4 Adjusting competence  In the first session we only have the results of the pretest to  estimate the competence of the respective student and to allocate a  suitable question. With every new test we get more and better  information for specifying the students current competence.    At the beginning of our tests we simply calculated the ratio of the  number of correct answers to all given answers. This leads to the  following formula:        () =                  This formula resulted in a too high volatility at the beginning of  the learning progress. For example if a learner answers the first 2  questions in the pretest correctly the program will assume a  degree of competence of 0,75. After answering the next 2  questions wrong the degree of competence decreases to 0,5. The  fast increase/decrease of the factor of course will cause that  learners become quickly frustrated.    A second approach with the intention of getting a more stable  value was to calculate the degree of competence by counting  answers only with the classification well-known (2). The  resulting formula is:        () =                      number of items = number of learning problems/flashcards provided by  the system   This formula has some disadvantages as well, especially for  learners with a too low estimated degree of competence resulting  from the pretest. For these learners it might be hard to increase  their degree of competence, i.e. to correct the erroneous data.  Finally the degree of competence is defined by the procedure as  follows:    We compute the ratio of both - number of correct answers as well  as well-known answers - and the number of items.    = (   (1)  (2)   )               The results are saved in the database. In general the learning  progress is considered as completed, if every question is signed  with 2 (well-known).   When a learner starts their first session the following procedure  combines the result of the pretest and the actual degree of  competence:    This special procedure is only used at the beginning if        <             Then we compute a weight:     =                With this weight, the degree of competence is calculated as a  combination of the estimated degree of competence of the pretest  and the previous performance:    =   (1  ) +        This degree of competence is important to find the next task in the  learning area. Therefore it must be guaranteed that the DOC is not  in-/decreasing too fast in order to provide adequate questions.    3.5 Selecting items  After the degree of competence is calculated the next item  presented to the learner must be chosen. For this purpose, the  algorithm chooses items out of three categories.    Extended and Actual Learning Area (items signed with  0)    Actual Learning Area (items signed with 1)    Actual Learning Area (items signed with 2)   A random number out of the interval 0 and 1 is used to decide  which category is activated. Therefore three cases are defined:    Case 1: If the random number (x) is smaller or equal  0,05 a well-known question marked with 2 is chosen.    Case 2: If the random number is 0,05 > x >= 0,15 a  known question marked with 1 is chosen.     Case 3: If the random number is x > 0,15 a (unknown)  question out of the extended and actual learning area is  chosen.   In general all items are preordered according to their difficulties  and have corresponding rank numbers.   In case 1 another random number is drawn to get a position in the  ranking of the known items. Beginning with this position the  algorithm is looking for the next less difficult item, which is  signed with 2.     In case 2 the algorithm is starting with the rank of the easiest item  and is looking for the next item, which is signed with 1.   In case 3 our selection is adjusted with the degree of competence.  We compute an upper rank for the selection:    =        1,25         Afterwards the algorithm is starting to search beginning with rank  position rank or less difficult items. The next item, which is  signed with 0, is selected.    In the case the algorithm is running out of the ranking, because no  suitable item could be found, the procedure is repeated again and  again until it finds an item. We are sure that an item is found,  because the program selects already known problems for  repetition with a p=15%. If all items are signed with 2 the student  will receive this information together with an option to finish. The  student may proceed with the training if preferred so.   4. Algorithm overview  Finally a short overview of the workflow (algorithm) is given:   1. Pretest (see section  3.2, figure 2)  2. Calculate the degree of competence (see 3.4)  3. Choose next question according to the degree of   competence (See section 3.5). For this, the probability to  choose a question out of the three classes is (could be  adjusted):   o 75% learning area  o 10% known questions  o 5% well-known questions   4. Process the given answer (See section 3.2)  5. Calculate new degree of competence (See section 3.4)   76    6. Check if learning progress is finished. If yes output a  message to the user otherwise go to step 3. Repeat the  algorithm until the user stops playing.   5. Prototype  The program is implemented as a game. Learners can earn points  for each correct answer given and reach new stages (called rank).  Points are gathered or can be lost if the answer given was wrong.  Furthermore the answering speed is also important. The quicker a  question is answered correctly the more points are gained. Finally  the game requites also consecutive correct answers by giving  more points.   The prototype is a web application currently available following  the URL (last visited October 2011): http://vlpc01.tu- graz.ac.at/~georg/index.php/user/login.   After registering an account learners are able to login. Depending  on the learners user type they are redirected to the according  page. The learners will get the main game interface (see figure 3).  Administrators will see an additional navigation bar, which allows  choosing between getting statistical information about all existing  learners and simply playing the game. Due to the fact that it is a  first prototype the interface is kept simple with a first design  suggestion.      Figure 3 Main screen of the application      Figure 3 displays the main game interface (currently only a  German version is running). The markers 1 to 6 refer to the most  important areas of the interface:   1. The questions which have to be answered (item of the  multiplication table)   2. Free space for feedback to the user, for example  Correct answer, Wrong answer or Not entered a  number   3. Input field for providing the answer. It can be submitted  by clicking on the green button Antworten or pressing  Enter on the keyboard.   4. The timeline shows the remaining time for giving an  answer. Actually the learner has 60 seconds to answer  (predefined by the teacher). Depending on the  answering speed points are calculated.   5. Shows the actual rank of the user.  6. Displays the actual points of the learner. Of course the   more points a learner gets, the higher is their rank.   6. Study  After finishing the web application the first research study was  carried out at a primary school in Austria. In summer semester   2011 the program was handed out to 42 pupils of the primary  school Laubegg6 (age: 9-10). After a short introduction of the  main principles of the program and setting personal accounts for  each learner the study was started. It lasted at least 4 weeks. Some  of the learners ignored this time restriction and played the game  again and again over months. Learners learned on computers at  the school as well as on their personal computers at home. It can  be summarized that in the time frame of the study 12.926 answers  where given which means that on average each learner answered  308 questions or in summary they did the whole multiplication  table 3,4 times. Bearing in mind that there was no real pressure  from teachers side using the program it is a considerable pleasant  high number. Furthermore it can be stated that pupils seemed to  enjoy using the application or at least got not bored.   7. Discussion   7.1 General Overview  First of all an overview of the general result is given (figure 4) by  displaying the number of trials versus the final number of well- known items.   In detail figure 4 shows that in summary 18 learners (43%)  mastered every 90 single multiplications at least twice (to get  marked 2) of the multiplication tables. Many of them obviously  enjoyed the game: One of them solved even 1.486 assignments on  voluntary basis.   If learners make no mistakes, they need approximately 190 trials  to show their competence for each item and get marked well- known (2). According to our defined algorithm and due to the  fact that the probability of getting a correctly solved item twice in  the very first beginning of the game is only about 10% the number  of 190 trials seems to be a quite solid one.   More than half of the learners are below the aspired 90 correct  single multiplications of the multiplication tables. There might be  several reasons for this:    The learner    did not get used to / has problems with the interface    does not know the necessary operations; is not able to  solve the learning problem correctly    misinterprets an assignment    is distracted by their environment and makes wrong  clicks     is badly concentrated for several reasons.   As mentioned before, six learners proceed to work with the  program, even after the program offered to stop because every  assignment had been answered correctly twice. In such cases, the  program serves as a diagnostic instrument: It is more reliable than  a paper-pencil test when a learner masters the multiplication table  with this application.                                                                  6 Contact person VDir. Petra Steiner   VS Laubegg, 8413 Ragnitz, Austria   77       Figure 4 Well-known items versus trials      As mentioned before, more than half of the learners did not reach  the 100 percent level. We already listed possible reasons why a  learner does not solve the assignments correctly. Besides these  arguments, it is necessary to reflect whether weaker learners could  have fewer possibilities using the program, especially at home.  The above-mentioned high-trial learner for example is known as  someone who was able to train and use the program also at home.  Eventually, the better students had more chances to work at the  computers in classes, so that the teachers were able to work with  weaker learners. Perhaps, learners with worse performances had  general problems, like to open and to handle the program.  Nevertheless, these are only further assumptions, which must be  considered in future studies.   7.2 Demotivated learner (ID 21)  One learner with a weak performance attracted our attention  because of a very high number of trials (513) (figure 5). A  detailed inspection showed that she/he did not work very  intensively. In the first two tasks he/she failed, then eleven tasks  were ok, her/his performance rose abruptly. However, afterwards,  she/he continued approximately 400 times to wait the whole  answering time without doing anything but asking for a new  assignment.      Figure 5 Unmotivated performance   7.3 Motivated captain  Figure 6 shows the learning history of the most diligent learner. In  the beginning, the assignments were solved correctly, then some  mistakes occurred, afterwards a learning process can be  recognized and finally with some occasional mistakes the learner  works on a high performance level. Obviously, the learner was  highly motivated to deal with the assignments given by the  program: She/he absolutely wanted to be and stay the first in the  high score. Furthermore there are an amazing high number of  trials, which leads to the assumption that the learner likes to do  the exercises. It seems that there is a dependence between the high  number of trials and the almost perfect knowledge of the learning  field.       Figure 6 Motivated captain   Teachers have to face such situations. Why did the learner  become demotivated Why did he/she did not proceed going on  with learning and gave up This case is quite dramatic, because  the learner her/himself disrupted the ongoing learning process.   7.4 The medium learner (ID 115)  One of the most interesting learning histories can be seen in figure  7. It points out the way the application works if a learner is not  performing very well. In the beginning, the learner made mistakes  in every second assignment (0,5), followed by 7 mistakes  consecutively. This is the reason for the big decrease (0,15).  Afterwards, the learner gave a number of right answers and the  rate of correct answers increased back to 0,5.  In the following  phase an up and down can be seen till a number of right  consecutive answers helps to reach a level of 0,7. But then the  number of mistakes rose again and the rate went down to about  0.5. This characteristic curve illustrates how a learner is learning  from mistakes and is getting better by failing an assignment and  slowly solving it next time - learning by failing.   0  200  400  600  800  1000  1200  1400  1600  0 20 40 60 80 100  max. Number of well known items  vs. Trials  0  2  4  6  8  1  3 3  6 5  9 7  1 2 9  1 6 1  1 9 3  2 2 5  2 5 7  2 8 9  3 2 1  3 5 3  3 8 5  4 1 7  4 4 9  4 8 1  5 1 3  correct answers / total number of processed   items - ID21  0.82  0.84  0.86  0.88  0.9  0.92  0.94  0.96  0.98  1  1.02  1  8 5  1 6 9  2 5 3  3 3 7  4 2 1  5 0 5  5 8 9  6 7 3  7 5 7  8 4 1  9 2 5  1 0 0 9  1 0 9 3  1 1 7 7  1 2 6 1  1 3 4 5  1 4 2 9  1 5 1 3  correct answers / total number   of processed items  78       Figure 7 The medium learner   This effect can be seen more detailed in figure 8; it shows the  learning rate 2 of the same student. As mentioned in previous  chapters an item is marked as 2 if it was solved at least twice  consecutively. In the initial phase the same items are not  presented very often due to the fact that the probability is defined  with 15% (see chapter 4) and because the program prefers  assignments that were not correctly answered beforehand.  Afterwards, the performance increases fast to a certain stage (20  items are marked as 2) and falls a little bit (learning by failing).  The next 140 questions caused a so to say sideward trend; the  student answered some questions right some wrong. Then the  curve jumped up. The student has learned the solutions of a more  difficult group of items, we do not know whether they got training  beside the program or not. It needed about 120 questions more  until the curve was growing again to foster the learning efforts.  Finally we see that the last 15 assignments obviously lastingly  cause some problems. It can be concluded that the learner needs  just some additional time or other exercises and materials to  perform the whole multiplication table perfectly.      Figure 8 Unsteady and slow growing learning curve   Figure 9  the learning rate 1 of the student  underlines the  learning by failing effect. There were about 300 trials almost  every problem is signed as known (one time) - but only 20 items  are signed as well-known (2 times). Furthermore in about 20  percent of the items marked as well-known mistakes occurred  (figure 8). Finally, the learner seems to be concentrated better or  they learned the tables and solved 70 of the 90 items twice.      Figure 9 Learning rate  the DOC      7.5 The weak learner  Figure 10 illustrates the exercise list of one learner, who had  performed easy questions with simple assignments, but got worse  afterwards, when the difficulty of the questions jumped over their  degree of competence. We call the effect the fast increase  effect, which means the increase is too fast and not appropriate to  the learners knowledge.       Figure 10 Fast increase effect   The learning rate curve of the same learner in figure 11 describes  the same behavior:  a fast increase followed by a very unstable  phase and decreasing output. In comparison to the case before, the  learner is only able to manage about 25 percent of all items  correctly. It is assumable that the learner is not mastering the  whole multiplication table. By all means, they work with the  program, make mistakes, are corrected, learn and for sure need  much more time to master all items finally.     Figure 11 Learning rate of a weak learner   0  0.2  0.4  0.6  0.8  1  2 9  5 7  8 5  1 1 3  1 4 1  1 6 9  1 9 7  2 2 5  2 5 3  2 8 1  3 0 9  3 3 7  3 6 5  3 9 3  4 2 1  correct answers / total number   of processed items - ID115  0  20  40  60  80  1  2 9  5 7  8 5  1 1 3  1 4 1  1 6 9  1 9 7  2 2 5  2 5 3  2 8 1  3 0 9  3 3 7  3 6 5  3 9 3  4 2 1  Learning rate 2 /wellknown  - ID115  0  0.5  1  1.5  1  2 7  5 3  7 9  1 0 5  1 3 1  1 5 7  1 8 3  2 0 9  2 3 5  2 6 1  2 8 7  3 1 3  3 3 9  3 6 5  3 9 1  4 1 7  Learning rate 1 - ID115  0  0.5  1  1.5 1  1 5  2 9  4 3  5 7  7 1  8 5  9 9  1 1 3  1 2 7  1 4 1  1 5 5  1 6 9  1 8 3  1 9 7  2 1 1  correct answers / total number   of processed items - id 156  0  0.1  0.2  0.3  0.4  1  1 5  2 9  4 3  5 7  7 1  8 5  9 9  1 1 3  1 2 7  1 4 1  1 5 5  1 6 9  1 8 3  1 9 7  2 1 1  Learning rate 1 - id 156  79       7.6 Lazy bones  Figure 12 illustrates the learning curve of lazy learners because of  the very little activity in relation to the competence. Of about 100  problems 80% were solved. We need more precise observation,  what is going on there. Is it a problem, that these students need an  additional (social) motivation The process is very slow and the  performance is not good in this test situation. Could some other  problems be the cause for this lack   Figure 10 A typical learning rate for learners with low trials     7.7 General remarks  We think it is useful to test and to train the students with such  programs. They have possibilities to get out of the stream with  provocations to learn, which is the same in a normal class. But  here we have a protocol of learning progress. In every case the  delivered tasks are more frequent and more precisely orientated  on the individual level and next reachable tasks than in a standard  situation, with a teacher without such an information-processing  capacity tool.    8. Conclusion and future work  In this publication we discuss the implementation of a new and  intelligent algorithm to assist school children training one of the  basic learning goals in primary schools  the multiplication table.  Furthermore a field study with 42 learners was carried out and  analyzed. The power of learning analytics allows the research  team to think about the outcomes and carries out different types of  learning curves.    In general we can state that there are some major types of curves   learners who are very knowledgeable, those who are in a stage  close to being knowledgeable and those who still need a lot of  learning effort to reach the learning goals. However it is  recognized that analyzing just one curve (correct answers/total  number of process items) is not sufficient enough to cover the  state of the learning process in most of the provided cases. There  is a need to have a look at learning rate 1 and 2 too. Moreover  sometimes the curves gave the researchers and teachers only a  hint that a pedagogical intervention is absolutely necessary to  enhance the learning results.   For our future work we consider a couple of ideas how the current  application can be improved, but mainly it must be stated that  there should be a much closer look at the learners. This can  perhaps be done in some more intelligent analyses of the data or  in more cooperation with the teacher. We will have to analyze the  reasons why some learners are demotivated or why their learning  rate decreases during longer intervals. We also discussed whether   it could be more appropriate to change the design from gaming to  a more informative display of the actual knowledge. This could be  a matrix, which displays the results with the 0,1 or 2 signs. In the  future we also need a better aggregation, compression and  visualization of the learning outcomes.  This could indicate those  people, who need more attention from the teacher and probably  immediate interventions. Another point is that we measured the  time to solve the questions only to limit the time but not to  construct additional aspects of the learning behavior.    Overall we are convinced that the application is a further step  towards an interesting learning future. The teacher saves time for  management. Analyzing learners results and performance over a  longer time period brings more reliable and systematical insights  to teachers for their daily work in classrooms and improve the  learning success of each learner. Nevertheless learning is a highly  social process and is an active process on the part of the learner,  where knowledge and understanding is constructed by the learner.  With other words the implemented tools will help teachers to get a  better feeling about the individual learning process and allow a  just-in-time reaction.   9. Acknowledgements  We express our gratitude to the teachers of the primary school in  Laubegg (Styria, Austria) as well as all participating school  children.    We are equally indebted to our funding agency Internet  Foundation Austria (IPA) for supporting our ideas and helping us  to work on the future of education.   10. REFERENCES  [1] Butterworth, B. and Yeo, D. 2004. Dyscalculia Guidance:   Helping Pupils with Specific Learning Difficulties. Windsor:  NFER Nelson.   [2] Duval, E. 2010. Attention Please! Learning Analytics for  Visualization and Recommendation. To appear in:  Proceedings of LAK11: 1st International Conference on  Learning Analytics and Knowledge 2011.  (https://lirias.kuleuven.be/bitstream/123456789/315113/1/la2 .pdf last visited October 2011)   [3] Fuson, K.C. 1990. Using a base-ten blocks learning/teaching  approach for first- and second-grade- place-value and  multidigit addition and subtraction . In: Journal for Research  in Mathematics Education.   21, No. 3, 180-206  Northwestern University . DOI=  http://webschoolpro.com/home/projectlead/Research%20Arti cles%20and%20links/Base%20Ten%20Blocks.pdf  .   [4] Gerber, M.M., Semmel, D.S., Semmel, M.I. 1994.  Computer-based dynamic assessment of multidigit  multiplication.  (http://www.freepatentsonline.com/article/Exceptional- Children/15824003.html)   [5] Gerster, H.D. 2009. Schwierigkeiten bei der Entwicklung  arithmetischer Konzepte im  Zahlenraum bis 100.- In:  Rechenschwche. Lernwege, Schwierigkeiten und Hilfen bei  Dyskalkulie. Fritz, A., Ricken, G., Schmidt, S. (Ed.)-  Weinheim, Basel, Berlin: Beltz, p 248268.   [6] Ginsburg, H. P. 1989. Childrens Arithmetic. How They  Learn It And How You Teach It. Pro-Ed , Austin, Tex  (2nd  ed.)    0  0.2  0.4  0.6  0.8  1  1 7 13 19 25 31 37 43 49 55 61 67 73 79 85 91 97  learning rate 1 - id 93  80    [7] Kendall, J. Multisensory maths for teaching SEN. DOI=  http://www.teachingexpertise.com/articles/multisensory- maths-teaching-sen-5209   [8] Klein, J. S., and Bisanz, J. 2000. Preschoolers doing  arithmetic: The concepts are willing but the working memory  is weak. In: Canadian Journal of Experimental Psychology,  54, 105-115.   [9] Landerl, K.,  Butterworth, B., Lorenz, J.H.,  and  Mderl, K.  2003. Rechenschwche - Rechenstrung - Dyskalkulie:  Erkennung - Prvention  Frderung.  Leykam, Graz.   [10] Leitner-System. DOI:  http://en.wikipedia.org/wiki/Leitner_system  .   [11] Long, P., Siemens, G. 2011. Penetrating the Fog: Analytics  in Learning and Education. EDUCAUSE Review Magazine.  Volume 46. 5. p. 31-40  (http://www.educause.edu/EDUCAUSE+Review/EDUCAU SEReviewMagazineVolume46/PenetratingtheFogAnalyticsin Le/235017; last visited October 2011)    [12] Russel, D.: Timestables in 21 Days.  DOI=http://math.about.com/cs/arithmetic/a/timestables.htm   [13] Scandura, J.M. 2011. What TutorIT Can Do Better Than a  Human and Why: Now and in the Future.  http://www.scandura.com/Articles/191-What_TutorIT.pdf     [14] Skwarchuk,  S. . Look Who's Counting! The 123s of  Children's Mathematical Development During the Early   School Years. Encyclopedia of Language and Literacy  Development.   DOI=http://literacyencyclopedia.ca/index.phpfa=items.sho w&topicId=243 .   [15] Vygotsky, L. S. 1978. Mind in society: The development of  higher psychological processes. Cambridge, MA: Harvard  University Press.   81      "}
{"index":{"_id":"15"}}
{"datatype":"inproceedings","key":"Bramucci:2012:SIS:2330601.2330625","author":"Bramucci, Robert and Gaston, Jim","title":"Sherpa: Increasing Student Success with a Recommendation Engine","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"82--83","numpages":"2","url":"http://doi.acm.org/10.1145/2330601.2330625","doi":"10.1145/2330601.2330625","acmid":"2330625","publisher":"ACM","address":"New York, NY, USA","keywords":"Sherpa, personalization, recommendation engines, student success","Abstract":"Students flock to online services like Amazon, Pandora and Netflix that offer personalized recommendations, in stark contrast to the one size fits all services in higher education. In this session we demonstrate Sherpa, a recommendation engine for courses, information and services that utilizes both human and machine intelligence.","pdf":"Sherpa: Increasing Student Success with a  Recommendation Engine  Robert Bramucci  South Orange County Community   College District  28000 Marguerite Pkwy, Mission   Viejo, CA 92692  1-(949) 582-4960   rbramucci@socccd.edu    Jim Gaston  South Orange County Community   College District   28000 Marguerite Pkwy, Mission   Viejo, CA 92692   1-(949) 582-4336   jgaston@socccd.edu   ABSTRACT  Students flock to online services like Amazon, Pandora and  Netflix that offer personalized recommendations, in stark contrast  to the one size fits all services in higher education.  In this  session we demonstrate Sherpa, a recommendation engine for  courses, information and services that utilizes both human and  machine intelligence.   Categories and Subject Descriptors  H.3.4 [Systems and Software]: User profiles and alert services.   General Terms  Algorithms, Performance, Design, Human Factors.    Keywords  Sherpa, Recommendation Engines, Personalization, Student  Success.   1. INTRODUCTION  An unprecedented alignment of forces in the United Statesfrom  President Obama to state governors to private foundationsis  calling for America to regain lost educational ground by once  again having the highest proportion of students graduating from  college by 2020.  Concurrently, however, educational funding is  shrinking.  How can we make large gains in student success while  spending less By leveraging the sort of intelligent, automated  computer recommendation engines proven successful by  companies like Amazon, Netflix, Pandora and Apple.   South Orange County Community College District (SOCCCD), a  two-college district in Southern California with 43,000 students,  has created Sherpa, an academic recommendation engine that  combines human expertise and predictive analytics to provide  students with the right information at the right time to facilitate  better academic decisions. Sherpa uses time, event, or location- based triggers to deliver multimodal (email, SMS, voice, text- to-speech, or Facebook announcements) personalized  communications such as:    Helping students find acceptable alternatives when their  preferred courses are full    Targeting at-risk students for academic interventions.    Tailoring information about campus events to individual  interests   At this session, we discuss the compelling nature of personalized  online services, outline our software development process and  provide a live demonstration of the Sherpa system.   2. DEVELOPMENT  2.1 Precursors   Previously, SOCCCD had developed MySite, an enterprise  academic web portal, and My Academic Plan (MAP), an online  academic planning tool that has been used by students to create  over 107,000 academic plans since it went online in April 2007.  Though MySite and MAP were successful, they, like nearly all  systems in higher education, were passive in nature. We wanted a  more proactive system capable of assisting students decision- making processes in a manner that would nudge them toward  making better-informed academic decisions.    Initially, Sherpa was envisioned as a proactive academic planning  tool that would focus on course selection. However, the more we  discussed such a system, the more expansive our vision became.  We realized that if we built a platform rather than an isolated  product--i.e., a recommendation architecture (see below) rather  than a specialized system--it could provide guidance on a wide  range of decisions including student services and specific  instructional content.          Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   82    2.2 Modalities  Though our two-college district has 43,000 students, the  California Community College System, with 2.7 million students  in 112 colleges, is the largest system of higher education in the  United States.  From the outset, we wanted to create a system  capable of scaling up to serve millions of students. In addition, we  wanted the system to be capable of delivering nudges using  multiple communication modalities: our web portal, personalized  RSS feeds, email messages, text messages, voice calls, text-to- speech audio, mobile device apps, or a custom Facebook  application.   2.3 Nudges  The term nudge was chosen deliberately to reflect the open- access nature of community colleges, where students are rarely  forbidden from taking any classes they desire.  Sherpa includes  three categories of nudges: Courses, Information, and Services.    The Courses module provides assistance in finding open  course sections during class registration.  Currently, its  decision rules are codified by human subject matter  experts; soon, other rules will be generated by data  mining legacy data in order to base course  recommendations on the academic performance of  academically-successful students with similar college  transcripts.     Information channels provide data feeds to students  based on whether their personal attributes match  attributes the author of the information felt would be  relevant.     The Services module presents students with  personalized links to online services such as course  registration, book purchasing, or Matriculation.    Nudges are created by subject matter experts using Boolean  operators.  First, a target population is created by concatenating  rules (e.g., [At-Risk Athletes = [Student Athlete] + [GPA < 2.0]).   Next, trigger conditions are set. Then, a message to the target  population is crafted. Nudges can be print, audio, or video-based.   Delivery of nudges can be triggered by dates, actions, or  locations.      Date-based messages can be set to be delivered on  absolute (xx/xx/xxxx) or relative dates (e.g., three days  before this individual students registration  appointment).      Actions are triggered by data changes, such as the  appearance of a students grade in the Student  Information System (SIS) or a class status change.     Location-based services await completion of Phase II of  our Mobile App project, whereupon students who opt-in  to sharing GPS information from their mobile devices  will be able to have nudges triggered by geographic  location (e.g., a student walking by the library receives  a text informing his/her that the book they requested  through interlibrary loan has arrived).   2.4 Design Team and Methodology  SOCCCD utilizes the Agile SCRUM software development  methodology, which maximizes user involvement and flexibility.  User involvement is maximized by using including  administrators, staff, faculty, and students on the development  team; flexibility is aided by creating software via an iterative  series of three-week Sprints, the object of each Sprint being a  functioning module of software.   The Sherpa team includes a vice president, dean, public  information officer, administrative assistant, a technology support  staff member, student services manager, outreach specialist,  instructors, and most importantly, students.     2.5 Feedback Mechanisms  In addition to the broad constitution of the design team, other  feedback mechanisms include Quality Assurance Testing, User  Acceptance Testing, usability studies conducted with  TechSmiths MORAE software, focus groups, and an annual  survey.  In addition, each nudge is accompanied by a 1-5 star  rating system and a comment box; the former is used to  automatically rank nudges according to students perception of  their relative importance and the latter is reviewed regularly to  fine-tune nudges for clarity.   2.6 Training  Since the Sherpa project is driven by a focus on students, we  thought it appropriate to have students introduce the system to  their peers. After consideration, we rejected print-based training  in favor of short videos available on our MySite web portal:    In this video, students from the Sherpa design team  describe the kinds of problems they face in college and  how Sherpa can help solve those problems.  www.youtube.com/watchv=hIZIvgwsHM     In this video, students introduce the changes to the  MySite web portal necessitated by the Sherpa project.   They dont mention Sherpa in the video because the  MySite portal acts as the web face of Sherpa:  www.youtube.com/watchv=-oMlahqo4iQ        3. RESULTS   Sherpa helped students who were closed out of a class   find an acceptable alternative class 6,606 times since its  deployment in the Fall 2011 semester.      Sherpa is now used as the messaging engine for our  custom-created student information system (SIS), and is  Instead of generic announcements, announcements are  personalized for each student and integrated with our  MySite web portal, either by the students inclusion in  the target set for a given nudge or by allowing students  to opt-in to various communication channels (e.g.,  Admissions, Athletics, Matriculation).       83      "}
{"index":{"_id":"16"}}
{"datatype":"inproceedings","key":"Brooks:2012:UIE:2330601.2330626","author":"Brooks, Christopher A. and Greer, Jim and Gutwin, Carl","title":"Using an Instructional Expert to Mediate the Locus of Control in Adaptive e-Learning Systems","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"84--87","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330626","doi":"10.1145/2330601.2330626","acmid":"2330626","publisher":"ACM","address":"New York, NY, USA","keywords":"adaptive systems, instructional design, learning analytics, locus of control","Abstract":"This paper considers the issue of the locus of control in adaptive e-learning environments from the perspective of a new stakeholder; the instructional expert. With an ever increasing ability to gain insight into learners based on their online activities, instructors and instructional designers are poised to add value to the process of adaptation, a process normally reserved for either systems designers or the end user. This work describes the design of an e-learning system which provides automated analytics information to these experts for consideration, and then leverages the insights these experts have made as the basis for content and feature adaptation.","pdf":"Using an Instructional Expert to Mediate the Locus of Control in Adaptive E-Learning Systems  Christopher A. Brooks University of Saskatchewan  Saskatoon, SK, Canada cab938@mail.usask.ca  Jim Greer University of Saskatchewan  Saskatoon, SK, Canada greer@cs.usask.ca  Carl Gutwin University of Saskatchewan  Saskatoon, SK, Canada gutwin@cs.usask.ca  ABSTRACT This paper considers the issue of the locus of control in adap- tive e-learning environments from the perspective of a new stakeholder; the instructional expert. With an ever increas- ing ability to gain insight into learners based on their online activities, instructors and instructional designers are poised to add value to the process of adaptation, a process normally reserved for either systems designers or the end user. This work describes the design of an e-learning system which pro- vides automated analytics information to these experts for consideration, and then leverages the insights these experts have made as the basis for content and feature adaptation.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.1 [Computer Uses in Education]: Collaborative learning, Computer-assisted instruction (CAI), Computer-managed in- struction (CMI), Distance learning  Keywords Learning Analytics, Adaptive Systems, Locus of Control, Instructional Design  1. INTRODUCTION An classic tension between the fields of adaptive systems  and human computer interaction centres on the question of the locus of control : should the system adapt to perceived user needs, or should the system be adaptable by the end user at their demand There is no clear right answer, nor is it a binary choice that must be made. Instead, a variety of successful systems have made choices between the poles of the adapt/adaptable continuum, taking into account the users, tasks, and domains at play.  The goal of learning analytics is to provide insight into learners based upon their activity in e-learning systems. How this insight is used is up to the administrators, in- structors, and instructional designers that have access to  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK12, 29 April  2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM 978-1-4503-1111-3/12/04 ...$10.00.  it. Remediation of known suboptimal behaviours is per- haps the principle interest of the area, and a number of different techniques ranging from automated (as in intelli- gent tutoring systems), semi-automated (as in nudge analyt- ics [3], perhaps exemplified in the signals project [5]), and non-automated (through instructor or peer help interaction) remediation have been explored. Similar to the issue of the locus of control, there is no clear correct choice between these three levels of automation; all of the previously de- scribed techniques have shown that they can increase learner knowledge and satisfaction, and choosing one over others de- pends largely on the capabilities and quality of data you have available.  In this work we address the question of the locus of con- trol in adaptive e-learning systems in light of having instruc- tional experts empowered with analytics knowledge. Instead of either automatically adapting to or being adaptable by learners, we aim to consider how instructors or instructional designers can gain insights from usage data which they then use to parametrize the way in which adaptation in the sys- tem is to take place. Thus the tension of the locus of control is mediated by a pedagogue who acts as a guiding hand, quietly informing how the system both adapts and presents cues to the learner for adaptation.  2. MOTIVATION & CONTEXT In previous work we have shown how lecture video con-  tent can be automatically segmented into meaningful pieces using a combination of expert data and image recognition [1]. Through interviews with a group of learners, it became clear that different learners approached the issue of how to segmenting video differently, and this difference was largely the result of the perceived usefulness of segments for a given learning task.  Using the same system we have shown that learners, based on their usage in the system, can be automatically clustered into different groups [2]. These groups appear to be in- dicative of end-user task; some learners would watch lecture videos every week, some only during the early or late por- tions of the course, while still others would watch video only when assessment drew near.  Since clustering is an unsupervised technique, the group- ings of students found in this second investigation arent meaningful until an instructional expert has labelled them. Without knowing this label, the system is unable to provide different segmentations in a principled manner.  It is possible to provide this labelling once for the applica- tion as a whole, which can then use these labels in choosing  84    Figure 1: A mockup of a lecture video playback system.  an appropriate segmentation algorithm for a given learner. But we would be remiss to do so without first verifying that the clusters discovered are true for all domains, instructors, and circumstances that the system might be used in  a sig- nificant endeavour indeed! Further, even if it were shown that clusters are stable across domains, and clusters were validated with respect to educational tasks, video segmenta- tion is but one piece of an adaptive e-learning environment; this process would need to be repeated for each element in the system that is to be made adaptive.  3. DESIGNING INSTRUCTIONAL EXPERTS INTO THE PROCESS  Our solution to this issue is to not design the system as an adaptive system per se, but to design it as an adapt- able system where an instructional expert chooses how and when the system should present itself to the end user. In short, the system monitors learner usage, presents analytics information to an instructor or instructional designer who then labels meaningful patterns and parametrizes how adap- tion within the system should occur when these patterns are found.  Consider the case of the video lecture system described in [2] a mockup1 of which is shown in figure 1. In the sys- tem there are multiple videos show to users depending on the capabilities of the classroom. Data projector video is segmented, and a list of segments is shown to the user on the left hand side. Clicking on a segment navigates the user to the corresponding portion of the video., and traditional video scrubber tools as well as a note taking widget are avail- able. In this system the note taking widget contains both a private note-taking space, as well as the combined outputs of all students who have taken notes (a shared space).  1To clarify this is a design proposition and not a fully devel- oped solution this work will present all designs as low fidelity prototypes.  As students use the system they leave behind traces of what they have done; segments they have clicked on; pieces of video they have watched, paused, and rewatched; notes that they have made; etc. An ongoing challenge is how to present this information to instructional experts who may not understand statistical clustering techniques. We are con- sidering a learner-first approach, in which visualizations of the results of clustering are shown using treemaps, where the top level treemap describes all learners who are registered in the course2 (figure 2). The expert can then modify the crite- ria by which learners are clustered using attributes available to them on the left hand side, and explore the results of the clustering process on the right hand side.  Key to this method is that the clusters have no meaning to the system until they are labelled. The instructional expert does this by selecting a cluster (a rectangle in the treemap), inspecting the data using traditional charting tools (shown at the bottom), and editing the label field. Each cluster is hierarchical, allowing the expert to recursively inspect and label sub-clusters of the data by double clicking. Clustering is static process based upon the attributes which the peda- gogue has identified (in the left hand window). Membership of learners in clusters will change over time as more user data is collected, but the definitions of each cluster (the centroid) will not change until the expert chooses to delete labels.  A learner may be in multiple clusters at once. The in- structional expert may choose to cluster data around some attribute set and provide labels for those clusters, then clus- ter around another attribute set for other purpose and come up with different labels. The effect of being in multiple clusters is that the system may be able to adapt the user interface in multiple ways.  For example, a learner who is reviewing content for an exam and is a social constructivist learner may be recognized  2Or those learners who have used the tool, in the case of courses that have no set registration.  85    Figure 2: Data exploration page; a list of the possible attributes to cluster by are shown on the left hand side. The treemap at the top right shows the clusters found, as well as the number of learners in each cluster and the expert-provided label. In this example, the expert has labelled the smallest cluster reviewers, and is exploring the data through traditional charts and graphs at the bottom right.  Figure 3: Parametrization of the segmentation widget. Note that each widget (in background) has a drop down allowing the pedagogue to delete the widget, add a new widget, or parameterize the widget that already exists. The parameters are supplied for each cluster label in the system; in this case there are three labels (Reviewing, Social, and First Time). Widget parameters, such as 1 every 30s, are specific to the widget being customized, and we envision the use of controlled vocabularies and interface mechanisms to make this natural.  86    as such, and the system may adapt lecture video segment- ing to provide overviews of relevant material while at the same time making available social tools such as chatrooms or shared notetaking features. Or, a learner who regularly returns to content and is a non-native language speaker may be shown closed captioning tools and more detailed segments to aid in navigation, while learners who had been shown to navigate quickly between segments may be provided video in high speed playback.  Once clustering data has been labelled, the instructional expert can make these kinds of parametrizations to describe how adaptation takes place. We envision this using an in- terface similar to that which the student sees, where the pedagogue can add, remove, or characterize widgets based on the clusters a learner may belong to (figure 3). Parame- ters are widget-specific, and a default application view exists for those learners who are not in a labelled cluster.  4. CONCLUSIONS This work is proposing that instructors and instructional  designers be included as mediating agents with respect to the locus of control for adaptable systems where learning analytics data is available. By having instructional experts parametrize how adaptation happens, the burden of vali- dating the educational effectiveness of a given adaptation by system developers is lessened. Further, this approach provides an inclusive method of customizing an adaptive e- learning system for different educational domains, tasks, and contexts.  As a design, this work leaves us with unanswered questions of end-user perceptions of such a system, some of which we elaborate on here:   Will instructors, content experts, and instructional de- signers see value in attaining the insights and providing methods of adaptation   Can the system be written such that it is accessible to these experts, and uses language and terms they understand   Does this approach force on already burdened educa- tors the need (either explicitly or implicitly) to mi- cromanage the adaptive systems that support their courses   Will adaptations be natural for learners, or does more of the adaptation process need to be opened up to them (for instance, through scrutable modelling [4])   Are adaptations reusable enough to be shared such that they can serve as a starting point for new instruc- tors and instructional designers who want to partake in this kind of endeavour  The areas of educational data mining, adaptive hyperme- dia, artificial intelligence in education, and intelligent tutor- ing systems are largely void of researchers situated in tradi- tional education departments. With this work, were hoping broadening the dialogue around adaptive e-learning systems to include these experts of instruction directly. We do so by proposing that the starting point for adaptation sit in the hands of instructors and instructional designers, and that they determine, based on learning analytics, what actions  should be taken. In designing the parameters for these envi- ronments, we believe instructional experts will reason more deeply about the patterns found in their classroom data. We aim to capitalize on this insight, and hope that not only will those experts see pedagogical gains in their daily activ- ities, but that education researchers will use these methods to contribute to the growth of the field of e-learning.  5. REFERENCES [1] C. Brooks, K. Amundson, and J. Greer. Detecting  Significant Events in Lecture Video using Supervised Machine Learning. In Proceeding of the 2009 conference on Artificial Intelligence in Education: Building Learning Systems that Care: From Knowledge Representation to Affective Modelling, pages 483490, Brighton, UK, 2009. IOS Press.  [2] C. Brooks, C. Demmans Epp, G. Logan, and J. Greer. The Who, What, When, and Why of Lecture Capture. In Proceedings of the 1st Annual Conference on Learning Analytics (LAK11), Banff, AB, Canada, 2011.  [3] C. Carmean and P. Mizzi. The Case of Nudge Analytics, 2010.  [4] J. Kay. Scrutable adaptation: Because we can and must. Adaptive Hypermedia and Adaptive Web-Based, pages 11  19, 2006.  [5] U. of Purdue. Course Signals - Stoplights for Student Success.  87      "}
{"index":{"_id":"17"}}
{"datatype":"inproceedings","key":"McKay:2012:AIE:2330601.2330627","author":"McKay, Tim and Miller, Kate and Tritz, Jared","title":"What to Do with Actionable Intelligence: E2Coach As an Intervention Engine","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"88--91","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330627","doi":"10.1145/2330601.2330627","acmid":"2330627","publisher":"ACM","address":"New York, NY, USA","keywords":"predictive models for student performance, tailored communication","Abstract":"In this paper, we describe a new, analytics driven approach to supporting students in large introductory physics courses. For this project, we have assembled data for more than 49,000 physics students at the University of Michigan. For each, we combine an extensive portrait of background and preparation with details of progress through the course and final outcome. This information allows us to construct models predicting student performance with a dispersion of half a letter grade. We explore residuals to this model, conducting structured interviews with students who did better (and worse) than expected, identifying strategies which lead to student success (and failure) at all levels of preparation. This work was done in preparation for the launch of E2Coach: a computer tailored educational coaching project which provides a model for an intervention engine, capable of dealing with actionable information for thousands of students.","pdf":"What to do with actionable intelligence:   E2Coach as an intervention engine    Tim McKay  University of Michigan  Department of Physics   450 Church St., Ann Arbor, MI  001-734-763-1462  tamckay@umich.edu   Kate Miller  University of Michigan  Department of Physics   450 Church St., Ann Arbor, MI  001-734-763-1462  katemi@umich.edu   Jared Tritz  University of Michigan  Department of Physics   450 Church St., Ann Arbor, MI  001-734-763-1462  jtritz@umich.edu   ABSTRACT In this paper, we describe a new, analytics driven approach to  supporting students in large introductory physics courses. For this  project, we have assembled data for more than 49,000 physics  students at the University of Michigan. For each, we combine an  extensive portrait of background and preparation with details of  progress through the course and final outcome. This information  allows us to construct models predicting student performance with  a dispersion of half a letter grade. We explore residuals to this  model, conducting structured interviews with students who did  better (and worse) than expected, identifying strategies which lead  to student success (and failure) at all levels of preparation. This  work was done in preparation for the launch of E2Coach: a  computer tailored educational coaching project which provides a  model for an intervention engine, capable of dealing with  actionable information for thousands of students.   Categories and Subject Descriptors K.3.1 [Computer Uses in Education]: Computer Assisted  Instruction  General Terms Measurement, Design, Experimentation, Human Factors   Keywords Tailored communication, Predictive models for student  performance   1. INTRODUCTION  Nationally, more than half of students who arrive in college  intending to complete degrees in STEM disciplines fail to do so  [3]. The most disastrous drop-off is associated with gateway  introductory courses in math, physics, chemistry, and biology [8]. These courses are usually large and always challenging, with   average grades well below those in typical college courses. Many  students emerge from these gateway courses having done worse  than expected; their confidence is undermined and their desire to  continue in a STEM discipline strongly diminished. This happens  to students across the spectrum of performance: from solid A  students receiving their first B+, to struggling C+ students  slipping into the D range. If we wish to increase the number of  students completing degrees in STEM disciplines, we must  address the loss of potential STEM majors due to large,  impersonal gateway courses.  Students in gateway STEM courses are diverse by many  measures, yet we ask them to learn using a single generic  approach. They all read the same texts, hear the same lectures, do  the same homework and class assignments, get the same advice,  and are assessed using the same exams. We have worked hard at  the University of Michigan to design physics classes that optimize  learning for the typical student; these courses are excellent in the  mean. But we have done little to adjust our teaching methods to  meet the unique needs of individuals.   We can do better. Technology exists which can give each student  individualized coaching that will dynamically recognize their  strengths, weaknesses, and performance trends, understand their  motivations and goals, and guide them through the course, all the  while encouraging them to continue toward a STEM degree.  Intelligent tutoring systems focusing on domain specific  knowledge have a long heritage [1][5] and are known to be  effective. More recently, educational systems that focus on  learners motivation and affect have received increased attention  [2]. Tailored communication techniques are well established in  the world of public health, where their efficacy has been  extensively tested.    Our goal is to gather relevant data about the students, collect the  expert advice of both faculty and students, encode this in tailoring  logic, and deliver personalized expert electronic coaching to the  more than 1900 students who take introductory physics at  Michigan each term.   2. CUSTOMIZING THE APPROACH  To better understand this approach, consider some of the details  of the introductory physics course at Michigan. In such a course,  we offer students a dozen tools for achieving their learning goals:  commercial textbooks, custom coursepacks, pre-lecture reading  quizzes, online homework systems with real-time feedback,  interactive lectures with Peer Instruction, modeling of expert   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee. LAK12, 29 April  2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00  88    problem solving with additional problems to work in groups,  multiple practice exams with solutions, notecards to use during  exams, student led study groups organized by the UM Science  Learning Center, all day tutoring facilities, an up-to-date online  gradebook for feedback, and office hours with faculty. This wide  array of learning tools is provided in the hope that it will meet all  the needs of a diverse group of students.    Unfortunately, most students receive no individualized advice  about how to utilize these tools. They lack the flexibility to skip  what will not help them or to access more of what they really  need, and never receive feedback or encouragement that is aware  of their personal goals, identity, or interests. A few percent do get  personal advice and feedback; those who visit office hours  regularly. Faculty experience with the lucky few suggests that  specialized advice and encouragement can make a substantial  difference in outcomes. Indeed personalized mentoring strategies  of this kind are perhaps the only proven tools for STEM retention  [10].  To provide customized, personal advice to each student, we must  accomplish three separate tasks. First, we must generate some  actionable intelligence  to gather data representing the state of  each student adequate to decide what help and support they need.  Second, we must know what action to take in each case  to gather expert advice. Finally, we need to have a mechanism for  delivering the appropriate feedback to each and every student in  the class  something which is impossible with the current system  of face-to-face office hours. We need a technological way to  provide each student with customized advice.   3. GENERATING ACTIONABLE   INTELLIGENCE: THE BETTER-THAN- EXPECTED  The data which inspired this project have been collected and  studied as part of the UM Physics Departments Better Than  Expected (BTE) project [6]. For this project we have gathered  detailed information describing the progress of 48,579 students  through our introductory physics courses over the last 14 years.  From the Universitys data warehouse, we combine a portrait of  each student at the time they enter the class with internal  gradebook information and final grades. These data allow us to  quantify the impact of student preparation and background on  course outcomes, and to construct predictive models of student  performance. While a number of parameters correlate with final  grade, prediction with a half letter grade dispersion can be  accomplished using just one parameter: each students University  of Michigan GPA at the start of the term. With hindsight this is  not too surprising: A students are largely A students and C  students are largely C students. Several examples of the  correlation between incoming GPA and physics grade are shown  in Figure 1.   Our ability to track outcomes for many groups is especially  important, as we have clear evidence that subsets of students  underperform relative to others in these courses, even when  controlling for a variety of parameters related to technical  preparation. For example, first generation college students and  those from low income families (<$50K/year) receive final grades  about a quarter of a letter grade lower than their classmates when  compared at the same entering ACT Math score. Female students  are similarly disadvantaged, falling 0.2-0.3 letter grades below  male students at all measures of incoming preparation: SAT or   Figure 1: Example results from the UM Better than Expected project for Physics 140, the first semester course for engineering and  physical science students. Left: Mean grades and dispersion as a function of UM GPA at the time the course begins. Right: Mean grades as a function of SAT math score. Results are shown for both male (diamonds and dotted lines) and female (triangles and dashed  lines) students. Dotted and dashed lines show the 1 dispersion for male and female students repectively A strong gender disparity in  physics grade is seen, with female students faring worse than male students at all levels of GPA and SAT math score.  i l l f h h d j f h i h fi f i i d  89    ACT math score, High School GPA, and even prior University of  Michigan GPA (see Figure 1). Reducing these disparities with  appropriate interventions is one of our primary goals. Research  suggestsError! Bookmark not defined. that eliminating this  underperformance will also have a substantial impact on the  STEM retention of these students.    3.1 Deciding what to do: gathering expert  advice for success  From the BTE project, we know what performance to expect for  each student, but significant dispersion in outcomes remains. It is  here that we aim to act: we want to find out what interventions  would help every student do better-than-expected. To this end, we  gathered expertise from three sources: individual students, physics  faculty members, and student study group leaders employed by  the UM Science Learning Center.    To begin, we have identified subsets of students who did better  than expected (BTE) or worse than expected (WTE) in previous  terms and have conducted structured interviews of them to help us  understand what leads to these disparate outcomes. The initial  round of interviews has revealed several important predictors of  success previously invisible from our data. Response to the first  exam is often the most important factor in ultimate student  performance. Students who change their approach to the class are  likely to improve their outcomes. We need to be able to encourage  this behavior change, and to provide students with detailed  information about how they should change. Prompt attention to  setbacks is also essential.   We have also conducted interviews of two groups of instructors;  physics faculty members with many years of experience, and more  advanced student study group leaders who have successfully  completed these courses and often provide advice to current  students. Advice from all three groups tells us what we should do  with each student  what we should do with our actionable  intelligence. Now we need only a method for delivering our  intervention.   4. THE MICHIGAN TAILORING SYSTEM  AND INDIVIDUALIZED EDUCATIONAL  INTERVENTIONS  We are able to provide individual advice and coaching to every student by leveraging a powerful, proven tool: the Michigan  Tailoring System. MTS is an open-source tailoring toolkit created  by the UM Center for Health Communications Research (CHCR)   [4]. The CHCR team has worked for decades to deliver expertly  tailored health behavior interventions over a wide variety of  topics, populations, settings, and communications channels.   Computer tailored communications harness the power of  personalized coaching from an expert, based on specific  knowledge of the subject, while delivering services inexpensively  to large, distributed populations. Systems of this kind outstrip the  abilities of individual expert coaches in important ways; they  access a wider range of information, intrinsically quantify the  efficacy of their advice in the outcomes of a wide variety of  subjects, and are not limited in time or space. MTS applications  can be constantly refined; their efficacy always being improved,  never forgetting a lesson learned, and never running out of time,  patience, or enthusiasm. A great strength of computer tailoring is  tireless scalability. Once the systems are constructed, they can   coach ever larger groups of subjects with minimal additional  investment.   Tailoring approaches have been extensively tested within the  public health community, where their efficacy is clearly  established in peer reviewed journals. To give one example, a two  group randomized trial of web-based tailoring in a smoking  cessation project showed a 25% increase in continuous abstinence  compared for those who received tailored as opposed to  untailored communications [9]. Tailoring approaches in public  health have also been commercialized widely. One example,  HealthMedia Inc., was founded in 1998 by UM Professor, CHCR  Founding Director Vic Strecher. HealthMedia now provides  services to millions of participants each year across a broad range  of interventions.    During Fall 2010 the UM Physics Department joined forces with  CHCR. Using support from the EDUCAUSE Next Generation  Learning Challenge (NGLC), we began working to create an  educational adaptation of the MTS system for use in our large  introductory physics courses. This project has now been  implemented, with a first intervention beginning in January 2012.  4.1 E2Coach  We call this educational adaptation of tailoring E2Coach, where  we intend the E2 to evoke both electronic and expert. At the start  of each class, E2Coach uses the results of a survey to absorb a  complex array of information about each student. This voluntary  initial survey will provide a rich portrait of each student who opts- in as they enter the course, including details about their  background in physics and mathematics, their motivations for  taking the course, desired and expected grades, attitudes toward  physics, and confidence in being able to accomplish their goals. This initial portrait is augmented as the term goes on, with new  information coming both from the gradebook of the course.  The  combination will provide a real-time portrait of each students  progress. Combined with historical expectations for their final  performance based on the BTE project, we have what we need to  intervene.    E2Coach provides the interface between students and the  extensive and powerful resources available in each course,  customizing recommendations for study habits, assignments for  practice, feedback on progress, and encouragement they receive.  At important points in the course, each student receives detailed  feedback on their current status, along with normative information  about how their work compares to their peers and predictions for  what final grade they are most likely to receive if they continue to  approach the course in the same way. A significant strength of this  system is its ability to realistically predict how much better each  student might hope to do if they improve their approach to the  course. These predictions are based on the extensive historical  information from the BTE project.    E2Coach advice is delivered to each student as a personal web  page filled with information and advice tailored to both their state  and identity. Research in public health tailoring has clearly shown  the power of personalized communication, with the efficacy of  advice given in this way rising substantially with increased  degrees of personalization. For example, testimonials provide a  very effective way of delivering advice, and are much more  effective when the identity of the testifier is closely matched to  that of the recipient.    90    For our purposes, testimonials are derived from UM students (the  study group leaders). Each student receives advice from a former  student who shares their background, goals, and concerns. So  premedical students will not receive testimonials from particle  physics faculty or engineering students who love physics, but  from other premedical students less familiar with physics when  they started and who, like the current student, felt their future as a  physician was put at risk by this class.   4.2 E2Coach progress and evaluation  NGLC funding for the E2Coach system for Physics began in May  2011 and will continue for 15 months. The system has undergone  a rapid development cycle, and was launched across all  introductory physics courses in January 2012. Refinements will  take place during Summer 2012, and a second semester of  E2Coach courses will be offered in Fall 2012. This NGLC project  will deliver tailored coaching messages to 3800 introductory  physics students before the project ends in December 2012.   By uniting student activity data with continual measures of  performance, we also establish a powerful system for quantifying  efficacy  one that is intrinsically sensitive to the diverse nature of  our student population. Because of the grade prediction schemes  detailed above, we can separately assess the impact of preparation  on students across the spectrum: those likely to struggle, certain to  succeed, and headed for an average outcome. Since the system  addresses each student individually, we have the opportunity to  improve the performance of our students at all levels. Indeed this  ability to have a positive impact on both at-risk and exceptionally  successful students using the same system is one of the most  attractive features of tailored coaching.   To test the overall impact of E2Coach we will compare the  performance of students using it to that of the 48,579 students in  our BTE historical data set. We will look in particular for changes  in some of the measures which motivated this work. At the most  basic level, we will compare the performance of students in  tailored classes to what we have found in untailored classes  offered in the past. We have found that performance in these  classes is affected not only by preparation but also by gender,  socioeconomic status, and degree of family experience with  higher education. These performance disparities may well be  caused by the kind of psychosocial influences which tailoring is  particularly well suited to address. The recent success of a values  affirmation intervention in reducing the gender disparity at the  University of Colorado supports this notion [10]. Eliminating  these disparities is a central goal of this project: no group taking  these classes should be disadvantaged by psychosocial factors  unrelated to content knowledge. To test our success, we will  compare the magnitude of each disparity in our new tailored  courses to that seen in our historical data.   The BTE project has also shown a disappointingly strong  correlation between first exam performance and subsequent work  for all students. Ideally struggle on a first exam would trigger a  revised commitment to the course, rather than sealing a students  fate. This provides a principle focus for our tailoring design: to  encourage struggling students to change their study habits, use  course resources more effectively, and address weaknesses in their  preparation directly. To test the impact of E2Coach on the  likelihood that students will recover from a rocky start, we will  compare the correlation of first and later exams in tailored and  untailored courses.   It is possible that the three goals outlined above might be  achieved by merely increasing the level of communication with  students. It is important that we should separate the effect of  individual tailoring of messages from that of merely increased  communication. We will conduct this test during the Fall 2012  semester, randomly dividing students into two groups. The first  group will receive the fully tailored E2Coach intervention. The  second group will all receive identical communications tailored  for the statistically average student. Neither instructors nor  students will know who occupies each group until after the term.   Once the term is complete, we will compare results for the  individually tailored and uniform communication students. This  will allow us to confidently separate the effects of tailoring from  the effects of merely increased communication.     A significant goal of this paper is to make the Michigan Tailoring  System better known to the learning analytics community, and we  will provide an overview of the work required to adopt this  mature, open-source tool for educational applications.   5. REFERENCES  [1]  Anderson, J.R., Conrad, F.G., & Corbett, A.T. (1989). Skill   acquisition and the LISP Tutor, Cognitive Science,13, 467- 506.  [2]  Du Bulay, et al., 2010, Towards Systems That Care: A  Conceptual Framework based on Motivation, Metacognition  and Affect, International Journal of Artificial Intelligence in  Education, 20, http://iaied.org/pub/1310/  [3]  Hayes, R.Q., Whalen, S.K., and Cannon, B., 2009, CSRDE  STEM retention report, Center for Institutional Data  Exchange and Analysis, University of Oklahoma, Norman.   [4] http://chcr.umich.edu/ [5] Koedinger, K., Anderson, J., Hadley, W., and Mark, M.,   1997, Intelligent tutoring goes to school in the big city.  International Journal of Artificial Intelligence in Education, 8, 30-43.  [6] Miller, Kate, 2011, Gender Matters: Assessing and  Addressing the Persistent Gender Gap in Physics Education,  University of Michigan Honors Senior Thesis.   [7] Miyake, Akira, Lauren E Kost-Smith, Noah D Finkelstein,  Steven J Pollock, Geoffrey L Cohen, and Tiffany a Ito. 2010.  Reducing the gender achievement gap in college science: a  classroom study of values affirmation. Science (New York,  N.Y.) 330, no. 6008 (November): 1234-7.  doi:10.1126/science.1195996.   [8] Rask, K., 2010, Attrition in STEM fields at a liberal arts  college: The importance of grades and pre-collegiate  preferences, Economics of Education Review, 29, 892.   [9] Strecher, Victor J, Saul Shiffman, and Robert West. 2005.  Randomized controlled trial of a web-based computer- tailored smoking cessation program as a supplement to  nicotine patch therapy. Addiction. 100, no. 5 (May): 682-8.  doi:10.1111/j.1360-0443.2005.01093.x.  http://www.ncbi.nlm.nih.gov/pubmed/15847626.   [10] Wilson, Z., et al., 2011, Hierarchical Mentoring, A  Transformative Strategy for Improving Diversity and  Retention in Undergraduate STEM Disciplines, Journal of  Science Education Technology, DOI 10.1007/s10956-011- 9292-5.  91    "}
{"index":{"_id":"18"}}
{"datatype":"inproceedings","key":"Shum:2012:LDT:2330601.2330629","author":"Shum, Simon Buckingham and Crick, Ruth Deakin","title":"Learning Dispositions and Transferable Competencies: Pedagogy, Modelling and Learning Analytics","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"92--101","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330629","doi":"10.1145/2330601.2330629","acmid":"2330629","publisher":"ACM","address":"New York, NY, USA","keywords":"21st century skills, educational assessment, effective lifelong learning inventory, learning analytics, learning dispositions, learning how to learn, learning power, transferable skills","Abstract":"Theoretical and empirical evidence in the learning sciences substantiates the view that deep engagement in learning is a function of a complex combination of learners' identities, dispositions, values, attitudes and skills. When these are fragile, learners struggle to achieve their potential in conventional assessments, and critically, are not prepared for the novelty and complexity of the challenges they will meet in the workplace, and the many other spheres of life which require personal qualities such as resilience, critical thinking and collaboration skills. To date, the learning analytics research and development communities have not addressed how these complex concepts can be modelled and analysed, and how more traditional social science data analysis can support and be enhanced by learning analytics. We report progress in the design and implementation of learning analytics based on a research validated multidimensional construct termed learning power. We describe, for the first time, a learning analytics infrastructure for gathering data at scale, managing stakeholder permissions, the range of analytics that it supports from real time summaries to exploratory research, and a particular visual analytic which has been shown to have demonstrable impact on learners. We conclude by summarising the ongoing research and development programme and identifying the challenges of integrating traditional social science research, with learning analytics and modelling.","pdf":"Learning Dispositions and Transferable Competencies:   Pedagogy, Modelling and Learning Analytics     Simon Buckingham Shum   Knowledge Media Institute  The Open University   Milton Keynes, MK7 6AA, UK  +44-1908-655723   s.buckingham.shum@gmail.com   Ruth Deakin Crick  Graduate School of Education   University of Bristol  Bristol, BS8 1JA, UK  +44-117-3314338   ruth.deakin-crick@bristol.ac.uk      ABSTRACT  Theoretical and empirical evidence in the learning sciences  substantiates the view that deep engagement in learning is a  function of a complex combination of learners identities,  dispositions, values, attitudes and skills. When these are fragile,  learners struggle to achieve their potential in conventional  assessments, and critically, are not prepared for the novelty and  complexity of the challenges they will meet in the workplace, and  the many other spheres of life which require personal qualities  such as resilience, critical thinking and collaboration skills. To  date, the learning analytics research and development  communities have not addressed how these complex concepts can  be modelled and analysed, and how more traditional social  science data analysis can support and be enhanced by learning  analytics.  We report progress in the design and implementation of  learning analytics based on a research validated multidimensional  construct termed learning power. We describe, for the first time,  a learning analytics infrastructure for gathering data at scale,  managing stakeholder permissions, the range of analytics that it  supports from real time summaries to exploratory research, and a  particular visual analytic which has been shown to have  demonstrable impact on learners. We conclude by summarising  the ongoing research and development programme and identifying  the challenges of integrating traditional social science research,  with learning analytics and modelling.   Categories and Subject Descriptors   J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI)   General Terms  Design, Human Factors, Theory   Keywords  learning analytics; learning dispositions; learning power; learning  how to learn; transferable skills; 21st century skills; educational  assessment, Effective Lifelong Learning Inventory   1. INTRODUCTION  Information infrastructure embodies and shapes worldviews. The  work of Bowker and Star [1] elegantly demonstrates that the  classification schemes embedded in information infrastructure are  not only systematic ways to capture and preservebut also to  forget, by virtue of what remains invisible. Moreover, the user  experience foregrounds certain information, thus scaffolding  particular forms of human-computer and human-human  interaction, which in turn promotes or obstructs sensemaking [2].    Learning analytics and recommendation engines are no exception:  they are designed with a particular conception of success, thus  defining the patterns deemed to be evidence of progress, and  hence, the data that should be captured. A marker of the health of  the learning analytics field will be the quality of debate around  what the technology renders visible and leaves invisible, and the  pedagogical implications of design decisions, whether the design  rationale is explicit or implicit. In this paper we focus on the  challenge of designing learning analytics that render visible  learning dispositions and the transferable competencies   associated with skillful learning in diverse contexts. These are  dimensions of learning that both research and practice are  demonstrating to be increasingly important, but which the  learning analytics field has yet to engage with deeply.    Mastery of discipline knowledge as defined by an explicit  curriculum is obviously a critical yardstick in learning, and it is  not surprising that currently, this is the dominant focus of most  learning analytics research and product development, since this is  the dominant paradigm in educational institutions. We know that  this is greatly assisted when aspects of the domain and learner can  be modelled: user models compare the inferred cognitive model  against an ideal model (intelligent tutoring, eg. [3]); presentation  layers may tune content dynamically if progress is deemed to be  too slow (adaptive educational hypermedia, e.g. [4]); data mining  techniques can be deployed, which usually assume the goal is to  pass the course (e.g. [5]).    In a different part of the learning analytics design space, we see  the use of generic learning management systems that are agnostic  as to the subject matter (and indeed have only a rudimentary  model of the domain, if any). The trend to generic platforms is  accompanied by their disaggregation, as open, social platforms,  managed by many entities, are used for informal, self-directed  learning, sometimes around the edges of formal courses. Learning  analytics in these contexts must address a very different learning  context, in which the domain, learning objectives, learner cohort  and course materials may all be unknown in advance, and may not  be controllable (Massive, Online, Open Courses  MOOCs  may  be the extreme instance).    Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.   LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   92    Converging with these technology-driven trends, is traditional  social science research into the personal qualities that enable  effective learning across contexts. There is substantial and  growing evidence within educational research that learners  orientation towards learningtheir learning dispositions significantly influence the nature of their engagement with new  learning opportunities, in both formal and informal contexts.  Learning dispositions form an important part of learning-to-learn  competences, which are widely understood as a key requirement  for life in the 21st Century. Despite this, employers complain  increasingly that many graduates from our school and university  systems, while proficient at passing exams, have not developed  the capacity to take responsibility for their own learning and  struggle when confronted by novel, real world challenges [6].   In this paper we argue that by combining extant research findings  from the social science field of education, particularly concerning  engagement in learning and pedagogy, with the affordances of  learning analytics, we can develop learning platforms that more  effectively catalyse the processes of learning for individuals and  collectives.    We introduce the concept of meta-competencies (2) as one of  several approaches to characterising the demands on learners  made by todays society, and we note the escalating problem of  school disengagement (3). We then summarise some of the core  insights in the literature around engagement and learning  dispositions (4), before explaining the use of self-report as a  means of gathering dispositional data (5). In 6 we introduce  Learning Power, a multi-dimensional construct for modelling  learning dispositions, which has been under development and  validation for over a decade, but in this paper we present for the  first time the Learning Warehouse platform which underpins it  (7). This generates a visual analytic spider diagram for  individuals, which renders the underlying model (8), plus cohort  summary statistics which can inform pedagogical intervention. In  9 we consider qualitative, quantitative and narrative ways to  validate dispositional analytics of this sort, including evidence  that the visual analytic has pedagogical affordances which build  learners self-awareness. We also provide examples of how the  analytics platform facilitates deeper analyses within and across  datasets. 10 summarises four key forms of service that the  platform is facilitating which help to close the research-practice  gap. We conclude by summarising the contributions that this  research makes (11), and outlining some of the avenues now  being pursued (12).   2. META-COMPETENCIES  Where formal learning is highly specialised and discipline bound,  very often graduates, including those with traditional degrees in  vocational subjects like engineering or law, find themselves with  jobs in which they cannot make much use of whatever specialist  knowledge they possess [7]. The acquisition of subject matter  knowledge is no longer enough for survival and success in a  society characterized by massive data flows, an environment in  constant flux, and unprecedented levels of uncertainty (e.g.  around how socio-technical complex systems will behave, and  around what can or should be believed a true, or ethically sound).   What is needed in addition is the ability to identify and nurture a  personal portfolio of competencies that enable personal and  collective responses to complex challenges.     We understand competence as a combination of knowledge, skills,  understanding, values, attitudes and desires, which lead to  effective, embodied human action in the world, in a particular  domain. Skillful accomplishment in authentic settings requires not   only mastery of knowledge, but the skills, values, attitudes,  desires and motivation to apply it in a particular socio-historical  context, requiring a sense of agency, action and value [8].     Writing from the perspective of education, Haste summarises  competencies required for 21st century survival. She identifies   one overarching meta-competence which is the ability to  manage the tension between innovation and continuity, and argues  that this  is constituted in five sub-competences: the ability to  (i) adaptively assimilate changing technologies (ii) deal with   ambiguity and diversity (iii) find and sustain community links   (iv) manage motivation and emotion and (v) enact moral   responsibility and citizenship.  To be competent in this richer,  more expansive sense, the possession of knowledge is necessary  but not sufficient. Also required are personal qualities and  dispositions, a secure-enough sense of identity and purpose, and a  range of new skills that enable links to be made across domains  and processes.    Bauman has argued that deep engagement in learning is  particularly important today for two reasons [9]. Firstly, as many  school and university teachers will recognise, there is a  contemporary search for identity in todays fluid, globalised  society, and secondly, educational philosophy and theory face  the unfamiliar and challenging task of theorising a formative  process which is not guided from the start by the target form  designed in advance (p.139). That is, as we transition  increasingly to a world where relevant outcomes in a real world  context can no longer be pre-determined with the confidence of  earlier times, and where a learners intrinsic capacity to rise and  adapt to a challenge is a highly valued trait, we need a theory and  practice of engagement in learning that facilitates the formation of  identity, combined with scaffolding the processes of knowledge- creation and authentic performance.      Thomas and Seely Brown [10] argue for the need to embrace a  theory of learning to become (p.321) in contrast to theories that  see learning as a process of becoming something. They argue that  the 20th century worldview shift from learning as transmission to  learning as interpretation, is now being replaced by learning as  participation, fuelled by structural changes in the way  communication happens through new technologies and media.  Participation is embodied and experienced, and critically, requires  indwelling:    The potential revolution for learning that the networked   world provides is the ability to create scalable environments   for learning that engages the tacit as well as the explicit   dimensions of knowledge. The term we have been using for   this, borrowed from Polanyi, is indwelling. Understanding   this notion requires us to think about the connection between  experience, embodiment and learning. [10] (p.330)   3. LEARNER DISENGAGEMENT  The development of the above kinds of competencies presents a  challenge for policies and pedagogies that validate learning solely  in terms of standardised outcomesdesigned (as are all analytics)  to facilitate the generation of certain kinds of insight, for certain  kinds of stakeholders. An over-emphasis on these indices is in  tension with the need to take into account the complexity of  learners sense of identity and their whole attitude to learning. If  learners are, for whatever reason, fundamentally not disposed to  learn, then extrinsic drivers around exam performance are  unlikely to succeed. As Dewey (1933) observed:   93    Knowledge of methods alone will not suffice: there must be   the desire, the will, to employ them. This desire is an affair of  personal disposition. [11] (p.30)   Rising disengagement is a problem in many developed countries  education systems. Research undertaken for the English  Department for Education [12] reported in 2008 that 10% of  students hate school, with disproportionate levels amongst less  privileged learners (however, highly engaged students from poor  backgrounds tend to outperform disengaged students from  wealthy backgrounds). The Canadian Education Association  regularly surveys student attitudes to school, reporting in 2009  that intellectual engagement falls during the middle school years  and remains at a low level throughout secondary school [13]. A  2009 US study across 27 states reported that 49% students felt  bored every day, 17% in every class [14]. These disturbing data  point to a widening disconnect between what motivates and  engages many young people, and their experience of schooling.  This is serving as a driver for action research into new models  focused on the wholistic design of learning, catalysing academics  [15-18] and national schools networks (e.g. the UKs  WholeEducation.org).    How can learning analytics research and development engage  with this challenge Certainly, there is a contribution to be made  by providing more detailed, more timely information about  performancebut while dismal analytics will help educators,  their impact on already disengaged learners might be  counterproductive. We propose that disposition analytics could  spark intrinsic motivation by giving learners insight into how they  approach learning in general, and how they can become more  skillfully equipped for many other aspects of their lives beyond  school. We construe this challenge as one of defining, measuring,  modelling and formalizing computationally the constructs  associated with learning dispositions.   4. DEFINING DISPOSITIONS  What we are seeking to track, and model for analytics purposes, is  a set of dispositions, values and attitudes that form a necessary but  not sufficient, part of a learning journey. Figure 1 summarises this  conceptualisation of learning dispositions, values and attitudes.  This is a complex and embedded journey because it takes  seriously the social, historical, cultural and personal resources that  shape, and are shaped by, peoples behaviour and dispositions.  Learning dispositions are personal, and autogenic. On the one  hand they reflect backwards (the personal left side of Figure 1)  to the identity, personhood and desire of the learner, and on the  other hand, they can be skilfully mobilised to scaffold forwards  towards the acquisition of the knowledge, skills and  understanding necessary for individuals to develop into competent  learners (the public right side of Figure 1). Competence in  learning how to learn requires agency, intention and desire, as  well as the dispositions or virtues necessary to acquire the skills,  strategies and knowledge management necessary for making the   most of learning opportunities over a lifespan, in the public  domain.    Although the term disposition is imprecise, both theoretically  and in practice, it is widely agreed that it refers to a relatively  enduring tendency to behave in a certain way [19]. It is a  construct linked to motivation, affect and valuing, as well as to  cognitive resources [20-24]. Dispositions may be culture specific  as well as a relatively enduring feature of personality.  A  disposition arises from desire, or motivation, which provides the  energy necessary for action [17, 25-27]. A disposition can be  identified in the action a person takes in a particular situation  for  example someone who is disposed to be curious will  demonstrate this in the manner in which they consistently  generate questions and investigate problems.   In practice, in education the term is often used interchangeably  with competence or style or capability, and it is frequently  subsumed within the concept of personal development as  distinct from academic development or attainment. There are  many dispositions which are relevant for education  ranging  from the specific to the very general, with varying conceptions as  to how fixed or malleable they are. Our focus is on malleable  dispositions that are important for developing intentional learners,  and which, critically, learners can recognise and develop in  themselves.   5. MEASURING DISPOSITIONS   Learning analytics cannot operate without data. For some  approaches, this data is a by-product of learner activity, data  exhaust left in the online platform as learners engage with each  other and learning resources. Other approaches depend on users  self-disclosing metadata about themselves intentionally,  knowing that it will be sensed and possibly acted on by people or  machines, known and unknown to them. Such intentional  metadata typically discloses higher order information about ones  state or intentions, which are harder to infer from low-level  system event logs. Examples of higher order metadata would  include emotional mood during ones studies, the decision to  play with an idea or perspective, or setting out to build ones  reputation in a group. These might be disclosed in twitter-style  updates, blog posts, comments in a meeting, written work and  responses to quizzes/questionnaires. In looking to future research  at the end, we signal new work on inferring dispositions from the  exhaust traces that learners leave in online environments, but the  focus of this paper is on self-reported data gathered via a self- diagnostic  quiz (the research-validated ELLI survey introduced  below).   Self-report is a standard means of gathering data in the social  sciences about an individuals values, attitudes and dispositions,  partly because of the challenges of observation at scale in non- digital environments, and partly because, however astute the  observer may be, what a person thinks or feels is by definition       Figure 1: Dispositions as a personal attribute, embedded in a learning journey oscillating between personal and public   94    idiosyncratic and cannot be confirmed only by the external  behaviours and artifacts: take for example an engaged, motivated  learner, with low academic ability, who may produce a lower  graded piece of work than a bored, disengaged high achiever  who submits something they have no personal interest in. From  the perspective of a complex and embedded understanding of  learning dispositions, what learners say about themselves as  learners is important and indicative of their sense of agency and   of their learning identity (indeed at the personal end of the  spectrum in Figure 1, authenticity is the most appropriate measure  of validity).   6. MODELLING DISPOSITIONS  Learning Power is a multi-dimensional construct that has come to  used widely in educational contexts in the last ten years. It is  derived from literature analysis, and interviews with educational  researchers and practitioners about the factors, which in their  experience, make good learners. The seven dimensions which  have been identified harness what is hypothesised to be the  power to learn  a form of consciousness, or critical subjectivity  [28], which leads to learning, change and growth.    An extensive literature review informed the development of a  self-report questionnaire called ELLI (Effective Lifelong Learning  Inventory) whose internal structure was factor analysed, and  validated through loading against seven dimensions [28]. As  detailed later, these dimensions have been since validated with  diverse learner groups, ranging in age from primary school to  adults, demographically from violent young offenders and  disaffected teenagers, to high achieving pupils and professionals,  and culturally from middle-class Western society to Indigenous  communities in Australia. The term learning power has been used  to describe the personal qualities associated with the seven  dimensions, particularly by Claxton [29, 30], although in this  paper, its meaning is specifically related to the ELLI inventory.    The inventory is a self-report web questionnaire comprising 72  items in the schools version and 75 in the adult version. It  measures what learners say about themselves in a particular  domain, at a particular point in time. A brief description of the  seven dimensions is set out below, with three examples from the  questionnaire shown for each dimension:   Changing & learning: Effective learners know that learning itself  is learnable.  They believe that, through effort, their minds can get  bigger and stronger, just as their bodies can and they have energy  to learn (cf. [22]).  The opposite pole of changing and learning is  being stuck and static.    I expect to go on learning for a long time.   I like to be able to improve the way I do things.  Im continually improving as a learner.   Critical curiosity: Effective learners have energy and a desire to  find things out.  They like to get below the surface of things and  try to find out what is going on. The opposite pole of critical  curiosity is passivity.   I dont like to accept an answer till I have worked it out for   myself.   I like to question the things I am learning.   Getting to the bottom of things is more important to me than  getting a good mark.   Meaning Making: Effective learners are on the lookout for links  between what they are learning and what they already know.   They like to learn about what matters to them. The contrast pole  of meaning making is data accumulation.   I like to learn about things that really matter to me.   I like it when I can make connections between new things I am   learning and things I already know.   I like learning new things when I can see how they make sense  for me in my life.   Dependence and Fragility: Dependent and fragile learners more  easily go to pieces when they get stuck or make mistakes.  They  are risk averse.  Their ability to persevere is less, and they are  likely to seek and prefer less challenging situations. The opposite  pole of dependence and fragility is resilience.    When I have trouble learning something, I tend to get upset.   When I have to struggle to learn something, I think its   probably because Im not very bright.  When Im stuck I dont usually know what to do about it.   Creativity: Effective learners are able to look at things in  different ways and to imagine new possibilities.   They are more  receptive to hunches and inklings that bubble up into their minds,  and make more use of imagination, visual imagery and pictures  and diagrams in their learning.  The opposite pole of creativity is  being rule bound.   I get my best ideas when I just let my mind float free.   If I wait quietly, good ideas sometimes just come to me.  I like to try out new learning in different ways.   Learning Relationships: Effective learners are good at managing  the balance between being sociable and being private in their  learning.  They are not completely independent, nor are they  dependent; rather they work interdependently.  The opposite pole  of learning relationships is isolation and dependence.   I like working on problems with other people.   I prefer to solve problems on my own.   There is at least one person in my community who is an  important guide for me in my learning.   Strategic Awareness: More effective learners know more about  their own learning.  They are interested in becoming more  knowledgeable and more aware of themselves as learners.  They  like trying out different approaches to learning to see what  happens.  They are more reflective and better at self-evaluation.  The opposite pole of strategic awareness is being robotic.    If I get stuck with a learning task I can usually think of   something to do to get round the problem.   If I do get upset when Im learning, Im quite good at making   myself feel better.   I often change the way I do things as a result of what I have   learned.   7. LEARNING WAREHOUSE PLATFORM   Without a learning analytics platform, it is impossible to gather  ELLI data globally, with quality and access controls in place, and  generate analytics fast enough to impact practice in a timely  manner. ELLI is hosted (with other several other research- validated tools) within a learning analytics infrastructure called  the Learning Warehouse. A mature analytics infrastructure needs  not only to gather and analyse data, but orchestrate the tools  offered to different stakeholders, and manage data access  permissions in an ethical manner. Learners, trainers/educators,  researchers, and organisational administrators and leaders are  provided with customised organisational portals onto the Learning  Warehouse which offers them different tools and levels of  permission to datasets as follows: learners sign in to complete the  right version of the ELLI questionnaire (e.g. Child or Adult) and  receive their personal ELLI visual analytic (detailed in next   95    section); administrators can upload additional learner metadata or  datasets; educators/organisational leaders access individual and  cohort analytics, scaling to the organisation as a whole if required,  in the form of visualised descriptive statistics. Authorised  researchers can see all of the above, together with other datasets  depending on the bases on which they were gathered. The portals  also house learner identity metadata, held separately from the  survey data in the Learning Warehouse, and destroyed after use.  Learning Warehouse uses the JSR-268 portlet standard1 enabling  ELLI profiles to be written and read via external platforms. 2    The researcher interface is provided for querying within and  across the anonymised datasets (at time of writing >40,000 cases).  Where a data owner requires analysis involving identifiable data,  the researchers are given permission to access this from the user  portal, held on a different server, for the purposes of the specific  project. The researcher interface enables access to aggregated,  anonymised datasets over learner cohorts and across tools for  researchers, with appropriate permissions within strict ethical  guidelines. Researchers are then able to undertake system-wide  research on a range of cases, across jurisdictions, instruments and  domains, and can curate the data generated from it and make it  available for secondary data analysis. Raw data may be  downloaded for analysis in Excel and SPSS, with a unique  identifier enabling integration of datasets, and in some cases  matching with nationally procured datasets.   Up to this point, the use of data has fallen within the traditional  social science domain in the way that it is used, as well as in the  pedagogical domain through providing immediate, visual  feedback for learners to appropriate and use in improving their  learning power.  The next step which we are now beginning to  explore is a more integrated researcher experience, which  incorporates tools more familiar to the learning analytics world,  for example by providing web-based visual analytics tools for  querying and interactively exploring data, drawing inspiration  from user experiences such as Google Analytics and Gapminder.3  A second development emerges from recent work with  collaborative, social applications, which are generating new kinds  of data streams at a finer granularity than a complete ELLI  questionnaire, and in real time rather than several months apart  (e.g. the start and end of a conventional educational research  project). We introduce this under future work.   8. ELLI VISUAL ANALYTICS  Visual analytics are helpful when it comes to comprehending and  discussing a 7-dimensional construct such as learning power. On  completion of an ELLI web survey, the Learning Warehouse  generates a spider diagram (Figure 2), providing a visualization  for the learner to reflect on (their own perception of) their learning  power. The scores produced are a percentage of the total possible  score for that dimension. The spider diagram graphically depicts  the pattern and relative strength of individual scores. Note that  unlike most spider diagrams, the axes are not numbered, but  labeled A little like me, Quite like me, and Very much like me. As  discussed shortly, a visual analytic such as this has a number of                                                                 1  http://jcp.org/aboutJava/communityprocess/final/jsr286   2 The Learning Warehouse is developed and owned by ViTaL   Partnerships, a charitable knowledge exchange partner to the  University of Bristols Centre for Systems Learning and  Leadership. ViTaL delivers training in the approaches and tools  described in this paper: www.vitalpartnerships.com    3  www.google.com/analytics / www.gapminder.org    important properties, which can be both empowering, but also  potentially demoralizing, and it is a principle behind the approach  that learners are not left to ponder its meaning alone. It is crucial  that the learner validates and thus owns the profile, a matter for  the coaching conversation that follows with a trained mentor.       Figure 2: An ELLI spider diagram generated from the   Learning Warehouse. The shaded blue region shows the initial   profile, while the outer red profile indicates stretch on  certain dimensions later in the learning project.      Data can be aggregated across groups of learners in order to  provide a mentor or teacher with a view of the collective profile  on all or specific learning power dimensions (Figure 3).             96    Figure 3: Visual analytics on aggregate ELLI data, for all  learning power dimensions, and a specific dimension.   The spider diagram has been further extended through the use of  visual imagery, creating a culturally relevant character to  represent each dimension. Examples (Figure 4) include the  Simpsons cartoon characters when working with disaffected  English teenagers [31], and iconic animals for Australian  indigenous young people [32].            Figure 4: Adding visual symbols to Learning Power   dimensions to localise them culturally. Top example also   shows the addition of metaphorical zones to the dimensions,   to create mental spaces for learners to inhabit.      9. VALIDATION  Thorough validation of a learning analytic is a multi-faceted  challenge. In this section we describe some of the facets relevant  to a dispositional analytic such as ELLI.   9.1 Construct validity of ELLI  When analysing self-report data there are several ways of  ascertaining reliability and validity. Sample size is important, with  larger numbers giving greater confidence in standard statistical  tests of reliability that explore how the instrument operates in  repeated tests. Construct validity refers to whether the tool  measures what it was designed to measure, for which there exist  well established criteria in the social sciences. The reliability and  validity of ELLI has been reported in several peer reviewed  educational publications [33-35].   9.2 Correlation with standardized attainment  Intuitively, one might hypothesise that learners who are curious,  resilient, creative and strategic (i.e. in the terms of this paper,  demonstrating learning power) should also record higher  attainment in traditional tests, because they have, for instance, a  much greater desire to learn, and ability to stretch themselves.   This argument is made strongly by proponents of learning-to-learn  who argue for such approaches to be woven into teaching practice  rather than being consigned to be taught as special topics (e.g.  [30]).    The evidence for this remains inconclusive, to date. Consistent  with this line of thought, one would predict ELLI to correlate  positively with conventional attainment analytics, and indeed,  several studies do report a positive correlation [33, 36, 37]. This is  an intriguing finding, but this relationship requires further  interrogation: it might also be argued that more developed  learning power should correlate negatively with higher test  scores. For instance, an analysis of reliability and validity  statistics for ELLI (N=10496) in 2008, replicated a 2004 finding  that the mean score on students learning power profiles gets  significantly lower as students get older [35]. This takes us back  to the earlier data reviewed on school disengagement: it points to  a widening disparity between the dispositions that reflect learners  taking skillful responsibility for their own learning in authentic  contexts, and the demands of curricula and associated assessment  regimes that focus on test results gathered under artificial  conditions.    9.3 Pedagogical validity of ELLI profiles  In information visualization, visual analytics are judged in terms  of the qualities of information design. We would argue that visual  learning analytics should go one step further: when they are  intended to empower learners we need to understand their  pedagogical affordances  the insight yielded for both educators,  and learners.    In school, workplace and Masters programmes, educators are  trained how to use individual and cohort ELLI profiles to shape  interventions and classroom practice, but space precludes a  detailed report on this. We focus here on the methodological  question of how does one validate the pedagogical affordances of  the ELLI profile for learners, where the objective is to catalyse  changes in dispositions towards learning Evidence of personal  change is gathered using a mixed-methods approach combining  quantitative pre- and post-test measures of learning power, plus  qualitative and narrative evidence from student interviews. This  has proven to be a powerful means of triangulating and validating  evidence of impact, and communicating the findings [31, 32, 36,  38-41].   In one 2007 study in a UK school [39], quantitative analysis  showed significant changes between pre- and post-measures  across a whole year group; qualitative evidence identified key  themes, and narrative evidence provided an insiders perspective  on the experience, such as the following statements from two 15  and16 year old students:   Its (about) understanding  because you can pass exams   without understanding..Its self growth and achievement.   Our personal experience is important.Learning to tell your   own story would make it easier to do all the other things you   have to do  learn subjects, get grades etc   When I was a childI was always much keener to do   something if I knew I would get a reward at the end of it..   the performance was important and not the process and   thats the way the education system works its very results   driven Its a bit of a trust thing. they dont trust you to do   it in your own way.its a trust thing It all ties together  its   about self awareness more than anything else .. self  awareness is not even touched upon in the education system    97    In another project with NEET learners (Not in Employment,  Education or Training), a 16 year old made significant changes  in his pre-post profile through a personalised enquiry project  supported by coaching and scaffolding using learning power [31].  In his final debrief for the project he said:    Its opened my eyes quite a bit to learn how to do these things,  and its changed what I think I can do.   Qualitative evidence from the English Learning Futures project  demonstrates that a rich language for describing learning is crucial  for deep engagement, as well as an authenticity, agency and  identity in learning. In one particular example [42] the ELLI  dimensions were identified by a 12 year old boy as the most  significant aspect of his learning because:   Its helped me get a long life Like I never used to know like   all this stuff, like strategic awareness, or critical curiosity, I   never knew it existed, like changing learning resilience. And   as soon as I got it all into my head, Ive never ever gave up on  stuff I need to reach my goal.   These examples, drawn from a series of research studies over  eight years, show something of the quality of impact on  engagement in learning which this visual analytic can have.    We are developing a theoretical account of how a visual analytic  for dispositions, such as the ELLI profile, operates. The profile  provides the framework for a coaching conversation which moves  between the self-perception and identity of the learner (Is this like  me) and a projected learning outcome (Where do I need to get  to) [35, 43]. Reflection on the spider diagram is thus a starting  point, moving from self-diagnosis to strategy, and forms the basis  of authentic enquiry projects that lead to different types of  performance outcomes. The seven dimensions of learning power  also support personal knowledge construction  for example  critical curiosity is a foundation for generating questions, or  meaning making is a necessary part of knowledge mapping, both  of which are primary forms of knowledge generation [32, 44]    Perhaps one of the most powerful aspects of this feedback arises  from its position at the interface of identity, purpose and  performance. In other words, referring back to Figure 1, the visual  feedback facilitates reflection backwards to the Self of the  learner (the stories that constitute who they are and who they want  to become), whilst at the same time providing a scaffolding  forwards towards a personally chosen outcome. Do I tend to be  a curious person is a question of identity, whilst I use my  curiosity to negotiate my way through a learning task, to an   authentic performance is about scaffolding knowledge  construction for a particular form of publicly validated outcome.   What has emerged from the research programme which has  accompanied the model of change we describe here, with the  Learning Warehouse at its heart, is substantive evidence that  visual literacy is a crucial part of personal and social development  and that profound personal change can be achieved, and  described, by pedagogical practices (for example coaching for  learning) which support such reflection and action [32, 37, 42, 45- 47].   Let us illustrate this with two examples. An Indigenous  Community in New South Wales chose the emu, an animal in  sacred stories, as a symbol for the dimension of Critical Curiosity.  What we theorise to be happening is that this visual image de- centres the teacher, the learner projects onto the emu the qualities  of critical curiosity, which they then emulate  copy or behave  like the emu  and so begin to experience curiosity as well as  being able to describe and deploy it. Furthermore in this particular   case study, the young people involved who chose this and six  other animals to represent the seven learning power dimensions  composed a story, ratified by the community elders, of how the  seven animals collaborated to escape from a zoo. This illustrates  how symbolism and narrative literacy catalysed by visual  analytics have the power to connect learning with communities at  a different level (emotional, socio-political) from traditional,  didactic teaching [48, 49].    In another Australian case study, a teacher in a remote school in  the Northern Territory of Australia copied his spider diagram onto  a whiteboard, enriched the spider legs with original art work of  traditional sacred animals chosen to represent that communitys  understanding of learning power, and is now using this to model  learning and change to his community.4   9.4 Cross-dataset validation  Thus far, we have described the instant visual analytics that the  Learning Warehouse generates for learners and educators, and we  have explored the pedagogical affordances of the ELLI spider  diagram. However, the researcher interface onto Learning  Warehouse enables deeper analyses to investigate the relationship  of learning power to other datasets. This is another form of  analytics validation: do we find statistically, pedagogically and  theoretically significant patterns when a given analytic is  combined with others    We discussed earlier a cross sectional analysis demonstrating  that the mean score on students learning power profiles gets  significantly lower as students get older [35].     We have reported positive associations between teachers  learner-centred practices, their beliefs about learning,  students learning power, the level of organisational  emotional literacy, and attainment by National Curriculum  measures [50].     We have demonstrated distinctive patterns of learning power  profiles for under-achieving students, and enhanced the  findings with school based, qualitative measures and teacher  professional judgment [36].     A two year study in 2009-11 with fifteen schools  participating in the national Learning Futures5 programme,  enabled an evaluation of the impact of learning power and  enquiry-based learning pedagogies on student engagement  [37, 47].     In 2011 a further analysis enabled the testing of ELLI with  an adult sample (N=5762) and an analysis of the differences  between age ranges in terms of the internal construction of  learning power [51].   Each of these studies discusses the ways in which these results  might be interpreted; the larger point is that when the ELLI  analytic is combined with other datasets, statistically robust  findings emerge which illuminate pedagogical and theoretical  debate. This would not be expected from an analytic lacking the  other forms of validity discussed.   10. CLOSING THE RESEARCH- PRACTICE GAP   Longitudinal and cross-dataset analytics such as the above have  been developed as traditional social science methodologies, but  from a learning analytics platform perspective, the point is that                                                                 4  slideshare.net/ruthdeakincrick/gappawiak-elli-presentation   5  Learning Futures programme: www.learningfutures.org    98    such analyses are greatly assisted when learner identities and  metadata are curated in an analytics platform, and moreover,  enable more timely feedback and interventions. In this section, we  describe kinds of services that the Learning Warehouse is being  used to deliver.   The Learning Warehouse provides portals to organisations  including remote Australian communities, schools in China,  Malaysia, Germany, Italy, US, and corporates in the UK. The  researcher interface for the Learning Warehouse is being used in  four major ways to deliver learning analytics to different  stakeholders:    i. The provision of bespoke organisational analyses as a service  (e.g. to schools and corporates) to inform leadership for  organisational change. Such information may include the  learning characteristics of particular groups, recent examples  include a gender cohort in a school; a marketing department in  a bank; underachieving students; or measures of change over  time in a school.    ii. Analysis of data for particular research projects across a  cohort of organisations (for example the impact of Learning  Futures pedagogies on student engagement in learning [37,  47]    iii. Secondary analysis of large-scale datasets for research  purposes (see Section 9.2).   iv. Collaborative research data analysis service for researchers  around the world who wish to use the instruments in their  research projects or to avail themselves of secondary datasets.   These forms of analysis are traditional in terms of social science  research, but there is typically a two-year gap between data  collection and feedback in such projects. By automating the  capture of learning power data via ELLI, and then aggregating  ELLI with other datasets around unique learner identities, the  Learning Warehouse accelerates this into immediate feedback in  some cases (e.g. cohort aggregate data graphs), or weeks for more  complex analysis requiring human intervention. Thus, learning  analytics platforms offer new possibilities for the educational  research community to demonstrate societal impact, by making  possible more rapid, iterative models of engagement from  research outputs to impact in practice, and back again. An  international community of researchers, practitioners,  policymakers and social entrepreneurs has grown around the  shared platform provided by the Learning Warehouse, in order to  build an evidence base using research validated tools, sharing data  within an appropriate ethical framework.    11. CONTRIBUTIONS  We have described the central role that learning dispositions and  transferable skills play in the future learning landscape, and have  argued that this has relevance for learning analytics. We propose  that the research programme described makes the following  specific contributions to learning analytics research and  development:   1. It is possible to model learning dispositions and transferable  skills. The 7-dimensional construct of Learning Power,  embodied in the validated design of the ELLI survey  instrument, provides a computationally tractable  representation of what has, until now, remained an elusive  set of personal qualities from a formal modelling (and hence  analytics) perspective.   2. The spider diagram visual analytic has proven value for  learners, mentors and organisations. When visualized, the 7-  dimensional profile enables individuals to reflect on their  learning self and to take responsibility for their own  learning, while enabling teachers to assess the learning  profile characteristics of groups and individuals. It provides  three elements which have been shown to be transformative  in some cases: (i) a common language for reflecting on ones  learning dispositions and transferable skills (in this paper we  have not had time to describe the programmes of work  embedding this language in educational practices); (ii) a  visual analytic which through a mentored conversation  provokes reflection on ones relative strengths and  weaknesses, and interventions to try; and (iii) through the  addition of culturally relevant visual imagery, we have  shown that this can provide memorable symbols which  motivates young learners, helping them to connect their  learning with their sense of identity and their everyday lives.    3. The Learning Warehouse exemplifies a collaborative  learning analytics platform to acquire, share and analyse   datasets. We have given examples of the kinds of analytics  afforded by an ELLI dataset drawn from diverse learner  populations over years, in combination with other data  connected to those learners. Aggregated cohort data informs  academic analytics from an institutional perspective.   12. FUTURE WORK  ELLIs design rationale from the start was not the creation of a  tool for academic analysis (although as described, it has grown to  support this). The goal is to provide educators with a practical tool  to enable rapid assessment and intervention of a complex quality,  to stimulate change in learners. It is this pragmatic driver that now  motivates a research programme to ground new forms of online  social learning platforms, through end-user tools and underlying  analytics, in Learning Power. The long-term goal is to see learners  operating in different blended learning configurations  (offline/online, synchronous/asynchronous, classroom/mobile,  etc.) supported by an underlying infrastructure utilizing static  datasets and live data streams to maintain learning power profiles,  and recommend possible resources and actions. At all times,  however, learners are asked to take increasing responsibility for  their own learning process, rather than surrendering control.    Reflective learning blog. A suite of Wordpress plugins has  been developed for EnquiryBlogger, whose piloting has  been positively received [52]. This generates visual  analytics reflecting the development of ones learning  power.    Online ELLI mentoring. A prototype synchronous tool  called ELLIment is in development, for online reviewing of  ELLI profiles and recording interventions [53].    Learning Power-based user classification. We are  investigating the application of discourse and social  analytics as ways to classify online learners and activity  against ELLI dimensions.    Learning Power-based recommender services. This line of  work represents one of the most challenging, but exciting  possibilities. ELLI profiles provide a new way for a  recommendation engine to connect learners to each other,  and to educational resources, based on similar or  complementary strengths on different dimensions. ELLI  coaches might benefit by finding peers who designed  interventions for learners with a given profile and history  (there are many reasons why a profile may be as it is, so  great care needs to be taken in efforts to automate this).   99    While trained ELLI mentors are an expensive resource, one  of the most ambitious future directions is to study, codify  and formally model the intervention strategies that mentors  use when presented with a given profile and a learners  history, and investigate if they can inform the design of  automated recommender systems. The challenge of  modelling sufficiently rich user contextual metadata to do  this is significant, but one worth exploring given the huge  amounts of data becoming available. It is possible, however,  that this cannot be done with great sophistication, just as  some might doubt that an artificial agent could replace any  but the most rudimentary skills displayed by a good coach  or counsellor.     The internal structure of Learning Power. As the range of  instruments within the Learning Warehouse increases, the  possibilities for exploration of relationships and theoretical  testing of hypotheses also increase. The application of  Structural Equation Modelling (SEM) path analysis, for  example, has further illuminated the internal structure of the  dimensions in the Adult Learning Power model, not only  confirming its validity and reliability, but extending our  understanding of how dimensions influence each other, with  implications for how we improve practice. For example,  SEM appears to confirm that dispositions for learning are a  gatekeeper construct, on which others depend (e.g.  interest, affect and involvement in learning). As the South  Australian Department for Education and Childrens  Services collects data through its ongoing Teaching for  Effective Learning project, a combination of data from ELLI  and TfEL will enable new analyses.     Organisational learning applications in the workplace. A  line of work now developing uses ELLI to facilitate  organisational self-reflection and learning, with the goal of  aligning individual and organisational identity and purpose  [51].   In order to advance this research programme, a network has been  established [LearningEmergence.net] connecting researchers and  practitioners whose interests lie at the intersection of deep  learning, complex systems, transformative leadership and  knowledge media. We very much welcome the engagement of  researchers, practitioners, policymakers and technologists who  share our interest in the particular role that analytics can play in  building robust learning dispositions and transferable skills.   13. ACKNOWLEDGEMENTS  The research reported is the fruit of many colleagues  collaborative efforts, as cited. From our most recent work, we  gratefully acknowledge the support of the Paul Hamlyn  Foundation in the national Learning Futures programme.   14. REFERENCES  [1] Bowker, G. C. and Star, L. S. Sorting Things Out:   Classification and Its Consequences. MIT Press,  Cambridge, MA, 1999.   [2] Pirolli, P. and Russell, D. M. Special Issue on Sensemaking.  Human-Computer Interaction, 26, 1, (2011), 1-259.   [3] Lovett, M., Meyer, O. and Thille, C. The Open Learning  Initiative: Measuring the effectiveness of the OLI statistics  course in accelerating student learning. Journal of  Interactive Media in Education, 14, (2008).   [4] Hsiao, I. H., Sosnovsky, S. and Brusilovsky, P. Guiding  students to the right questions: adaptive navigation support   in an E-Learning system for Java programming. Journal of  Computer Assisted Learning, 26, 4, (2010), 270-283.   [5] Romero, C., Ventura, S., Pechenizkiy, M. and Baker, R. S.  J. d. Handbook of Educational Data Mining. CRC Press.  2010.   [6] Haste, H. Ambiguity, autonomy and agency. OECD,  Hogreffe and Huber. 2001.   [7] Jaros, M. and Deakin Crick, R. Personalised Learning in the  Post Mechanical Age. Journal of Curriculum Studies, 39, 4,  (2007), 423-440.   [8] Hoskins, B. and Deakin Crick, R. Competencies for  Learning to Learn and Active Citizenship: different   currencies or the same coin Joint Reserach Centre  European Commission, Ispra, 2008.   [9] Bauman, Z. The Individualized Society. Polity Cambridge  Press., Cambridge, 2001.   [10] Thomas, D., Brown, J.S. Learning for a World of Constant  Change: Homo Sapiens, Homo Faber & Homo Ludens  Revisited. In Proceedings of the University Research for  Innovation: Proc. 7th Glion Colloquium (Montreux,  Switzerland, 2009). Economica.   [11] Dewey, J. How We Think: A Restatement of the Relation of  Reflective Thinking to the Educative Process. Heath and Co,  Boston, 1933.   [12] Gilby, N., Hamlyn, B., Hanson, T., Romanou, E., Mackey,  T., Clark, J., Trikka, N. and Harrison, M. National Survey of  Parents and Children: Family Life, Aspirations and   Engagement with Learning in 2008. UK Dept. Children,  Schools & Families, London, 2008.    [13] Willms, J. D., Friesen, S. and Milton, P. What did you do in  school today Transforming classrooms through social,   academic, and intellectual engagement. First National  Report, Canadian Education Association, Toronto, 2009.    [14] Yazzie-Mintz, E. Charting the Path from Engagement to  Achievement: A Report on the 2009 High School Survey of   Student Engagement. Center for Evaluation & Education  Policy, Indiana University, 2009.    [15] Deakin Crick, R. Pedagogical challenges for  personalisation: integrating the personal with the public  through contextdriven enquiry. Curriculum Journal, 20, 3,   (2009), 185-306.   [16] Gardner, H. Frames of Mind: The Theory of Multiple  Intelligences. Basic Books, New York, 1983.   [17] Perkins, D., Jay, E. & Tishman S. Beyond Abilities: A  Dispositional Theory of Thinking,. Merrill-Palmer  Quarterly, 39, 1, (1993), 1-21.   [18] Claxton, G. Wise Up: Learning to Live the Learning Life.  Network Educational Press, Stafford, 2001.   [19] Katz, L. Dispositions in Early Childhood Education.  ERIC/EECE Bulletin, 18, 2, (1985), 1-3.   [20] Ames, C. The enhancement of student motivation. JAI Press.  1987.   [21] Ames, C. Developing a Learning Orientation. 1990.   [22] Dweck, C. Self Theories: Their Role in Motivation,  Personality and Development. Philadelphia, PA.,  Psychology Press,, 1999.   100    [23] Nicholls, J. Achievement Motivation: Conceptions of  Ability Subjective Experience, Task, Choice and  Performance. Psychological Review, 91, (1984), 328-346.   [24] Nicholls, J. and Miller, A. The differentiation of the  concepts of difficulty and ability. Child Development, 54,  (1983), 961-969.   [25] Perkins, D., Jay, E. & Tishman S. New Conceptions of  Thinking: From Ontology to Education,. Educational  Psychology,, 28, 1, (1993), 67-55.   [26] Ritchhart, R. and Perkins, D. Life in the Mindful Classroom:  Nurturing the Disposition of Mindfulness. Journal of Social  Issues, 56, 1, (2000), 27-47.   [27] Tishman, S., Jay, E. and Perkins, D. Teaching Thinking  Dispositions: From Transmission to Enculturation,. Theory  into Practice, 32, (1993), 147-153.   [28] Deakin Crick, R., Broadfoot, P. and Claxton, G. Developing  an Effective Lifelong Learning Inventory: The ELLI  Project. Assessment in Education, 11, 3, (2004), 248-272.   [29] Claxton, G. Hare Brain, Tortoise Mind: Why Intelligence  Increases when You Think Less,. Fourth Estate, London,  1997.   [30] Claxton, G. Wise Up: The Challenge of Lifelong Learning,.  Bloomsbury, London, 1999.   [31] Millner, N., Small, T. and Deakin Crick, R. Learning by  Accident. Report No. 1, ViTaL Development and Research  Programme, University of Bristol, 2006.    [32] Deakin Crick, R. and Grushka, K. Signs, Symbols and  Metaphor: linking self with text in inquiry based learning.  Curriculum Journal, 21, 1, (2010).   [33] Deakin Crick, R. Learning how to learn: the dynamic  assessment of learning power. The Curriculum Journal, 18,  2, (2007), 135-153.   [34] Deakin Crick, R., Broadfoot, P. and Claxton, G. Developing  an Effective Lifelong Learning Inventory: The ELLI Project.  Lifelong Learning Foundation, Bristol, 2002.    [35] Deakin Crick, R. and Yu, G. The Effective Lifelong  Learning Inventory (ELLI): is it valid and reliable as a  measurement tool Education Research, 50, 4, (2008), 387 402.   [36] Ren, K. Could Do Better! So Why Not An Exploration of  the Learning Dispositions of Underachieving Students.  Doctoral Dissertation, Graduate School of Education,  University of Bristol, Bristol, UK, 2010.   [37] Deakin Crick, R., Jelfs, H., Ren, K. and Symonds, J.  Learning Futures. Paul Hamlyn Foundation, London, 2010.    [38] Small, T. Learning Outside the Box. Report No. 3, ViTaL  Development and Research Programme, University of  Bristol, 2007.    [39] Small, T. The Learning Agents! Report No. 4, ViTaL  Development and Research Programme, University of  Bristol, 2007.    [40] Small, T. and Burn, M. The Learning Engineers: Bridging  Values and Learning Report No. 2, ViTaL Development  and Research Programme, University of Bristol, 2006.    [41] Small, T. and Deakin Crick, R. Learning and Self- Awareness: An Enquiry into Personal Development in   Higher Education. ViTaL Development and Research  Programme, University of Bristol, 2008.    [42] Deakin Crick, R. and Bond, T. 'It's like a Gift: How to get a  long life easier': Narratives of Personalisation. Cambridge  Journal of Education, submitted, (2012).   [43] Deakin Crick, R. Inquiry-based learning: reconciling the  personal with the public in a democratic and archaeological  pedagogy. Curriculum Journal, 20, 1, (2009), 73  92.   [44] Deakin Crick, R. Inquiry-based learning: reconciling the  personal with the public in a democratic and archaeological  pedagogy. Curriculum Journal, 20, 1, (2009), 73 - 92.   [45] Grushka, K. Meaning and identities: a visual performative  pedagogy for socio-cultural learning. Curriculum Journal,  20, 3, (2009), 237 - 251.   [46] Deakin Crick, R., Grushka, K., Heitmeyer, D. and  Nicholson, M. Learning, Place and Identity: An exploration  of the affordances of a pedagogy of place amongst   Indignous Australian students. University of Bristol, Bristol,  2010.    [47] R., D. C., Jelfs, H., Huang, S. and Wang, Q. Learning  Futures Final Report. Paul Hamlyn Foundation, London,  2012.    [48] Goodson, I. and Deakin Crick, R. Curriculum as narration:  tales from the children of the colonised. Curriculum  Journal, 20, 3, (2009), 225-236.   [49] Students, W. I. Taronga Zoo Break Out. Singleton High  School Ka Wul Centre, Singleton, 2010.    [50] Deakin Crick, R., McCombs, B. and Haddon, A. The  Ecology of Learning: Factors Contributing to Learner  Centred Classroom Cultures. Research Papers in Education,  22, 3, (2007), 267-307.   [51] Deakin Crick, R., Haigney, D., Huang, S., Coburn, T. and  Goldspink, C. Learning Power in the Work Place: The  Effective Lifelong Learning Inventory (ELLI) and its  Reliability, Validity and Implications for Learning and  Development. Submitted to: International Journal of  Human Resource Management, (2012).   [52] Ferguson, R., Buckingham Shum, S. and Deakin Crick, R.  EnquiryBlogger  Using widgets to Support Awareness and  Reflection in a PLE Setting. In Proceedings of the  Workshop on Awareness and Reflection in PLEs, Personal   Learning Environments Conference (Southampton, 11 July,  2011).   [53] Ullmann, T. D., Ferguson, R., Buckingham Shum, S. and  Deakin Crick, R. Designing an Online Mentoring System  for Self- Awareness and Reflection on Lifelong Learning  Skills. In Proceedings of the Workshop on Awareness and  Reflection in PLEs, Personal Learning Environments  Conference (Southampton, 11 July, 2011).         101      "}
{"index":{"_id":"19"}}
{"datatype":"inproceedings","key":"McAuley:2012:ERO:2330601.2330630","author":"McAuley, John and O'Connor, Alexander and Lewis, Dave","title":"Exploring Reflection in Online Communities","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"102--110","numpages":"9","url":"http://doi.acm.org/10.1145/2330601.2330630","doi":"10.1145/2330601.2330630","acmid":"2330630","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics, collaboration, commons-based peer production, online communities, social network visualisation, visualisation","Abstract":"Commons-based Peer Production is the process by which internet communities create media and software artefacts. Learning is integral to the success of these communities as it encourages contribution on an individual level, helps to build and sustain commitment on a group level and provides a means for adaption at an organisational level. While some communities have established ways to support organisational learning -- through a forum or thread reserved for community discussion -- few have investigated how more in-depth visual and analytic interfaces could help formalise this process. In this paper, we explore how social network visualisation can be used to encourage reflection and thus support organisational learning in online communities. We make the following contributions: First, we describe Commons-Based Peer Production, in terms of a socio-technical learning system that includes individual, group and organisational learning. Second, we present a novel visualisation environment that embeds social network visualisation in an asynchronous collaborative architecture. Third, we present results from an evaluation and discuss the potential for visualisation to support the process of organisational reflection in online communities.","pdf":"Exploring reflection in online communities     John McAuley  KDEG   Trinity College Dublin  Ireland   john.mcauley@scss.tcd.ie   Dr. Alexander OConnor  KDEG   Trinity College Dublin  Ireland   alex.oconnor@scss.tcd.ie   Dr. Dave Lewis  KDEG   Trinity College Dublin  Ireland   dave.lewis@scss.tcd.ie     ABSTRACT  Commons-based Peer Production is the process by which internet  communities create media and software artefacts.  Learning is  integral to the success of these communities as it encourages  contribution on an individual level, helps to build and sustain  commitment on a group level and provides a means for adaption  at an organisational level. While some communities have  established ways to support organisational learning  through a  forum or thread reserved for community discussion  few have  investigated how more in-depth visual and analytic interfaces  could help formalise this process. In this paper, we explore how  social network visualisation can be used to encourage reflection  and thus support organisational learning in online communities.  We make the following contributions: First, we describe  Commons-Based Peer Production, in terms of a socio-technical  learning system that includes individual, group and organisational  learning. Second, we present a novel visualisation environment  that embeds social network visualisation in an asynchronous  collaborative architecture. Third, we present results from an  evaluation and discuss the potential for visualisation to support  the process of organisational reflection in online communities.    Categories and Subject Descriptors  H.1.2 [User/Machine Systems]: Human factors   General Terms  Management, Performance, Design, Human Factors, Theory.   Keywords  Commons-Based Peer Production, Visualisation, Online  Communities, Social Network Visualisation, Analytics,  Collaboration.   1. INTRODUCTION  Commons-Based Peer Production (CBPP) is the process by which  internet communities create and maintain digital public goods [2].   Generally, these systems work in what would appear to be a very  chaotic manner.  There is no fixed or coordinated schedule for  contributors to a peer production project and users tend to  contribute when and how they want.  At the same time, there is no   fixed management structure, and users are generally promoted in  the community by virtue of their contribution.  Often there is an  area of the communitys space reserved for users to participate in  discussions that relate to the communitys operation, such as the  Village Pump in Wikipedia1 or Meta in the Stack Overflow2. This  is, in essence, how the community learns as an organisation   users put forward their individual experiences that are then  discussed with the boarder community.  This can result in the  implementation of new community policy as the community  begins to evolve and adapt to how it once currently existed.   While analytics are increasingly being explored as reflective  mechanisms for leaning in a variety of contexts, there has been  little research conducted on the use of analytic interfaces to  support the organisational aspects of online communities.  We  believe that visual analytics can be used to support organisational  learning in peer-based communities by not only providing a space  for reflection but also promoting discussion and debate in the  broader community. To asses this claim, we have developed a  novel visual analytics tool, called Explore.su, which we have  embedded in a collaborative architecture. This work builds on  previous research into both social visualisation, which provoked  reflection and story-telling in groups and communities [7], and  social data analysis, which opened collaborative data analysis to a  non-expert community [14].   We divide this paper into two sections.  In the first section we  consider CBPP systems in terms of socio-technical learning  systems, which supports three interrelated levels of learning:  individual, group and organisational. We argue that each level of  learning contributes to the system as a whole and then concentrate  on how organisational learning is currently addressed in online  communities.  In concluding this section, we emphasise how the  process of reflection is considered essential to organisational  learning and argue that visualisation and the application of visual  analytics can be used to support organisational learning in peer- based systems. In the second section, we introduce Explore.su, a  collaborative visual analytics environment, which we developed  to explore the process of reflection in a large Q&A community.  We describe the design, implementation and evaluation of  Explore.su and conclude with a discussion on reflection and the  implementation of visual analytics in online communities.   2. LEARNING & PEER PRODUCTION  Commons-Based Peer Production (CBPP) is an approach to the  creation and maintenance of digital public goods3 that harnesses                                                                     1 http://en.wikipedia.org/wiki/Wikipedia:Village_pump  2 http://meta.stackoverflow.com/  3 Public goods are non-excludable (available to everyone) and   non-rival (do not deplete with use) goods.      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.      102    the capability of communities organised, principally, without  traditional management hierarchies. Communities engaged in  CBPP have complex socio-technical infrastructures, which are  predicated on cooperation, yet are subject to continuous change.  We conceive of CBPP communities as socio-technical learning  systems in which learning is undertaken at a number of  interrelated levels: individual, group and organisational.     2.1 Individual Learning  Individual learning is one of the key motivating factors to the  success of Open Source Software (OSS) [25]. Developers join  OSS communities to learn, hone and improve their skills.  In these  communities, there is a much stronger emphasis placed on the  social recognition of human-capital and on the autonomy of the  individual. OSS developers are free to choose projects they  consider cool, in vogue, or they feel could benefit from their  contribution. The result is a more concerted effort on behalf of the  individual both as a participant in the project and as an individual  learner. The sort of learning can be generalised to other peer- production systems. In technical Q&A communities, for example,  users can learn a great deal about a subject domain in a more  interactive and participative manner.        2.2 Group Learning  When individuals come together the dynamics of learning is  transformed as those individuals begin to work towards a shared  consensus [3].  Peer production provides ample opportunity to  work in groups, however, the nature of the collaboration will  impact upon the approach to learning. Haythornthwaite, for  instance, considers peer production as either lightweight or  heavyweight, depending on the degree of freedom or ambiguity  that exists around an individuals role in the system [11]. Both  lightweight and heavyweight models of peer production provide  opportunities for learning; however, this occurs in the form of a  learning collective or community of practice.    2.2.1 Learning Collectives  Thomas and Brown [6] argue that in communities, individuals  learn to belong while in collectives individual learn to participate.   While subtle in some regards, this shift recognises the importance  of participation without over-emphasising the nature or indeed  level of participation. Thus, learning can involve much less  intensive forms of action such as rating or commenting on web  content.   2.2.2 Communities of Practice  One of the most widely recognised (and highly cited) models of  group learning is the community of practice model originally  proposed by Wenger [26]. Communities of practice are founded  on the recognition that learning is a continuous process and  should not be separated out from the everyday activity of the  learner. Rather, effective learning occurs when individuals are  engaged in meaningful practices as part of the communities they  value. Communities of practice require more intensive and often  situated collaborations, than those found in learning collectives,  and can often develop in many peer-based communities.    2.3 Organisational Learning  There is a broad academic and practitioner literature on  organisational learning and a variety of perspectives have  emerged in regards to the use of the term. In this section, we will  briefly and rather broadly, address some of the core tents of  organisational learning and then focus on how online   communities, and in particular peer production communities,  typically learn to adapt and change.   Schon and Argyris published one of the first theoretical accounts  of organisational learning, in which they described  organisational learning as a method of detecting and correcting  errors at an organisational level [1].  Their account introduced  the notion of single loop and double loop learning and placed a  particular emphasis on the role that reflection plays in effective  organisational learning.  Other authors, such as Kim, reinforced  this view suggesting that organisational learning increases an  organisations capacity to take effective action [18].  Levitt and  March looked at the practice of organisational learning.  They  argued that experiential learning is prone to human interpretation  and thus there is a need to address organisational learning with  more objective analytical tools (such as statistical analysis for  example) [19]. Crossan et al.s 4I framework conceives of  organisational learning as a process of dynamic renewal that  occurs across three levels of an organisation: individual, group  and organisational [5]. Their framework articulates how  experience, intuited and interpreted at the individual level, is  shared and integrated at the group level and then institutionalised,  as new policies and procedures, at the organisational level.     Organisational learning has been also addressed in the literature  on online communities, however, rarely is the actual term  organisational learning used. The majority of this research has  been concerned with how online communities manage to adapt  and change, given that power is distributed amongst the  community members and that the technical and social elements of  the community are tightly integrated. Butler et al., for example,  looked at the policy mechanism in Wikipedia. They found that the   Wikipedia had grown substantially - from a relatively open  system to a much more sophisticated bureaucracy [4].  Jahnke  found  that a student online community evolved in a similar  fashion from an undefined and amorphous state to a defined and  rigid social structure [17]. Forte and Bruckman found that smaller  autonomous projects had emerged within Wikipedia, each with  their own set of operating norms and policy rules yet the basic  norms of behaviour were inherited from the original site  [8].  Schindler and Vrandecic describe how the introduction of new  technical features in the German Wikipedia was the result of  increasing pressure by the community [21].    Our aim is to investigate ways to support organisational learning  in online communities so that the policy mechanisms move  beyond an experiential based model, which is prone to the  problems of human interpretation, to one that is based on the  application of more objective analytic interfaces.  As a first step  in this direction, we have developed a novel visual analytics tool  that provides a community with an area for reflection in their  social space.  We believe that visual analytics cannot only be used  to passively inform organisational learning in online communities  but also provides a means for discussion and debate as regards to  the ongoing evolution of community processes.     3. EXPLORE.SU  In the rest of this paper, we present the work done on Explore.su   a visual analytics tool that was developed to explore reflection in  the Super User4 online community.                                                                     4 http://superuser.com/   103    3.1 Related Work  Before continuing, we will briefly review the related work on  social network visualisation and social data analysis.   3.1.1 Social Network Visualisation  There is a wide range of software that supports social network  visualisation. UCINET, Pajek and Gephi, for example, provide  both statistical analysis and graph visualisation.  While popular,  many of these packages are developed for professional analysts  and require substantial knowledge and expert interpretation  during use.  Researchers have, as a consequence, sought to  develop social network visualisation tools that support the less- expert end-user.        Figure 1 shows a node-link (graph-based) visualisation from   Vizster (Heer and boyd 2005).   Vizster, by Heer and boyd, is an example of a node-link  visualisation that enables end-users to explore their own social  network [13].  While provoking cycles of analysis, reflection and  story-telling, Vizster (as illustrated in Figure 1) was designed for  ego-centric networks, which are focused around a single user, and  the approach will not scale elegantly with larger networks, such as  the Super User online community.  Indeed, this is a broader  reflection of node-link (graph-based) visualisations in general [9].   Networks of over a 150 nodes are generally incomprehensible  when visualised using node-link diagrams. While algorithms such  as spring or force-directed layout help to organise the clutter of  nodes and links into more meaningful representations, when  conducted on large networks, the execution time is prohibitive.   Perer and Shneiderman argue that many professional network  analysts render the visualisation for publication only after having  conducted the analysis [20].  When wishing to visualise change in  a network, this problem is significantly exacerbated, as node-link  layout algorithms remain sluggish or inelegant when dealing with  the evolution of a network over time.     These difficulties have led researchers to investigate the use  adjacency matrices as an alternative approach to node-link  diagrams. Henry developed Matrix Explorer, a social network  analysis tool that visualises large-scale social networks at both  global and local levels [16].  Matrices are used for global  representations while the more familiar node-link visualisations  are used to show local relationships.  The visualisation aims to  shift towards the end-user (historian or sociologist), moving away  from a complex interface to a more user friendly visualisation  environment.  To handle scale, Henry implements block  modelling, a common clustering technique used in network  analysis, to order and represent the social network.  While  certainly a more compact approach, the result of block-modelling  can often appear arbitrary and difficult to interpret.  In an attempt  to deal with such constraints, Van Ham et al. developed  Honeycomb, a visualisation for large scale social networks [10].   Their approach, as illustrated in Figure 2, involves using   concrete organisational hierarchies to drive the analysis. This is  a more intuitive clustering approach that, unlike block-modelling,  reflects a users prior understanding of the social network.     Adjacency matrices can also handle the dynamics of a network in  a much more elegant fashion than typical node-link diagrams.  A  matrices representation remains static as each node is already  assigned a position in space. Thus, reflecting change is more  accurate, and the result is generally more intuitive. This can be  beneficial in two ways. First, change can be illustrated as a heat  map in which significant change is represented by the intensity of  colour (Figure 2). Second, the visualisation can accurately  represent network spread, changes in the network that result from  emergent patterns, such as population growth.  Matrices are also  able to portray areas of inactivity (or the absence of connections)  as well as those of intense activity [10].  Again, this is because  each node already holds a position in space that, unlike node-link  diagrams, can show the absence of information over time.         Figure 2 shows a snapshot of the Honeycomb visualisation   developed by Van Ham et al 2009.  Connections are organised   by country and then continent.  Blue indicates negative   change, red indicates positive change and grey indicates a   connection.  Colour intensity is used to illustrate the quantity   of change.   Other approaches, which are not strictly network visualisations  but have been used to visualise online communities, include  History flow [24], AuthorLines [23] and Communication-garden  [27].  While each helps to illustrate user activity in online  communities, and portray the characteristics of a community,  none of these methods convey community dynamics (or the  interaction between users) as successfully as node-link or matrix  visualisations.    3.1.2 Social Data Analysis  Social data analysis seeks to take advantage of social as well as  cognitive and perceptual process, during the visual analysis of  data.  Much of the work to date has sought to render visualisation  and collaboration tools more usable to communities of non- professional end-users.  By far the most popular example is Many  Eyes, a collaborative visualisation site developed by IBM [22].   Many Eyes has a complete suite of visualisations, from wordles to  graphs, and users are supported in uploading their own datasets or  exploring those uploaded by others.  However, the collaborative  functionality of Many Eyes remains relatively un-advanced, as  users are unable to embed annotations in the visualisations.   Heers Sense.us implements more advanced annotation  mechanisms, such as view sharing, doubled-linked discussions  and embedded annotations [15].  While illustrating that  visualisation, and analytic reasoning, can include social as well as  perceptual and cognitive processes, asynchronous collaboration  has yet to be applied to social network visualisation.   3.1.3 Summary  While, at present, node-link visualisations experience difficulty  with large and dynamic networks, other social visualisations do   104    not express the dynamics (or interactions between members) of an  online community effectively.  Matrices visualisations, on the  other hand, handle large and dynamic networks with reasonable  elegance, and can illustrate areas of little as well as much network  activity.  Although social data-analysis has been implemented  with varying degrees of success, there has been relatively no work  on implementing asynchronous collaboration mechanisms with  social network visualisation.     3.2 The design of Explore.su  We outlined several design goals that helped inform the  development of the system.      1. Visualise the social and temporal patterns of the entire  Super User community.  The social patterns describe  interactivity between members of the community and the  temporal patterns describe how these interactive patterns  change over time [7].   2. Visualise different user actions.  Members of Super User  can post questions and answers, vote, comment as well as  revise their own and other members posts.  Each of these  actions is considered as a separate communication-act and  thus each is addressed with a separate visualisation.   3. Provide annotation and collaboration tools to encourage  discussion and reflection   3.3 The Super User Online Community  The focus of the evaluation is the Super User online community,  an implementation of Commons-Based Peer Production, which  was introduced to address the domain of computer hardware and  software.  Super User is one of a series of community-driven  question and answer sites called the Stack Exchange Network,  which, with Stack Overflow, has risen to a position of prominence  over the last number of years. The socio-technical implementation  of Super User is of particular interest as it seeks to address many  of the perceived deficiencies with traditional question and answer  sites. First, Super User has both comments and posts. Posts (as  questions and answers) are considered first class entities, while  comments provide a way for users to seek clarification in regards  to a post or suggest an amendment to a post.  Posts can be  considered as analogous to Wikipedia article pages while  comments can be considered similar to an articles talk pages.  Second, each community is specific to a domain (such as  programming, gaming or cooking) and questions that are  considered off topic can be migrated from one community to  another.  Again, this approach is analogous to Wikipedias name  spaces.  Third, Super User has an explicit reputation reward, of  badges and reputation, which systematically encourages and  rewards behaviour.   Finally, Super User is collaboratively edited,  a feature again drawn from Wikipedia, and has implemented a  similar approach to the Wikipedia's Village Pump called a meta  site.    3.4 Visualisation  We choose to use adjacency matrices as a representation of  communitys network.  This was largely because matrices can  handle scale and evolution in a more elegant manner than node- link diagrams.  Furthermore, matrices illustrate the absence, as  well as the presence, of network activity.  Figure 3 illustrates the  user interface for Explore.su, Figure 4 shows revision patterns for  a 24 hours period and Figure 5 shows a popup, which lists all the  individual communication-acts for a glyph in the matrix, and is  displayed when a user clicks any glyph in the matrix.  This   provides access to both a global view (illustrated by the adjacency  matrix) and a local view of individual interactions.      Figure 3: The Explore.su interface comprises of the following   components:   A) Adjacency Matrix  visualises the communication-acts   (questions and answer patterns or commenting patterns or   revision patterns or patterns of accepted answers) for the   entire community over a given 24 hour period.         B) Adjustable Timeline (graph and slider)  allows the user to   traverse time, which in turn updates the matrix to a given 24   hour period.  Dragging the slider animates the adjacency   matrix, thus visualising temporal patterns as shifts in the   communitys social patterns.  The green line graph illustrates   variations in community activity.  In the above image, each   dip in green line graph illustrates a dip in activity at the   weekend.           C) User Actions, Graphical Annotation Tools and bookmark.    User Actions  enables the user to visualise different   communication-acts.  For example, a user can visualise   questions and answer patterns or commenting patterns   or revision patterns and or patterns of accepted   answers.    Graphical annotation tools  allows users to   graphically annotate the visualisation.  E.g. point,   draw or highlight.     Bookmark  allows the user to attach an annotated   visualisation to a comment.  This bookmark will retain   the state of the visualisation (date and annotations).    D) Threaded discussion  a set of threaded comments with   visualisation bookmarks that allow users to load a   visualisations state (including any contributed annotations)   into the adjacency matrix.   3.4.1 Social Patterns  The matrix visualises community activity for a given 24 hour  period.  The visualisation is updated nightly (12.01 GMT), giving  a relatively up-to-date representation of the communitys  communication-acts.  As is generally the case with social network  visualisations, the Explore.su visualisation is created from the  communitys reply-to graph.  So if user A replies to user B, a  connection is created between those two users.  However,  communication behaviour in online communities is often non- reciprocal. So, user A replying to user B creates an in-link   105    between those two users without the need for user B to reply-back  to user A.   Plotting the in-links between users creates an  asymmetrical visualisation, which not only highlights areas of  activity but also illustrate areas of inactivity (via the absence of  connections).        Figure 4 illustrates revision activity in the Super User online   community.  Colour intensity is used to illustrate the number   of connections between users of different reputation   categories. There are two noticeable patterns.  The first dark   diagonal line illustrates users revising their own posts, an   activity encouraged by the community.   The second clump of   activity on the left, lower section of the diagonal illustrates   users with a high reputation revising the posts of users with a   low reputation.  This is a convention of the community, as   rarely do new users revise other users posts.   Given the scale of the community (~50,000 users at the time of  writing), rendering a single one-to-one matrices was unfeasible.  We therefore adopted the approach proposed by van Ham et al.  and used a hierarchical overview to drive analysis.  We choose  reputation as a way to structure the visualisation as it is both  intuitive and meaningful to the community.  As discussed under  the heading Common-Based Peer Production, reputation is a  systematic mechanism for rewarding user contributions to the  community.  So users with a high reputation are generally trusted  by the community, while users with a lower reputation are  considered more recent members or are members who have yet to  contribute significantly (in terms of reward-able activities) to the  community.      The distribution of reputation can be represented with a log scale.   There are a large number of users (~26,655), for example, with a  reputation of below ten, while there are only a few users with a  reputation of over 50,000.  To reflect this distribution, we divided  reputation into 37 categories, starting with 0-10 and finishing with  90,000-100,000.  The first 10 categories are increased by 10, so 0- 10, 10-20, 20-30 etc.  The next ten categories are increased by  100, so 100-200, 200-300, 300-400 etc.  The next ten categories  are increased by 1000, so 10000-2000, 2000-3000, 3000-4000 and  finally the remaining categories are increased by 10,000, so  10,000-20,000, 30,000-40,000 etc.  The aim of this representation  is to visualise a large community yet, at the same time, reflect the   scale-free network topology familiar in large, open web-based  systems and provide an intelligible visualisation for end-users.   3.4.2 Temporal Patterns  We also aimed to visualise, and hence provide access to, the  communitys unfolding temporal patterns.  Temporal patterns  describe the naturally occurring rhythms of collaboration, which,  when visualised, can help to illustrate aspects of a communitys  evolution, such as sudden growth or network spread.  At the same  time, temporality is proven to play an important role in  collaboration activities, enabling groups to coordinate their  conception of time and improve efficiency and effectiveness.   While there are several approaches to visualising temporal  patterns in adjacency matrices, including thumbnail views for  example, we implemented an adjustable timeline (as a line graph  and slider in Figure 3), which enables users to adjust the date,  which in turn updates the adjacency matrix.  Dragging the slider  continually updates and thus animates the matrix, enabling the  user to quickly observe temporal shifts in the communitys social  patterns.       Figure 5: Clicking on any square on the adjacency matrix will   show a popup, which displays the communication-acts   represented by that square.  In this example, the popup   displays questions and answers from users with a reputation   of 100 to 200.  This approach provides both a global and local   representation of the communitys activity.   3.5 Collaboration Support  Collaboration support draws from Sense.us and from Heers  recommendations for social data analysis [12], providing simple  yet intuitive ways for users to gesture towards items of interest  and to share observations with other users of the system.   3.5.1 Graphical annotation tools  We implemented several graphical annotation tools to help users  communicate intent and contextualise an observation.  Pointing is  the most important tool in this context, providing users with a  way to gesture towards a feature of interest or simply say look  here.  Other forms of annotation, such as highlighting and  drawing, are also provided.   3.5.2 Bookmarking and view sharing  To analyse the visualisation, users must be able to see the same  view, providing context for observations and user actions. In  Explore.su, this is achieved with simple collaboration  functionality in the form of visualisation bookmarking,  commenting and view sharing.   3.5.3 Threaded discussions  Comments enable users to elaborate on their annotations, to  describe the observations in more detail and to share their  observations with other users.  Comments provide a platform for  discussion.   106    4. EVALUATION  We conducted a two staged evaluation, which involved a pilot lab  study followed by an exploratory user study with the Super User  community. The pilot study was conducted to ensure that the  visualisation was comprehensible and that the overall system  design was coherent.  The exploratory study use was run as a live  deployment over a two week period.       4.1 Pilot study  The pilot study involved 7 subjects (6 males and 1 female).  All  subjects are computer science PhD researchers from our faculty.   None of the subjects are online community researchers and their  specialities range from semantic technologies to ubicomp  systems.  In addition, none of the subjects were familiar with  Super User, however, 4 of the subjects had used Stack Overflow,  and all of the subjects were familiar with Yahoo Answers (an  alternative non-domain specific question/answer site).  The first 5  subjects were given a brief tutorial of the system, asked to play  around or familiarise themselves with the different interactive  elements and then asked to comment on their findings (submit a  minimum of 2 comments).  The second 2 subjects were given a  tutorial video, outlining the system and the process of the study,  asked to play around and familiarise themselves with the tools  and then comment on their findings (min of 3 comments).    While  all the subjects were familiar with the core tenets of information  visualisation, none are visualisation researchers and none were  familiar with collaborative visualisation.  No tasks were given,  instead users were asked to browse around the visualisation,  identify patterns of interest and comment on those patterns using  the graphical annotation and collaboration tools. Each session  lasted between 50 and 60 minutes.  Each session was recorded,  notes were taken by an observer and the subjects were asked to  think aloud.  Having submitted 3 comments to the system,  subjects were asked to complete two short questionnaires - a  participant questionnaire and a SUS (Simple Usability Score) test.   4.1.1 Evaluation Feedback  Some users expressed frustration at not being able to apply more  fine grained filtering to the visualisation, to drill down a little  deeper, or to compare and contrast the different user actions   such as following a specific thread over time or identifying the  actions taken during the lifecycle of a thread.  Other users  expressed frustration at not being able to see what people were  talking about.  For example, one participant asked what is the  relation between the different topics and the activity represented  by the visualisation  Another participant inquired what are  they talking about  Some subjects also found it difficult to  initially understand the matrices  as a visual metaphor   especially considering the use of reputation as an abstract  representation.  Few users were familiar with use adjacency  matrices as most indicated more familiarity with node-link  visualisations. However, once they grasped the general idea, and  understood the visualisation as an overview tool, most users set  about identifying patterns of interest.     4.2 Exploratory user study  The pilot study allowed us to refine our initial implementation  and to evaluate how users perceived the matrices over the more  familiar node-link diagrams, the second evaluation provided us  with a platform to study reflection in the Super User online  community. We recruited members of the Super User online  community.  A proposal for the study was posted on the Super   User meta-site5.  This was initially endorsed by a community  moderator but subsequently removed within twelve hours by a  Stack Exchange moderator who argued that this was an  advertisement and against Stack Exchange policy.  As a  consequence, each day we advertised in the Super User chat  rooms, which tend to be used by the more committed user-base.   28 people signed into the system, and 17 completed the  evaluation.  As with the lab study, participants were asked to sign  in, watch a short tutorial video, submit a minimum of five  comments and then complete two short questionnaires.  The first  questionnaire related to the process of the study, inquiring about  their impressions of the study and if they would be amenable to  further contact, while the second questionnaire was, again, a SUS  (Simple Usability Score) test. The process, they were informed,  would take a minimum of 30 minutes.  Six Amazon vouchers  were raffled to encourage participation.     4.2.1 Findings  We received positive initial feedback about the prototype,  especially from the more senior members of the site. Initially,  some interesting comments were submitted outside of the  Explore.su system  in the Super User chat room and on the  Super User meta site. For example, on the Super User meta-site,  one user commented considering how insightful this is, maybe  SE (Stack Exchange) should think of implementing  In the  Super User chat room, another commented on an insight they  discovered using the tool fascinating, 100-rep people are the  ones who're asking the most, 1000-3000 rep ones are the ones to  answer the most.        Figure 6: Results of the content analysis on Explore.su   commentary.  Each category is considered mutually exclusive.    Observations were most prevalent.  However, several   participants submitted hypotheses or questioned a previous   users observation.   Later, the same user contributed comments and revisions have  the same pattern of distribution, obviously comments distribution  is more dense [sic].  Participants also submitted comments as  part of the Explore.su system.  For example, one participant  commented these two clusters, based on the Questioner's rep,  make me wonder how users cross the gap from 50 to 100 rep  (reputation).  Also, several users identified clusters around                                                                     5 http://meta.superuser.com/   107    reputation of a 100.  For instance, here there's a concentration  around 100 rep (reputation) and there's a lot of activity  concentrated around the 100 rep (reputation).  Finally, one  participant remarked that while there is a large variation of the  reputation of commenters [sic], there is good correlation of users  presumably actively responding to comments (diagonal line),  especially in the 100-1000 range.   4.2.2 Content analysis of comments  We conducted a content analysis on the commentary from the  exploratory user study.  We wanted to learn to what extent the  visualisation provoked instances of reflection.  We also wanted to  assess the depth of reflection (whether an observation or a  hypothesis for example), whether any instance of reflection  involved discussion amongst the participants in the study or led to  a broader discussion about the general operation of the  community (in regards to conventions of norms for instance).  We  adopted the coding scheme that Heer et al. developed previously  for their work on social data analysis [15].  However, five out of  the eleven categories in Heer at al.s coding scheme proved  redundant during the coding procedure.  The redundant categories  were linking, socialising, testing, tips and to-do.  The seven  categories that were used during coding were data integrity,  question, affirmation, hypothesis, system design and observation.   Two coders coded the comments and we computed the Cohen  kappa statistic for inter-rater reliability at 0.77. The results of the  content analysis are illustrated in Figure 6.   5. DISCUSSION  We found that in both studies participants were able to  comprehend and interpret the visualisation.  From the exploratory  study content analysis (Figure 6), it is clear that the majority of  comments were observational (48%), considered as descriptions  of the activity portrayed in the visualisation, which is in contrast  to more reflective or hypothesis-driven comments (15%).  In  some cases participants asked questions of the data (9%) or  affirmed an observation contributed previously by another  participant (9%). System design was also commented upon, which  included feature suggestions and requests (15%).  However,  instances of reflection did occur in which participants attributed  additional meaning to the underlying patterns or suggested a  reason for their observations beyond that of what is evident in the  visualisation.  In some cases, this involved participants replying,  and affirming, a previous comment submitted by another  participant, or indeed suggesting an alternative hypothesis for the  original observation. While participants used the threaded  discussion, under half the comments had annotations (45%), and  15% of comments were submitted as private.  In the rest of this  section, we discuss our findings.  We concentrate on reflection  and the implementation of visual analytics in online communities.    5.1 Reflection in online communities  We have found supporting evidence to suggest social network  visualisation can provoke discussion and provide a context for  reflection in online communities.  During the exploratory user  study, several participants reflected upon the communitys  revision process (the most clearly identifiable pattern in the  visualisation).  For example, one participant noted:     Posters with a reputation below 500 are often   edited, mostly by users with rep (reputation) from   700 up to 10k. People with lower rep (reputation)   don't really seem to edit that much, although they   theoretically can... [sic]   While a second participant replied:   The users who appear to be editing posts at   lower rep levels appear to be the original   question asker.  I would guess that at lower rep   levels people are much more adverse to messing   up other peoples questions and instead focus on   their own... [sic]   As with Wikipedia, Super User is collaboratively edited; thus  users are encouraged to revise both their own and other users  submissions.  While the revision process is not officially related  to user reputation, there is a clear correlation between the amount  of revision work a user undertakes and their standing (as  measured by reputation) in the Super User community (as  illustrated in the above examples).         Another example of reflection occurs between two participants  who discuss the relationship between reputation and post activity.    Although it shifts from day to day, the   highest correlation can be found here, either   showing that there are many users with rep from   100 to 1k, or they are the ones who answer most,   and most questions are posted by users below 1k   However, a second participant replied:    I noticed that too, and it might be   interesting to note that the 100 rep (reputation)   range can be users that have migrated from other   (Stack Exchange) sites.   In this example, the second participant suggests an alternative  hypothesis for why the majority of activity occurs between users  with a reputation of around 100.  He suggests that these users may  have migrated from another Stack Exchange site in which they  would have amassed enough reputation to automatically receive a  reputation of a 101 when arriving on Super User.  Hence, the  concentration of activity could be explained by migrations to  Super User as opposed to a glut of users within that specific  reputation category.  Many questions are migrated from other  Stack Exchange sites to Super User, as the domain of hardware  and software is often confused with other domains such as  programming or Web Applications (two other Stack Exchange  sites), and users may find a question they submitted to Stack  Overflow appearing in the Super User online community.   This view was affirmed in a further comment submitted by  another participant.  Here the participant suggests that the  migrated users are well accustomed to the conventions of the  Stack Exchange network:    I find it interesting that the (100,100) box has   had a high number consistently. This furthers   xxxx's idea of a poster commenting on his/her   own post as the typical 'migrant' from another   SE site already knows how to use the Questions   and Comments correctly...   In the context of this study, the depth of reflection was limited by  the approach to visualisation, which, fundamentally, sought to  provide an overview of community activity. However, the  visualisation provoked some interesting observations, some of  which led to a deeper reflection based on conversations between  participants.  While we used live data for the study, and thus  could not guarantee visible patterns of interest, users were still   108    able to recognise the conventions of their community and to  attribute additional meaning to their observations.  Beyond  developing a deeper understanding of community processes, it is  difficult to argue whether visualisation could support a decision- making episode in an online community or indeed lead to a  change in community policy, especially when considering the  collaborative nature of policy-making.  However, reflection in a  shared context, as illustrated in the above examples, can help  provoke discussion in regards to the conventions and norms of the  community.         5.2 Visualisation communicates norms  From our work on Explore.su, we also found that visualisation is  an effective way to communicate norms. Figure 7 shows the  distributions for both revisions and comments in the Super User  online community.  Clearly, there is a strong correlation between  reputation and revision, as illustrated in the first picture; while,  comments, on the other hand, are more evenly distributed across  user reputations.  Users, regardless of reputation, make use of  comments as a way to seek and provide clarification and thus  refine and improve posts.  This can have two applications: First,  visualisation can provide feedback that can help to encourage  productive and discouraging unproductive forms of behaviour in  online communities.  Second, visualisation can be used as a way  to socialise new users by illustrating the norms and conventions  of the community.        Figure 7: Communicating norms.  The image on the left   illustrates the distribution of revisions in Super User.  Clearly,   users revise their own posts.  Also, users with a high   reputation revise other users posts.  The image on the right   illustrates the distribution of comments in Super User.  In this   image there is no clear correlation between comments and   reputation as the distribution is much more evenly spread   across reputation categories.     5.3 Networks for end-users  Advances in visualisation JavaScript libraries, coupled with the  increasing prevalence of public APIs, provides a great opportunity  for visualisation and community researchers to develop web- based visualisations that can improve transparency, increase  awareness and support more concerted processes such as  reflection.  However, the question still remains how useful are  social network visualisations for communities of end-users   While we found that participants expressed a genuine interest in  exploring their community through Explore.su, the majority of  those who participated in the study were community elders or  community moderators.  For instance, one user commented that  from what I can tell most of the people who I know would find it  useful have already been using it (i.e. community moderators).         5.4 Viz as a conversational component  While the exploratory user study provided a platform for  discussion, introducing new collaboration spaces is difficult and  can be perceived as redundant by online communities that have  existing and well established collaboration architectures.  From  the outset this was evident in the exploratory user study, as  participants repeatedly returned to the Super User chat room to  share observations drawn from Explore.su.  As a consequence, we  suggest that a more successful approach could support the  generation of small, addressable and interactive visualisations that  community members embed in their existing social software  to  monitor a process, highlight a particular activity or provide a  context for discussion.  This approach harnesses the communitys  existing collaborative infrastructure, which requires no additional  appropriation by the community.   5.5 Limitations  Finally, there are a number of limitations to this study.   Participants tended to analyse the visualisation as opposed to the  activity represented by the visualisation. This could be improved  by either providing a more interactive visualisation or focusing on  a sub-group of users, such as the moderators, who are more  actively engaged in the community.  New users may have less of  an interest in the activity of a community over those who are  already more committed.  Also, the sample size is relatively  small, when compared with the population of the community. We  ran the evaluation over a two week period and had some  difficulties attracting participants (due to Super Users policy in  regards to advertising on their site or meta-site). While evaluating  visualisation remains difficult - and there are a range of other  methodologies available to the researcher - the importance placed  on engaging the community underscored our approach.   Nevertheless, explicitly differentiating between the visualisation  and the act of reflection may prove as a more beneficial approach  to evaluation when conducted in similar contexts.   6. CONCLUSIONS AND FUTURE WORK  Commons-Based Peer Production has enabled large, online  communities of diverse demographics to produce media and  software artefacts. These communities have distributed  management structures and are subject to continuous change.  We  propose visual analytics as a way to encourage reflection and  support organisational learning in peer-based communities.  To  evaluate these ideas, we developed a social network visualisation,  called Explore.su, which we embedded in a collaborative  architecture.  We evaluated Explore.su with members of the Super  User online community and found that the visualisation provoked  both observational and hypothesis-driven commentary.  While  providing some evidence for the merits of reflection in online  communities, the approach to visualisation requires further  attention.  Embedding visualisations in existing social spaces, for  example, will increase the likelihood of adoption by community  members.  At the same time, how instructive network  visualisations are for end-users remains an open question and an  avenue for further research.    There are three directions for future work.  First, the depth of  reflection was constrained by the approach to visualisation.  The  visualisation did not provide the ability to explore the data in  great detail.  Thus, there remains scope to refine and then to re- evaluate the approach to visualisation.   Second, as successful  online communities have existing collaboration spaces, there is   109    room to investigate the application of small addressable  visualisations that users embed in their existing social spaces.  Third, providing access to social and temporal patterns may not  necessarily inform established users to any great degree. There is  scope to investigate the degree to which network visualisations,  and the process of reflection, can educate users about their  community.  This would involve creating a measure and applying  pre-and-post questionnaires to assess the depth of learning.    7. ACKNOWLEDGMENTS  This research is supported by the Science Foundation Ireland  (Grant 07/CE/I1142) as part of the Centre for Next Generation  Localisation (www.cngl.ie) at Trinity College Dublin. Thanks to  Super User for participating and providing the data used in this  research.     8. REFERENCES  [1]. Argyris, C. and Schon, D.A. Organisational Learning: A   Theory of Action Persepctive. Addison Wesley, 1978.   [2]. Benkler, Y. Coases Penguin, or, Linux and The Nature of  the Firm. The Yale Law journal 112, (2002), 369.   [3]. Bontis, N., Crossan, M.M., and Hulland, J. Managing An  Organizational Learning System By Aligning Stocks and  Flows. Journal of Management Studies 39, 4 (2002), 437- 469.   [4]. Butler, B., Joyce, E., and Pike, J. Dont look now, but we've  created a bureaucracy: the nature and roles of policies and  rules in wikipedia. Proceeding of the twenty-sixth annual  SIGCHI conference on Human factors in computing systems,  ACM (2008), 1101-1110.   [5]. Crossan, M.M., Lane, H.W., and White, R.E. An  Organizational Learning Framework: From Intuition to  Institution. Academy of management review 24, 3 (1999),  522537.   [6]. Douglas Thomas, J.S.B. A New Culture of Learning:  Cultivating the Imagination for a World of Constant Change.  CreateSpace, 2011.   [7]. Fisher, D. and Dourish, P. Social and temporal structures in  everyday collaboration. Proceedings of the 2004 conference  on Human factors in computing systems - CHI  04, (2004),  551-558.   [8]. Forte, A. and Bruckman, A. Scaling Consensus: Increasing  Decentralization in Wikipedia Governance Andrea Forte and  Amy Bruckman. Proceedings of the 41st Hawaii  International Conference on System Sciences (HICSS), IEEE  Computer Society (2008), 15-17.   [9]. Ghoniem, M., Fekete, J.-D., and Castagliola, P. A  Comparison of the Readability of Graphs Using Node-Link  and Matrix-Based Representations. IEEE Symposium on  Information Visualization, , 17-24.   [10]. Ham, F. Van, Schulz, H.-jrg, and Dimicco, J.M.  Honeycomb: Visual Analysis of Large Scale Social  Networks. Ifip International Federation For Information  Processing, (2009), 429-442.   [11]. Haythornthwaite, C. Crowds and Communities: Light and  Heavyweight Models of Peer Production. Proceedings of the   42nd Hawaii International Conference on System Sciences   (HICSS), IEEE (2008).   [12]. Heer, J. and Agrawala, M. Design considerations for  collaborative visual analytics. Information Visualization 7, 1  (2008), 49-62.   [13]. Heer, J. and Boyd, D. Vizster: visualizing online social  networks. IEEE Symposium on Information Visualization,  2005. INFOVIS 2005., (2005), 32-39.   [14]. Heer, J. and Hellerstein, J. Data Visualization and Social  Data Analysis. Proceedings of the VLDB Endowment,  (2009), 1656-1657.   [15]. Heer, J., Vigas, F., and Wattenberg, M. Voyagers and  voyeurs: Supporting asynchronous collaborative  visualization. Communications of the ACM, April (2009), 87- 97.   [16]. Henry, N. and Fekete, J.-D. MatrixExplorer: a dual- representation system to explore social networks. IEEE  transactions on visualization and computer graphics 12, 5  (2006), 677-84.   [17]. Jahnke, I. Socio-technical communities: From informal to  formal In B. Whitworth and A.D. Moor, eds., Handbook of  Research on SocioTechnical Design and Social Networking.  2009, 763-777.   [18]. Kim, D. The link between individual and organizational  learning. Sloan Managemnet Review 33, 1 (1996), 37-50.   [19]. Levitt, B. and March, J.G. Organizational Learning. Annual  Review of Sociology 14, 1 (1988), 319-340.   [20]. Perer, A. and Shneiderman, B. Integrating statistics and  visualization. Proceeding of the twenty-sixth annual CHI  conference on Human factors in computing systems - CHI    08, (2008), 265.   [21]. Schindler, M. and Vrandecic, D. Introducing new features to  Wikipedia: Case studies for Web Science. Proceedings of the  WebSci09: Society On-Line, (2009).   [22]. Viegas, F.B., Wattenberg, M., Ham, F. van, Kriss, J., and  McKeon, M. ManyEyes: a site for visualization at internet  scale. IEEE transactions on visualization and computer  graphics 13, 6 (2007), 1121-8.   [23]. Vigas, F.B. and Smith, M. Newsgroup Crowds and  AuthorLines: Visualizing the Activity of Individuals in  Conversational Cyberspaces. Proceedings of the 37th Hawaii  International Conference on System Sciences (HICSS),  (2004).   [24]. Vigas, F.B., Wattenberg, M., and Dave, K. Studying  cooperation and conflict between authors with history flow  visualizations. Proceedings of the SIGCHI conference on  Human factors in computing systems, ACM (2004).   [25]. Weber, S. What is Open Source and why does it work In  The success of Open Source. Harvard University Press, 2005.   [26]. Wenger, E. Communities of practice: learning, meaning, and  identity. Cambridge University Press; New Ed edition, 1999.   [27]. Zhu, B. and Chen, H. Communication-Garden System:  Visualizing a computer-mediated communication process.  Decision Support Systems 45, 4 (2008), 778-794.       110      "}
{"index":{"_id":"20"}}
{"datatype":"inproceedings","key":"Rivera-Pelayo:2012:AQS:2330601.2330631","author":"Rivera-Pelayo, Ver'onica and Zacharias, Valentin and Muller, Lars and Braun, Simone","title":"Applying Quantified Self Approaches to Support Reflective Learning","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"111--114","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330631","doi":"10.1145/2330601.2330631","acmid":"2330631","publisher":"ACM","address":"New York, NY, USA","keywords":"framework, learning analytics, mobile applications, quantified self, reflective learning","Abstract":"This paper presents a framework for technical support of reflective learning, derived from a unification of reflective learning theory with a conceptual framework of Quantified Self tools -- tools for collecting personally relevant information for gaining self-knowledge. Reflective learning means returning to and evaluating past experiences in order to promote continuous learning and improve future experiences. Whilst the reflective learning theories do not sufficiently consider technical support, Quantified Self (QS) approaches are rather experimental and the many emergent tools are disconnected from the goals and benefits of their use. This paper brings these two strands into one unified framework that shows how QS approaches can support reflective learning processes on the one hand and how reflective learning can inform the design of new QS tools for informal learning purposes on the other hand.","pdf":"Applying Quantified Self Approaches to Support Reflective Learning  Vernica Rivera-Pelayo, Valentin Zacharias, Lars Mller, and Simone Braun FZI Research Center for Information Technologies  Haid-und-Neu Str. 10-14, Karlsruhe, Germany rivera@fzi.de, zach@fzi.de, lmueller@fzi.de, braun@fzi.de  ABSTRACT This paper presents a framework for technical support of reflective learning, derived from a unification of reflective learning theory with a conceptual framework of Quantified Self tools  tools for collecting personally relevant informa- tion for gaining self-knowledge. Reflective learning means returning to and evaluating past experiences in order to pro- mote continuous learning and improve future experiences. Whilst the reflective learning theories do not sufficiently con- sider technical support, Quantified Self (QS) approaches are rather experimental and the many emergent tools are dis- connected from the goals and benefits of their use. This paper brings these two strands into one unified framework that shows how QS approaches can support reflective learn- ing processes on the one hand and how reflective learning can inform the design of new QS tools for informal learning purposes on the other hand.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.1 [Computer Uses in Education]: Collaborative learning, Computer-assisted instruction (CAI), Computer-managed in- struction (CMI), Distance learning  General Terms Theory  Keywords Reflective learning, Quantified Self, Learning Analytics, Framework, Mobile applications  1. INTRODUCTION Reflection is becoming of relevance in the learning commu-  nity and therefore reflective learning is being investigated in both educational and work settings. According to Boud et al. [1], learning by reflection (or reflective learning) offers the  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 12, 29 April - 2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM 978-1-4503-1111-3/12/04 ...$10.00.  chance of learning by returning to and evaluating past work and personal experiences in order to improve future experi- ences and promote continuous learning. Several approaches show initiatives to support reflective learning through tech- nology in different settings [13, 8, 6], but we lack an unifying framework that describes the role of technology in the reflec- tive process.  On the pragmatic side, a new kind of lifelogging approaches pursued by a community known as Quantified Self (QS)1  are becoming increasingly popular. QS is a collaboration of users and tool makers who share an interest in self- knowledge through numbers, i.e. self-knowledge through self-tracking. This interest results in a variety of tools to collect personally relevant information for self-reflection and self-monitoring, with the purpose of gaining knowledge about ones own behaviors, habits and thoughts. Hence, QS ap- proaches offer a rich source of data for learning analytics that has not been available for learning processes before.  This way, whereas QS approaches are pragmatic, having as main driver the experimentation; reflective learning is driven by theories that are evolving since the beginning of the 19th century. In an approach to join these two streams, this paper presents a framework that shows how QS ap- proaches can support the process of learning by reflection and informs the design of new QS tools for informal learning purposes. The starting point for the design of the framework was the survey of several QS tools, which allowed to analyze the characteristics these tools may have in common. More- over, the continuous advances in technology can facilitate the data gathering and therefore the quality and features of the tools. Sensor technologies are being improved, mobile technologies and devices are more widespread and Internet provides ubiquitous access to information.  In the following, we describe the theoretical and prag- matic background of reflective learning and Quantified Self in Sec. 2, before we present our framework to apply QS ap- proaches to support reflective learning (Sec. 3). Finally, we conclude this paper with its discussion in Sec. 4.  2. BACKGROUND OF THE FRAMEWORK  2.1 Theoretical Background Decades of research in reflective learning have highlighted  different aspects of reflective learning, leading to multiple theories [3, 7, 1, 12]. Hence, it is difficult to define a shared understanding about reflection. We were looking for a the- ory that provides insights into the cognitive processes and  1http://quantifiedself.com  111    can be a basis for the integration of technology into the re- flection process. We chose the model introduced by Boud et al. [1] because it considers the complete cognitive process, including affective aspects, but does not define the concrete activities around this process or a specific domain.  In the model by Boud et al., reflective learning refers to those intellectual and affective activities in which individ- uals engage to explore their experiences in order to lead to new understandings and appreciations [1]. Therefore, the reflective process is based on the experiences of the learner, i.e. the total response of a person to a situation, including behavior, ideas and feelings. The reflection process con- sists of three stages, in which the learner re-evaluates past experiences by attending to its various aspects, and thereby producing outcomes, which can be cognitive, affective or be- havioral. The reflection process and its context, experiences and outcomes, are depicted in Figure 1. A critical point is the start of the reflection process that leads to the initial re- turn to an experience, which is not explicitly defined by [1] because most events which precipitate reflection arise out of normal occurrences of ones life. However, the provided examples can be linked to cognitive dissonance theory [5].  Figure 1: The reflection process in context [1].  2.2 Pragmatical Background On the pragmatic side, we have a new kind of lifelogging  approaches with the recently emerged QS community, that promotes self-knowledge through numbers. Their lifel- ogging experiments and their tools have the intention of gaining knowledge about their own behaviors, habits and thoughts by collecting relevant information related to them. The starting point of the QS initiative are not scientific the- ories, but based on empirical self-experimentation. Apart from QS, all these approaches and tools are also known as personal informatics, living by numbers, self-surveillance, self-tracking and personal analytics [9].  Since the QS community was founded, we have seen a wide variety of approaches where people track, e.g., more than 40 different categories of information about the own health, the power usage of a thatched cottage or Vitamin D consump- tion [2]. Besides, plenty of tools are already available, which facilitate the tracking of different aspects of our lives. Some of these tools are web-based applications (e.g. Dopplr2 , day- tum3 , moodscope4) others are devices provided with phys- iological or environmental sensors (e.g. MIO5 , SenseCam6, DirectLife7) and yet others consist of mobile applications  2http://www.dopplr.com/ 3http://daytum.com/ 4http://www.moodscope.com 5http://www.mioglobal.com/ 6http://research.microsoft.com/en-us/ um/cambridge/projects/sensecam/ 7http://www.directlife.philips.com/  Figure 2: Role of the three QS potentials in the process of reflective learning. c FZI  (e.g. Sleep Cycle8, oneLog9, My Tracks10).  3. A FRAMEWORK TO APPLY QS APPROACHES TO SUPPORT REFLECTIVE LEARNING  In the previous section, reflective learning and QS were in- troduced and defined for the purpose of this paper. We now present a framework that combines these research strands into a model for the technical support for reflective learn- ing; centered around the model of Boud et al [1].  In our framework, three main support dimensions are iden- tified, namely: tracking cues, triggering and, recalling and revisiting experiences (see Fig. 2):  (a) Tracking cues: capturing and keeping track of certain data as basis for the whole reflective learning process.  (b) Triggering: fostering the initiation of reflective processes in the learner, based on the gathered data and the anal- ysis performed on it.  (c) Recalling and revisiting experiences: supporting learn- ers in recalling and revisiting through the enrichment and presentation of data in order to make sense of past experiences.  Figure 2 shows these three dimensions in relation to the reflective learning model of Boud et al. presented in the previous section. Firstly, tracking cues is directly related to tracking of behavior, ideas and feelings, which are the source of the reflective process on the one hand, and on the other hand related to the measurement of outcomes (e.g. new perspectives or change in behavior), which are continu- ously integrated with the original cues in order to feed future iterative reflection processes. Secondly, triggering is related to the start of the reflective process. Finally, the recalling and revisiting experiences enrich the process of returning to and evaluating experiences, as well as that of attending to feelings.  In the following we further differentiate the support di- mensions based on how these can be instantiated by QS tools.  8http://mdlabs.se/sleepcycle/ 9http://www.schmitzware.org/Software/OneLog/index.shtml  10http://mytracks.appspot.com/  112    3.1 Tracking Cues Tracking means the observation of a person and his/her  context in order to aid the reflective process. Tracking strives to quantify (aspects of) a persons life in order to enable some objectivity in understanding it. Tracking facilitates reflective learning by collecting data on experiences and out- comes that can then be used as objective basis in reflec- tion and triggering. We further characterize tracking by the means that are used, the object that is tracked, and the goal that is being strived for.  3.1.1 Tracking means Two main ways for tracking exist: self reporting through  often specialized software and hardware sensors that directly track behavior.  Software Sensors: Software sensors are applications that aid the user in capturing experiences and may be desktop- based, web-based or mobile-based. Software sensors are par- ticularly important for experiences that cannot (currently) be directly measured (such as feelings, ideas) and are of- ten much simpler, more flexible and cheaper to realize than hardware sensors. Software sensors are currently used in a broad variety of QS applications.  Hardware Sensors: Hardware sensors are devices that automatically capture data that can be used to deduce ex- periences or collect contextual information. Common cate- gories of sensors are: environmental sensors (e.g., light sen- sors, thermometers or microphone) and physiological sen- sors (accelerometers, heart rate sensors, sphygmomanome- ters, etc. ).  3.1.2 Tracked Aspects Of crucial importance to QS applications is the selection of  data about experiences and outcomes that is being tracked; what is tracked is likely to have a large effect on user ac- ceptance and efficiency for reflective learning. The tracked aspects found in QS applications can be classified in the following way:  Emotional Aspects: Emotional aspects such as mood, stress, interest, anxiety, etc.  Private and Work Data: Data from work processes and our lives such as photographies, the browsers history, digital documents, music, or use of a particular software etc.  Physiological Data: These are physical indicators and biological signals that describe a persons state of health. The main approaches comprise the measurement of physi- cal activity (for applications focusing on sport) and factors indicating health and sickness (e.g. glucose level).  General Activity: Data about a users general activity such as the number of cigarettes, cups of coffee or hours spent in a certain activity.  3.1.3 Purposes Another important classification dimension is that of the  purpose of a QS application; the goal which the user tries to achieve by using this application. This purpose drives and guides which measures are tracked and which means are appropriate.  3.2 Triggering Within the reflective learning process, triggers are respon-  sible for starting the actual reflection process. The role of triggers is to raise awareness and detect discrepancy. We  differentiate between active and passive triggering.  3.2.1 Active Triggering Active triggering consists of the tool sending a notification  or catching the attention of the user explicitly. In order to support active triggering, an application must perform data analysis to detect experiences that are suitable for initiating reflection. Such a situation may be a mismatch between a users goals and current level, comparison to a global thresh- old or other persons or a deviation from personal patterns.  3.2.2 Passive Triggering A system supporting only passive triggering does not iden-  tify experiences suitable for fostering reflection or it would not actively contact the user. This kind of system only dis- plays the collected data in a suitable way. It relies on the user to be triggered by somethings outside of the system or on the user regularly visiting the site and then detecting something that starts a reflection process.  3.3 Recalling and Revisiting Experiences Different aspects affect the recalling and revisiting of past  experiences, when analyzing the benefits that QS approaches could offer. Enrichment and presentation of the data may facilitate the revisiting of the data to analyze past expe- riences and reflect about them, and therefore enhance the learning process of the user.  So support of QS applications can exist along multiple dimensions: Contextualization, Data Fusion, Data Analysis, and Visualization.  3.3.1 Contextualizing The data being tracked can be enriched with other con-  text data. This contextualization of the data with other sources of information may be performed by the same tool or result from the interaction between tools (e.g. two mobile applications or a sensor with a desktop application).  Adapting the context definition from [4] we define context within this framework as: Context is any information that can be used to characterize the situation of a tracked entity and that can aid the reflection process.  Social Context: Data can be augmented with informa- tion about the social context of the user. This can be a comparison to Facebook friends or a comparison to all users. This helps to compare own performance/measures with the others and provides additional data to others in expectation to retrieve more data in exchange and ultimately see ones own experiences in relation to others. An aggregation of data over multiple users may provide new perspectives on experiences and offer new abstraction levels. Such an ag- gregation can be useful for individual reflection but also at a collaborative level, e.g. reviewing team performance over one month [10].  Spacial Context: The location in terms of city, street or even the room. As context this data can aid reflection by helping the user to understand the relation between place and her behavior - such as understanding the effects of high altitude on his or her heart rate or the calming effect of visits to specific places.  Historical Context: Comparing current values to his- toric ones allows to see upward or downward trends or to identify deviations from a historic norm that may indicate a problem. It may also help to identify the difference be-  113    tween periodic fluctuations (such as variations in weight or fitness according to the seasons) and other deviations from the norm that may indicate progress or a problem.  Item Metadata: Any metadata available about the things a user interacts with  such as the information that a par- ticular website is not work related but rather distracting, or that a food contains a large amount of sugar.  Context From Other Datasets: There are also numer- ous datasets (e.g. weather or work schedule) that might can also be used in contextualizing.  3.3.2 Data Fusion: Objective, Self, Peer and Group Assessment  One important aid to the reflection process can be the fu- sion and comparison of objective (i.e. measured by sensors), self (i.e. self reported data from the user), peer and group assessment (reported data from others about a user). There may be differences and discrepancies between these views that can foster reflection, can help to bridge the gap from subjective to objective experiences and in this way yield new insights and lead to learning. This relates to stage two of the reflection process  attending to feelings. Negative impres- sions can be discharged by comparing the individual per- spective to objective measurements. Aggregation of subjec- tive articulations over time or over different users can result in a more objective view (see also [11]).  3.3.3 Data Analysis: Aggregation, Averages, etc. Different forms of data processing help to present the  user useful measurements (e.g. number of cups of tea per day/week, average mood of my colleagues, etc.). In [10] we suggested formal, graphic and mathematic aggregation, de- pending on the data and purpose of the aggregation. For instance, aggregation in tag clouds as example of formal ag- gregation may need large amounts of data to become valu- able but can be applied to semi or unstructured data like texts. Further, it might be desirable to hide the source of the underlying data through aggregation and in this way to create anonymity and privacy.  3.3.4 Visualization Attractive and intuitive presentation and visualization forms  for the users should be chosen which, at the same time, fos- ter the analysis of the data for reflective learning purposes and being otherwise one of the major barriers (see [9]).  4. DISCUSSION AND CONCLUSION This paper presented a framework for the application of  QS applications to support reflective learning. In addition to ordering this strand of research, this framework is geared towards being used to understand the design space this kind of applications as well as understanding which parts havent been addressed by research. In the following we want to introduce some of the issues that were identified when re- viewing existing research into QS applications within this framework.  Assuming that QS tools can be shown to help people achieve their desired outcomes, there is also a luck of under- standing on how to identify the situations where they are likely to work, which are the right measures to track and finally how to spread the user beyond the current relatively narrow user base.  Currently there is also relatively little work on contextual- izing the data to improve the reflective process. Different QS applications are islands where data from one application and sensor cannot be used to understand that of another people. The use of external data sets (such as historic weather data in fitness applications) is even less common.  Overall the proposed combination of reflective learning and QS applications in the proposed framework concretizes the vision of learning analytics for a particular model of learning and class of support tools. In doing so it allows to identify promising venues for future research. It also shows the way how the notion of learning analytics can be applied beyond classroom settings in daily life to support all kinds of learning and self improvement.  Acknowledgements Work presented in this paper was partly conducted within the project MIRROR - Reflective learning at work funded under the FP7 of the European Commission (project num- ber 257617).  5. REFERENCES [1] D. Boud, R. Keogh, and D. Walker. Reflection:  Turning Experience into Learning, chapter Promoting Reflection in Learning: a Model., pages 1840. Routledge Falmer, New York, 1985.  [2] J. Brophy-Warren. The New Examined Life. http://online.wsj.com/article/  SB122852285532784401.html, Dec. 2008.  [3] J. Dewey. Experience and Education. Macmillan, London & New York, 1938.  [4] A. K. Dey. Understanding and using context. Personal and Ubiquitous Computing, 5:47, 2001.  [5] L. Festinger. A theory of cognitive dissonance. Stanford Univ. Press, 1957.  [6] R. Fleck. Supporting reflection on experience with sensecam. In CHI Workshop on Designing for Reflection on Experience, 2009.  [7] D. A. Kolb. Experiential Learning: Experience as the source of learning and development. Englewood Cliffs, N.J.: Prentice Hall, 1984.  [8] B. Krogstie and M. Divitini. Shared timeline and individual experience: Supporting retrospective reflection in student software engineering teams. In Proc. of CESEET 2009, pages 8592, Washington, DC, USA, 2009. IEEE Computer Society.  [9] I. Li, A. Dey, and J. Forlizzi. A stage-based model of personal informatics systems. In Proc. of CHI 2010, pages 557566, New York, NY, USA, 2010. ACM.  [10] L. Muller, B. Krogstie, and A. Schmidt. Towards capturing learning experiences. In ConTEL: Theory, methodology and design, ECTEL 2011, 2011.  [11] L. Muller, V. Rivera-Pelayo, and A. Schmidt. MIRROR D3.1 - User studies, requirements, and design studies for capturing learning experiences. MIRROR project deliverable D3.1, June 2011.  [12] D. A. Schon. Educating the Reflective Practitioner. Jossey-Bass, San Fransisco, 1 edition, 1987.  [13] K. Strampel and R. Oliver. Using technology to foster reflection in higher education. In Proc. of ascilite Singapore 2007, 2007.  114      "}
{"index":{"_id":"21"}}
{"datatype":"inproceedings","key":"Siadaty:2012:LSA:2330601.2330632","author":"Siadaty, Melody and Gavsevi'c, Dragan and Jovanovi'c, Jelena and Miliki'c, Nikola and Jeremi'c, Zoran and Ali, Liaqat and Giljanovi'c, Aleksandar and Hatala, Marek","title":"Learn-B: A Social Analytics-enabled Tool for Self-regulated Workplace Learning","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"115--119","numpages":"5","url":"http://doi.acm.org/10.1145/2330601.2330632","doi":"10.1145/2330601.2330632","acmid":"2330632","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative learning, learning analytics, self-regulated learning, semantic technologies, workplace learning","Abstract":"In this design briefing, we introduce the Learn-B environment, our attempt in designing and implementing a research prototype to address some of the challenges inherent in workplace learning: the informal aspect of workplace learning requires knowledge workers to be supported in their self-regulatory learning (SRL) processes, whilst its social nature draws attention to the role of collective in those processes. Moreover, learning at workplace is contextual and on-demand, thus requiring organizations to recognize and motivate the learning and knowledge building activities of their employees, where individual learning goals are harmonized with those of the organization. In particular, we focus on the analytics-based features of Learn-B, illustrate their design and current implementation, and discuss how each of them is hypothesized to target the above challenges.","pdf":"Learn-B: A Social Analytics-enabled Tool for    Self-regulated Workplace Learning  Melody Siadaty1, 2 , Dragan Gaevi1, 2, Jelena Jovanovi1,2, 3 , Nikola Miliki3,    Zoran Jeremi3, Liaqat Ali1, Aleksandar Giljanovi2 and Marek Hatala2     1 School of Computing and Information Systems, Athabasca University, Canada  2 School of Interactive Arts and Technology, Simon Fraser University, Canada   3 FON-School of Business Administration, University of Belgrade, Serbia     melody_siadaty@sfu.ca, dgasevic@acm.org, jeljov@gmail.com, nikola.milikic@gmail.com,   zoran.jeremic@gmail.com, liaqata@sfu.ca, aleksgiljanovic@gmail.com, mhatala@sfu.ca        ABSTRACT    In this design briefing, we introduce the Learn-B environment,   our attempt in designing and implementing a research prototype   to address some of the challenges inherent in workplace learn-  ing: the informal aspect of workplace learning requires know-  ledge workers to be supported in their self-regulatory learning   (SRL) processes, whilst its social nature draws attention to the   role of collective in those processes. Moreover, learning at   workplace is contextual and on-demand, thus requiring organi-  zations to recognize and motivate the learning and knowledge   building activities of their employees, where individual learn-  ing goals are harmonized with those of the organization. In par-  ticular, we focus on the analytics-based features of Learn-B, il-  lustrate their design and current implementation, and discuss   how each of them is hypothesized to target the above chal-  lenges.    Categories and Subject Descriptors   J.1 [Administrative Data Processing] Education; K.3.1   [Computer Uses in Education] Collaborative learning   Keywords  Learning Analytics, workplace learning, self-regulated learning,   collaborative learning, semantic technologies   1. INTRODUCTION  In the last few years, the growing emergence and acceptance of   social software tools, social media and Social Web (Web 2.0)   paradigm have brought forth a new perspective to the concept   of learning  [4] [14] [7], demonstrating a transition from conven- tional pedagogical approaches to a more social and collective   knowledge paradigm of learning, in that creativity, social-  embeddedness, and the capacity to gain knowledge from a sea   of collective are highly expected and valued  [9] [13].    Such a perspective to learning is especially important in the   context of workplace  [5], where learning is social, affects and is  affected by the social context and the available collective   knowledge.    To keep up with and adapt to the contextual needs of workplace   settings, learning at workplace mostly happens as a by-product   of work. This on-demand and informal approach to learning    [1] requires contemporary knowledge workers to have Self- Regulatory Learning (SRL) skills in identifying their learning   needs and conducting appropriate learning strategies to attain   them  [8]. The majority of conventional interpretations of SRL  are based on an individualistic perspective, where the impact of   the collective is often assumed less significant than individual-  based factors  [6]. Such perspectives contradict the nature of the  workplace, where individuals work and learning activities are   highly social and collective-centred. The recent research on   workplace learning clearly stresses the role of the collective   and other forms of social exchange in both individual learning   and organizational development  [4] [1]; findings on patterns of  defining learning goals in the workplace show that in the proc-  ess of setting and managing their learning goals, individuals   draw from and contribute to the collective knowledge in their   organization  [8].     To support users in their SRL processes in modern workplaces   as well as scaffolding organizational learning, there is a need   for systems that collect learningrelated contributions, re-  aggregate and analyse them to create further new knowledge,   and make this new knowledge available to users. Such new   knowledge can be beneficial to users in every step of their   learning process from identifying their learning needs and set-  ting their goals (e.g. they can get aware how other employees   with similar organizational positions have defined their goals),   to monitoring their learning progress and comparing it with that   of their colleagues who hold the same position or work in the   same project, and sharing and documenting their learning ex-  periences (e.g. by observing how actively their colleagues are   sharing their learning experiences and comparing it with their   own sharing activities, or to see how their shared knowledge   has been useful to other members of the organization).     Designing systems that unlock the collective knowledge, and   the collective intelligence in higher levels of inference for the   purpose of scaffolding learning, however, is not a straightfor-  ward task  [4]. Semantic technologies and Linked Data para-  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise,  or republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.   LAK12, April 29 May 2, 2012, Vancouver, BC, Canada  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.      115    digm could provide the required technical backbone for tack-  ling this challenge. Todays knowledge workers often use di-  verse tools and services in their everyday working and learning   practices; therefore, the traces and outcomes of their activities   are dispersed among different tools/services that often lack the   capability of interchanging and/or integrating users data. If   properly applied, the Linked Data paradigm and the associated   Semantic Web technologies would enable meaningful data in-  tegration and knowledge structuring.    Needless to say, to be successfully deployed and to lead to the   expected results, these advanced technologies need to be sup-  ported by proper pedagogical and motivational approaches. We   base the foundations of our pursued pedagogical approach on a   well-known organizational knowledge building model proposed   by Nonaka and Takeuchi  [10] (to address the challenge of har- monizing individual and organizational learning), and extend it   with SRL practices (to support users in initiating and conduct-  ing their individual learning processes)  [11], and motivational   elements  [12] (to address the challenge of motivating users to  share their personal knowledge and learning experiences, and   contributing to the collective knowledge in their organization).    For this pedagogical framework to work effectively, we hypo-  thesize that Learning Analytics (i.e. collecting users contribu-  tions, aggregating them, analyzing them and reporting back to   the users and the organization) play an important role: it allows   for the organization to better align its learning objectives with   those of its employees by knowing about their learning practic-  es; it supports users SRL processes by providing them with the   necessary input from the social context of the workplace; and it   enhances the motivation of individuals to take part in learning   and knowledge building activities and sharing their experiences   by providing them with feedback from the collective. In this de-  sign briefing, we introduce the Learn-B environment, our at-  tempt in designing and implementing a research prototype to   support workplace learning that addresses the above chal-  lenges. Learn-B stands for Learning Biosis (biosis meaning a   way of life), i.e. learning as a way of life. In particular, in this   design briefing we report on the learning analytics aspects pre-  sently supported by Learn-B.    2. THE LEARN-B ENVIRONMENT  The design of the Learn-B environment was driven by the re-  quirements for effective learning and knowledge building in or-  ganizational settings. It is designed to integrate different tools   that employees often interact with during their everyday (work-  ing and learning) practices. In particular, so far we have inte-  grated a wiki (MediaWiki), a social networking and collabora-  tion platform (Elgg), and a bookmarking tool (Tagging tool    implemented within this research as a Firefox plugin). Learn-B   serves as the central hub for this integrated environment, and   relies on an interlinked set of ontologies as its underlying   (linked) data model. These ontologies are available at:   http://goo.gl/Saui4. A current demo of the main functionalities   of the Learn-B environment is available at: http://goo.gl/RaiIm.    Figure 1 illustrates the multi-layer architecture of Learn-B   which can be adapted to and applied in a wide range of organi-  zations. There is no strong boundary between the layers and   components defined within each layer. In this design briefing,   we only focus on the Analytics-enabled functionalities provided   within the Processing Service Group.  This service group is   responsible for tracking all the events that happen in Learn-B,   and other integrated tools (i.e. MediaWiki, Elgg and the Tag-  ging Tool), processing and analyzing the gathered data, and   providing users with the resulting feedback and analytics. In   particular, Event Dispatcher (Figure 1.K) is responsible for   processing all of the events occurring in the Learn-B environ-  ment, storing them into the RDF repository (Figure 1.A) and   distributing them to other services. Analytics Service (Figure   1.L) is responsible for processing and analyzing the data about   users learning activities and their interaction with diverse   kinds of learning resources. It makes use of the interaction data   stored in the RDF repository to provide users with feedback,   primarily through different kinds of visualizations, to support   them in planning, performing and monitoring their learning   process.      Figure 1. The architecture of the Learn-B Environment    Usage Information is one type of the provided analytics which   comes in the form of statistics, Social Waves or the collective   stand. Derived from the collected knowledge within the system,   this functionality supports the recommender services (Figure   1.H-I) and more importantly, provides users with analytics rep-  resenting the collective knowledge around a resource and as-  sists them in planning their learning processes. Statistics and   Social Wave analytics are implemented as a set of various visu-  alization charts, each conveying the intended feedback/analytics   data. The feedback reflecting the collective stand about a learn-  ing resource comes in diverse forms such as annotations, reflec-  tions (e.g. comments and notes), ratings and tags of other users.    For instance, Figure 2 illustrates how each organizational ob-  jective, defined in terms of competences, is accompanied by   statistical analytics such as the number of users who have ac-  quired that competence and their roles in the organization, and   the Social Wave stream of that competence showing the activi-  ties performed on or events happened around it over a certain   period. Such analytics represent the popularity of a given   competence, indicating whether and to what extent it is (so-  cially) alive. The comments of other users can be viewed under   the Comments tab in Figure 2. The recommendation of a learn-  ing path, via the Learning Path Recommender service (Figure   1.H), to achieve a competence (in this research, each learning   path is comprised of one or more learning activities that lead to   the attainment of a specific competence at a specific level) is   further augmented with analytics such as the number of users   who have successfully finished this learning path or a revision   of it, or are still working on it, or have abandoned it. Users can   also see the organizational positions of users in each of the   above categories (i.e. active, finished, abandoned). Similar to   the competences, each learning activity is also accompanied by   Social stream and collective stand analytics.      116          Figure 2. Analytics - Usage Information provided for each organizational objective a) Statistics b) Social Waves   Progress-o-meters represent another type of the provided ana-  lytics; they aid users to monitor their learning progress in the   organizational context, by showing them their progress flow in   achieving their defined learning goals and the competences in-  cluded within those goals, and are implemented as a set of line   charts (Figure 3). Moreover, Progress-o-meters provide users   with a comparison of their progress flow with that of their col-  leagues who have the same learning goal (e.g., a goal shared by   the members of a project), or are working on the same compe-  tence. We hypothesize that observing oneself within the social   context of the organization helps users to monitor their progress   toward achieving their goals, thus also assisting them in further   regulating their learning strategies.       Figure 3. Analytics  Progress-o-meters   Knowledge Sharing Profiles inform users of their reflections, in   terms of sharing their learning resources, within an organiza-  tion. Via this type of provided analytics, users can see how ac-  tively they are sharing each of their learning resources, and also   compare their sharing activities with the average within their   organization (Figure 4). As a factor targeting individuals ex-  trinsic motivation  [12], we hypothesize that such feedback can  help users to regulate their knowledge sharing activities.      Figure 4. Analytics  Knowledge Sharing Profiles   Motivational Messages are another type of provided analytics   which aim to support users stronger engagement with the sys-  tem. Generally, a user (learner) model represents user knowl-  edge, goals, interests, and other features that allow for better   recommendations or provided adaptivity by the sys-  tem. Opening the learner model may bring additional benefits   to users, allowing them to take charge of their own learning ex-  perience. However, collecting explicit data from users is often   challenging and a strong motivation is needed on learners part   to provide explicit feedback about their learning  [1]. Motiva- tional Messages aim to tackle this challenge by providing users   with personalized messages indicating to what extent the col-  lective has opened their models in terms of sharing their per-  sonal preferences and learning experiences. For instance, Fig-  ure 5 shows a set of motivational messages related to the de-  gree of completeness of a users preferences compared to oth-  er users, where these preferences are used to adjust the recom-  mendations generated for the user.     117       Figure 5. Analytics  Motivational Messages   Last but not least, the Analytics Service supports the harmoni-  zation of individual and organizational learning objectives.   Browsing the different forms of Analytics available for a cer-  tain competence, updates the managers of an organization on,   for instance, how frequently this specific competence has been   used within the organization, in the context of which learning   goals it has been used, by users of what organizational posi-  tions, and what the main issues regarding this competence are.   This allows managers to apply any necessary modifications in   the definition of the competence itself or the learning paths as-  sociated with it, to better harmonize organizational and indi-  vidual learning needs. Also discovery of emerging competences   or other learning resources can be learned via this service. On   the other hand, if some user-created competences are frequently   being re-used by members of an organization; the managers   might consider them as emerging organizational goals. As can   be seen, organizational goals are also dynamic and can evolve   via the contributions of the community members. Accordingly,   this targets individuals intrinsic motivation for knowledge   sharing by giving them the feeling of being competent in con-  tributing to the organizations goals and objectives.   3. CONCLUSIONS   In this design briefing, we demonstrated the analytics-based   features of Learn-B, which are built on Semantic technologies   and Linked Data paradigm, and backed with an extended peda-  gogical and motivational framework. In our empirical work   with Learn-B, we aim to answer if and to what extend these   features, along with the other functionalities provided within   Learn-B, address the existing challenges in supporting   workplace learning. In particular, to support users self-  regulatory practices in the context of workplace learning, we   hypothesize that the usage information analytics accompanying   each learning resource in Learn-B (i.e. statistics, Social Waves   and the collective stand) assist users in planning their learning   goals; Progress-o-meters, on the other hand, provide users with   feedback on their progress flow and thus help them with moni-  toring and evaluating their learning progress. Knowledge Shar-  ing Profiles inform users of their reflections, in terms of shar-  ing their learning resources and experiences within an organi-  zation. Accordingly, we propose that these profiles support us-  ers to align their reflections and sharing of their learning re-  sources and experiences. Motivational Messages are another   analytics-based means designed to foster users contributions   and to motivate them to provide higher quality inputs to the   system.    4. ACKNOWLEDGMENTS  This demonstration was partially supported/co-funded by   NSERC, Athabasca University, and the European Community   under the Information and Communication Technologies theme   of the 7th Framework Program for R&D. This document does   not represent the opinion of NSERC, Athabasca University, and   the European Community, and NSERC, Athabasca University,   and the European Community are not responsible for any use   that might be made of its content.   5. REFERENCES  [1] Bull, S., Kay, J. 2010. Open Learner Models. In R. Nkam-  bou, J. Bordeau & R. Miziguchi (eds), Advances in Intel-  ligent Tutoring Systems, Springer. 301-322.   [2] Fenwick, T. 2008. Understanding relations of individual-  collective learning in work: A review of research. Man-  agement Learning. 39 (3), 227-243.   [3] Fischer, G. 2000. Lifelong Learning - More Than Train-  ing. Journal of Interactive Learning Research, 11 (3/4),   265-294.   [4] Gruber, T. 2008. Collective Knowledge Systems: Where   the Social Web meets the Semantic Web. Journal of Web   Semantics, 6(1), pp. 4-13.   [5] Hart, J. 2010. The State of Learning in the Workplace To- day. Center for Learning and Performance Technologies   website. Available online at: http://goo.gl/Ipt2g   [6] Jackson, T., MacKenzie, J., & Hobfoll, S. E. 2000. Com- munal aspects of self-regulation. In M. Boekaerts, P. R.   Pintrich, and M. Zeidner (Eds.), Handbook of self-  regulation. San Diego, CA: Academic Press. 275-299.    [7] Jovanovic, J., Gasevic, D., Torniai, C., Bateman, S., Hata- la, M. 2009. The Social Semantic Web in Intelligent   Learning Environments - State of the Art and Future Chal-  lenges. Interactive Learning Environments, 17(4), 273-  308.   [8] Littlejohn, A., Margaryan, A, and Milligan, C. 2009.  Charting collective knowledge: Supporting self-regulated   learning in the workplace. In. Proc. of the 9th IEEE Inter-  national Conference on Advanced Learning Technologies,   208-212.   [9] McLoughlin, C. and Lee, M.J.W. 2010. Pedagogy 2.0:  Critical challenges and responses to Web 2.0 and social   software in tertiary teaching. In M.J.W. Lee & C.   McLoughlin (Eds), Web 2.0-based e-learning: applying   social informatics for tertiary teaching. Hershey, PA: In-  formation Science Reference. 4369.   [10] Nonaka, I., & Takeuchi, H. 1995. The knowledge-creating  company. How Japanese companies create the dynamics of   innovation. New York: Oxford University Press.   [11] Siadaty, M., Jovanovi, J., Pata, K., Holocher-Ertl, T.,  Gaevi, D., Miliki, N. 2011. A Semantic Web-enabled   Tool for Self-Regulated Learning in the Workplace. In   Proc. 11th IEEE Intl. Conf. on Advanced Learning Tech-  nologies. (Athens, Athens, GA , USA, 2011). 66-70.   [12] Siadaty, M., Jovanovi, J., Gaevi, D., Jeremi, Z., Ho- locher-Ertl, T. 2010.  Leveraging Semantic Technologies   118    for Harmonization of Individual and Organizational Learn-  ing. In Proc. 5th Euro. Conf. Technology-enhanced Learn-  ing. (Barcelona, Spain, 2010). 340-356.   [13] Tynjl, P. 2008. Perspectives into learning at the   workplace, Educational Research Review, 3, 130-154.     [14] Vassileva, J. 2008. Towards Social Learning Environ-  ments, IEEE Trans. on Learning Technologies, 1 (4), 199-  214.      119      "}
{"index":{"_id":"22"}}
{"datatype":"inproceedings","key":"Drachsler:2012:PLA:2330601.2330634","author":"Drachsler, Hendrik and Greller, Wolfgang","title":"The Pulse of Learning Analytics Understandings and Expectations from the Stakeholders","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"120--129","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330634","doi":"10.1145/2330601.2330634","acmid":"2330634","publisher":"ACM","address":"New York, NY, USA","keywords":"attitude, expectations, innovation, learning analytics, learning technologies, privacy, survey, understanding","Abstract":" While there is currently much buzz about the new field of learning analytics [19] and the potential it holds for benefiting teaching and learning, the impression one currently gets is that there is also much uncertainty and hesitation, even extending to scepticism. A clear common understanding and vision for the domain has not yet formed among the educator and research community. To investigate this situation, we distributed a stakeholder survey in September 2011 to an international audience from different sectors of education. The findings provide some further insights into the current level of understanding and expectations toward learning analytics among stakeholders. The survey was scaffolded by a conceptual framework on learning analytics that was developed based on a recent literature review. It divides the domain of learning analytics into six critical dimensions. The preliminary survey among 156 educational practitioners and researchers mostly from the higher education sector reveals substantial uncertainties in learning analytics. In this article, we first briefly introduce the learning analytics framework and its six domains that formed the backbone structure to our survey. Afterwards, we describe the method and key results of the learning analytics questionnaire and draw further conclusions for the field in research and practice. The article finishes with plans for future research on the questionnaire and the publication of both data and the questions for others to utilize","pdf":"The Pulse of Learning Analytics   Understandings and Expectations from the Stakeholders      Hendrik Drachsler  Open University of the Netherland   Valkenburgerweg 177  6419AT Heerlen, Netherlands   +31 (0)45 576-21-74   hendrik.drachsler@ou.nl   Wolfgang Greller  Open University of the Netherlands   Valkenburgerweg 177  6419AT Heerlen, Netherlands   +31 (0)45 576-21-74   wolfgang.greller@ou.nl     ABSTRACT  While there is currently much buzz about the new field of learning   analytics  [19] and the potential it holds for benefiting teaching and  learning, the impression one currently gets is that there is also  much uncertainty and hesitation, even extending to scepticism. A  clear common understanding and vision for the domain has not  yet formed among the educator and research community. To  investigate this situation, we distributed a stakeholder survey in  September 2011 to an international audience from different  sectors of education. The findings provide some further insights  into the current level of understanding and expectations toward  learning analytics among stakeholders. The survey was scaffolded  by a conceptual framework on learning analytics that was  developed based on a recent literature review. It divides the  domain of learning analytics into six critical dimensions. The  preliminary survey among 156 educational practitioners and  researchers mostly from the higher education sector reveals  substantial uncertainties in learning analytics.   In this article, we first briefly introduce the learning analytics  framework and its six domains that formed the backbone structure  to our survey. Afterwards, we describe the method and key results  of the learning analytics questionnaire and draw further  conclusions for the field in research and practice. The article  finishes with plans for future research on the questionnaire and the  publication of both data and the questions for others to utilize.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed   instruction (CMI), Distance learning, A.1; [Introductory and  survey]; H.1.1 [Information Systems] Models and principles,  Systems and Information Theory; J.4 [Social and behavioral  sciences].    General Terms  Measurement, Documentation, Design, Human Factors, Theory.   Keywords  Learning analytics, survey, understanding, expectations, attitude,  privacy, learning technologies, innovation.   1. INTRODUCTION  Despite the great enthusiasm that is currently surrounding learning  analytics, it also raises substantial questions for research. In  addition to technically-focused research questions such as the  compatibility of educational datasets, or the comparability and  adequacy of algorithmic and technological approaches, there  remain several softer issues and problem areas that influence the  acceptance and the impact of learning analytics. Among these are  questions of data ownership and openness, ethical use and dangers  of abuse, and the demand for new key competences to interpret  and act on learning analytics results.    This motivated us to identify the six critical dimensions (soft and  hard) of learning analytics, which need to be covered by the  design to ensure an appropriate exploitation of learning analytics  in an educationally beneficial way. In a submitted article to the  special issue on learning analytics [5], we developed the idea of a  conceptual framework encapsulating the design requirements for  the practical application of learning analytics. The framework  models the domain in six critical dimensions, each of which is  subdivided into sub-dimensions or instantiations. Figure 1. below  graphically represents the framework. In brief, the dimensions of  the framework contain the following perspectives:   - Stakeholders: the contributors and beneficiaries of learning  analytics. The stakeholder dimension includes data clients as well  as data subjects. Data clients are the beneficiaries of the learning  analytics process who are entitled and meant to act upon the  outcome (e.g. teachers). Conversely, the data subjects are the  suppliers of data, normally through their browsing and interaction  behaviour (e.g. learners). In some cases, data clients and subjects  can be the same, e.g. in a reflection scenario.    - Objectives: set goals that one wants to achieve. The main  opportunities for learning analytics as a domain are to unveil and  contextualise so far hidden information out of the educational data  and prepare it for the different stakeholders. Monitoring and  comparing information flows and social interactions can offer new  insights for learners as well as improve organisational  effectiveness and efficiency  [23]. This new kind of information  can support individual learning processes but also organisational  knowledge management processes as describe in  [25]. We  distinguish two fundamentally different objectives: reflection and  prediction. Reflection  [14] is seen here as the critical self- evaluation of a data client as indicated by their own datasets in  order to obtain self-knowledge. Prediction  [11] can lead to earlier  intervention (e.g. to prevent drop-out), or adapted activities or  services.     Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy   otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.   120    Figure 1. The learning analytics framework   - Data: the educational datasets and their environment in which  they occur and are shared. Learning analytics takes advantage of  available datasets from different educational systems. Institutions  already possess a large amount of student data, and use these for  different purposes, among which administering student progress  and reporting to receive funding from the public authorities are  the most commonly known. Linking such available datasets  would facilitate the development of mash-up applications that can  lead to more learner-oriented services and therefore improved  personalization  [24]. However, most of the data produced in  institutions is protected, and the protection of student data and  created learning artefacts is a high priority for IT services  departments. Nevertheless, similar to Open Access publishing and  related movements, calls for more openness of educational  datasets have already been brought forward  [13]. Anonymisation  is one means of creating access to so-called Open Data. How open  educational data should be, requires a wider debate but, already in  2010, several data initiatives (dataTEL, LinkedEducation) began  making more educational data publicly available. A state of the art  overview of educational datasets can be found in  [15].   - Method: technologies, algorithms, and theories that carry the  analysis. Different technologies can be applied in the development  of educational services and applications that support the  objectives of the different educational stakeholders. Learning  analytics takes advantage of so-called information retrieval  technologies like educational data mining (EDM)  [20]  [17],  machine learning, or classical statistical analysis techniques in  combination with visualization techniques  [18]. Under the  dimension Methods in our model, we also include theoretical  constructs by which we mean different ways of approaching data.  These ways in the broadest sense translate raw data into  information. The quality of the output information and its  usefulness to the stakeholders depend heavily on the methods  chosen.   - Constraints: restrictions or potential limitations for anticipated  benefits. New ethical and privacy issues arise when applying  learning analytics in education  [16]. These are challenging and  highly sensitive topics when talking about datasets, as described  in  [13]. The feeling of endangered privacy may lead to resistance  from data subjects toward new developments in learning  analytics. In order to use data in the context of learning analytics  in an acceptable and compliant way, policies and guidelines need  to be developed that protect the data from abuse. Legal data and  privacy protection may require that data subjects give their   explicit and informed consent and opt-into data gathering  activities or have the possibility to opt-out and have their data  removed from the dataset. At the same time, as much coverage of  the datasets as possible is desirable.   - Competences: user requirements to exploit the benefits. In order  to make learning analytics an effective tool for educational  practice, it is important to recognise that learning analytics ends  with the presentation of algorithmically attained results that  require interpretation  [21]  [22]. There are many ways to interpret  data and base consecutive decisions and actions on it, but only  some of them will lead to benefits and to improved learning  [25].  Basic numeric and other literacies, as well as ethical  understanding are not enough to realise the benefits that learning  analytics has to offer. Therefore, the optimal exploitation of  learning analytics data requires some high level competences in  this direction, but interpretative and critical evaluation skills are  to-date not a standard competence for the stakeholders, whence it  may remain unclear to them what to do as a consequence of a  learning analytics outcome.   To further substantiate the currently dominant views on the  emerging domain, we turned to the wider education community  for feedback. The aim was to extrapolate the diverse opinions  from different sub-groups and roles (e.g. researchers, teachers,  managers, etc.) in order to see: (a) what the current  understandings and the expectations of the different target groups  are and (b) if a common understanding of learning analytics has  already been developed.    The mentioned framework was used to structure the questionnaire  in order to avoid as much as possible bias toward a single  perspective of learning analytics, e.g. the data technologies, and in  order to get a balanced overview of the field as a whole. The  questionnaire took concrete aspects into focus in the following  way: The stakeholders dimension inquired about the expected  beneficiaries; objectives tried to highlight the preference  between reflective use of analytics and prediction; It also checked  for the development areas where benefits are most likely or are  expected; the data section looked into stances on sharing and  access to datasets; methods investigated trust in technology and  algorithmic approaches; constraints focused on observations on  ethical and privacy limitations (so-called soft-barriers); and,  finally, competences looked into the confidence for exploiting  the results of analytics in beneficial ways.   Although we wont go into this issue in this article, we are aware  that there may be cultural, organizational, and personal  differences that influence the subjective evaluation of the  dimensions of learning analytics.   In section two below, we go on to describe in more detail the set- up of the questionnaire, the participants and the distribution  method. In section three, we present and discuss results and  statistical effects.   2. EMPIRICAL APPROACH   2.1 Method  To evaluate the understanding and expectations of the educator  and research community in learning analytics, we decided to use a  questionnaire for reasons of ease of distribution and world-wide  reach. In a globally distributed learning analytics community, this  promised the best effort-return ratio, as opposed to other deeper,  but more restrictive and effort intensive methods such as  interviews. However, we anticipated the questionnaire as a first   121    exploratory step toward more refined questioning and deeper  analysis that would follow.   Table 1: Overview of question items and answer types. The full  questionnaire and the cleaned dataset are available at  http://dspace.ou.nl/handle/1820/3850. This includes also the tested  statements of the multiple-choice and rank order questions that  can not be mentioned here due to space issues.   Dom-  ains   Questions Answer types and   data range   S ta  k e-  h o ld  er s   Q3.2 In your opinion, who will benefit the most  from Learning Analytics   Rank order   Q3.3 Which relationships between stakeholder  groups do you think Learning Analytics will  strengthen most   Side by side correlation   O b  je ct  iv es     Q4.2 In your opinion, how much will Learning  Analytics influence the following areas   Matrix table with Likert scale  (dont know - 1, not at all - 2,  a little - 3, very much - 4)   Q4.3 Which added innovation can Learning  Analytics bring to educational technologies   Rank order   Q4.4 What should the main objective for  Learning Analytics be   Single choice   Q4.5 When thinking of impact, how much  attention should Learning Analytics pay to...   Slider  (0 -100)  (0 = not desirable  100 = very desirable)   D a ta     Q5.2 How important do you consider the  following attributes of educational datasets   Matrix table with Likert scale  (don't know - 1, not important  - 2, important- 3, highly  important - 4)   Q5.3 Which IT systems does your organization  use for teaching and learning   Multiple choice   Q5.4 Would you share educational datasets of  learners openly in an international research  community, provided these datasets are  anonymised according to standard principles   Single choice  (Yes, No, Dont know)   M et  h o d   Q6.2 Learning Analytics is based on algorithms,  s, and theories that translate data into  meaningful information. How hopeful are you  that these methods produce an accurate picture  of learning in the following areas   Slider (0-100)  (0 = I dont think so ,   100 = Im certain of it )   C o n  st ra  in ts     Q7.2 In your opinion, how much will Learning  Analytics influence the following subjects   Matrix table with Likert scale  (dont know - 1, not at all - 2,  a little -3, very much - 4)   Q7.3 Learning Analytics utilises data which is  protected by data protection legislation. Do you  think a national standard anonymisation process  would alleviate fears of data abuse   Single choice    (Yes, No, Dont know)   Q7.4 Does your institution have and operate  ethical guidelines that regulate the use of  student data (for example in research)   Single choice   (Yes, No, Dont know)   Q7.5 Should institutions internally share all the  data they hold on students with all members of  staff (academic, technical, administrative,  research)   Single choice   (Yes, No, Dont know)   Q7.6 How much do you feel Learning Analytics  and automated data gathering affect the privacy  and rights of individuals   Matrix table with Likert scale   (Don't know - 1, not at all -2 ,  a little - 3, very much - 4)   C o m  p et  en ce  s   Q8.2 How important do you consider the  following skills to be present in learners to  benefit from Learning Analytics   Matrix table with Likert scale  (don't know - 1, not important  - 2, important -3, highly  important - 4)   Q8.3 Do you think the majority of learners  would be competent enough to learn from their  Learning Analytics reports   Single choice   (Yes, No, Dont know)      Q8.4 How sure can we be that end users will  draw the right conclusions from their data, and  decide on the best course of action   Slider (0-100)  (0 = Range not sure  100= absolutely certain)    For a more representative study the dissemination of the  questionnaire should be supported as well over public bodies like  school foundations and governmental institutions.   To give the questionnaire an organized structure that would  capture the domain in its entirety, where pedagogic and personal  perceptions would have equal attendance to technical challenges  or legalistic barriers, we divided the instrument into the six  dimensions as indicated by the framework (see above). For each  dimension, we asked the participants two-three questions and  offered the opportunity for open comments. Questions were  formulated in a variety of types, including prioritization lists (rank  order), Likert scales, matrix tables, and multiple and single choice  questions.    Another criterion we felt necessary to adhere to in our evaluation  of the current perception of learning analytics as a domain was  openness. Rather than selecting a handful of renowned experts in  the field, or to involve a particular education sector or even a local  school (which would most probably have just revealed a  widespread ignorance about this developing research domain), we  wanted to compile an overview cutting across national, cultural,  sectorial boundaries, and even roles of people involved in learning  analytics. Although this would unavoidably lead to a much fuzzier  picture, we felt the benefits to our understanding of the interest  and hesitations toward learning analytics, in what is a general  trend to much wider open educational practices, outweighed such  concerns, allowing us to better assess the potential impact of  learning analytics to education.   Before publishing it, the questionnaire was validated in a small  internal pilot with two PhD students and two senior staff members  within the newly founded Learning Analytics research unit in our  institution. In order to reach a wide network of a globally  distributed Community of Practice, we designed and hosted the  questionnaire online, using the free limited version of Qualtrics  (qualtrics.com). This online tool is pleasantly designed and easy  to use. It provides several sophisticated question-answer types  with more being available for premium users. The data and the  questionnaire are exportable in a number of popular formats  including MS Excel and SPSS. The free version came with a  limitation of 250 responses. All excess answers were recorded,  but discarded in the analysis and data export. In our case, with a  small sampling community, the free version proved to be  sufficient.   2.2 Reach  We first promoted the questionnaire in a learning analytics  seminar at the Dutch SURF foundation, a national body for  driving innovation in education in the Netherlands. We then went  on to distribute the questionnaire through the JISC network in the  UK and via social media channels of relevant networks like the  Google group on learning analytics, the SIG dataTEL at  TELeurope, the Adaptive Hypermedia and the Dutch computer  science (SIKS) mailing lists and to participants in international  massive open online courses (MOOCs) in technology enhanced  learning (TEL) using social network channels like facebook,  twitter, LinkedIn, and XING. This distribution method is reflected  in the constituency reached, in that there is, for example, a limited  response rate from Romance countries (France, Iberia, Latin  America) against a high return from Anglo-Saxon countries. The  lack of responses from countries like Russia, China or India,  maybe due to a number of factors: the distribution networks not  reaching these countries, the language of the questionnaire  (English), or a general lack of awareness of learning analytics in  these countries. Still, we found that with the numbers of returns,  we received a meaningful number of people interested in the  domain.   122    The survey was available for four weeks, during September 2011.  After removal of invalid responses we analysed answers from 156  participants, with 121 people (78%) completing the survey in full.  In total, the survey now covers responses from 31 countries, with  the highest concentrations in the UK (38), the US (30), and the  Netherlands (22) (see Figure 2. below).       Figure 2. Geographic distribution of responses   2.3 Participants  Although we tried to promote the questionnaire equally to  schools, universities and other education sectors, including e- learning companies, we received a significantly higher response  from the tertiary sector (further and higher education) with 74%  (n=116). It is probably fair to say that learning analytics as a topic  is not yet popular or well-known in other educational sectors with  the combined K-12 sector amounting to 9% (n=13) and some 11%  (n=17) coming from the adult, vocational, and commercial  sectors. The remaining 6% (n=9) in the other subgroup includes  cross-sector and other individuals, such as retirees from the  education sector.   The only other demographic information we collected from  participants was their role in the home institution. Here we  received a broad variety of stakeholder groups that deal with  learning analytics. Multiple answers were possible, taking into  account that people may have more than one role in their  organisation.   The three largest groups of our test sample were teachers with  44% (n=68), followed by researchers with 36% (n=56) and  learning designers with 26% (n=41). With 16.1% (n=32) senior  managers too were identified as a representative group of which  two thirds (65.6%) came from HE institutions. 40.4% of the 156  participants claimed more than one role in their institution, of  which again 40.3% were teacher/researchers (16.7% of the total  sample).   Next, well present the most relevant results from the online  questionnaire regarding expectations and understanding of  learning analytics.   3. RESULTS  Our report on the results is organized along the lines of the six  dimensions of the learning analytics framework (cf. section 1  above). We paid special attention to mapping opinions against  institutional roles in order to identify any significant agreement or  discord in each of the dimensions.   One uncertainty underlying the outcomes is the lack of an  established domain definition and/or established domain   boundaries through practice. The term learning analytics is still  rather vague, shared practice in the area is only just emerging and  a scientifically agreed definition lacking. From on-going research  and development work we know that some researchers subsume  for example educational business analysis or academic analytics  [8], or action analytics [7] under learning analytics [2]. Thus, the  domain name itself carries a highly subjective interpretation,  which almost certainly influenced the answers in the survey. We  have no doubt that as the domain matures further, this  interpretation will be narrowed down, leading to a better graspable  scope and possibly more congruencies in the responses.   3.1 Stakeholders  In this section, we wanted to know: (a) who was expected to  benefit the most from learning analytics, and, (b) how much will  learning analytics influence specific bilateral relationships   Regarding the prioritisation of the stakeholder of learning  analytics, the majority of respondents agreed that learners and  teachers were the main beneficiaries of learning analytics where 1  was the highest score on the Likert scale. The weighting of the  155 responses shows that learners were rated highest at 1.9 mean  rank, followed by teachers with 2.1. However, the ranking  distribution and standard deviation for learners was higher (1.12)  than for teachers (0.88). Institutions came in third place with an  average rank of 2.6. There was also substantial contribution to the  other category with suggestions for further beneficiaries. Among  those and most prominent were government and funding bodies,  but also employers and support staff were mentioned.      Graph 1. Relationships affected (1)   Graph 1 above illustrates the outcomes of question (b) and  confirms the findings of question (a) above. The peaks identify  the anticipated intensity of the relationship. Relationships with  parents are not seen as majorly impacted, which is probably due to  the fading influence parents have in tertiary education. It would be  interesting to complete this picture with more responses from the  K-12 domain. The highest impact is seen in the teacher - student  relationship (83.5%, n=111, of respondents emphasised this),  whereas the reverse student - teacher connection is strengthened  slightly less (63.2%, n=84). Only less than half the participants  see peer relationships as being strengthened through learning  analytics: learner - learner by 45.9% (n=61), and teacher - teacher  by 41.4% (n=55). At roughly the same level comes the  relationship between institution and teachers (46.6%, n=62).   123       Graph 2. Relationships affected (2)   In the spider diagram (graph 2 above), the area indicates that it is  the relationships of teachers that are expected to be most widely  affected, followed by learners, institutions, and parents at a  minimal level.   3.2 Objectives  In this section, we asked participants in which way learning  analytics will change educational practice in particular areas. Of  the total answers given in all 13 areas (n=1543), collected from  119 participants, only 10.8% of responses anticipated no change  at all. On the other hand, the remaining responses left it open  whether the expected changes will be small (43.8%) or extensive  (45.4%).      Graph 3. Objectives for learning analytics   Looking at the individual areas (cf. graph 3 above), the highest  impact was expected in more timely information about the  learning progress (item 2), and better insight by institutions on  what's happening in a course (item 8). On the bottom end were  expectations with respect to assessment and grading (items 6 and  5), where the least changes were anticipated.   Further, we contrasted the importance of three generic objectives  for learning analytics: (a) reflection, (b) prediction, (c) unveil  hidden information. 47% (n=61) of the respondents felt that  stimulating reflection in stakeholders about their own  performance was the most important goal to achieve, while 37%  (n=48) expressed the hope that learning analytics would unveil  hidden information about learners (cf. graph 4). Both are not  necessarily in contradiction to each other, since insights into new  information can be seen as motivator for reflection. However the  case may be, only 16% (n=20) favoured the prediction of a  learners performance or adaptive support as a key objective.      Graph 4. Generic preference   When looking at these objectives from the perspective of the  different roles of participants, we find that teachers show a fairly  equal interest in unveiling hidden information 44.6% (n=25), and  in reflection 37.5% (n=21). This is a reasonable finding as many  teachers expect learning analytics to support them in their daily  teaching practice by offering additional indicators that go beyond  reflection processes. On the other hand, 60.4% (n=29) of  researchers indicated a clear preference for reflection.   Translated into technological development, the expectations  favoured more adaptive systems (highest rank), followed by data  visualisations of learning, and better content recommendations in  third place. Further interesting suggestions were learning  paths/styles adopted by students, the clustering of learning types,  and applications for the acknowledgement of prior learning.   A further question surveyed the perception of learning analytics  being a formal or less-formal instrument for institutions. In two  intermixed sets of three options, one set represented formal  institutional criteria: mainstream activities, standards, and quality  assurance, all relating to typically tightly integrated domains that  are governed by institutional business processes and strategies.  The other set contained three less-formal and less monitored areas  of pedagogic creativity, innovation, and educational  experimentation. All three items represented individual choice of  staff members to be innovative, experimental, and creative in their  lesson planning and teaching activities. As indicated in graph 5  below, among the 129 responses, there was a noticeable  preference towards less formal institutional use of learning  analytics at a ratio of 55:45 per cent. Quality assurance ranked  highest in importance among the formal criteria, whereas  innovation was seen as most important aspect of all criteria.      Graph 5. Formality versus innovation   124    One participant summed up the situation of these findings in the  following statement: It would be easy for learning analytics to  become a numbers game focused on QA, training/instruction and   rankings charts, so promoting its creative and adaptive potential   for lifelong HE/professional-life learning is going to be key for the   sector - unless learning analytics people want to spend all their   lives doing statistical analysis   3.3 Educational data  The section on data investigated the parameters for sharing  datasets in and across institutions. The potential of shareable  educational datasets as benchmarking tools for technology  enhanced learning is explicitly addressed by the Special Interest  Group (SIG) dataTEL of the European Association of Technology  Enhanced Learning (EATEL) and has been demonstrated in [11].  Sharing of learning analytics data is impeded by the lack of some  standard features and attributes that allows the re-use and re- interpretation of data and their applied algorithms [3]. For  researchers, the most important feature was the availability of  added context information (n=43, means 3.42) with a maximum  value of 4 on the Likert scale. Perhaps, equally unsurprising was  that for the manager group sharing within the institution (n=16,  means 3.63) and anonymisation (n=19, means 3.53) were the most  important values. Teachers, on the other hand, valued context  (n=52, means 3.42) and meta-information (n=47 means 3.47) the  most. At the other end of the spectrum, version control was the  least important attribute across all constituencies (n=106, means  2.93). However, despite version control of educational datasets  was ranked the lowest, we still believe that this will play an  important role in an educational data future. Version controlled  datasets will offer additional insights into reflection and  improvements through learning analytics by comparing older and  newer datasets.   Graph 6 illustrates the importance of the given data attributes.  Note that the notion of important outweighs the highly  important overall, which results in a lower means value.      Graph 6. Data attributes   To get an idea of existing educational data, we asked participants  about their institutional IT systems. For learning analytics, the  landscape of data systems will play an important part in  information sharing and comparison between institutions.   In the tertiary education sector alone (Further and Higher  Education), 93.9% (n=92) reported an institutional learning  management system, which made this the most popular data  platform by far. This was followed by a student information  system 62.2% (n=61) and the use of third-party services such as  Google Docs or Facebook 53.1% (n=52). Table 2 below shows a  summary inventory of institutional systems in use across all  sectors of education covered in our demographics.   We assume that the more widely available a type of system is, the  more potential it would hold for inter-institutional sharing of data,  which could be utilised for comparison of educational practices or  success factors. However, such sharing would depend on the  willingness of institutions to share educational datasets with each  other. When asked this question, a majority of people (86.6%,  n=71) were happy to share data when anonymised according to  standard principles.   Table 2. Data systems       What is slightly contradictory is that people who indicated before  that anonymisation was not an important attribute for data are less  inclined to share (n=18, 83.3% yes : 16.7% no) than people who  felt that it was highly important (n=40, 92.5% yes : 7.5% no).   3.4 Methods  Learning analytics is based on algorithms (formulas), methods,  and theories that translate data into meaningful information.  Because these methods involve bias [1], the questionnaire  investigated the trust people put into a quantitative analysis and in  accurate and appropriate results. Within the 100% rating range,  where 100% would indicate total confidence and 0% no  confidence at all, the responses were located at mid-range. Among  the given choices, slightly higher trust was placed on the  prediction of relevant learning resources. This may be due in  analogy to the amazon.com recommendation model, which is  well-known and widely trusted. Other recommendations, such as  predictions on peers or performance were rated rather low. The  percentage on the horizontal axis in graph 7 below shows the level  of confidence.      Graph 7. Confidence in accuracy   125    One comment criticised that it was disappointing that you  included institutional markers, rather than personal ones for the   learners, e.g. while learning outside the institution, which in my   view are much more important and interesting. We are not  aware that the questions actually reflected an institution-centric  perspective. At the same time, we still remain sceptical that  analytics might currently be able to seamlessly capture learning in  a distributed open environment, but mash-up personal learning  environments are on the rise [12] and may soon provide suitable  opportunities for personal learning analytics as has recently been  presented in [6], and [9].   3.5 Constraints  The constraints section focuses on the mutual impact that wider  use of learning analytics may have on a variety of soft barriers  like privacy, ethics, data ownership, data openness, and  transparency of education (see graph 8 below). It should provide  more detailed information on potential restrictions or limitations  for the anticipated benefits of learning analytics. Most of the  participants agree that learning analytics will have some or very  much influence on the mentioned characteristics. Only a few did  not expect any effects on privacy (10.4%, n=96) and ethics (8.8%,  n=102). The majority of the responses believe that learning  analytics will have the biggest impact on data ownership (66.4%,  n=107) and data openness (63%, n=108) followed by more  transparency of education (61.3%, n=111).      Graph 8. Problem areas of learning analytics   After the general weighting of the expected impact on these  constraints, we explicitly asked the participants how they  estimated the influence of learning analytics and automated data  gathering on the privacy rights of individuals by further  describing what we mean with privacy rights in four statements  (see graph 9 below).   From 123 responses it appears that there is much uncertainty  about the influence of learning analytics on privacy rights (cf.  graph 9). The answers are widely spread from no effect at all  until very much effect. But the majority of participants believe  that learning analytics will influence all four privacy dimensions  at least a little. By recoding the given answers into a negative  voting (will have no effect) and a positive voting (will have an  effect) we got a clearer picture of the expectations of the  participants. Regarding statement 1, about two thirds (65.8%,  n=81) believe that learning analytics will affect privacy and  personal affairs. Equally, in statement 3 - ownership and  intellectual property rights -, we can again see a clear majority  (60.1%, n=74) convinced that these will be affected by learning  analytics. Statement 2 - Ethical principle, sex, political and  religious and statement 4 - Freedom of expression are close  together, but with the majority in the negative, thus expressing  they do not think that learning analytics affects these privacy  domains (statement 2 negative effect size 54.5%, n=67; statement   4 negative effect size 53.7%, n=66). Taking further into account  the large presence of dont know answers, we conclude that to  most participants, the impact on privacy is not yet fully  determinable.      Graph 9. Soft issues   To get further information on these pressing soft barriers, we  wanted to know if the participants have already (a) an ethical  board and guidelines that regulate the use of student data for  research. Further, we wanted to know (b) if they trust  anonymisation technologies, and finally (c) how they rate a  concrete example for data access in their own organisation to test  the two answers before.   Regarding (a), the majority of the participants 61% (n=75)  indicated that they have an ethical board in place. Another 18%  (n=22) said that they did not have such a body in place, whereas  21% (n=26) were unsure. Yet to us, such an organisational  infrastructure represents an important starting point for more  extended learning analytics research that is ethically backed up  through proper procedure.   With respect to (b), we went on to ask the participants whether  they thought a (national) standard anonymisation process would  alleviate fears of data abuse. With 49% (n=60), the majority of the  123 participants showed high trust in anonymisation technologies,  whereas 24% (n=29) did not believe that anonymisation would be  effective to reduce data abuse. 21% (n=26) indicated they know  too little to answer this question. This leads us to the interpretation  that in case learning analytics utilises data that is protected by  legislation, participants expect further development of effective  anonymisation techniques to deal with this issue.   After having asked participants about ethical guidance and their  trust in anonymisation, we tested with question (c) how the  participants estimated the use of educational data within their  organisation. We asked them whether institutions should allow  every staff member to view student data internally in the  organisation. In this, we received a significant negative response  from the participants. 43% (n=53) did not want to allow all staff  members to view student data, only 30% (n=37) did not see any  problem with shared access. We also received 15% open text  responses to this question that mainly emphasised the need for  levelled access to student data in compliance with the law and  ethical regulation and the strong need to anonymise data. The  tenor in the comments strongly pointed to a need to know  rational. That is to say that participants felt that only people who  had good reasons to see such data should be permitted to access  them. As one commentary phrased it: Only if legitimately  necessary and only for those who have a need to know.   3.6 Competences  In our section on the competences dimension, we wanted to  identify the key competences connected with learning analytics.   126    We also asked for the confidence experts have in the  independence of learners to exploit learning analytics for their  own benefit.   According to the learning analytics framework we suggested the  following seven key skills: 1. Numerical skills, 2. IT literacy, 3.  Critical reflection, 4. Evaluation skills, 5. Ethical skills, 6.,  Analytical skills, 7. Self-directedness. We wanted to know which  of these skills the participants find important to benefit from  learning analytics Graph 10 shows the spider diagram of the  answers.      Graph 10. Competences   The participants found all mentioned skills rather important for  learning analytics. By way of means ranking with a maximum  value of 4 on the Likert scale, participants identified self- directedness (means 3.53) and critical reflection (means 3.42) as  the most important competences required from beneficiaries.  These were rated as highly important by 59.3% (n=67) and  48.7% (n=57) respectively. Numerical skills (means 2.83) and  ethical thinking (means 2.95) were on the bottom end of the scale.  We consider this in line with the previous answers to ethical  aspects of learning analytics.   In addition to the required skills, we wanted to know whether  participants thought that learners were competent enough to  independently learn from learning analytics reports. It turned out  that a significant majority did not think that learners would be  able to deal with learning analytics reports without additional  support (70.2%, n=85). Only 21% (n=26) believed that learners  were competent enough to do so. We have to admit that we did  not ask the same question with respect to skills required of  teachers. This would have been an interesting comparison at this  point.   4. SUMMARY AND CONCLUSIONS   The current article reported the results of an exploratory  community survey in learning analytics that aimed at extracting  the perceptions, expectations and levels of understanding of  stakeholders in the domain. Divided up into six different  dimensions we came to a number of conclusions which we are  going to present below.   - Stakeholders: Participants identified the main beneficiaries in  learning analytics as learners and teachers followed by  organisations. Furthermore, the majority of respondents agreed  that the biggest benefits would be gained in the teacher-to-student  relationship and that learners would almost certainly require  teacher help to learn from an analysis and for taking the right   course of action. This is rather surprising as learning analytics is  seen by many researchers as an innovative liberating force that  would be able to change traditional learning by reflection and peer  support, thus strengthening independent and lifelong learning.  This latter opinion on independence could be seen in the  objective section of the survey (cf. chapter 3.2 above) where the  majority expressed a preference for learning analytics to pay  special attention to non-formalised and innovative ways of  teaching and learning. Yet, respondents expect less potential  impact on the student-to-student and the teacher-to-teacher  relationships. This current perspective may be affected by the  scarcity of learning analytics applications that demonstrate the  innovative possibilities for learning and teaching. Thus people  may not have a clear point of reference as, for example, is the case  for social networks where an established group of competitive  platforms already exists.   - Objectives: The survey concludes further that research on  learning analytics should focus on reflection support. The attained  results clearly emphasized the importance of stimulating  reflection in the stakeholders about their own performance. This  goal could be supported by revealing hitherto hidden information  about learners, which was the second most important objective. At  the same time more timely information, institutional insights, and  insights into the learning context were other areas of interest to  the constituency.   - Data: Our institutional inventory in chapter 3.3 gives an  overview of the most widespread IT systems. These could be  prioritised by learning analytics technologies to gain an  institutional foothold. They also provide the best ground for inter- institutional data sharing. Anonymisation can perhaps be seen as  the most important enabler for such sharing to happen. It is  emphasised in a number of responses as the second most  important data attribute and confirmed in the willingness of  people to share if data is anonymised. For a clear majority  anonymisation also reduces fears of privacy breaches through  sharing (cf. chapter 3.5). On the other hand, when it comes to  internal sharing with departments and operations units of the  same institution, the use of available data will continue to be an  uphill struggle, and, according to participants, require good  justification. Here, perhaps, a clearer mandate to ethical boards  may help. These are already widely in place.   - Methods: Chapter 3.4 on methods revealed that trust in learning  analytics algorithms is not well developed. We interpret the mid- range return levels as hesitation towards calculating education  and learning. What seems interesting to us is that the widely  interpretable hope for gaining a comprehensive view on the  learning progress was given the highest confidence, but perhaps  this shows wishful thinking rather than a real expectation. Overall  rather low was the expectations of impact on assessment. A  majority of people did not see easier or more objective  assessments coming out of learning analytics (cf. chapter 3.2).  They were also not fully convinced that it would provide a good  assessment of a learners state of knowledge (cf. chapter 3.4).   - Constraints: A large proportion of respondents thought learning  analytics may lead to breaches of privacy and intrusion. Yet, they  ranked privacy and ethical aspects as of lesser importance to  consider (cf. chapter 3.5) or as belonging to further competence  development (cf. chapter 3.6). However, data ownership was  expressed as highly important. This may be interpreted in that  way that if ownership of data lies with the learners themselves,  there is no perceived risk for privacy or ethical abuse. In any case,  it seems that many organisations have ethical boards and   127    guidelines in place. These may come to play an increasingly  important role for institutional data exploitation since a large  number of respondents trust that anonymisation of educational  data is possible but not necessarily sufficient to enable full  internal exploitation of the educational data within an  organisation.   - Competences: In the area of competences, participants mainly  stressed the importance of self-directedness, critical reflection,  analytic skills, and evaluation skills. On the other hand, few  believe that students already possess these skills. This indicates to  us a need to support students in developing these learning  analytics competences. In conclusion of this section we can say,  that the results suggest that there is little faith that learning  analytics will lead to more independence of learners to control and  manage their learning process. This identifies a clear need to  guide students to more self-directedness and critical reflection if  learning analytics should be applied more broadly in education.  This interpretation is quite in contrast with some suggestions  made with respect to empowerment of learners through providing  graphical reflection of the learning process and further access to  additional information regarding their learning progress [4].   5. LIMITATIONS   We are aware of several limitations to both the questionnaire and  the presented results. As has been mentioned in the introduction,  there is a dominance of responses from the Higher Education  sector that makes the study only partly representative for other  educational domains. Another limitation is the virtual absence of  students (undergraduate or secondary) although we did receive a  tiny fraction of responses from lifelong learners. This makes the  results of the survey biased toward a top down perspective on  learning analytics. In an open environment, these shortcomings  may still be interpreted as representative for the expressed interest  and the pervasiveness of the topic in particular constituencies. Or,  they may simply be due to the limited reach social networks like  facebook, linkedin, twitter, etc. have with respect to the  distribution of the questionnaire.   Furthermore, the survey only represents a select few Western  cultures. We need to be aware that substantial differences exist in  educational cultures and that learning is always local. It would  perhaps provide for interesting future research to compare these  results with other dominant education cultures. Furthermore, for a  more comprehensive study, the dissemination of the questionnaire  should be supported by public bodies like school foundations and  governmental institutions as it can be expected that the used  dissemination channels reached a more technically interested  group of the stakeholders.   One hopefully time-restricted limitation is the low awareness of  learning analytics among the target survey group and learners  especially. With the rise of useful and popular learning analytics  applications, we hope that this limitation will ease away over time  and thus yield more concrete insights into the field of attitudes in  learning analytics. As such, this survey can only be taken as an  indicative insight of innovators and early adopters.   To address these limitations and to gain more valid insights into  expectations from and understanding of learning analytics we  intend to do further research and plan to target the K-12 and adult  education sector more specifically. Additionally, investigating the  student perspective more intensely might reveal interesting  contrasts to the above reports.   6. FUTURE RESEARCH   As announced in the beginning, this short survey was conceived  as a first step toward more refined research into perceptions and  potential for learning analytics. After collecting a more  representative and selective dataset, we plan to apply more  advanced analysis methods like the Group Concept Mapping [10]  to further analyse different stakeholder groups and to identify  consensus about particular issues in learning analytics among  them. Group Concept Mapping will allow identifying thematic  groups within learning analytics and it allows making a clear  distinction between different aspects of learning analytics.   Finally, we want to announce that the underlying datasets of this  article and the planed extended version of this dataset will be  made publicly available at the dspace.ou.nl repository (at  http://dspace.ou.nl/handle/1820/3850). In that way, we would like  to encourage the learning analytics community to gain additional  insights from our dataset for the fast evolving of the learning  analytics research topic.   7. ACKNOWLEDGMENTS  We would like to thank all participants in the survey and those  who further disseminated it to their networks. Further, we want to  thank the NeLLL funding body and the AlterEgo project that  sponsored part of the authors efforts.   8. REFERENCES  [1] Anrig, B., Browne, W., Gasson, M. (2008). The Role of   algorithms in profiling. In M. Hildebrandt, S. Gutwirth,  (Eds.) Profiling the European Citizen. Cross-Disciplinary  Perspectives. Springer 2008.   [2] Dawson, S., Heathcote, L. and Poole, G. (2010). Harnessing  ICT potential: The adoption and analysis of ICT systems for  enhancing the student learning experience, International  Journal of Educational Management 24(2) pp. 116-128.    [3] Drachsler, H., Bogers, T., Vuorikari, R., Verbert, K., Duval,  E., Manouselis, N., Beham, G., Lindstaedt, S., Stern, H.,  Friedrich, M., Wolpers, M. (2010). Issues and Considerations  regarding Sharable Data Sets for Recommender Systems in  Technology Enhanced Learning. Elsevier Procedia Computer  Science, 1, 2, pp. 2849 - 2858.   [4] Govaerts, S., Verbert, K., Klerkx, J., Duval, E., (2010).  Visualizing Activities for Self-reflection and Awareness, The  9th International Conference on Web-based Learning, ICWL,  December 7-11, 2010, Shanghai University, China.   [5] Greller, W. & Drachsler, H., (submitted). Turning Learning  into Numbers  A learning analytics Framework.  International Journal of Educational Technology & Society.    [6] Mdritscher, Felix; Krumay, Barbara; Kadlec, Edgar;  Taferner, Wolfgang (2011): On reconstructing and analyzing  personal learning environments of scientific artifacts.  Proceedings of the Workshop on Data Sets for Technology  Enhanced Learning (dataTEL 2011) at the STELLAR Alpine  Rendez-Vous (ARV), La Clusaz, France, March 2011.    [7] Norris, D., Baer, L., Leonard, J., Pugliese, L. and Lefrere, P.  (2008). Action Analytics: Measuring and Improving  Performance That Matters in Higher Education,  EDUCAUSE Review 43(1). Retrieved October 20, 2011:  http://www.educause.edu/EDUCAUSE+Review/EDUCAUS EReviewMagazineVolume43/ActionAnalyticsMeasuringand Imp/162422.   128    [8] Oblinger, D. G. and Campbell, J. P. (2007). Academic  Analytics, EDUCAUSE White Paper. Retrieved October 20,  2011 from  http://www.educause.edu/ir/library/pdf/PUB6101.pdf.   [9] Reinhardt, W., Metzko, C., Drachsler, H., & Sloep, P. B.  (2011). AWESOME: A widget-based dashboard for  awareness-support in Research Networks. Proceedings of the  PLE Conference 2011. July, 11-13, 2011, Southampton, UK.   [10] Stoyanov, S., Hoogveld, B., Kirschner, P.A., (2010).  Mapping Major Changes to Education and Training in 2025,  in JRC Technical Note JRC59079., Publications Office of the  European Union: Luxembourg.   [11] Verbert, K., Drachsler, H., Manouselis, N., Wolpers, M.,  Vuorikari, R., & Duval, E. (2011). Dataset-driven Research  for Improving Recommender Systems for Learning. 1st  International Conference learning analytics & Knowledge.  February, 27 - March, 1, 2011, Banff, Alberta, Canada.   [12] Wild, F., Palmr, M., and Kalz, M. (2011). IJTEL: Special  Issue on Mash-Up Personal Learning Environments. Special  Issue of the International Journal of Technology Enhanced  Learning. Inderscience publishers. March 2011.   [13] Drachsler, H., Bogers, T., Vuorikari, R., Verbert, K., Duval,  E., Manouselis, N., Beham, G., Lindstaedt, S., Stern, H.,  Friedrich, M., Wolpers, M. (2010). Issues and Considerations  regarding Sharable Data Sets for Recommender Systems in  Technology Enhanced Learning. Elsevier Procedia Computer  Science, 1, 2, pp. 2849 - 2858.   [14] Florian, B., Glahn, C., Drachsler, H., Specht, M., & Fabregat,  R. (2011). Activity-based learner-models for Learner  Monitoring and Recommendations in Moodle. In M.  Wolpers, C. D. Kloos, & D. Gillet (Eds.), Towards  Ubiquitous Learning (pp. 111-124). LNCS 6964; Heidelberg,  Berlin: Springer.   [15] Verbert, K., Manouselis, N., Drachsler, H., & Duval, E.  (accepted). Dataset-driven Research to Support Learning and  Knowledge Analytics. (Eds.) Siemens, George and Gaevi,  Dragan. Educational Technology & Society, xx (x), xxxx.   [16] Hildebrandt, M. (2006). Privacy and Identity, Privacy and  the Criminal Law; E. Claes, A. Duff and S. Gutwirth (eds.),  Antwerpen - Oxford: Intersentia 2006, p. 43-58. Available at:   http://www.imbroglio.be/site/spip.phparticle21, accessed 28  June 2011.   [17] Stamper, J.C., Koedinger, K.R., Baker, R.S.J.D., Skogsholm,  A., Leber, B., Rankin, J., & Demi, S. (2010) PSLC  DataShop: A Data Analysis Service for the Learning Science  Community.  In Proceedings of Intelligent Tutoring Systems,  LNCS, Volume 6095/2010:455-456.   [18] Buckingham Shum, S. and Ferguson, R. (2011). Social  Learning Analytics. Available as: Technical Report KMI-11- 01, Knowledge Media Institute, The Open University, UK.  http://kmi.open.ac.uk/publications/pdf/kmi-11-01.pdf   [19] Siemens, G. (2010). What are Learning Analytics Retrieved  9 August 2011 from  http://www.elearnspace.org/blog/2010/08/25/what-are- learning-analytics/.   [20] Romero, C., Ventura, S. Espejo, P.G., & Hervs, C. (2008).  Data Mining Algorithms to Classify Students. In de Baker,  R., Barnes, T. and Beck, J. (eds), Proceedings of the 1st  International Conference on Educational Data Mining,  pages 817.   [21] Reffay, C., & Chanier, T. (2003). How social network  analysis can help to measure cohesion in collaborative  distance learning. Computer Supported Collaborative  Learning (pp. 1-10). Kluwer.   [22] Mazza, R., & Milani, C. (2005). Exploring Usage Analysis in  Learning Systems: Gaining Insights From Visualisations. In  Workshop on Usage Analysis in Learning Systems at the 12th   International Conference on AIED (6 pages). IOS Press,  Amsterdam.   [23] Govaerts, S., Verbert, K., Klerkx, J., & Duval, E. (2010).  Visualizing Activities for Self-reflection and Awareness. In  Luo, X., Spaniol, M., Wang, L., Li, Q., Nejdl, W. & Zhang,  W. (eds), Proceedings of ICWL 2010, LNCS, Volume  6483/2010:91-100.   [24] Butoianu, V., Vidal, P., Verbert, K., Duval, E. & Broisin, J.   (2010). User Context and Personalized Learning: a  Federation of Contextualized Attention Metadata. J.UCS,  16(16):2252-2271.   [25] Butler, D. L., & Winne, P. H. (1995). Feedback and self- regulated learning: A theoretical synthesis. Review of  Educational Research, 65, 245-281          129      "}
{"index":{"_id":"23"}}
{"datatype":"inproceedings","key":"Prinsloo:2012:LAC:2330601.2330635","author":"Prinsloo, Paul and Slade, Sharon and Galpin, Fenella","title":"Learning Analytics: Challenges, Paradoxes and Opportunities for Mega Open Distance Learning Institutions","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"130--133","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330635","doi":"10.1145/2330601.2330635","acmid":"2330635","publisher":"ACM","address":"New York, NY, USA","keywords":"Thirdspace, University of South Africa (Unisa), learning analytics, open university (OU), student walk","Abstract":"Despite all the research on student retention and success since the first conceptual mappings of student success e.g. Spady [12], there have not been equal impacts on the rates of both student success and retention. To realise the potential of learning analytics to impact on student retention and success, mega open distance learning (ODL) institutions face a number of challenges, paradoxes and opportunities. For the purpose of this paper we critique a 'closed' view of learning analytics as focusing only on data produced by students' interactions with institutions of higher learning. Students are not the only actors in their learning journeys and it would seem crucial that learning analytics also includes the impacts of all stakeholders on students' learning journeys in order to increase the success of students' learning. As such the notion of 'Thirdspace' as used by cultural, postmodern and identity theorists provide a useful heuristic to map the challenges and opportunities, but also the paradoxes of learning analytics and its potential impact on student success and retention. This paper explores some of these challenges, paradoxes and opportunities with reference to two mega ODL institutions namely the Open University in the UK (OU) and the University of South Africa (Unisa). Although these two institutions share a number of characteristics, there are also some major and important differences between them. We explore some of the shared challenges, paradoxes and opportunities learning analytics offer in the context of these two institutions","pdf":"Learning analytics  Challenges, paradoxes and  opportunities for mega open distance learning institutions   Paul Prinsloo  TVW10-156, University of South Africa   P O Box 392, Unisa  0003, Pretoria, South Africa   +27(0) 12 429 3683   prinsp@unisa.ac.za    Sharon Slade  Open University   Foxcombe Hall, Boars Hills  Oxford, UK   +44(0)1865 486250   s.slade@open.ac.uk    Fenella Galpin  Open University   Foxcombe Hall, Boars Hills  Oxford, UK   +44(0) 1865 486284   f.a.v.galpin@open.ac.uk          ABSTRACT  Despite all the research on student retention and success since the  first conceptual mappings of student success e.g. Spady [12], there  have not been equal impacts on the rates of both student success  and retention. To realise the potential of learning analytics to  impact on student retention and success, mega open distance  learning (ODL) institutions face a number of challenges,  paradoxes and opportunities.     For the purpose of this paper we critique a closed view of  learning analytics as focusing only on data produced by students  interactions with institutions of higher learning. Students are not  the only actors in their learning journeys and it would seem  crucial that learning analytics also includes the impacts of all  stakeholders on students learning journeys in order to increase  the success of students learning.  As such the notion of  Thirdspace as used by cultural, postmodern and identity  theorists provide a useful heuristic to map the challenges and   opportunities, but also the paradoxes of learning analytics and its  potential impact on student success and retention.    This paper explores some of these challenges, paradoxes and  opportunities with reference to two mega ODL institutions namely  the Open University in the UK (OU) and the University of South  Africa (Unisa). Although these two institutions share a number of  characteristics, there are also some major and important  differences between them.  We explore some of the shared  challenges, paradoxes and opportunities learning analytics offer in  the context of these two institutions.    Categories and Subject Descriptors  K3.1 [Computers and Education]: Computer Uses in Education   distance learning.   General Terms  Management, Measurement, Performance, Design, Human  Factors   Keywords  Learning analytics, Open University (OU), student walk,   Thirdspace, University of South Africa (Unisa)   1. INTRODUCTION  One of the basic premises of learning analytics is that if higher  education institutions optimise and analyse the data they hold on  their students, they can identity and (more) effectively and  appropriately address the challenges that students face, whether  they are at risk, underprepared or high performance students.    Siemens [10] suggests that learning analytics refers to student- produced data and analysis models to discover information and  social connections, and to predict and advise on learning  (emphasis added).  It is true that students produce data and leave  trails that higher education institutions may not fully exploit. To  focus only on the data trails which students produce may result in  the incomplete assumption that they are the primary actors in their  learning journeys. Students trails and data regarding their  activities, actions or non-actions are a useful baseline, but often  institutional decisions, efficiencies and non-action on the side of  the institution impact equally on students choices, and their  actions or non-action. This latter perhaps falls into the category of  academic rather than learning analytics, though both approaches  have many overlapping elements and both are relevant here.   We do not see learning analytics as the panacea which will solve  all the complexities in understanding student success, attrition or  retention. Several authors [7, 8] have cautioned that learning  analytics can very easily serve to increasingly bureaucratise  students learning even further, or serve a panoptical purpose and  culture of increasing surveillance rather than empowering students  and their institution to facilitate more appropriate choices.    In this paper we present two case studies and propose that  learning analytics can at least support student success if we  consider that both students and institutional data trails are found in  the student walk as the space where these two actors meet in a  Thirdspace (as described by various authors) [2, 4, 6, 9, 11].   Our discussions of the potential of learning analytics to help map  and engage with this Thirdspace are set against the concerns  expressed by Tinto [15] who bemoans the fact that, despite all the  research done since the first conceptual mappings of student  success and retention, the impact on success and retention rates  has been minimal. Tinto [14, 15] and others [5] suggest that  student departures are more of a puzzle than we (currently)  accept, and that knowing why students fail does not give us an  equal understanding of why students persist or stay despite failing.    In their attempt to unravel the student departure puzzle [5, 13]  indicate that student success and retention is a multidimensional  phenomenon where a number of interrelated and often     Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy   otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April May 2, 2012, Vancouver, BC, Country.   Copyright 2012 ACM 978-1-4503-1111-3/12/04 $10.00.      130    interdependent variables meet in complex relationships.  In their  socio-critical model for understanding and predicting student  success, retention and throughput, Subotzky and Prinsloo [13] aim  to provide a conceptual map to identity measureable and  actionable data in contrast with data which may ostensibly shape  student success, but which are outside the locus of control of both  students and the institution. They propose that three interrelated  and often interdependent levels of factors impact on student  success namely: individual (academic and attitudinal attributes,  and other personal characteristics and circumstances); institutional  (quality and relevance of academic, non-academic, and  administrative services); and supra-institutional (macro-political  and socio-economic factors) [13].   Subotzky and Prinsloo [13] propose a number of constructs that  underscore their socio-critical conceptual model; and which we  find useful as a shared point of departure for our comparative  analysis and discussion of the role of learning analytics in mega  open distance learning institutions. The constructs are as follows:   1. Students and the institution as situated agents: Success is  seen as the outcome of the mutually influential activities,  behaviours, attitudes, and responsibilities of students and  the institution. The situatedness of their agency relates to  the structural conditions of their historical, geographical,  socio-economic, and cultural backgrounds and  circumstances. Within these structural constraints, both  students and the institution are agents, and not merely  passive recipients or providers of services.   2. The second construct is that of the student walk which  embodies the mutually constitutive interactions and  relations between students and the institution.    3. The notion, amount and role of capital  whether social,  epistemological, intellectual or other forms of capital   provides a basis for understanding the socio-critical nature  of the student walk where mutual engagement and  transformation are shaped by engagement and exchanges.   4. The fourth construct refers to the impact of habitus on the  agency of both students and the institution, where habitus  refers to refers to socially acquired, embodied systems of  dispositions and/or predispositions [3].   5. The fifth construct is the notion that both students and the  institution have inter and intra-relational aspect shaping  their agency. Students intrapersonal relations are shaped by  self-efficacy, attribution and locus of control, while their  interpersonal relations are the multiple networks impacting  and shaping students learning. Self-efficacy, attribution and  locus of control also apply to the institution within three  different domains namely academic, administrative, and  non-academic social domains of institutional life.    6. Student success, as a final construct is more broadly  constructed than just course success, but also refers to  students satisfaction with their learning journeys, optimal  fit between their aspirations and abilities and the  institutions offerings. Student success can also imply not  graduating or completing their initial educational aims.    Though the detail of the mapping of students journeys differs  between the OU and Unisa, the constructs developed by Subotzky  and Prinsloo [13] encompass, from our understanding, a shared  basis for our continued exploration.   In determining the potential for analytics to help us make sense of  students journeys through a Thirdspace, we must also accept  that it is not always feasible, from a student or from an  institutional perspective, to act on what the data may be telling us.   2. THE STUDENT JOURNEY AS  THIRDSPACE   The conceptual model described above illustrates a Thirdspace,  a mostly temporary nexus where students and the institution  engage. In a certain sense, this nexus of engagement is a  temporary diasporic space for both students and institution. The  notion of Thirdspace, liminal or diasporic space is used in a  range of contexts such as identity, multicultural,  phenomenological geography and identity theories discourses by  authors such as Bhabha [2], Brah [4] and Soja [11]. Soja [11]  describes the Firstspace as the material world in which individuals  and communities live; Secondspace as their mental world of  beliefs, assumptions and epistemologies. Thirdspace is the space  where these two worlds merge and become one temporal space. In  the work of Bhabha [2], third space functions as a space where  individuals negotiate and renegotiate their assumptions, beliefs,  identities in a constant space of becoming.    The notion of Thirdspace is not commonly used in describing  the engagement between students and institutions, except for by  Burnapp [6], Whitchurch [16] and in an indirect sense, Barnett  [1]. Burnapp [6] uses the notion of the Thirdspace in describing  international student experiences whilst Whitchurch [16] uses it to  describe the fluidness of academic identity in a digital age.   Barnett [1] refers to the notion of a third world where students  find themselves in their trajectories of being and becoming. In  this so-called third world or Thirdspace, students have left the  known pre-enrolment spaces and move into a space where their  identities, epistemologies and ontologies are shaped by their  engagement with academic and professional discourses. A student  enrolling in higher education moves from often a highly  structured place to an undefined and liminal and unstructured  space [6]. In this Thirdspace students are caught in a liminal  space between what they were and what they are becoming. They  may be labelled as underprepared, at risk, illiterate, or  deficient  and blamed for not fitting in into the world of  higher education. Early conceptual models attempting to  understand and map student success and retention  disproportionately emphasised the responsibility of students to fit  in, to prepare for and ensure that they are sufficiently assimilated  and integrated into the epistemologies and ways of being required  by the higher education of their choice (see for example [5], [13]).    The student walk as Thirdspace is a temporary space where yet  another identity construct and role are imposed on students. This  new identity shapes and is shaped by their other identities as  mothers, professionals, etc. Students and especially distance  education students in ODL settings do not leave their other  identities outside of their learning, but rather find them in ever- increasing networks of identity constructs. On the other hand,  students engagement with their studies and institution has the  potential to shape their multiple identities in often profound ways.   This Thirdspace also has implications for the institution which  provides learning based on students choices, prior knowledge and  aspirations. The success of the ability of the institution to match  the aspirations, prior knowledge and levels of preparedness of  students has a profound impact on the success of students,  attrition and throughput rates.   Although this Thirdspace is actually, in the context of ODL, a  non-place or a space of placelessness [6],  students and other  institutional stakeholders leave traces which, if harvested, can  help us to understand the complexities of student success, attrition  and throughput. Using the actionable intelligence provided by  learning analytics allows this Thirdspace to be a safe and critical   131    non-place of becoming. We suggest that the notion of a  Thirdspace provides useful pointers for understanding the  potential of learning analytics in higher education institutions and  more particularly, in mega ODL institutions.   We now turn briefly to providing short overviews of two different  ODL contexts as basis for our exploration of the challenges,  paradoxes and potential of learning analytics.   3. ANALYTICS AT THE OPEN  UNIVERSITY: A SHORT CASE STUDY  The OU supports around 200,000 students each year and collects  vast amounts of data about its students, the majority of which is  been collated and disseminated to academic units and support  departments by a central unit. This unit provides several services  in support of the University in supplying external reports and in  helping internal staff to better understand student cohorts:    Providing information systems and easy access to student  retention and progression data, and demographic profiles     Delivering and reporting internal and external institutional  surveys (student feedback)     Disseminating institutional data and information analysis    Collaborating internally to undertake ad-hoc projects aimed   at enhancing the quality of the student experience     Supporting internal review processes and external audits    Academic teams typically make use of faculty or module level  information to inform curriculum design, for example, by using  feedback from surveys sent to students at the end of their module.  Other datasets relating to points of withdrawal and student such  information have been used to create, for example, a single  University-wide model of vulnerability based on historical shared  student characteristics.   At a very broad level then, the OU has made good use of ongoing  data to make adjustments to curriculum design and to form a view  of how to provide effective student support. This understanding is  well communicated and has provided a shared understanding of a  model of support as a generic good fit for all students.    Since 2005, the OU has captured all outward and inward  communications with students and tutors. Currently, over 7.5  million contacts have been recorded, each categorized to reflect  the nature of the contact and the resultant outcomes.    Until recently, this dataset has largely been a repository for  student information and has not been widely exploited to extract  cohort information, patterns of behaviour or useful insights into  commonalities between programmes of study, approaches to  assessment and modes of delivery.    In the last two years, greater use has been made of this  information and data captured at registration, to develop a fuller  understanding of the reasons which lead to student contact and the  triggers for student behaviours, which can then be matched to a  variety of anticipatory support behaviours.    In addition, much work has been invested in the OUs ability to  interrogate its Moodle-based VLE system to track student  behaviour and engagement on and between modules.    The OU is now moving toward a tailored, at scale and largely  automated approach to student support that does not assume that a  single model of support fits all, but allows curriculum-based  support teams to provide the most time effective, appropriate  support for their own student cohort.     4. ANALYTICS AT THE UNIVERSITY OF  SOUTH AFRICA: A SHORT CASE STUDY  Unisa reports to various national higher education and legislative  networks on student throughput, module success, and attrition.  Most of the data required relates to programme cohort analysis,  though analyses regarding student profiles and success in  individual modules are also available. Analyses are also available  on request by departments, schools or individual lecturers.    Until recently, most of the analyses were used by institutional  structures for operational planning purposes, and, to a lesser  extent, by departments and/or individual lecturers in planning  module specific interventions or teaching strategies.    Up to 2010, academic and learning analytics at Unisa remained  fragmented. There was no coherent and shared understanding of  student success as a phenomenon, nor any committee or task team  that were either representative of all stakeholders involved in the  development, delivery and support of teaching and learning; nor  having access to appropriate analyses of institutional and module  (course)-specific trends.  Different departments responded in their  individual capacities to increase student success and retention.    Compounding the impact of this fragmented approach was the  fact that the analyses conducted focused more on cohort analyses  in programmes and institution-wide trends, and not necessarily at  module level. In addition, institution-wide interventions and  strategies impacted on individual modules with no input from the  academic and tutoring staff involved in those modules.    However, 2010 saw a major change in the institutional  comprehension of the role and impact of learning analytics. Three  major developments emerged, namely   1. The development and formal acceptance of the socio-critical  conceptual model [13] has provided Unisa with an integrated  and shared framework for understanding and predicting  student success and retention. While there was a general  understanding of the notion of the student walk or student  journey, there was no clear understanding of the  complexities facing both students and the institution in their  reciprocal engagement in a Thirdspace.    Successful implementation of a framework will hugely  depend on the role and function of learning analytics.  Currently the main centralised sources of student data are:    Information provided by students during the application  and registration processes    Submission of assignments    Financial interactions with the University    Student activity on the learning management system   Other data sources, for example, interactions with tutors or  support staff, are not centrally recorded.    2. The second major development in the context of realizing a  future for learning analytics is the development and piloting  of a student tracking system. The aim is to map student risk  on all currently held historical data. This system will  eventually house and track all interactions between students  and the institution and generate automated (where  appropriate) and personal proactive and reactive responses.   3. The third and final development realizing the potential of  learning analytics is the formation of a Student Success  Committee. This comprises the major role-players dealing  with student retention and success ranging from Senate to  administrative, professional and academic departments.    132    5. CHALLENGES, PARADOXES AND  POINTERS  From the above case studies, the following issues emerge:    Both institutions (like most other higher education and ODL  institutions) have huge student datasets.     Perversely, the sheer volume of available data can act as a  constraint rather than as an enabler of better understanding  both student and institutional behaviours.    At present it is not clear whether the two institutions in  question fully understand or have a conceptual map of how  the data is used, by whom, and for what purpose.    Both institutions provide analytical services to a range of  customers but may need a meta-picture of how data is used  and the impact of different strategies based on analyses.     How do overarching institutional goals, for example,  widening participation and open access act with or against  messages provided by analyses That is, mega ODL  institutions are often balancing conflicting drivers.    While the two institutions in question have different structures  and different approaches to the notion of cohort, it is not clear  whether there is an institutional perspective which makes  sense of cohort and module specific trends.     It is not clear how the results of analyses flow through the  organisation, that is, to individuals or support departments and  back    Monitoring and evaluation of support systems based largely  on the output of an analytics approach needs to be ongoing for  support systems to remain effective and optimal. Such  analyses are time intensive.      Both institutions encourage the scholarship of teaching and  learning to increase evidence-based approaches to  interventions aimed at improving student retention and  success. How best then to capture and integrate scholarship  practices into institutional sense making processes     Academics involved in such scholarship may find their efforts  to change delivery and teaching strategies based on found  evidence frustrated by a lack of institutional support.     Although the data may suggest tailoring, it is not practical for  mega ODL institutions to have a multitude of differentiated  support systems in place.   There are however also some pointers for consideration. The use  of analytics at all levels would be more successful if founded on a  shared and institutionally-accepted conceptual understanding of  the nexus or Thirdspace of student and institutional interaction.  Analytics should provide an integrated, coordinated and holistic  platform for all stakeholders to make sense of and find their own  way in supporting student learning and institutional efficiency  recognising interrelations and interdependencies.   It falls outside of the scope of this paper to argue for a centralised  or decentralised approach to analytics, but rather to point to a need  for an integrated, coordinated and holistic approach involving all  stakeholders who can contribute or use the analyses.    6. CONCLUSIONS  Learning analytics aims to help us to teach more effectively by  providing us timely and appropriate actionable data on which to  make choices regarding pedagogy, assessment strategies, student  support interventions and use of technology to mention but a few.    Using the notion of Thirdspace to describe the space where  students and institution meet, we explored some of the challenges,   paradoxes and potential for learning analytics to better support  learning outcomes and student success. If learning analytics is  considered only as a tool, then simply having more information  about our students may not necessarily change the way we teach.  There would be a danger that learning analytics might become  part of the broader bureaucratisation of student learning.    If however, learning analytics is embedded in organisational  culture, systems, and processes, there is the potential to really  impact and shape our approaches to student needs, whether as  individuals or as groups.   Learning analytics is an essential tool for mega ODL institutions  for personalising learning as far as possible for very diverse  groups of students with even more diverse prior experiences,  contexts, aspirations and futures.    7. REFERENCES  [1] Barnett, R. 1996. Being and becoming: a student trajectory.   International Journal of Lifelong Education, 15(2): 72-84.   [2] Bhabha, H. 1994. The location of culture. London:   Routledge.  [3] Bourdieu, P. 1977. Outline of a theory of practice.   Cambridge University Press  [4] Brah, A. 1996. Cartographies of diaspora: Contesting   identities. London: Routledge.  [5] Braxton, J.M. (Ed.). 2000. Reworking the student departure   puzzle. Nashville, TN: Vanderbilt University Press.  [6] Burnapp, D. 2006. Trajectories of adjustment of international   students: U-curve, learning curve, or Thirdspace.  Intercultural Education 17(1):  8193.   [7] Cloggy, W. 2011. The value of analytics in an educational  and learning context. Blog posted on 4 March 2011,   http://ritakop.blogspot.com/2011/03/value-of-analytics-in- educational-and.html. Retrieved 11 October, 2011.    [8] Duval, E. 2011. What to measure When does support  become enslaving Blog posted on 29 August 2011.   http://erikduval.wordpress.com/, retrieved  11 October, 2011.    [9]  Elsden-Clifton, J. 2006.  Constructing Thirdspaces:  Migrant students and the visual arts.  Studies in Learning,  Evaluation Innovation and Development3 (1): 1-11.    [10] Siemens, G. 2010. What is learning analytics.  http://www.elearnspace.org/blog/2010/08/25/what-are- learning-analytics/. Retrieved 11 October 2011.   [11] Soja, E. W. 1996. Thirdspace: Journeys to Los Angeles and  other real and imagined places. Malden, MA: Blackwell.    [12] Spady, W.G. 1970. Dropouts from higher education: An  interdisciplinary review and synthesis. Interchange, 1(1): 64- 85.    [13] Subotzky, G. & Prinsloo, P. 2011. Turning the tide: a socio- critical model and framework for improving student success  in open distance learning at the University of South Africa,  Distance Education, 32:2, 177-193   [14] Tinto, V. 1975. Dropout from higher education: a theoretical  synthesis of recent research. Review of Educational Research  45: 89-125.   [15] Tinto, V. 2006. Research and practice of student retention:  What next Journal of College Student Retention 8(1): 1-19.   [16] Whitchurch, C. 2008. Shifting identities, blurring boundaries:  The changing roles of professional managers in higher  education. Retrieved from  http://escholarship.ucop.edu/uc/item/3xk701cn  on 11  October 2011.     133      "}
{"index":{"_id":"24"}}
{"datatype":"inproceedings","key":"Clow:2012:LAC:2330601.2330636","author":"Clow, Doug","title":"The Learning Analytics Cycle: Closing the Loop Effectively","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"134--138","numpages":"5","url":"http://doi.acm.org/10.1145/2330601.2330636","doi":"10.1145/2330601.2330636","acmid":"2330636","publisher":"ACM","address":"New York, NY, USA","keywords":"academic analytics, analytics, feedback, learning analytics, policy","Abstract":"Despite all the research on student retention and success since the first conceptual mappings of student success e.g. Spady [12], there have not been equal impacts on the rates of both student success and retention. To realise the potential of learning analytics to impact on student retention and success, mega open distance learning (ODL) institutions face a number of challenges, paradoxes and opportunities. For the purpose of this paper we critique a 'closed' view of learning analytics as focusing only on data produced by students' interactions with institutions of higher learning. Students are not the only actors in their learning journeys and it would seem crucial that learning analytics also includes the impacts of all stakeholders on students' learning journeys in order to increase the success of students' learning. As such the notion of 'Thirdspace' as used by cultural, postmodern and identity theorists provide a useful heuristic to map the challenges and opportunities, but also the paradoxes of learning analytics and its potential impact on student success and retention. This paper explores some of these challenges, paradoxes and opportunities with reference to two mega ODL institutions namely the Open University in the UK (OU) and the University of South Africa (Unisa). Although these two institutions share a number of characteristics, there are also some major and important differences between them. We explore some of the shared challenges, paradoxes and opportunities learning analytics offer in the context of these two institutions","pdf":"The Learning Analytics Cycle: Closing the loop effectively  Doug Clow   The Open University   Walton Hall, Milton Keynes   MK7 6AA, United Kingdom   +44 1908 654861   d.j.clow@open.ac.uk   ABSTRACT  This paper develops Campbell and Oblingers [4] five-step model  of learning analytics (Capture, Report, Predict, Act, Refine) and  other theorisations of the field, and draws on broader educational  theory (including Kolb and Schn) to articulate an incrementally  more developed, explicit and theoretically-grounded Learning  Analytics Cycle.    This cycle conceptualises successful learning analytics work as  four linked steps: learners (1) generating data (2) that is used to  produce metrics, analytics or visualisations (3). The key step is  closing the loop by feeding back this product to learners through  one or more interventions (4).   This paper seeks to begin to place learning analytics practice on a  base of established learning theory, and draws several  implications from this theory for the improvement of learning  analytics projects. These include speeding up or shortening the  cycle so feedback happens more quickly, and widening the  audience for feedback (in particular, considering learners and  teachers as audiences for analytics) so that it can have a larger  impact.   Categories and Subject Descriptors   J.1 [Administrative Data Processing] Education; K.3.1   [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning   General Terms  Algorithms, Management, Measurement, Performance, Design,  Economics, Human Factors, Theory, Legal Aspects.   Keywords  Learning analytics, academic analytics, analytics, policy,  feedback   1. INTRODUCTION  A concern with improving learning is foundational within the  field of learning analytics. It was there in Campbell and  Oblingers early work [4] and is there in the definition of learning   analytics from the First International Conference on Learning  Analytics and Knowledge (LAK11) [22]:    the measurement, collection, analysis and reporting of  data about learners and their contexts, for purposes of  understanding and optimising learning and the  environments in which it occurs.   The importance of interventions in learning analytics to close the  feedback loop has been clear in the literature (if not always the  practice) from the birth of the field. Analytics seeks to produce  actionable intelligence [5]; the key is that action is taken.  Campbell and Oblinger [4] thus set out five steps in learning  analytics: Capture, Report, Predict, Act, Refine. Act explicitly  includes making appropriate interventions, and this is echoed  across the literature (e.g. [3, 9, 10, 13]).   This paper builds on these ideas to articulate a Learning Analytics  Cycle that makes the necessity of closing the feedback loop  through appropriate interventions unmistakable. It also draws on  the wider educational literature, seeking to place learning  analytics on an established theoretical base, and develops a  number of insights for learning analytics practice.   2. THE LEARNING ANALYTICS CYCLE      Figure 1, the Learning Analytics Cycle.   The cycle, shown in figure 1, starts with learners. They may be  students studying a course at a university, or informal learners  taking part in a MOOC (a Massive Open Online Course, where  the learners and materials are distributed across the web),     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   134    participants at a research conference, or casual learners browsing  Open Educational Resources (OER).    The next step is the generation and capture of data about or by  the learners  for instance, demographic information about a  potential student logged during a phone call enquiry about study  at a university; login and clickstream data generated in a  VLE/LMS; postings to a forum; assessment results; or even  alumni status. Some can be generated automatically; some  requires a large multidisciplinary team to expend significant  effort.   The third step is the processing of this data in to metrics or   analytics, which provide some insight in to the learning process.  These include visualisations, dashboards, lists of at risk  students, comparisons of outcome measures with benchmarks or  previous cohorts, aggregations, and so on. Again, some can be  generated automatically, but others may take significant effort.  This stage is the heart of most learning analytics projects, and has  been the focus of great innovation in tools, methods and  methodologies  e.g. dashboards, predictive modelling, social  network analysis, recommenders, and so on.   However, the cycle is not complete until these metrics are used to   drive one or more interventions that have some effect on  learners. This might be a dashboard for learners that enables them  to compare their activity with their peers or previous cohorts, or a  tutor making personal contact with a student that a model has  identified to be at very high risk of dropping out.    The cycle can be complete even where the intervention does not  reach the learners who originally generated the data. To take a  very simple example, a teacher reviewing the final grades for a  course and using that to inform how to teach it with the following  cohort is an example of the cycle in action.    Learning analytics does not necessarily include all fours steps. A  project that created reports about learners, but without any  mechanism to feed this back in to an improved learning  experience, would still be a learning analytics project, but not a  very effective one.   3. LEARNING THEORY  The Learning Analytics Cycle has so far been presented as a  development of previous theorisations of learning analytics.  However, it is also more fundamentally, a development of much  older learning theory.   3.1 Kolb  One of the most prevalent learning theories is Kolbs Experiential  Learning Cycle [11], which builds on Dewey and Piaget, and adds  Lewins conception of learning through feedback, which was  inspired by electrical engineering (to which this paper returns).   Kolbs Learning Cycle takes concrete experience as its starting   point; reflective observation on this experience in turn builds   abstract conceptualisation, which feeds through in to active   experimentation, the source of further concrete experience.   There are two levels at which the Learning Analytics Cycle  develops Kolbs cycle.   Firstly, taking the system as a whole, there is a direct  correspondence: actions by or about learners (concrete  experience) generate data (observation) from which metrics  (abstract conceptualisation) are derived, which are used to guide  an intervention (active experimentation).    Secondly, at an individual level, learning analytics can greatly  facilitate the learning process of individuals, by making reflective  observation and abstract conceptualisation easier and more readily  available. These stages correspond to the interventions  component of the Learning Analytics Cycle: the learning  analytics system makes metrics available to an individual, who  observes, conceptualises, and then experiments by making (or  attempting to encourage) some change to learner behaviour.   Kolbs cycle and related ideas have been critiqued extensively  (see e.g. [19]). One main line of critique is that they are reductive  of a holistic, emotional process to a rational, cognitive  phenomenon, which would apply equally to learning analytics.  The other fundamental charge against Kolbs model  that it lacks  strong empirical evidence  is one that learning analytics is in an  excellent position to refute, or should be.   3.2 Schn  Another prevalent theorisation of learning arises from the work of  Donald Schn [1, 17, 18] on reflective practice: how professionals  learn and adapt their behaviour. Schn emphasised the importance  of reflection-in-action and reflection-on-action. In this view,  reflection is a form of feedback process or loop, an iteration  between espoused theories and theories-in-use.   The Learning Analytics Cycle instantiates and enables reflective  learning, at both an individual and organisational level. As with  Kolb, the intervention stage of the Learning Analytics Cycle is  where reflective practitioners compare their espoused theories  with theories-in-use.   One significant conceptualisation developed and popularised by  Schn and Argyris [1] is a distinction between single-loop  learning and double-loop learning. Single-loop learning is aimed  at achieving a set outcome by adjusting practice; double-loop  learning includes the possibility of changing the set outcome.  They use the example of a domestic thermostat: it turns the heat  on or off to achieve its set temperature (single-loop learning); but  a human can adjust the set temperature (double-loop learning).   A learning analytics system may be used simply to attempt to  achieve set goals (single-loop learning); greater value and insight  will come if those goals themselves can be interrogated,  challenged, and developed (double-loop learning). Learning  analytics can thus be a powerful force for informing and  validating learning theories.   3.3 Laurillard  In the UK, another widely-cited theory is Diana Laurillards  Conversational Framework [12], which draws on Kolbs cycle  and Pasks Conversation Theory [14]. In this theory, learning  takes place through a series of conversations between a teacher  and a student (and with other students), underpinned by reflection  and adaptation. These conversations happen on two levels: at the  level of action, and at the level of conception or description.    At an individual level, a Learning Analytics Cycle facilitates the  conversation between the teacher and student: providing  information on the students actions and conceptions, enabling  richer adaptations and feedback in turn from the teachers  constructed environment.   The parallels at a whole-system level are less transparent but  perhaps even richer. The Learning Analytics Cycle can be  conceptualised as enabling conversations at multiple levels,  between multiple actors, with iterative, adaptive feedback.   135    3.4 Other educational literature   The approaches to learning literature (e.g. [16, 21]) identifies  qualitatively different approaches to study  a deep, surface or  strategic approach. This literature has uncovered associations at  the population level between approaches (of the learner and  teacher) and the final outcome, including to widely-used  evaluation questionnaires. Learning analytics offers the  possibility of tracking and researching these associations in real  time, and  most importantly  using them to enhance the  learners experience before they come to the end of their study.   4. ENGINEERING THEORY  The educational theories mentioned above take inspiration from  the cybernetic conception of control theory, and in particular, the  closed-loop control system used widely in engineering of all sorts.   In a closed-loop control system, the output(s) of the system is  measured and then processed by a controller, which in turn makes  an appropriate adjustment(s) to the input(s), creating a feedback  loop. In an open-loop control system, the controller adjusts the  input purely based on its own settings, without taking any account  of the output.   Open-loop control systems are typically quicker, simpler and  easier to implement. However, closed-loop control systems are  more robust at achieving the desired output, particularly when  something within the system changes or the system is complex.    The parallels for learning analytics are readily apparent.  Organising learning without feedback from the outputs is akin to  an open-loop control system: it may be quicker, but the final  output may not be the desired one.   The Learning Analytics Cycle works analogously to a closed-loop  control system: the data generated by or about learners is the  output, which is compared to some reference (e.g. previous  learner data, or a desired outcome), which is then used to drive an  intervention which alters the learning process (input).    5. IMPROVING EFFECTIVENESS: SPEED  AND SCALE  A key consideration for the effectiveness of the feedback cycle is  the speed and scale of the intervention. These are properties of the  entire system: that is, they include the people, policies and  practices connected to the learning.    The people involved can be classified in to the following four  stakeholder groups:    learner  anyone engaged in learning    teacher  anyone engaged directly in facilitating learning:  includes teaching assistants, associate lecturers, adjunct  faculty, faculty, academic staff, and peers in some contexts  such as MOOCs    manager  anyone responsible for the organisation or  administration of teachers: includes departmental-level and  institutional-level management (e.g. managers,  administrators, heads of department, Deans, executive  officers (CxOs), presidents, provosts, vice-chancellors,  rectors and their deputies)    policymaker  anyone responsible for the setting of policy,  whether at a local, regional, state, national, or transnational/  intergovernmental level, and including funders.   As shown in figure 2, the learner is the closest to the learning  activity. They can make very quick changes to their own learning,  but limited changes to others. The teacher is one step away from  the learning activity, but is able to make interventions that may  span several learners. At one further remove is the manager, who  is slowed down by the need to receive second-hand reports from  the teachers, but may well be able to make interventions that  affect more learners than an individual teacher. Furthest from the   learning activity is the policymaker, likely to be slowest of the  four in speed of response, but with the widest responsibility.   Figure 2: The scale (horizontal axis) and speed (vertical axis)   of intervention readily achieved by different stakeholder   groups in a learning analytics system, with proximity to the   learning activity (depth axis).   In a given learning analytics project, there are three strategies by  which the effectiveness of the cycle can be improved.   Firstly, the speed of response can be enhanced, e.g. by real-time  feedback to stakeholders who can act more quickly, such as the  teacher and the learner themselves.   Secondly, the scale of response can be enhanced, e.g. by  providing feedback to a larger number of stakeholders.   Thirdly, the quality of the intervention itself can be improved, e.g.  by testing the intervention to see whether it is effective, through  feedback from the outputs of the learning (Schns double-loop  learning discussed above), or by enabling more stakeholders to  participate. (This has parallels in the Open Source Software  dictum that Given enough eyeballs, all bugs are shallow. [15])  There may well also be an increase in quality if feedback is  directed to those who have the best information about the  learning, such as learners and teachers.   The idea of improving learning through feedback via teachers and  learners themselves is far from new (see e.g. [7, 8]), and notably,  the Signals project at Purdue University [2], perhaps the best- known successful example of learning analytics, has feedback to  learners and teachers at its heart.   6. ASSESSMENT AND INAPPROPRIATE  USE OF METRICS  Assessment can be considered to be a special case of the Learning  Analytics Cycle. In traditional marking, a learner takes a test,  which the teacher marks and returns to the learner (learner  generates data which is processed in to a metric). All too often,  the cycle is not completed at this point.   136    Many learning analytics models treat assessment as the final  outcome measure to be optimised, rather than an interim one.   This is extremely valuable. But assessment data has far more  potential than this: treating it as an input or intermediate variable  can yield extremely valuable insights, and learning analytics  systems can provide assessment-like feedback even in informal  settings (e.g. [6]).   It has been established for at least 40 years [20] that learners  identify the hidden curriculum revealed in the assessment.  Teachers may say they want their students to pursue intellectual  problems, apply their creativity and make mistakes from which  they then learn, but if the assessment tasks predominantly reward  rote learning, learners are likely to study that way.   This is a specific instance of a general risk in learning analytics:  of optimising to a metric that does not reflect what is more  fundamentally desired as an outcome. All metrics carry a danger  that the system will optimise for the metric, rather than what is  actually valued. This danger is not new  Kolb argued  emphatically that learning is best conceived as a process, not in  terms of outcomes ([11] p. 26)  but learning analytics makes it  more pressing.   Thus learning analytics should generate metrics that relate to what  is valued in the learning process. If the final assessment rewards  undesired behaviour, improving the control system to more  effectively optimise the results will make the learning worse.   7. OPENNESS  Being open and transparent benefits learning analytics in (at least)  three different ways.   Firstly, it makes learning analytics more effective. As discussed  above, if more people can see the metrics, there are more people  to understand. Opening up metrics reduces potential barriers to  effective working (e.g. teachers password expired, wrong  permissions set, system complexity and performance).    Secondly, transparency leads to greater social acceptability.  Egregious misapplications of analytics are more likely to be  identified and challenged by stakeholders  and correct, if the  learning system of the organisation does not prevent it.   Thirdly, data protection legislation may make it a legal  requirement.    This is not, of course, simple or straightforward. One cannot  simply make all learners data and metrics available to the entire  world on the web. However, significant potential is lost when  restrictions are added needlessly.   8. CONCLUSION  The Learning Analytics Cycle, with its theoretical grounding,  suggests ways in which learning analytics projects can be made  more effective.    Fundamentally, this requires closing the feedback loop through  effective interventions that reach learners. These loops can be  made more effective if they are faster, or larger in scale.  Strategies to achieve this include considering learners and  teachers as audiences for learning analytics as well as managers  and policymakers.          9. REFERENCES   [1] Argyris, C. and Schn, D.A. 1974. Theory In Practice:  Increasing professional effectiveness. Jossey-Bass.   [2] Arnold, K.E. 2010. Signals: Applying Academic  Analytics. Educause Quarterly. 33, 1 (2010).   [3] Brown, M. 2011. Learning Analytics: The Coming Third  Wave. EDUCAUSE Learning Initiative Brief. (2011).   [4] Campbell, J.P. and Oblinger, D.G. 2007. Academic  Analytics. EDUCAUSE Quarterly. October (2007).   [5] Campbell, J.P. et al. 2007. Academic Analytics: A New  Tool for a New Era. Educause Review. 42, October  (2007), 40-57.   [6] Clow, D. and Makriyannis, E. 2011. iSpot Analysed:  Participatory learning and reputation. Learning Analytics  and Knowledge (LAK)  11 (Banff, AB, 2011).   [7] Dron, J. and Anderson, T. 2009. Lost in social space:  Information retrieval issues in Web 1.5. Journal of  Digital Information. 10, 2 (Apr. 2009).   [8] Dron, J. and Anderson, T. 2009. On the Design of  Collective Applications. 2009 International Conference  on Computational Science and Engineering (2009), 368- 374.   [9] Elias, T. 2011. Learning Analytics: Definitions,  Processes and Potential.   [10] Johnson, L. et al. 2011. The 2011 Horizon Report.   [11] Kolb, D.A. 1984. Experiential Learning: Experience as  the source of learning and development. Prentice Hall.   [12] Laurillard, D.M. 2002. Rethinking University Teaching:  a conversational framework for the effective use of   learning technologies. Routledge Falmer.   [13] Long, P. and Siemens, G. 2011. Penetrating the Fog:  Analytics in Learning and Education. Educause Review.  46, 5 (2011), 31-40.   [14] Pask, G. 1976. Conversational techniques in the study  and practice of education. British Journal of Educational  Psychology. 46, (1976), 12-25.   [15] Raymond, E.S. 2000. The Cathedral and the Bazaar.  Thyrsus Enterprises.   [16] Richardson, J.T.E. 2000. Researching student learning:  Approaches to studying in campus-based and distance   education. Society for Research in to Higher Education  & Open University Press.   [17] Schn, D.A. 1983. The Reflective Practitioner: How  professionals think in action. Temple Smith.   [18] Schn, D.A. ed. 1991. The Reflective Turn: Case studies  in and on educational practice. Teachers College Press.   137    [19] Seaman, J. 2008. Experience, Reflect, Critique: The end  of the Learning Cycles era. Journal of Experiential  Education. 31, 1 (2008), 3-18.   [20] Snyder, B.R. 1971. The Hidden Curriculum. MIT Press.   [21] Trigwell, K. and Prosser, M. 2004. Development and  Use of the Approaches to Teaching Inventory.  Educational Psychology Review. 16, 4 (2004).   [22] 2011. 1st International Conference on Learning  Analytics and Knowledge (LAK 11).       138      "}
{"index":{"_id":"25"}}
{"datatype":"inproceedings","key":"Lauria:2012:MAD:2330601.2330637","author":"Laur'ia, Eitel J. M. and Baron, Joshua D. and Devireddy, Mallika and Sundararaju, Venniraiselvi and Jayaprakash, Sandeep M.","title":"Mining Academic Data to Improve College Student Retention: An Open Source Perspective","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"139--142","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330637","doi":"10.1145/2330601.2330637","acmid":"2330637","publisher":"ACM","address":"New York, NY, USA","keywords":"course management systems, data mining, learning analytics, open source","Abstract":"In this paper we report ongoing research on the Open Academic Analytics Initiative (OAAI), a project aimed at increasing college student retention by performing early detection of academic risk using data mining methods. The paper describes the goals and objectives of the OAAI, and lays out a methodological framework to develop models that can be used to perform inferential queries on student performance using open source course management system data and student academic records. Preliminary results on initial model development using several data mining algorithms for classification are presented.","pdf":"Mining academic data to improve college student  retention: An open source perspective   (Research in progress)  Eitel J.M. Laura, Joshua D. Baron, Mallika Devireddy,    Venniraiselvi Sundararaju, Sandeep M. Jayaprakash   Marist College, Poughkeepsie, NY, USA      ABSTRACT  In this paper we report ongoing research on the Open Academic  Analytics Initiative (OAAI), a project aimed at increasing college  student retention by performing early detection of academic risk  using data mining methods. The paper describes the goals and  objectives of the OAAI, and lays out a methodological framework  to develop models that can be used to perform inferential queries  on student performance using open source course management  system data and student academic records. Preliminary results on  initial model development using several data mining algorithms  for classification are presented.     Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1   [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed   instruction (CMI), Distance learning   General Terms  Algorithms, Measurement, Design, Experimentation.   Keywords  Learning Analytics, Open Source, Data Mining, Course  Management Systems.   1. INTRODUCTION  Academic or learner analytics has received significant attention  within higher education, including being highlighted in the  recently released 2011 Horizon Report [4].  This interest can, in  part, be traced to the work at Purdue University which has moved  the field of academic analytics from the domain of research to  practical application through the implementation of Course  Signals.  Results from initial Course Signal pilots between fall  2007 and fall 2009 have demonstrated significant potential for  improving academic achievement [1].  Despite this early success,  academic analytics remains an immature field that has yet to be  implemented broadly across a range of institutional types, student  populations and learning technologies [2].  The Open Academic  Analytics Initiative (OAAI), supported by a grant from  EDUCAUSEs Next Generation Learning Challenges program, is   developing and deploying an open-source ecosystem for academic  analytics as means to further research into this emerging field.   This paper will focus on two of the five primary objectives of the  OAAI: (a) research into the portability of predictive models for  student performance; and (b) the development and initial  deployment of an open source model.    To support real-world adoption, OAAI bases its development on  open-source technologies already in widespread use at  educational institutions, and on established protocols and  standards that will enable an even wider variety of existing open- source and proprietary technologies to make use of OAAI code   and practices.    The OAAI analyzes student event data generated by Sakai  Collaboration and Learning Environment (CLE). The Sakai CLE  is an enterprise-level open-source teaching, learning, research,  and collaboration software platform initially developed in 2004  by a core group of five institutions (Indiana University, MIT,  Stanford University, University of Michigan, and UC Berkeley).  Today, Sakai is in use in hundreds of institutions around the  world and supported by a vibrant community of developers,  designers, educators, and commercial support vendors. Predictive  models are developed using the Pentaho Business Intelligence  Suite (http://www.pentaho.com/), perhaps the worlds most  popular open source BI suite, with integrated reporting,  dashboard, data mining, and data integration capabilities. Pentaho  includes Weka [8], an open source, Java-based sophisticated data  mining tool with growing popularity in the data mining  community, which is a central piece in the development and  testing of predictive models within the OAAI.     An initial set of predictive models for student performance has  been developed using Fall 2010 data from Marists Sakai system,  along with student aptitude and demographic data.  These models  have been deployed using both open-source Weka and IBMs  SPSS Modeler to help ensure compatibility between data mining  tools.  At the conclusion of the OAAI, these predictive models  will be released under a Creative Commons/open-source license  through the OpenEdPractices.org web site for other institutions to  use.   The models will be published using the vendor-agnostic  XML-based standard Predictive Modeling Markup Language  (PMML) which will allow for the importing of models into a  range of other analytics tools.  Over time this will facilitate an  open-source community effort to enhance the predictive models  using new datasets from different academic contexts as well as  new analytic techniques.   To explore the effectiveness of the Predictive Model for Student  Performance, a series of pilots will be run over spring 2012 at  three partner institutions, College of the Redwoods (2-year  community college), Cerritos College (2-year community college)     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   139    and Savannah State University (Historically Black College and  University).  To support these pilots a Sakai Student Effort Data  (SED) API has been developed that captures the user activity data  Sakai already records to its event logs and expose it through a  secure standard interface for use by both open-source  (Pentaho/Weka) and proprietary external academic analytics  tools, such as IBM SPSS Decision Management for Student  Performance and SunGard Higher Education Course Signals.   Longer-term, Student Information System (SIS) data extraction  will be automated and enhanced by leveraging the recently  released IMS Global Learning Information Services or LIS  standard to facilitate data extraction from Student Information  Systems (SIS).     Table 1. Correlations between course grades and CMS data    Marist  Fall 2010  N=18968 N=27276  Correlation 0.147  Significance 0.000(**)  N 11195  Correlation 0.098 0.112  Significance 0.000(**) 0.000(**)  N 7651 19205  Correlation 0.133 0.068  Significance 0.000(**) 0.000(**)  N 1552 7667  Correlation 0.233 0.061  Significance 0.000(**) 0.000(**)  N 1507 7292  Correlation 0.146 0.163  Significance 0.000(**) 0.000(**)  N 3245 4309  Correlation 0.161 0.238  Significance 0.000(**) 0.000(**)  N 1423 4085  (**) Significant at the 0.01 level (2-tailed)  Marist data uses ratios over course mean instead of frequencies  (no values   reported)  Campbell   (2007)  Undergraduate  CMS event   frequencies  Assign.   Submitted  Asssmts   Submitted  Course  Grade  Sessions   Opened  Content   Viewed  Discussions   Read  Discussions   Posted     One of the initial objectives of the OAAI has been to research into  the portability of predictive models used in academic analytics  to better understand how models developed for one academic  context (e.g. large research university), can be effectively  deployed in another (e.g. community college).  This component of  the project is building on the pioneering work of John Campbell  whose dissertation research at Purdue University investigated the  predictive power of CMS usage data, student aptitude (e.g. SATs  and ACT standardized test scores) and student demographic data  with regards to student academic success in courses [3].  We have  applied similar analytical techniques using Fall 2010 CMS data  from Sakai (http://sakaiproject.org), an open-source Course  Management System (CMS) started in 2004 and now in use in  hundreds of institutions around the world, in production at Marist  College to investigate whether similar correlations are found.     Although Marist College and Purdue University differ in obvious  ways (e.g., institutional type and size) they do share a number of  similarities which are particularly pertinent to this study.  These  include (2010 data) percentage of students receiving federal Pell   Grants (Marist 11%, Purdue 14%), percentage  Asian/Black/African American/Hispanic students (Marist 11%,  Purdue 11%), and ACT composite 25th/75th percentile (Marist  23/27, Purdue 23/29) [6].     Table 1 shows similarities in correlation values between course  grades and CMS frequencies when comparing both institutions.  Only comparable metrics between CMS systems (Blackboard in  the case of Purdue and Sakai at Marist) have been displayed. As  in the case of Purdue, all these metrics are found to be  significantly correlated with course grade, with rather low  correlation values. Thus, our analysis, as detailed above, provides  an initial insight with regards to how portable models may be  with regards to institutional type and size.   2. METHODOLOGICAL FRAMEWORK  The data mining models considered in our work are based on  supervised learning (classification) techniques given that labeled  training data is available (data sets used for training purposes  carry both input features describing student characteristics and  course management system events, as well as student academic  performance).  The goal is to discriminate between students in  good standing and students that are not doing well (a binary  classification process)   Our methodological framework consists of six phases, namely  Collect data, Rescale/Transform Data, Partition Data, Balance  Training Data, Train Models, and Evaluate Models using Test  Data. The first four phases deal with preparing the input data used  to build (train) and subsequently evaluate (test) models.  Trained  and tested models can then be used to score incoming data.   Collect Data: Student demographic data and course enrollment  data  is extracted from the student records system as well as from  Sakai  (Open source CMS) .  Identifying student information is  removed during the data extraction process Sakai logs data of  individual course events tracked by each of the tools used by an  instructor in a given course shell (e.g. Sessions, Content,  Discussion Forums, Assignments, Assessments) as well as scores  (grade contributions)  on gradable events recorded by the  Gradebook tool.    Rescale/Transform Data: Data is recoded / processed according to  specific needs of the classification model building process. The  end product is a data set that collects data of each  course taken by  each student in a given semester, augmented with student  demographic data, CMS event data  and partial scores derived  from (Gradebook) data. The target (class) variable named  Academic Risk establishes a threshold of questionable academic  performance (e.g. a cutoff of a C grade or lower defines poor  academic performance; a grade above C defines good academic  performance).    This stage is also concerned with the removal of outliers, handling  of missing data, and addressing the issue of variability among  courses in terms of assessment and student activity. The  aforementioned variability is dealt with by replacing counts with  ratios computed with respect to the average metric for the full  course. An aggregated score is derived from partial (Gradebook)  scores on gradable events. Once again the purpose is to shave  variability across courses and compute a metric that can be used  to make early predictions on student academic performance a few  weeks into the semester.     140    Partition Data: input data is randomly divided in two datasets: a  training data set, and a test data set. The training data set is used  to build the models. Models are then tested using test data to  compute a realistic estimate of the performance of the model(s) on  unobserved data. We use a ratio of 70% of the data used for  training, and 30% testing, following standard data mining  practice.   Balance Training Data: The input data used in the binary  classification process is typically unbalanced, as there are usually  (many) more students in good academic standing than students at  academic risk. In such case where class values are present in  highly unequal proportions the number of student-at-academic- risk cases may be too small to render useful information from  what distinguishes from good students (the dominant class value).   Therefore records of students at academic risk in the training data  set are oversampled to level the proportion of classes, and  therefore improve the performance of the trained model at  detecting such cases. The test data set maintains the original  proportions of class values.   Build Predictive Models:   We train different models with the  training dataset, using different statistical and machine learning  processes. We chose three classifiers for comparison purposes:   logistic regression, support vector machines, and C4.5 decision  trees as they are state of the art robust classification methods that  can deal with both categorical and continuous features. Logistic  regression is a highly popular parametric classification method,  where the target value is a logit function of the linear combination  of the predictor features. The C4.5 [5] decision tree algorithm is a  non-parametric classifier that learns rules from data. Support  Vector Machines (SVM) are powerful discriminative models  initially proposed by Vapnik [7], that classify data in categories  by finding an optimal decision boundary that is as far away from  the data in each of the classes as possible.    Evaluate Models: Trained models are evaluated used test data  using measures of predictive performance derived from the  confusion matrix that yields counts of true positives (TP), true  negatives (TN), false positives (FP) and false negatives (FN).  Given the unbalanced nature of classes, the overall accuracy  (TP+TN)/(TP+TN+FP+FN) is not a good metric for evaluating  the classifier, as it is dominated by the  student-in-good-standing  class (TN+FP). We therefore appeal to two other accuracy  metrics: sensitivity (TP/(TP+FN)), which measures the ability of  the classifier to detect  the class of interest (academic risk); and  specificity (TN/(TN+FP)) that measures the number of false  alarms raised by the classifier.   3. EXPERIMENTAL SETUP  A data sample corresponding to Fall 2010 undergraduate students  was gathered from four different sources: Students biographic  data and course related data; Course management (Sakai) event  data and Sakais Gradebook data. Datasets were joined and data  was cleaned, recoded, and aggregated to produce an input data  file of 3877 records corresponding to courses taken by students.  All features considered in the input dataset are listed in Table 2.   FTPT is a flag indicating whether the student is full-time or part- time; ACADEMIC _STANDING identifies probation, regular, or  Deans list students; ENROLLMENT is the course size.  RMN_SCORE is the aggregated metric derived from partial  (Gradebook) scores described in the previous section.  R_SESSIONS and R_CONTENT_READ are Sakai event metrics:   R_SESSIONS measures the number of Sakai course sessions  opened by a student compared to the course average;  R_CONTENT_READ measures the number of content resources  viewed by a student compared to the course average.      Table 2. Features (predictors and target) in input dataset   Feature Type Feature Name  P re  d ic  to rs  GENDER, SAT_VERBAL,   SAT_MATH, APTITUDE_SCORE,   FTPT,  CLASS,  CUM_GPA,   ENROLLMENT, ACADEMIC   _STANDING, RMN_SCORE,   R_SESSIONS, R_CONTENT_READ  Target ACADEMIC_RISK (1 = at risk; 0  student in good standing)    Experiments were conducted using Weka 3.6 and IBM SPSS  Modeler 14.2. For each of these tools a flow of execution was  developed to perform the experiments. The experiments followed  these guidelines:   (i) Out of the input dataset, generate five different random  partitions (70% for training, 30% for testing) by varying the  random seed   (ii) Balance each training dataset by oversampling records with  class ACADEMIC_RISK=1    For each balanced training dataset and each of three classification  algorithms (Logistic Regression, C4.5 Decision Tree, SVM), train  a predictive model, 5 x 3 = 15 models all in all. For the purpose of  this experimental work a radial basis function (RBF) kernel was  considered for the SVM algorithm, with a gamma parameter of  0.2; the regularization parameter C was set to 10.   (iii) Using each corresponding test dataset , evaluate each   classifiers performance  by measuring their predictive  performance (sensitivity, specificity)   (iv) Produce summary measures (mean  and standard error)     4. RESULTS  Table 3 displays the   assessment   of   predictive performance   of  all three classifiers over five different trials. Predictive  performance is summarized as a point estimate (mean value) and  an error bar.   Both the logistic regression and the SVM algorithms considerably  outperform the C4.5 decision tree in terms of their ability to  detect students at academic risk: the logistic regression classifier  attains a mean sensitivity of 87.67% on the test data set, and the  SVM yields 82.60%. This means that these algorithms detect,  respectively, 87.67% and 82.60% of the student population at  risk.   In terms of specificity, the logistic regression classifier attains a  mean value of 89.51% on the test data set, and the SVM yields  90.51%. This means that these algorithms produce, respectively,  10.35% and 9.51% of false positives on the test data. These  values are moderately high, especially when compared with the  specificity scores of the C4.5 classifier (97.03%, meaning that  less that 3% of the test data are false positives)   141    We performed and assessment of the relative predictive power of  the predictors under consideration. This helps to focus the  modeling efforts on those predictors that matter most and consider  ignoring those with low predictive power.  For logistic regression  the RMN_SCORE stands in first place, followed closely by  ACADEMIC_STANDING and CUM_GPA; in third place  R_SESSIONS and SAT_VERBAL. For the SVM classifier  RMN_SCORE occupies the first place, followed by CUM_GPA,  ACADEMIC_STANDING, R_SESSIONS and SAT_VERBAL.  The Decision tree follows a similar pattern, although the relative  difference in predictive performance of the predictors under  considerations seems to be minimal. This finding, paired with the  low sensitivity values exhibited by the decision tree classifier,  demands further analysis. ACADEMIC_STANDING and CUM  GPA are typical predictors of academic performance, as descried  in the literature. The use of the RMN_SCORE metric as a  predictor seems promising if partial grades (final grade  contributions of gradable events, such as assignments, or tests) are  available at prediction time, but its validity and usefulness require  further investigation. CMS events appear to be second tier  predictors when compared to the performance metrics described  above.   Table 3.  Results of the Classification Performance Analysis   Classif   Algorithm Train Test Train Test  1 95.91% 86.15% 90.85% 90.78%  2 93.94% 77.46% 91.43% 91.35%  3 92.64% 86.30% 90.47% 88.89%  4 94.51% 77.78% 91.50% 90.71%  5 95.83% 85.29% 90.76% 90.81%  Mean 94.57% 82.60% 91.00% 90.51% SE 1.37% 4.56% 0.45% 0.94%  1 100.00% 66.15% 99.92% 96.24%  2 99.39% 61.97% 99.88% 97.67%  3 100.00% 58.90% 99.88% 96.91%  4 100.00% 55.56% 99.92% 96.96%  5 99.40% 54.41% 99.88% 97.37%  Mean 99.76% 59.40% 99.90% 97.03% SE 0.33% 4.80% 0.02% 0.54%  1 92.98% 86.15% 89.50% 89.17%  2 89.09% 88.73% 90.06% 90.33%  3 90.80% 90.41% 89.51% 88.36%  4 92.07% 83.33% 90.21% 89.33%  5 91.07% 89.71% 89.55% 91.09%  Mean 91.20% 87.67% 89.77% 89.65%  SE 1.46% 2.91% 0.34% 1.06%  S u  p p  o rt   V ec  to r   M a  ch in  es C  4 .5   D ec  is io  n  T  re es  Trial  L o  g is  ti c   R eg  re ss  io n  Sensitivity Specificity     5. CONCLUSION  This paper reports on the goals and objectives of the Open  Academic Analytic, providing a detailed description of the  methodology used to develop predictive models in academic  analytics using and open source platform. This research derives  its motivation from the need of introducing model development  approaches that can be used in practical settings to predict  academic performance and carry out early detection of students at  risk .The methodology presented in this research has been initially  applied on real-world data extracted from Marist College and  some preliminary results are reported As the project progresses,   this analytical framework for academic success will be deployed  using data of other institutions and will overtime be enhanced  through open-source community collaboration. The goal is to  advance our understanding of technology-mediated intervention  strategies by investigating the impact that engagement in an  online academic support environment has on student success.    We hope that this initiative is imitated by other higher education  institutions as a template to facilitate development of predictive  models for early detection of academic risk.   6. ACKNOWLEDGEMENTS  This research is supported by EDUCAUSEs Next Generation  Learning Challenges, funded through the Bill & Melinda Gates  Foundation and The William and Flora Hewlett Foundation. It is  also partially supported by funding from the National Science  Foundation, award numbers 1125520 and 0963365.   7. REFERENCES  [1] Arnold, Kimberly E. Signals: Applying Academic   Analytics, EDUCAUSE Quarterly, Volume 33, Number 1,  2010.    [2] Baepler, P., Murdoch, C.J. (2010, July). Academic Analytics  and Data Mining in Higher Education.    [3] Campbell, J. P. (2007). Utilizing Student Data within the  Course Management System to Determine Undergraduate  Student Academic Success: An Exploratory Study (Doctoral  dissertation, Purdue  University, 2007).  (UMI No. 3287222).   [4] Johnson, L., Smith, R., Willis, H., Levine, A., and Haywood,  K., (2011). The 2011 Horizon Report.Austin, Texas: The  New Media Consortium.   [5] Quinlan, J.R., C4.5 : programs for machine learning. The  Morgan Kaufmann series in machine learning. 1993, San  Mateo, Calif.: Morgan Kaufmann Publishers.    [6] U.S. Department of Education, National Center for  Education Statistics. Integrated Postsecondary Education  Data System, Fall 2010.  Retrieved February 15, 2011 from  http://nces.ed.gov/collegenavigator.    [7] Vapnik, V.N., The nature of statistical learning theory. 2nd  ed. Statistics for engineering and information science. 2000,  New York: Springer.   [8] Hall, M., Frank, E., Holmes, G.,Pfahringer, B., Reutemann,  P., Witten, I.H. (2009); The WEKA Data Mining Software:  An Update; SIGKDD Explorations, 11(1)     8. AUTHORS ADDRESSES  Eitel J.M. Laura (corresponding author), Mallika Devireddy,  Venniraiselvi Sundararaju, Sandeep M. Jayaprakash: School  of   Computer    Science  and   Mathematics,    Marist   College,   Poughkeepsie, NY   12601, USA.   Joshua D. Baron, Senior Academic Technology Officer, Marist    College,  Poughkeepsie, NY 12601. USA   Email: {Eitel.Lauria, Josh.Baron, Mallika.Devireddy1,  Vennirai.Sundararaju1, Sandeep.Jayaprakash1}@marist.edu      142      "}
{"index":{"_id":"26"}}
{"datatype":"inproceedings","key":"Santos:2012:GVA:2330601.2330639","author":"Santos, Jose Luis and Govaerts, Sten and Verbert, Katrien and Duval, Erik","title":"Goal-oriented Visualizations of Activity Tracking: A Case Study with Engineering Students","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"143--152","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330639","doi":"10.1145/2330601.2330639","acmid":"2330639","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, reflection, visualization","Abstract":"Increasing motivation of students and helping them to reflect on their learning processes is an important driver for learning analytics research. This paper presents our research on the development of a dashboard that enables self-reflection on activities and comparison with peers. We describe evaluation results of four iterations of a design based research methodology that assess the usability, use and usefulness of different visualizations. Lessons learned from the different evaluations performed during each iteration are described. In addition, these evaluations illustrate that the dashboard is a useful tool for students. However, further research is needed to assess the impact on the learning process.","pdf":"Goal-oriented visualizations of activity tracking: a case study with engineering students  Jose Luis Santos, Sten Govaerts, Katrien Verbert, Erik Duval Departement Computerwetenschappen, K.U.Leuven  Celestijnenlaan 200A B-3001 Leuven, Belgium  {joseluis.santos, sten.govaerts, katrien.verbert,erik.duval} at cs.kuleuven.be  ABSTRACT Increasing motivation of students and helping them to reflect on their learning processes is an important driver for learn- ing analytics research. This paper presents our research on the development of a dashboard that enables self-reflection on activities and comparison with peers. We describe eval- uation results of four iterations of a design based research methodology that assess the usability, use and usefulness of different visualizations. Lessons learned from the different evaluations performed during each iteration are described. In addition, these evaluations illustrate that the dashboard is a useful tool for students. However, further research is needed to assess the impact on the learning process.  Categories and Subject Descriptors H.5.2 [Information interfaces and presentation ]: User interfaces; K.3.2 [Computers and Education]: Computer Science Education  General Terms Design, Experimentation, Human Factors  Keywords Learning analytics, Visualization, Reflection  1. INTRODUCTION Increasing student motivation and assisting students with  self-reflection on their learning processes is an important driver for learning analytics research. Student motivation can improve when students can define their own goals [30]. Visualizations of time spent and resource use can improve awareness and self-reflection [15]. Learning management systems (LMS) track most of the user interaction that can be used for learning analytics. However, many of the activ- ities take place outside of the LMS, such as brainstorming or programming activities.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK12 29 April - 2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM 978-1-4503-1111-3/12/04 ...$10.00.  This paper presents the first results of a case study in a problem solving and design course for second year engi- neering students at the Katholieke Universiteit Leuven. In this course, the students have to develop software and go through the different phases of software development pro- cess, such as design, programming and reporting. To this end, they use tools such as LibreOffice1, the Eclipse IDE2  and Mozilla Firefox3. They have to share tasks and respon- sibilities between group members. Controlling the risks and evolution of such tasks is part of the assignment.  We developed a dashboard with visualizations of activity data. The overall goal of this dashboard is to enable stu- dents to reflect on their own activity and compare it with their peers. The time spent with different tools, websites and Eclipse IDE documents are tracked by RescueTime4  and the Rabbit Eclipse plug-in5. The collected information is displayed in a dashboard containing goal-oriented visu- alizations. In the visualizations, the students can filter by different criteria, such as course goals and dates. Such filters allow contextualization of the visualized data for the user. Linking the visualizations with the learning goals can help students and teachers to assess whether the goal has been achieved [12].  The dashboard is developed using the design-based re- search methodology, which relies on rapid prototyping, de- ployment of artifacts and observation in iterative cycles.  The paper is organized as follows: in the next section, we present related work. Section 3 presents the research methodology. The four iterations of the design process are discussed in sections 4, 5, 6 and 7. Future work and conclu- sions are presented in Section 8 and 9.  2. RELATED WORK Learning analytics considers the analysis of communica-  tion logs [33, 6], learning resources [25], learning manage- ment system logs and existing learning designs [21, 32], and the activity outside of the learning management systems [29, 9]. The result of this analysis can be used to improve the creation of predictive models [37, 13], recommendations [42] and reflection [15].  This paper focuses on activity outside learning manage- ment systems using existing tracking tools. Self-tracking  1LibreOffice, http://www.libreoffice.org/ 2IDE Eclipse, http://eclipse.org 3Mozilla Firefox, http://www.mozilla.org/ 4RescueTime, http://rescuetime.com/ 5Rabbit Eclipse plug-in, http://code.google.com/p/rabbit- eclipse/  143    tools can be used for capturing activities of students with different tools. The goal is to help students learn how they and other students are using their applications to achieve concrete goals.  Self-tracking is becoming popular in many domains, in- cluding Personal Informatics [20]. Applications in these do- mains help people understand their habits and behavior, through tracking and visualization, e.g. for sleep tracking and email communication patterns. Tracking of health data can motivate users with fitness exercises [31, 4] and enable early diagnosis of illness [39, 2, 1, 16]. Within companies, tracking and visualizations are used to analyze business and manufacturing processes [35], as well as productivity [3]. Be- hind these tools are communities where users can share ex- periences, publish their tracking data in social networks or compare the data with others. In a learning context, stu- dents and teachers are part of a community. These tools can play an important role to share and learn from their behav- ior with applications to achieve the goals of the course.  Khan Academy enables tutors to check progress of stu- dents [8]. A dashboard is used where a table provides a goal status overview per student. For every student, a time- line shows the distribution of achieved goals and a bar chart visualizes the time spent with different kinds of resources.  Other learning dashboards use pie charts to describe the access distribution of different user roles, simple plots to ex- press time spent and tables to indicate the success rate for assignments [23]. In adaptive learning environments, dash- boards contain box plots to compare grades and averages of users who have followed different paths [5]. In mashup learning environments, pie charts have been used to repre- sent knowledge in different areas [24]. Tree visualizations are useful to express learning paths and to describe prereq- uisites. Each path can represent a knowledge area or subarea in a domain [26, 27]. In addition, there are models explor- ing ways to analyze electronic traces to create group models that can operate as mirrors which enable the individuals and teams to reflect on their progress through visualizations [41, 18].  The work presented in this paper focuses on tracking ac- tivity from different applications. Our dashboard uses dif- ferent trackers that generate different kinds of data and ap- plies different visualization techniques. The overall goal of this mashup of visualizations is to enable students to learn how they are using the tools and how much progress they make towards goals in comparison with peers.  3. RESEARCH METHODOLOGY The design-based research methodology has been applied  to conduct this research. This methodology relies on rapid prototyping to evaluate ideas in frequent short iteration cy- cles [43]. The approach enables to collect both qualitative and quantitative evaluation data during the whole software design process [28].  In the two first iterations, we developed a paper-based and a digital prototype. The evaluation of those iterations collected qualitative data from interviews and user observa- tions of 15-30 minutes using the think-aloud protocol [19]. Six teachers and teaching assistants participated in the first iteration and 5 in the second iteration. Evaluations with these participants are useful to collect requirements and to identify potential usability issues with the interaction tech- niques.  The third and fourth iteration are conducted by a mixed research evaluation methodology with questionnaires and open ended questions. In these iterations, we conducted the evaluations with 36 and 10 students, respectively. These questionnaires focused on concrete aspects of the application and allowed statistical analysis of the evaluation data.  4. PAPER PROTOTYPE Paper prototyping is an important first step in user inter-  face design to get quick feedback [28] and minimize costs in the software design process [11].  4.1 Design and implementation User activities and their visualization need to be related  to learning goals in order for teachers or students to be able to reflect and make decisions. Linking these visualizations to the intended goals allows to assess whether these goals have been achieved [12].  The design of the paper prototype focuses on the above- mentioned use cases: (a) Students can reflect on different visualizations contextualized by the learning goals of the course and (b) enable social support through communica- tion between students and teachers.  The first use case is addressed by using visualizations of user activities. More specifically, we visualize the behavior of students with different tools they are using for course ac- tivities (e.g. Eclipse IDE for programming and Microsoft Word for writing) to gain insight into what students have done to achieve a goal. To this end, the students and status of the goals are visualized as a table (visualization 1 Figure 1), as such visualization is one of the simplest ways to get an overview of the course [36]. Students are listed in the first column and the rest of columns represent the goals of the course and their status. A timeline [17] with bubbles (visualization 3) represents the number goals over time. Vi- sualization 5 is a timeline that shows the number of events or time (depending of the tracked source of information) per tool that the user has used along the time to achieve the goal. Finally, visualization 5, 7 and 8 display time spent or number of events per weekday, actions and different items.  All visualizations together in the dashboard enable filter- ing information from a generic perspective (table with goals and students) to a more detailed description (type of doc- uments and activity used), following the visual information seeking mantra [38]. This prototype enables users to interact with the visualizations, i.e. if a user clicks on the Monday bar (visualization 6), then visualization 7 and 8 displays re- lated information to the clicked day.  The second use case is addressed by providing chat func- tionalities for communication between teachers and students (number 2 and 4 in Figure 1). To enable social support, communication between users and sharing experiences can help users to achieve their goals. We provide two widgets intended for this purpose. One widget shows publicly the message and the other is reserved for private communica- tion. The communication is always related to a specific goal contextualizing the scope of the conversation.  4.2 Evaluation  4.2.1 Demographics and Evaluation Setup Users were interviewed and observed during 15-30 min-  utes. They had to perform different predefined tasks such  144    Figure 1: Paper prototype  as filtering goals of this week. The think-aloud protocol was applied.  The paper prototype was evaluated with six people (1 fe- male and 5 males computer science teachers and assistants). Three participants were between 25-30 years old and two participants between 40-50.  4.2.2 Evaluation results and discussion In this subsection, we introduce first the more remark-  able problems and suggestions of the users, and finally the proposed solutions.  Three issues were highlighted with visualization 1. First, the headers in the table are the titles of the goals. The size of the table increases proportionally to the number of goals. If the number of goals is high, the user will not be able to get easily an overview due to the size of the table. Although the user can filter the goals on the table restricting the period of time, it requires additional steps for the user and affects the usability of the application. Second, the filtering feature is defined by drop-down lists for the day, month and year and requires too much clicks. Third, the table is showing a pop-up with static information when the mouse hovers over a cell. Pop-ups showing always the same information were identified as redundant by the users. In addition, users requested more sorting options for the table.  There were several problems to understand visualization 3. The visualization shows redundant information compared with the table. The table also includes goals and users can filter by time, so they can obtain the same information with this visualization. Users expected some additional informa- tion that they did find in the table. Although the problem of this visualization is the redundant information, from previ- ous evaluations [34] we also know that visualizations can be difficult to understand depending on the user background.  Users proposed to replace chat functionality with activ-  ity streams such as Facebook or Twitter. In addition, there were disagreements about merging communication and vi- sualization in the same use case.  In general, there is a lack of information about what the visualizations are showing.  We propose several solutions to address these problems. The headers of the table will be replaced by goal identifiers (used approach in Khan academy to represent goals). Addi- tional information such as goal title, description and filters can be displayed in a different place. The drop-down lists can be replaced by a calendar feature that is more intuitive. The pop-ups can be replaced by legends.  Regarding visualization 3, we propose to replace it by a motion chart visualization. Such a visualization allows to show the evolution of the user activity and to compare it with the average of a group over time. In this way, we provide additional information as requested by the users.  Personalization of dashboards can be a solution for differ- ent user backgrounds, as users can choose the visualization that they want to see. However, we have to keep in mind that personalization is an additional option. Users need to have a starting point to work with the application at the beginning. We can not offer the users a white screen and rely on the user for the whole configuration.  Chat functionality is discarded because the focus of this research is visualization of learning analytics.  Finally, we centralize the filter information in one place to fix the lack of information. Similar to chart legends do with charts, we try to provide a place that helps to understand the visualizations. In addition, we include extra information in every visualization to explain what it shows.  5. DIGITAL PROTOTYPE This iteration focuses on addressing usability issues de-  145    Figure 2: Distribution of technologies  tected in the previous iteration.  5.1 Design and implementation Dashboard personalization is provided by using widget  technology. Such technology enables easy addition and dele- tion of widgets with different visualizations (see figure 2 to see the different technologies). We use the OpenSocial speci- fication6 to enable deployment in OpenSocial compliant wid- get containers, e.g. iGoogle7 or Apache Shindig8.  OpenSocial enables inter-widget communication via Ope- nApp [14] that allows to send information from one widget to another. User interactions with one widget are broad- casted to other widgets. These widgets can then also act upon these events, i.e. to filter data. Furthermore, iGoogle supports the concept of spaces. These spaces can be used to support different organizations of widgets.  Regarding visualization libraries, we chose the Google Chart library9, as it provides a convenient event system and it has a large support community. New visualizations are contin- uously being added.  In this iteration, we deploy seven widgets based on the visualizations from the previous iteration in iGoogle (see vi- sualizations 1,3,5,6,7 and 8 in figure 1) . Second, we changed the timeline (visualization 3 in Figure1) by a motion chart (widget 2 in Figure 3). In the motion chart, x-axis is the ac- tivity of the user and y-axis is the peers average activity. A timeline chart can also be used to represent this data. How- ever, when several goals overlap and are represented over the same time period, the user could be confused with too much lines and colors. A motion chart simplifies the representa- tion. Third, widget 3 in Figure 3 is added to the dashboard and centralizes filter information.  5.2 Evaluation  5.2.1 Evaluation data Users were interviewed and observed during 15-30 min-  utes. They had to perform different predefined tasks such as filtering goals of this week following the think-aloud pro- tocol.  6Open Social api, http://code.google.com/apis/opensocial/ 7iGoogle, http://www.google.com/ig 8Apache Shindig, http://shindig.apache.org/ 9Google Chart library, http://code.google.com/apis/chart/  In this iteration, we use hardcoded dummy data for the goal table and motion chart and data from a previous evalua- tion [34]. This allows us to emulate more realistic dashboard behavior.  5.2.2 Demographics and Evaluation setup The digital prototype was evaluated with five male com-  puter science teachers and assistants. Four of them are be- tween 25 and 30 years old and one between 40 and 50. All of them know what a widget is. Three of them participated in the previous evaluation.  5.2.3 Evaluation results and discussion One remark on this iteration is about our rationale (see  Subsection 4.2.2) to create widget 3. The idea is to centralize all information in one place. However, the user perception is that every widget is independent from others, so they do not want to look up this information in another widget. In addition, depending on the screen resolution they have to scroll up to see the widget information and scroll down for the widget with the visualization. This dependency affects to the usability of the application.  In this iteration, the selection of the table visualization re- ceives good feedback due to the sorting functionality. How- ever, regarding the calendar feature, users suggested to have different possibilities, such as a slider. In addition, function- alities to organize the goals by weeks and buttons for next and previous weeks were requested.  The motion chart was more complex than expected. Users spent quite some time using the different configuration op- tions such as color and size. Although users consider it diffi- cult to understand at the beginning, they indicated that the motion chart can provide useful information. However, all users remarked that they would like to see the dashboard in a real use case in order to assess its usefulness.  There are also minor remarks such as letter font and data inconsistency, small size of the text boxes and table filtering style.  We focus on solving the first issue. We propose to elim- inate widget 3 and adding titles to the visualizations that can be updated based on filters. Removing this widgets also provides more space for bigger visualizations. For the next iteration, we eliminate calendar features because we do not need this kind of functionality in the use case study.  6. FIRST WORKING RELEASE This iteration focuses on the real deployment of the dash-  board. We selected an existing tracking system and adapted the existing widgets for this new scenario.  6.1 Design and implementation We considered two tracking systems: RescueTime and  Wakoopa10. They categorize tools and websites based on a functionality taxonomy such as Development, Browsers and Design. We selected RescueTime because it offers better security to access user data. As the next iteration with stu- dents involves real student data, such security and privacy considerations are very important.  We use the Rabbit Eclipse plug-in to track IDE Eclipse interaction. Students are developing software in Java and the Rabbit Eclipse plug-in allows tracking who is working  10http://www.wakoopa.com  146    Figure 3: Digital prototype  Figure 4: Source data aggregation  on which part of the project. The plug-in is open source and also tracks the time spent on documents (see figure 4).  The tracked information is collected via web services and exposed to the widgets via JSON. The dataset describes the time spent per application, document and website. This information is displayed in 8 different widgets as described in previous iterations.  In this iteration, we modify some widgets, because the RescueTime taxonomy enables us to categorize the tools by intended activity. This information can be useful for the students.  Widget 1 and 2 in Figure 5 are the same as described in Subection 5.1. Widget 3 shows the time per day spent by activity based on the taxonomy classification of Rescue- Time. The information is visualized using an annotated time line. Widget 4 is a bar chart that compares the global time spent per activity compared with the average time. Widget  5 shows the time spent per application. Widget 6 compares the time spent per application with other members of the group. Widget 7 shows the time spent on Eclipse projects files and, finally, widget 8 shows the time spent on websites compared with the average of the group.  The widgets use inter-widget communication for dataset filtering. Table 1 presents the connection details. This ta- ble explains which information is sent by every widget, and which widgets listen to events to filter their visualizations. For instance, when users click on a user in widget 1, this widget broadcasts the identifier of the user. Other widgets listen to this event and can show the information related to the user identifier.  Table 1: Overview of events Widget Event Listening widget  1 user identifier 2,3,4,5,6,7 and8 2 goal identifier, 3,4,5,6,7 and 8  start date and end date  4 selected range of time 3,5,6,7 and 8 5 type of activity 4 and 6 6 range of time 5 and 7  147    Figure 5: First release implementation  148    6.2 Evaluation  6.2.1 Evaluation data In this iteration, we evaluated the dashboard with stu-  dents. The data is tracked with RescueTime and the Rabbit Eclipse plug-in. As this evaluation took place at the start of the course, we did not have data from students, so two users (a developer and a project manger of our team) offered their RescueTime and Eclipse data for the experiment. The approach might influence the perceived usefulness, because students can not relate to their real data yet. However, the evaluation enabled us to obtain first feedback from students before the data collection started.  6.2.2 Demographics and Evaluation setup This experiment ran with 36 students between 18 and 20  years old (30 males and 6 females) in an engineering bach- elor course. We presented the dashboard the first day of the course. The privacy constraints and the data tracking characteristics of the experiment were explained. Students were also informed that they can stop RescueTime when they think it can affect their privacy. A questionnaire was used to collect quantitative data regarding first perceived usefulness, effectiveness, usability, satisfaction and privacy concerns. The questionnaire also has two open-ended ques- tions about privacy considerations and general positive and negative aspects.  We wanted to evaluate whether students consider the dash- board useful and whether specific changes were needed to deploy the dashboard in this course. In the first question of the evaluation, the students get 80 points that they have to divide over the widgets to rank them. This question was in- tended to get insight into which visualizations are considered more valuable by the students. The next seven questions are extracted from the USE questionnaire [22]. The full ques- tionnaire was not used due to time restrictions. Three ques- tions are related to usefulness and effectiveness and the next four questions to usability and user satisfaction. Finally, the three last questions are related to privacy concerns. Ques- tion number 10 inquires whether students would be receptive to include tracking activity out of the lab.  6.2.3 Evaluation results and discussion Results of the widget scoring question indicate that there  is no clear winner (Figure 6), as we expected. Widget 3 and 4 have slightly higher scores. Both are related to activity type. Widgets 5 and 6 score the lowest. These widgets show the tools instead of activity type and can be found redundant.  Widget 8 provides information about what web sites have been visited and also scores high. In the open questions, 12 users find it useful to see what websites other students are visiting.  Widget 2, the motion chat, scores the lowest. Our percep- tion is that the motion chart is more difficult to understand. In the next iterations, we pay special attention to the learn- ability of this visualization.  The questionnaire results are summarized in figure 7. The results indicate that students consider the dashboard useful (question 1 and 2) and that they think that the dashboard can help them to achieve goals (question 3). However, us- ability (question 4 and 5) and satisfaction (question 6 and 7) are scored neutral. As the students could not play yet with  Figure 6: Widget scores box plot  Figure 7: Questionnaire results  the dashboard, the scoring of these two factors was difficult. From question 8 (see Figure 7), I like to see what other  members do during the course, we learn that the students like to be aware of what their peers are doing. We can conclude from I feel confident using the tracking system in the lab during the course (question 9), that the lab is a suitable context to track their activity. Question 10 (I would feel confident using the tracking system outside of the lab) is rated the lowest. This outcome suggests that the students would feel uncomfortable if they were tracked outside the lab.  The open questions provide us with useful details. One of the most common remarks is that they like to see how students and their peers behave. 12 students like to compare their activity with others. However, 3 students indicate that they are disappointed that others can see their activity.  One student suggests that tracking can cause stress and consequently decrease productivity [40]. Most students men- tion that the feeling of being observed is a negative aspect.  149    Another student argues that our visualizations can possi- bly modify their working style because they may want to behave similar to other students. We have to consider all these factors in future evaluations.  One student suggested to add support for detecting user distraction. Three users suggested to also enable access to the raw tracked data. These students were interested to see how RescueTime tracks data. Another proposal is to store the tracking information locally and ask for user permission before sending the information. This is an important sug- gestion to deal with potential privacy concerns.  7. SECOND WORKING RELEASE The evaluation in the previous iteration is based on non-  course data and a demo of the application. In this iteration, we focus on first results of the dashboard evaluation with real student data in a real course setting. As we describe in Section 8, more evaluations will be performed during the course in the following months.  7.1 Design and implementation In this iteration, we analyzed the generated data to see  how the students behave during the lab sessions. The dashboard was made available to 36 students. We  created anonymous email and RescueTime accounts for each student. Students had to configure RescueTime and the Rabbit Eclipse plugin with their credentials.  7.2 Evaluation  7.2.1 Evaluation data In this iteration, we evaluate the dashboard with student  data. Students carried out different tasks, such as elabora- tion of scenarios, use cases and an implementation of a small web application during four lab sessions. As these tasks are partially performed without the computer, the tracked data is still limited in this phase.  7.2.2 Demographics and Evaluation setup This experiment ran with 10 students between 18 and 20  years old (8 males and 1 females), a subgroup from the pre- vious iteration. A subgroup was used to be able to better assist the students if problems would show up.  Students were encouraged to reflect on the dashboard vi- sualizations during 10 minutes. They filled in a SUS ques- tionnaire [10] afterwards. Such a questionnaire allows us to compare our application with more than 200 studies [7]. We added questions to score widgets (as used in the previ- ous iteration), evaluate usefulness and satisfaction, and open ended questions.  During the evaluation, we removed widget 7 (see figure 5) because the only activity in Eclipse was the development during a tutorial, which would not provide useful informa- tion.  7.2.3 Evaluation results and discussion The final SUS score is 72 points out of 100. Based on [7],  this score rates the dashboard as good regarding usability. The widget scoring question results are summarized in  Figure 8. Widgets 4, 6 and 8 are rated highest. We think that this is due to the limited data because we are in the initial period of the course. While bar charts display abso- lute information and are valuable even with limited data,  Figure 8: Widget scores box plot  timelines loose meaning because the user cannot see much evolution over the different sessions.  The 5-item likert scale, I would feel confident using it in another course, inquires about the usefulness and was rated on average 2.9. Users are not used to reflection on their own work using these tools. If the reflection task is mandatory during the course, they perform the task. However, they seem to prefer avoiding such tasks. The users do not find the dashboard beneficial enough to use it regularly. Part of this research is intended to increase the users interest for these kind of tools.  We asked the students about what they learnt. Three students highlighted the fact that there is not much data because they have not been working with the computers all the time. For instance, the dashboard does not represent the time students spent on the scenarios. Three other stu- dents found patterns in their Internet use. For instance, one student pointed out that his peers did not visit the course wiki as often as he did and realized that he was the person in charge to check this information. Five students indicated that visualizations are nice or even fun to use.  8. FUTURE WORK The experiment runs during the whole semester and two  more evaluations are scheduled. The essence of our future work is to actually evaluate the dashboard with the students as the course evolves and collects more real data. Such more elaborate data is required to assess in more detail the added value of these tools.  The current version of the dashboard enables students to compare their progress with peers on tasks that are defined by the teacher. In the next phase, we will add support to enable students to define their own goals. For instance, they could define how much time they want to spend every day on concrete tasks. Self-definition of goals is an important part of Personal Informatics.  In addition, the dashboard technology allows easy cus- tomization. We can easily develop more visualization wid- gets that users can set up based on the context and their visualization background. We need to evaluate the influence of such customization factors in additional experiments.  150    9. CONCLUSION In this paper, we presented the first results of a case study  with second year engineering students. We conclude that students consider the dashboard useful  to learn how they are using the tools. However, users are not motivated to use the dashboard.  Visualization enables exploration of large datasets, but different visualization backgrounds can influence on the un- derstanding of the data. Implementing the dashboard as a mash-up of widgets is our proposal to address this issue. The aproach allows us to offer the users different visualiza- tion configurations.  The dashboard can be useful to support self-reflection and progress in comparison with peers. Students are interested to be aware of what their peers are doing. However, pri- vacy concerns are involved in this process. The students are receptive to be tracked during their lab sessions. However, they do not like to be tracked outside a course environment due to privacy concerns. As additional work out of the lab sessions is not required for the course, this does not have implications on the current evaluation setup. However, the issue needs to be researched in order to generalize these kinds of experiments beyond the current course setting.  10. ACKNOWLEDGMENTS The research leading to these results has received fund-  ing from the European Community Seventh Framework Pro- gramme (FP7/2007-2013) under grant agreement no 231396 (ROLE). Katrien Verbert is a Postdoctoral Fellow of the Research Foundation  Flanders (FWO).  11. REFERENCES [1] Health engage - http://www.healthengage.com/ - last  checked on october 2011.  [2] Health tracking - http://www.healthtracking.net/ - last checked on october 2011.  [3] Rescuetime - http://www.rescuetime.com - last checked on october 2011.  [4] Run keeper - http://runkeeper.com/ - last checked on october 2011.  [5] D. Albert and A. Nussbaumer. Towards generic visualisation tools and techniques for adaptive e-learning. Proceedings of the 18th International Conference on Computers in Education ICCE 2010, 2010.  [6] S. D. Aneesha Bakharia1. Snapp: A birds-eye view of temporal participant interaction. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [7] A. Bangor, P. Kortum, and J. Miller. An empirical evaluation of the system usability scale. International Journal of Human-Computer Interaction, 24(6):574594, 2008.  [8] C. Bingham. Two educational ideas for 2011 and beyond. Studies in Philosophy and Education, 30:513519, 2011. 10.1007/s11217-011-9253-8.  [9] P. Blikstein. Using learning analytics to assess students behavior in open-ended programming tasks. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [10] J. Brooke. Sus-a quick and dirty usability scale. Usability evaluation in industry, dustry:189194, 1996.  [11] F. D. Davis and V. Venkatesh. Toward preprototype user acceptance testing of new information systems: implications for software project management. Engineering Management, IEEE Transactions on, 51(1):3146, 2004.  [12] E. Duval. Attention please! Learning analytics for visualization and recommendation. In Proceedings of LAK11: 1st International Conference on Learning Analytics and Knowledge,. ACM, 2011. Accepted.  [13] S. E. Fancsali. Variable construction for predictive and causal modeling of online education data. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [14] S. Govaerts, K. Verbert, D. Dahrendorf, C. Ullrich, S. Manuel, M. Werkle, A. Chatterjee, A. Nussbaumer, D. Renzel, M. Scheffel, and et al. Towards Responsive Open Learning Environments : the ROLE Interoperability Framework. CEUR workshop proceedings, 2011.  [15] S. Govaerts, K. Verbert, J. Klerkx, and E. Duval. Visualizing activities for self-reflection and awareness. In Lecture Notes in Computer Science, pages 91100. Springer, Dec. 2010.  [16] N. Kamal, S. Fels, and K. Ho. Online social networks for personal informatics to promote positive health behavior. In Proceedings of second ACM SIGMM workshop on Social media, WSM 10, pages 4752, New York, NY, USA, 2010. ACM.  [17] G. M. Karam. Visualization using timelines. In Proceedings of the 1994 ACM SIGSOFT international symposium on Software testing and analysis, ISSTA 94, pages 125137, New York, NY, USA, 1994. ACM.  [18] J. Kay, J. Kay, N. Maisonneuve, N. Maisonneuve, K. Yacef, K. Yacef, P. Reimann, and P. Reimann. The big five and visualisations of team work activity. In Proceedings of Intelligent Tutoring Systems, 2006.  [19] C. Lewis and J. Rieman. Task-centered user interface design: a practical introduction. University of Colorado, Boulder, Dept. of Computer Science, 1993.  [20] I. Li, A. Dey, and J. Forlizzi. A stage-based model of personal informatics systems. In Proceedings of the 28th international conference on Human factors in computing systems, CHI 10, pages 557566, New York, NY, USA, 2010. ACM.  [21] L. Lockyer and S. Dawson. Learning designs and learning analytics. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [22] A. M. Lund. Measuring usability with the use questionnaire. Usability Interface, 8(2), 2001.  [23] R. Mazza and C. MILANI. Exploring usage analysis in learning systems: Gaining insights from visualisations. In In AIED Workshops (AIED05), juillet, 2005.  [24] L. Mazzola and R. Mazza. Gvis: a facility for adaptively mashing up and representing open learner models. In Proceedings of the 5th European conference on Technology enhanced learning conference on Sustaining TEL: from innovation to learning and practice, EC-TEL10, pages 554559, Berlin, Heidelberg, 2010. Springer-Verlag.  [25] K. Niemann, H.-C. Schmitz, M. Scheffel, and M. Wolpers. Usage contexts for object similarity: Exploratory investigations. In Proceedings of the  151    Learning Analytics and Knowledge conferencd (LAK11), 2011.  [26] A. Nussbaumer. Supporting Self-Reflection through Presenting Visual Feedback of Adaptive Assessment and Self-Evaluation Tools, volume 1. 2008.  [27] A. Nussbaumer, C. Steiner, and D. Albert. Visualisation Tools for Supporting Self-Regulated Learning through Exploiting Competence Structures, pages 35. Number September. Citeseer, 2008.  [28] C. H. Orrill, M. J. Hannafin, and E. M. Glazer. Disciplined inquiry and the study of emerging technology. Framework, pages 335354, 1998.  [29] A. Pardo and C. D. Kloos. Stepping out of the box. towards analytics outside the learning management system. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [30] P. R. Pintrich. A motivational science perspective on the role of student motivation in learning and teaching contexts. Journal of Educational Psychology, Vol 95(4):667686, December 2003.  [31] S. Purpura, V. Schwanda, K. Williams, W. Stubler, and P. Sengers. Fit4life: the design of a persuasive technology promoting healthy behavior and ideal weight. In Proceedings of the 2011 annual conference on Human factors in computing systems, CHI 11, pages 423432, New York, NY, USA, 2011. ACM.  [32] G. Richards and I. DeVries. Revisiting formative evaluation: Dynamic monitoring for the improvement of learning activity design and delivery. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [33] D. Rosen, V. Miagkikh, and D. Suthers. Social and semantic network analysis of chat logs. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [34] J. L. Santos, K. Verbert, S. Govaerts, and E. Duval. Visualizing PLE Usage, volume 773, pages 3438. CEUR workshop proceedings, 2011.  [35] M. Sedlmair, P. Isenberg, D. Baur, M. Mauerer, C. Pigorsch, and A. Butz. Cardiogram: visual analytics for automotive engineers. In CHI 11, pages 17271736. ACM, 2011.  [36] T. Selker. New paradigms for using computers. Commun. ACM, 39:6069, August 1996.  [37] M. Sharkey. Academic analytics landscape at the university of phoenix. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [38] B. Shneiderman. The eyes have it: A task by data type taxonomy for information visualizations. In Proc. of the IEEE Symp. on Visual Languages, page 336. IEEE, 1996.  [39] M. Swan. Emerging patient-driven health care models: An examination of health social networks, consumer personalized medicine and quantified self-tracking. International Journal of Environmental Research and Public Health, 6(2):492525, 2009.  [40] M. Tarafdar, Q. Tu, B. S. Ragu-Nathan, and T. S. Ragu-Nathan. The impact of technostress on role stress and productivity. Journal of Management Information Systems, 24(1):301328, 2007.  [41] K. Upton and J. Kay. Narcissus: Group and  individual models to support small group work. In Proceedings of the 17th International Conference on User Modeling, Adaptation, and Personalization: formerly UM and AH, UMAP 09, pages 5465, Berlin, Heidelberg, 2009. Springer-Verlag.  [42] K. Verbert, H. Drachsler, N. Manouselis, M. Wolpers, R. Vuorikari, and E. Duval. Dataset-driven research for improving recommender systems for learning. In Proceedings of the Learning Analytics and Knowledge conferencd (LAK11), 2011.  [43] F. Wang and M. Hannafin. Design-based research and technology-enhanced learning environments. Educational Technology Research and Development, 53:523, 2005. 10.1007/BF02504682.  152      "}
{"index":{"_id":"27"}}
{"datatype":"inproceedings","key":"Essa:2012:SSS:2330601.2330641","author":"Essa, Alfred and Ayad, Hanan","title":"Student Success System: Risk Analytics and Data Visualization Using Ensembles of Predictive Models","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"158--161","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330641","doi":"10.1145/2330601.2330641","acmid":"2330641","publisher":"ACM","address":"New York, NY, USA","keywords":"data fusion, data visualization, learner success, predictive analytics, predictor ensembles","Abstract":"We propose a novel design of a Student Success System (S3), a holistic analytical system for identifying and treating at-risk students. S3 synthesizes several strands of risk analytics: the use of predictive models to identify academically at-risk students, the creation of data visualizations for reaching diagnostic insights, and the application of a case-based approach for managing interventions. Such a system poses numerous design, implementation, and research challenges. In this paper we discuss a core research challenge for designing early warning systems such as S3. We then propose our approach for meeting that challenge. A practical implementation of an student risk early warning system, utilizing predictive models, must meet two design criteria: a) the methodology for generating predictive models must be flexible to allow generalization from one context to another; b) the underlying mechanism of prediction should be easily interpretable by practitioners whose end goal is to design meaningful interventions on behalf of students. Our proposed solution applies an ensemble method for predictive modeling using a strategy of decomposition. Decomposition provides a flexible technique for generating and generalizing predictive models across different contexts. Decomposition into interpretable semantic units, when coupled with data visualizations and case management tools, allows practitioners, such as instructors and advisors, to build a bridge between prediction and intervention.","pdf":"Student Success System: Risk Analytics and Data Visualization using Ensembles of Predictive Models  Alfred Essa and Hanan Ayad Desire2Learn Incorporated  151 Charles Street W Kitchener, Ontario, Canada  {alfred.essa,hanan.ayad}@desire2learn.com  ABSTRACT We propose a novel design of a Student Success System (S3), a holistic analytical system for identifying and treating at- risk students. S3 synthesizes several strands of risk ana- lytics: the use of predictive models to identify academically at-risk students, the creation of data visualizations for reach- ing diagnostic insights, and the application of a case-based approach for managing interventions. Such a system poses numerous design, implementation, and research challenges. In this paper we discuss a core research challenge for de- signing early warning systems such as S3. We then propose our approach for meeting that challenge. A practical im- plementation of an student risk early warning system, uti- lizing predictive models, must meet two design criteria: a) the methodology for generating predictive models must be flexible to allow generalization from one context to another; b) the underlying mechanism of prediction should be easily interpretable by practitioners whose end goal is to design meaningful interventions on behalf of students. Our pro- posed solution applies an ensemble method for predictive modeling using a strategy of decomposition. Decomposition provides a flexible technique for generating and generaliz- ing predictive models across different contexts. Decompo- sition into interpretable semantic units, when coupled with data visualizations and case management tools, allows prac- titioners, such as instructors and advisors, to build a bridge between prediction and intervention.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.1 [Computer Uses in Education]: Collaborative learning, Computer-assisted instruction (CAI), Computer-managed in- struction (CMI), Distance learning  Keywords Learner Success, Predictive Analytics, Data Visualization, Predictor Ensembles, Data Fusion  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04...$10.00  1. INTRODUCTION Improving student retention, graduation, and completion rates is a fundamental challenge in improving educational delivery.[] S3 is intended as a practical end-to-end solu- tion for identifying which students are at risk, understand- ing why they are at risk, designing interventions to mitigate that risk, and finally closing the feedback loop by assessing the success.  Current approaches to building predictive models for identi- fying at-risk students are stymied by two serious limitations. First, the predictive models are one-off and, therefore, can- not be extended easily from one context to another. We can- not simply assume that a predictive model developed for a particular course at a particular institution is valid for other courses. Can we devise a flexible and scalable methodology for generating predictive models that can accommodate the considerable variability in learning contexts across different courses and different institutions Secondly, current model- ing approaches, even if they generate valid predictions, tend to be black boxes from the standpoint of practitioners. The mere generation of a risk signal (e.g. green, yellow, red) does not convey enough information for the purpose of designing meaningful personalized interventions on behalf of students.  As a way of overcoming these limitations we devise a model- ing strategy that begins with a generic measure called Suc- cess Index which is decomposed initially into five indices: Preparation, Attendance, Participation, Completion, and Social Learning. (As we proceed with the implementation of S3 we anticipate discovering and adding other indices.) Each index is itself a composite expressing a number of rel- evant activity-tracking variables. These tracking variables are measured on different scales, primarily in terms of the frequency with which a particular action or task is performed or the time spent on-task. These indices or semantic units serve as the foundation for applying an ensemble method for predictive modeling.  2. RELATED WORK In education predictive models for identifying at-risk stu- dents was pioneered by John Campbell and the Course Sig- nals Project at Purdue University.[] Similar work has been underway at Capella University, Rio Salado College and other institutions. In this section we discuss methodological limitations with current risk modeling approaches in educa- tion. In Section 3 we provide a quick overview of S3. In Section 4 we propose how to overcome current methodolog-  158    ical limitations in risk modeling.  2.1 Predictive Models in Education The Course Signals system and recent research studies [] provide early evidence that student elearning activities are predictive of academic success. Regression modelling such as logistic regression has been applied to build a best-fit course- based predictive models. Such models incorporate the most significant LMS variables such as total number of discussion messages posted, total number of mail messages sent, and total number of assessments completed. The mathematical aspects of this modeling strategy is briefly described in Sec 4.1. Macfadyen and Shane [] discuss the limitations of this work in terms of its overall generalizability and interpreta- tion. In particular, the generalizability of such models can be limited by the sample courses used for model fitting, or by focussing on fully online courses within one institution.  A core problem in current approaches, as applied in Course Signals-type systems, is that a single hypothesis/model that best fits a collection of course data, is chosen from the space of all possible hypotheses, and then applied to make pre- dictions across different courses in different programs and institutions. There are potential sources of bias in this so- lution. This methodology is expected to work well when courses on which the model is applied have a relatively con- sistent instructional model with the courses used to discover the best-fit model, but otherwise lead to a risk of systematic errors in predictions, i.e. relatively high bias.  The limitations of this modeling strategy, in terms of gen- eralizability and interpretability, critically hinder the wide- ranging deployment of discovered models to educational in- stitutions in a meaningful way. Hence, it limits the potential benefits that institutions can draw from their data through the development of predictive analytics capabilities for mod- eling learner success. In this work, we propose a predictive modeling strategy that aims at closing this gap. We fo- cus on providing a highly-generalizable modeling strategy that is well-suited for supporting wide-ranging needs of ed- ucational institutions and for taking full advantage of pre- dictive analytics. We propose an adaptive framework and a stacked-generalization modeling strategy whereby intelli- gent data analysis can be applied at all levels and graciously combined to express higher-level generalizations.  A second key problem is that current predictive modeling systems do not provide diagnostic information. For exam- ple, Course Signals generates a prediction that indicates the identified level of risk; however, there is no direct insight into the specific causes, thus making a recommended reme- diation difficult to specify. Furthermore, the system does not incorporate human insight that can be leveraged via model tuning, if needed. If a system is designed to facilitate inter- pretability and self-explanation, a by-product is the ability to support a meaningful tuning functionality, thus taking the insight of business domain experts into account.  To enable an effective synthesis of machine intelligence and human insight, the proposed S3 provides an interpretable model and data visualizations. In particular, we focus on developing an interpretable modeling strategy, intuitive hu- man experience and powerful interaction with the data and  models. Furthermore, for predictive analytics to be success- fully applied at an institution, it needs to be deeply inte- grated into business process, where decision makers can use it in their natural workflow every day.  Another issue with a Signals-type model is that it ignores potentially key aspects of learning. One such example is social learning. For example, in [], social network analy- sis plays a key role in providing insights into the student learning community and the patterns of peer interactions. In S3, a social network analysis and visualization is incor- porated to capture and explain the social learning aspect. Similarly, the treatment of content comprehension is limited to tracking the number of content topics visited. On the other hand, intelligent tutoring systems and related work [] develop specialized data analysis and domain knowledge representation to model learner behavior and abilities in re- lation to content usage and knowledge acquisition.  In S3, we propose an ensemble strategy whereby a domain- specific decomposition allows for the development and inte- gration of specialized models and algorithms that are best suited for different aspects of learning. In particular, in S3, the proposed decomposition provides an abstraction of learning behavior into semantically meaningful units. Pre- diction ensembles provide a powerful and flexible paradigm for enhancing the relevance and generalizability of predic- tive analytics. It can also be viewed as enabling a collabo- rative platform, whereby institution can plug their own pro- prietary model as part of the ensemble. Thus, it enables an open, community-driven R&D platform for the application of predictive models to advance learning analytics as well as institutional analytics capabilities.  3. STUDENT SUCCESS SYSTEM In this section we provide a functional overview of S3. This will serve as background for the modeling strategy described in Section 4. The overview is not intended to be compre- hensive. Our aim is to provide enough context for stating the research problem and our proposed solution.  3.1 S3 Functionality The workflow for S3 is analogous to the workflow associated with the steps in a patient-physician relationship. When a patient sees a physician the basic workflow is: a) understand the problem; b) reach a diagnosis; c) prescribe a course of treatment; d) track the success.  S3 follows a similar workflow. First, upon login to S3 an ad- visor (a possible role in S3) is presented with a pictorial list of her students. Associated with each student is a risk indi- cator: green indicates not at-risk, yellow indicates possibly at-risk, and red means at-risk. The advisor can immediately click on a particular student or view the screen showing the list of students in a particular category (e.g. high risk).  Next, associated with each student is his Student Profile Screen. The Student Profile Screen provides an overview of the students profile, including projected risk at both the course and institution level. The screen also serves as a gateway to other screens, including Course Screens which provide views into course-level activity and risks. The Notes Screen provides case notes associated with the stu-  159    dent while Referral Screen provides all the relevant referral options available at the institution.  3.2 Data Visualizations As the user of the S3 navigates through the various success indicators, the underlying models and data are presented in an intuitive and interpretable manner, going from one level of aggregation to another. S3 contains a number of visualiza- tions for diagnostic purposes. These include: Risk Quadrant, Interactive Scatter Plot, Win-Loss Chart, and Sociogram.  For illustrative purposes we provide a representation of the Interactive Scatter Plot and the Win-Loss Chart. A user of S3 is able to explore the data that make up the predictive model by selecting the success indicators associated with each domain and visualize patterns such as cluster struc- tures and relations between different indicators and mea- sures of performance. The chart is also dynamic in the sense that data can be animated to visualize paths/trails depicting changes in learner behaviors and performance over time.  Figure 1: Visualization - Interactive Scatter Plot.  Another example of the charts available in S3 is the Win- Loss Chart. As shown below, one can see at glance how the student compares to peers in the overall success indicator and along each of the sub-indicators. Values above, within, or below average are indicated by green, orange and red bars. Option is provided to compare current indicators with the students own history. This option help visualize changes in students own learning behavior over time.  Figure 2: Visualization - Win-Loss Chart.  4. ENSEMBLE MODELING STRATEGY The idea of prediction ensemble is to enable the selection of a whole collection, or ensemble, of hypotheses from the hy- pothesis space and combine their predictions appropriately. A key rationale is that various indicators of learning suc- cess and risks can be found by analyzing different aspects of the learning and teaching processes, the educational tools and instructional design, the pre-requisite competencies, the dynamics of a particular course, program or institution, as well as the modality of learning being fully online, live, or hybrid. We argue that there is a need for the discovery and blending of multiple models to effectively express and man- age complex and diverse patterns of the elearning process.  Ensemble methods are designed to boost the predictive gen- eralizability by blending the predictions of multiple models [, , ]. For example, stacking, also referred to as blend- ing, is a technique in which the predictions of a collection of base models are given to a second-level predictive modeling algorithm, also referred to as a meta-model. The second- level algorithm is trained to combine the input predictions optimally into a final set of predictions.  Classifier ensembles allow solutions that would be difficult (if not impossible) to reach with only a single model []. Stacking, data fusion, adaptive boosting, and related ensem- ble techniques have successfully been applied in many fields to boost prediction accuracy beyond the level obtained by any single model [].  S3 represents a particular instance of the ensemble paradigm. It employs aspects of data fusion as explained in Sec 4.1 to build base models for different learning domains. Further- more, the system utilizes a stacked generalization strategy as explained in 4.2. A best-fit meta-model takes as input pre- dictors the output of the base models and optimally combine them into an aggregated predictor, referred to as a success indicator/index. In this type of stacked generalization, opti- mization is typically achieve by applying EM (Expectation Maximization) algorithm [].  4.1 Base Models The data fusion model is useful for building individual pre- dictive models that are well suited for sub-domains of an application. In the context the S3, these models correspond to each data-tracking domain and represent different aspects of the learning process. That is, each model is designed for a particular domain of learning behaviour. An initial set of domains are defined as: Attendance, Completion, Participa- tion, and Social Learning.  Consider the attendance domain: learner tracking data re- flecting online attendance is collected, including for exam- ple, number of course visits, total time spent, average time spent per session, in addition to other administrative as- pects of the elearning activities such as number of visits to the grade tool, number of visits to the calendar/schedule tool, number of news items/announcements read. A simple logistic regression model, or a generalized additive model, is suitable for this domain.  On the other hand, in the case of the social learning domain, social network analysis SNA techniques would need to be ap-  160    plied. The work by [] demonstrates the key importance for specialized analysis of this aspect of the elearning process. In fact, SNA, in conjunction with text mining on learners discourse, is needed for the extraction of meaningful risk factors and success indicators. In other words, the logistic regression model described above for the attendance domain is considered insufficient for meaninful predictive analysis of the social learning domain.  In S3 predictive models for each domain are built indepen- dently. Each generate an abstracted success sub-indicator represented as a predicted class and an associated probabil- ity estimate (y, p), where p = p(Y = y|X), and X denotes domain-related activities being tracked.  4.2 Combining Model Ouputs A key design aspect of ensemble systems is the combining process. Combination strategies for ensemble systems are characterized along two dimensions []: (1) trainable ver- sus non-trainable rules, and (2) applicability to class labels versus class-specific probabilities.  By selecting a trainable rule, the blending weights associated with the prediction of individual models are optimized to obtain a best-fit meta-model. By selecting a non-trainable combination rule, the business user is able to adjust the weight of the base predictions. For example, in a hybrid course where emplasis on discussion and social learning are primarily conducted face-to-face, the instructor can choose to dampen the effect of the social learning model from the overall prediction. The proposed ensemble system takes ad- vantage of the estimated probabilties in combining the base predictions. In S3, there are three risk-levels, and each base model generates as output a vector of three probability val- ues corresponding to estimated probability for each of the levels At-Risk, Potential Risk, Success.  Let {g1, g2, . . . , gL} denote the learned prediction functions of L predictive models with gi : X  i  (Y, p  [0, 1]c)),i, where Y are the risk categories, p is the associated prob- ablity vector, and c is the number of risk categories, i.e. c = 3. For the described instance of S3 we have L = 4 corresponding to each of the data-tracking domains, at the course grouping/template level. The meta-model takes as input a matrix G with c = 3 columns represent the risk cat- egories and L = 4 predictive models, where gij represents the probablity of risk-level j according to predictive model gi. It also takes as input the corresponding true outcomes y in the training dataset.  A simple non-trainable combining process would be to av- erage the values gij for each column of G. Normalization to add to 1 over all categories may be applied. Then, the maximum likelihood principle is applied by selecting the risk category with maximum posterior probability as the aggre- gated success indicator. Alternatively, the outputs of the base models are used as input to find the best-fit second- level mapping between the ensemble outputs and the correct outcome (risk level) as given in the training dataset.  Typically, to find the best-fit meta-model, an iterative k-fold cross validation process is applied []. The training dataset is divided into k = L blocks and each of the first level model  is first trained on L  1 blocks, leaving one block for the second-level model, at each iteration through the L blocks. The process is designed to achieve a reliable model fitting.  Linear regression stacking seeks a blended prediction func- tion b represented as b(x) =   i wigi(x),x  X, where a  key advantage of this linear model is that it lends itself natu- rally to intepretation. Furthermore, the computational cost involved in fitting such a model is modest.  5. CONCLUSIONS We proposed a holistic ensemble-based analytical system S3 for tracking student academic success. From a design perspective, the unique synthesis of using predictive mod- els to identify at-risk students, creating data visualizations to reach diagnostic insights, and incorporating a case-based methodology for managing interventions provides a just-in- time mechanism and personalized approach to improving student retention and student success. From a research per- spective, an ensemble-based approach to predictive model- ing using semantic decomposition overcomes two significant shortcomings in current approaches, namely generalizabil- ity and interpretability. Ensemble methods are designed to boost the predictive generalizability by blending the predic- tions of multiple models. In S3, a stacked generalization strategy is applied to combine the predictions of a collection of base models via a second-level predictive modeling algo- rithm, a meta-model. The second-level algorithm is trained to combine the input predictions optimally into a more in- formed set of predictions. To facilitate model interpretabil- ity, abstraction of the elearning process into meaningful do- mains in conjunction with data visualization, interactive and intuitive interface are all part of S3. Furthermore, S3 can be tuned by business experts to best suit their needs. Fu- ture work will apply ensemble techniques to real datasets to demonstrate the full power of this methodology.  6. REFERENCES [1] John P. Campbell, Peter B. DeBlois, and Diana G.  Oblinger. Academic analytics: A new tool for a new era. EDUCAUSE Review, July/August:4057, 2007.  [2] Carlos Delgado Kloos, Denis Gillet, Raquel M. Crespo Garcia, Fridolin Wild, and Martin Wolpers, editors. Towards Ubiquitous Learning. 6th European Conference on Technology Enhanced Learning, EC-TEL 2011, volume LNCS:6964, Palermo, Italy, September 2011. Springer.  [3] Leah P. Macfadyen and Shane Dawson. Mining lms data to develop an early warning system for educators: A proof of concept. Computers & Education, 54:588599, 2010.  [4] Nikunj C. Oza and Kagan Tumer. Classifier ensembles: Select real-world applications. Information Fusion, v.9 n.1:420, January 2008.  [5] Robi Polikar. Ensemble based systems in decision making. IEEE Circuits and Systems Magazine, Third Quarter:2145, 2006.  [6] J. Sill, G. Takacs, L. Mackey, and D. Lin. Feature-weighted linear stacking. arXiv, 0911.0460v2:117, 2009.  161      "}
{"index":{"_id":"28"}}
{"datatype":"inproceedings","key":"Leony:2012:GLA:2330601.2330642","author":"Leony, Derick and Pardo, Abelardo and de la Fuente Valent'in, Luis and de Castro, David S'anchez and Kloos, Carlos Delgado","title":"GLASS: A Learning Analytics Visualization Tool","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"162--163","numpages":"2","url":"http://doi.acm.org/10.1145/2330601.2330642","doi":"10.1145/2330601.2330642","acmid":"2330642","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, visualization framework, visualization system","Abstract":"The use of technology in every day tasks enables the possibility to collect large amounts of observations of events taking place in different environments. Most tools are capable of storing a detailed account of the operations executed by users in certain files commonly known as logs. These files can be further analyzed to infer information that is not directly visible such as the most popular applications, times of the day with highest activity, calories burnt after a running session, etc. Graphic visualizations of this data can be used to support this type of analysis as shown in [1]. Visualization can also be applied in the domain of learning experiences to track and analyse the data obtained from both learners and instructors. There are several tools that have been proposed in specific environments such as, for example, in personal learning environments [5], to foster self-reflection and awareness [2], and to support instructors in web-based distance learning [3]. These visualizations need to take into account aspects such as how to access and protect personal data, filter management, multi-user support and availability. In this paper, the web-based visualization platform GLASS (Gradient's Learning Analytics System) is presented. The architecture of the tool has been conceived to support a large number of modular visualizations derived from a common dataset containing a large number of recorded events. The tool was developed following a bottom-up methodology to provide a set of basic operations required by any visualization. The design goal is to provide a highly versatile, modular platform that simplifies the implementation of new visualizations. The main functionality elements considered in GLASS are database access, module management, visualization parameters, and the web interface. The platform uses datasets stored using the CAM schema (Contextualized Attention Metadata) [6]. This schema allows to capture events occurring during the use of various computer applications which, in our case, are the tools used by students when working in a learning environment. The process to obtain events from learning environments has been described in [4]. GLASS is able to connect to more than one CAM database, thus allowing access to events obtained in different contexts. The tool is extensible through the installation of modules. A module is a structured set of scripts and resources that, given a dataset of events and a set of filters, generates at least one visualization. In order to simplify the development of new modules, the platform provides an API to manage common visualizations settings such as the date range and other typical filters. A visualization may include a simpler version suitable to be displayed in the user's Dashboard, which is the entry page of the application. Figure 1 shows an example of dashboard in GLASS. Additionally, visualizations can be exported as HTML code to be embedded in another website. The GLASS architecture consists of four layers: data layer, code base, modules and visualizations, as depicted in Figure 2. The data layer is composed of a set of CAM databases and a database to store the platform parameters. The code base is in charge of the main functionalities of GLASS regarding module and user management and interfaces. Modules must comply with the platform specifications to generate visualizations and the settings that can affect their appearance. Currently, the tool includes a default module that provides two visualizations as shown in Figure 1): a frequency time line of activity events and a bar-chart with grouped bars of events generated by different user groups (e.g. events from students individually, or groups). The default module also serves as an example of how to develop a additional modules. Currently, GLASS is able to support new visualizations and is undergoing additional testing in different learning scenarios. Preliminary results obtained from user tests indicate that visualizations need to be very intuitive for both instructors and learners. The current development effort is focused on providing visualizations that show the most-common learners events and the most active learners in a given context. To encourage its use in other institutions, the tool has been released with an open source license and can be obtained from http://glass.mozart.gast.it.uc3m.es. A video demonstrating the tool is available at http://bit.ly/glass-lak12","pdf":"GLASS: A Learning Analytics Visualization Tool  Derick Leony, Abelardo Pardo, Luis de la Fuente Valentn, David Snchez de Castro, Carlos Delgado Kloos  University Carlos III of Madrid Avenida Universidad 30  28911 Legans (Madrid) Spain dleony@it.uc3m.es, abel@it.uc3m.es, lfuente@it.uc3m.es,  dscastro@inv.it.uc3m.es, cdk@it.uc3m.es  ABSTRACT The use of technology in every day tasks enables the pos- sibility to collect large amounts of observations of events taking place in different environments. Most tools are capa- ble of storing a detailed account of the operations executed by users in certain files commonly known as logs. These files can be further analyzed to infer information that is not directly visible such as the most popular applications, times of the day with highest activity, calories burnt after a run- ning session, etc. Graphic visualizations of this data can be used to support this type of analysis as shown in [1]. Visu- alization can also be applied in the domain of learning ex- periences to track and analyse the data obtained from both learners and instructors. There are several tools that have been proposed in specific environments such as, for example, in personal learning environments [5], to foster self-reflection and awareness [2], and to support instructors in web-based distance learning [3]. These visualizations need to take into account aspects such as how to access and protect personal data, filter management, multi-user support and availability. In this paper, the web-based visualization platform GLASS (Gradients Learning Analytics System) is presented. The architecture of the tool has been conceived to support a large number of modular visualizations derived from a com- mon dataset containing a large number of recorded events. The tool was developed following a bottom-up methodology to provide a set of basic operations required by any visu- alization. The design goal is to provide a highly versatile, modular platform that simplifies the implementation of new visualizations.  The main functionality elements considered in GLASS are database access, module management, visualization param- eters, and the web interface. The platform uses datasets stored using the CAM schema (Contextualized Attention Metadata) [6]. This schema allows to capture events occur- ring during the use of various computer applications which, in our case, are the tools used by students when working in a learning environment. The process to obtain events from  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 12, 29 April  2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM ACM 978-1-4503-1111-3/12/04 ...$10.00.  learning environments has been described in [4]. GLASS is able to connect to more than one CAM database, thus allowing access to events obtained in different contexts.  The tool is extensible through the installation of modules. A module is a structured set of scripts and resources that, given a dataset of events and a set of filters, generates at least one visualization. In order to simplify the development of new modules, the platform provides an API to manage common visualizations settings such as the date range and other typical filters. A visualization may include a simpler version suitable to be displayed in the users Dashboard, which is the entry page of the application. Figure 1 shows an example of dashboard in GLASS. Additionally, visual- izations can be exported as HTML code to be embedded in another website.  The GLASS architecture consists of four layers: data layer, code base, modules and visualizations, as depicted in Fig- ure 2. The data layer is composed of a set of CAM databases and a database to store the platform parameters. The code base is in charge of the main functionalities of GLASS re- garding module and user management and interfaces. Mod- ules must comply with the platform specifications to gen- erate visualizations and the settings that can affect their appearance. Currently, the tool includes a default module that provides two visualizations as shown in Figure 1): a frequency time line of activity events and a bar-chart with grouped bars of events generated by different user groups (e.g. events from students individually, or groups). The de- fault module also serves as an example of how to develop a additional modules.  Figure 2: The four layers of GLASS architecture.  Currently, GLASS is able to support new visualizations  162    Figure 1: GLASS Dashboard. Four visualizations provided by one module.  and is undergoing additional testing in different learning scenarios. Preliminary results obtained from user tests in- dicate that visualizations need to be very intuitive for both instructors and learners. The current development effort is focused on providing visualizations that show the most- common learners events and the most active learners in a given context. To encourage its use in other institutions, the tool has been released with an open source license and can be obtained from http://glass.mozart.gast.it.uc3m.es. A video demonstrating the tool is available at http://bit. ly/glass-lak12.  Keywords Learning analytics, visualization system, visualization frame- work  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.1 [Computers Uses in Education]: Collaborative learning, Computer-assisted instruction (CAI), Computer-managed in- struction (CMI), Distance learning  Acknowledgment Work partially funded by the EEE project, Plan Nacional de I+D+I TIN2011-28308-C03-01 and the Emadrid: In- vestigacion y desarrollo de tecnologias para el e-learning en la Comunidad de Madrid project (S2009/TIC-1650).  1. REFERENCES [1] S. Card, J. Mackinlay, and B. Shneiderman. Readings  in information visualization: using vision to think, chapter 1, pages 134. Morgan Kaufmann, 1999.  [2] S. Govaerts, K. Verbert, E. Duval, and A. Pardo. The Student Activity Meter for Awareness and Self-reflection. In ACM SIGCHI International Conference on Human Factors in Computing Systems, 2012.  [3] R. Mazza and V. Dimitrova. Visualising student tracking data to support instructors in web-based distance education. In Proceedings of the 13th international World Wide Web conference on Alternate track papers & posters, pages 154161. ACM, 2004.  [4] V. A. Romero Zaldvar, A. Pardo, D. Burgos, and C. Delgado Kloos. Monitoring Student Progress Using Virtual Appliances : A Case Study. Computers & Education, 58(4):10581067, 2012.  [5] J. Santos, K. Verbert, S. Govaerts, and E. Duval. Visualizing ple usage. In Proceedings of EFEPLE11 1st Workshop on Exploring the Fitness and Evolvability of Personal Learning Environments. CEUR workshop proceedings, 2011.  [6] M. Wolpers, J. Najjar, K. Verbert, and E. Duval. Tracking actual usage: the attention metadata approach. Journal of Technology Education & Society, 10(3):106121, 2007.  163      "}
{"index":{"_id":"29"}}
{"datatype":"inproceedings","key":"Smolin:2012:AAI:2330601.2330644","author":"Smolin, Denis and Butakov, Sergey","title":"Applying Artificial Intelligence to the Educational Data: An Example of Syllabus Quality Analysis","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"164--169","numpages":"6","url":"http://doi.acm.org/10.1145/2330601.2330644","doi":"10.1145/2330601.2330644","acmid":"2330644","publisher":"ACM","address":"New York, NY, USA","keywords":"architectures for educational technology system, cognitive graphics, evaluation methodologies, intelligent tutoring systems, learning analytics","Abstract":"Developing new courses and updating existing ones are routine activities for an educator. The quality of a new or updated course depends on the course structure as well as its individual elements. The syllabus defines the structure and the details of the course, thus contributing to the overall quality of the course. This research proposes a new AI based framework to manage the quality of the syllabus. We apply AI methods to automatically evaluate a syllabus on the basis of such characteristics as validity, usability, and efficiency. We provide user trials to show the advantages of the developed approach against the traditional human-based process of syllabi verification and evaluation.","pdf":"APPLYING ARTIFICIAL INTELLIGENCE TO THE   EDUCATIONAL DATA: AN EXAMPLE OF SYLLABUS   QUALITY ANALYSIS   Denis Smolin  American University in Bosnia and Herzegovina    Mije Keroevia Guje 3, 75000 Tuzla,   Bosnia and Herzegovina     denis.smoline@gmail.com   Sergey Butakov  SolBridge International School of Business    151-13 Samsung 1-dong, Dong-gu,   Daejeon, 300-814, South Korea    +82.42.630.8531, butakov@solbridge.ac.kr      ABSTRACT  Developing new courses and updating existing ones are routine  activities for an educator. The quality of a new or updated  course depends on the course structure as well as its individual  elements. The syllabus defines the structure and the details of  the course, thus contributing to the overall quality of the course.  This research proposes a new AI based framework to manage  the quality of the syllabus. We apply AI methods to  automatically evaluate a syllabus on the basis of such  characteristics as validity, usability, and efficiency. We provide  user trials to show the advantages of the developed approach  against the traditional human-based process of syllabi  verification and evaluation.    Categories and Subject Descriptors  J.1 [ADMINISTRATIVE DATA PROCESSING] Education;  K.3.1 [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning; I.2 [ARTIFICIAL  INTELLIGENCE]: Applications and Expert Systems.   General Terms  Design, Human Factors, Measurement   Keywords  Architectures for educational technology system, evaluation  methodologies, intelligent tutoring systems, cognitive graphics,  learning analytics.   1. INTRODUCTION: THE IMPORTANCE   OF SYLLABUS STRUCTURE ANALYSIS   How to increase the efficiency of teaching is one of the top   problems in this new information century. For years, the best  educators have been developing new approaches to make the  process efficient while keeping it affordable. From the  beginning of the computer era, educators believe that  informational technologies could significantly improve the  outcomes that students get from the courses. Some of these  expected advantages are now experimentally proven:    -increase of student productivity  [10] as the result of  implementation of new teaching tools  [3] and more relevant  computer adaptive testing  [18];    -increase of learning efficiency with the help of new forms  of information presentation. For example, dual-coding approach    [2] with use of multimedia allows students to show better  performance on tests in comparison with those who learn from  just animation or text-based materials.   In exploring these advantages, many authors pay special  attention to the structural changes in the learning process itself.  IT changes the traditional roles of an instructor and a student  and these changes are not always positive. An educator should  strive to accentuate the positive and eliminate the negative  elements of the course structure. Among well-known negative   effects, we should mention digital plagiarism  [19]; loss of  learning efficiency due to disruption and information   overwhelming  [10] [11]; visual or mental fatigue  [22] and others.   This paper considers a syllabus as a good scenario in which an  instructor and a student play their roles to achieve course  objectives. We concentrate on the quality of the syllabus and  apply AI methods to eliminate negative elements from the  syllabus structure.    2. SYLLABUS QUALITY CHARACTE-  RISTICS FOR THE AUTOMATED ANALYSIS   Public and private institutions in different national   educational systems use a variety of methods to evaluate   syllabus or course quality  [21]:    -standardized tests to evaluate students levels of  competence;   -interview-like exams and final course projects;   -student end-of-course evaluations. Many researchers state  that such evaluations provide independent opinions that are   correlated with students' knowledge and skill sets  [5] [14]. But  some claim that students can misuse them or that they can be   misinterpreted by the administration  [8] [15].    The evaluation of quality depends on the evaluators point  of view. The European Foundation for Quality Management  defines the following groups of evaluators:   -the corporate world of potential employers;    -students families and prospective students, who need  information on which to base their choice;    -alumni, who may require additional training and others;      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are   not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,   requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   164    Thus, it is necessary to align the syllabus structure with the  sets of criteria and methods provided by the above-mentioned  groups of evaluators. This work is obviously labor intensive and  requires some level of automation. In practice the educator  adopts the most convenient syllabus prototype from his or her  archive or any other accessible source. For example, in  computer related disciplines it could be community-driven  Microsoft Faculty Connect  (www.microsoft.com/education/facultyconnection/) or it could  be MITs famous OpenCourseWare or some other discipline  specific courses.    Unfortunately, there is no search by quality function in  such archives. In recent publications, we can find methods for   quality assessment  [7] [20], but most of these works are based on  an expert-centric approach. Community-driven resources in  most of the cases have some ranking mechanisms mostly based  on user opinions. In this research, we choose a statistical  approach because it makes the evaluation process more  objective. The following quality characteristics have been  studied in this paper:    - Validity. Each syllabus has an underlying cognitive model   of the subject as a basis for the set of topics to be taught  [9].  This model should be valid. To evaluate its validity, we need to  check how closely the real outcomes of the course match the  expected outcomes, as declared by the course author.   - Usability. The problem of syllabus usability has two  dimensions: usability for the student and usability for the  instructor. The first is mainly about the accuracy of the course  description and clarity of the objectives. The syllabus  presentation should facilitate the understanding of its content.  One of the key factors here is the requirement for a syllabus to  be understandable by a student. The issue of instructor usability  focuses more on the ease with which the educator can  implement and modify / re-use the syllabus.    - Efficiency. Because all educational institutions have  resource constraints, the syllabus should consume the minimum  amount of resources while achieving the course objectives. For  example, if one can show that, in a Digital Systems course,  students have mastered the same skills using simulation software  instead of expensive circuit boards, and then we should  recommend the syllabus that uses such simulation software. E.g.  one structural element of the course has been substituted with  another one.   Statistics procedures could be used for syllabus evaluation  across the characteristics discussed above. To perform such an  evaluation, an institution has to have a database with course  outcome metrics, information about student performance,  estimates of course implementation costs, faculty evaluations  and other. Fortunately, all this information could be found in  course management systems such as BlackBoard, Angel,  Moodle, etc.    3. ARTIFICIAL INTELLIGENCE FOR   QUALITY EVALUATION   A key feature of the proposed approach is the two-step   procedure based upon a priori and a posteriori syllabus  validity and efficiency evaluation. The first evaluation detects a  number of latent problems in the syllabus structure with the help  of rule-based knowledge base. With such an evaluation, one can  correct the faulty course structure before implementing the  syllabus in an actual class. The second evaluation (a posteriori,  on the basis of students examination results and some  additional information) allows us to measure how close the real   student outcomes are to the expected outcomes. This is an  intelligent criterion for step-by-step quality improvement.    To evaluate a priori validity, we analyze the consistency of the  syllabus. Even though the syllabus structure differs from one  educational institution to another, some acknowledged patterns   could be found in the vast majority of syllabi ( [12] [6]):   1. basic information (current year and semester, course  title, etc);    2. prerequisites for the course;   3. general learning goals or objectives;    4. conceptual structure of the course, activities,  textbooks, assignments, term papers, and exams;   5. grading and evaluation criteria, policies and others.    These sections are linked to each other and these relations are  not homogeneous (see Figure 1). In fact, this is a semantic  network. And this network contains some problems in its  structure. For example, the Course calendar and the Textbooks  sections are linked by the refers to relation. Sometimes, the  books are listed in one section, and also repeated in another one.  This duplicate causes a problem while modifying the syllabus.          Figure 1. Syllabus as a semantic network   The mapping relation requires explanation. We suggest using  it if one element of a syllabus affects the content of another one.  For example, the same set of learning objectives might be  fulfilled by various conceptual structures, a topic understanding  could be assessed with one or more tests (see Figure 2), etc.        Figure 2. Mapping of tests to topics.   Table 1 shows the most typical relations between elements of a  syllabus. This table highlights the fact that almost every type of  relation may have problems in real syllabi. For example, some  sections might be linked to non-existent elements (dead link);   Final test   Test 2   Topic 3   Test 1   Topic 1  Topic 2   lim it s    re fe  rs  t o     mapping   mapping   Course calendar   Textbooks   Learning  Goals   defines   Additional  materials   Conceptual  structure   Assignments, term papers, and exams   165    some elements may be not linked at all (redundant link).  Please note, that we describe these problems in the term of  rules. A link is dead if there is no appropriate book in the  textbooks section.   Table 1. Typical Syllabus Section Relations   Relation  type   Typical  Problems   (Rule Left Side)   Possible Examples   (Rule Right Side)   Refers to dead link if it doesnt exist in the appropriate  section of the syllabus.   redundant link if  it is never used in the syllabus  sections.    maldistribution  of links if   one topic of the syllabus is   supported with much more (ratio is  more than threshold) links than  others.   Mapping  polysemy if  the test for the topic covers more   than this topic or allows different  sequences of tests.    non-optimal  mapping if    bad correlation between the names  of the learning goals and the names  of topics.   Defines polysemy if allows selection of at least two tests  with significantly different  characteristics. For example, if one   test has to be complete in 50 minutes  while the other one in 160 minutes.   illogical  connection if   the listed textbook is not related to  the topic.    Limits redundant  limitations if    limits on the Internet traffic.    diseconomy if the use of excessive tests.   Summing contradiction if the number of course hours in basic  info section contradicts the really   needed time (sum of hours to  complete the tests, implement the  assignments etc).    To obtain a complete description of the syllabus structure,  we also need to consider internal relations within syllabus  elements. For example, the internal structure of the textbooks  section has the one relation type alphabetical order. Another  example of internal relations is shown in Figure 3.       Figure 3. An internal structure and two corresponding   syllabi.   In Figure 3, one can see that two possible pathways on the   negotiated syllabus  [4] lead to the same final exam. These  syllabi are conceptually equal but might demand different  amounts of resources (e.g. time, labs, etc).   To perform the a priori statistical evaluation of the  syllabus quality, we need to count numerous problems detected  in the structure of the syllabus. Being hardly implemented with a  traditional expert-based evaluation process the problem solves  easily with the help of ordinal rule-based system and the rules  like the following: Count limit violation problem if time for  the test is more than the class time.   A posteriori validity evaluation is based on a statistical  procedure that compares expected student outcomes and  examination results (interviews, tests or practical work). There  are several methods to calculate a posteriori validity, and we  employ the simplest one. One is expected grade distribution set  by an institution. For example: about 15 percent of students get  A mark, about 20 percent get B, about 50 percent get C  and D and up to 15 percent fail a course. If the real results are  significantly better (see Figure 4) than the expected distribution,  we consider the course to be too simple for the students. If the  results are shifted towards poor grades, then the course is  considered to be too difficult for them to understand. This could  be caused by insufficient methodological supplies, by the  professors errors, or by some other reasons. We shape some  requirements to the statistical distribution of results.      Figure 4. Exam results for the very difficult, very simple   courses and a course of required difficulty.   We believe that the worst results appear when the outcome  is far different from the expected statistical distribution. For the  above-mentioned requirements, the mean3.5 and the  dispersion0.99. If we consider the worst possible outcome  (100% of students got F mark), mean=2, and the  dispersion=0. Finally, if consider the  best possible  result (in  fact this result is very unwanted) the mean=5, and the  dispersion=0. We use the chi-square method to determine  whether the real course outcomes are close to the declared ones.  The course is reliable if it is valid in a number of applications.  One of the simplest ways to calculate the reliability of the course  is a re-examination. The course is reliable if the result of re- examination repeats the original examination result. If the re- examination result is better, then we believe that a student has  obtained some additional materials (not included in the course)  that helped him or her to improve the result. So, the course is not  reliable (maybe due to the insufficiency). If the re-examination  result is worse than the first examination, then the course is also  not reliable (perhaps due to the redundancy or because it is too  hard for students). To calculate the reliability, a simple sign test  method could be implemented. It allows the detection of some  typical shifts in data.    The following section is dedicated to the practical  implementation of the discussed theoretical issues with the help  of specially developed software tool, called Chopin.   4. EXPERT SYSTEM PROTOTYPE  System prototype has been created to test the above   mentioned statistical and rule-based techniques. Top level  system structure is presented in Figure 5. System employs two  internal languages to represent all its data: Test Description  Language (TDL) and Expert Description Language (EDL). TDL  describes adaptive test structure. It is similar to the well-known   QTI language by IMS Global Learning Consortium  [13] used in  modern CSM (e.g. Moodle), but also has some advanced     D        C       B         A   students results     2         3        4         5   n u  m b er   o f   re su  lt s   Test 1 Test 2   Final test Syllabus 1    Test 1  Test 2      Syllabus 2    Test 2  Test 1      166    features  [17]. EDL describes the syllabus as a mathematical   graph (see Figure 6). An instructor can draw a graph with EDL  editor or fill it in as a MS Word template which converts itself to  EDL automatically. The program called VIPES applies rules,  evaluates the syllabus quality and visualizes the syllabus as a  graph.   When a student starts the VIPES program, it graphically  visualizes the syllabus and the students achievements. The  student chooses the objective for his/her next activities, and the  system generates the learning path consisting of virtually any  type of activities, from reading to peer-reviewing of submitted  assignments. The choice here is to generate either the complete  path to the end of the course or just to cover some topics. If the  generated plan (scenario) does not match the students  expectations, then she/he can ask the system to schedule an on- line chat with the instructor. This is a very effective new  teaching method that is hardly feasible in the traditional class  format. Now instructor concentrates on the core of the students  problem, while the VIPES system helps to diagnose and  visualize the problem. In creating the syllabus, an instructor  develops a set of scenarios to match the needs of as many  students as possible. Thus, each syllabus is a set of scenarios (in  contrast to a traditional single-scenario syllabus), so it has more  chances to fit the needs of an individual student. On the other  hand, it takes much more time to create and test a multi-scenario  syllabus. That is why this approach demands automatic quality  correction procedures.   Using this system, with data collected over more than seven   years, the set of experiments have been performed on syllabus  quality evaluation and syllabus structure variations. Results of  two experiments are presented in the following section.   5. RESULTS AND DISCUSSION   To study the effectiveness of the proposed approach and to  evaluate the scope of its implementation, we discuss two  continuous experiments here. The first one was aimed at  evaluating the quality of many syllabi, and, in the second  experiment, we monitored the structure changes and evaluations  of one syllabus for several years.  For the first experiment, we  have selected a set of 15 syllabi (Altai State Technical  University, MIS program) and performed the a priori quality  estimation (see Table 2). While performing evaluations, we  eliminated the identified problems by restructuring of the  syllabi. We also checked the correlation between the number of  identified problems and the students' results. The experiment  shows the significant correlation between syllabus consistency  and students outcomes. As can be seen from Table 2, there is a  direct relation between the number of consistency violations and  the percentage of unsatisfactory student outcomes. Different  consistency problems affect student results differently: for  example, insufficient mapping affects the students' results  more than non-optimal mapping, etc. In practice, the majority  of syllabi which were already checked by experts had some level  of consistency problems.          Figure 5. Structural elements of the system.   `     Figure 6. A syllabus as an EDL script and its graph visualization.   A fragment of EDL script:   ExpertName: AI2007.01;   Tree:   A1,1=A2,1;              A2,1=A3,1*A3,2   Tests:  A3,1=test 1;   Links:    A3,1=AI1999b, AI2005c;   Items:   A1.1=the course is complete and the results are  acceptable;   A2.1=topic 3;   Course ToDo  list: goals, links  and tests      Database   of EDL scripts,  TDL tests and   testing results, logs,  etc.   EDL -based  Expert System     VIPES   TDL- based  Testing program   EDL editor    TDL Test editor   A set of data analysis    programs  Syllabus   developers,  teachers, test   developers and   administrators      Student   167       Table 3.  A posteriori monitoring and optimization of the E-business syllabus  Step Validity    (chi-square test)  Reliability   (sign test)   Results of re-examination   original syllabus non valid not reliable, maybe  the course is  insufficient   41% improved and 9% worsen their result. It  means that the course results are unstable.   detected problems checked non valid not reliable 38% improved and 12% worsen their result   updated content non valid not reliable 27% improved and 6% worsen their result   revised course, the sequenced of  topics changed   valid reliable 15% improved and 6% worsen their result. Sign  test evaluates these deviations as insignificant.    full hard copy of course materials  is available for students   valid reliable 0% improved and 3% worsen their result   full copy of course materials is  available on internet-site   valid reliable 0% improved and 6% worsen their result   updated content, lectures and  practice are divided into two  independent blocks with two  professors   valid not reliable       50% improved and 6% worsen their result. Sign  test says that exam and re-exam distributions are  significantly different.   course content reordered, two  professors   valid reliable 6% improved and 6% worsen their result   using of CMS valid reliable 9% improved and 6% worsen their result             Table 2. A fragment of a priori monitoring of the syllabi database   Syllabus name  number of   detected problems   unsatisfactory   exam results, %  Notes   Networks and  communications    2 30% Maldistribution of tests during the course, the first part of  the course covered with many tests while the rest of the  course is uncovered at all; it also has dead links.   Information Systems 1 30% Non-optimal mapping. The most important topics covered  with tests but not practical work.   Databases 1 16% practical course time exceeds the available (10-15%)   contradictions   Operating systems 0 15%    E-business 5 40% Insufficient mapping due to the lack of tests, case studies,  etc; also a dead link.   Artificial Intelligence 0 10%    Information Security 0 10%    Geographical  Information Systems   2 27% practical course time exceeds the available (more than 15%)   contradictions, insufficient lab facilities   Decision Making 0 10%    168    The second part of the user trials was to follow the changes in  one syllabus. The E-Business course was selected for this part of  the research. It was offered several times a year for different  majors. The students results showed some deviations, the  causes of which were not immediately obvious. Sometimes, all  the students were very successful in examinations, and  sometimes, the majority of students got poor results. The course  was originally taught by one professor. Later, the theoretical and  practical parts of the course were separated and taught by two  different professors. Such a change was implemented to make  the students evaluations more clear and to exclude any personal  preferences from the examination. It was necessary to establish  an indicator that would inform the department chair that some  problems must be corrected in order to improve the quality of  the course. Table 3 shows the results of the validity and  reliability monitoring.    The table shows how the validity and reliability characteristics  could be applied as objective tests to changes in the syllabus.  The professors modified the syllabus step by step until its  validity and reliability were not stabilized. Later they updated its  contents and optimized it with the same characteristics as the  goal function. They assumed the elements to be included to the  course (such as on-line materials instead of paper-based) and  checked the efficiency of these changes with the objective  criteria.   The users (instructors, administrators and students) agreed that  this approach has positive effects on the syllabi database. They  stated that it helps them to make the evaluation more objective  and to highlight the key factors affecting the quality of teaching.    The users also agreed that this AI-based technique is a  promising one for accreditation activities. The comparison of  syllabi/curricula is one of the most critical stages of this process.    This approach contributes also to the theory of semantic  matching problem. It is also applicable in practice, in the case  of a syllabus we need to compare short parts of documents: in  particular, the course goals and the description of course content  should be compared with appropriate parts of the guide from the  accreditation body. This could be considered as a possible new  direction for development of these software tools, i.e., as an  extension for this project. Another promising extension is  related to the development of EDL and TDL languages which  promise to be more efficient in their narrow fields than the   existing common ones  [17] [16]. The existence of well defined  and statistically proven criteria for the evaluation and  optimization of test and syllabus structure gives us a prospect to  build up a complete logical system for this particular case of  educational data analysis.      REFERENCES      [1] ACM CR (2007), ACM Curricula Recommendations.  Available from http://www.acm.org/education/curricula- recommendations    [2] Anderson, J. R. (2005). Cognitive Psychology and its  implications. New York: Worth Publishers.   [3] Chittaro, L., Ranon, R. (2007). Web3D Technologies in  Learning, Education and Training: Motivations, Issues,  Opportunities. Computers & Education, 49(1), 318.   [4] Clarke, D. (1991). The Negotiated Syllabus: What Is It And  How Is It Likely To Work Applied Linguistics, 12, 13-28.   [5] Danielson, C., McGreal, T. (2000) Teacher Evaluation to  Enhance Professional Practice. Alexandria: ASCD.    [6] Davis, B. (1993). Tools for Teaching. San Francisco:  Jossey-Bass Publishers.   [7] Diamond, R. (1998). Designing & assessing courses &  curricula: A practical guide. San Francisco:  Jossey-Bass  Publishers.   [8] Greenwald, A. (1997). Validity concerns and usefulness of  student ratings of instruction. American Psychologist,  52(11), 1182-1186.    [9] Grunert, J. (1997). The Course Syllabus: A Learning- Centered Approach. Bolton, Massachusetts: Anker  Publishing Company, Inc.   [10] Jing Lei, Yong Zhao, (2007). Technology uses and student  achievement: A longitudinal study. Computers &  Education, 49(2), 284-296.   [11] Mahdizadeh, H. et al., (2007). Determining factors of the  use of e-learning environments by university teachers.  Computers & Education,  doi:10.1016/j.compedu.2007.04.004 (in press)    [12] McKenney, S. (2001). Computer based support for science  education materials developers in Africa: exploring  potentials. University of Twente, Enschede.   [13] QTI, (2008). IMS Question & Test Interoperability  Specification. Available from   http://imsproject.org/question/   [14] Scriven, M. (1995). Student ratings offer useful input to  teacher evaluations. (ERIC Reproduction Service No.  ED39824)   [15] Simpson, R. (1995). Uses and misuses of student  evaluations of teaching effectiveness. Innovative Higher  Education, 20(1), 3-5.   [16] Smolin D. (2011). Testing with the Computer: State of the  Art, Needs and Perspective. In Advances in Psychology  Research. Nova Science Publishers, 87, 71-76.    [17] Smoline D. (2008). Some problems of computer-aided  testing and interview-like tests, Computers & Education, 51  (2), 743-756.    [18] Thissen, D., & Mislevy, R.J. (2000). Testing Algorithms. In  Wainer, H. (Ed.) Computerized Adaptive Testing: A  Primer. Mahwah, NJ: Lawrence Erlbaum Associates.   [19] Underwood,J., Szabo, A. (2003) Academic offences and e- learning: individual propensities in cheating Source. British  Journal of Educational Technology, 34(4), 467-477(11)   [20] Wolf, P., Hill A., Evers F. (2006). Handbook for  curriculum assessment. University of Guelf.    [21] Zhao, Yong. (2007) China and the Whole Child.  Educational Leadership,64(8), 70-73.   [22] Zheng Yan, Liang Hu, Hao Chen and Fan Lu. (2008)  Computer Vision Syndrome: A widely spreading but  largely unknown epidemic among computer users.  Computers in Human Behavior, 24, 20262042.          169      "}
{"index":{"_id":"30"}}
{"datatype":"inproceedings","key":"Garcia-Solorzano:2012:EMT:2330601.2330645","author":"Garc'ia-Sol'orzano, David and Cobo, Germ'an and Santamar'ia, Eug`enia and Mor'an, Jose Antonio and Monzo, Carlos and Melench'on, Javier","title":"Educational Monitoring Tool Based on Faceted Browsing and Data Portraits","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"170--178","numpages":"9","url":"http://doi.acm.org/10.1145/2330601.2330645","doi":"10.1145/2330601.2330645","acmid":"2330645","publisher":"ACM","address":"New York, NY, USA","keywords":"data portraits, faceted browsing, information visualization, instructor support, learning analytics, student monitoring","Abstract":"Due to the idiosyncrasy of online education, students may become disoriented, frustrated or confused if they do not receive the support, feedback or guidance needed to be successful. To avoid this, the role of teachers is essential. In this regard, instructors should be facilitators who guide students throughout the teaching-learning process and arrange meaningful learner-centered experiences. However, unlike face-to-face classes, teachers have difficulty in monitoring their learners in an online environment, since a lot of learning management systems provide faculty with student tracking data in a poor tabular format that is difficult to understand. In order to overcome this drawback, this paper presents a novel graphical educational monitoring tool based on faceted browsing that helps instructors to gain an insight into their classrooms' performance. Moreover, this tool depicts information of each individual student by using a data portrait. Thanks to this monitoring tool, teachers can, on the one hand, track their students during the teaching-learning process and, on the other, detect potential problems in time.","pdf":"Educational Monitoring Tool Based on Faceted Browsing and Data Portraits  David Garca-Solrzano Universitat Oberta de  Catalunya Rambla del Poblenou, 156  Barcelona, Spain dgarciaso@uoc.edu  Germn Cobo Universitat Oberta de  Catalunya Rambla del Poblenou, 156  Barcelona, Spain gcobo@uoc.edu  Eugnia Santamara Universitat Oberta de  Catalunya Rambla del Poblenou, 156  Barcelona, Spain esantamaria@uoc.edu  Jose Antonio Morn Universitat Oberta de  Catalunya Rambla del Poblenou, 156  Barcelona, Spain jmoranm@uoc.edu  Carlos Monzo Universitat Oberta de  Catalunya Rambla del Poblenou, 156  Barcelona, Spain cmonzo@uoc.edu  Javier Melenchn Universitat Oberta de  Catalunya Rambla del Poblenou, 156  Barcelona, Spain jmelenchonm@uoc.edu  ABSTRACT Due to the idiosyncrasy of online education, students may become disoriented, frustrated or confused if they do not receive the support, feedback or guidance needed to be suc- cessful. To avoid this, the role of teachers is essential. In this regard, instructors should be facilitators who guide students throughout the teaching-learning process and arrange mean- ingful learner-centered experiences. However, unlike face- to-face classes, teachers have difficulty in monitoring their learners in an online environment, since a lot of learning management systems provide faculty with student tracking data in a poor tabular format that is difficult to understand. In order to overcome this drawback, this paper presents a novel graphical educational monitoring tool based on faceted browsing that helps instructors to gain an insight into their classrooms performance. Moreover, this tool depicts infor- mation of each individual student by using a data portrait. Thanks to this monitoring tool, teachers can, on the one hand, track their students during the teaching-learning pro- cess and, on the other, detect potential problems in time.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3 [Computer Uses in Education]: Distance Learning; H.5 [User Interfaces]: Graphical user interfaces  General Terms Management, Design, Human Factors  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 12, 29 April  2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM 978-1-4503-1111-3/12/04 ...$10.00.  Keywords Learning analytics, information visualization, student mon- itoring, instructor support, faceted browsing, data portraits  1. INTRODUCTION In the last years, distance learning has become very pop-  ular, especially, its modality of online education. In this regard, online students have different characteristics, needs and preferences to the ones who attend face-to-face classes. For instance, online learners are often over 30 years old, which means having learners with different professional and educational backgrounds in the same classroom. This het- erogeneity entails helping students with different needs at the same time. Likewise, because most learners are adult, they have a lot of responsibilities, such as: working, taking care of children, etc. Consequently, it is hardly surprising that they prefer studying at their convenience, i.e. anytime, anywhere and at their own pace.  As a result of the aforementioned particularities, online education should avoid using a teacher-centered paradigm, since it treats all learners as if they were the same and forces them to do the same things in the same amount of time [32]. Instead, online education, as any kind of distance learning, involves a student-centered approach in which the instructor is a facilitator and students engage in peer learning [19, 25]. Thus, students are required to take primary responsibility for their learning process [4]. To do this, learners need to have some specific skills, e.g. self-regulation. However, some studies [23, 26] provide evidence that a lot of students need some help to learn these skills, since most of them are not able to achieve these abilities on their own. Consequently, the role of teachers shifts from a masterful figure to a facilita- tor who guides learners throughout the course and arranges meaningful learner-centered experiences [29]. This role is es- sential in online education, since learners may become dis- oriented, frustrated or confused if they do not receive the support, feedback or guidance needed to be successful [8].  In order to carry out a good instruction in an online en- vironment, instructors need appropriate means to set up a monitoring process so that they can be aware of the stu-  170    dents learning process and provide learners with just-in- time assistance. In addition, monitoring allows teachers to forecast potential problems (e.g. dropouts) and avoid them in time. In this regard, instructors can use tracking infor- mation (e.g. logs) and monitoring tools which are available in learning management systems (LMSs). However, a com- parative study on tracking functionalities of various LMSs [10] concluded that none of them offer much tracking abil- ity. One of the reasons of this devastating conclusion may be the fact that these platforms often provide tracking data in a tabular format which is commonly poorly structured, incomprehensible and difficult to understand [21]. To avoid the aforesaid drawbacks, this paper presents a  novel graphical educational monitoring tool based on faceted browsing and data portraits. On the one hand, faceted browsing, as an exploratory search technique, helps instruc- tors to narrow the class roster down until finding those learn- ers who meet teachers requirements. On the other hand, data portraits depict summarized information about an in- dividual student in a single image. The rest of the paper is structured as follows: a catego-  rization of educational monitoring tools can be found in the next section. In section 3, an educational monitoring tool based on faceted browsing and data portraits is proposed. Finally, conclusions and future work are in section 4.  2. CLASSIFICATION OF EDUCATIONAL MONITORING TOOLS  This paper classifies educational monitoring tools into two categories: (1) according to the data processing techniques that they use, and (2) the element which they monitor. They both are described below.  2.1 Data Processing Techniques The main goal of any monitoring tool, whether this has an  educational purpose or not, is to give users insights on the data at which they are looking. To this end, two different data processing techniques can mainly be used. On the one hand, information visualization (IV) techniques and, on the other, data mining (DM) algorithms. Next, they both are explained in sections 2.1.1 and 2.1.2, respectively. At the same time, some relevant educational examples related to each method are described briefly. Finally, a brief discussion about the main differences between IV and DM is presented in the section 2.1.3.  2.1.1 Information Visualization One of the most well-known definitions of information vi-  sualization (IV) was proposed by Card et al. [3]: the use of computer-supported, interactive, visual representations of abstract data to amplify cognition. Thus, IV encompasses a set of techniques that transform data into effective graph- ical representations by taking properties of human visual perception into consideration. Thereby, these visual repre- sentations reveal facts and trends which allow users to infer some unknown information by combining the visual inputs with their knowledge of data. To stress that the data pro- cessing which is performed in IV is mainly based on simple mathematical and statistical functions, such as sum, calcu- lation of percentages, mean, median, mode, and so on. In the educational context, some proposals based on IV  techniques have been suggested. Two of the most relevant  tools are CourseVis [21] and, its successor, GISMO [22]. The former is a stand-alone visualization tool that obtains track- ing data from WebCT, transforms them into a form con- venient for processing and generates graphical representa- tions that can be explored and manipulated by instructors. Thereby, teachers can examine social (it uses a 3D visual- ization of discussion boards), cognitive (it uses a matrix in which each cell is the grade attained by a specific student in a particular quiz), and behavioral (it shows information about access and participation in a timeline) aspects of stu- dents. CourseVis mainly uses 2D visualization techniques, but it also uses color and shape as a third dimension.  As far as GISMO is concerned, this also uses LMS track- ing data, but in this case from Moodle, to display graphical representations (e.g. bar charts, matrices, etc.) about over- all classroom accesses and detailed information of a specific student. This was developed as a Moodle block, but its interface is detached from Moodles one.  Similarly, Zhang et al. [35] designed Moodog, a visual student tracking data plug-in for Moodle which also sends automatic reminder e-mails to students. This displays in- formation about the course, students, resources and access time. Unlike GISMO, the information provided by Moodog is integrated into Moodles interface, keeping the original Moodles layout as much as possible.  There are many other works that have also proposed dif- ferent representations based on IV techniques. For instance, Hardy et al. [9] constructed, as a graph, the route taken by one student through the course material during a sin- gle work session; Hijon-Neira and Velazquez-Iturbide [11] used an interactive graph (for students grades) and a data mountain (for access) by using Prefuse API; Juan et al. [14], in turn, proposed scatter plots and quadrants as well as an evolution graph in order to represent students performance; and Bakharia et al. [2] proposed SNAPP, a social network analysis tool that displays, as a graph, the evolution of par- ticipants relationships within an LMSs discussion forums.  2.1.2 Data Mining According to Romero and Ventura [28], data mining (DM)  techniques can be classified into four categories: (1) cluster- ing, classification and outlier detection; (2) association rule mining and sequential pattern mining; (3) text mining; and (4) statistics and visualization. However, the latter is not universally seen as a category of DM [1], since the data pro- cessing that is carried out is minimum compared to the rest of categories, which are based on artificial intelligence algo- rithms. In fact, the fourth category proposed by Romero and Ventura accords with the definition of information visu- alization (IV) presented in the section 2.1.1. Hence, in this paper, only the first three categories defined by Romero and Ventura are considered as belonging to data mining.  DM techniques are able to infer underlying patterns from a large database, generating some new type of valuable in- formation such as student models or predictions. These new data can be delivered in writing or visually.  For some time now, more and more educational monitor- ing tools are based on DM. For example, Kosba et al. [17] developed TADV, a system that builds student, group and classroom models by using fuzzy logic. From these models, TADV gives instructors advice, e.g. to advise a learner to review a concept. Thereby, teachers have extra information to make appropriate decisions during the course.  171    On the other hand, Hung and Zhang [12] used statistical models and machine learning algorithms to analyze patterns of online learning behaviors and, at the same time, to make predictions on learning outcomes. Zorrilla et al. [37], in turn, proposed a decision support  system that utilized different techniques. First, two clus- tering algorithms, Expectation-Maximization (EM) and K- Means, were used to characterize, on the one hand, students and, on the other, sessions. The output of EM, a probabil- ity distribution, allowed to determine the number of clusters with which K-Means would be executed. Depending on the input data, the clusters obtained from K-Means described either student behavior models or session patterns. In ad- dition to the information provided by the clusters, the pro- posal of Zorrilla et al. also indicated the resources that were commonly used together. To obtain these data, Apriori, an algorithm for finding association rules, was employed. More proposals of educational monitoring tools based on  DM techniques can be found in [1, 28].  2.1.3 Information Visualization vs. Data Mining One of the main differences between information visual-  ization (IV) and data mining (DM) is how new information is inferred. Due to the complexity of the algorithms used in DM, these are able to suggest latent information with an explanation based on text, rules, clusters, etc., e.g. student S is about to drop out because she has not acceded to the classroom for two weeks. By contrast, in the case of tools based on IV techniques, new information is not inferred by an algorithm, but by users through observing graphics and taking advantage of their knowledge of the domain. As a result of the manner of inferring new information,  two issues arise: (1) reliability, and (2) computational cost. With regard to reliability, the users of tools based on DM must rely on the accuracy of the information suggested by the algorithm, whereas the users of tools based on IV infer reliable information based on their own expertise. As far as computational cost is concerned, IV techniques  calculates statistical data that require minimum processing, while tools based on DM executes complex artificial intelli- gence algorithms which are usually time-consuming. Another important aspect is user-friendliness. Merceron  and Yacef [24] claim that it is essential to use techniques and measurements which are fairly intuitive and easy to inter- pret. In this regard, the explanation provided by the tools based on DM often requires users to master the algorithm so as to understand it. Unfortunately, a lot of users neither have this knowledge nor can make time for acquiring it. On the contrary, tools based on IV seem to meet these require- ments better, since they use simple statistical data and focus on displaying information in a visual and effective way. Despite the differences between information visualization  and data mining, they can work together in an application. For instance, a system can first infer some latent informa- tion from a large database by using a DM algorithm and then display it by using a visual representation based on IV tech- niques. Thereby, users can benefit from the advantages of each technique: on the one hand, the possibility of inferring new data automatically and, on the other, the capability of visual representations to convey information effectively. This combination of DM algorithms with IV techniques is called visual data mining [15]. There is every indication that this new discipline will become a promising research area.  2.2 Monitored Element Although the most popular name is student monitoring  tools, it would be more correct to call them educational monitoring tools. The reason is that they can monitor other items in addition to learners. In this regard, this section describes the elements that are commonly monitored in an educational context. Finally, to stress that both IV and DM techniques can be used to track any of the following items.  2.2.1 Classroom The most common item is the whole classroom, i.e. all  students are considered a unique entity. This entity is char- acterized by the overall information that comes from com- bining all students tracking data (e.g. classs average grade in each assignment). Thereby, instructors are provided with an overview of their learners performance. Thanks to this, teachers can make decisions that affect the whole class. Most educational monitoring tools show data of the classroom.  2.2.2 Student Group A particular case of the previous monitoring is the one  which focuses on groups. This is useful when group activi- ties are proposed, e.g. tasks that belong to a project-based learning. The supervision of groups allows to obtain in- formation about how students interact each other, who are the most and least participative members of a group, which group has the best and the worst performance, etc. [2, 14, 27] are examples of group monitoring tools.  2.2.3 Individual Student Teachers often need to take a closer look at a particu-  lar learner or make comparisons between students. Conse- quently, they need tools that provide detailed information about an individual learner (e.g. how many times a student has accessed the LMS). These data may help instructors to gain understanding of the reasons why a specific learner has a particular behavior. Thereby, teachers can offer each stu- dent a better support and a tailored learning experience. Re- garding this, CourseVis [21] and Moodog [35] are two tools that display information of a particular student.  2.2.4 Resources The term resource encompasses a wide spectrum of el-  ements, from learning materials (e.g. documents, videos, quizzes, etc.) to educational tools (e.g. forum, chat, LMSs pages, etc.). Regarding this, there are multiple types of data related to resources that can be tracked, e.g. how many times a document has been read, how many messages a fo- rum has, the path that a learner followed while she was nav- igating through an LMS, etc. Hence, monitoring resources may provide teachers with valuable information about the instructional design (e.g. to detect a bad design of content pages) and students performance (e.g. to detect whether learners are engaging in the course thanks to forums partic- ipation). An example of this kind of monitoring is [37].  Likewise, a graphic can simultaneously provide informa- tion about various items depending on how this is read. For instance, Mazza and Dimitrova [21] use a matrix to show students performance on quizzes. Thereby, if teachers paid attention to a specific column, they would observe the per- formance of a particular student in all course quizzes. By contrast, if a row were observed, then teachers would analyze the performance of the whole class in a specific quiz.  172    3. PROPOSAL From the observations done in the previous section, this  paper presents a graphical interactive educational monitor- ing tool which uses information visualization (IV) techniques. This allows instructors to monitor the class and, at the same time, look details of a particular student. The first part, monitoring of the classroom, is based on faceted browsing, a type of exploratory search. As far as the second part is concerned, a novel technique called data portrait is used to depict the information of a specific learner. A detailed ex- planation of both parts of the tool is exposed below. Nevertheless, before explaining the proposal of this paper,  a brief list of the most common characteristics of educational monitoring tools based on IV is given. This will help to better understand the work presented in this article.  3.1 Features of Educational Monitoring Tools Based on Information Visualization  3.1.1 Stand-alone vs. Built-in There are tools which have their own interface and collect  data from an LMS [21, 22], whereas there are others that, due to the success of LMSs (e.g. Moodle), are integrated into their framework as a plug-in [35].  3.1.2 Representation 2D vs. 3D There are two kinds of representation according to the  number of axes: 2D and 3D. In this regard, 2D graphics are firmly established. They consist of two dimensions (or axes) in which each of them represents a variable (or attribute) of information (e.g. students, grades, etc.). To a lesser extent, 3D graphics has also been proposed,  e.g. the scatter plot for discussion boards suggested in [21]. In general, 2D representations are usually more intuitive  for instructors than 3D ones. For that reason, most of edu- cational monitoring tools display 2D graphics.  3.1.3 Multivariate Data Much more variables than the number of axes of the graphic  (i.e. 2D or 3D) can be displayed thanks to the use of differ- ent techniques. Next, some of them are described briefly:   Single-axis composition [18]: this is a method whereby an axis represents a large set of variables (e.g. content topics, number of accesses, etc.). This is used in [21].   Visual components: elements such as color, shape and size, are often used as a data dimension.   Quadrants: these allow to organize data into four dif- ferent groups. These are employed in [14].   Mouse events: different mouse actions, such as click and rollover, are also used to show information (e.g. relationships between elements [11]).   Multivariate representations: unlike most of the pro- posals that depict data as points, bars, lines and so on, there are tools that use a single representation to display multivariate observations with an arbitrary number of variables. This is the case of star plots [13], which show each observation (e.g. a learner) as a star-shaped figure with one ray for each variable (e.g. grades, participation, etc.).  3.1.4 Manipulation Zoom, rotation (in 3D) and filtering are typical actions  that educational monitoring tools allow instructors to do. With regard to filtering, this refers to hide some visual  elements so as to emphasize others. Thereby, some values in the graphic act as layers that can be shown and hidden. Take the example of a bar chart that shows the number of posts written by the students of a class. In this case, the graphic has the variables student (X-axis), whose values are {S1,S2,S3,S4,S5}, and number of posts (Y-axis) with values {4,6,7,5,8}. If the graphic focuses on the students, then this has 5 layers (i.e. S1-S5). Hence, a teacher can indicate that the graphic only shows the bars that belong to the students S2 and S5. However, she cannot set a general constraint that asks to show those learners who have written between 3 and 9 messages, since the layers 3 and 9 do not exist. Similarly, she cannot filter the information based on other variables that are not layers (e.g. number of accesses).  A lot of educational monitoring tools with filtering op- tions are based on static layers, therefore this limits the ex- ploratory search that an instructor can carry out.  3.2 Monitoring of the Classroom  3.2.1 What Is Faceted Browsing Faceted browsing is becoming a popular method to allow  users to interactively search and navigate through complex information spaces [16]. This is widely used on a lot of e- commerce websites such as eBay or Wal-Mart.  A faceted browser provides users with facet-value pairs that are used for query refinement. Faceted browsing is made up of three stages [34]: opening, middle game and end game. In the opening, the interface shows the whole collec- tion and all facets that can be used. The middle game, in turn, allows users to iteratively narrow down the result set by defining constraints on the values of one or several facets, which refines the search query. Finally, the end game occurs when the user finishes the search by selecting an individual item from the result set and its information is detailed.  3.2.2 Why Is Faceted Browsing Suitable for Educa- tional Monitoring in an Online Environment  As said in section 1, teachers in online education should be guides. To carry out this role, they need to observe students behavior by analyzing any feature associated with learners. Thereby, teachers may detect any kind of problem in time.  Unlike the other proposals, which show graphics with pre- defined attributes or queries, faceted browsing allows teach- ers to perform an exploratory search strategy by using a wide range of orthogonal variables (i.e. facets). Thereby, instruc- tors gain an insight into the behavior of their students by iteratively submitting tentative queries based on more than one facet at the same time. This iterative process (i.e. the middle game) finishes when teachers find relevant informa- tion that they did not know or when the result set meets a specific set of requirements that teachers wanted. In that moment, instructors can either use the information found to make decisions (e.g. to send an e-mail or start a new search), or click on a student to see detailed information about her.  As seen, faceted browsing seems to be good at monitoring a classroom, since it allows instructors to define any tailored query and find relevant information that they did not know while they explore/refine the result set.  173    Figure 1: The opening stage of the faceted browser proposal. Students remain anonymous.  3.2.3 Proposal of Faceted Browser This paper presents a built-in faceted browser (see Figure  1) that collects data from an ad hoc learning management system [7]. This uses information visualization techniques in order to effectively display data, leaving knowledge inference in teachers hands. Therefore, data mining techniques are not used to obtain underlying information about students. Regarding its interface, 2D graphics are used. Its design is  divided into two areas. The main one shows the class roster as a set of cards in which each one includes students name and photo. The use of a photo is better than identifying students by looking at points, squares, bars, etc. Actually, Zhao et al. [36] states that there is evidence for the existence of a dedicated face processing system in humans brain. On the left, there is a menu that has different facets  whereby teachers can narrow down or sort out the class. Table 1 gathers the facets that have been included along with their possible values. The chosen facets encompass the three aspects studied by Mazza [20]: sociality (i.e. forum participation), cognition (i.e. assignments grade) and be- havior (i.e. studying pace). To stress that facets related to resources, such as the number of times that an item has been accessed, were not included because the platform, in which the faceted browser is placed, does not provide these data. As seen in Table 1, facets can have nominal or numeric  values. Those facets that allow gradation are represented with sliders. To indicate the number of students in each facet value, numbers in parentheses and histograms are used. Their values are updated depending on the data set that is shown in the main area every time. This helps teachers to set more meaningful queries during the middle game. As said, teachers can sort out the result set by students  name, studying pace or the average grade of assignments, in ascendant or descendant order.  Finally, different visual elements are used to represent more variables in the main area. The background color of each card indicates the learners studying pace. Thereby, the more orange the background is, the more advanced the student is. Besides the background color, the card can have a red border that means that the average grade of assign- ments is C- or less and, hence, that student would not pass the course if the term finished at that moment. Moreover, the border of the photo can be drawn with black dashes. This means that the student is repeating the subject. Like- wise and, because the result set can be ordered, the position of the cards is another visual element that gives extra infor- mation. Thereby, if a teacher sorted learners by the average grade of assignments in ascendant order, then students with lower grades would be on the top positions.  3.2.4 An Example of Using the Faceted Browser From the opening stage shown in Figure 1, a teacher may  define any query, e.g. to retrieve, ordered by studying pace in a descendant way, those students who, regardless of their participation in forums, are not repeating the course and, at the same time, have achieved 24% of activities as well as they have an average grade of assignments equal or greater than C+. If the previous query were executed, then the result set would be that shown in Figure 2b. In this regard, Figure 2a shows the transition between the opening (see Figure 1) and the middle game (see Figure 2b). As seen, instructors can see how students change their positions or even they dissapear. Thanks to the possibility of seeing the transition, teachers can gain extra understanding of their learners.  Finally, it is worth emphasizing that the result of the Fig- ure 2b may be either an intermediate step of the middle game or the last one. This depends on whether the teacher is satisfied with the result and selects one student, or prefers to narrow down the result set by changing some facets.  174    Table 1: Set of facets with their values  Category Facet Values Type  Basic information Gender male, female or all  Nominal Status new (i.e. first time in the course), repeat or all  Performance Studying pace [0%, 100%] Numerical  Assignments grade [NA (Not Assessed),NP (Not Presented),D,C-,C+,B,A] Nominal  Forums participation  Num. initial posts [0, max. num. messages posted by a student]  Numerical Num. replies [0, max. num. replies written by a student]  Num. messages read [0, num. messages in the forum] Average grade of messages [0, 5] Num. highlighted messages [0, num. messages highlighted by the teacher]  (a) Transition (b) Middle game  Figure 2: Execution of a query in the proposed faceted browser.  3.3 Monitoring of an Individual Student So far, only information about the whole classroom has  been represented. However, instructors need to know details of a specific learner quite often. In this regard, the great majority of monitoring tools usually show the same kind of graphic for both the classroom and the learner. Instead, the present paper suggests using a novel technique, called data portraits, to display an individual students information.  3.3.1 What Is a Data Portrait According to Donath [5], data portraits depict their sub-  jects accumulated data rather than their faces. They can be visualizations of discussion contributions, browsing histo- ries, social networks, travel patterns, etc. (...). Data por- traits depict a person through their digital archive. In short, the idea behind data portraits is to compactly convey a large amount of information from an individual in a single graphic. An example of data portrait used for discussion forums  is PeopleGarden [33]. This is a metaphor in which each user is a flower and, hence, the forum is a garden. Each petal symbolizes a message written by the user. Thereby, the number of petals indicates the users posting frequency. Thus, the more petals a flower has, the more active the user is. Likewise, the petals color represents if the message is an initial post (in magenta) or a reply (in blue). Moreover, pistil-like circles are used on top of the petals to show how many responses each message has received. To display how old a message is, petals, like in the real life, fade over time. Another similar proposal is daisy maps [13]. A daisy map  is a star-shaped glyph that displays the scores received on different parts of an assignment (e.g. reading, writing, etc.).  Thereby, each student is a daisy map and each ray has a color depending on the score attained.  Lexigraphs [6] is, in turn, a group of data portraits in which each user is represented as a face-like outline. Each one is drawn by the words written in the users Twitter account. Thus, silhouettes are updated with each new tweet.  Finally, Authorlines [31] is an horizontal timeline with ver- tical monthly dividers that represents the users yearly post- ing behavior in a set of newsgroups. Each month is divided into weeks, and each week is shown as a vertical lineup of circles. Each circle represents a conversation and its size indicates the number of authors messages in that thread. Authorlines places threads that were initiated by the author above the timeline, whereas the rest of threads to which she contributed are placed underneath the timeline.  3.3.2 Proposal of Student Data Portrait The proposed data portrait creates a snapshot of an indi-  vidual student from the data of her learning process. Each portrait appears when the instructor moves the mouse over a card of the proposed faceted browser. The items shown in the data portrait are the facets of Table 1, except gender.  The data portrait (see Figure 3) is a bar that is divided into five squares: number of initial posts (in dark blue), num- ber of replies (light blue), number of read messages (pink), the average grade of the messages (green) and the num- ber of highlighted messages (orange). Therefore, these five squares represent the students forums participation. Each one of them change its opacity in order to indicate the level of achievement. The lighter the color of the square is, the lower students performance in that item is, and vice versa.  175    (a) Active (b) Threads initiator (c) Replies a lot (d) Lurker  (e) Pass and repeat (f) Fail and no repeat (g) Drop out and repeat  Figure 3: Different examples of student data portraits.  Likewise, the border of the bar can be red or black. Red means that the student would not pass if the course finished at that moment. Moreover, this can be drawn with dashes. This indicates that the learner is repeating the course. Finally, there are two red markers above and underneath  the bar. The former represents the studying pace (in per- centage), i.e. how many activities the learner has already finished. Thereby, the top of the bar works as a continu- ous axis that goes from 0% to 100%. As far as the second marker is concerned, this indicates the average grade of the assignments. In this case, the bottom of the bar works as a discrete axis whose values are: NA/NP, D, C-, C+, B and A. Each value coincides with the ends of each square.  3.3.3 Examples of Using the Student Data Portrait Figure 3 shows different students behaviors by using the  proposed data portrait. For example, the learner in Figure 3a participates actively in forums, since she writes (opaque dark blue) and replies (opaque light blue) a lot and, at the same time, she reads most of messages (opaque pink). On the other hand, the student in Figure 3b initiates a lot  of conversations (opaque dark blue), but she does not reply to other messages (transparent light blue). Therefore, she only writes when she initiates the conversation. By contrast, the learner in Figure 3c helps classmates by participating in threads initiated by others (opaque light blue). However, she does not initiate any conversation. Moreover, the teacher has highlighted some of her messages (opaque orange). As seen, the students in Figure 3b and Figure 3c participate in discussion forums, but in an opposite way. According to Taylor [30], the learner in Figure 3d would  be a lurker. A lurker can be defined as a user that writes occasionally or not at all (transparent dark and light blues), but she regularly participates as a reader (opaque pink). Likewise, lurker students usually obtain similar grades to more active learners. For instance, in Figure 3d, this student has a B as the average grade of her assignments. The last three data portraits show: Figure 3e) a student  who is repeating the course (dashed border) and, at that moment, she would pass the course (she has a C+); Figure 3f) a lurker who does activities, but she would fail (red bor- der and bottom marker in the beginning); and Figure 3g) a student who has dropped out (all squares are transpar- ent, her studying pace is low and she has not handed in any assignment) and, moreover, is repeating (dashed border). As seen, thanks to the proposed data portrait, it is possi-  ble to compact a students learning process in a single image. Thereby, teachers can get an overall idea of a student at a glance and compare learners with each other easily.  4. CONCLUSIONS AND FUTURE WORK Online education involves a student-centered approach in  which the instructor is a facilitator. Consequently, teachers should be guides who help their students to achieve learning goals successfully. To carry out this role properly, instruc- tors should be provided with the maximum amount of data about both the whole classroom and a particular student.  With regard to this necessity, this paper introduces a novel approach of educational monitoring tool whose contribution is twofold. On the one hand, the use of facet browsing to monitor a classroom is proposed. As seen, facet browsers provide a user-friendly way to navigate through data collec- tions. Regarding its use in online education, faceted brows- ing may help instructors to gain understanding of the be- havior of their classrooms, since it allows teachers to explore and analyze the class by requesting iterative tailored queries based on different combinations of facets (i.e. attributes).  On the other hand, this paper promotes to employ a novel technique called data portrait that allows to compact the users information in a single picture. With regard to its use in online education, a data portrait can be a good way to summarize the students data visually, allowing teachers to gain understanding of a students behavior at first glance. In this regard, the data portrait of this paper is a first proposal of this kind of visualization in the educational context. As seen, this is in its very early stages and it thus needs to be studied in depth in future research.  This paper also proposes a classification of educational monitoring tools from two points of view. The first one focuses on the data processing technique that is used, i.e. information visualization and data mining, whereas the sec- ond one is based on the element that is monitored. Likewise, this paper presents a study on the most relevant features that educational monitoring tools based on IV usually have.  As future work, the proposed faceted browser must be integrated into the most popular LMSs, e.g. Moodle. To this end, a plug-in version must be developed. Thus, educational community will be able to take advantage of this tool.  Finally, an experiment with teachers throughout a term must be conducted so as to check the effectiveness and use- fulness of both the faceted browser and the data portrait. After the experiment, a survey could be sent in order to detect strong and weak points of the proposal. Likewise, a focus group or semi-structured interviews with 4-6 instruc- tors may also be useful to know their opinion in depth. From the results of the survey and focus group/interviews, valu- able conclusions might be drawn. Thereby, new features and improvements might be defined and added to the tool.  176    5. REFERENCES [1] R. S. Baker and K. Yacef. The State of Educational  Data Mining in 2009: A Review and Future Visions. JEDM - Journal of Educational Data Mining (ISSN 2157-2100), 1(1):317, Oct. 2009.  [2] A. Bakharia and S. Dawson. Snapp: a birds-eye view of temporal participant interaction. In Proceedings of the 1st International Conference on Learning Analytics and Knowledge, LAK 11, pages 168173, New York, NY, USA, 2011. ACM.  [3] S. Card, J. Mackinlay, and B. Shneiderman. Readings in information visualization: using vision to think. The Morgan Kaufmann series in interactive technologies. Morgan Kaufmann Publishers, 1999.  [4] N. Dabbagh and A. Kitsantas. Supporting self-regulation in student-centered web-based learning environments. International Journal on E-Learning, 3(1):4047, 2004.  [5] J. Donath, A. Dragulescu, A. Zinman, F. Viegas, and R. Xiong. Data portraits. In SIGGRAPH 10: ACM SIGGRAPH 2010 Art Gallery, pages 375383, New York, NY, USA, 2010. ACM.  [6] A. C. Dragulescu. Data Portraits: Aesthetics and Algorithms. PhD thesis, Massachusetts Institute of Technology, 2009.  [7] D. Garca-Solorzano, G. Cobo, E. Santamaria, J. A. Moran, and J. Melenchon. Representation of a course structure focused on activities using information visualization techniques. In ICALT, pages 443445, 2011.  [8] N. Hara and R. Kling. Students distress with a web-based distance education course. Information, Communication and Society, 3(4):557579, 2000.  [9] J. Hardy, S. Bates, J. Hill, and M. Antonioletti. Tracking and Visualisation of Student Use of Online Learning Materials in a Large Undergraduate Course, volume 4823 of Lecture Notes in Computer Science, pages 464474. Springer Berlin / Heidelberg, 2008.  [10] R. Hijon and Angel Velazquez. E-learning platforms analysis and development of students tracking functionality. In E. Pearson and P. Bohman, editors, Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications 2006, pages 28232828, Chesapeake, VA, June 2006. AACE.  [11] R. Hijon-Neira and J. A. Velazquez-Iturbide. How to improve assessment of learning and performance through interactive visualization. In ICALT 08: Proceedings of the 2008 Eighth IEEE International Conference on Advanced Learning Technologies, pages 472476, Washington, DC, USA, 2008. IEEE Computer Society.  [12] J. L. Hung and K. Zhang. Revealing Online Learning Behaviors and Activity Patterns and Making Predictions with Data Mining Techniques in Online Teaching. Journal of Online Learning and Teaching, 4(4):428437, 2008.  [13] I. Icke and E. Sklar. A visualization tool for student assessments data. In From Theory to Practice: Design, Vision and Visualization Workshop. IEEE VisWeek (2008), 2008.  [14] A. A. Juan, T. Daradoumis, J. Faulin, and F. Xhafa.  Developing an information system for monitoring students activity in online collaborative learning. In CISIS 08: Proceedings of the 2008 International Conference on Complex, Intelligent and Software Intensive Systems, pages 270275, Washington, DC, USA, 2008. IEEE Computer Society.  [15] D. A. Keim, W. Muller, and H. Schumann. Visual data mining. In D. Fellner and R. Scopigno, editors, STAR Proceedings of Eurographics 2002, pages 4968. Eurographics Association, 2002.  [16] J. Koren, Y. Zhang, and X. Liu. Personalized interactive faceted search. In Proceeding of the 17th international conference on World Wide Web, WWW 08, pages 477486, New York, NY, USA, 2008. ACM.  [17] E. Kosba, V. Dimitrova, and R. D. Boyle. Using Student and Group Models to Support Teachers in Web-Based Distance Education. In L. Ardissono, P. Brna, and A. Mitrovic, editors, UM2005 User Modeling: Proceedings of the 10th International Conference, pages 124133, Edinburgh, Scotland, 2005. Springer Verlag.  [18] J. Mackinlay. Automating the design of graphical presentations of relational information. ACM Trans. Graph., 5(2):110141, 1986.  [19] D. Maor. The teachers role in developing interaction and reflection in an online learning community. Educational Media International, 40(1/2):127138, 2003.  [20] R. Mazza and V. Dimitrova. Informing the design of a course data visualisator: an empirical study. In 5th International Conference on New Educational Environments (ICNEE 2003), May 2003.  [21] R. Mazza and V. Dimitrova. Visualising student tracking data to support instructors in web-based distance education. In 13th International World Wide Web Conference - Educational Track, pages 154161. ACM Press, 2004.  [22] R. Mazza and C. Milani. GISMO: a graphical interactive student monitoring tool for course management systems. In T.E.L.04 Technology Enhanced Learning 04 International Conference. Milan, pages 1819, 2004.  [23] C. McInnis, R. James, and C. McNaught. First year on campus. a commissioned project of the committee for the advancement of university teaching. Technical report, Canberra: Australian Government Publishing Service, 1995.  [24] A. Merceron and K. Yacef. Interestingness measures for associations rules in educational data. In EDM08, pages 5766, 2008.  [25] T. J. F. Mitchell, S. Y. Chen, and R. D. Macredie. The relationship between web enjoyment and student perceptions and learning using a webaARbased tutorial. Learning, Media and Technology, 30(1):2740, 2005.  [26] OECD. Learning for tomorrows world | first results from pisa 2003. Technical report, Organisation for Economic Co-operation and Development (OECD), 2004.  [27] R. Pirrone, V. Cannella, and G. Russo. A map-based visualization tool to support tutors in e-learning 2.0. In HSI09: Proceedings of the 2nd conference on  177    Human System Interactions, pages 482487, Piscataway, NJ, USA, 2009. IEEE Press.  [28] C. Romero and S. Ventura. Educational data mining: A survey from 1995 to 2005. Expert Syst. Appl., 33(1):135146, 2007.  [29] P. C. Salomon. Effective and responsible teaching: The new synthesis, chapter The changing role of the teacher: From information transmitter to orchestrator of learning, pages 3549. Jossey-Bass, San Francisco, CA, 1992.  [30] J. Taylor. Teaching and learning online: the workers, the lurkers and the shirkers. In 2nd Conference on Research in Distance and Adult Learning in Asia, CRIDALA 2002, 57 June 2002.  [31] F. B. Viegas. Newsgroup crowds and authorlines: Visualizing the activity of individuals in conversational cyberspaces. In In: Proceedings of the 37th Hawaii International Conference on System Sciences, IEEE, 2004.  [32] W. R. Watson and S. L. Watson. An Argument for Clarity: What are Learning Management Systems, What are They Not, and What Should They Become TechTrends, 51(2):2834, Mar. 2007.  [33] R. Xiong and J. Donath. Peoplegarden: creating data portraits for users. In UIST 99: Proceedings of the 12th annual ACM symposium on User interface software and technology, pages 3744, New York, NY, USA, 1999. ACM.  [34] K.-P. Yee, K. Swearingen, K. Li, and M. Hearst. Faceted metadata for image search and browsing. In Proceedings of the SIGCHI conference on Human factors in computing systems, CHI 03, pages 401408, New York, NY, USA, 2003. ACM.  [35] H. Zhang, K. Almeroth, A. Knight, M. Bulger, and R. Mayer. Moodog: Tracking students online learning activities. In Proceedings of World Conference on Educational Multimedia, Hypermedia and Telecommunications 2007, pages 44154422, 2007.  [36] W. Zhao, R. Chellappa, P. J. Phillips, and A. Rosenfeld. Face recognition: A literature survey. ACM Comput. Surv., 35:399458, December 2003.  [37] M. Zorrilla, D. Garca, and E. Alvarez. A decision support system to improve e-learning environments. In Proceedings of the 2010 EDBT/ICDT Workshops, EDBT 10, pages 11:111:8, New York, NY, USA, 2010. ACM.  178      "}
{"index":{"_id":"31"}}
{"datatype":"inproceedings","key":"Liu:2012:EQA:2330601.2330646","author":"Liu, Haiming and Macintyre, Ronald and Ferguson, Rebecca","title":"Exploring Qualitative Analytics for e-Mentoring Relationships Building in an Online Social Learning Environment","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"179--183","numpages":"5","url":"http://doi.acm.org/10.1145/2330601.2330646","doi":"10.1145/2330601.2330646","acmid":"2330646","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, learning network and ties, mentoring, online social learning, relationships","Abstract":"The language of mentoring has become established within the workplace and has gained ground within education. As work based education moves online so we see an increased use of what is termed e-mentoring. In this paper we explore some of the challenges of forming and supporting mentoring relationships virtually, and we explore the solutions afforded by online social learning and Web 2.0. Based on a conceptualization of learning network theory derived from the literature and the qualitative learning analytics, we propose that an e-mentoring relationships is mediated by a connection with or through a person or learning objects. We provide an example to illustrate how this might work.","pdf":"Exploring Qualitative Analytics for E-Mentoring  Relationships Building in an Online Social Learning   Environment   Haiming Liu 1,*  , Ronald Macintyre 2  and Rebecca Ferguson  3    1  Knowledge Media Institute, The Open University, Walton Hall, Milton Keynes, MK7 6AA, United Kingdom   2  The Open University in Scotland, 10 Drumsheugh Gardens, Edinburgh, EH3 7QJ, United Kingdom   3 The Institute of Educational Technology, The Open University, Walton Hall, Milton Keynes, MK7 6AA, UK   {h.liu, r.macintyre and r.m.ferguson}@open.ac.uk      ABSTRACT  The language of mentoring has become established within the   workplace and has gained ground within education.  As work   based education moves online so we see an increased use of what   is termed e-mentoring. In this paper we explore some of the   challenges of forming and supporting mentoring relationships   virtually, and we explore the solutions afforded by online social   learning and Web 2.0. Based on a conceptualization of learning   network theory derived from the literature and the qualitative   learning analytics, we propose that an e-mentoring relationships   is mediated by a connection with or through a person or learning   objects. We provide an example to illustrate how this might   work.   Categories and Subject Descriptors  K.3.1 [Computer Uses in Education] Collaborative learning,   Computer-assisted instruction (CAI), Distance learning; H.1.2   [User/Machine Systems]: Human factors; J.4 [Social and   Behavioural Sciences]: Sociology   General Terms  Design, Human Factors, Theory   Keywords  Online Social Learning, Learning Analytics, Mentoring,   Relationships, Learning Network and Ties   1. INTRODUCTION  The Open University (OU) is a UK based open and distance   learning provider. It has about 12,000 staff and around 200,000   students distributed across the world at any one time. Like any   large organization mentors play a vital role in the professional   development of individuals [11]. Mentoring is a social and   psychological relationship and typically takes place face to face,   where the value is seen to come from those personal interactions.   Those type of relationships present a key challenge to a   distributed organization like the OU, where students and staff are   not necessarily co-located. In an effort to develop a good online   or e-mentoring service, we started to investigate how to build up   and better support mentoring relationships using Web2.0   technology by creating a platform called SocialLearn (SL). This   paper proposes a framework and features that focus on how we   might employ transient connections (weak ties) within social   media to develop more meaningful (strong ties).   2. ONLINE SOCIAL LEARNING  Mentoring is a social relationship. Conole (2008) [5] noted that   the real opportunities that Web 2.0 affords is within online social   and situated learning. Online social and situated learning focus   on learning as social participation and shifting from an individual   and information focused learning to an online social learning and   communication/collaboration.  To foster these relationships   online social learning platforms should focus on social   interactions through activity streams, following and making   connections, to draw users towards content or learning objects.    This paper explores these social relations with reference to   mentoring.   3. MENTORING  Haggard, et al. (2011) [11] systematically reviewed the   mentoring literature between 1980 and 2009. They found over 40   different definitions for mentoring. In this section we explore   some of the ways that mentoring has been defined.   3.1 What is Mentoring  Classic mentoring features one to one relationships between a   more senior or experienced individual and a less senior less   experienced individual. Attempts to create a mentoring typology   often focus on formality (formal to informal) and structure   (professional competency to unstructured). However, while the   degree of formality is a factor, the relationships can be far more   complex than this, as it is a personal relationship [17]. Wong and   Premkumar (2007) [19] present three mentoring models: The   apprentice model, the competency model, and the reflective   model. In the apprentice model, akin to traditional     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee.   LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00      * The author is also at Department of Computer Science and Technology,  University of Bedfordshire, Park Square, Luton, LU1 3JU, United  Kingdom.   179    apprenticeships, the mentee learns through observing and   copying the mentor; in the competency model, the mentors   provide feedback on the mentee's performance; in the reflective   model, the mentors focus on developing self-reflection of the   mentees.   The different mentoring definitions and models invite us to think   about the different mentoring relationships and how the   mentoring theories relate to Web 2.0 pedagogies. Within online   social learning the focus is on learning by interactions and   connections with and through a person or a learning object which   is likely to be informal and unstructured. In addition to these   mentoring models that focus on the learning process, Haggard, et   al. (2011) [11] suggested that we must also consider a range of   other personal factors, such as gender, career stage, age   difference between mentor and mentee, etc. This suggests that   within any online mentoring1 relationship the ability to be able to   know something about another person is as important as what   they do. A persons profile on a social media sites help us   know the person. This suggests that we need to create   mechanisms that allow people to see others activity and view   aspects of their profile.    3.2 Mentoring Motivation  Wong and Premkumar (2007) [19] provided a mentor motivation   checklist to illustrate some of the reasons why people engage in   mentoring relationship, such as people like the feeling of   advising others, they find it satisfying, etc. It is interesting to see   that the motivations are associated with engaging in Web 2.0   communities and earning credit and reputation. For example,   generalized reciprocity [5] in online spaces is widely reported;   there is a suggestion that newcomers develop an obligation to   help others in the future through the valuable help and advice   they receive as newcomers [15]. Allen (2007) [2] found that in   addition to the simple motivations cited earlier more complex   psychosocial themes emerge. Perceived similarity is a factor  so   called similarity-attraction paradigm- where selection is based on   the overlaps in interests, Another important factor to consider is   mentor and mentee's performance where social exchange theory   drives mentor-mentee selection, with mentors choosing mentees   with strong performance, high ability and ample willingness -   with links to quality within social media and again the role of   the profile becomes important. Online social learning spaces   cannot account for all of these factors, but they inform our   thinking. For example, we can account for similarity, allowing   users to see a profile and judge similarity, and by using analytics   we can make recommendations, we can account for performance,   by creating criteria that allow users to display, view, rate and   evaluate other users through their profiles.    3.3 Shifting Sense of Mentoring  Our discussions above has shown that experience or   seniority is read in slightly different way between classic   mentoring and the mentoring in the online social learning   context. There is a flatter hierarchy in online mentoring than we   see in classic mentoring and this is considered to have benefits   in terms of student engagement retention and progression. The   importance of peer support in learning is also recognized in the                                                                 1 Online mentoring = e-mentoring   workplace. The informal social interaction around shared tasks   and challenges is now seen as a vital part of learning at work [9].    Discourses on e-mentoring also destabilize notions of classic   mentoring. The ability for mentoring to be relatively anonymous   and for mentors to be involved in multiple overlapping   relationships changes the relationship psychologically and   practically.  Alevizou (2010)s work  [1] on Web 2.0 argued that   the peer interaction and collaboration learning  fostered by Web   2.0 is a kind of  distributed online mentoring. These   opportunities are being realized by several companies that offer   secure mentoring services (e.g. Mentor Pro2), and open sites like   Horsemouth3. These sites offer a complex range of online   mentoring, from career, education, business, to secure services   for vulnerable adults and young people, and what might be more   accurately termed life coaching.   We need to account for classic mentoring. However, in Web   2.0 discussions on online mentoring is a diffuse relationship. It is   seen as part of the democratization of education where   communities support each other to create, understand and   share resources [6]. This means classic mentoring theories are   not enough to explain different types of e-mentoring relationships   that might evolve. We wanted to explore how the form of online   social networks informed the development closer relationships.     4. SOCIAL NETWORKING   Granovetter (1973) [10]s work on social ties explores the role   that weaker ties have within networks. Strong ties are those   things that bind groups (strong ties have overlapping network   and interests), while weak ties (casual contacts) allows us to   connect with other networks and open up new areas to explore.    Sites like LinkedIn4 and Academia.edu5 operate on this principal.   Haythornthwaite (2002) [12] investigated the relationship   between latent (he inactivated weak ties), weak and strong ties   and different communication media. She found that new   mediums of communication layered over existing ones can help   strengthen weak ties.  In addition, a new communication medium   can turn latent ties into weak ties. This suggests that online   social media can play a role in activating and strengthening ties.   Our reading of networks and ties is not one where networks and   ties only exist between individual, we recognize that they also   exist between what we call mediating objects (people, images,   groups, event posting, etc.). While people can and do connect   with each other, that connection is often mediated through a   mediating object [8]. Without these mediating objects it is   difficult for people to form connections [14].  Our imprint on   those objects helps us connect through the mediation objects   with other people and learning objects.  These social traces   help us make sense of the online world, and as we make sense of   it our tracks help others.   Returning to mentoring, the suggestion here is that an e-  mentoring relationship can be built up from a general connection                                                                 2   http://www.e-mentoring.org/   3  http://www.horsesmouth.co.uk/   4  http://www.linkedin.com/   5  http://www.academia.edu/   180    to a mediating object. The ability to connect with and through   rich content that Web 2.0 affords can play a role in developing   and maintain communities that support online mentoring [16].   We can activate those latent ties. If we then harness some of the   motivational cues identified earlier in mentoring, for example,   similarity or performance, then we may be able to turn some of   those weak ties into strong ties.    5. PILOT STUDY AND QUALITATIVE  ANALYSIS RESULTS   The data we draw on in this paper is from a six months pilot   study that looks at how, and what kinds of work based learning   SL can support. SL is a web2.0 online social learning platform   and tool kit developed by the OU.   Twelve members of staff occupying a range of positions   participated in the pilot.  Each participant attended a two-hour   initial workshop, which provided an introduction to the pilot   study and to SL and offered hands-on experience. Participants   activities on SL were screen captured using Camtasia software6.   During the sessions they were asked to narrate their journey.   Thematic analysis has been applied to the screen captures and   audio scripts. We used semi-structured interviews to explore the   theme analysis results in more details. Nine out of twelve   original participants were interviewed. The interviews were   transcribed and coded to identify dominant themes. The   qualitative thematic analysis was also employed for analyzing the   interview script. The qualitative analysis results show that whilst   the participants see SLs potential as: a complement to existing   work-based learning tools; a way of supporting flexible work-  based learning; a way of building learning networks; a way of   bringing resources together; a way of providing training and   support for staff and employees based in the regions and nations,   they also look for particular kind of mentoring functions to   support their online social learning.   Our wider reading and observations of e-mentoring systems   indicated that while our system could readily support   traditional mentoring, it also has the potential to support more   diffuse relationships that would support a sense of community   and generalized reciprocity. We found that confident social   media users were already doing this. During pilot they filled in   their profile and quickly began to establish connections, they   then used those connections to locate and make other   connections. What also became clear in the interviews (even   before the launch of Google+7) is that people wanted to be able   to differentiate between different types of connection.  This has   been found in informal work related online networks [18], and it   is our sense that this will be important in the workplace. Our   solution is to allow users to connect in different ways, for   example follow, and to be able to add tags that specify the type of   connection, for example adviser  or even mentor.    The use of the profile and activities to connect and make sense of   SL, along with issues raised in interviews highlighted the   importance of users profiles and making connections visible.                                                                 6  http://www.techsmith.com/camtasia/   7  https://plus.google.com/#    This links with the similarity attraction paradigm in mentoring   and also touches on aspects of trust. Trust is important online   forums, as we often lack the normal cues that allow us to assess   whether to trust another person, or source of information.    Research on large online forums has found trust, or the cognitive   decisions we make around how credible a source is based on our   perception of how honest and reliable the source is, our sense of   what the intention is, and competence [3].  Online we use social   factors (rating and voting) to assess reliability, the users profile   and badges to demonstrate competence.   One of the key functions of any social site is the ability to   connect with others. We noted earlier the role that instant   messages sites like Twitter play in connections and establishing   ties.  While following is essentially a weak tie [4], coupled   with other channels of communication these weak ties can   become strong ties. While our early build did support   connections, the sense of sharing the space with others appeared   to be absent. It appears that users need to be able to see others   (through their activities), and be able to understand and interpret   what others were doing.  While this highlights the importance of   the profile and activity streams, it also asks us to consider how   we push content to users. In our framework (Figure 1) we   suggest that users connect with learning objects and people, and   through those they can in turn connect to other learning objects   and people  this is what they pull towards themselves.    However, it became apparent that users also wanted us to push   content and people to them.    6. BUILDING ONLINE MENTORING  THROUGH CONNECTION AND   RECOMMENDATION   In this section, we explore how connections are made with and   through mediating objects and how that informs the content we   push to (recommend to) users and the way users can pull   (search) content to themselves within the framework (Figure 1).   Following work on the role of learning objects in mediating   social interactions online [8], we extended the notion of networks   and connections from people to what we call  mediating  objects.   While we recognize that users will connect and interact with   learning objects and people in different ways, we consider these   ties are important, and just like our connections with other   people the strength of our ties will vary. This is important to our   understanding how people connect with and through people and   learning objects, and thus an important factor in understanding   how the people use connections in online social learning.   The framework indicates that users connect with other people   and content (key 1 and 2), as they build up those connections,   they leave traces that allow them and other users to connect   through to more people and content (key 3). Further, the users   connection behaviors through actions (key 4) will contribute to   develop recommendations for the users (key 5).   181       Figure 1: Connection and recommendation framework for   building online mentoring relationships in a social learning   environment      7. CONCLUSION  What we describe here is not classic mentoring, and we did not   aim to merely illustrate classic mentoring online. Instead we   took some of the key elements of mentoring and reflected through   social learning and Web 2.0. We found that many of the tension   and difficulties that arise in online mentoring relate to attempts   to see it only in relation to classic mentoring. Mentoring online   is ambiguous and opens to multiple interpretations [11]. Our   proposed online mentoring tools do account for classic   mentoring.  However, our main focus is the underlying psycho-  social factors and Web 2.0 connections. We have found a great   deal of common ground with research on social learning and Web   2.0. For example, the importance of performance criteria and   being able to select people based in similarity within mentoring   [2], is mirrored by the need to see and know about others online   [13] before developing connections and trust [3].  In this model of   mentoring (closer to peer mentoring or peer support in work-  based learning) the relationships are likely to be more diffuse   and feature connections that vary in frequency and intensity.    Here we drew on and developed the work of [10] on the strength   of weak ties (see also [12]). We know that these weak ties are   important in accessing new knowledge and information [10], and   that Web 2.0 tools (e.g. following in Twitter/Facebook) are   effective at creating networks of weak ties [4]. We explored this   in relation to people and learning objects [8], and the role that   ties played in developing a sense of place (for example [7] on   Twitter), and how that sense of commonality fostered   generalized reciprocity. Clearly this is only one of the ways   that users may develop online mentoring relationships. One to   one relationships or the allocation of mentors is far more   common in the workplace, and over the next few months we will   be developing those types of tools and pilot and evaluate the   updated specifications.    8. REFERENCES   [1] Alevizou, P. 2010. Distributed mentoring: peer   interaction and collaborative learning in P2PU. Seventh   Annual Open Education Conference (2010).   [2] Allen, T.D. 2007. Mentoring relationships: From the   perspective of the mentor. The handbook of mentoring   at work. B.R. Ragins and K.E. Kram, eds. Thousand   Oaks, CA: Sage. 123-147.   [3] Casalo, L.V. et al. 2011. Understanding the intention to   follow the advice obtained in an online travel   community. Computers in Human Behaviour. 27, 2   (2011), 622-633.   [4] Chen, G.M. 2011. Tweet this: A uses and gratification   perspective on how active Twitter use gratifies a need to   connect with others. Computers and Human Behaviour.   27, 2 (2011), 755-762.   [5] Conole, G. 2008. New schemas for mapping pedagogies   and technologies. ARIADNE.   [6] Conole, G. and Alevizou, P. 2010. A literature review of   the use of Web 2.0 tools in Higher Education.   [7] Dunlap, J.C. and Lowenthal, P.R. 2009. Tweeting the   Night Away: Using Twitter to Enhance Social Presence.   Journal of Information Systems Education. 20, 2 (2009),   129-135.   [8] Engestrm, J. 2005. Why some social network services   work and others dont - or the case for object-centered   sociality.   [9] Eraut, M. et al. 2002. Learning from other people at   work. Supporting lifelong learning. 1, (2002), 127-145.   [10] Granovetter, M. 1973. The Strength of Weak Ties .   American Journal of Sociology. 78, 6 (1973), 1360-  1380.   [11] Haggard, D.L. et al. 2011. Who is a mentor A review of   evolving definitions and implications for research.   Journal of management. 37, 1 (2011), 280-304.   [12] Haythornthwaite, C. 2002. Strong, Weak, and Latent   Ties and the Impact of New Media. The Information   Society. 18, (2002), 385-401.   [13] Hew, K.F. and Hara, N. 2007. Knowledge Sharing in   Online Environments: A Qualitative Study. Journal of   the American Society for Information Science and   Technology. 58, 14 (2007), 2310-2324.   [14] Keitzman, J.H. et al. 2011. Social media Get serious!   Understanding the functional building blocks of social   media. Business Horizons. 54, 3 (2011), 241-251.   182    [15] Mathwick, C. et al. 2007. Social Capital Production in a   Virtual P3 Community. Journal of Consumer Research.   34, (2007), 832-849.   [16] McLoughlin, C. et al. 2007. Peer-to-peer: An e-  mentoring approach to developing community, mutual   engagement and professional identity for pre-service   teachers. Australian Association for Research in   Education Conference (2007).   [17] Santos, A. and Okada, A. 2010. The role of mentoring in   facilitating the process of repurposing OER. The seventh   annual open education conference (2010).   [18] Tang, L. 2010. Development of Online Friendships in   Different Social Spaces: A case Study. Information,   Communication & Society. 13, 4 (2010), 615-633.   [19] Wong, A.T. and Premkumar, K. 2007. An introduction   to mentoring principles, processes, and strategies for   facilitating mentoring relationships at a distance.          183      "}
{"index":{"_id":"32"}}
{"datatype":"inproceedings","key":"Lonn:2012:BGK:2330601.2330647","author":"Lonn, Steven and Krumm, Andrew E. and Waddington, R. Joseph and Teasley, Stephanie D.","title":"Bridging the Gap from Knowledge to Action: Putting Analytics in the Hands of Academic Advisors","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"184--187","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330647","doi":"10.1145/2330601.2330647","acmid":"2330647","publisher":"ACM","address":"New York, NY, USA","keywords":"design-research, higher education, learning analytics, undergraduate engineering","Abstract":"Developing new courses and updating existing ones are routine activities for an educator. The quality of a new or updated course depends on the course structure as well as its individual elements. The syllabus defines the structure and the details of the course, thus contributing to the overall quality of the course. This research proposes a new AI based framework to manage the quality of the syllabus. We apply AI methods to automatically evaluate a syllabus on the basis of such characteristics as validity, usability, and efficiency. We provide user trials to show the advantages of the developed approach against the traditional human-based process of syllabi verification and evaluation.","pdf":"Bridging the Gap from Knowledge to Action:   Putting Analytics in the Hands of Academic Advisors  Steven Lonn   University of Michigan  USE Lab, Digital Media Commons   3350 Duderstadt Ctr, 2281 Bonisteel  Ann Arbor, MI 48109-2094 USA   +1 (734) 615-4333   slonn@umich.edu   Andrew E. Krumm  University of Michigan   School of Education & USE Lab  Suite 1228 N, 610 E University Ave   Ann Arbor, MI 48109-1259 USA  +1 (734) 615-4333   aekrumm@umich.edu   R. Joseph Waddington  University of Michigan   School of Education & USE Lab  Suite 4220, 610 E University Ave  Ann Arbor, MI 48109-1259 USA   +1 (734) 615-4333   rjwadd@umich.edu   Stephanie D. Teasley  University of Michigan   School of Information & USE Lab  4384 North Quad, 105 S. State St.   Ann Arbor, MI 48109-1285 USA   +1 (734) 763-8124   steasley@umich.edu  ABSTRACT  This paper presents current findings from an ongoing design- based research project aimed at developing an early warning  system (EWS) for academic mentors in an undergraduate  engineering mentoring program. This paper details our progress in  mining Learning Management System data and translating these  data into an EWS for academic mentors. We focus on the role of  mentors and advisors, and elaborate on their importance in  learning analytics-based interventions developed for higher  education.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Computer-assisted instruction  (CAI)   General Terms  Management, Measurement, Performance, Design.   Keywords  Learning Analytics, Design-Research, Undergraduate  Engineering, Higher Education.   1. INTRODUCTION  This paper reports on the first iteration of a design-based research  project focused on developing an early warning system (EWS) for  a mentoring program that supports undergraduate engineering  students. The EWS described in this paper represents an  application of learning analytics that is gaining popularity across  colleges and universitiesthe near real-time aggregation and  analysis of students' use of campus information technology (IT)  systems to support the immediate identification of students in  need of academic support (e.g., [1; 2]).   The mentoring program for which our EWS was developed, called  the M-STEM Academy (Michigan Science, Technology,  Engineering, and Mathematics Academy) [3], provides an  integrated student development program for first- and second-year   undergraduate engineering students. The M-STEM Academy is  aimed at increasing academic success and retention of students  who, for reasons of socioeconomic status, first generation college  status, racial or gender bias, or lack of rigor in their high school  preparation, might not be successful at a highly competitive, elite  research university. For the first two years of their undergraduate  career, students enrolled in the M-STEM Academy participate in a  variety of core activities including a summer transition program, a  program living community, peer study groups, and monthly  meetings with their academic advisor, or  mentor . The M-STEM  Academy is an academic support program modeled on the  Meyerhoff Scholars Program at the University of Maryland- Baltimore County [4] and the Biology Scholars Program (BSP) at  the University of California Berkeley [5]. The EWS developed in  this project provided Academy mentors with frequent updates on  students' academic progress and streamlined the presentation of  data to allow immediate identification of students in need of  support.   Research on  Data Driven Decision Making  conducted on in K- 12 schools has shown that the access to data does not  automatically lead to improvements in organizational functioning  or student learning [6]. In our research program, we viewed  mentors as critical to increasing students' learning. We provided  EWS data to mentors on a weekly basis, allowing mentors to  engage students to different degrees or in different ways as a  result of having access to the EWS data. Based on more frequent  interactions with better informed mentors, students can engage  with academic support services in a more timely manner and by  doing so may positively affect their course performances. With  this model, we provided mentors' with access to data but allowed  them to determine the frequency and intensity of connecting  students to academic support services provided by the M-STEM  Academy and the College of Engineering.    2. BACKGROUND  Our design-based research project draws from previous research  on utilizing data from Learning Management Systems (LMSs) for  the purposes identifying  students at risk  of academic failure.  Below, we describe recently deployed solutions that aim to  leverage LMS data to improve various stakeholders' decision- making, and ultimately, retention and academic success of  students.   2.1 Leveraging LMS Data   The vast majority (over 95%) of postsecondary institutions  support, augment, and facilitate classroom instruction using a      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12: 2nd International Conference on Learning Analytics &  Knowledge, 29 April  2 May 2012, Vancouver, BC, Canada   Copyright 2012 ACM 1-58113-000-0/00/0010$10.00.     184    LMS [7]. Given the penetration of LMSs throughout higher  education and the breadth of use on most campuses, these systems  offer a ready source of near real-time data that can be analyzed to  support a multitude of academic decisions for an increasing  number of students. Our research lies at the intersection of  learning analytics-based interventions and mining LMS data,  where the use of LMS data, in particular, has been demonstrated  to provide a useful stream of data to support just-in-time decision- making around students' academic performances [9].     Prior efforts to create  early warning systems  that alert students  and/or faculty to sub-par student performance have generated  useful proofs of concept. For example, Morris, Finegan, and Wu  [10] found positive correlations between how long students  remain on an LMS course site and the degree to which they  participate in online discussions with students' likelihood of  persisting in an online course. Focusing on online courses,  Goggins, Galyen, and Laffey found that students were able to use  LMS feedback to identify what their peers were doing, and what  they, in turn, might need to accomplish in order to catch up to the   herd  [11]. Macfayden and Dawson [9] attempted to identify  specific online activitiessupported by an LMSthat have  unique predictive power; using a variety of statistical techniques,  these authors used LMS data to correctly identify 81% of students  who failed the course.   Two recently deployed systems are examples of scalable learning  analytics products in higher education. At Purdue University,  Course Signals [1] analyzes students' use of the campus LMS to  provide students feedback on how their use of the system  corresponds to their possible future grade in a course. At the  University of Maryland-Baltimore, the Check My Activity [2]  tool supports students' awareness of how their use of the LMS and  their current grades compares to that of their peers. The Course  Signals project represents the most robust EWS using LMS data;  however, this project has yet to pair data visualizations with  specific interventions, relying instead on generating notifications  to students and instructors without identifying how instructors or  students can best use this information. The study presented here  addresses this gap between identification of a problem and  defining specific interventions.    3. METHODOLOGY  Our research agenda is organized around the principles of design- based research [e.g., 12]. Design-based research involves the  iterative production of theories, artifacts, and practices that can  potentially impact teaching and learning. A distinguishing feature  of design-based research is that the development of learning tools  and environments is a collaborative effort among researchers and  practitioners [13].    3.1 Participants  Four cohorts, roughly 50 students each, have been recruited and  admitted into the 2-year M-STEM Academy. Data from these four  cohorts show that after participating in the Academy, the M- STEM students have GPAs at or above those of their peers in the  College of Engineering (CoE), and that they outperform the  average CoE student in the three entry-level math courses [3].  Although this represents overall success for the M-STEM  Academy, there are still individual students who  fall between the  cracks  in the mentoring programespecially after the first  yearand their failure to perform well is not discovered until   final course grades are submitted.  The EWS described in this  paper tracked approximately 200 M-STEM students across 165  unique courses.     3.2 Procedure  Our work with the M-STEM Academy began in the Winter 2011  term. We provided grade information stored in the LMS to  mentors on a weekly basis during the middle weeks of the term.  Also during this time, we conducted multiple collaboration  sessions with mentors to determine their information requirements  and to get feedback on the earliest versions of the EWS. From  these sessions, we developed a plan for displaying data, clarified  ways in which data visualizations would be helpful to mentors,  and identified how mentors used the initial data visualizations  before and during mentoring sessions with students. Below, we  discuss the specific development of the EWS, and the mentors'  reactions to and use of the system.   4. FINDINGS  Data to support the development of the EWS were drawn from the  campus LMS. Up-to-date grades from the LMS's Gradebook and  Assignments tools were used to track students' performance.  Further, we integrated a proxy for student  effort  in a course  through the use of LMS course website login events. While each  tool on the LMS tracks user actions, the number of times that a  student logs into a course website provides a parsimonious metric  that can be used across all course websites. These data sources  were aggregated and translated into a variety of visualizations.  Data visualizations included figures displaying students'  developing grades and use of the LMS in comparison to their  peers, students' performances on specific assignments, and week- to-week classifications of students. These week-to-week  classifications were the primary features of the EWS, designed to  provide mentors with an easy-to-interpret assessment of students'  current academic performances.    Based on specific combinations of students' grades and course site  login frequency, the EWS displayed whether mentors should take  one of three actions:  encourage  students to keep doing well,   explore  students' progress in more detail, or immediately   engage  students to assess possible academic difficulties. To  develop the classifications of encourage, explore, and engage, we  worked closely with M-STEM Academy faculty and mentors.  Classifications of encourage, explore, and engage were generated  using three rules: (1) whether a student's percent of points earned  was at or above the thresholds of 85%, 75%, or 65%; (2) whether  a student was 10% or 5% below the course average in percent of  points earned; and (3) whether a student was below the 25th  percentile in number of logins. These rules put a premium on  students' absolute performances as well as their performances  relative to their classmates'. Each student's percentile rank for  course site logins was used to classify students close to the  borderline on the first two rules.   One of the difficulties of using system log data to gauge student  performance is the variation in how LMS tools are used course-to- course. There are two general strategies for overcoming this  variability. First, create a composite metric that integrates some  combination of tools used on the course site whereby this metric  is in units that locate students within an intra-course distribution.  Second, identify system events that are shared across all course  sites. We chose to work with only the events that are shared   185    across all course sites given the transparency of what these events  track; the mentors interpreted this as an acceptable proxy for  student effort.    Classifications were aligned to a single student enrolled in a single  course; for example, a single student enrolled in four courses  would receive four classifications. Thus, all comparisons between  students were intra-course comparisons and the statistics used,  such as percentile ranks, were appropriate for such comparisons.  Using a variety of data mining techniques as well as feedback  from mentors, we continuously modified the thresholds for each  of the above rules and are currently testing the accuracy of the  rules across larger samples of courses and students.   Data visualizations and classification schemes were compiled and  released to mentors on a weekly basis. The EWS was comprised  of a summary sheet and multiple, individual student display sheets  (see Figure 1). The summary sheet provided a broad overview of  all students and the student detailed sheets provided figures  depicting an individual student's academic progress. These  detailed individual student displays supported mentors in  examining students classified as  explore  or  engage .   Because students transition to a less structured participation in the  M-STEM Academy during their second year followed by a  transition to the general engineering student population in their  third year, the mentors became interested in tracking upper  division students.  Almost immediately upon receiving the first set  of performance indicators from the EWS, mentors noticed  multiple third- and forth-year students who had fallen behind.   They then contacted these students to schedule an advising  session where the student received guidance and suggestions for  how to improve their grades. Currently, mentors are immediately  contacting any student classified as  engage , regardless of their  year in the program.  The mentors report scheduling meetings  with these students to discuss their academic performance and to  identify the most appropriate course of action.   In our future work, we will review the current version of the EWS  with academic mentors. We also plan to use this version of the  EWS as a template for a web-based version that will allow us to  more easily merge various datasets and mathematical functions, as  well as integrate data from the university's larger data warehouse.  Lastly, we intend to closely monitor the types of services   recommended by mentors and the degree to which students follow  up on mentors' recommendations to utilize those services.   5. DISCUSSION  Our goal with the M-STEM Academy design-based research  project is not simply to design data displays for Academy  mentors, but to use this unique group of faculty, staff, and  students as a test bed for scaling the EWS to the College of  Engineering and, eventually, to the entire university. By  beginning the research project with an eye towards broad scale,  we hope to avoid the difficulties that can develop when tailoring  analytics to a small population that cannot then be implemented  with a wider group [e.g., 8].   An important aspect of learning analytics research that surfaced in  our investigations of the MSTEM Academy concerns  organizational capacity. Though there are multiple definitions  (e.g., [14]), here we define organizational capacity as the  resources and routines that both enable and constrain access and  application of learning analytics tools and related services.  Resources include the actual support services available to (all)  students, as well as the availability of dedicated mentors or  advisors who have time to meet with and assess students' needs.     More generally, the culture of data use (e.g., whether data is  valued for supporting decision-making as well as that which is  considered to be valid data) within an academic unit, such as a  school or college, can be an important resource that enables the  use of learning analytics. A culture of data use, moreover, can be  instantiated in specific policies that incentivize data use or restrict  key actors' access to data. Thus, factors such as values and  policies are not solely barriers, though often framed as such, but  rather possible mechanisms that can be targeted to improve the  effectiveness of learning analytics-based interventions.    Thus, one of the important resources for supporting student  success is ensuring their access to academic mentors and advisors.  Faculty members may be too burdened with teaching, research  and service to effectively interpret the complex data that is  available from the Learning Management System without  significant training efforts that have proven difficult at a large  research institution like Michigan. Furthermore, while competitive  students are typically motivated to continually check on their own  progress and standing within their courses (hence the pattern of   Figure 1. Screen Shots of Data Displays for Academy Mentors. Summary Display (Left) and Detailed Display (Right) Examples.   186    use seen in Fritz's work [2]), students who are already falling  behind are less likely to engage in help-seeking behavior and can  therefore become  stuck  in a pattern that they do not know how  to appropriately confront. Designated academic advisors are  uniquely positioned to discuss course requirements with  individual faculty and apply that knowledge when engaging with  students. Helping these advisors to know which students to  contact during the term will allow the students who have the  greatest need to get connected with the resources already available  and improve their chance of success before the final course grade  is ultimately determined.     The M-STEM Academy model has already received acclaim and  support throughout our university administration and has spawned  new initiatives in biology and other STEM disciplines. With that  commitment has come an acknowledgement that in order to scale  appropriately, the staff and faculty supporting these academies  need data about their assigned students in order to have timely  interventions with students before they are in serious jeopardy of  academic failure. We believe that our model of presenting data to  academic mentors about students' performance and effort is not  only scalable, but also particularly appropriate given the  complexities involved in course planning, locating academic  services, and cultivating positive academic behaviors.    In our future work, we are exploring several additions to our  system that can make the task of identifying students who need  assistance easier by leveraging appropriate data sources and  technologies. First, we plan to include displays of admissions and  registrar data that can help inform students who might be  predisposed to greater difficulty in particular courses. We believe  that this type of information could be valuable at the start of term  as well as when students are planning their course schedules. We  plan to test this hypothesis as our design-research project  progresses. We will also investigate the potential to integrate  additional LMS data (e.g., file downloads) into our existing  model. Additionally, we are planning to construct tailored  messages to students. These messages will encourage students to  be pro-active about seeking advising services and utilizing the  support mechanisms available to them that are most appropriate  for each course (e.g., the science learning center for physics  courses, the writing center for English courses). This type of  message will supplement, rather than replace, the role of the  academic advisor. Our overall goal is to put the most timely and  helpful data in the hands of individuals who can analyze, interpret,  and translate the presented data into actionable strategies to  support student success. Learning is complex and so too,  justifiably, is the task for leveraging appropriate analytics to serve  that endeavor.   6. ACKNOWLEDGMENTS  Our thanks to the continued participation and feedback from our  partners in the M-STEM Academy including the academic  mentors Darryl Koch, Mark Jones, and Debbie Taylor and staff  members Cinda-Sue Davis and Dr. Guy Meadows. Also thanks to  Gierad Laput for recent efforts for future work in this project and  to Dr. Timothy McKay for his feedback about the application of  this research program at the University of Michigan.   7. REFERENCES  [1] Campbell, J., Deblois, P., & Oblinger, D. (2007). Academic   analytics: A new tool for a new era. EDUCAUSE Review,  42(4), 4057.   [2] Fritz, J. (2011). Classroom walls that talk: Using online  course activity data of successful students to raise self- awareness of underperforming peers. Internet and Higher  Education, 14(2), 89-97.   [3] Davis, C. S., St. John, E., Koch, D. & Meadows, G. (2010).  Making academic progress: The University of Michigan  STEM academy. Proceedings of the joint WEPAN/NAMEPA  Conference, Baltimore, Maryland.   [4] Maton, K. I., Hrabowski III, F. A., & Schmitt, C. L. (2000).  African American college students excelling in the sciences:  College and postcollege outcomes in the Meyerhoff Scholars  Program. Journal of Research in Science Teaching, 37(7),  629-654. doi:10.1002/1098-2736(200009)37:7<629::AID-  TEA2>3.0.CO;2-8   [5] Matsui, J., Liu, R., & Kane, C. M. (2003). Evaluating a  science diversity program at UC Berkeley: More questions  than answers. Cell Biology Education, 2(2), 113-121.  doi:10.1187/cbe.02-10-0050   [6] Mandinach, E. B., & Honey, M (Eds.). (2008). Data-driven  school improvement: Linking data and learning. New York:  Teachers College Press.   [7] Dahlstrom, E., de Boor, T., Grunwald, P., & Vockley, M.  (2011). The ECAR national study of undergraduate students  and information technology, 2011. Boulder, CO:  EDUCAUSE.    [8] Lonn, S., Teasley, S. D., & Krumm, A. E. (2011). Who needs  to do what where: Using learning management systems on  residential vs. commuter campuses. Computers & Education,  56(3), 642-649. doi:10.1016/j.compedu.2010.10.006   [9] Macfadyen, L. P., & Dawson, S. (2010). Mining LMS data to  develop an  early warning system  for educators: A proof of  concept. Computers & Education, 54(2), 588599.  doi:10.1016/j.compedu.2009.09.008   [10] Morris, L. V., Finnegan, C., & Wu, S. (2005). Tracking  student behavior, persistence, and achievement in online  courses. The Internet and Higher Education, 8(3), 221231.  doi:10.1016/j.iheduc.2005.06.009.   [11] Goggins, S., Galyen, K., & Laffey, J. (2010). Network  analysis of trace data for the support of group work: Activity  patterns in a completely online course. Proceedings of the  16th ACM International Conference on Supporting Group  Work (pp. 107-116), Sanibel Island, FL.  doi:10.1145/1880071.1880089   [12] Brown, A. L. (1992). Design experiments: Theoretical and  methodological challenges in creating complex interventions  in classroom settings. The Journal of the Learning Sciences,  2(2), 141-178.   [13] Cobb, P., Confrey, J., diSessa, A., Lehrer, R., & Schauble, L.  (2003). Design experiments in educational research.  Educational Researcher, 32(1), 9-13, 35-37.   [14] Cohen D. K., & Moffitt, S. L. (2009). The ordeal of equality:  Did federal regulation fix the schools Cambridge, MA:  Harvard University Press.  187      "}
{"index":{"_id":"33"}}
{"datatype":"inproceedings","key":"Sherin:2012:UCM:2330601.2330649","author":"Sherin, Bruce","title":"Using Computational Methods to Discover Student Science Conceptions in Interview Data","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"188--197","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330649","doi":"10.1145/2330601.2330649","acmid":"2330649","publisher":"ACM","address":"New York, NY, USA","keywords":"conceptual change, learning analytics","Abstract":"This paper presents current findings from an ongoing design-based research project aimed at developing an early warning system (EWS) for academic mentors in an undergraduate engineering mentoring program. This paper details our progress in mining Learning Management System data and translating these data into an EWS for academic mentors. We focus on the role of mentors and advisors, and elaborate on their importance in learning analytics-based interventions developed for higher education.","pdf":"Using computational methods to discover student science  conceptions in interview data   Bruce Sherin  Northwestern University   2120 Campus Drive  Evanston, IL USA  1-847-920-9987   bsherin@northwestern.edu          ABSTRACT  A large body of research in the learning sciences has focused on  students commonsense science knowledgethe everyday  knowledge of the natural world that is gained outside of formal  instruction. Although researchers studying commonsense science  have employed a variety of methods, one-on-one clinical  interviews have played a unique and central role. The data that  result from these interviews take the form of video recordings,  which in turn are often compiled into written transcripts, and  coded by human analysts. In my teams work on learning  analytics, we draw on this same type of data, but we attempt to  automate its analysis. In this paper, I describe the success we have  had using extremely simple methods from computational  linguisticsmethods that are based on rudimentary vector space  models and simple clustering algorithms. These automated  analyses are employed in an exploratory mode, as a way to  discover student conceptions in the data. The aims of this paper  are primarily methodological in nature; I will attempt to show that  it is possible to use techniques from computational linguistics to  analyze data from commonsense science interviews. As a test bed,  I draw on transcripts of a corpus of interviews in which 54 middle  school students were asked to explain the seasons.   Categories and Subject Descriptors  H.3.3. [Information search and retrieval]: clustering; I.2.7  [Natural Language Processing]: Text analysis; J.1  [Administrative Data Processing]: Education; K.3.1 [Computer  Uses in Education] Computer-assisted instruction (CAI)).   General Terms  Algorithms, Experimentation.   Keywords  Learning Analytics, Conceptual Change   1. INTRODUCTION  Much of the recent interest in learning analytics has been driven  by the great surge in the amount and kinds of data that are  available. This paper, in contrast, applies learning analytic   techniques to a type of data that has a long history, and that  predates recent technological advances. For the last few decades,  a large body of research in the learning sciences has focused on  students commonsense science knowledgethe everyday  knowledge of the natural world that is gained outside of formal  instruction. Although researchers studying commonsense science  knowledge have employed a variety of methods, one-on-one  clinical interviews have played a unique and central role. The data  that result from these interviews take the form of video  recordings, which in turn are often compiled into written  transcripts, and coded by human analysts.    In my teams work on learning analytics, we draw on this same  type of data, but we attempt to automate its analysis. In this paper,  I describe one part of this work. The automated analyses I present  here are not intended to code the data using categories developed  by human analysts. Rather, these analyses are employed in an  exploratory mode, as a way to discover student conceptions in the  data. Furthermore, my goal in this paper is not to contribute new  results to research on commonsense science. Rather, my aims are  primarily methodological in nature; I will attempt to show that it  is possible to use relatively simple techniques from computational  linguistics to analyze the type of data that is typically employed  by researchers in commonsense science. As a test bed, I draw on  transcripts of a corpus of interviews in which 54 middle school  students were asked to explain the seasons.   It should be emphasized that it is not at all obvious that it should  be possible to analyze data of this sort using simple computational  techniques. Unlike some other applications in learning analytics,  the total amount of data I have is relatively small. Furthermore,  the speech that occurs in commonsense science interviews can  pose particular difficulties for comprehension. Student utterances  are often halting and ambiguous. Furthermore, gestures can be  very important, and external artifacts such as drawings are  frequently referenced. However, our analysis algorithms only  have access to written transcripts of the words spoken by  participants.   Even with all of this complexity, my general approach is to go as  far as possible with simple methods, before proceeding to more  complex methods. Thus, the analyses I describe here make use of  extremely simple methods from computational linguistics methods that are based on rudimentary vector space models and  simple clustering algorithms.    2. LITERATURE REVIEW  2.1 Commonsense Science  It is now widely accepted that many of the key issues in science  instruction revolve around the prior conceptions of students. This  focus on commonsense science leads to a perspective in which the  central task of science instruction is understood as building on,      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   188    adapting, and, when necessary, replacing students prior  knowledge. One outcome of this focus has been the growth of a  veritable industry of research on students prior conceptions. The  bibliography compiled by Pfundt and Duit  [1], which lists  literature on the science conceptions of teachers and students,  provides one measure of the scale of this effort. As of early 2009,  the bibliography had over 8300 entries, spanning a wide range of  scientific disciplines, including, for example, what students  believe about the shape of the earth [2], evolution [3], and  nutrition [4].   In discussing the literature on commonsense science, it has  become commonplace to distinguish two theoretical poles. At one  extreme is the theory-theory perspective. According to this  perspective, commonsense science knowledge consists of  relatively well-elaborated theories [5]. At the other extreme, is the  knowledge-in-pieces (KiP) perspective. In this perspective, it is  assumed that: (a) commonsense science knowledge consists of a  moderately large number of elementsa systemof knowledge  and (b) the elements of the knowledge do not align in any simple  way with formal science domains [6, 7].   I believe that the computational methods described in this paper  should be of interest to a broad range of researchers who study  commonsense science, and adopt a range of theoretical  perspectives. However, the exploration of computational methods  presented in this paper was biased by my own theoretical  perspective, which lies closer to the KiP pole. As I hope will  become evident, my exploration of computational methods has  been driven by a desire to get at the more basic knowledgethe  piecesthat I believe comprise commonsense science knowledge.  And I have attempted to capture the dynamics that unfold as  students construct explanations during an interview.   2.2 Vector space models and their  applications in education research  Generally speaking, the goal of this work is to attempt to use  computational techniques in order to see student conceptions in  transcripts of commonsense science interviewers. There are many  techniques from computational linguistics that could be employed  in this way. The techniques I will use are based primarily on a  type of vector space model [8]. In vector space models, the  meaning of a block of texta word, paragraph, essay, etc.is  associated with a vector, usually in a high dimensional space. So,  two blocks of text have the same meaning to the extent that their  vectors are the same. In this way, a vector space analysis makes it  possible to compute the similarity in meaning between any pair of  words or blocks of text. In Section 4, I will describe, in some  detail, the algorithms employed in the particular analyses used in  this work.   One particular variant of vector space model, Latent Semantic  Analysis (LSA), has had increasing prominence across a range of  disciplines and applications [9-11]. LSA incorporates several  innovations that distinguish it from the most basic form of vector  space analysis; most centrally, it makes use of an auxiliary  training corpus that provides information about the wider contexts  in which terms appear, and it reduces the dimensionality of the  vector space, which has the effect of uncovering latent relations  among terms.   Vector space methods have seen increasing use in educational  research. These applications have been greatly dominated by uses  of LSA. In fact, outside of information retrieval, some of the  earliest and most persistent uses of LSA have been in applications  related to education [12]. These applications have been of two   broad types. First, LSA has been used as a research tool by  educational researchersthat is, as a means of analyzing data, in  order to study thinking and learning. Second, LSA has been used  as a component of intelligent instructional systems. The majority  of these educational applications, across both types, have been  focused on the teaching of reading and writing.  For example, LSA-based systems have been employed to  automatically score essays written by students [10, 13]. In a  number of applications, students are asked to summarize a  passage or document that they have just read, and an LSA-based  system is used to evaluate these summaries. In one such  application, Shapiro and McNamara [14] had students read and  summarize portions of psychology textbooks. Using LSA, these  summaries were then compared both to the text the students read,  and to model essays composed by experts. Similarly, Magliano  and colleagues conducted a wide range of studies in which LSA  was used to assess the strategies employed by readers and their  reading skill, more broadly [15, 16].   In many of these uses of LSA, the data consisted of written text  produced by participants in the research. However, in some  instances, LSA has been applied to transcriptions of verbal data.  For example, in their study mentioned above Shapiro and  McNamara [14] found that LSA could be applied successfully  both to written summaries of the textbook and to transcriptions of  verbal summaries given by students. Similarly, Magliano and  Millis [15] applied LSA to think-aloud protocols that students  produced as they read passages of text.   As mentioned above, LSA has been used as a component of  intelligent instructional systems. For example, intelligent systems  have been constructed that provide feedback to students on  summaries that they write of a given text passage [17, 18]. One  LSA-based system, AutoTutor, is of particular interest here  because it has been applied to teach science-related subject matter  [19, 20]. AutoTutor teaches physics by first posing a problem or  question. The student responds by typing a response into the  system. The system then evaluates that response by using LSA to  compare the students text to a set of predefined expectations and  misconceptions; the expectations are pre-specified components of  a correct response and the misconceptions are possible erroneous  ideas that might be expressed by the student. Based on this  analysis, the system responds by posing further questions to the  student, either to help correct the misconceptions, or to draw out  more components of a complete answer to the original problem.   I want to say a bit about where the work described in the present  paper fits within the space of uses of vector space models in  education. First, in this work, SNLP is used as an analytic tool for  researchers; I will not be describing an LSA-based system that is  used by students. Second, I apply my analyses to verbal data. As  mentioned above, many applications of vector space models in  education use text that is typed by a student, either in the form of  an essay or short responses. Furthermore, prior research that has  worked with verbal data has employed data that is very different  than that employed in the present work. For example, the work by  Shapiro and McNamara [14] and Magliano and colleagues [15,  16], which I mentioned above, employed a more constrained type  of think-aloud protocol, focused on passages of text that were just  read. In contrast, the verbal data employed in this work consists of  relatively free-flowing discussions involving back-and-forth  between an interviewer and interviewee.   Third, in all these applications, answers given by students,  whether in written or verbal form, were evaluated by comparison  to a predefined model. This model might be, for example, some   189    portion of the text just read, or an ideal answer constructed by the  researcher. In contrast, as mentioned above, I will describe  techniques for automatically inducing a set of conceptions from  the data itself.   Finally, I want to emphasize one other respect in which this work  differs from prior work in education that made use of LSA;  namely, I am not using LSA! As noted above, I believe it makes  sense to begin with simpler techniques, and then to pursue more  sophisticated methods as it seems necessary.   3. THE INTERVIEWS  3.1 Subject matter and interview design  The data used in this work was drawn from a larger corpus  collected by the NSF-funded Conceptual Dynamics Project  (CDP).1 For the present work, I draw from a set of 54 interviews  in which students were asked to explain Earths seasons [21].    The seasons have long been a popular subject of study in research  on commonsense science, and a significant number of studies  have set out to study student and adult understanding in this area  [22-26]. Our seasons interview always began with the interviewer  asking Why is it warmer in the summer and colder in the  winter After the student responded, the interviewer would, if  necessary, ask for elaboration or clarification. The interviewer had  the freedom, during this part of the interview, to craft questions  on-the-spot in order to clarify what the student was saying.   Next, the student was asked to draw a picture to illustrate their  explanation. Then, once again, the interviewer could ask follow- up questions for clarification. Our interviewers were also prepared  with a number of specific follow-up questions to be asked, as  appropriate, during this part of the interview. Some of these  questions were designed as challenges to specific explanations  that students might give.   3.2 Overview of student responses  In prior work with our seasons data, Conceptual Dynamics  researchers have adopted a strongly KiP perspective [21]. We  assume that students possess a system consisting of many  knowledge elementsthe piecesthat may potentially be  drawn upon as they endeavor to explain the seasons. When a  student is asked a question during an interview, some subset of  these elements are activated. The student then reasons based on  this set of elements, and works to construct an assemblage of  ideas in the service of explaining the seasons. We refer to this  assemblage of ideas as the dynamic mental construct or DMC, for  short. For the purpose of the present work, it is not a bad  approximation to think of a DMC as a students current working  explanation of the seasons. So, throughout this manuscript, I will  use the terms DMC and explanation interchangeably.   The explanations of the seasons given by the students we  interviewed varied along a number of dimensions. But it is  helpful, nonetheless, to begin with a number of reference points,  in the form of a few categories of explanations (DMCs). The first  category, closer-farther, is illustrated by the diagram in Figure 1a.  In closer-farther explanations, the earth is seen as orbiting (or  moving in some other manner) in such a way that it is sometimes  closer to the sun and sometimes farther. When the earth is closer  to the sun then it experiences summer; when its farther away it  experiences winter.                                                                        1 NSF grant #REC-0092648. Conceptual dynamics in complex  science interventions (B. Sherin, PI).   The second category of DMC, side-based, is illustrated in Figure  1b. Side-based explanations are usually focused on the rotational  motion of the earth, rather than its orbital motion. In side-based  explanations, the earth rotates so that first one side, then the other,  faces the sun. The side facing the sun at a given time experiences  summer, while the other side experiences winter.    (a)     (b)     (c)  Figure 1. Closer-farther, side-based, and tilt-based DMCS.   The third and final category of DMC, tilt-based, is depicted in  Figure 1c. Tilt-based DMCs depend critically on the fact that the  earths axis of rotation is tilted relative to a line connecting it to  the sun. In a tilt-based explanation, the hemisphere that is tilted  toward the sun experiences summer and the hemisphere that is  tilted away experiences winter. This category includes the  normative scientific explanation, as well as some non-normative  explanations.   As discussed in Sherin et al. [21], during an interview, students  tend to move among DMCs. In some cases, students do begin the  interview with what appears to be a fully-formed explanation. In  other cases, a student might construct an explanation during the  interview, slowly converging on an explanation they find  reasonable. Finally, students can be to seen to shift from one  DMC to another, sometimes in response to a challenge from the  interviewer.   3.3 Example interviews  Now I will briefly discuss a few example interviews. These  examples will play a role as important reference points when I  discuss the automated analysis. In this first example, a student,  Edgar, began by giving an explanation focused on the fact that the  Earth rotates, and he stated that light would hit more directly on  the side facing the sun. He made the drawing shown in Figure 2,  as he commented:   E: Heres the earth slanted. Heres the axis. Heres the North  Pole, South Pole, and heres our country. And the suns  right here [draws the circle on the left], and the rays hitting  like directly right here. So everythings getting hotter over  the summer and once this thing turns, the country will be  here and the sun can't reach as much. It's not as hot as the  winter.   After a brief follow up question by the interviewer, Edgar seemed  to recall that the Earth orbited the sun, in addition to rotating. He  then shifted to a closer-farther type explanation:   E Actually, I don't think this moves [indicates Earth on  drawing] it turns and it moves like that [gestures with a  pencil to show an orbiting and spinning Earth] and it turns  and that thing like is um further away once it orbit around  the s- Earth- I mean the sun.   190    I Its further away   E Yeah, and somehow like that going further off and I think  sun rays wouldnt reach as much to the earth.   Thus Edgars interview illustrates a case in which a student began  with a side-based explanation and transitioned to a closer-farther  explanation. It is also worth noting that Edgars language was  halting, imprecise, and made significant use of gestures and his  drawings. These are features that might well pose difficulties for  automated analysis.     Figure 2. Edgar's drawing.   I want to briefly introduce interviews with two other students  from the corpus, both of whom gave variants of tilt-based  explanations. The first example is from an interview with Caden.   I:  So the first question is why is it warmer in the summer and  colder in the winter   C:  Because at certain points of the earths rotation, orbit around  the sun, the axis is pointing at an angle, so that sometimes,  most times, sometimes on the northern half of the  hemisphere is closer to the sun than the southern  hemisphere, which, change changes the temperatures. And  then, as, as its pointing here, the northern hemisphere it  goes away, is further away from the sun and gets colder.   I:  Okay, so how does it, sometimes the northern hemisphere  is, is toward the sun and sometimes its away   C:  Yes because the atIm sorry, the earth is tilted on its axis.  And its always pointed towards one position.   Note that, in Cadens explanation, the tilt of the earth affects  temperature because the hemisphere tilted toward the sun is closer  to the sun, and the hemisphere tilted away is farther from the sun.  (This is not correct.) In contrast, another student, Zelda gave a tilt- based explanation, but her explanation made use of the fact that  the tilt of the earth causes rays to strike the surface more or less  directly, and this is what explains the seasons.   Z: Because, I think because the earth is on a tilt, and then, like  that side of the Earth is tilting toward the sun, or its facing  the sun or something so the sun shines more directly on that  area, so its warmer.   Thus, Caden and Zelda both gave tilt-based explanations, but they  differed in how exactly the tilt of the earth affected the seasons.  For Caden the tilting causes one hemisphere or the other to be  closer to the sun. For Zelda, the tilting causes parts of the earth to  receive the suns rays more or less directly. This illustrates some  of the types of features we would like the automated analysis to  resolve.    4. VECTOR SPACE ANALYSIS OF THE  SEASONS CORPUS  In order to captured students conceptions expressed in the  seasons interviews, my team explored the use of techniques from  statistical natural language processing. In particular, we explored  the use of vector space models, augmented with cluster analysis.  These choices make sense for a number of reasons. As mentioned  above, one type of vector space model, LSA, has already been  employed, with some success, in applications that are in some   respects close to my own [10, 14-17, 19, 27].   In addition, initial attempts by Gregory Dam and Stefan  Kaufmann to apply LSA to my research teams data proved  promising, and thus justified further exploration [28]. Dam and  Kaufmann employed techniques based on one variant of LSA to  apply a given coding scheme to an earlier subset of this corpus.    The work described in this manuscript extends the work of Dam  and Kaufmann in several respects. First, Dam and Kaufmanns  analysis did not discover student conceptions in the data corpus.  Instead, it began with the conceptions identified by human  analysts and used those conceptions to code transcript data.  Second, unlike Dam and Kaufmann, I will be exploring the use of  simpler vector space models, rather than LSA.    Third, Dam and Kaufmann were primarily concerned with coding  at the level of students. Each student was coded by the computer  in terms of just one of three possible explanations of the seasons.  The success of this analysis was judged by comparison to an  analysis of these same transcripts by human coders, restricted to  the same set of three explanations. However, this type of analysis  represented a drastic simplification over our earlier qualitative  analyses of the corpus. As exemplified in the description of  Edgars interview above, the explanations given by students over  the course of an interview were quite clearly dynamic. Thus,  assigning a single code to each manuscript was often a dramatic  simplification. In this new work, all of my analysis is done at a  finer time scale; I look to identify student ideas only in small  segments of text.   In the rest of this section, I describe an exploratory analysis of our  data. Here, I restrict myself to one pathway through the analysis,  using one set of parameters and algorithms. In Section 5, I briefly  describe the results I obtain when employing different parameters  and algorithms.   4.1 The basics: Converting text to vectors  The central idea underlying any vector space model of text  meaning is relatively simple: Every passage of textwhether it is  a word, sentence, or essayis mapped to a single vector. The  direction in which this vector points is taken to be a representation  of the meaning of the passage. More precisely, the similarity  between two passage vectors is quantified as the cosine of the  angle between the two vectors (or, equivalently, the dot product of  the vectors if we assume the vectors are of unit length).   Table 1. Partial vocabulary and sample counts   sun 4 2.1  earth 2 1.7  side 0 0  away 2 1.7  tilted 1 1  closer 1 1  axis 2 1.7  day 0 0  farther 1 1  time 3 2.1   The question, of course, is how we go about converting a passage  of text to a vector. In the most rudimentary forms of vector space  models, this mapping is accomplished in a rather straightforward  manner. First, we look across the entire corpus of text that we  wish to include in our analysis, and we compile a vocabulary, that  is, a complete list of all of the words that appear somewhere in the   191    corpus. This vocabulary is then pruned using a stop list of  words. This stop list consists primarily of a set of highly common  non-content words, such as the, of, and because. For the corpus  used in this work, this resulted in a vocabulary consisting of 647  words. (The stop list used contained 782 terms.) If the vocabulary  is sorted from the most common to least common words, the top  10 words correspond to the list shown in the left hand column of  Table 1.   This vocabulary can be used to compute a vector for a passage  from an interview transcript as follows. First, we take the  transcript and remove everything except the words spoken by the  student. Any portion of the remaining text can now be converted  to a vector. To do so, we go through the entire vocabulary,  counting how many times each word in the vocabulary appears in  the text being analyzed. When this is done, we get a list of 647  numbers. If, for example, we process the portion of Cadens  transcript presented above, we obtain the values listed in the  middle column of Table 1 for the 10 most common words in the  larger corpus.   Finally, in most vector space analyses, the raw counts are  modified by a weighting function. In the analyses reported on in  this section, I replaced each count with (1 + log(count)). This has  the effect of dampening the impact of very frequent words. (Raw  counts of 0 were just left as 0.) Appropriately weighted values are  shown in the third column of Table 1.   4.2 Using passage vectors to discover  meanings in the data corpus  We now have a means of mapping a passage of text to a vector  consisting of 647 numbers. This capability can now be used to  discover units of meaning that exist across the 54 interviews that  comprise my data corpus. This process involves four steps which I  will now discuss: (1) preparing and segmenting the corpus, (2)  mapping segments to vectors, (4) clustering the vectors, and (5)  interpreting the results.    4.2.1 Preparing and segmenting transcripts  First, as discussed above, the transcripts are reduced to that they  include only the words spoken by the student during the  interview. Next, recall that, in our earlier analyses of this corpus  conducted by my research team, we found that students could be  seen to construct explanations of the seasons out of large number  of knowledge resources, and that their explanations could shift as  an interview unfolded. We thus need a way to attach meanings to  small parts of an interview transcript. This requires a means of  segmenting a transcript into smaller parts.    In keeping with my goal of using simple methods, I segmented the  transcripts by breaking each transcript into 100-word segments. In  order to lessen problems that might be caused by the fact that this  introduces arbitrary boundaries, I chose to employ overlapping  100-word segments, with the start of each segment beginning 25  words after the start of the preceding segment. So the first  segment of a transcript would include words 1-100, the second  words 26-125, the third 51-150, etc. When all of the 54 interview  transcripts were segmented in this manner, I ended up with 794  segments of text. These specific choices for segment size and step  size are, of course, somewhat arbitrary. In Section 5, I will briefly  present results with different values of these parameters.   4.2.2 Mapping segments to vectors  The next step in the analysis is to map each of these 794 segments  to a vector. To accomplish this, I employ precisely the method   described above. The result is 794 vectors, each consisting of a  list of 647 numbers.   However, here I must introduce one complication. There is one  inherent problem with applying vector space models to an  analysis of this sort of data. Vector space models such as LSA  were originally developed as a means to find documents in a large  corpus that pertain to a given topic. They were thus not developed  for finding fine distinctions in meaning among documents  pertaining to very similar topics. However, all of the documents  involved in my analysis are about very similar subject matter; they  all explain the seasons, and they almost all do so by talking about  the position and motion of the earth in relation to the sun.   In fact, the clustering analysis (described in the next section) does  not produce meaningful results if I use the raw document vectors  that are produced by the method described above. (I will say more  about this problem in Section 5.) Instead, I need a means of  modifying the vectors so that they highlight their more unique  featuresthe features that, on average, tend to differentiate the  segment from the other 793 segments of text.   For that purpose, I compute what I call deviation vectors. To  compute the deviation vectors for two vectors V1 and V2, I first  find their average, and then break each vector into two  components, one that lies along the average, and another that is  perpendicular to the average (refer to Figure 3). The perpendicular  components, V1' and V2', are the deviation vectors. If we use  these deviation vectors in place of the original vectors, the result  is that V1 and V2, have each been replaced by the component that  defines its unique piece  a piece that characterizes how it differs  from the average.    The same procedure can be employed with any number of vectors.  For the next steps of the analysis, I replaced the 794 segment  vectors in just this way; I found their average, and then replaced  each vector with its deviation from this average.     Figure 3. How to compute deviation vectors.   4.2.3 Clustering the vectors  Now each of the 794 segments has been mapped to a vector that  we understand as representing the meaning of that segment. The  next step is to identify common meanings amongst these  segments. To do that, we look for natural clusterings of the 794  vectors.   To cluster the transcript vectors, I employed the very general  technique called hierarchical agglomerative clustering (HAC). In  HAC, we begin by taking all of the items to be clustered, and  placing each of these items in its own cluster. Thus, we begin with  a number of clusters equal to the total number of items. Then we  pick two of those clusters to combine into a single cluster  containing two items, thus reducing the total number of clusters  by one. The process then iterates; we again pick two clusters to  combine, and the total number of clusters is decreased by one.  This repeats until all of the items are combined into a single  cluster. The result is a list of candidate clusterings of the data,   192    with each candidate corresponding to one of the intermediate  steps in this process.   A central issue in applying this algorithm is determining which  clusters to combine on each iteration. In practice, there are many  rules that can be applied. Throughout my discussions here, I will  present results that were obtained using a technique called  centroid clustering. At each step in the iteration, I first find the  centroid of each cluster (the average of all of the vectors currently  in the cluster). Then I find the pair of centroids that are closest to  each other, and merge the associated clusters. An explanation of  centroid clustering, including its application to vector space  models, can be found in [29].   4.2.4 Determining the number of clusters  The result of the clustering analysis can be thought of as a table  with 794 rows. At the top is a row in which each segment is in a  single cluster. At the bottom is a row in which all of the segments  are in a single cluster. Table 2 displays the results for just a part of  this large table. The bottom row, for example, shows the results  when the segments are grouped into three clusters that contain 271  segments, 279 segments, and 244 segments respectively. As you  move up the table the number of clusters grows, and the size of  each cluster shrinks. In each row of Table 2, clusters contain  segments that have been grouped together because, from the point  of view of our vector space model, they have similar meanings.  This means that each row in Table 2, constitutes a candidate  coding schemeit is a scheme for sorting segments into  categories. The puzzle, of course, is which row to select.    Table 2. Sizes of clusters for selected clusterings   10 19 72 9 68 140 62 44 122 136 122   9 19 72 68 62 44 122 136 122 149   8 19 72 68 44 122 136 122 211   7 72 68 44 122 122 211 155   6 68 44 122 122 211 227   5 68 122 122 211 271   4 122 122 271 279   3 271 279 244    Unfortunately, there is no simple answer to this question. In  general, there is a tradeoff. When the number of clusters is high,  we obtain a better fit to the data. However, we get this better fit at  the expense of a more complex model. Because each cluster is  described by a list of 647 values, each additional cluster  represents a dramatic increase in model complexity.2   Here, as elsewhere, I make my choice in a heuristic manner.  Across multiple analyses, I have found that working with a set of  about 7 clusters strikes a workable balance. With 7 clusters, it is  possible to resolve interesting features of the data, while  producing results (in the form of graphs) that are not overly  difficult to interpret.   4.2.5 What do the clusters mean  We now have grouped the 794 segments into 7 clusters, each                                                                        2 For this reason, if I use traditional measures for determining the   appropriate number of clusters (e.g., Bayesian information  criterion or Akaike information criterion), the terms  corresponding to the model complexity always dominate, and  the model with the smallest number of clusters prevails.   containing between 44 and 211 segments (refer to Table 2). The  next question we must answer is: What do these clusters mean  Each of the 7 clusters can be thought of as defined by its centroid  vectorthe average of all of the vectors that comprise the cluster.  These centroids each, in turn, are described by a list of 647  entries, each of which corresponds to one of the words in the  vocabulary. One way to attempt to understand the meaning of the  clusters, then, is to look at the words that have the largest value in  each centroid vector.    When this is done I obtain the results shown in Figure 4. For each  cluster, I list the 10 words that are most strongly associated with  that cluster, ignoring words that appeared less than 30 times in the  overall corpus. In addition, the second column in each table has  the value from the centroid vector corresponding to this word. The  third column in each table lists the total number of times that the  word appears across the entire corpus.    4.2.6 Interpreting the clusters based on the word lists  In many respects, the lists of words shown in Figure 4 clusters are  suggestive. First, several of the clusters seem to align with the  three broad classes of seasons explanations listed in Section 3. For  example, it seems natural to associate Cluster 1, which starts with  the words tilted, towards, and away, with tilt-based explanations.  Similarly, it seems natural to associate Cluster 4 (side, facing)  with side-based explanations, and Cluster 7 (farther, closer) with  closer-farther explanations of the seasons.   Cluster 1Cluster 1Cluster 1  tilted 0.767 82 towards 0.199 40 away 0.186 83 north 0.098 30 part 0.084 46 guess 0.077 31 closer 0.044 82 warmer 0.042 40 sun 0.03 545 farther 0.017 71  Cluster 2Cluster 2Cluster 2  earth 0.4 395 spinning 0.366 37 spins 0.2 38 time 0.198 65 axis 0.121 77 seasons 0.068 30 tilted 0.031 82 angle 0.017 31 north 0.014 30 chicago 0.006 45  Cluster 3Cluster 3Cluster 3  hemisphere 0.603 47 northern 0.522 31 colder 0.119 52 facing 0.106 46 closer 0.043 82 farther 0.035 71 warmer 0.023 40 axis 0.021 77 away 0.02 83 rays 0.018 33  Cluster 4Cluster 4Cluster 4  side 0.722 95 facing 0.091 46 earth 0.085 395 part 0.068 46 chicago 0.018 45 guess 0.008 31 seasons -0.008 30 time -0.01 65 heat -0.025 30 rotates -0.026 54  Cluster 5Cluster 5Cluster 5  rays 0.293 33 north 0.197 30 angle 0.194 31 light 0.188 41 chicago 0.163 45 sun 0.134 545 heat 0.076 30 towards 0.045 40 warmer 0.02 40 side 0.019 95  Cluster 6Cluster 6Cluster 6  day 0.415 75 moon 0.398 52 night 0.377 63 rotates 0.178 54 rotating 0.068 32 earth 0.055 395 spins 0.05 38 facing 0.048 46 light 0.046 41 seasons 0.046 30  Cluster 7Cluster 7Cluster 7  farther 0.413 71 closer 0.403 82 away 0.379 83 colder 0.216 52 sun 0.103 545 warmer 0.064 40 rotates 0.033 54 time 0.028 65 heat 0.02 30 rotating 0.013 32    Figure 4. Top words associated with each cluster.  But these clusters are not supposed to necessarily align with full- fledged explanations of the seasons. They are clusters of  segments, which it is hoped can align with smaller conceptual  units that, when combined, form the basis of a constructed   193    explanation. And, indeed, the additional clusters do seem to offer  the possibility of an analysis of that sort. For example, we should  expect tilt-based explanations to often be seen in concert with talk  about the Earths hemispheres (Cluster 3). And recall that tilt- based explanations invoke different mechanisms by which the  changing tilt of the earth impacts temperature. For example,  Caden argued that the tilting of the Earth causes parts of the earth  to be alternately closer or farther from the sun. In contrast, Zeldas  explanation focused on the impact of the Earths tilt and how it  impacts the angle and directness of the suns rays. We should thus  be able to see these ideas in combination, when we look at  individual interviews.   Similarly, we should expect to see side-based explanations  (Cluster 4) in tandem with clusters having to do with the rotation  of the Earth. Ideas about the rotation of the Earth seem to appear  in Cluster 2 and Cluster 6. Cluster 2 seems to truly be focused on  the spinning of the Earth. Cluster 6, in contrast, seems to be more  about day and night. Not surprisingly, talk about the rotation of  the Earth was often combined with talk about the day/night cycle.    4.3 Application to segmented transcripts  The clusters shown in Figure 4 thus seem to have reasonable  interpretations in terms of our understanding of the data corpus.  We have thus identified a set of common underlying ideas. A  next step I can take is to apply this set of ideas back to the original  transcripts. I want to use these ideasthese units of meaningto  interpret individual student interviews.   In order to accomplish this, I begin by preparing each of the  interview transcripts precisely as before; the transcripts are  reduced so that they include only the words spoken by a student,  then they are broken into 100-word segments using a moving  window that steps forward by 25 words. Next I compute the  vector for each of these segments, again using the same  techniques described earlier. Finally, each of these vectors for the  segments is compared to the 7 centroid vectors corresponding to  the 7 clusters (by taking the cosine of the angle between the  vectors and each centroid).   I begin my discussion of the results with Zelda, since her analysis  produces a graphic that is relatively easy to read. In Figure 5 we  see that Zeldas transcript has been broken into 5 overlapping  segments. Each of these segments is associated with 7 bars, one  bar for each of the 7 clusters. For all of the segments, Cluster 1 is  the clear winner. This makes sense since, as discussed earlier,  Zelda gave an answer that was very close to the accepted  scientific explanation of the seasons. Note, also, that the bar for  Cluster 5 is slightly elevated in three of the segments. Cluster 5  had to do with rays striking the earth at an angle. Again, this  makes sense given what we can read in Zeldas transcript.     Figure 5. Segmenting analysis of Zelda's transcript.   The interview with Caden provides an interesting contrast. When  Cadens transcript is analyzed using the segment centroids, we get  8 segments with the bars shown in Figure 6. Like Zelda, we  understood Caden as giving an explanation that emphasized the  tilt of the Earth. But, in Figure 6, we see that Cluster 3  dominatesthe cluster having to do with the Earths hemispheres   although there are hints of Cluster 1 (tilted-toward) in the  earlier segments. The predominance of Cluster 3 is not too  surprising. As I noted earlier, tilt-based explanations should be  closely associated with discussion of the two hemispheres of the  earth. Indeed, glancing at the portion of Cadens transcript  presented earlier, there is an emphasis on the different effects on  the northern and southern hemispheres.     Figure 6. Segmenting analysis of Caden's transcript.   Both Zelda and Caden were relatively stable in the explanations  that they gave. We would now like to see if this analysis can  capture shifts that occur as an interview unfolds. To see this, we  can now return to the interview with Edgar. Looking at Figure 7 it  seems clear that the interview has a two major parts. The first part  is dominated by Cluster 5, which has to do with rays striking the  Earths surface. The latter part is strongly dominated by Cluster 7,  which is the closer-farther cluster. Thus, once again, it seems  possible to interpret the automated analysis in a manner that is  consistent with our qualitative analysis of the interview.     Figure 7. Segmenting analysis of Edgars transcript   5. Alternative analysis methods  To this point, my exploration has been limited in a particular way;  I have looked at some of the results produced by my analysis, but  all of these results were produced by a single set of algorithms,  and with a single set of input parameters. In particular, my  analysis followed the following plan: (1) The transcripts were  pruned so that they only contained the words spoken by the  interviewee; (2) the resulting documents were broken into 100- word segments, with a step size of 25 words; (3) a vector was  computed for each segment, using a weight function of (1 +  log(count)), and ignoring words in my stop list; (4) the resulting  vectors were replaced with their deviation vectors; and (5) the  vectors were clustered using hierarchical agglomerative  clustering. These choices of algorithms and parameters were  chosen, in part, because they produced interpretable results. In this  section, I want to briefly give a sense for the results produced by  alternative approaches, including some that did not produce  interpretable results.   5.1 Alternative parameters  There are many ways in which the above analysis could be  altered, while still employing a method that is very similar in  outline. For example, the composition of the stop list could be  changed, and the transcripts could be pruned in a different  manner. For example, in pruning the transcripts, I needed to  decide what to do about word fragments, whether to leave them,  delete them, or complete them. More dramatically, I could have   194    opted to stem words, that is, to reduce them to their base or root.  For the most part, these smaller changes produced similar  interpretable results across a large range of variations.    For illustration, I will present the results obtained if the transcripts  are segmented into 50 word segments, with a step size of 10  words (rather than 100 and 25). When this is done, I end up with  2320 segments. When these segments are clustered, I obtain the  results shown in Table 4. Like Table 3, this table shows the sizes  of the clusters that are produced during some of the latter steps in  the clustering. Note that when the segments are grouped into 7  clusters, one of the clusters contains only 1 segment. For that  reason, it makes to sense to look at the next row in the table,  where the segments are grouped into 6 clusters. Figure 8 shows  the word lists associated with these clusters, which were produced  in the same method used to produce the lists in Figure 4.   Table 3. Sizes of clusters for selected clusterings   10 1 78 88 160 156 11 628 235 137 638   9 1 88 160 156 11 628 235 638 215   8 1 160 11 628 235 638 215 244   7 1 160 235 638 215 244 639   6 160 235 638 215 639 245  5 235 638 215 639 405   4 638 639 405 450  3 638 450 1044    Now we can compare the lists in Figures 8 and 4. Although there  are many differences, it is not difficult to discern an alignment.  Clusters 1, 3, 4, and 5 in the new analysis seem to be similar to the  corresponding clusters in the original analysis. Cluster 6 seems to  be similar to Cluster 7 in the original analysis. Finally, Cluster 2  in the new analysis is similar to both Cluster 2 and Cluster 6 in the  original analysis. (Note that it makes sense for Clusters 2 and 6 in  the original analysis to be grouped together since they both pertain  to the rotation of the Earth.) Thus, while there are certainly  differences, the qualitative picture produced by this new analysis  seems to bear a close resemblance to the original analysis.   Cluster 1Cluster 1Cluster 1  tilted 0.718 82 north 0.353 30 towards 0.185 40 part 0.106 46 away 0.076 83 guess 0.043 31 warmer 0.022 40 angle 0.015 31 chicago 0.0 45 hemispher e  -0.009 47  Cluster 2Cluster 2Cluster 2  earth 0.522 395 moon 0.298 52 day 0.218 75 rotates 0.206 54 night 0.197 63 axis 0.164 77 spinning 0.16 37 spins 0.1 38 rotating 0.069 32 time 0.061 65  Cluster 3Cluster 3Cluster 3  hemisphere 0.618 47 northern 0.433 31 facing 0.312 46 colder 0.096 52 part 0.033 46 rotating 0.029 32 light 0.018 41 away 0.015 83 axis 0.011 77 towards 0.008 40  Cluster 4Cluster 4Cluster 4  side 0.849 95 earth 0.048 395 seasons 0.023 30 facing 0.015 46 warmer 0.013 40 rotates 0.007 54 guess 0.004 31 chicago -0.013 45 part -0.016 46 northern -0.017 31  Cluster 5Cluster 5Cluster 5  chicago 0.6 45 light 0.35 41 rays 0.315 33 heat 0.113 30 time 0.071 65 towards 0.045 40 facing 0.041 46 sun 0.037 545 warmer 0.022 40 seasons 0.014 30  Cluster 6Cluster 6Cluster 6  sun 0.431 545 closer 0.305 82 farther 0.261 71 away 0.208 83 colder 0.09 52 angle 0.059 31 heat 0.038 30 warmer 0.014 40 guess -0.001 31 rays -0.024 33    Figure 8. Top words associated with each cluster.   5.2 Alternative algorithms  Of course, it is possible to make much more substantial changes to  the analysis presented in Section 4. Here I will consider changes  related to one unusual feature of my analysis, the use of deviation  vectors. Recall that I introduced the deviation vectors as a means  of addressing the fact that there was substantial overlap in the  vectors that are produced by my initial computation of document  vectors.    Table 4 shows the results that are produced when I do not  compute deviation vectors before clustering the segments. Note  that in each of the candidate clusterings shown, I obtain one very  large cluster, containing most of the segments, and several very  small clusters. When I look at earlier stages of the clustering, I see  the following behavior: initially the segments are all clustered into  a large number of relatively small changes, then the these small  clusters begin to agglomerate, one at a time, onto one large  cluster. In the final stages, which we see in Table 4, some small  remaining clusters are swept up into the large cluster. In short, this  analysis does not seem to discover a small number of moderately- sized clusters that we can associate with conceptions.   Table 4. Sizes of clusters for selected clusterings   10 2 2 3 4 3 4 9 6 11 750   9 2 2 3 4 3 4 6 11 759   8 2 2 3 4 3 4 6 770   7 2 2 3 3 4 6 774   6 2 3 3 4 6 776   5 2 3 3 4 782   4 2 3 4 785   3 3 4 787    The question remains as to whether there are other more standard  approaches that might improve on the results shown in Table 4. In  particular, it is standard practice to use judiciously-chosen  weighting functions as a means of accentuating the differences  among documents. Recall that the counts in my vectors were all  weighted by (1 + log(count)), where count is the number of  times a word appears in a given document. We can modify this  function so that it weights words that appear across many  documents less strongly than words that appear in in just a few  documents. I tried several such weighting functions, including  variants of the so-called tf-idf weighting. In all cases, I obtained  results that looked like Table 4.   6. Discussion  6.1 Summary  I began this paper with the observation that research on  commonsense science knowledge typically focuses on data  derived from one-on-one clinical interviews. To date, researchers  in this field have generally used humans as instruments for  analyzing this data. I believe that we have done so because of  some tacitly-held beliefs: we have tended to assume that, to make  sense of clinical interview data, it is necessary to have an  instrument with an ability to understand natural language. We  have also assumed that it is necessary to have access to as much  of the interaction as possible. We need not just the words spoken;  we also need gestures, facial expressions, drawings, etc. It also  seems to require the ability to make leaps that look across the  breadth of a data corpus.  The task of analysis is also, in some respects, complicated by the   195    theoretical position I adopted in this work. I believe that, in many  cases, it is simply not possible to understand a student as  expressing a single model of the seasons. Instead, students  construct and shift explanationsDMCsas the interview  unfolds. I want to capture this movement in explanations.    Nonetheless, this work set out to explore how much can be  accomplished with a relatively simple suite of techniques from  statistical natural language processing. Stated crudely, the  statistical techniques rely primarily on counting words.  Furthermore, from among the bag of words models that are  employed by linguists, I chose to begin with one of the simplest  possible models.   In short, there was every reason to think that the types of analysis  described here would not be very successful. Nonetheless, these  results are at least suggestive that these simple techniques can  give meaningful results. The clustering algorithm produced a set  of clusters that seemed to have meaningful interpretations interpretations that made sense given earlier qualitative analyses  of the same corpus. And, when these clusters were employed to  produce a segmented analysis of individual transcripts, they  produced a narrative analysis of the transcript that aligned with  the descriptions produced by human analysts.   That said, most of my presentation in this manuscript has had an  exploratory character. My goal has been only to begin to map the  boundaries of what might be possible with a family of relatively  simple computational techniques. In this final section of the paper,  I reflect on what we can conclude, and I discuss caveats.   6.2 What do these computational techniques  buy us  What role might these computational techniques play in the  toolkit of researchers, especially researchers who use clinical  interviews to study the conceptions of science students I  presented many results that were intriguing, but my results were  only intended to be about the methods themselves; I didnt use the  methods in the service of any scientific agenda. So what, in the  long term, might these techniques buy us   One question to ask is whether computational techniques can and  should replace human analysts, or at least reduce the work  required. Whether or not this may ultimately be possible, I should  be clear that the analyses presented in this paper were still highly  dependent on human interpretation. For example, I had to make a  judgment about the appropriate number of clusters to work with.  Even more importantly, the analysis required me to make sense of  the lists of words that were associated with each of the clusters.  Nonetheless, the computational techniques described here might  play a useful role in our toolkits, even in the short term. In  particular, I believe that the biggest and most immediate  contribution will be in the support computational techniques can  provide for traditional kinds of analysis. I believe that the primary  contribution of the computational techniques will be in their  ability to provide a type of triangulation that helps us to establish  the validity of our analyses. This point is worth some elaboration.  When two humans code the same data in order to establish the  reliability of a coding procedure, they are, in a fundamental way,  doing the same thing. Thus, when two humans code, we have two  sets of measurements, both essentially performed with the same  type of experimental apparatus. In contrast, if we can find a way  to obtain confirmatory results, using a very different type of  apparatus, then that should more profoundly increase our  confidence in the validity of our results. It is that type of support   that I believe is the biggest potential contribution of the  computational techniques.    6.3 Open issues and next steps  A large number of problems remain unsolved, some of which I  have been careful to highlight, others which I have glossed over.  Here I will mention a few.    6.3.1 Additional subject matter  One obvious next step is to try some of these same analyses on  different interview data, about topics other than the seasons. It is  possible that there is something special about the seasons as  subject matter. For example, it might be that, in this territory, a  small number of key words (e.g., tilt) can do a lot of the work of  discriminating among explanations.    6.3.2 Systematic comparison to human analysis  My presentation in this manuscript was, in places, selective and  anecdotal. I did not discuss the segmented analysis of every  transcript; I just selected a few to give the flavor of the analysis  produced, and I relied on the readers intuition to judge whether  the automated and human analyses were in accord. Thats in  keeping with the exploratory approach adopted in this manuscript.  But, ultimately, I want to perform an analysis in which I  systematically compare the output of the automated analysis to  codes produced by human analysts. As mentioned earlier, Dam  and Kaufmann [28] performed such an analysis in which each  transcript was given a single code. However, their analysis did not  capture the dynamic features of interviews. An analysis focused  on a smaller grain size is in greater accord with the theoretical  perspective I have adopted, and I believe it will be possible to  obtain good agreement between human and automated codes.   6.3.3 Systematic investigation of alternative analysis  methods.  In Section 5, I very briefly talked about the results I obtained  when using different methods and parameters. In future work, I  want to extend this exploration of alternative analysis methods so  that it is both more deep and more broad. I believe that it is  important to have a more deep understanding of why some  methods work and others do not. And I want to look more broadly  and systematically at alternative techniques, including some that  begin to depart significantly from the methods discussed here,  including latent semantic analysis [9-11], probabilistic latent  semantic indexing [30], and latent Dirichlet allocation [31].    6.3.4 Why does this work  Finally, perhaps the greatest puzzle raised by this research is the  question of why these techniques work at all. Where is the magic  In my view, this question is almost, on its own, worthy of a  program of research. Are gestures and diagrams really so  unimportant to understanding the explanations given in interviews  of this sort Are a few key words enough to understand what  students are saying Why did the clustering analysis pick out  precisely the same set of categories as our human coders  Answering these questions may do more than tell us something  about this new class of methods, it might lead to a deeper  understanding of the very phenomena about thinking and learning  that were are seeking to study. Ultimately this question will need  to be a focus of future research.    7. ACKNOWLEDGMENTS  This work was funded in part by NSF grant #REC-0092648.   196    8. REFERENCES  [1] Duit, R.  2009. Bibliography: Students' and Teachers'   Conceptions and Science Education. Leibniz Institute for  Science Education, Kiel, Germany.   [2] Vosniadou, S. and Brewer, W. F. 1992. Mental models of the  earth: A study of conceptual change in childhood. Cognitive  Psychol., 24, 4 (Oct. 1992), 535-585.    [3] Samarapungavan, A. and Wiers, R. W. 1997. Children's  thoughts on the origin of species: A study of explanatory  coherence. Cognitive Sci., 21, 2 (Apr.-Jun. 1997), 147-177.    [4] Wellman, H. M. and Johnson, C. N. 1982. Children's  understanding of food and its functions: A preliminary study  of the development of concepts of nutrition. J. Appl. Dev.  Psychol., 3 135-148.    [5] McCloskey, M. 1983. Naive theories of motion. In Mental   Models, D. Gentner and A. Stevens eds. Erlbaum, Hillsdale,  NJ, 289-324.    [6] Sherin, B. 2001. How students understand physics equations.  Cognition Instruct., 19, 4 479-541.    [7] Smith, J. P., diSessa, A. A. and Roschelle, J. 1993.  Misconceptions Reconceived: A Constructivist Analysis of  Knowledge in Transition. J. Learn. Sci., 3, 2 115-163.    [8] Salton, G., Wong, A. and Yang, C. S.  1974. A vector space  model for automatic indexing. Dept. of Computer Science  Cornell University, Ithaca N Y.   [9] Deerwester, S., Dumais, S. T., Furnas, G., Landauer, T. and  Harshman, R. 1990. Indexing by latent semantic analysis. J.  Am. Soc. Inform. Sci., 41, 6 (Sep. 1990), 391-407.    [10] Landauer, T., Foltz, P. W. and Laham, D. 1998. An  introduction to latent semantic analysis. Discourse Process.,  25, 2-3 259-284.    [11] Berry, M. W., Dumais, S. T. and O'Brien, G. W. 1995. Using  linear algebra for intelligent information retrieval. Siam Rev.,  37, 4 (Dec. 1995), 573-595.    [12] Haley, D., Thomas, P., De Roeck, A. and Petre, M. 2005. A  Research Taxonomy for Latent Semantic Analysis-Based  Educational Applications. In Proceedings of the International  Conference on Recent Advances in Natural Language  Processing (Borovets, Bulgaria, 2005).   [13] Landauer, T., Laham, D. and Foltz, P. W. 2003. Automatic  essay assessment. Assessment in Education: Principles,  Policy and Practice, 10, 3 295-308.    [14] Shapiro, A. M. and McNamara, D. S. 2000. The Use of  Latent Semantic Analysis as a Tool for the Quantitative  Assessment of Understanding and Knowledge. J. Educ.  Comput. Res., 22, 1 1-36.    [15] Magliano, J. P. and Millis, K. K. 2003. Assessing reading  skill with a think-aloud procedure and latent semantic  analysis. Cognition Instruct., 21, 3 251-283.    [16] Millis, K., Kim, H.-J. J., Todaro, S., Magliano, J. P.,  Wiemer-Hastings, K. and McNamara, D. S. 2004.  Identifying reading strategies using latent semantic analysis:   Comparing semantic benchmarks. Behav. Res. Methods, 36,  2 213-221.    [17] Wade-Stein, D. and Kintsch, E. 2004. Summary Street:  Interactive computer support for writing. Cognition Instruct.,  22, 3 333-362.    [18] Wiemer-Hastings, P. and Graesser, A. C. 2000. Select-a- Kibitzer: A Computer Tool that Gives Meaningful Feedback  on Student Compositions. Interact. Learn. Envir., 8, 2 (Jan 1  2000), 149-169.    [19] Graesser, A. C., Lu, S., Jackson, G. T. and Mitchell, H. 2004.  AutoTutor: A tutor with dialogue in natural language. Behav.  Res. Methods, 36, 2 (May 2004), 180-192.    [20] Graesser, A. C., Wiemer-Hastings, P. and Wiemer-Hastings,  K. 2000. Using Latent Semantic Analysis to Evaluate the  Contributions of Students in AutoTutor. Interact. Learn.  Envir. (Jan 1 2000), 129-147.    [21] Sherin, B., Krakowski, M. and Lee, V. R. 2012. Some  assembly required: How scientific explanations are  constructed during clinical interviews. J. Res. Sci. Teach.,  49, 2 166-198.    [22] Atwood, R. K. and Atwood, V. A. 1996. Preservice  Elementary Teachers' Conceptions of the Causes of Seasons.  J. Res. Sci. Teach., 33, 5 (May 1996), 553-563.    [23] Newman, D. and Morrison, D. 1993. The conflict between  teaching and scientific sense-making: The case of a  curriculum on seasonal change. Interact. Learn. Envir., 3 1- 16.    [24] Sadler, P. M. 1987. Alternative conceptions in astronomy. In  Proceedings of the Second international seminar on  Misconception and Educational Strategies in Science and  Mathematics (Ithaca, NY, 1987). Cornell University Press.   [25] Trumper, R. 2001. A cross-college age study of science and  nonscience students' conceptions of basic astronomy. Journal  of Science Education and Technology, 10, 2 192-195.    [26] Lelliott, A. and Rollnick, M. 2010. Big Ideas: A Review of  Astronomy Education Research 1974-2008. Int. J. Sci. Educ.,  32, 13 1771-1799. Doi 10.1080/09500690903214546.   [27] Foltz, P. W., Kintsch, W. and Landauer, T. 1998. The  measurement of textual coherence with latent semantic  analysis. Discourse Process., 25, 2-3 285-307.    [28] Dam, G. and Kaufmann, S. 2008. Computer assessment of  interview data using latent semantic analysis. Behav. Res.  Methods., 40, 1 (Feb. 2008), 8-20. Doi 10.3758/Brm.40.1.8.   [29] Manning, C. D., Raghavan, P. and Schtze, H.  2008.  Introduction to information retrieval. Cambridge University  Press, New York.   [30] Hofmann, T. 2001. Unsupervised Learning by Probabilistic  Latent Semantic Analysis, 42, 1 177.    [31] Blei, D. M., Ng, A. Y. and Jordan, M. I. 2003. Latent  Dirichlet Allocation. J. Mach. Learn. Res., 3, 4/5 (May 15  2003), 993-1022.         197      "}
{"index":{"_id":"34"}}
{"datatype":"inproceedings","key":"Sherin:2012:UCM:2330601.2330649","author":"Sherin, Bruce","title":"Using Computational Methods to Discover Student Science Conceptions in Interview Data","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"188--197","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330649","doi":"10.1145/2330601.2330649","acmid":"2330649","publisher":"ACM","address":"New York, NY, USA","keywords":"conceptual change, learning analytics","Abstract":"A large body of research in the learning sciences has focused on students' commonsense science knowledge---the everyday knowledge of the natural world that is gained outside of formal instruction. Although researchers studying commonsense science have employed a variety of methods, one-on-one clinical interviews have played a unique and central role. The data that result from these interviews take the form of video recordings, which in turn are often compiled into written transcripts, and coded by human analysts. In my team's work on learning analytics, we draw on this same type of data, but we attempt to automate its analysis. In this paper, I describe the success we have had using extremely simple methods from computational linguistics---methods that are based on rudimentary vector space models and simple clustering algorithms. These automated analyses are employed in an exploratory mode, as a way to discover student conceptions in the data. The aims of this paper are primarily methodological in nature; I will attempt to show that it is possible to use techniques from computational linguistics to analyze data from commonsense science interviews. As a test bed, I draw on transcripts of a corpus of interviews in which 54 middle school students were asked to explain the seasons.","pdf":"Using computational methods to discover student science  conceptions in interview data   Bruce Sherin  Northwestern University   2120 Campus Drive  Evanston, IL USA  1-847-920-9987   bsherin@northwestern.edu          ABSTRACT  A large body of research in the learning sciences has focused on  students commonsense science knowledgethe everyday  knowledge of the natural world that is gained outside of formal  instruction. Although researchers studying commonsense science  have employed a variety of methods, one-on-one clinical  interviews have played a unique and central role. The data that  result from these interviews take the form of video recordings,  which in turn are often compiled into written transcripts, and  coded by human analysts. In my teams work on learning  analytics, we draw on this same type of data, but we attempt to  automate its analysis. In this paper, I describe the success we have  had using extremely simple methods from computational  linguisticsmethods that are based on rudimentary vector space  models and simple clustering algorithms. These automated  analyses are employed in an exploratory mode, as a way to  discover student conceptions in the data. The aims of this paper  are primarily methodological in nature; I will attempt to show that  it is possible to use techniques from computational linguistics to  analyze data from commonsense science interviews. As a test bed,  I draw on transcripts of a corpus of interviews in which 54 middle  school students were asked to explain the seasons.   Categories and Subject Descriptors  H.3.3. [Information search and retrieval]: clustering; I.2.7  [Natural Language Processing]: Text analysis; J.1  [Administrative Data Processing]: Education; K.3.1 [Computer  Uses in Education] Computer-assisted instruction (CAI)).   General Terms  Algorithms, Experimentation.   Keywords  Learning Analytics, Conceptual Change   1. INTRODUCTION  Much of the recent interest in learning analytics has been driven  by the great surge in the amount and kinds of data that are  available. This paper, in contrast, applies learning analytic   techniques to a type of data that has a long history, and that  predates recent technological advances. For the last few decades,  a large body of research in the learning sciences has focused on  students commonsense science knowledgethe everyday  knowledge of the natural world that is gained outside of formal  instruction. Although researchers studying commonsense science  knowledge have employed a variety of methods, one-on-one  clinical interviews have played a unique and central role. The data  that result from these interviews take the form of video  recordings, which in turn are often compiled into written  transcripts, and coded by human analysts.    In my teams work on learning analytics, we draw on this same  type of data, but we attempt to automate its analysis. In this paper,  I describe one part of this work. The automated analyses I present  here are not intended to code the data using categories developed  by human analysts. Rather, these analyses are employed in an  exploratory mode, as a way to discover student conceptions in the  data. Furthermore, my goal in this paper is not to contribute new  results to research on commonsense science. Rather, my aims are  primarily methodological in nature; I will attempt to show that it  is possible to use relatively simple techniques from computational  linguistics to analyze the type of data that is typically employed  by researchers in commonsense science. As a test bed, I draw on  transcripts of a corpus of interviews in which 54 middle school  students were asked to explain the seasons.   It should be emphasized that it is not at all obvious that it should  be possible to analyze data of this sort using simple computational  techniques. Unlike some other applications in learning analytics,  the total amount of data I have is relatively small. Furthermore,  the speech that occurs in commonsense science interviews can  pose particular difficulties for comprehension. Student utterances  are often halting and ambiguous. Furthermore, gestures can be  very important, and external artifacts such as drawings are  frequently referenced. However, our analysis algorithms only  have access to written transcripts of the words spoken by  participants.   Even with all of this complexity, my general approach is to go as  far as possible with simple methods, before proceeding to more  complex methods. Thus, the analyses I describe here make use of  extremely simple methods from computational linguistics methods that are based on rudimentary vector space models and  simple clustering algorithms.    2. LITERATURE REVIEW  2.1 Commonsense Science  It is now widely accepted that many of the key issues in science  instruction revolve around the prior conceptions of students. This  focus on commonsense science leads to a perspective in which the  central task of science instruction is understood as building on,      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   188    adapting, and, when necessary, replacing students prior  knowledge. One outcome of this focus has been the growth of a  veritable industry of research on students prior conceptions. The  bibliography compiled by Pfundt and Duit  [1], which lists  literature on the science conceptions of teachers and students,  provides one measure of the scale of this effort. As of early 2009,  the bibliography had over 8300 entries, spanning a wide range of  scientific disciplines, including, for example, what students  believe about the shape of the earth [2], evolution [3], and  nutrition [4].   In discussing the literature on commonsense science, it has  become commonplace to distinguish two theoretical poles. At one  extreme is the theory-theory perspective. According to this  perspective, commonsense science knowledge consists of  relatively well-elaborated theories [5]. At the other extreme, is the  knowledge-in-pieces (KiP) perspective. In this perspective, it is  assumed that: (a) commonsense science knowledge consists of a  moderately large number of elementsa systemof knowledge  and (b) the elements of the knowledge do not align in any simple  way with formal science domains [6, 7].   I believe that the computational methods described in this paper  should be of interest to a broad range of researchers who study  commonsense science, and adopt a range of theoretical  perspectives. However, the exploration of computational methods  presented in this paper was biased by my own theoretical  perspective, which lies closer to the KiP pole. As I hope will  become evident, my exploration of computational methods has  been driven by a desire to get at the more basic knowledgethe  piecesthat I believe comprise commonsense science knowledge.  And I have attempted to capture the dynamics that unfold as  students construct explanations during an interview.   2.2 Vector space models and their  applications in education research  Generally speaking, the goal of this work is to attempt to use  computational techniques in order to see student conceptions in  transcripts of commonsense science interviewers. There are many  techniques from computational linguistics that could be employed  in this way. The techniques I will use are based primarily on a  type of vector space model [8]. In vector space models, the  meaning of a block of texta word, paragraph, essay, etc.is  associated with a vector, usually in a high dimensional space. So,  two blocks of text have the same meaning to the extent that their  vectors are the same. In this way, a vector space analysis makes it  possible to compute the similarity in meaning between any pair of  words or blocks of text. In Section 4, I will describe, in some  detail, the algorithms employed in the particular analyses used in  this work.   One particular variant of vector space model, Latent Semantic  Analysis (LSA), has had increasing prominence across a range of  disciplines and applications [9-11]. LSA incorporates several  innovations that distinguish it from the most basic form of vector  space analysis; most centrally, it makes use of an auxiliary  training corpus that provides information about the wider contexts  in which terms appear, and it reduces the dimensionality of the  vector space, which has the effect of uncovering latent relations  among terms.   Vector space methods have seen increasing use in educational  research. These applications have been greatly dominated by uses  of LSA. In fact, outside of information retrieval, some of the  earliest and most persistent uses of LSA have been in applications  related to education [12]. These applications have been of two   broad types. First, LSA has been used as a research tool by  educational researchersthat is, as a means of analyzing data, in  order to study thinking and learning. Second, LSA has been used  as a component of intelligent instructional systems. The majority  of these educational applications, across both types, have been  focused on the teaching of reading and writing.  For example, LSA-based systems have been employed to  automatically score essays written by students [10, 13]. In a  number of applications, students are asked to summarize a  passage or document that they have just read, and an LSA-based  system is used to evaluate these summaries. In one such  application, Shapiro and McNamara [14] had students read and  summarize portions of psychology textbooks. Using LSA, these  summaries were then compared both to the text the students read,  and to model essays composed by experts. Similarly, Magliano  and colleagues conducted a wide range of studies in which LSA  was used to assess the strategies employed by readers and their  reading skill, more broadly [15, 16].   In many of these uses of LSA, the data consisted of written text  produced by participants in the research. However, in some  instances, LSA has been applied to transcriptions of verbal data.  For example, in their study mentioned above Shapiro and  McNamara [14] found that LSA could be applied successfully  both to written summaries of the textbook and to transcriptions of  verbal summaries given by students. Similarly, Magliano and  Millis [15] applied LSA to think-aloud protocols that students  produced as they read passages of text.   As mentioned above, LSA has been used as a component of  intelligent instructional systems. For example, intelligent systems  have been constructed that provide feedback to students on  summaries that they write of a given text passage [17, 18]. One  LSA-based system, AutoTutor, is of particular interest here  because it has been applied to teach science-related subject matter  [19, 20]. AutoTutor teaches physics by first posing a problem or  question. The student responds by typing a response into the  system. The system then evaluates that response by using LSA to  compare the students text to a set of predefined expectations and  misconceptions; the expectations are pre-specified components of  a correct response and the misconceptions are possible erroneous  ideas that might be expressed by the student. Based on this  analysis, the system responds by posing further questions to the  student, either to help correct the misconceptions, or to draw out  more components of a complete answer to the original problem.   I want to say a bit about where the work described in the present  paper fits within the space of uses of vector space models in  education. First, in this work, SNLP is used as an analytic tool for  researchers; I will not be describing an LSA-based system that is  used by students. Second, I apply my analyses to verbal data. As  mentioned above, many applications of vector space models in  education use text that is typed by a student, either in the form of  an essay or short responses. Furthermore, prior research that has  worked with verbal data has employed data that is very different  than that employed in the present work. For example, the work by  Shapiro and McNamara [14] and Magliano and colleagues [15,  16], which I mentioned above, employed a more constrained type  of think-aloud protocol, focused on passages of text that were just  read. In contrast, the verbal data employed in this work consists of  relatively free-flowing discussions involving back-and-forth  between an interviewer and interviewee.   Third, in all these applications, answers given by students,  whether in written or verbal form, were evaluated by comparison  to a predefined model. This model might be, for example, some   189    portion of the text just read, or an ideal answer constructed by the  researcher. In contrast, as mentioned above, I will describe  techniques for automatically inducing a set of conceptions from  the data itself.   Finally, I want to emphasize one other respect in which this work  differs from prior work in education that made use of LSA;  namely, I am not using LSA! As noted above, I believe it makes  sense to begin with simpler techniques, and then to pursue more  sophisticated methods as it seems necessary.   3. THE INTERVIEWS  3.1 Subject matter and interview design  The data used in this work was drawn from a larger corpus  collected by the NSF-funded Conceptual Dynamics Project  (CDP).1 For the present work, I draw from a set of 54 interviews  in which students were asked to explain Earths seasons [21].    The seasons have long been a popular subject of study in research  on commonsense science, and a significant number of studies  have set out to study student and adult understanding in this area  [22-26]. Our seasons interview always began with the interviewer  asking Why is it warmer in the summer and colder in the  winter After the student responded, the interviewer would, if  necessary, ask for elaboration or clarification. The interviewer had  the freedom, during this part of the interview, to craft questions  on-the-spot in order to clarify what the student was saying.   Next, the student was asked to draw a picture to illustrate their  explanation. Then, once again, the interviewer could ask follow- up questions for clarification. Our interviewers were also prepared  with a number of specific follow-up questions to be asked, as  appropriate, during this part of the interview. Some of these  questions were designed as challenges to specific explanations  that students might give.   3.2 Overview of student responses  In prior work with our seasons data, Conceptual Dynamics  researchers have adopted a strongly KiP perspective [21]. We  assume that students possess a system consisting of many  knowledge elementsthe piecesthat may potentially be  drawn upon as they endeavor to explain the seasons. When a  student is asked a question during an interview, some subset of  these elements are activated. The student then reasons based on  this set of elements, and works to construct an assemblage of  ideas in the service of explaining the seasons. We refer to this  assemblage of ideas as the dynamic mental construct or DMC, for  short. For the purpose of the present work, it is not a bad  approximation to think of a DMC as a students current working  explanation of the seasons. So, throughout this manuscript, I will  use the terms DMC and explanation interchangeably.   The explanations of the seasons given by the students we  interviewed varied along a number of dimensions. But it is  helpful, nonetheless, to begin with a number of reference points,  in the form of a few categories of explanations (DMCs). The first  category, closer-farther, is illustrated by the diagram in Figure 1a.  In closer-farther explanations, the earth is seen as orbiting (or  moving in some other manner) in such a way that it is sometimes  closer to the sun and sometimes farther. When the earth is closer  to the sun then it experiences summer; when its farther away it  experiences winter.                                                                        1 NSF grant #REC-0092648. Conceptual dynamics in complex  science interventions (B. Sherin, PI).   The second category of DMC, side-based, is illustrated in Figure  1b. Side-based explanations are usually focused on the rotational  motion of the earth, rather than its orbital motion. In side-based  explanations, the earth rotates so that first one side, then the other,  faces the sun. The side facing the sun at a given time experiences  summer, while the other side experiences winter.    (a)     (b)     (c)  Figure 1. Closer-farther, side-based, and tilt-based DMCS.   The third and final category of DMC, tilt-based, is depicted in  Figure 1c. Tilt-based DMCs depend critically on the fact that the  earths axis of rotation is tilted relative to a line connecting it to  the sun. In a tilt-based explanation, the hemisphere that is tilted  toward the sun experiences summer and the hemisphere that is  tilted away experiences winter. This category includes the  normative scientific explanation, as well as some non-normative  explanations.   As discussed in Sherin et al. [21], during an interview, students  tend to move among DMCs. In some cases, students do begin the  interview with what appears to be a fully-formed explanation. In  other cases, a student might construct an explanation during the  interview, slowly converging on an explanation they find  reasonable. Finally, students can be to seen to shift from one  DMC to another, sometimes in response to a challenge from the  interviewer.   3.3 Example interviews  Now I will briefly discuss a few example interviews. These  examples will play a role as important reference points when I  discuss the automated analysis. In this first example, a student,  Edgar, began by giving an explanation focused on the fact that the  Earth rotates, and he stated that light would hit more directly on  the side facing the sun. He made the drawing shown in Figure 2,  as he commented:   E: Heres the earth slanted. Heres the axis. Heres the North  Pole, South Pole, and heres our country. And the suns  right here [draws the circle on the left], and the rays hitting  like directly right here. So everythings getting hotter over  the summer and once this thing turns, the country will be  here and the sun can't reach as much. It's not as hot as the  winter.   After a brief follow up question by the interviewer, Edgar seemed  to recall that the Earth orbited the sun, in addition to rotating. He  then shifted to a closer-farther type explanation:   E Actually, I don't think this moves [indicates Earth on  drawing] it turns and it moves like that [gestures with a  pencil to show an orbiting and spinning Earth] and it turns  and that thing like is um further away once it orbit around  the s- Earth- I mean the sun.   190    I Its further away   E Yeah, and somehow like that going further off and I think  sun rays wouldnt reach as much to the earth.   Thus Edgars interview illustrates a case in which a student began  with a side-based explanation and transitioned to a closer-farther  explanation. It is also worth noting that Edgars language was  halting, imprecise, and made significant use of gestures and his  drawings. These are features that might well pose difficulties for  automated analysis.     Figure 2. Edgar's drawing.   I want to briefly introduce interviews with two other students  from the corpus, both of whom gave variants of tilt-based  explanations. The first example is from an interview with Caden.   I:  So the first question is why is it warmer in the summer and  colder in the winter   C:  Because at certain points of the earths rotation, orbit around  the sun, the axis is pointing at an angle, so that sometimes,  most times, sometimes on the northern half of the  hemisphere is closer to the sun than the southern  hemisphere, which, change changes the temperatures. And  then, as, as its pointing here, the northern hemisphere it  goes away, is further away from the sun and gets colder.   I:  Okay, so how does it, sometimes the northern hemisphere  is, is toward the sun and sometimes its away   C:  Yes because the atIm sorry, the earth is tilted on its axis.  And its always pointed towards one position.   Note that, in Cadens explanation, the tilt of the earth affects  temperature because the hemisphere tilted toward the sun is closer  to the sun, and the hemisphere tilted away is farther from the sun.  (This is not correct.) In contrast, another student, Zelda gave a tilt- based explanation, but her explanation made use of the fact that  the tilt of the earth causes rays to strike the surface more or less  directly, and this is what explains the seasons.   Z: Because, I think because the earth is on a tilt, and then, like  that side of the Earth is tilting toward the sun, or its facing  the sun or something so the sun shines more directly on that  area, so its warmer.   Thus, Caden and Zelda both gave tilt-based explanations, but they  differed in how exactly the tilt of the earth affected the seasons.  For Caden the tilting causes one hemisphere or the other to be  closer to the sun. For Zelda, the tilting causes parts of the earth to  receive the suns rays more or less directly. This illustrates some  of the types of features we would like the automated analysis to  resolve.    4. VECTOR SPACE ANALYSIS OF THE  SEASONS CORPUS  In order to captured students conceptions expressed in the  seasons interviews, my team explored the use of techniques from  statistical natural language processing. In particular, we explored  the use of vector space models, augmented with cluster analysis.  These choices make sense for a number of reasons. As mentioned  above, one type of vector space model, LSA, has already been  employed, with some success, in applications that are in some   respects close to my own [10, 14-17, 19, 27].   In addition, initial attempts by Gregory Dam and Stefan  Kaufmann to apply LSA to my research teams data proved  promising, and thus justified further exploration [28]. Dam and  Kaufmann employed techniques based on one variant of LSA to  apply a given coding scheme to an earlier subset of this corpus.    The work described in this manuscript extends the work of Dam  and Kaufmann in several respects. First, Dam and Kaufmanns  analysis did not discover student conceptions in the data corpus.  Instead, it began with the conceptions identified by human  analysts and used those conceptions to code transcript data.  Second, unlike Dam and Kaufmann, I will be exploring the use of  simpler vector space models, rather than LSA.    Third, Dam and Kaufmann were primarily concerned with coding  at the level of students. Each student was coded by the computer  in terms of just one of three possible explanations of the seasons.  The success of this analysis was judged by comparison to an  analysis of these same transcripts by human coders, restricted to  the same set of three explanations. However, this type of analysis  represented a drastic simplification over our earlier qualitative  analyses of the corpus. As exemplified in the description of  Edgars interview above, the explanations given by students over  the course of an interview were quite clearly dynamic. Thus,  assigning a single code to each manuscript was often a dramatic  simplification. In this new work, all of my analysis is done at a  finer time scale; I look to identify student ideas only in small  segments of text.   In the rest of this section, I describe an exploratory analysis of our  data. Here, I restrict myself to one pathway through the analysis,  using one set of parameters and algorithms. In Section 5, I briefly  describe the results I obtain when employing different parameters  and algorithms.   4.1 The basics: Converting text to vectors  The central idea underlying any vector space model of text  meaning is relatively simple: Every passage of textwhether it is  a word, sentence, or essayis mapped to a single vector. The  direction in which this vector points is taken to be a representation  of the meaning of the passage. More precisely, the similarity  between two passage vectors is quantified as the cosine of the  angle between the two vectors (or, equivalently, the dot product of  the vectors if we assume the vectors are of unit length).   Table 1. Partial vocabulary and sample counts   sun 4 2.1  earth 2 1.7  side 0 0  away 2 1.7  tilted 1 1  closer 1 1  axis 2 1.7  day 0 0  farther 1 1  time 3 2.1   The question, of course, is how we go about converting a passage  of text to a vector. In the most rudimentary forms of vector space  models, this mapping is accomplished in a rather straightforward  manner. First, we look across the entire corpus of text that we  wish to include in our analysis, and we compile a vocabulary, that  is, a complete list of all of the words that appear somewhere in the   191    corpus. This vocabulary is then pruned using a stop list of  words. This stop list consists primarily of a set of highly common  non-content words, such as the, of, and because. For the corpus  used in this work, this resulted in a vocabulary consisting of 647  words. (The stop list used contained 782 terms.) If the vocabulary  is sorted from the most common to least common words, the top  10 words correspond to the list shown in the left hand column of  Table 1.   This vocabulary can be used to compute a vector for a passage  from an interview transcript as follows. First, we take the  transcript and remove everything except the words spoken by the  student. Any portion of the remaining text can now be converted  to a vector. To do so, we go through the entire vocabulary,  counting how many times each word in the vocabulary appears in  the text being analyzed. When this is done, we get a list of 647  numbers. If, for example, we process the portion of Cadens  transcript presented above, we obtain the values listed in the  middle column of Table 1 for the 10 most common words in the  larger corpus.   Finally, in most vector space analyses, the raw counts are  modified by a weighting function. In the analyses reported on in  this section, I replaced each count with (1 + log(count)). This has  the effect of dampening the impact of very frequent words. (Raw  counts of 0 were just left as 0.) Appropriately weighted values are  shown in the third column of Table 1.   4.2 Using passage vectors to discover  meanings in the data corpus  We now have a means of mapping a passage of text to a vector  consisting of 647 numbers. This capability can now be used to  discover units of meaning that exist across the 54 interviews that  comprise my data corpus. This process involves four steps which I  will now discuss: (1) preparing and segmenting the corpus, (2)  mapping segments to vectors, (4) clustering the vectors, and (5)  interpreting the results.    4.2.1 Preparing and segmenting transcripts  First, as discussed above, the transcripts are reduced to that they  include only the words spoken by the student during the  interview. Next, recall that, in our earlier analyses of this corpus  conducted by my research team, we found that students could be  seen to construct explanations of the seasons out of large number  of knowledge resources, and that their explanations could shift as  an interview unfolded. We thus need a way to attach meanings to  small parts of an interview transcript. This requires a means of  segmenting a transcript into smaller parts.    In keeping with my goal of using simple methods, I segmented the  transcripts by breaking each transcript into 100-word segments. In  order to lessen problems that might be caused by the fact that this  introduces arbitrary boundaries, I chose to employ overlapping  100-word segments, with the start of each segment beginning 25  words after the start of the preceding segment. So the first  segment of a transcript would include words 1-100, the second  words 26-125, the third 51-150, etc. When all of the 54 interview  transcripts were segmented in this manner, I ended up with 794  segments of text. These specific choices for segment size and step  size are, of course, somewhat arbitrary. In Section 5, I will briefly  present results with different values of these parameters.   4.2.2 Mapping segments to vectors  The next step in the analysis is to map each of these 794 segments  to a vector. To accomplish this, I employ precisely the method   described above. The result is 794 vectors, each consisting of a  list of 647 numbers.   However, here I must introduce one complication. There is one  inherent problem with applying vector space models to an  analysis of this sort of data. Vector space models such as LSA  were originally developed as a means to find documents in a large  corpus that pertain to a given topic. They were thus not developed  for finding fine distinctions in meaning among documents  pertaining to very similar topics. However, all of the documents  involved in my analysis are about very similar subject matter; they  all explain the seasons, and they almost all do so by talking about  the position and motion of the earth in relation to the sun.   In fact, the clustering analysis (described in the next section) does  not produce meaningful results if I use the raw document vectors  that are produced by the method described above. (I will say more  about this problem in Section 5.) Instead, I need a means of  modifying the vectors so that they highlight their more unique  featuresthe features that, on average, tend to differentiate the  segment from the other 793 segments of text.   For that purpose, I compute what I call deviation vectors. To  compute the deviation vectors for two vectors V1 and V2, I first  find their average, and then break each vector into two  components, one that lies along the average, and another that is  perpendicular to the average (refer to Figure 3). The perpendicular  components, V1' and V2', are the deviation vectors. If we use  these deviation vectors in place of the original vectors, the result  is that V1 and V2, have each been replaced by the component that  defines its unique piece  a piece that characterizes how it differs  from the average.    The same procedure can be employed with any number of vectors.  For the next steps of the analysis, I replaced the 794 segment  vectors in just this way; I found their average, and then replaced  each vector with its deviation from this average.     Figure 3. How to compute deviation vectors.   4.2.3 Clustering the vectors  Now each of the 794 segments has been mapped to a vector that  we understand as representing the meaning of that segment. The  next step is to identify common meanings amongst these  segments. To do that, we look for natural clusterings of the 794  vectors.   To cluster the transcript vectors, I employed the very general  technique called hierarchical agglomerative clustering (HAC). In  HAC, we begin by taking all of the items to be clustered, and  placing each of these items in its own cluster. Thus, we begin with  a number of clusters equal to the total number of items. Then we  pick two of those clusters to combine into a single cluster  containing two items, thus reducing the total number of clusters  by one. The process then iterates; we again pick two clusters to  combine, and the total number of clusters is decreased by one.  This repeats until all of the items are combined into a single  cluster. The result is a list of candidate clusterings of the data,   192    with each candidate corresponding to one of the intermediate  steps in this process.   A central issue in applying this algorithm is determining which  clusters to combine on each iteration. In practice, there are many  rules that can be applied. Throughout my discussions here, I will  present results that were obtained using a technique called  centroid clustering. At each step in the iteration, I first find the  centroid of each cluster (the average of all of the vectors currently  in the cluster). Then I find the pair of centroids that are closest to  each other, and merge the associated clusters. An explanation of  centroid clustering, including its application to vector space  models, can be found in [29].   4.2.4 Determining the number of clusters  The result of the clustering analysis can be thought of as a table  with 794 rows. At the top is a row in which each segment is in a  single cluster. At the bottom is a row in which all of the segments  are in a single cluster. Table 2 displays the results for just a part of  this large table. The bottom row, for example, shows the results  when the segments are grouped into three clusters that contain 271  segments, 279 segments, and 244 segments respectively. As you  move up the table the number of clusters grows, and the size of  each cluster shrinks. In each row of Table 2, clusters contain  segments that have been grouped together because, from the point  of view of our vector space model, they have similar meanings.  This means that each row in Table 2, constitutes a candidate  coding schemeit is a scheme for sorting segments into  categories. The puzzle, of course, is which row to select.    Table 2. Sizes of clusters for selected clusterings   10 19 72 9 68 140 62 44 122 136 122   9 19 72 68 62 44 122 136 122 149   8 19 72 68 44 122 136 122 211   7 72 68 44 122 122 211 155   6 68 44 122 122 211 227   5 68 122 122 211 271   4 122 122 271 279   3 271 279 244    Unfortunately, there is no simple answer to this question. In  general, there is a tradeoff. When the number of clusters is high,  we obtain a better fit to the data. However, we get this better fit at  the expense of a more complex model. Because each cluster is  described by a list of 647 values, each additional cluster  represents a dramatic increase in model complexity.2   Here, as elsewhere, I make my choice in a heuristic manner.  Across multiple analyses, I have found that working with a set of  about 7 clusters strikes a workable balance. With 7 clusters, it is  possible to resolve interesting features of the data, while  producing results (in the form of graphs) that are not overly  difficult to interpret.   4.2.5 What do the clusters mean  We now have grouped the 794 segments into 7 clusters, each                                                                        2 For this reason, if I use traditional measures for determining the   appropriate number of clusters (e.g., Bayesian information  criterion or Akaike information criterion), the terms  corresponding to the model complexity always dominate, and  the model with the smallest number of clusters prevails.   containing between 44 and 211 segments (refer to Table 2). The  next question we must answer is: What do these clusters mean  Each of the 7 clusters can be thought of as defined by its centroid  vectorthe average of all of the vectors that comprise the cluster.  These centroids each, in turn, are described by a list of 647  entries, each of which corresponds to one of the words in the  vocabulary. One way to attempt to understand the meaning of the  clusters, then, is to look at the words that have the largest value in  each centroid vector.    When this is done I obtain the results shown in Figure 4. For each  cluster, I list the 10 words that are most strongly associated with  that cluster, ignoring words that appeared less than 30 times in the  overall corpus. In addition, the second column in each table has  the value from the centroid vector corresponding to this word. The  third column in each table lists the total number of times that the  word appears across the entire corpus.    4.2.6 Interpreting the clusters based on the word lists  In many respects, the lists of words shown in Figure 4 clusters are  suggestive. First, several of the clusters seem to align with the  three broad classes of seasons explanations listed in Section 3. For  example, it seems natural to associate Cluster 1, which starts with  the words tilted, towards, and away, with tilt-based explanations.  Similarly, it seems natural to associate Cluster 4 (side, facing)  with side-based explanations, and Cluster 7 (farther, closer) with  closer-farther explanations of the seasons.   Cluster 1Cluster 1Cluster 1  tilted 0.767 82 towards 0.199 40 away 0.186 83 north 0.098 30 part 0.084 46 guess 0.077 31 closer 0.044 82 warmer 0.042 40 sun 0.03 545 farther 0.017 71  Cluster 2Cluster 2Cluster 2  earth 0.4 395 spinning 0.366 37 spins 0.2 38 time 0.198 65 axis 0.121 77 seasons 0.068 30 tilted 0.031 82 angle 0.017 31 north 0.014 30 chicago 0.006 45  Cluster 3Cluster 3Cluster 3  hemisphere 0.603 47 northern 0.522 31 colder 0.119 52 facing 0.106 46 closer 0.043 82 farther 0.035 71 warmer 0.023 40 axis 0.021 77 away 0.02 83 rays 0.018 33  Cluster 4Cluster 4Cluster 4  side 0.722 95 facing 0.091 46 earth 0.085 395 part 0.068 46 chicago 0.018 45 guess 0.008 31 seasons -0.008 30 time -0.01 65 heat -0.025 30 rotates -0.026 54  Cluster 5Cluster 5Cluster 5  rays 0.293 33 north 0.197 30 angle 0.194 31 light 0.188 41 chicago 0.163 45 sun 0.134 545 heat 0.076 30 towards 0.045 40 warmer 0.02 40 side 0.019 95  Cluster 6Cluster 6Cluster 6  day 0.415 75 moon 0.398 52 night 0.377 63 rotates 0.178 54 rotating 0.068 32 earth 0.055 395 spins 0.05 38 facing 0.048 46 light 0.046 41 seasons 0.046 30  Cluster 7Cluster 7Cluster 7  farther 0.413 71 closer 0.403 82 away 0.379 83 colder 0.216 52 sun 0.103 545 warmer 0.064 40 rotates 0.033 54 time 0.028 65 heat 0.02 30 rotating 0.013 32    Figure 4. Top words associated with each cluster.  But these clusters are not supposed to necessarily align with full- fledged explanations of the seasons. They are clusters of  segments, which it is hoped can align with smaller conceptual  units that, when combined, form the basis of a constructed   193    explanation. And, indeed, the additional clusters do seem to offer  the possibility of an analysis of that sort. For example, we should  expect tilt-based explanations to often be seen in concert with talk  about the Earths hemispheres (Cluster 3). And recall that tilt- based explanations invoke different mechanisms by which the  changing tilt of the earth impacts temperature. For example,  Caden argued that the tilting of the Earth causes parts of the earth  to be alternately closer or farther from the sun. In contrast, Zeldas  explanation focused on the impact of the Earths tilt and how it  impacts the angle and directness of the suns rays. We should thus  be able to see these ideas in combination, when we look at  individual interviews.   Similarly, we should expect to see side-based explanations  (Cluster 4) in tandem with clusters having to do with the rotation  of the Earth. Ideas about the rotation of the Earth seem to appear  in Cluster 2 and Cluster 6. Cluster 2 seems to truly be focused on  the spinning of the Earth. Cluster 6, in contrast, seems to be more  about day and night. Not surprisingly, talk about the rotation of  the Earth was often combined with talk about the day/night cycle.    4.3 Application to segmented transcripts  The clusters shown in Figure 4 thus seem to have reasonable  interpretations in terms of our understanding of the data corpus.  We have thus identified a set of common underlying ideas. A  next step I can take is to apply this set of ideas back to the original  transcripts. I want to use these ideasthese units of meaningto  interpret individual student interviews.   In order to accomplish this, I begin by preparing each of the  interview transcripts precisely as before; the transcripts are  reduced so that they include only the words spoken by a student,  then they are broken into 100-word segments using a moving  window that steps forward by 25 words. Next I compute the  vector for each of these segments, again using the same  techniques described earlier. Finally, each of these vectors for the  segments is compared to the 7 centroid vectors corresponding to  the 7 clusters (by taking the cosine of the angle between the  vectors and each centroid).   I begin my discussion of the results with Zelda, since her analysis  produces a graphic that is relatively easy to read. In Figure 5 we  see that Zeldas transcript has been broken into 5 overlapping  segments. Each of these segments is associated with 7 bars, one  bar for each of the 7 clusters. For all of the segments, Cluster 1 is  the clear winner. This makes sense since, as discussed earlier,  Zelda gave an answer that was very close to the accepted  scientific explanation of the seasons. Note, also, that the bar for  Cluster 5 is slightly elevated in three of the segments. Cluster 5  had to do with rays striking the earth at an angle. Again, this  makes sense given what we can read in Zeldas transcript.     Figure 5. Segmenting analysis of Zelda's transcript.   The interview with Caden provides an interesting contrast. When  Cadens transcript is analyzed using the segment centroids, we get  8 segments with the bars shown in Figure 6. Like Zelda, we  understood Caden as giving an explanation that emphasized the  tilt of the Earth. But, in Figure 6, we see that Cluster 3  dominatesthe cluster having to do with the Earths hemispheres   although there are hints of Cluster 1 (tilted-toward) in the  earlier segments. The predominance of Cluster 3 is not too  surprising. As I noted earlier, tilt-based explanations should be  closely associated with discussion of the two hemispheres of the  earth. Indeed, glancing at the portion of Cadens transcript  presented earlier, there is an emphasis on the different effects on  the northern and southern hemispheres.     Figure 6. Segmenting analysis of Caden's transcript.   Both Zelda and Caden were relatively stable in the explanations  that they gave. We would now like to see if this analysis can  capture shifts that occur as an interview unfolds. To see this, we  can now return to the interview with Edgar. Looking at Figure 7 it  seems clear that the interview has a two major parts. The first part  is dominated by Cluster 5, which has to do with rays striking the  Earths surface. The latter part is strongly dominated by Cluster 7,  which is the closer-farther cluster. Thus, once again, it seems  possible to interpret the automated analysis in a manner that is  consistent with our qualitative analysis of the interview.     Figure 7. Segmenting analysis of Edgars transcript   5. Alternative analysis methods  To this point, my exploration has been limited in a particular way;  I have looked at some of the results produced by my analysis, but  all of these results were produced by a single set of algorithms,  and with a single set of input parameters. In particular, my  analysis followed the following plan: (1) The transcripts were  pruned so that they only contained the words spoken by the  interviewee; (2) the resulting documents were broken into 100- word segments, with a step size of 25 words; (3) a vector was  computed for each segment, using a weight function of (1 +  log(count)), and ignoring words in my stop list; (4) the resulting  vectors were replaced with their deviation vectors; and (5) the  vectors were clustered using hierarchical agglomerative  clustering. These choices of algorithms and parameters were  chosen, in part, because they produced interpretable results. In this  section, I want to briefly give a sense for the results produced by  alternative approaches, including some that did not produce  interpretable results.   5.1 Alternative parameters  There are many ways in which the above analysis could be  altered, while still employing a method that is very similar in  outline. For example, the composition of the stop list could be  changed, and the transcripts could be pruned in a different  manner. For example, in pruning the transcripts, I needed to  decide what to do about word fragments, whether to leave them,  delete them, or complete them. More dramatically, I could have   194    opted to stem words, that is, to reduce them to their base or root.  For the most part, these smaller changes produced similar  interpretable results across a large range of variations.    For illustration, I will present the results obtained if the transcripts  are segmented into 50 word segments, with a step size of 10  words (rather than 100 and 25). When this is done, I end up with  2320 segments. When these segments are clustered, I obtain the  results shown in Table 4. Like Table 3, this table shows the sizes  of the clusters that are produced during some of the latter steps in  the clustering. Note that when the segments are grouped into 7  clusters, one of the clusters contains only 1 segment. For that  reason, it makes to sense to look at the next row in the table,  where the segments are grouped into 6 clusters. Figure 8 shows  the word lists associated with these clusters, which were produced  in the same method used to produce the lists in Figure 4.   Table 3. Sizes of clusters for selected clusterings   10 1 78 88 160 156 11 628 235 137 638   9 1 88 160 156 11 628 235 638 215   8 1 160 11 628 235 638 215 244   7 1 160 235 638 215 244 639   6 160 235 638 215 639 245  5 235 638 215 639 405   4 638 639 405 450  3 638 450 1044    Now we can compare the lists in Figures 8 and 4. Although there  are many differences, it is not difficult to discern an alignment.  Clusters 1, 3, 4, and 5 in the new analysis seem to be similar to the  corresponding clusters in the original analysis. Cluster 6 seems to  be similar to Cluster 7 in the original analysis. Finally, Cluster 2  in the new analysis is similar to both Cluster 2 and Cluster 6 in the  original analysis. (Note that it makes sense for Clusters 2 and 6 in  the original analysis to be grouped together since they both pertain  to the rotation of the Earth.) Thus, while there are certainly  differences, the qualitative picture produced by this new analysis  seems to bear a close resemblance to the original analysis.   Cluster 1Cluster 1Cluster 1  tilted 0.718 82 north 0.353 30 towards 0.185 40 part 0.106 46 away 0.076 83 guess 0.043 31 warmer 0.022 40 angle 0.015 31 chicago 0.0 45 hemispher e  -0.009 47  Cluster 2Cluster 2Cluster 2  earth 0.522 395 moon 0.298 52 day 0.218 75 rotates 0.206 54 night 0.197 63 axis 0.164 77 spinning 0.16 37 spins 0.1 38 rotating 0.069 32 time 0.061 65  Cluster 3Cluster 3Cluster 3  hemisphere 0.618 47 northern 0.433 31 facing 0.312 46 colder 0.096 52 part 0.033 46 rotating 0.029 32 light 0.018 41 away 0.015 83 axis 0.011 77 towards 0.008 40  Cluster 4Cluster 4Cluster 4  side 0.849 95 earth 0.048 395 seasons 0.023 30 facing 0.015 46 warmer 0.013 40 rotates 0.007 54 guess 0.004 31 chicago -0.013 45 part -0.016 46 northern -0.017 31  Cluster 5Cluster 5Cluster 5  chicago 0.6 45 light 0.35 41 rays 0.315 33 heat 0.113 30 time 0.071 65 towards 0.045 40 facing 0.041 46 sun 0.037 545 warmer 0.022 40 seasons 0.014 30  Cluster 6Cluster 6Cluster 6  sun 0.431 545 closer 0.305 82 farther 0.261 71 away 0.208 83 colder 0.09 52 angle 0.059 31 heat 0.038 30 warmer 0.014 40 guess -0.001 31 rays -0.024 33    Figure 8. Top words associated with each cluster.   5.2 Alternative algorithms  Of course, it is possible to make much more substantial changes to  the analysis presented in Section 4. Here I will consider changes  related to one unusual feature of my analysis, the use of deviation  vectors. Recall that I introduced the deviation vectors as a means  of addressing the fact that there was substantial overlap in the  vectors that are produced by my initial computation of document  vectors.    Table 4 shows the results that are produced when I do not  compute deviation vectors before clustering the segments. Note  that in each of the candidate clusterings shown, I obtain one very  large cluster, containing most of the segments, and several very  small clusters. When I look at earlier stages of the clustering, I see  the following behavior: initially the segments are all clustered into  a large number of relatively small changes, then the these small  clusters begin to agglomerate, one at a time, onto one large  cluster. In the final stages, which we see in Table 4, some small  remaining clusters are swept up into the large cluster. In short, this  analysis does not seem to discover a small number of moderately- sized clusters that we can associate with conceptions.   Table 4. Sizes of clusters for selected clusterings   10 2 2 3 4 3 4 9 6 11 750   9 2 2 3 4 3 4 6 11 759   8 2 2 3 4 3 4 6 770   7 2 2 3 3 4 6 774   6 2 3 3 4 6 776   5 2 3 3 4 782   4 2 3 4 785   3 3 4 787    The question remains as to whether there are other more standard  approaches that might improve on the results shown in Table 4. In  particular, it is standard practice to use judiciously-chosen  weighting functions as a means of accentuating the differences  among documents. Recall that the counts in my vectors were all  weighted by (1 + log(count)), where count is the number of  times a word appears in a given document. We can modify this  function so that it weights words that appear across many  documents less strongly than words that appear in in just a few  documents. I tried several such weighting functions, including  variants of the so-called tf-idf weighting. In all cases, I obtained  results that looked like Table 4.   6. Discussion  6.1 Summary  I began this paper with the observation that research on  commonsense science knowledge typically focuses on data  derived from one-on-one clinical interviews. To date, researchers  in this field have generally used humans as instruments for  analyzing this data. I believe that we have done so because of  some tacitly-held beliefs: we have tended to assume that, to make  sense of clinical interview data, it is necessary to have an  instrument with an ability to understand natural language. We  have also assumed that it is necessary to have access to as much  of the interaction as possible. We need not just the words spoken;  we also need gestures, facial expressions, drawings, etc. It also  seems to require the ability to make leaps that look across the  breadth of a data corpus.  The task of analysis is also, in some respects, complicated by the   195    theoretical position I adopted in this work. I believe that, in many  cases, it is simply not possible to understand a student as  expressing a single model of the seasons. Instead, students  construct and shift explanationsDMCsas the interview  unfolds. I want to capture this movement in explanations.    Nonetheless, this work set out to explore how much can be  accomplished with a relatively simple suite of techniques from  statistical natural language processing. Stated crudely, the  statistical techniques rely primarily on counting words.  Furthermore, from among the bag of words models that are  employed by linguists, I chose to begin with one of the simplest  possible models.   In short, there was every reason to think that the types of analysis  described here would not be very successful. Nonetheless, these  results are at least suggestive that these simple techniques can  give meaningful results. The clustering algorithm produced a set  of clusters that seemed to have meaningful interpretations interpretations that made sense given earlier qualitative analyses  of the same corpus. And, when these clusters were employed to  produce a segmented analysis of individual transcripts, they  produced a narrative analysis of the transcript that aligned with  the descriptions produced by human analysts.   That said, most of my presentation in this manuscript has had an  exploratory character. My goal has been only to begin to map the  boundaries of what might be possible with a family of relatively  simple computational techniques. In this final section of the paper,  I reflect on what we can conclude, and I discuss caveats.   6.2 What do these computational techniques  buy us  What role might these computational techniques play in the  toolkit of researchers, especially researchers who use clinical  interviews to study the conceptions of science students I  presented many results that were intriguing, but my results were  only intended to be about the methods themselves; I didnt use the  methods in the service of any scientific agenda. So what, in the  long term, might these techniques buy us   One question to ask is whether computational techniques can and  should replace human analysts, or at least reduce the work  required. Whether or not this may ultimately be possible, I should  be clear that the analyses presented in this paper were still highly  dependent on human interpretation. For example, I had to make a  judgment about the appropriate number of clusters to work with.  Even more importantly, the analysis required me to make sense of  the lists of words that were associated with each of the clusters.  Nonetheless, the computational techniques described here might  play a useful role in our toolkits, even in the short term. In  particular, I believe that the biggest and most immediate  contribution will be in the support computational techniques can  provide for traditional kinds of analysis. I believe that the primary  contribution of the computational techniques will be in their  ability to provide a type of triangulation that helps us to establish  the validity of our analyses. This point is worth some elaboration.  When two humans code the same data in order to establish the  reliability of a coding procedure, they are, in a fundamental way,  doing the same thing. Thus, when two humans code, we have two  sets of measurements, both essentially performed with the same  type of experimental apparatus. In contrast, if we can find a way  to obtain confirmatory results, using a very different type of  apparatus, then that should more profoundly increase our  confidence in the validity of our results. It is that type of support   that I believe is the biggest potential contribution of the  computational techniques.    6.3 Open issues and next steps  A large number of problems remain unsolved, some of which I  have been careful to highlight, others which I have glossed over.  Here I will mention a few.    6.3.1 Additional subject matter  One obvious next step is to try some of these same analyses on  different interview data, about topics other than the seasons. It is  possible that there is something special about the seasons as  subject matter. For example, it might be that, in this territory, a  small number of key words (e.g., tilt) can do a lot of the work of  discriminating among explanations.    6.3.2 Systematic comparison to human analysis  My presentation in this manuscript was, in places, selective and  anecdotal. I did not discuss the segmented analysis of every  transcript; I just selected a few to give the flavor of the analysis  produced, and I relied on the readers intuition to judge whether  the automated and human analyses were in accord. Thats in  keeping with the exploratory approach adopted in this manuscript.  But, ultimately, I want to perform an analysis in which I  systematically compare the output of the automated analysis to  codes produced by human analysts. As mentioned earlier, Dam  and Kaufmann [28] performed such an analysis in which each  transcript was given a single code. However, their analysis did not  capture the dynamic features of interviews. An analysis focused  on a smaller grain size is in greater accord with the theoretical  perspective I have adopted, and I believe it will be possible to  obtain good agreement between human and automated codes.   6.3.3 Systematic investigation of alternative analysis  methods.  In Section 5, I very briefly talked about the results I obtained  when using different methods and parameters. In future work, I  want to extend this exploration of alternative analysis methods so  that it is both more deep and more broad. I believe that it is  important to have a more deep understanding of why some  methods work and others do not. And I want to look more broadly  and systematically at alternative techniques, including some that  begin to depart significantly from the methods discussed here,  including latent semantic analysis [9-11], probabilistic latent  semantic indexing [30], and latent Dirichlet allocation [31].    6.3.4 Why does this work  Finally, perhaps the greatest puzzle raised by this research is the  question of why these techniques work at all. Where is the magic  In my view, this question is almost, on its own, worthy of a  program of research. Are gestures and diagrams really so  unimportant to understanding the explanations given in interviews  of this sort Are a few key words enough to understand what  students are saying Why did the clustering analysis pick out  precisely the same set of categories as our human coders  Answering these questions may do more than tell us something  about this new class of methods, it might lead to a deeper  understanding of the very phenomena about thinking and learning  that were are seeking to study. Ultimately this question will need  to be a focus of future research.    7. ACKNOWLEDGMENTS  This work was funded in part by NSF grant #REC-0092648.   196    8. REFERENCES  [1] Duit, R.  2009. Bibliography: Students' and Teachers'   Conceptions and Science Education. Leibniz Institute for  Science Education, Kiel, Germany.   [2] Vosniadou, S. and Brewer, W. F. 1992. Mental models of the  earth: A study of conceptual change in childhood. Cognitive  Psychol., 24, 4 (Oct. 1992), 535-585.    [3] Samarapungavan, A. and Wiers, R. W. 1997. Children's  thoughts on the origin of species: A study of explanatory  coherence. Cognitive Sci., 21, 2 (Apr.-Jun. 1997), 147-177.    [4] Wellman, H. M. and Johnson, C. N. 1982. Children's  understanding of food and its functions: A preliminary study  of the development of concepts of nutrition. J. Appl. Dev.  Psychol., 3 135-148.    [5] McCloskey, M. 1983. Naive theories of motion. In Mental   Models, D. Gentner and A. Stevens eds. Erlbaum, Hillsdale,  NJ, 289-324.    [6] Sherin, B. 2001. How students understand physics equations.  Cognition Instruct., 19, 4 479-541.    [7] Smith, J. P., diSessa, A. A. and Roschelle, J. 1993.  Misconceptions Reconceived: A Constructivist Analysis of  Knowledge in Transition. J. Learn. Sci., 3, 2 115-163.    [8] Salton, G., Wong, A. and Yang, C. S.  1974. A vector space  model for automatic indexing. Dept. of Computer Science  Cornell University, Ithaca N Y.   [9] Deerwester, S., Dumais, S. T., Furnas, G., Landauer, T. and  Harshman, R. 1990. Indexing by latent semantic analysis. J.  Am. Soc. Inform. Sci., 41, 6 (Sep. 1990), 391-407.    [10] Landauer, T., Foltz, P. W. and Laham, D. 1998. An  introduction to latent semantic analysis. Discourse Process.,  25, 2-3 259-284.    [11] Berry, M. W., Dumais, S. T. and O'Brien, G. W. 1995. Using  linear algebra for intelligent information retrieval. Siam Rev.,  37, 4 (Dec. 1995), 573-595.    [12] Haley, D., Thomas, P., De Roeck, A. and Petre, M. 2005. A  Research Taxonomy for Latent Semantic Analysis-Based  Educational Applications. In Proceedings of the International  Conference on Recent Advances in Natural Language  Processing (Borovets, Bulgaria, 2005).   [13] Landauer, T., Laham, D. and Foltz, P. W. 2003. Automatic  essay assessment. Assessment in Education: Principles,  Policy and Practice, 10, 3 295-308.    [14] Shapiro, A. M. and McNamara, D. S. 2000. The Use of  Latent Semantic Analysis as a Tool for the Quantitative  Assessment of Understanding and Knowledge. J. Educ.  Comput. Res., 22, 1 1-36.    [15] Magliano, J. P. and Millis, K. K. 2003. Assessing reading  skill with a think-aloud procedure and latent semantic  analysis. Cognition Instruct., 21, 3 251-283.    [16] Millis, K., Kim, H.-J. J., Todaro, S., Magliano, J. P.,  Wiemer-Hastings, K. and McNamara, D. S. 2004.  Identifying reading strategies using latent semantic analysis:   Comparing semantic benchmarks. Behav. Res. Methods, 36,  2 213-221.    [17] Wade-Stein, D. and Kintsch, E. 2004. Summary Street:  Interactive computer support for writing. Cognition Instruct.,  22, 3 333-362.    [18] Wiemer-Hastings, P. and Graesser, A. C. 2000. Select-a- Kibitzer: A Computer Tool that Gives Meaningful Feedback  on Student Compositions. Interact. Learn. Envir., 8, 2 (Jan 1  2000), 149-169.    [19] Graesser, A. C., Lu, S., Jackson, G. T. and Mitchell, H. 2004.  AutoTutor: A tutor with dialogue in natural language. Behav.  Res. Methods, 36, 2 (May 2004), 180-192.    [20] Graesser, A. C., Wiemer-Hastings, P. and Wiemer-Hastings,  K. 2000. Using Latent Semantic Analysis to Evaluate the  Contributions of Students in AutoTutor. Interact. Learn.  Envir. (Jan 1 2000), 129-147.    [21] Sherin, B., Krakowski, M. and Lee, V. R. 2012. Some  assembly required: How scientific explanations are  constructed during clinical interviews. J. Res. Sci. Teach.,  49, 2 166-198.    [22] Atwood, R. K. and Atwood, V. A. 1996. Preservice  Elementary Teachers' Conceptions of the Causes of Seasons.  J. Res. Sci. Teach., 33, 5 (May 1996), 553-563.    [23] Newman, D. and Morrison, D. 1993. The conflict between  teaching and scientific sense-making: The case of a  curriculum on seasonal change. Interact. Learn. Envir., 3 1- 16.    [24] Sadler, P. M. 1987. Alternative conceptions in astronomy. In  Proceedings of the Second international seminar on  Misconception and Educational Strategies in Science and  Mathematics (Ithaca, NY, 1987). Cornell University Press.   [25] Trumper, R. 2001. A cross-college age study of science and  nonscience students' conceptions of basic astronomy. Journal  of Science Education and Technology, 10, 2 192-195.    [26] Lelliott, A. and Rollnick, M. 2010. Big Ideas: A Review of  Astronomy Education Research 1974-2008. Int. J. Sci. Educ.,  32, 13 1771-1799. Doi 10.1080/09500690903214546.   [27] Foltz, P. W., Kintsch, W. and Landauer, T. 1998. The  measurement of textual coherence with latent semantic  analysis. Discourse Process., 25, 2-3 285-307.    [28] Dam, G. and Kaufmann, S. 2008. Computer assessment of  interview data using latent semantic analysis. Behav. Res.  Methods., 40, 1 (Feb. 2008), 8-20. Doi 10.3758/Brm.40.1.8.   [29] Manning, C. D., Raghavan, P. and Schtze, H.  2008.  Introduction to information retrieval. Cambridge University  Press, New York.   [30] Hofmann, T. 2001. Unsupervised Learning by Probabilistic  Latent Semantic Analysis, 42, 1 177.    [31] Blei, D. M., Ng, A. Y. and Jordan, M. I. 2003. Latent  Dirichlet Allocation. J. Mach. Learn. Res., 3, 4/5 (May 15  2003), 993-1022.         197      "}
{"index":{"_id":"35"}}
{"datatype":"inproceedings","key":"Ammari:2012:DGP:2330601.2330650","author":"Ammari, Ahmad and Lau, Lydia and Dimitrova, Vania","title":"Deriving Group Profiles from Social Media to Facilitate the Design of Simulated Environments for Learning","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"198--207","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330650","doi":"10.1145/2330601.2330650","acmid":"2330650","publisher":"ACM","address":"New York, NY, USA","keywords":"augmented user models, data mining for learning, learning analytics, learning needs, mining social media, social profiles","Abstract":"Simulated environments for learning are becoming increasingly popular to support experiential learning in complex domains. A key challenge when designing simulated learning environments is how to align the experience in the simulated world with real world experiences. Social media resources provide user-generated content that is rich in digital traces of real world experiences. People comments, tweets, and blog posts in social spaces can reveal interesting aspects of real world situations or can show what particular group of users is interested in or aware of. This paper examines a systematic way to analyze user-generated content in social media resources to provide useful information for learning simulator design. A hybrid framework exploiting Machine Learning and Semantics for social group profiling is presented. The framework has five stages: (1) Retrieval of user-generated content from the social resource (2) Content noise filtration, removing spam, abuse, and content irrelevant to the learning domain; (3) Deriving individual social profiles for the content authors; (4) Clustering of individuals into groups of similar authors; and (5) Deriving group profiles, where interesting concepts suitable for the use in simulated learning systems are extracted from the aggregated content authored by each group. The framework is applied to derive group profiles by mining user comments on YouTube videos. The application is evaluated in an experimental study within the context of learning interpersonal skills in job interviews. The paper discusses how the YouTube-based group profiles can be used to facilitate the design of a job interview skills learning simulator, considering: (1) identifying learning needs based on digital traces of real world experiences; and (2) augmenting learner models in simulators based on group characteristics derived from social media.","pdf":"Deriving Group Profiles from Social Media to Facilitate the  Design of Simulated Environments for Learning   Ahmad Ammari  University of Leeds  Leeds LS2 9JT, UK  +44 113 3431116   a.ammari@leeds.ac.uk  Lydia Lau  University of Leeds  Leeds LS2 9JT, UK  +44 113 3435454   l.m.s.lau@leeds.ac.uk  Vania Dimitrova  University of Leeds  Leeds LS2 9JT, UK  +44 113 3431674   v.g.dimitrova@leeds.ac.uk  ABSTRACT Simulated environments for learning are becoming increasingly  popular to support experiential learning in complex domains. A  key challenge when designing simulated learning environments is  how to align the experience in the simulated world with real  world experiences. Social media resources provide user-generated  content that is rich in digital traces of real world experiences.  People comments, tweets, and blog posts in social spaces can  reveal interesting aspects of real world situations or can show  what particular group of users is interested in or aware of. This  paper examines a systematic way to analyze user-generated  content in social media resources to provide useful information  for learning simulator design. A hybrid framework exploiting  Machine Learning and Semantics for social group profiling is  presented. The framework has five stages: (1) Retrieval of user- generated content from the social resource (2) Content noise  filtration, removing spam, abuse, and content irrelevant to the  learning domain; (3) Deriving individual social profiles for the  content authors; (4)  Clustering of individuals into groups of  similar authors; and (5) Deriving group profiles, where interesting  concepts suitable for the use in simulated learning systems are  extracted from the aggregated content authored by each group.  The framework is applied to derive group profiles by mining user  comments on YouTube videos. The application is evaluated in an  experimental study within the context of learning interpersonal  skills in job interviews. The paper discusses how the YouTube- based group profiles can be used to facilitate the design of a job  interview skills learning simulator, considering: (1) identifying  learning needs based on digital traces of real world experiences;  and (2) augmenting learner models in simulators based on group  characteristics derived from social media.  Categories and Subject Descriptors J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI), Computer-managed  instruction (CMI), Distance learning.  General Terms Algorithms, Experimentation, Human Factors.   Keywords Augmented User Models, Data Mining for Learning, Learning  Analytics, Learning Needs, Mining Social Media, Social Profiles.   1. INTRODUCTION 1.1 Facilitating Learning Simulator Design  Simulated environments for experiential learning, or learning  simulators, create a practical and social context in which new  skills can be learned, applied and mastered. These environments  are increasingly popular as a means to turn experience into  knowledge, and are being applied in a variety of domains and  learning contexts. A key success factor for learning simulators is  the ability to connect learners to the real job-world, helping them  to recognize what they need to learn. However, existing learning  simulators suffer from the lack of such ability. This is because  they incorporate a limited understanding of the learner based on  skills and knowledge acquired only within the simulated world  and disconnected from the learners real job experiences. The  poor understanding of the learner needs is mainly due to the  limited scope of the initial learner model, where the simulator  scope for observing the learner is constrained within the particular  application. Such a limited learner model leads to missing key  learning aspects. These aspects may include learning domain  concepts within a real world situation, which the learner, or  people who share similar characteristics with him, are either  aware of, or need to know more about. For example, the  interpersonal skills and body language signals that job applicants  should be aware of during job interviews. Deriving user profiles  that include these learning aspects may help in facilitating the  design of the learning simulators in two main aspects:  1. The derived profiles can provide a means for the trainers or the  content providers to identify key learning needs for the simulator  learners. 2. The learning domain concepts derived from the profiles can  form a rich resource for augmenting the limited learner model to  overcome the classic cold start problem in user adaptive  environments.  1.2 Motivation: Mining Social Media to  Support Learning  Social media stand for a new culture of participation on the Web.  In the last decade, people have been more and more involved into   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.   198    Figure 1. Generic Framework to Derive Group Profiles from Social Media Resources to Facilitate Learning Simulators Design  contributing and shaping content on the Web. In social media,  people may comment on multimedia objects like YouTube1 videos and Flickr1 images, share their thoughts on micro-blogging  systems like Twitter1, publish their bookmarks on Delicious1, or  use services like CiteULike1 to organize scientific publications  they are interested in. With the increasing popularity of social  media resources, it is likely that people leave authentic digital  traces of their profiles and real life experiences in the domain of  their interests on social media sites [1]. Mining such digital traces  promises to be very beneficial for various applications [5].   The motivation for this work arises from the interesting challenge  of mining social media resources to bridge the gap between the  simulated world in learning simulators and the actual experiences  in the real job-world and better understand the learners needs in  the context of the learning domain of interest and within the real  world community the learner belongs to.     This interesting challenge leads to several research questions  that we would like to answer with this work, namely:   How to mine the digital traces in social spaces to derive  profiles of user groups  Can the derived group profiles be used to identify key  learning needs for potential learners  Can the derived group profiles be used to augment the  learner model with useful learning domain concepts      To answer the questions, this work presents a novel hybrid  framework by combining knowledge representation as provided  by semantics with knowledge discovery as exploited by machine  learning, for mining the user-generated content retrieved from  social media resources to derive profiles of user groups that can  be further exploited to facilitate the design of learning simulators.  The framework is perceived as a key contribution to mining social  media in learning analytics to support learning simulators design.    The rest of the paper is organized as follows: In the next Section,  we introduce a novel, semantically-enriched machine learning  framework that derives social group profiles from social media to  facilitate design of learning simulators. In Section 3, the  framework is instantiated within a case study to derive group  profiles from mining user comments on YouTube videos. In  Section 4, an experimental study is conducted to evaluate the  application of our framework within the YouTube-based content.  In Section 5, we position our work in the relevant literature on  mining social media to support learning. The paper concludes in                                                                     1 {youtube, Flickr, Twitter, Delicious, CiteULike}.com   Section 6, pointing at requisites for replicating the study and  future work.  2. DERIVING GROUP PROFILES FROM  SOCIAL MEDIA: GENERIC FRAMEWORK  To address our first research question, we introduce a hybrid  framework that integrates supervised machine learning  represented by classification, unsupervised machine learning  represented by clustering, and semantics. Figure 1 depicts the  generic framework processes and the flow of data processing.  The processes are described as follows:   1. User-Generated Content Retrieval: The digital traces that  users create on the selected social media resource are  retrieved using a content search process. Search is tailored  toward retrieving content that falls within the context of the  learning domain of interest. This is dependent of the social  media resource selected. For example, to retrieve the user- created discussions on job interviews from YouTube, a  collection of videos on job interviews can be selected by  domain experts (e.g expert job interviewers) and the  comments on these videos can be retrieved using the  YouTube API.  2. Semantically-enriched Classification Model: Because  social media content is full of noise, such as content that is  irrelevant to the learning domain of interest, spam, and abuse  content. We introduce a supervised machine learning process  that classifies the content into relevant and noisy and selects  the relevant content for subsequent processing. The process  employs a semantically expanded Bag of Words (BoW) and  a content scoring mathematical model described in [3] to  score and label the training data set used for building the  classification model to filter the noisy social media content.    3. Semantic-driven Individual User Profiling: using the  filtered user-generated content, this process constructs a  social user profile for every content author by merging all  the content written by the author into a term weight vector  representation. A semantically-enriched filtration layer  consisting of domain concepts relevant to the learning  domain is used to represent the content author. Using the  semantically-enriched layer, the domain-relevant terms that   199    Figure 2. Instantiated Framework to Derive Group Profiles from User Comments on YouTube Videos to Facilitate Design of  Simulated Environments for Learning Interpersonal Skills in Job Interviews   the author used in the content written by him are extracted  and included into his social profile. Furthermore, available  demographic information that the author may have input in  his social media user profile is retrieved and integrated into  his user profile.  4. Group Clustering Model: An unsupervised machine  learning process is exploited to cluster the social user  profiles generated in process 3 into groups of users based on  the similarity of their term weight vector representation.  Clustering validity measures are used to determine the  number of distinct groups that are mostly representative to  the users based on the content they wrote.   5. Group Profiling: The learning domain concepts that the  content authors who belong to each derived group are  interested in or aware of are extracted from the cluster  centroid of that group. In addition, statistical analysis is  performed on the demographic information of the authors.  The profile of each group is derived using the extracted  domain-relevant concepts and statistical distributions of the  authors demographics.  In the next section, we illustrate how the generic framework can  be exemplified within the context of a specific social media  resource, namely the YouTube video sharing site. For this, we  present a YouTube-driven instantiation of the framework to  derive social group profiles from YouTube-based corpus.  3. DERIVING GROUP PROFILES FROM  YOUTUBE: INSTANTIATED  FRAMEWORK YouTube has become the most successful Internet website  providing a new generation of short video sharing service. Since  its establishment in early 2005, more than 100 million videos are  being watched every day on YouTube, making it to rank second  in traffic among all the websites in the Internet by the survey of  Alexa [6]. YouTube provides several social tools for community  interaction, including the possibility to comment on published  videos. The analysis of comments constitutes a potentially   interesting data source to mine for obtaining implicit knowledge  about users, videos, categories and community interests [17].  YouTube allows the batch retrieval of these comments with its  public API. In addition to comments, YouTube enables registered  users to create individual user profiles on the site. A YouTube  user profile contains information about a user, such as the user's  age, gender, country of residence, hobbies, occupation, or favorite  books, music and movies. Any personal information that appears  in a user profile feed will have been entered by that user for  publication on YouTube. The YouTube Data API allows you to  retrieve user profiles.     Motivated by the data acquisition and analysis opportunities  that mining YouTube corpus can bring, we present a YouTube- driven instantiation of our framework to derive group profiles  purely based on YouTube corpus. The derived group profiles are  expected to provide two main functions: (i) help training  professionals to identify key learning needs that can be  considered in the storyboarding design of learning simulators, and  (ii) identify learning domain concepts that can be augmented in  the model of a learner who shares similar demographics with all  users in one group. Figure 2 illustrates the components of this  YouTube-driven instantiation. To demonstrate the process flow of  the framework instantiation, we aim to derive group profiles that  facilitate the design of learning simulators that train users to be  aware of the various interpersonal communication skills for the  job interview. Here, users can be either inexperienced job  interviewers, or job applicants.  Both job interviewers and job  applicants need to know more about the different interpersonal  communications during the interview session in order to improve  their job interviewing skills, or increase their chances of getting  hired after conducting the job interviews, respectively.  The framework process flow consists of 5 main stages:  Stage 1 - Video Selection: Uploaded YouTube videos about Job  Interviews are selected for the retrieval of their public comments.  Selection of the videos can be based on Domain expert selection,  or keyword-based search of the YouTube API. The YouTube API  provides many search filters to improve the relevance of the  search results. This includes filtering search results to show only  videos that match a given set of categories and/or user-defined   200    keywords. Each video can have many keywords but can only be  associated with one YouTube category   Stage 2 - Comment Retrieval: all the public comments on the  videos selected in Stage 1 are retrieved using the YouTube API.   Stage 3 - Filtration of Noisy Comments: Using the  semantically-enriched machine learning technique described in  [3], a YouTube noisy comment filtration component is  implemented to predict the noisy comments and filter them out  from the retrieved comment set. The predictive model is trained  using corpus retrieved from YouTube classified into two classes;  relevant and noisy, using a novel mathematical scoring model and  a semantically expanded Bag of Words that consist of concepts  highly relevant to the job interview domain. Noisy comments are  those that (i) contain too little job interview-related concepts; (ii)  are totally irrelevant to Job Interviews; or (ii) are spam or abusive  comments. The remaining comments are those that contain a  considerable rate of Job Interview-related concepts. Table 1  shows four example comments on the left that have been  predicted as noisy by the noise filtration component. Obviously,  the first three ones do not comment on the job interview video  being watched, whereas the fourth one is a spam. The machine  learning-based component aligned with that, classifying them as  noise. The two comments on the right clearly contain concepts  that are relevant to the job interview activity. Therefore, the  component classified them as relevant.   Table 1. YouTube Comments, with their Scores and Labels  Determined by the Machine Learning Component  Stage 4 - Retrieving YouTube User Profiles: Given the subset  of comments that have been filtered using the noise filtration  component, the YouTube Data API is utilized for each comment  in order to retrieve the demographic characteristics of the  comment author. Firstly, given the comment entry element in the  comments feed, the comment author username is retrieved for that  comment. Secondly, given the comment author username, the  demographic characteristics are retrieved from the YouTube user  profile, which contains information that the user lists on his  YouTube profile page. The authors age, gender, and location are  retrieved.  Stage 5a - Clustering-based Group Profiles: Groups of  comment authors are derived based on content similarity of the  video comments they write. A semantically-enriched clustering  algorithm is employed to derive the group profiles. The objective  of the derived groups is to support trainers in identifying key  learning needs to embed in the design of the learning simulator  storyboarding design. The algorithm is further described in  Section 3.1.   Stage 5b - Demographic-based Group Profiles: Groups of  comment authors can also be constructed based on user  predefined demographics. The objective of the derived group is to  identify job interview-related concepts that users who share  specific demographics with a potential learner are interested in or  aware of. These identified concepts can be used to augment the  model of that learner. For example, given two adult female  learners who live in the United States and Great Britain, stage 5b  in Figure 2 shows synthetic key Job Interview Concepts derived  from the comments of female adults who live in US & GB. The  demographic-based group profiling method is further described in  Section 3.2.   3.1 Clustering-based Group Profiling  The methodology of the Clustering-based Group Profiling  consists of the following steps:   3.1.1 Semantic filtration of the Comments Content  The textual content of each comment output by the noise filtration  component is represented by the terms that exist in a semantically  enriched Job Interview-related Bag of Words (BoW). This BoW  is derived from an experimental study documented in [9].  Pre-processing the experimentally-controlled user comments  includes two main steps:  Text Preprocessing: includes NLP techniques for text analysis  using the Antelope NLP framework2, i.e. sentence splitting,  tokenization, Part of Speech tagging and syntactic parsing using  the Stanford parser for linguistic analysis. This enables the  extraction of a structured form text representation to empower  further analysis using semantics.   Semantic Enrichment: representing Ontology-based word sense  disambiguation and linguistic semantic text expansion. The first  filter applied concerns the selection of specific lexical categories  implemented within the WordNet3 Lexicon English language  thesaurus to directly exclude non-significant terms for the job  interview activity. For the words remained, the Suggested Upper  Merged Ontology (SUMO) [7] has been exploited to provide  direct mappings of WordNet English word units. WordNet  Lexicon queries were then performed to retrieve synonyms,  antonyms and word lexical derivations to expand the word set.  Furthermore, DISCO [13] has been exploited to retrieve  distributionally similar words from the Wikipedia corpus, and the  filters discussed above have been applied, i.e. lexical category and  SUMO concept mapping.   Table 2. Sample YouTube comment in original form (top) and  reduced form (bottom)   I will say both are bad. If you focus more on the guy, he  kept shaking his legs very often so thats not a good sign in  an interview.   say bad focus shaking legs good sign interview                                                                     2 www.proxem.com/Default.aspxtabid=119  3 http://wordnet.princeton.edu/   201    Figure 3. Text Clustering Process in RapidMiner to Cluster the YouTube Comment Authors into User Groups based on Learning  Domain Concept Similarity in Their Comments   Each relevant YouTube comment is represented by only the terms  that exist in the semantically-enriched Bag of Words. Table 2  depicts a sample YouTube comment in its original form (top) as  well as in its reduced, annotated form (bottom).   3.1.2 Representing Authors by Their Filtered  Comments Given the semantically-filtered YouTube comments derived  according to the method Section 3.1.1 and the author usernames  retrieved in Stage 4, each comment author is represented by a  term vector that consists of an aggregation of all the comments he  wrote. Each derived term vector corresponds to one author. Table  3 depicts a term vector representation of one comment author,  which is derived from combining two different annotated  comments. The original two comments are shown on the right  side of Table 3.   Table 3. Term vector representation of Comment Authors   Term Vector  Representation of Author Original comments   { excellent, video, many, aspects, body, language, business, years, can, always, provide, superb, real, experience, useful, introduction, body,  language, work,  already, decided,  change, handshake,  use, reference, future,  thank }  Another EXCELLENT video  by Peter Clayton.  I have  studied many aspects of body  language in business over the  years and can always rely on  Peter to provide superb  insights, hints and tips, all  backed up by real life  experience.  A very useful introduction to  body language in work.  I  have already decided to  subtly change my handshake,  and will use this as a  reference in the future.   Thank you for sharing this.  As can be seen in table 3, the vector terms derived from each  comment are depicted in a color similar to the original comment.  This vector representation of the author allows us to infer the  overall awareness of that author in the job interview domain. For  example, the authors first comment indicates that the author is  already aware of body language aspects in business, whereas his  second comment indicates that the video enabled him to become  aware of proper handshaking. The overall awareness can be seen  by the authors term vector representation.   3.1.3 Clustering Comment Authors  A RapidMiner4 process has been built to perform text clustering  of the comment authors, where each unique author is represented  by a term vector derived according to the method described in  Section 3.1.2. Figure 3 shows the flow of operators used in the  text clustering process.  The functionality of each operator in the RapidMiner process is  described as follows:   Read Database: Reads the term vector dataset from the  database into the RapidMiner process.  Set Role: Sets the role of the author username as a row  identifier and assigns the term vector attribute as an input for  text clustering.  Extract Docs: Builds a text document representation for  each term vector from the input dataset. This step is required  for text clustering in RapidMiner.  Pre-Processing: Performs a number of text pre-processing  sub-operators on the input text documents. These include: (i)  tokenization, (ii) transforming to lower cases, (iii) stop word  removal, (iv) filtering too short terms, and (v) generating  bigram phrases.  Clustering: Clusters the comment authors using the Feature  Weighting K-Means Algorithm, which has been previously  used to perform text clustering with good performance  results [11]. The optimum value of K (the number of  clusters) is selected after performing a clustering evaluation  strategy in RapidMiner. The clustering evaluation strategy is  described in Section 4.2.  Centroids: Extracts the cluster centroids from the resulted  derived clusters. Each centroid represents a vector of the  TFIDF weights of the terms in each cluster. These are  written to a CSV file using the Write CSV operator.  Select Attribs: Extracts the cluster membership (cluster  number) that each comment author belongs to. This is then  written back to the database using the Write Database operator.  3.1.4 Deriving the Group Profiles  A Profile for each group of comment authors is derived in this  step, where each profile consists of the following elements:   Percentage of the comment authors in the group.                                                                     4 http://rapid-i.com/content/view/181/196/   202    Figure 4. Statistical Distribution of Three Demographic Properties for the YouTube Comment Authors   List of the job interview-related terms the authors of the  group are interested in or aware of. These concepts are  retrieved from the cluster centroid elements having  maximum TFIDF weights.  Percentages of the gender distribution.  Percentages of the age groups in years (11-20, 21-30, 31-40,   etc.).  Percentages of the location (country) distribution.  Sample comments written by authors who belong to the  group. These comments are selected according to their  relevance scores. The scores are computed based on the  mathematical model presented in [3]. The top n comments  having the maximum relevance scores are shown in the  profile, where n can be a fixed number (e.g 1 comment) or a  proportion of the comments written by the group authors (e.g  10%).  3.2 Demographic-based Group Profiling  As discussed in Section 1, the initial learner models in simulated  learning environments do not contain sufficient information about  the learner. This creates a user-adaptive problem for learning  simulators to well adapt the learner needs. On the other hand,  users who share demographic characteristics may have similar  interests or can be aware of the same concepts. For example,  people who are of certain age groups may be interested of the  same kinds of songs. Collaborative filtering techniques in  recommender systems are based on similar concept, where users  are recommended items based on how their user preferences are  similar to each other [20]. To address the limited scope problem  of the initial learner model, a group profile for the YouTube users  who share demographic characteristics with the learner can be  derived by aggregating all the comment author vectors that meet  specific demographic criteria. Then, the key job interview- concepts that the users in this group are interested in can be  identified as the list of terms having the maximum n TFIDF  weights. These concepts can then be augmented into the model of  the learner.   4. EXPERIMENTAL STUDY  To evaluate the framework instantiation in YouTube, an  experimental study has been conducted. In the following  Subsections, we describe the YouTube data that has been used in  the study; the clustering validation techniques that have been  employed to determine the number of groups to derive by the  group clustering component of the framework; and present sample  group profiles, demonstrating in examples how the derived group  profiles can be used to identify key learning needs and augment  initial learner models.   4.1 Data Description  Prototypical versions of the clustering-based and demographic- based group profiling components in the framework have been  implemented to illustrate how the framework can be exploited to  facilitate the design of a simulated environment for learning  interpersonal skills and good practices in job interviews. Table 4  provides a statistical description of the experimental data.   Table 4. Overview of the YouTube Corpus   Data Property Value  Number of Job Interview-related YouTube Videos 17 Number of Comments Retrieved 1465 Number of Remaining Comments after Noise  Filtration  471 (32%)  Number of Unique Comment Authors 393 Comment to Author Ratio 1.20 Number of Derived YouTube User Groups 17  Seventeen YouTube videos that show teaching examples of  interpersonal skills in job interviews have been selected by  training professionals who are expert in the job interview domain.  As can be noticed from table 4, Most of the public comments  (68%) on those videos are irrelevant to the job interview domain  although they are within the comments on videos that are highly  relevant to job interviews. This shows that filtering out noisy  comments from high traffic social media resources, such as  YouTube, is important to remove the comments that are not  valuable in deriving group profiles, which can describe the  awareness of the comment authors in the job interview domain  concepts. This noise removal results in producing better clusters  and reduces the computational time of the clustering process.   Figure 4 depicts the gender distribution (a); the user age groups in  years (b); and the 10 most frequent countries the authors are  located in (c).   These demographic statistics suggest that most comment authors  who write relevant comments about job interview videos on  YouTube are adult males who are located in Western countries.  However, the figures also show that female users, elderly users,  and users who live in different parts of the world, such as Turkey,  Canada, Asia, and Australia, write relevant comments on  YouTube job interview videos. Moreover, it would be interesting  to compare the distribution of the demographic characteristics for  each derived group with a generic profile for the overall YouTube  community, provided such a generic profile exists.   203    4.2 Clustering Validation  In order to select the right number of clusters for the clustering- based group profiling to derive the YouTube groups, 9 feature  weighting k-means clustering models are trained using the input  dataset, where each model has a unique number of clusters (k).  For each model trained, two cluster validity measures  implemented in RapidMiner are computed: (i) Davies Bouldin  index [8], and (ii) Cluster density performance measure. Davies  Bouldin index is the ratio of the sum of within-cluster scatter to  between-cluster separation. This ratio has smaller values for a  model that derives clusters which are more compact and farer  from each other than other models. The cluster density  performance calculates the average distance between the  YouTube comment authors in each cluster and multiplies the  result by the number of comment authors in that cluster minus 1.  The Euclidean distance is used as the distance measure. The  smaller the density value is, the more similar the comment authors  are to each other in the cluster, and thus the more compact the  cluster is. Figure 5 shows the Davies Bouldin index (top) and  average cluster density values (bottom) for the 9 feature  weighting k-means clustering models, with (k) ranging between 2  and 10.   Figure 5. Davies Bouldin Indices (top) & Cluster Density  Values (bottom) for 9 Clustering Models  Figure 5 (top) shows that the model having 9 clusters produce the  minimum Davies Bouldin index. Because RapidMiner negates the  density values, the best cluster density values are those closest to  zero. As can be seen in Figure 5 (bottom), the cluster density  values become closer to zero as the value of (k) increases.  However, the difference of the increase in density after the model  with k = 8 clusters becomes insignificant with the subsequent  models. The consideration of the two models suggests that  clustering the YouTube comment authors into 8, 9, or 10 clusters  can derive groups of users who are relatively close to each other  inside each group and far from each other from one group to  another. In the sext subsection, we base our analysis on the model  that derives 9 groups, as suggested by the Davies Bouldin index  results.  4.3 Example Group Profiles and Usage  4.3.1 Identification of Learning Needs  We illustrate in the example how clustering-based group profiles  can be used by training professionals to identify learning needs  for group of learners who meet the demographic criteria of the  users who belong to the example groups. Table 5 shows the details of an interesting sample group which  contains around 10% of the comment authors. The key job  interview-related terms in this group suggest that the users who  have been associated to this group by the framework express  anxiety (e.g anxious, nervous, worried). Other bigram  phrases detected by the framework, such as good_words, question_interviewer, and job_experience suggest that  these topics could be the reason for the anxiety expressed in the  comments. In the GUI interface, the identified concepts can be  used by training professionals to browse the comments written by  the users who are associated to this group to better understand the  reason behind their anxiety. We simulated that by showing  sample comments written by authors who are associated to this  group by the clustering algorithm. As the first sample comment  reads, the author is a potential job applicant who is worried that  the interviewer might ask him about his little previous job experience. Hence, he seeks an advice on proper answers (good words) that he can say to well justify such a potential question  during his job interview. Similarly, the second comment author is  nervous because he is not sure what good questions he may ask  the interviewer if he has been asked to do so during the  interview. The trainers can identify these learning needs by  depicting the identified learning concepts and use them to browse  and read the authors comments that are linked to these concepts.   Furthermore, based on the distribution of the demographic  characteristics of the group users, the trainer may link these  learning needs to adult applicants who live mostly in the US and  Europe, as the age group and the location distributions suggest for  this group.   4.3.2 Learner Model Augmentation  As described in Section 3.2, group profiles may also be derived  by aggregating all the individual user profiles of the comment  authors  represented by their semantically-filtered comment term  vectors  whose demographic characteristics, such as their age  groups and locations, match a potential learner for a learning  simulator. The learning domain concepts extracted from the group  are augmented into the model of the learner to overcome the cold  start problem, providing the learning simulator with more learning  domain-relevant information about the learner.  To evaluate the effect of having different demographic properties  on the learning domain concepts, we assume an artificial example  of having three adult learners with different known locations:  Great Britain (GB), United States (US) and Asia. Given their  ages and locations as input to the framework, three demographic- based group profiles are derived by aggregating the term vectors  of the YouTube individual comment authors who meet the  demographic properties of each of the learners. Table 6 depicts  the three different demographic properties.  As can be seen, the key learning concepts that the leaners are  interested in or aware of considerably differ from a group to  another. Differences are illustrated in three job interview aspects:   204    Table 5. A Sample Clustering-based Profile for a YouTube Group having 10% of Comment Authors   Body Language Signals: the frequent body language signal  that adults in GB talk about is the eye contact. However,  for US adults. On the other hand, other body parts used in  body language signals, such as fingers and hands, frequently exist in the comments written by US users.  Extracted concepts from content written by users in Asia do  not suggest that they are interested in or aware of body  language signals.   Emotions: US adult job candidates are more inclined to  express their anxious emotional states when talking about job  interviews. This can be sensed from the nervous concept  being only in the group of US adults. GB adults on the other  hand tend to show more confidence by using terms like  hope and helpful. Interests: Asian users show more interests in watching  interview and job hunting guides than users in US and GB.  Interest by users in Asia and US in the financial aspect can  also be seen in relevant terms such as money (Asia, US)  and pay (Asia). US users show more interests in  companies and education in addition to money. Both  GB and US users tend to mention the interviewer more  frequently than mentioning the interviewee, as opposite to  users in Asia who tend to mention the candidate more  frequently. Mentioning humans, as can be seen in terms like  people and girl is more apparent in comments written  by authors who live in the US.   Table 6. Demographic-based Group Profiles   5. RELATED WORK  In this Section, we position our work within the literature works  in three main relevant aspects, namely: (i) Augmenting user  models for personalization and adaptation in simulated  environments, (ii) Mining user-generated content in social media  for learning support and learner modeling, and (iii) Exploiting  machine learning techniques to mine social media content.   Augmenting user models by deriving characteristics of other  similar users for personalization and adaptation in digital  environment is well founded in the literature. [16] presented a  machine learning framework that addresses the lack on  interoperability between different multiple electronic systems  offering recommendations to users. The output is a cross system  personalization that supports augmenting user models from  learning about what other users know in different electronic  systems. In this framework, a set of examples from one system is  used to train a machine learning model and the model is then  applied to augment user models in another system. Likewise,  [22]  described the design of an extension to a lifelong learning system,  called people like me, which supports personalized searching  for timelines of people who share similar characteristics with the  learner. In our framework, we further extend the concept of  augmenting user models with characteristics mined from digital  traces written by other users, involving social media as the  resource for these traces.   Learner model development and augmentation to support learning  is also discussed. [12] presented a method based on fuzzy sets to  describe the uncertainty of learner knowledge used for learner  modeling and identification of the learners needs for adaptive  educational systems. A fuzzy user model is proposed to deal with  vagueness in the users knowledge description. However, in  learning simulators, the main problem lies in the limited scope of  information about the learner that exists in his initial learner  model, and not in the imprecision and vagueness of the learners  descriptions of his knowledge. Similar fuzzy techniques have  been applied in [14] to use tracking information generated by a  Web Course Management System to build student, group, and  class models that generate advice to the course instructors and  facilitators. This can be effective when the limitation of the  learner model is due to psychological barriers, such as the  students' feeling of isolation and the instructors' communication  overhead and difficulty to address the needs of each individual  student. The lack of sufficient information about the learner due   205    to the limited scope of the learning application has not been  addressed. [23] presented an activity-visualization prototype  extended to the Moodle virtual learning environment.   The prototype supports personalization of learning by enabling  learners to visualize their virtual learning activities in Moodle and  mirroring them back to the learners using Web2.0 services.  However, these activities are constrained within the capabilities  that Moodle provides, or more specifically, what Moodle plug-ins  the learning service provider chooses to install. Our framework on  the other hand, is not tied to digital traces that users may produce  in one platform. For example, the platform can extend the  YouTube-based corpus used for deriving the group profiles by  adding user tweets about Job Interviews retrieved from twitter,  user blog posts in blogs that address interpersonal skills, and  professional discussions about Job Interview skills from LinkedIn.   Researchers have also focused on the involvement of training  professionals in the construction of learner models. Examples  include the works in [19], [18], and [15]. In [19], the structure of  the student model has been specified by domain experts and then  the parameters were mined from the data obtained in transcripts of  problem-based learning sessions. In [18], a Bayesian student  model was constructed based on a combination of elicitation from  the domain experts and automated methods. In [15], training  professionals compared the performance of two different  knowledge student model structures for a virtual learning  environment, where evaluation parameters are revealed in the  model data. In our work, we built on this concept by enabling  training professionals to augment learner models by identifying  key learning needs for learners based on machine-derived profiles  of user groups with which the learner has a potential to share  similar characteristics.   Literature on mining content written by users in social media  resources to support learning environments is also relevant to our  work. [10] presented a method to mine the micro digital traces, or  tweets, that the learner writes on twitter to extract knowledge  about the learner and use them to augment the learner model,  which can be helpful to overcome the cold-start problem in e- learning systems. Other works have also analyzed learners tweets  to support the learning process. [4] and [21] analyzed the  usefulness of twitter micro-blogging activities in second language  learning. For that, students have been required to write certain  number of tweets per week and then analysis of the  communication patters between students using their twitter  activities has been conducted to investigate any improvements in  many learning competences that students acquire, including  Sociolinguistic, strategic, and cultural competences. However, the  discussed methods can be useful only if the learner has an active  twitter account. This is not a controlled feature for learners in  simulated learning environments. Moreover, tweets written by  users who share similar characteristics with the learner, which  may also imply additional features about the learner, have been  overlooked.  Machine learning techniques have been recently considered to  mine social media content for many purposes. [2] reviewed many  machine learning-based works to analyze blogs and blog posts to  discovery communities in social media; detect influential and  trustful blogs and bloggers; and filter spam blogs. Both cluster  analysis and classification techniques have been utilized in these  works. However, semantic enrichment techniques to improve  clustering results and predicting rates have been clearly   overlooked. From a technical perspective, the extension of our  approach to these works is perceived in providing a semantic  enrichment layer to the machine learning techniques in order to  improve, firstly, the noise filtration capability, by training the  classifier to filter out content irrelevant to the learning domain,  and secondly, the clustering capability of the individual profiles to  user groups based on their similarity of the learning domain  content they provide. Furthermore, the approach presented in our  work is tailored toward supporting the learning domain by  employing the machine learning outputs to facilitate learning  simulator design.   6. CONCLUSIONS Mining user-generated content and Modeling users from social  media resources are becoming increasingly important thanks to  the huge available content that users generate. However, the focus  on simulated environments for learning has not been addressed  extensively by researchers. Learning simulators may not have  sufficient information about learners and thus adaptation to leaner  needs becomes problematic. Most of the works that mine social  media content to support learning environments assume that  learners have active social media accounts. In many cases,  learners may not have social media accounts or may choose not to  reveal them.  In this paper, we introduced a novel framework to  facilitate the design of user-adaptive learning simulators to better  meet the learners needs. The framework intuitively aims to  utilize the collective intelligence aspect employed in collaborative  filtering techniques. This is achieved by deriving social group  profiles and extract key learning domain-relevant concepts that  can reveal many interesting findings about those who are  demographically similar to the simulator learners. We have  exemplified the generic framework within the context of  YouTube, illustrating how it can be instantiated to a specific  social media resource. We evaluated how this can support  learning simulator design by helping trainers to identify key  learning needs as well as extracting learning domain concepts  from the group profiles that can be used to augment an initial  learner model that has a cold start problem.   6.1 Requisites to Run Similar Experiments  The following requisites to replicate the similar experiment on  same or different contexts should be taken into consideration: 1. Semantic enrichment relies on Bag of Words extracted from   domain ontologies. However, vocabularies that describe  different learning domains may be absent. To address this  requisite, ontologies such as SUMO and DBPedia as well as  lexical resources such as WordNet Domains can be used to  extract domain-relevant Bag of Words.   2. Expert selection is needed to both select the domain-relevant  YouTube videos and to evaluate how relevant the used  domain vocabulary to determine comments relevant for  deriving user profiles.   3. The dataset used to derive the group profiles needs  broadening to well represent the YouTube community. This  can be addressed by retrieving more domain-relevant videos  using the search capabilities of the YouTube data API, such  as search by keyword, categories, and developer tags.   6.2 Future Work  A user-based evaluation study involving learners and training  professionals will be conducted in the ImREAL project. Learning   206    interpersonal skills for job interviews is the first targeted use case.  Other use cases involve learning social signals in different  cultures to help learners involved in student exchange programs to  be aware of the various cultural communication signals.   7. ACKNOWLEDGMENTS The research leading to these results has received funding from  the European Union Seventh Framework Programme (FP7/2007- 2013) under grant agreement no ICT 257831 (ImREAL project).  8. REFERENCES [1] Abel, F., Henze, N., Herder, H., and Krause, D. 2010.   Interweaving public user profiles on the Web. Proc. of Int.  Conf. on User Modeling, Adaptation, and Personalization, 16-27. Springer   [2] Agarwal N, and Liu, H. 2009. Modelling and Data Mining in  Blogosphere, Synthesis Lectures on Data Mining and  Knowledge Discovery, R. Grossman, ed., Morgan &  Claypool Publishers, vol. 1   [3] Ammari, A., Dimitrova, V., and Despotakis, D. 2011.  Semantically Enriched Machine Learning Approach to Filter  YouTube Comments for Socially Augmented User Models.  Proc. of the Int. Workshop on Augmenting User  Models,UMAP11 Conference, Girona, Spain, 6  17   [4] Borau, K., Ullrich, C., Feng, J., and Shen, R. 2009.  Microblogging for Language Learning: Using Twitter to  Train Communicative and Cultural Competence, Advances  in Web Based Learning  ICWL. Lecture Notes in Computer  Science, Vol. 5686/2009, DOI: 10.1007/978-3-642-03426- 8_10, 78-87   [5] Brusilovsky, P., Kobsa, A., and Nejdl, E. 2007. The  Adaptive Web, Methods and Strategies of Web  Personalization, Lecture Notes in Computer Science, vol.  4321, Springer   [6] Cheng, X., Dale, C., and Liu, J. 2008. Statistics and social  network of YouTube videos. Proceedings of the 16th  International Workshop on Quality of Service, Enschede,  The Netherlands, 229238.   [7] Chung, S. F., Kathleen, A., and Chu-Ren, H. 2004. Using  WordNet and SUMO to Determine Source Domains of  Conceptual Metaphors, Proc. of 5th Chinese Lexical  Semantics Workshop (CLSW-5). Singapore: COLIPS. 91-98   [8] Davies, D., and Bouldin, D. W. 1979. A Cluster Separation  Measure, IEEE trans. on Pattern Analysis and Machine  Intelligence, PAMI-1 Issue:2, 224  227   [9] Despotakis, D., Lau, L., Dimitrova, V. 2011. A Semantic  Approach to Extract Individual Viewpoints from User  Comments on an Activity. Proc. of the Int. Workshop on  Augmenting User Models, UMAP11 Conference, Girona,  Spain.   [10] Hauff, C., and Houben, G. J. 2011. Deriving Knowledge  Profiles from Twitter, Proc. Of the 6th European Conf. on  Technology Enhanced Learning (EC-TEL'11)  [11] Jing, L., Ng, M., Xu, J., and Huang, J. Z. 2005. Subspace  Clustering of Text Documents with Feature Weighting K- Means Algorithm, Lecture Notes in Computer Science, Vol.  3518/2005, 802-812   [12] Kavcic, A. 2004. Fuzzy user modeling for adaptation in  educational hypermedia. IEEE Trans. on Systems, Man, and  Cybernetics, Part C: Applications and Reviews, vol. 34(4),  439449  [13] Kolb, P., 2008. DISCO: A Multilingual Database of  Distributionally Similar Words. Proc. of KONVENS-08, Berlin  [14] Kosba, E., Dimitrova, V., and Boyle, R. 2003. Using Fuzzy  Techniques to Model Students in Web-Based Learning  Environments, LNCS, 2003, Vol 2774/2003.  [15] Manske, M., and Conati, C. 2005. Modelling Learning in an  Educational Game. Proc. of 12th World Conference of  Artificial Intelligence and Education AIED'05. IOS Press   [16] Mehta, B. 2007. Learning from what others know: Privacy  preserving cross system personalization. LNCS, vol. 4511,  Springer, ISBN 978-3-540-73077-4, 57-66   [17] Siersdorfer, S., Chelaru, S., Nejdl, W., and Pedro, J., S.  2010. How useful are your comments analyzing and  predicting YouTube comments and comment ratings,  Proceedings of the 19th international conference on World  Wide Web, Raleigh, North Carolina, USA, 26-30   [18] Stacey, K.P., Sonenberg, L., Nicholson, A., Boneh, T., and  Steinle, V. 2003. A Teaching Model Exploiting Cognitive  Conflict Driven by a Bayesian Network. Proc. of 9th  International User Modeling Conference. LNCS, Vol. 2702.  Springer-Verlag, 352-362   [19] Suebnukarn, S., and Haddawy, P. 2005. Modeling Individual  and Collaborative Problem Solving in Medical Problem- Based Learning. Proc. Of 10th Int. User Modeling  Conference, UM'2005. Lecture Notes in Artificial  Intelligence, Vol. 3538. Springer-Verlag, 377-386   [20] Sun, T., Wang, L., and Guo, Q. 2009. A Collaborative  Filtering Recommendation Algorithm Based on Item  Similarity of User Preference, 2nd Int. Workshop on  Knowledge Discovery and Data Mining, DOI 10.1109, 60-63   [21] Ullrich, C., Borau, K., and Stepanyan, K. 2010. Who  students interact with A social network analysis perspective  on the use of twitter in language training, Proc. Of the 5th  European Conf. on Technology Enhanced Learning (EC- TEL'10)  [22] Van Labeke, N., Magoulas, G. D., Poulovassilis, A. 2009.  Searching for people like me  in a lifelong learning system.  Proc. Of the 4th European Conf. on Technology Enhanced  Learning (EC-TEL'09)  [23] Verpoorten, D., Glahn, C., Kravick, M., Ternier, S., and  Specht, M. 2009. Personalization of learning in Virtual  Learning Environments, Proc. Of the 4th European Conf. on  Technology Enhanced Learning (EC-TEL'09)  207    "}
{"index":{"_id":"36"}}
{"datatype":"inproceedings","key":"Bienkowski:2012:LRB:2330601.2330651","author":"Bienkowski, Marie and Brecht, John and Klo, Jim","title":"The Learning Registry: Building a Foundation for Learning Resource Analytics","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"208--211","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330651","doi":"10.1145/2330601.2330651","acmid":"2330651","publisher":"ACM","address":"New York, NY, USA","keywords":"attention metadata, learning analytics, learning-resource analytics, social metadata","Abstract":"We describe our experimentation with the current implementation of a distribution system used to share descriptive and social metadata about learning resources. The Learning Registry, developed and released in a beta version in October 2011, is intended to store and forward learning-resource metadata among a distributed, de-centralized network of nodes. The Learning Registry also accepts social/attention metadata---data about users of and activity around the learning resource. The Learning Registry open-source community has proposed a schema for sharing social metadata, and has experimented with a number of organizations representing their social metadata using that schema. This paper describes the results and challenges, and the learning-resource analytics applications that will use Learning Registry data as their foundation.","pdf":"The Learning Registry: Building a Foundation for Learning  Resource Analytics   Marie Bienkowski  SRI International   333 Ravenswood Avenue  Menlo Park, CA 94025   +1 650 859 5485   marie.bienkowski@sri.com   John Brecht  SRI International   333 Ravenswood Avenue  Menlo Park, CA 94025   +1 650 859 2325  john.brecht@sri.com   Jim Klo  SRI International   4111 Broad Street  San Luis Obispo, CA 93401   +1 805-542-9330 x121  jim.klo@sri.com         ABSTRACT  We describe our experimentation with the current implementation  of a distribution system used to share descriptive and social  metadata about learning resources. The Learning Registry,  developed and released in a beta version in October 2011, is  intended to store and forward learning-resource metadata among a  distributed, de-centralized network of nodes. The Learning  Registry also accepts social/attention metadatadata about users  of and activity around the learning resource. The Learning  Registry open-source community has proposed a schema for  sharing social metadata, and has experimented with a number of  organizations representing their social metadata using that  schema. This paper describes the results and challenges, and the  learning-resource analytics applications that will use Learning  Registry data as their foundation.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1  [Computer Uses in Education] Computer-managed instruction  (CMI), Distance learning; H.3.5 [Online Information Services]  Data sharing, Web-based services   General Terms  Performance, Design, Experimentation, Standardization.   Keywords  Learning analytics, attention metadata, social metadata, learning- resource analytics.   1. INTRODUCTION  Learning resources are available from many agencies: federal and  national governments, state/provincial agencies, local school  districts, post-secondary institutions, and for-profit, and not-for- profit organizations. These resources are distributed across a  variety of repositories, using a variety of metadata standards and  access mechanisms. Open learning resources should be made  publicly available to alland in principle they arebut in  practice users must visit each organizations repository  individually and contend with their various interfaces. Harvesting   these resources for use elsewhere can involve complicated  metadata crosswalks, and metadata often is incomplete and out of  date. A searcher should be able to search across repositories for all  of the learning resources available on a particular topic or find  ones that have been authored by a particular person or institution.  Federated search or registries that collect metadata could provide  a single access point to these repositories but require a centralized  authority to maintain and prove difficult to upkeep. Updates to  descriptive metadata could be obtained from information  published on the web by scraping websites, use of microformats  on learning resource pages [1], from search terms entered into  repositories, or from analysis of online curriculum that uses  embedded resources.   Yet even if federated search and automated collection of metadata  were achievable, searching for learning resources based on  descriptive metadata alone (e.g., keywords, author, publication  date) may not yield adequate results. Newer methods of locating  relevant items take into account characteristics of the searcher to  provide recommendations. Online commerce and social  networking sites using analytics have demonstrated the value and  efficiency of recommendations, but these require that the search  mechanism have access to both the properties of the object of  search (metadata), and the properties and actions of the searchers  and their community. In education, this richer set of properties  and actions is increasingly coming from user interactions with  portals and repositories, and includes data such as counts of use  (in the classroom or online), contexts of use (e.g., what sorts of  classrooms/students/teachers), and reflections by users (e.g.,  ratings, descriptions). However, this rich social metadata teachers downloading, favoriting, rating, commenting uponis, at  present, locked inside portals and cannot be shared across a broad  network of education stakeholders (including researchers). The  Learning Registry provides a unique opportunity to share data  across these information siloes    The Learning Registry is an infrastructure that supports learning  resource discovery, sharing, and amplification. Consider the case  of an open educational resource (OER)1 about robotic planet  exploration (i.e., rovers) developed by a design museum. This  design OER could be hosted on the museums website in their  education corner and include alignment to standards. As the OER  becomes better known, links to it may appear in teacher portals,  resource repositories/aggregators, or national or state-level  curriculum. Related resources, for example, about an actual rover                                                                        1 Note that the Learning Registry is not limited to only open/free   resources.      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK'12: 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.   208          sent to Mars, may be available. This rover-design OER may also  be mentioned in blogs, discussion forums, and social media sites.  Data about this OER (including related OERs) is scattered ever  more widely as it grows in popularity.   Learning Analytics research and development efforts will build  learning environments that capture different kinds of data about  this rover resource: aligned to a standard; used in an online design  course; commented upon by an educator. A widely crowd-sourced  dataset is necessary to conduct the types of analytics needed for  educational improvement. Where does all of this disparate data  get aggregated In the absence of a solution such as the Learning  Registry, one option is to centralize all activity about a resource in  one site so that attention/social metadata can be captured there.  Another option is to create n-way connections among metadata  and social metadata sources to share data. A third option might be  to release a browser plugin that acts as a sensor to help track user  activity and engagements with and around learning resources  (e.g., with a learning management system, or LMS). Web crawlers  could scrape websites for (static) information about resources;  harvesters could extract metadata from repositories; or programs  could harvest attention metadata via APIs to social media sites.   Yet even with all these approaches, the big data still needs  to go somewhere. A for-profit company could collect this data and  monetize it, but the Learning Registry is working to foster the  alternative: that individuals and groups will contribute data to this  distributed and vast timeline-oriented dataset for use by  researchers and developers to improve teaching and learning with  online resources. The Learning Registry makes possible  answering empirical research questions such as: how effective is a  resource and for whom and what Learning Registry data could be  used to answer questions from analytics: if Learner A uses  resources U, V, W; Learner B uses U, V, X; and Learner B shows  more competence, then resource X may be worth noting.   2. DESIGN OF THE LEARNING  REGISTRY  The Learning Registry was envisioned as a store-and-forward  network based loosely on the NNTP model (e.g., see [2], [3]): a  network to which providers of learning resources, metadata and  social metadata can distribute information for consumption and  amplification by the community. To support these providers, the  design must accommodate a large volume of data expressed in a  variety of metadata standards. In this section, we provide a brief  overview of the design of the Learning Registry. More detailed  information can be found in [4] and at the starting points at  www.learningregistry.org.   2.1 Storing and Distributing Learning  Resource Information  The Learning Registry accepts, stores, and provides access to  learning resource descriptionsmetadata or social metadataas  documents expressed in JSON notation. The storage and access  mechanism used internally is CouchDB (couchdb.apache.org), a  lightweight, open-source document-based database. CouchDB  provides data access in the form of views generated by  MapReduce functions written in Javascript. Couchs replication  mechanisms make it easy to stand up a network of Couch nodes,  which serves the goal of decentralizing the Learning Registry and  eliminating single points of failure. Replication can also be  leveraged by high-volume users to create their own local  repository containing all or some of the Learning Registry data.    On top of CouchDB, the Learning Registry provides a layer of  services (written in Python) as APIs to publish data, to query  documents, and to retrieve documents. These services are the  principle public interfaces to the Learning Registry, though  developers are welcome (and encouraged) to provide other  services in addition to or on top of these services.   2.2 Submitting Descriptive Metadata  By design, the Learning Registry defines a loose format for the  submission of metadata about resources, and does not specify  what metadata schema should be used. The Resource Data  Description (RDD) document can be used to submit metadata or  social metadata (either linked to via a URI or embedded directly  as the payload), and can contain a reference to a URI that gives  the schema for the metadata. The RDD is a thin wrapper around  the submitted metadata; in this way, the Learning Registry can be  agnostic about metadata formats and, as the field develops, about  formats for social metadata. Early users have used the National  Science Digital Library (NSDL) Dublin Core schema [5] or IEEE  LOM [6], but nothing precludes the use of newer schemas such as  LRMI (lrmi.net) or custom ones (e.g., [7]). We expect that  services built on top of the Learning Registry can provide  extraction or crosswalk services across RDDs that use disparate  standards, or can assemble metadata fields from different schemas  into custom views.   Documents submitted to the Learning Registry are assigned a  unique document ID. This document ID is not a unique identifier  for the resource described by the document: multiple documents  in the Learning Registry can describe the same resource. We have  found it challenging to create a unique way to identify a resource  so that these descriptions (metadata or social metadata) can be  used for analytics. The document ID of a metadata description is  specific to just that one submission of metadata, and, indeed,  social metadata may be submitted for a learning resource before  the resource itself has been registered with a metadata submission.    For the present, the Learning Registry uses the resources URL as  this global identifier (called resource_locator). Using URL this  way is already posing difficulties because in practice multiple  URLs might refer to the same resource. (e.g., PBS Learning  Media, www.pbslearningmedia.org, desired to redirect its users to  local PBS affiliates, turning the URL into something like  ca.pbslearningmedia.org). While a solution such as OpenURL  could provide a standard URL to match resource metadata with  social metadata, it requires a central service. Over time, the  Learning Registry community may develop more consistent URL  conventions, adopt OpenURL, provide translation services, or  simply live with islands of data created by non-uniform naming.  The Registry may also eventually support assertions that could  allow community members to make formal statements to the  effect: I assert that this URL, x, and that URL, y, refer to the  same Learning Resource. and thus allow combined analytics for  x and y.     Uniquely identifying resources was also a difficulty. In our  experimentation, we found that publishers assumed that their local  URL was the canonical version. For example, a site such as  learnalot.com would submit http://learnalot.com/resource-1,  which is where the reference to the learning resource is hosted on  their site. However the resource may have originated from  http://federalagency.gov/resource-1) and, in order to aggregate all  paradata for this resource, a canonical locator must be used.  OpenURLs, which are used in many learning contexts such as  Google Scholar, rely on a query mechanism to locate a context-  209          sensitive URL for a resource, relative to the requestor. There is  currently no way to express an OpenURL in a canonical format  such that all context-sensitive copies can have an association  expressed within the Learning Registry   2.3 Submitting Social Metadata  The NSDL Com_para format, created for the STEM Exchange  prototype [8], was an early starting point for the social metadata,  or paradata format. After discussion with the attention  metadata developers (e.g., [9]), the Com_para format was  modified. Then, in an effort to move to a JSON-based format, the  Activity Streams format (activitystrea.ms) was investigated and  ultimately extended for use in the Learning Registry: the  extension was to allow aggregations of data across actors or  resources. Our intent is to collect any and all data we can at any  grain size, without enforcing a specific vocabulary. Because we  are building a data store for analytics, in principle we are not  concerned with data volume or expiration.    The current plan is to solicit ratings data, such as rated, favorited,  and bookmarked. We are also encouraging the submission of  usage data, such as downloaded, viewed, or aligned to a standard.  Other activity data could be enhancing descriptive metadata, such  as commenting and tagging. The schema for social metadata is  [actor], [verb], [object] with [modifiers] allowed for [verb] that  are most commonly used for measures and dates.   A challenge in the submission of social metadata is that the  characterization of the actor is left to the publisher of the data.  This potentially creates a bias in the data and may limit the ability  to do relationship analysis. For example, if a publisher asserts a  student watched video Y, later analysis cannot determine, from  the generic student, grade-level, cultural background, or  municipality. Our strategy is to build early rudimentary analytics  to show the community what can be done with them, and to  encourage best practices in social metadata expression.   2.4 Retrieving Stored Data  The Learning Registry offers two core functions for data retrieval:  obtain and harvest. Obtain is used to gather all RDDs at a node, or  a subset based on the resource_locator present in the RDD.  Harvest is based on OAI-PMH Harvest [10] to gather RDDs or  payload data for specific date ranges. Additionally, the Slice  service allows users to retrieve documents based on properties of  RDDs. Slice is not a deep search of full metadata, paradata  (another name for social metadata), or the resource itself, but a  means of querying high-level properties included in the RDD  wrapper around such payloads including (1) identity of the  resource owner, metadata author, or submitter; (2) date of  submission, and (3) tags (including keywords, schema format,  and the resource data type, i.e., paradata or metadata). We face  challenges in our application of CouchDBs views in storing data.  Because CouchDB is not a relational database, it relies on  extensive indexing to populate views. As such, users would  submit data and not see it immediately via the Slice interface, due  to the slowness of updating views. This issue can be addressed by  managing user expectations or by developing alternatives to Slice  that update their views more quickly.   2.5 Special Tools and Issues  The Learning Registry community has expressed interest in  building out tools and services to connect various data collection  platforms to the Learning Registry: e.g., Basic Learning Tool  Interoperability (BLTI) interfaces for Learning Management  Systems (LMS); usage data from LMSs; social metadata from   portals and repositories; and descriptive metadata of interest to K- 12 teachers (e.g., alignment to the Common Core). We anticipate  these being built out over the coming year. As an example, the  Learning Registry has created an OAI-PMH data pump to  support a one-time extraction of Dublin Core and NSDL_DC  metadata from repositories that support an OAI-PMH harvest end  point. This script extracts, from the harvested metadata, various  field values to create tags (called keys in the RDD) for the  submitted metadata.    An issue for the Learning Registry is ensuring that valid  information is submitted. The Learning Registry thus requires that  documents submitted must be digitally signed using a key  signature. Our approach to identity [11] relies, at present, on a  PGP-based signature attached to the document. (Signing  submissions turned out to be difficult for some users, and we also  found the need to create a PGP key store for some users.)   3. INITIAL EXPERIMENTATION  Our first experiment with the Learning Registry replicated an  existing linkup between NSDL (nsdl.org) and a teacher portal.  NSDL provided an OAI-PMH protocol to transfer data to a  teacher content management system (CTE Online, cteonline.org)  that could harvest math resources and send usage data back. This  basic exchange was replaced with Learning Registry functionality  to demonstrate feasibility. This early proof of concept led us to  create the OAI data pump, which became a useful tool for data  extraction.   The Learning Registry beta version was opened to an early- adopter community in the September-October 2011 timeframe.  This was done to ensure that the concept was viable, to see what  problems people faced as they published and extracted their data,  and to learn how expressive they found the paradata specification.   To demonstrate the utility of data submitted, we created an  implementation of a search amplified by Learning Registry data.  The AMPS Chrome browser extension2 inspects the results of an  Internet search performed on google.com and then injects related  activity data and standards alignment data from the Learning  Registry. AMPS utilizes a simple mapping from resource URLs to  data stored/referenced in the Learning Registry (emphasizing the  importance of canonical URLs).   AMPS provided a good-proof-of-concept for showing how the  Learning Registry can be used to connect data from disparate  sources. For example, ISKME submits Achieve rubric data on  alignment of open resources to the Common Core Standards, and  ratings, e.g., for quality of assessment and interactivity. These  data can be correlated, via canonical URL, to ratings data from a  distant teacher portal and surfaced in a search, using this  extension.  Submitters wanted a visual way to see connections among  disparate data submitted. The Learning Registry Visual Browser  (demolearningregistry.sri.com/browse) provides a browser-based  interface to explore data. The user can enter a search term and the  browser queries the Learning Registry for documents containing  that term as a tag or identity. A list of summaries of these  documents is displayed, as an ordinary search engine might, but  the browser also displays a cloud of related terms, which allows  the user to easily explore the (semantically) nearby space of  documents in the Learning Registry. (The visual browser for                                                                        2 Available from https://github.com/jimklo/AMPS-Chrome   210          different early-adopters can be accessed from various community  pages on the learningregistry.org website.)   4. LEARNING ANALYTICS ON LR DATA  The alternatives to a system such as the Learning Registry are for  searchers to visit individual websites and repositories, for  organizations to build federated search or to make n-way  connections among resource providers, or for web crawlers to  scrape for descriptive and social metadata. As we have described,  these are all barriers to resource locating, sharing, and amplifying.  The Learning Registry data store provides a unique value to  learning-resource analytics that is now not possible. In this  section, we describe applications for learning analytics, and for  each, how Learning Registry data could be used.    Relationship Mining: The Learning Registry could surface  relationships between people based on their attention to  resources. What institutions, portals, or groups of users have  shared/curated the same resource    User knowledge modeling: Learning Registry data could be  used to compute what a student might be expected to know. In  contrast to the coarse characterization of ranges of grade levels  present in most metadata, Learning Registry assertions could be  specific about the grade or level at which resources were  successfully used.    User experience modeling (Are users satisfied): Learning  Registry ratings social metadata could be used to compute  satisfaction and thus provide feedback to developers.    User profiling (What groups do users cluster into): Inverting  the [actor] [verb] [object] syntax, we could compute the types of  actors who use resources. Submitters of social metadata can also  be clustered in categories based on how and when they submit.    Domain modeling (How is content decomposed into  components and sequenced): Curriculum construction tools  could gather data on sequencing of learning resources or  alignment to standards.    Trend analysis (What changes over time and how): Trends  in attention to different resources could be computed if a  sufficiently fine-grain size of social metadata is submitted,  because the social metadata specifies a date or date range.    Recommendations (What next actions/resources can be  suggested for the user): The Learning Registry can support  recommendations by clustering users or by building a social  network graph and then recommending resources among a  cluster or network.    Feedback, Adaption, and Personalization (What actions  should be suggested for the user How should the user  experience be changed for the next user): Learning Registry  data could provide feedback to developers about the utility of  their resources, about who adapts them and how, and could  eventually cause widespread sharing of learning resources to  learners at the appropriate time.   The Learning Registry community is now creating analytics-based  applications such as these using its unique timeline-organized  dataset.   5. ACKNOWLEDGMENTS  SRI Internationals work on this project is supported by the US  Department of Education (ED-04-CO-0040/0010). The open- source Learning Registry project was conceived by Steve  Midgley, US Department of Education, and Dan Rehak,   Advanced Distributed Learning Initiative (ADL), U.S Department  of Defense. Susan Van Gundy, UCAR and NSDL, developed the  first paradata specification and Aaron Silvers (ADL) amplified it  based on the activity streams specification. Joe Hobson of  Navigation North worked out many of the bugs in submitting  metadata and paradata into the Learning Registry. Pat Lockley  conceived of the Chrome search plugin. Technical support is  provided by Lockheed Martin Global Training and Logistics  under contract to the US Dept. of Defense Many other people and  organizations contributed input and submitted data for the early  proof-of-concept. A complete list of community members can be  found at www.learningregistry.org.    The Learning Registry code is stored in an open GitHub  repository, https://github.com/LearningRegistry/.   6. REFERENCES  [1] Soylu, A., Kuru, S., Wild, F., and Mdrichter F. 2008. e-  Learning and Microformats: A Learning Object Harvesting  Model and a Sample Application, In Proceedings Mupple'08  Workshop, 57-65.   [2] Chang, L. K., Liu, K-Y. Wu, C-A. and Chen, H-Y.  2005.  Sharing Web-Based Multimedia Learning Objects Using  NNTP News Architecture. Proceedings of the Fifth IEEE  International Conference on Advanced Learning  Technologies (ICALT05).   [3] Smith, J., M. Klein, and M. Nelson. 2006. Repository  Replication Using NNTP and SMTP. Research and  Advanced Technology for Digital Libraries, J. Gonzalo, et  al., Eds. Springer Berlin / Heidelberg, 51-62.   [4] Jesukiewicz, P. and Rehak, D. 2011. The Learning Registry:  Sharing Federal Learning Resources. Presented at the  Interservice/Industry Training, Simulation, and Education  Conference (I/ITSEC).   [5] NSDL. n.d. NSDL_DC Metadata Guidelines,  http://nsdl.org/contribute/metadata-guide   [6] LOM. 2002.  IEEE Standard for Learning Object Metadata,  IEEE Computer Society, September 2002.   [7] Fait, H., and Hsi, S. 2005. From Playful Exhibits to LOM:  Lessons from Building an Exploratorium Digital Library.  JCDL05, (June 711, 2005) Denver, Colorado, USA.   [8] NSDL n.d. Paradata.  http://nsdlnetwork.org/stemexchange/paradata   [9] Najjar, J., Meire, M. and Duval, E. Attention Metadata  Management: Tracking the use of Learning Objects through  Attention.XML. In Proceedings of World Conference on  Educational Multimedia, Hypermedia and  Telecommunications. (2005). 1157--1161.   [10] OAI-PMH, (2008).  The Open Archives Initiative Protocol  for Metadata Harvesting, V2.0,  www.openarchives.org/OAI/openarchivesprotocol.html   [11] Bienkowski, M. and Klo, J. 2011. Identity in the Federal  Learning Registry. Position Paper for W3C Workshop on  Identity in the Browser (Mountain View, CA, USA, May 24   25, 2011).  http://www.w3.org/2011/identity- ws/papers/idbrowser2011_submission_27.pdf.          211      "}
{"index":{"_id":"37"}}
{"datatype":"inproceedings","key":"Larusson:2012:MSP:2330601.2330653","author":"L'arusson, J'ohann Ari and White, Brandon","title":"Monitoring Student Progress Through Their Written Point of Originality","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"212--221","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330653","doi":"10.1145/2330601.2330653","acmid":"2330653","publisher":"ACM","address":"New York, NY, USA","keywords":"evaluation of student writing, information visualization, learning analytics, lexical analysis, pedagogical adjustment/intervention, recasting","Abstract":"This paper describes a new method for the objective evaluation of student work through the identification of original content in writing assignments. Using WordNet as a lexical reference, this process allows instructors to track how key phrases are employed and evolve over the course of a student's writing, and to automatically visualize the point at which the student's language first demonstrates original thought, phrased in their own, original words. The paper presents a case study where the analysis method was evaluated by analyzing co-blogging data from a reading and writing intensive undergraduate course. The evidence shows that the tool can be predictive of students' writing in a manner that correlates with their progress in the course and engagement in the technology-mediated activity. By visualizing otherwise subjective information in a way that is objectively intelligible, the goal is to provide educators with the ability to monitor student investment in concepts from the course syllabus, and to extend or modify the boundaries of the syllabus in anticipation of pre-existing knowledge or trends in interest. A tool of this sort can be of value particularly in larger gateway courses, where the sheer size of the class makes the ongoing evaluation of student progress a daunting if not otherwise impossible task.","pdf":"Monitoring Student Progress Through Their Written Point of Originality  Jhann Ari Lrusson Library and Technology Services  Brandeis University 415 South Street  Waltham, MA 02454 johann@brandeis.edu  Brandon White Department of English  University of California, Berkeley 330 Wheeler Hall  Berkeley, CA 94709 brandonw@berkeley.edu  ABSTRACT This paper describes a new method for the objective evalu- ation of student work through the identification of original content in writing assignments. Using WordNet as a lex- ical reference, this process allows instructors to track how key phrases are employed and evolve over the course of a students writing, and to automatically visualize the point at which the students language first demonstrates original thought, phrased in their own, original words. The pa- per presents a case study where the analysis method was evaluated by analyzing co-blogging data from a reading and writing intensive undergraduate course. The evidence shows that the tool can be predictive of students writing in a man- ner that correlates with their progress in the course and engagement in the technology-mediated activity. By visual- izing otherwise subjective information in a way that is ob- jectively intelligible, the goal is to provide educators with the ability to monitor student investment in concepts from the course syllabus, and to extend or modify the boundaries of the syllabus in anticipation of pre-existing knowledge or trends in interest. A tool of this sort can be of value par- ticularly in larger gateway courses, where the sheer size of the class makes the ongoing evaluation of student progress a daunting if not otherwise impossible task.  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.1 [Computer Uses in Education]: Collaborative learning, Computer-assisted instruction (CAI), Computer-managed in- struction (CMI), Distance learning  Keywords learning analytics, evaluation of student writing, recasting, pedagogical adjustment/intervention, lexical analysis, infor- mation visualization  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 12, 29 April - 2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM 978-1-4503-1111-3/12/04 ...$10.00.  1. INTRODUCTION Today, it is not uncommon practice for faculty to de-  ploy an online instructional environment in order to pro- mote student conversations over course materials and ideally to heighten comprehension and retention of course readings. Co-blogging is an example of a technology-mediated learning activity of this sort. Thebloggingpart requires students to explain course material in their own words. The co- part makes the students exchange ideas and interact regarding the course material by discussing the alternate viewpoints that emerge within the blogosphere [26].  One of the trends in the American university system over a number of years has been the migration towards larger so-called gateway courses [28]. These introductory courses are typically a students first exposure to collegiate work. However, these large classes have a negative impact on the students learning process. Large lectures are useful for cer- tain things, but less useful for fostering higher order thinking [31, 10]. Technology like co-blogging, however, can be used to make the class seem smaller, and provide the foundation for students to learn more and better.  For most if not all learning activities, a substantial amount of an instructors time and effort is devoted to evaluating and monitoring the quality of students work, and thus, hopefully, the depth of their learning [11]. The purpose of this monitoring, however, is not merely the determination of grades; part of the instructors work is entirely self-reflective, enabling the instructor to concurrently, or ideally even pre- emptively, intervene to make adjustments to course peda- gogy based on students engagement or understanding [29]. While assigning grades might be facile, some difficulties com- plicate this second objective: how might an instructor intuit when, precisely, students have understood the material suf- ficiently Making this determination manually, specifically in larger gateway courses, would prove an intensely labori- ous and time-consuming process, far more complicated than simple reading and re-reading of any single students work.  When supporting learning using technology, however, a positive by-product is that students produce their work in an electronic form, which enables the creation of computer- assisted instructional aids [22]. This paper describes an au- tomated solution that can be used by educators to help re- solve these tensions. Through the application of lexical anal- ysis to student writing, we have implemented an analysis tool that allows an instructor to track how a students writ- ten language migrates from mere paraphrase to mastery, iso- lating the moment when the students understanding of core  212    concepts best demonstrates an ability to place that concept in his or her own words, a moment that weve chosen to call the point of originality. This process recreates the same cognitive activity that educators might ordinarily undergo, yet in an automatic manner that is less labor-intensive. Ul- timately, the resulting data is presented to the instructor by way of a custom visualization, which allows for continuous self-monitoring with minimally expended effort.  In order to demonstrate the utility and validity of the analysis method, this paper explores the potential of the point of originality to preemptively predict likely student success or failure. The remainder of the paper is organized as follows. Section II explores the benefits of co-blogging in education. Section III and IV provide background on the particular problems of, and potential solutions for, evalua- tion in larger gateway courses. Section V and VI introduce the theory behind, and the specifics of, the point of origi- nality analysis method. Section VII and VIII report on a case study where the point of originality method was used to evaluate student co-blogging data. The paper ends by discussing future work.  2. CO-BLOGGING AND EDUCATION Co-blogging is an example of a social computing activity  that can be very conducive to learning [13, 26]. Overall, blogging provides a platform that promotes individual ex- pression, enables students to establish their own voice and yields a richer conversational interactivity within a commu- nity [48, 47]. Each student has a blog, composed of multiple blog posts. Students can read each others blog posts and comment on them. Because blogs are easy to use, they can promote students digital fluency [23] and encourage stu- dents to explore and publish their own nascent ideas under less pressure than in the rough-and-tumble of in-class dis- cussions [3].  Writing a blog forces students to become analytic and crit- ical as they contemplate how their ideas may be perceived by others [47]. Being able to review older contributions af- fords reflection and enables students to revisit and revise their artifacts, further developing their own viewpoints in the context of each others writing as they sense how others understand the material similarly or differently [37]. Con- versations emerge when students read, and then comment on, each others blog posts, thus enabling them to exchange, explore, and present alternate viewpoints on the course ma- terial [17]. This type of social explanatory discussion can benefit learning [12, 9].  Alternative spaces such as asynchronous discussion forums are another example of a technology that is sometimes used to mediate online discussions between students. However, as predominantly shared community spaces, forums give stu- dents voices that are heard but are without a distinct, indi- vidual identity [14]. Critical thinking may emerge for indi- viduals, but the organization does not promote the coherent and interactive dialogue necessary for conversational modes of learning in the same way that co-blogging does [44].  A blogosphere can function as a repository of information, opinions, monologues and dialogues about course content, where students participate, and leverage each others con- tributions in other educational activities (e.g., when writing term papers) [2, 1]. Blogging enables students to gather their thoughts and come better prepared for class [24, 12] and can be predictive of student performance in a course  [13]. Even non-active student bloggers can benefit from the blogs educational value as it exposes them to differ- ent views of the material without necessarily participating directly [47].  Overall, having students discuss and/orargueabout course readings has significant educational utility [5, 4, 38]. Some discussion might take place during class, however, class time is a limited resource. This is particularly true for larger classes or so-called gateway courses that typically enroll large numbers of undergraduates where there is simply not time for everyone to speak up. Using co-blogging, students can both express individual voices and continue conversing with their peers outside the confines of the physical class- room. Unfortunately, the sheer size of these courses presents several challenges.  3. PROBLEMS WITH LARGER GATEWAY COURSES  The ability to monitor and respond to student progress is ever more imperative given the realities of the modern class- room. As noted even a decade ago, the political economies of American universities increasingly mandate large class sizes, particularly in the introductory or gateway courses that are typically a students first exposure to collegiate work [28]. These large classes have a negative impact on students and instructors alike. There is, for example, an abidingly inverse correlation between class size and student achieve- ment [20, 42]. The large lecture, while useful for reinforcing rote facts, is less successful in fostering higher-order think- ing [31, 10], or in encouraging students to construct their own understanding of core concepts [28]. Such a sizable student population further constrains instructors abilities to familiarize themselves with students individual learn- ing styles [28], thereby forcing instructors to assume that their audience consists of uniform types of learners [10]. Al- though the extent of feedback that students receive is one of the most powerful predictors of student achievement [46, 40], instructor feedback in large lecture courses is often slow and sporadic; students typically need to wait weeks - from, for example, one midterm assessment to the next - to put their course-related skills into practice, and even longer than that to have their assignments evaluated by an instructor [10]. Pedagogical adjustments, in other words, become both more unwieldy and more unlikely in the precise environment where they would be most necessary.  Given the problems inherent to large lectures classes, but given also their entrenched status within the American uni- versity system, it would thus logically be prudent to find a way to minimize their most pernicious consequences. Any broader attempt to remedy the problems of larger gateway courses should thus aspire to first, foster higher-order think- ing; second, to suit multiple types of learning styles; and third, to provide students with feedback as rapidly as pos- sible. These first two objectives are inherent virtues of the co-blogging process (e.g. [24, 12, 37, 9, 5, 4, 38]); the final objective is the focus of this paper.  4. PRIOR EFFORTS Several attempts to minimize the unintended consequences  of large gateway courses exist. Almost all efforts call for re- sizing the large class group, either by literally subdividing the class or else by designing activities to make the large  213    class seem small. This latter method, it might be argued, is the one already pursued by student participation in a co- blogging environment, where conversations take place in an ad hoc and freeform manner.  Known interventions can be roughly classified into two major groups: those interventions that are specifically meant for in-class use, and those interventions that are intended to take place between classes. Those activities that take place during class typically interrupt the lecture itself [32, 35, 8], asking students, for instance, to respond to a se- ries of prompts which they answer through remote devices. These same activities, however beneficial, generally disrupt the actual process of knowledge transmission and tend to re- ward rote memorization rather than higher-order thinking; what feedback students receive reflects only whether or not they got a prompt right or wrong, and not how well or how comprehensively they understood the material. Since the activities take place in the classroom, and in front of the en- tire student population, the activities themselves moreover treat all students in exactly the same manner regardless of learning style.  Interventions intended for use between classes are roundly invested in providing instructors with observable statistical modeling in near real-time [39, 8, 19], which can then be re- ferred to before the next session. These activities typically attempt to breed higher-order thinking by forcing students to reflect on their own learning, asking, for instance, that students rate their level of confidence before responding to prompts [8], or that they engage in a collaborative peer re- view of one anothers written work [39, 19, 21]. The benefits of this type of activity are directly analogous to the benefits of co-blogging.  5. ORIGINALITY IN STUDENT WRITING When students engage in a writing activity, the final eval-  uation of their work cannot only assess whether or not the student has provided the most closely correct answer. Pro- cess is just as relevant to student writing as content [43]. Student writing that exhibits exceptional higher-order think- ing is generally seen as that which demonstrates a mastery of the course material in new, profound or statistically un- usual ways [33]. The ideal is not only for students to confirm that theyve understood lectures, but to do so in ways that even the educator might not have thought of. This process of mastery need not take place all at once. As a student is continually exposed to the same material, or is given the independent opportunity to rethink, reframe, or revisit that material [45], their writing on the subject has the chance to evolve, from rote regurgitation to wholly original expres- sion [34]. At the level of language, this evolution is reflected through recasting.  Recasting is the learning process whereby a student refines his or her understanding of a concept found in course lec- tures or readings by putting that concept into his or her own words [41]. In the acquisition of new languages especially, this process can be useful, because it allows students to ac- quire new vocabulary using the assortment of words already available to them [41, 30]. Even where the students under- standing of a language is not an explicit concern, recasting can mark a students attempts to graduate to more sophis- ticated or professionalized terminology, or, inversely but to the same end, to place new concepts into terms that are nearer to what the student would naturally be more likely  to say [15]. Originality, fully defined, can of course take numerous forms. The concept of recasting, however, spans a number of theoretical orientations, with an influence on theories of schema formulation [25], the sensemaking process known as scaffolding [18], as well as the express principles of educational constructivism [27].  Originality, as deployed here, does not therefore strictly refer to a students creativity or capacity for non-conformity as might be suggested by more colloquial uses of the term. Rather, what the Point of Originality tool seeks to gauge is students ability to interpret, to place core concepts into new and diffuse usages. This definition of originality strad- dles the tiers of learning that Blooms taxonomy [6] asso- ciates with understanding and application. By interpret- ing core concepts and extrapolating to different terms, stu- dents demonstrate their understanding of the material, and when putting those concepts into play through iterative ex- ercises like co-blogging, they begin to apply that knowledge is newer and more diversedomains.  For an instructor, the simple identification of recast ter- minology within a students written work can provide an effective barometer for pedagogical self-reflection. If a sub- set of terms or concepts are deemed vital to the syllabus, repetitions and recast iterations of those same terms will at least suggest that those terms are being acknowledged and reflected upon. Although the presence of recast terminology is not the only metric representative of a students mastery, the central role that recasting plays in a host of pedagogies (e.g. [25, 18, 27]) suggests that writing demonstrating high or low levels of recasting will reflect other aspects of per- formance within the course. Yet if the instructor hopes not only to identify instances where key concepts are deployed, but to determine how comprehensively the concepts are be- ing internalized, it is first necessary to possess a method of scoring how original any given recast might be. In order to do this, we have developed a metric for isolating a specific point of originality within student writing.  6. EVALUATION OF CO-BLOGGING The process of computer-assisted evaluation of student  writing is primarily composed of two parts: the analysis method, and a custom-made visualization depicting each students originality at any given time throughout the du- ration of the semester.  6.1 Analysis Method: Theoretical Background WordNet is a lexical database that arranges nouns, verbs,  adjectives, and adverbs by their conceptual-semantic and lexical relationships [16]. Whereas a simple thesaurus would be able to identify any two words as synonyms or antonyms of one another, WordNet is able to note the similarity be- tween two words that dont have literally identical mean- ings. These relationships are ideally meant to mirror the same lexical associations made by human cognition.  WordNets arrangement is hierarchical, which is to say that certain terms are more closely related than others. Within WordNet, these relationships are displayed assynsets,clus- ters of terms that fork, like neurons or tree branches, from more specific to more and more diffuse associations (see Fig- ure 1). If two words are found within one anothers synset tree, it stands to reason that these terms are, in some way, related, be it closely or distantly. As discussed in the next sub-section, these distances between two terms can be calcu-  214    lated, and assigned a value commensurate with their degree of semantic relatedness [7].  Figure 1: Model synset tree (by hyponym relation)  The hierarchical arrangement inherent to WordNet pro- vides one method of determining the relationship between two terms. If the synset tree of one term encompasses an- other term, it is simple enough to note how many synset jumps it takes to move from one to another. In Figure 1, a Dalmatian is a type of dog, which itself belongs to the subcategory of domestic animals; thus there are two tiers of associations between the concepts of Dalmation and domestic animals. Unfortunately, however, just how closely any two terms might be related is not a purely lin- ear relationship. WordNet organizes related terms by their precise lexical entailment, such that nouns might be cate- gorized as synonyms, hypernyms, hyponyms, holonyms and meronyms, as seen in Table 1.  These possible entailments provide a rudimentary roadmap for all the ways in which two words might be related. Since WordNet attempts to map the cognitive associations auto- matically formed between words [16], a students evocation [36] of the holonym or hypernym of a given noun instead of the noun itself is more likely to form an associative recast of the original term.  To put these abstract concepts into the same terms used to describe the original problem, if an instructor in a large lecture course in animal biology wanted to monitor how stu- dents had grappled with the (admittedly basic but obvi- ously fundamental) concept animal, he or she would want to know not only that the students had deployed the literal term animal, but were also conversant in the other asso- ciated concepts found (in immensely abbreviated form) in Figure 1. This association is consistent with the pedagogi- cal principle of recasting, and with the concept of original expression as defined here. Where an instructor wants stu- dents to know one thing - in this case, about animals - that those students can deploy more diffuse and more con- crete examples of animals would be the readiest evidence that they actually understand.  Yet while this simple index displays just how any two terms might be related, all the possible relationships noted are not necessarily equal. Some relationships, like that be- tween synonyms smile and grin, are obviously bound to be more strongly associated than that between mammal and dog. Following a method first noted by Yang & Powers [49], it is possible to install a series of weights that can best cal-  culate the semantic distance between any two terms. This method in particular is useful because of all known methods, it bears the highest correspondence between its own distance calculations and the intuitions of actual human respondents (at 92.1 percent accuracy).  Synonym: X is a synonym of Y if X means Y Example: {smile,grin}  Hypernym: X is a hypernym of Y if every X is a kind of Y  Example: {dog,mammal} Hyponym: X is a hyponym of Y if every Y is a  kind of X Example: {mammal,dog}  Holonym: X is a holonym of Y if Y is part of X Example: {hand,finger}  Meronym: X is a meronym of Y if X is part of Y Example: {finger,hand}  Table 1: Possible lexical entailments for nouns in WordNet  6.2 Analysis Method: Implemention Determining the point of originality of a students blog  post depends upon the manual input of a specific query term by the instructor. The term relates to a key course topic and manual input of the topic reinforces the pedagog- ical utility of the process. For the query term, the process generates a WordNet synset tree. Words within the tree are then compared to the body of words extracted from a stu- dents blog post. Where matches are found, a summation of distance calculations between the original query term and the matches is performed as follows:  Let q be a query term supplied by the instructor. Then, let W = {w0, w1, ..., wn} be a set containing all synset word matches (w) from the WordNet database for q.  Let B = {b0, b1, ..., bn} be a set of all words composing a blog post by a particular student and let S = {s0, s1, ..., sn} be a set of stopwords, a list of common words in English usage (like the or and), to be omitted to speed up pro- cessing time. Then, M = {m0,m1, ...,mn}, the set of synset term matches found in a blog post for query term q can be defined as:  M = W  (B  S) (1)  WordNet stores synset matches in a tree structure with q as the root node. Then, , the distance (depth) for any given synset match (m  M) from the root node (query term q) is defined as:   =   0 if m = q  1 if m is first child of q  2 if m is second child of q      (2)  WordNet also supplies the lexical entailment of each synset term. Thus, t, the word type of any given synset term  215    match m M , is defined as:  t =   1.0 if m = q  0.9 if m = synonym/antonym  0.85 if m = hypernym/hyponym  0.85 if m = holonym/meronym  (3)  Then , the weight of any given synset term match is calculated as:   = (  0.7) t (4)  The depth for any given synset term is multiplied by a constant value of 0.7, which reflects the diminished associa- tions between two terms the farther separated they are along the synset tree. This value is selected because it corresponds with the calculation of distance between terms that yields the nearest match with human intuition [49].  Then, C, the cumulative originality score for a given query term q in a students blog post, can be defined as:  C(q) =  |M| n=0  n (5)  The point of originality for a particular course topic is in many cases defined by the presence of several related query terms, or in other words, the synset matches for those terms. By defining Q = {q0, q1, ..., qn} as the set of query terms sup- plied by the instructor at any one time, then P, the overall point of originality of a given students blog post for a par- ticular course topic (defined by Q), is:  P (Q) =  |Q| n=0  C(qn) (6)  Finally, repeating the point of originality calculation (Equa- tion 6) for each blog post written by a particular student, and plotting all instances of originality on a horizontal time- line, allows for an optimal instruction comprehension whereas the instructor can see recasts of a particular course topic (defined by Q) across the entire body of a students writing throughout a single course.  Although this paper focuses on the analysis of blog posts as students writing examples, given some additional pro- gramming work, any electronic form of student writing could be made compatible with the tool for subsequent analysis.  6.3 Visualization for the Point of Originality The timeline visualization, as seen in Figure 2, displays  a horizontal timeline that represents the time interval for the writing activity of any student for the duration of a particular semester. The numbered components of Figure 2 correspond to the following features.  1. This drop-down menu allows the instructor to select which students writing samples are currently being displayed.  2. This is where query terms (Q) are input by the in- structor.  3. This timeline displays the date/times of each of the students writing samples. Each marker is color-coded, from colder to warmer colors along the ROYGBIV spectrum, the higher the value of the point of origi- nality (P) score for any given writing sample. These  color assignments present an intuitive way for the in- structor to quickly recognize that the sample has been assigned a higher originality value.  4. If a writing sample marker is selected in the timeline window (see inset 3), the text of that writing sample is displayed here.  This assortment of visualization options allows the point of originality calculation to be displayed in a number of intu- itive ways: both within chronology (inset 3) and in context (inset 5).  7. CASE STUDY This section reports on a case study that explores the ca-  pability of using the Point of Originality tool to assess the originality of student writing in a semester-long co-blogging activity. More specifically, the study focuses on correlating originality scores assigned to students blog posts to their ac- tivities in the blogosphere during the semester and the final grades assigned to a term paper covering the same topics. Although primarily aimed at testing the validity of the point of originality method, this study models a likely use case. By demonstrating how low point of originality values corre- spond to poor performance in other aspects of the course, the Point of Originality tool could provide instructors with an early, near-instantaneous diagnostic of which students might require additional help. The tool might thus ideally streamline the process of conducting targeted pedagogical adjustments or interventions.  The co-blogging data was collected from a course taught in the Fall of 2008 in the Computer Science Department at Brandeis University. The course is an introductory course, an elective, focused on exposing students to topics such as the social life of information, virtual communities, privacy, intellectual property and peer-to-peer computing.  In the co-blogging activity, each student has a blog where he or she writes opinions on the course readings. Students can read each others posts and comment on the posts of their peers. The blogosphere, provides several features fo- cused on increasing students awareness of recent activity, and enabling them to find interesting blog posts to read and conversations in which to participate.  7.1 Participants There were 8 female and 17 male students, all undergrad-  uates, enrolled in the class. There were 3 science majors and 1 science minor in the class. There were 12 students majoring in the social sciences and 8 minoring in the social sciences. The remainder of the class was either in the hu- manities or fine arts. Three students were omitted from the data set because they did not begin blogging until the end of the semester following a warning from the instructor.  As an introductory course, open to non-majors, the tech- nical requirements for enrollment were few. No formal eval- uations were done to assess the students computer literacy or prior domain knowledge. In class discussions, most of the students expressed moderate or advanced technical skills.  The instructor and teaching assistant did not design, or implement, the co-blogging activity in such a way that it pre-assigned students into particular authoring roles in the online blogosphere, thus potentially influencing the students choice of writing topics or styles.  216    Figure 2: The Point of Originality timeline visualization  7.2 Procedure At the beginning of the semester, an in-class tour and ex-  ercise introduced the students to the important features of the co-blogging environment. The students were required to blog at the pace of one post per lecture: there were two lectures per week. A typical post was 1 or 2 paragraphs in length. The students were also required to read and com- ment on other contributions to the blogosphere. The co- blogging work of each student counted for 35% of the final grade.  During the semester, the students read four books and wrote a paper on one of these books. The focus of the anal- ysis presented in this paper is on the co-blogging work that the students did during the time the class read the book for which they wrote their papers.  7.3 Metrics Lectures were presented using slides that summarized the  key points of the presentation. At the beginning of each lecture, hard copies of the slides were handed out to support student note taking. We used the lecture slides as a basis for identifying the inputs to the blogosphere. For each set of slides, a set of key topics that were covered by the lecture ultimately became the query terms used for analysis.  7.4 Method All of the students online work was automatically recorded  in a transcript and analyzed using the Point Originality tool. Originality scores were generated for all blog posts and papers, which were then correlated to students final paper grades and to statistical data summarizing their read- ing and writing activities during the co-blogging part of the semester.  8. RESULTS The analysis was composed of two principle parts. The first part compared the degree to which the tool indi-  cated the originality of the students blog posts and how well the originality scores related to the grades that the instruc- tor assigned their papers. In the ideal situation, given that the instructor graded their papers based on how well the students expressed higher order understanding of the course material, or in other words their writing reflected original  thought, the tool should provide scores where higher origi- nality values would correspond to higher paper grades.  The second part sought to explore to what degree the stu- dents interactivity in the blogosphere influenced their un- derstanding of the course readings, and in what way their immersion in the co-blogging community positively or neg- atively impacted their levels of originality when writing pa- pers. Ideally, students would find sufficient impetus to be- come deeply involved in the co-blogging learning community and their exposure to alternate or similar viewpoints of the same materials would help them develop their own view- points or to strengthen existing ones, thus leading to more original thought and better papers.  Since the analysis was primarily concerned with ensuring that the tool could be used during a course to preemptively diagnose likely student success, the blog post dataset was fil- tered to only include blog posts written in what was defined as the lead-in period of co-blogging. During this period, the students were writing blog posts and comments on the topics that they eventually wrote their papers on, but at the time were unaware which specific topics they would have to address in those papers. The paper grades were assigned during the fall of 2008, roughly two years prior to the study described in this paper. Furthermore, grading was done by the course instructor, who is not a participant in the Point of Originality project.  8.1 Originality in the lead-in period We began by collecting the originality scores calculated  by our system for the blog posts written by each student on the paper topics and the actual grades that each student received for his or her paper. The average grade for student papers was 80.00 with a standard deviation of 16.83. The highest grade assigned was 95 and lowest was 40 on a scale from 0 to 100. The students blog posts received on average an originality score of 10.61 with a standard deviation of 4.29. The highest originality score assigned by our system was 18.30 whereas the lowest score was 3.92.  Soon, a pattern emerged indicating that the more original the students co-blogging work, the higher the paper grades assigned by the instructor. While this is to be expected, the importance here is that the Point of Originality tool is automatically producing results that potentially correlate to  217    standard approaches to pedagogy. A chi-square distribution test confirmed that there was indeed a positive correlation between the two factors. As students blog post originality scores increased, their final paper grades covering the same topics increased as well. In other words, as their blogging activity became more original, the students wrote better pa- pers:  c2(20, N = 22) = 0.492, p = .05 (7)  To further confirm the potential relationship between orig- inality while initially learning the course materials (during the lead-in period) and how well that work transformed into mastery of course content as reflected by paper writ- ing, students were divided into two groups based on their paper grades. Students whose paper received a grade above the average (80.00) were assigned to one group, the upper group, whereas students who scored below the average were assigned to the lower group.  Metric AVG SD SEM N Above average grade Paper grades 90.63 3.20 1.13 8 Originality variance -6.10 21.92 7.75 8  Below average grade Paper grades 66.79 15.14 4.05 14 Originality variance 21.49 27.63 7.38 14  Table 2: Originality variance and paper grades for two different groups of students  As shown in Table 2, the students in the upper group re- ceived an average grade of 90.63 on their papers whereas the students in the lower group received an average grade of 66.79. What is more interesting, however, is what can be defined as the originality variance: the difference between how original the students blog posts were compared to their final papers. While the lower student group had an origi- nality variance of 21.49, the variance for the students in the upper group was -6.10.  Because the variance for the upper group is negative, those students blog posts, written during the lead-in period, were on average more original than their final papers. It might seem then that those students were not necessarily more original than the students in the lower group, however, that is not the case. The fact that the variance is negative for the upper group is indicative of the fact that those students were at the height of their understanding of the materials even during the lead-in period. These students had mastered the materials in such a way that they had an easier time of writing their papers, whereas the students in the lower group were only first beginning to wrestle with this content after the papers were assigned. This is suggested by the fact that the originality variance for the lower group was a positive value of 21.49, a value more than twice as great as the students average originality score during the entire period.  A t-test of independent samples confirmed that the orig- inality variance between the upper and lower groups was indeed statistically significant. Students who had received higher grades for papers wrote blog posts that were more  original in the lead-in period:  t(20) = 2.42, p < .02 (8)  Similarly, a t-test also confirmed that the students distri- bution of grades was by itself significant:  t(20) = 4.35, p < .0003 (9)  The key observation is whether or not students retention of course materials was equal for both groups. Students that master materials when taking exams dont necessarily have the ability of applying that knowledge after the course ends because their grasping of the content was short lived. These students knew the material well enough to pass the exam but not necessarily well enough to be able to easily apply that knowledge later on. If students can get into the game earlier in the semester, they have greater opportuni- ties to participate in discussions, refine their understanding and lock it down deep so that they leave the course with a higher degree of mastery.  In a large reading- and writing-intensive course, where a bulk of the work towards mastery might take place in machine-readable form, it goes without saying that it would be advantageous for the instructor to be able to use technol- ogy to monitor each students progress. Specifically in larger gateway courses, where the odds are already stacked against student achievement and the need for interventions is more difficult to spot, students who fail to integrate completely with the class community - either because their experience comes from another discipline, or because they simply arent accustomed to the specific class environment - are likely to suffer poor performance. Having the ability to assess stu- dents mastery of the material, however, would enable the instructor to identify those students who are perhaps strug- gling or only falling behind, and to intervene to correct the students performance.  8.2 Interactivity in the blogosphere In an online technology-mediated community like the one  described in this paper, students benefit from the expo- sure to both similar and contrasting viewpoints of the same course material. If the students deep emersion in the co- blogging activity has a positive impact on their learning, one can assume that the originality score would correlate with the degree to which each student participates online. In other words, for those students that take advantage of the technology-mediated activity, frequently reading other stu- dents viewpoints and partaking in thoughtful conversations about the course readings, then originality scores should cor- relate with positive student outcomes.  To assess student participation in the blogosphere, each students exposure (reading blog posts and comments by others) and contributions (writing blog posts and comments oneself) were measured. These activities were then corre- lated with the originality scores assigned to each students paper. Table 3 summarizes these metrics.  Overall, the student papers received an average original- ity score of 53.49, with a standard deviation of 14.53. The highest originality score was 93.76, whereas the lowest score was 31.66.  In terms of exposure in the blogosphere, the average num- ber of times that a student was exposed to other students contributions was 4.36, with a standard deviation of 3.93. The highest number of contributions read by a student in  218    Metric AVG SD SEM N Paper originality score 53.49 14.53 3.10 22 Exposure 4.36 3.93 0.84 22 Contributions 4.18 2.17 0.746 22  Table 3: Originality and interactivity in the blogo- sphere  the blogosphere was 14, whereas one student read no con- tributions by the class at all. A chi-square test was used to explore the potential correlation between the originality of student papers and the degree of each students exposure in the blogosphere. As shown in Equation 10, there is a statistically significant positive correlation between the two factors. In other words, higher exposure in the blogosphere led to more original papers.  c2(20, N = 22) = 0.44, p = .05 (10)  In terms of contributing in the blogosphere, each student made on average 4.18 contributions during the lead-in pe- riod, with a standard deviation of 2.17. The highest num- ber of blog posts and comments written by a student was 9, whereas the lowest number of contributions was 1. As before, a chi-square test confirmed that there was a statisti- cally significant positive correlation between the number of contributions a student makes in the blogosphere and the eventual originality of his or her paper.  c2(20, N = 22) = 0.42, p = .05 (11)  9. CONCLUSION Integrating technology into higher education curricula to  extend the physical boundaries of the classroom can be of significant value, as it enables students to interact and learn outside of class time. This is particularly true in larger gate- way courses, where there are fewer opportunities for students to engage in higher order thinking and to construct their own understanding of core concepts. While the introduction of technology like co-blogging can create a successful learning experience, the large number of students creates additional noise that makes it harder for instructors to isolate the stu- dents most in need of help. This paper described a method and tool by which student writing can be automatically an- alyzed to determine whether or not students have reached a point of originality in their writing, reflecting mastery of the course content. The paper presented a case study where the tool was used to analyze co-blogging data collected from an interdisciplinary reading- and writing-intensive course. The evidence showed that the tool was generating original- ity scores for students blog posts that correlated both with the degree to which they participated in the online activity as well as the final grades that they received for their term papers. In other words, students who were more original during their co-blogging wrote better papers, and students who took advantage of the technology were more original.  Although the paper was primarily aimed at confirming the validity of the Point of Originality method, these findings suggest a likely use case for the technology. In a large class, where students engage both in iterative writing assignments like co-blogging and in summative writing assignments like midterm essays, an instructor might employ the Point of Originality tool at regular intervals throughout the semester  to see which students are utilizing recast terminology in their work. Given the correlation seen here between a students ability to recast key concepts and their eventual performance on an assessment of those concepts (see Equation 7), the in- structor could essentially use the tool to identify students with low point of originality values, and thus those most likely to do poorly on the assessment. Especially in large gateway courses, with potentially hundreds of students pro- ducing iterative assignments during the lead-in period, the process of identifying student learning styles and responding to student work is unwieldy [28, 10]. This strategy would allow an instructor to identify problems before it is too late: to determine which students might be struggling, to begin to isolate why, and to implement adjustments to pedagogy accordingly.  It goes without saying that any tool of this sort might give a skewed measure for some students. For example, in any given class, some students simply learn best through face- to-face participation in class discussions while others accrue the highest learning benefit through solitary reflective writ- ing outside lecture hours. However, this does not reduce the merit or applicability of learning analytics tools such as the Point of Originality. On the contrary, if the intan- gibility of a technology-mediated learning activity is what makes any kind of evaluation or monitoring difficult, even in smaller classes, then the production of tools that can assist the teacher in performing these activities would be a signifi- cant boon. It is perhaps better to consider tools of this sort to be part of a larger arsenal of assistive devices, where one can pick-and-choose the tools most appropriate for the needs of a particular instructor, student, learning activity or course, whether it be for exploring textual content, activity logs or other types of data. It is the ability to be able to conduct any diagnoses at all, with a tailor-made analytics set, which is the primary benefit.  10. FUTURE WORK The tool is currently being used to analyze even larger  gateway courses that typically enroll over 90 students. Ad- ditional features are also being developed to combat two not necessarily common but plausible anomalies where the composition of the writing examples themselves can produce false positives of originality.  First, within the same writing sample, a student might (in- advertently) repeatedly use a word that not only is a synset match, but a match that yields a particularly high  value (see Equation 4). Therefore, an unreasonable degree of orig- inality might be suggested for a particular writing sample. Currently, development is under way for a decay factor, that once enabled will gradually decrease the weight of the  value for a particular synset match, given how many times it has appeared before in the sample. The first mention gets the maximum weight, where the nth mention receives the relative lowest possible weight.  Second, where the evaluation of originality of a particu- lar course topic depends on the presence of synset matches of multiple query terms within the same writing sample, the set Q (see Equation 6), then the distance between those matches within the text may also be significant. For exam- ple, in response to a query for the compound term color blindness, the occurrence of a synset match for the word color in the first paragraph of a writing sample may be otherwise unrelated to a synset match for blindness four  219    paragraphs later. By implementing a distance factor, it will be possible for the instructor to specify a maximum distance (in terms of character, word, or paragraph count) between any two related synset matches in order for their  values to be included in the final originality calculations for a given writing sample.  11. ACKNOWLEDGMENTS Special thanks to Thanya Rajkobal for her contributions  to the Point of Originality analysis tool and to the students in the course for providing data for this research project.  12. REFERENCES [1] R. Alterman and J. A. Larusson. Collaborative  sensemaking in the blogosphere. Technical Report CS-09-272, Brandeis University, Department of Computer Science, 2009.  [2] R. Alterman and J. A. Larusson. Modeling participation within a community. In N. A. Taatgen and H. van Rijn, editors, Proceedings of the 31st Annual Conference of the Cognitive Science Society, pages 16801685. Cognitive Science Society, Austin, TX, 2009.  [3] S. L. Althaus. Computer-mediated communication in the university classroom: An experiment with on-line discussions. Communication Education, 46(3):158  174, 1997.  [4] J. Andriessen. Arguing to learn. In R. K. Sawyer, editor, The Cambridge handbook of the learning sciences, pages 443459. Cambridge University Press, New York, NY, 2006.  [5] J. Andriessen, M. Baker, and D. Suthers. Arguing to learn: confronting cognitions in computer-supported collaborative learning environments. Springer, June 2003.  [6] B. Bloom. Taxonomy of educational objectives: The classification of educational goals . Longmans, Green, New York, 1956.  [7] J. Boyd-Graber, D. Blei, and X. Zhu. A topic model for word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 10241033, 2007.  [8] C. A. Brewer. Near real-time assessment of student learning and understanding in biology courses. BioScience, 54(11):10341039, 2004.  [9] M. T. H. Chi and K. A. VanLehn. The content of physics self-explanations. Journal of the Learning Sciences, 1(1):69105, 1991.  [10] J. L. Cooper and P. Robinson. The argument for making large classes seem small. New Directions for Teaching and Learning, 2000(81):516, 2000.  [11] T. J. Crooks. The impact of classroom evaluation practices on students. Review of Educational Research, 58(4):438481, Dec. 1988.  [12] A. Deitering and S. Huston. Weblogs and the middle space for learning. Academic Exchange Quarterly, 8(4):273  278, 2004.  [13] H. S. Du and C. Wagner. Learning with weblogs: An empirical investigation. In Proceedings of the 38th  Annual Hawaii International Conference on System Sciences (HICSS 05), page 7b, Washington, DC, 2005. IEEE Computer Society.  [14] P. Duffy. Engaging the YouTube Google-Eyed generation: Strategies for using web 2.0 in teaching and learning. In The Electronic Journal of e-Learning, volume 6, pages 119130, 2008.  [15] B. Eilam. Phases of learning: ninth graders skill acquisition. Research in Science & Technological Education, 20(1):5, 2002.  [16] C. Fellbaum. WordNet: An electronic lexical database. The MIT press, 1998.  [17] R. E. Ferdig and K. D. Trammell. Content delivery in the blogosphere. T.H.E. Journal Online: Technological Horizons In Education, 31(7):1216, 2004.  [18] J. P. Gee and J. L. Green. Discourse analysis, learning, and social practice: A methodological study. Review of Research in Education, 23:119169, 1998.  [19] R. D. Gerdeman, A. A. Russell, and K. J. Worden. Web-Based student writing and reviewing in a large biology lecture course. Journal of College Science Teaching, 36(5):7, 2007.  [20] G. V. Glass and M. L. Smith. Meta-Analysis of research on class size and achievement. Educational Evaluation and Policy Analysis, 1(1):216, Jan. 1979.  [21] I. M. Goldin, K. D. Ashley, and R. L. Pinkus. Teaching case analysis through framing: Prospects for an its in an ill-defined domain. In R. Nkambou, E. Mephu Nguifo, and P. Fournier-Viger, editors, 8th International Conference on Intelligent Tutoring Systems, pages 395405, 2008.  [22] L. Greer and P. J. Heaney. Real-time analysis of student comprehension: An assessment of electronic student response technology in an introductory earth science course. Journal of Geoscience Education, 52:345351, 2004.  [23] D. Huffaker. The educated blogger: Using weblogs to promote literacy in the classroom. AACE Journal, 13(2):9198, 2005.  [24] Y. Juang. Learning by blogging: Warm-Up and review lessons to facilitate knowledge building in classrooms. In Proceedings of the Eighth IEEE International Conference on Advanced Learning Technologies, ICALT08, pages 574575, Washington, DC, 2008.  [25] F. Korthagen and B. Lagerwerf. Levels in learning. Journal of Research in Science Teaching, 32(10):10111038, 1995.  [26] J. A. Larusson and R. Alterman. Wikis to support the collaborative part of collaborative learning. International Journal of Computer-Supported Collaborative Learning, 4(4):371402, 2009.  [27] M. Lebrun. Des technologies pour enseigner et apprendre. De Boeck, Bruxelles, 2nd edition, 1999.  [28] J. MacGregor. Restructuring large classes to create communities of learners. New Directions for Teaching and Learning, 2000(81):4761, 2000.  [29] L. McAlpine, C. Weston, D. Berthiaume, G. Fairbank-Roch, and M. Owen. Reflection on teaching: Types and goals of reflection. Educational Research and Evaluation: An International Journal on  220    Theory and Practice, 10(4):337, 2004.  [30] K. McDonough and A. Mackey. Responses to recasts: Repetitions, primed production, and linguistic development. Language Learning, 56(4):693720, 2006.  [31] W. J. McKeachie and N. Chism. Teaching Tips. DC Heath, 1986.  [32] A. Mills-Jones. Active learning in IS education: Choosing effective strategies for teaching large classes in higher education. In Proceedings of 10th Australasian Conference on Inofrmtion Systems, pages 113, 1999.  [33] M. T. Moore. The relationship between the originality of essays and variables in the Problem-Discovery process: A study of creative and noncreative middle school students. Research in the Teaching of English, 19(1):8495, Feb. 1985.  [34] N. Nelson. Writing to learn: One theory, two rationales. In P. Tynjala, L. Mason, and K. Lonka, editors, Writing as a Learning Tool: Integrating Theory and Practice, pages 2336. Kluwer Academic Publishers, Dordrecht, The Netherlands, 2001.  [35] D. Nicol and J. Boyle. Peer instruction versus class-wide discussion in large classes: A comparison of two interaction methods in the wired classroom. Studies in Higher Education, 28(4):457473, Oct. 2003.  [36] S. Nikolova, J. Boyd-Graber, and C. Fellbaum. Collecting semantic similarity ratings to connect concepts in assistive communication tools. Modelling, Learning and Processing of Text-Technological Data Structures, ser. Springer Studies in Computational Intelligence. Springer, 2009.  [37] J. A. Oravec. Bookmarking the world: Weblog applications in education. Journal of Adolescent & Adult Literacy, 45(7):616621, 2002.  [38] A. Reznitskaya, R. C. Anderson, B. McNurlen, K. Nguyen-Jahiel, A. Archodidou, and S. Kim. Influence of oral discussion on written argument. Discourse Processes, 2001.  [39] R. Robinson. Calibrated peer reviewTM: an application to increase student reading & writing skills. The American Biology Teacher, 63(7):474480, 2001.  [40] B. Rosenshine and C. Meister. Scaffolds for teaching higher-order cognitive strategies. Teaching: Theory into practice, pages 134153, 1995.  [41] M. Shih. Content-Based approaches to teaching academic writing. TESOL Quarterly, 20(4):617648, Dec. 1986.  [42] M. L. Smith and G. V. Glass. Meta-Analysis of research on class size and its relationship to attitudes and instruction. American Educational Research Journal, 17(4):419433, Dec. 1980.  [43] B. P. Taylor. Content and written form: A Two-Way street. TESOL Quarterly, 15(1):513, Mar. 1981.  [44] M. J. W. Thomas. Learning within incoherent structures: the space of online discussion forums. Journal of Computer Assisted Learning, 18(3):351366, 2002.  [45] P. Tynjala, L. Mason, and K. Lonka. Writing as a Learning Tool: Integrating Theory and Practice. Kluwer Academic Publishers, Dordrecht, The  Netherlands, 1 edition, 2001.  [46] H. J. Walberg. Improving the productivity of americas schools. Educational Leadership, 41(8):19, May 1984.  [47] J. B. Williams and J. Jacobs. Exploring the use of blogs as learning spaces in the higher education sector. Australasian Journal of Educational Technology, 20(2):232247, 2004.  [48] L. Wise. Blogs versus discussion forums in postgraduate online continuing medical education. 2005. Paper presented at Blogtalk Downunder Conference, May 19-22, 2005, Sidney Australia.  [49] D. Yang and D. M. Powers. Measuring semantic similarity in the taxonomy of WordNet. In Proceedings of the Twenty-eighth Australasian conference on Computer Science-Volume 38, page 322, 2005.  221      "}
{"index":{"_id":"38"}}
{"datatype":"inproceedings","key":"McNely:2012:LAC:2330601.2330654","author":"McNely, Brian J. and Gestwicki, Paul and Hill, J. Holden and Parli-Horne, Philip and Johnson, Erika","title":"Learning Analytics for Collaborative Writing: A Prototype and Case Study","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"222--225","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330654","doi":"10.1145/2330601.2330654","acmid":"2330654","publisher":"ACM","address":"New York, NY, USA","keywords":"collaboration, distributed work, knowledge work, learning analytics, metacognition, programming, writing","Abstract":"This paper explores the ways in which participants in writing intensive environments might use learning analytics to make productive interventions during, rather than after, the collaborative construction of written artifacts. Specifically, our work considered how university students learning in a knowledge work model---one that is collaborative, project-based, and that relies on consistent peer-to-peer interaction and feedback---might leverage learning analytics as formative assessment to foster metacognition and improve final deliverables. We describe Uatu, a system designed to visualize the real time contribution and edit history of collaboratively written documents. After briefly describing the technical details of this system, we offer initial findings from a fifteen week qualitative case study of 8 computer science students who used Uatu in conjunction with Google Docs while collaborating on a variety of writing and programming tasks. These findings indicate both the challenges and promise of delivering useful metrics for collaborative writing scenarios in academe and industry.","pdf":"Learning Analytics for Collaborative Writing: A Prototype  and Case Study   Brian J. McNely, Paul Gestwicki, J. Holden Hill, Philip Parli-Horne, Erika Johnson  Ball State University  Departments of English and Computer Science   Muncie, IN 47306   bjmcnely@bsu.edu, pvgestwicki@bsu.edu, holdenhill@gmail.com, pparlihorne@bsu.edu,  etjohnson@bsu.edu       ABSTRACT  This paper explores the ways in which participants in writing  intensive environments might use learning analytics to make  productive interventions during, rather than after, the  collaborative construction of written artifacts. Specifically, our  work considered how university students learning in a knowledge  work modelone that is collaborative, project-based, and that  relies on consistent peer-to-peer interaction and feedbackmight  leverage learning analytics as formative assessment to foster  metacognition and improve final deliverables. We describe Uatu,  a system designed to visualize the real time contribution and edit  history of collaboratively written documents. After briefly  describing the technical details of this system, we offer initial  findings from a fifteen week qualitative case study of 8 computer  science students who used Uatu in conjunction with Google Docs  while collaborating on a variety of writing and programming  tasks. These findings indicate both the challenges and promise of  delivering useful metrics for collaborative writing scenarios in  academe and industry.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1   [Computer Uses in Education] Collaborative learning,  Computer-assisted instruction (CAI)   General Terms  Documentation, Design, Theory.   Keywords  writing, collaboration, programming, knowledge work, distributed  work, metacognition, learning analytics   1. INTRODUCTION  The successful deployment of learning analytics holds much  promise for educators and professionals who wish to make  productive interventions into knowledge work as it happens; these  interventions may be seen as a kind of formative assessment  regular, real time feedback about a specific deliverable as it is  being developed. In recent years, several germinal studies of  computer-supported cooperative work have emerged from the  overlapping fields of technical and professional communication  [see, for example, 1, 2, 3, 4, and 5]. Related studies of networked  writing technologies in particular have contributed to the  understanding of writing's formidable role in knowledge work, the  cross-disciplinary, collaborative, often distributed work  ubiquitous in post-industrial developed nations [see 6]. Indeed,  researchers have cogently argued that knowledge work often  looks like writing, leading to a focus on what writing does in  collaborative work environmentshow writing work both  represents and moves knowledge assets [7, 8]. Building upon this  body of research, we developed and studied a system that  generates visualizations of real time metrics about the  contribution and edit histories of collaboratively written  documents as a means for fostering formative assessment in both  peer-to-peer and instructor-to-student modes.   The system we developed, known as Uatu, interoperates with  Google Docs to better trace and represent the complexity of  collaboratively written documents. Our project was driven by two  conceptual research questions: first, do learning analytics related  to collaborative writing foster greater metacognition among  student participants; and second, does such analytic data  promote both instructor and peer opportunities for real time  interventions into ongoing collaborations as formative  assessment In the remainder of this paper, we address these  questions by first describing the technical details of our system;  we then report the methods, data, and initial findings from a  systematic qualitative case study conducted with eight computer  science undergraduates who used Uatu over fifteen weeks; and  finally, we explore the implications of our findings for the  learning analytics community. Our research revealed a strong  preference for co-located collaboration among participants, a  practice that directly reduced the efficacy of our system since  many co-located collaborative writing practices are ephemeral  and thus do not produce measurable data. These findings suggest,  therefore, that learning analytic systems designed to represent  collaborative writing are perhaps better suited to fully online  learning environments and to distributed teams in industry or  other professional domains.   2. DEVELOPING THE UATU PROTOTYPE  Our work was motivated by conceptual interest concerning the  ways in which writing functions as an arbiter of knowledge work  in organizations and academe, as well as our recognition of the  growing adoption and use of networked writing technologies such  as Google Docs. A key impetus for this project, therefore, was our     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00      222    interest in providing real time data about collaborative writing  histories to key stakeholdersstudents and instructors in  academe, and knowledge workers and project managers in  industry. We hypothesized that making such data readily visible  to all participants would lead to more frequent and robust  opportunities for metacognition. In this process, distributed  collaborators might gain a better sense of ongoing text  development [9] accompanied by a more thorough understanding  of their own contributions to the knowledge and practice of a  collaborative team. Provided with these metrics, students,  instructors, knowledge workers, and project managers would also  be equipped with data to better make formative assessments that  might help shape final collaboratively written documents. Our  focus, then, was on learning analytics for collaborative writing in  both industry and academe.   We chose the name Uatu for our prototype application in homage  to the Marvel Comics character The Watcher, a fictional being  tasked with watching over the earth. Uatu is a visualization  system that continuously watches documents created in Google  Docs and shared with a special user account that collects and  stores edit and revision data (about user contributions, changes in  document size, and time) in a MySQL database located on a local  server. As a visualization system, Uatu is comprised of three  interrelated modulesWatcher, Vis, and the Data Table  Servletthat interoperate with the Google Docs GData  application programming interface and the Google Visualization  application programming interface (see Figure 1).      Figure 1.  Uatu Module Architecture   In addition to the backend architecture, we created a front end  web site for users; when logged in, Uatu produces basic  visualizations of the overall collaborative writing activity for a  requested document (see Figure 2).      Figure 2. Uatu Visualizations   These visualizations are generated from document revision  histories stored in the local Watcher database. As such, these  values are not truly representative of the entire document, but  instead reflect a very close approximation based on frequent  polling (currently in one-minute intervals) of the document hosted  on Google's servers. In Figure 2, the top visualization includes  document revisions as they occurred over time, denoting who  made a particular revision, when the revision was saved (noted by  the horizontal axis of the graph), and the size of the contribution  (noted by the vertical axis of the graph). Users may adjust the  visualization by manipulating the left-hand side of the graph to  highlight specific revisions. The bottom visualization in Figure 2  is a horizontal bar chart displaying each user who has contributed  a revision and the number of saved revisions they have made to  the ongoing document.   Despite several early challenges encountered when interacting  with Google's application programming interface, Uatu is  presently a fully functioning prototype that faithfully watches and  logs revision data from any documents shared with a special user  account. Moreover, the front end web site delivers simple yet  effective visualizations of collaborative writing activity generated  in Google Docs. In addition to building the Uatu system,  however, a major goal of this project was testing and researching  the system with small groups working on authentic collaborative  writing tasks. In order to meet the latter objective, we conducted a  systematic qualitative case study of computer science  undergraduates who used Google Docs and Uatu during a project- based advanced programming course over fifteen weeks.   3. METHODS AND DATA  Work on the technical development of the Uatu prototype began  in July of 2010, while our systematic qualitative case study of  Uatu, Google Docs, and learning analytics about collaborative  networked writing activity began in January, 2011; data collection  concluded in May, 2011. We used a systematic qualitative case  study methodology conducted with ethnographic methods of field  research [10, 11]. Following Dourish, we were interested in  determining not only what Uatu can do as a system, but what  Uatu does for participants in the course of their everyday work  [12]. Consequently, we conducted a series of classroom  observations, usability observations followed by stimulated recall  interviews, observations of pair/group programming and group  presentations, and semi-structured interviews with participants.  These methods improved the reliability of our data, since  thorough triangulation across data types and instances led to a  deeper understanding of the collaborative behaviors we observed.  In total, our fieldwork consisted of: twenty different classroom  observations; twenty-four semi-structured and stimulated recall  interviews spaced evenly over 15 weeks; fourteen observations of  student writing and collaboration behaviors conducted outside of  the classroom and accompanied by talk-aloud protocols; over  seventy photographs; and the collection of nineteen participant- produced artifacts written in Google Docs, with granular revision  history data captured by Uatu.   Our study generated rich qualitative data about collaborative  writing strategies among novice computer programmers. There  were eight participants in total, and we collected in-depth and  well-triangulated data from six of those participants, all of whom  were undergraduate students at a mid-sized public research  university in the Midwestern region of the United States; most  were computer science majors, though two were computer science   223    minors. The initial findings detailed in the following section were  developed inductively through analysis of the granular data listed  above. We used a combination of qualitative coding methods,  including in vivo, descriptive, and process oriented approaches in  order to derive superordinate categories and themes from our data  [13]. Because of space limitations, we report these themes in  narrative form. Reliability was fostered by our collection and  analysis of data across multiple types (semi-structured interviews,  talk-aloud protocols, stimulated recall interviews, observational  fieldnotes) and instances (repeated observations and interviews  over 15 weeksfrom the first week of class until the final exam).   4. FINDINGS  In this section, we begin by briefly describing the project-based  pedagogical model in which our research participants worked  over the course of our study. We then detail three interrelated  themes to emerge from our analysistwo minor themes which  feed into our key conceptual theme. First, we describe our  participants' workflow practices and their overwhelming  preference for co-located collaboration. Next, we explore a key  outcome of this collaborative preference, one that is especially  relevant for learning analytics: much of our participants'  workflow practices in co-located collaborations are ephemeral  and thus do not render metrics that can be easily captured and  measured. These two themes lead directly into our third theme:  what counts as writing work for participants We describe how  certain forms of writing collaboration are often rendered invisible  by common approaches to the assessment of writing assignments.  This is the key conceptual finding of our study, one that has  significant implications for the use of learning analytics in  networked collaborative writing environments.   Though ostensibly a course in advanced programming at the  sophomore level, in actual practice our participants were  immersed in a curriculum that more closely resembled an upper  division software engineering course. Students learned methods of  agile software development, Scrum, risk matrices, UML  diagramming, and concepts in code reading and documentation  that were all modeled and carried out in a project-based learning  environment. Many students in the course were introduced to  version control (using Mercurial) for the first time; they learned  concepts in paper prototyping and usability, and were engaged in  a variety of collaborative writing tasks that supported their  programming practices (including project planning, requirements  gathering and analysis, and learning assessments). The course was  divided into two major units: a nine-week portion with two  smaller group projects and several individual assessments, and a  six-week major group project that involved designing,  developing, and testing a working prototype application in Java.  For this latter project, the three student teams of five were  required to maintain and develop a learning standards document  which eventually served as a major component of our study  (while also providing much of the data that drove Uatu  visualizations).   Our first minor theme concerns the collaborative practices of  participants. As they worked on the large six-week group project,  our participants displayed a strong preference for co-located  collaboration, despite access to distributed, networked  programming and writing environments (Mercurial and Google  Docs). In this sense, the actual collaborative practice of our  participants varied greatly from our expectations. Many students  in the course had never used Google Docs previously, and it was   our expectation that using the application would ease  collaborative writing tasks for groups since they could work  asynchronously or even in real time from distributed locations,  removing the potential barrier of coordinating school and work  schedules to complete face-to-face meetings.    However, participants in our study showed an overwhelming  preference for both ad hoc and planned collaboration sessions that  occurred face-to-face. In fact, two teams scheduled three to four  hour-long meetings each week during the large project,  collaborating on both programming and writing tasks during these  sessions. In short, participants strongly preferred co-located  verbal and gestural collaboration rather than distributed  programming and writing collaboration, but they used Google  Docs to develop their learning standards documents nonetheless.  The primary motivation for this preference appears to be centered  on alleviating programming anxiety among novice software  developers. For example, one participant noted that pair or group  work helped alleviate his programming anxiety since he could  receive immediate feedback from peers on a given challenge.  Another factor is convenience: distributed writing is labor  intensive, and participants simply found writing as a group to be  more efficient and less individually taxing.    But this preference for face-to-face writing collaboration caused  us to focus on exactly how our participants generated prose in  such sessions, our second minor theme. When he was asked who  contributed edits and saves to the major written component of the  six-week development project, Terry, one of our research  participants, responded: um, I think just me and Roy. Roy copied  down the stuff on the whiteboard. Roy also logged meeting notes  from the oral interactions of team members in face-to-face  development sessions. Roy, it turns out, was the designated  typist for his four-person group, both in the IDE and in Google  Docs. When we examined the edit histories of the ongoing  learning standards documents in Uatu, we noticed that often only  one or two members of a group made edits and saves to a work  that was to be explicitly written collaboratively. We navely  assumed that the combination of a version control system  (Mercurial) and a distributed writing application (Google Docs)  would lead to more distributed and asynchronous group  collaboration, and consequently, more group members  contributing commits and saves. Our assumption, however, was  diametrically opposed to observed student practices. Face-to-face  collaborations included practices such as verbal interaction,  whiteboarding and sketching activties, paper prototyping, and ad  hoc ideation (for example, via the Google Docs embedded chat  feature). In short, many important participant contributions were  ephemeral, and thus invisible to Uatu.    Because Uatu couldn't capture these more dynamic learning and  collaboration activities, we were forced to re-examine our  perspective on what counts as collaborative writing work during  the six-week projects. For example, even though Roy was the  designated typist for his group, he certainly didn't contribute a  corresponding proportion of the actual writing of the learning  objectives document. Typing does not equal writing. Examining  learning analytic data captured by Uatu would indicate that Roy  and Terry, to take one example, overwhelmingly wrote the final  learning standards document. But this would not reflect the actual  collaborative construction of that document, which was far more  complex and nuanced. For example, other group members  sketched ideas on white boards and paper, and they meaningfully  contributed to the writing of the document orally in face-to-face   224    sessions. We repeatedly observed dynamic collaboration sessions  in which all members contributed in meaningful (but not  necessarily equal) ways to the final document. However, because  groups preferred face-to-face meetings, gestural, oral, and non- digital contributions that were integral in the collaborative writing  of the learning standards document were rendered invisible in the  document's edit history. What counts as collaborative writing  work in such group sessions, therefore, cannot be accurately  measured with a traditional approach to assessment or analytics.   5. IMPLICATIONS AND OPPORTUNITIES  The limitations for visualizing collaborative writing activity with  Uatu are obvious: if group members are engaged in complex  writing work without writing, their individual contributions will  remain invisible to the system. This is actually an encouraging  finding, despite being contrary to our expectations and  hypothesis. Our participants collaborated on complex knowledge  work projects in creative and productive ways, using the systems  they had been given to best effect the inquiry-driven work they  had undertaken. In the process, we recognized that Uatu's utility  is limited for collaborative writing situations in which participants  are likely to work in shared space, and where input tasks are  delegated to one or two specific individuals. Another limitation of  Uatu concerns shorter documents that are collaboratively written  by a few (under 5 or 6) participants. An individual participant's  sense of the text is strong in these scenarios; they readily  apprehend the extent of their fellow team members' contributions  at any given stage in the project, so Uatu's visualizations yield a  representation of something they already implicitly understand. In  such scenarios, the utility of learning analytics for formative  assessment seems particularly limited.   While Uatu's metrics weren't especially useful for our research  site, the use of Google Docs as a real time collaborative writing  application was valued across participants. For example, when  participants contributed in ephemeral ways to collaboratively  written documents, they could see their respective contributions  reflected in real time via the operations of a given group's  designated typist. In this way, several participants noted that  seeing such contributions develop in real time caused them to  become more metacognitive about how their own feedback  played a part in the evolution of the learning standards document.  Perhaps more importantly, our key theme should lead us, as  educators, to re-evaluate how we assess collaborative writing and  learning scenarios. By broadening our understanding of what  counts as writing work, we can more readily account for  meaningful contributions across learning styles. The major  challenge for the learning analytics community, therefore, is  capturing data about more ephemeral forms of collaboration.   The real utility of a system like Uatu, it seems, is in visualizing  large, complex documents that are collaboratively written by  several distributed participants over an extended period of time,  where any individual's sense of text is likely to be overwhelmed  by the number of contributors, revisions, and saves. A project  manager tracking the development of a complex policy document,  for example, would greatly benefit from periodic Uatu  visualizations. Rather than laboriously sifting through pages and  pages of electronic text, the project manager could simply  produce a daily visualization that details ongoing development.  This is certainly no substitute for closer inspection; instead, it can  help a project manager determine when and how to more closely  and strategically investigate the developing text, a scenario in   which learning analytics might foster robust formative  assessment.   Another clear application of Uatu is in online education, where  visualizations of ongoing writing activity may help instructors  provide more productive formative feedback and assessment,  helping students learn as they work. Where collaborative writing  assignments may be seen as onerous in contemporary online  courses, a system such as Uatu could actually facilitate better  integration of such assignments. Because fully online students are  less likely to collaborate face-to-face, Uatu visualizations might  better reflect the collaborative construction of knowledge  occurring in and through writing work. Yet even in this scenario,  our study suggests that important ephemeral forms of ideation and  development would still contribute to final deliverables. These are  forms of thinking and doing that remain difficult to capture with  current learning analytic systems.   6. ACKNOWLEDGMENTS  This project was supported by a grant from the Indiana Space  Grant Consortium, a Ball State University Emerging Media  Initiative Innovation Grant, and the Ball State University Honors  College.   7. REFERENCES  [1] Johnson-Eilola, J. 1995. Datacloud: Toward a New Theory   of Online Work. Hampton Press, Cresskill, NJ, USA.   [2] Spinuzzi, C. 2003. Tracing Genres through Organizations.  MIT Press, Cambridge, MA, USA.   [3] Spinuzzi, C. 2007. Guest editors introduction: Technical  communication in the age of distributed work. Technical  Communication Quarterly 16, 265277.   [4] Swarts, J. 2007. Mobility and composition: The architecture  of coherence in non-places. Technical Communication  Quarterly 16, 279309.   [5] Slattery, S. 2007. Undistributing work through writing: How  technical writers manage texts in complex information  environments. Technical Communication Quarterly 16, 311 325.   [6] Reich, R. 1992 The work of nations: Preparing ourselves for  21st century capitalism. Vintage Press, New York, USA.   [7] Grabill, J., and Hart-Davidson, W. 2010. Understanding and  supporting knowledge work in everyday life. Language at  Work.  http://www.languageatwork.eu/readarticle.phparticle_id=31   [8] McNely, B. 2010. Exploring a sustainable and public  information ecology. In Proc. SIGDOC 10, ACM Press,  103108.   [9] Haas, C. 1996. Writing Technology. Lawrence Earlbaum  Associates, Mahwah, NJ, USA.   [10] MacNealy, M. S. 1999. Strategies for Empirical Research in  Writing. Allyn and Bacon, Boston, USA.   [11] Stake, R. 2010. Qualitative Research: Studying How Things  Work. The Giulford Press, New York, USA.   [12] Dourish, P. 2001. Where the Action Is: The Foundations of  Embodied Interaction. MIT Press, Cambridge, MA, USA.   [13] Saldana, J. 2009. The Coding Manual for Qualitative  Researchers. Sage, London, UK  225      "}
{"index":{"_id":"39"}}
{"datatype":"inproceedings","key":"Khan:2012:REP:2330601.2330655","author":"Khan, Tariq M. and Clear, Fintan and Sajadi, Samira Sadat","title":"The Relationship Between Educational Performance and Online Access Routines: Analysis of Students' Access to an Online Discussion Forum","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"226--229","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330655","doi":"10.1145/2330601.2330655","acmid":"2330655","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, online discussion, participation patterns","Abstract":"A study of behaviour patterns associated with students accessing an online discussion forum is presented. Data collected on the frequency of access and the duration of sessions is analysed to establish several categories of learners, which depict the differences among the cohort in terms of participation in social learning. A British business school course for second year undergraduates was studied over two years (i.e. two cohorts) and the results were combined to derive categories of learner types. We conclude that that academic attainment does not appear to be related to student access behaviour necessarily.","pdf":"The Relationship Between Educational  Performance and Online Access Routines:  Analysis of Students Access to an Online   Discussion Forum     Tariq M. Khan  Brunel Business School,   Uxbridge, UB8 3PH, London, UK   tariq.khan@brunel.ac.uk       Fintan Clear  Brunel Business School,   Uxbridge, UB8 3PH, London, UK   fintan.clear@brunel.ac.uk       Samira Sadat Sajadi  Brunel Business School,   Uxbridge, UB8 3PH, London, UK   samirasadat.sajadi@brunel.a c.uk   ABSTRACT  A study of behaviour patterns associated with students accessing  an online discussion forum is presented. Data collected on the  frequency of access and the duration of sessions is analysed to  establish several categories of learners, which depict the  differences among the cohort in terms of participation in social  learning. A British business school course for second year  undergraduates was studied over two years (i.e. two cohorts) and  the results were combined to derive categories of learner types.  We conclude that that academic attainment does not appear to be  related to student access behaviour necessarily.   Categories and Subject Descriptors   K.3.1 [Computers and Education]: Computer Uses in  Education   collaborative learning   General Terms  Measurement, Design.   Keywords  learning analytics; online discussion; participation patterns   1. INTRODUCTION  Social media is described as a web-based service that allows  individuals to construct a profile within an organized  framework, to generate a list of other users with whom they  share a connection, and to navigate their own list of connections  and view those made by others within the system [1]. Thus  social networks use the Web to facilitate their communications  as one element of what has been termed Web 2.0 - a term  coined by OReilly [2] and which has come to be used to  describe a wide range of Internet-based information and  communication technology (ICT) applications that offer the  potential for significantly increased interactivity with a high  degree of communication, cooperation, collaboration and   connection [3] between users. More recently attention has  turned to how these new applications can facilitate educational  attainment. It is often recognised that a constructivist  perspective is most appropriate for understanding learning  through social media. Insights on student behaviour and  academic performance however are relatively limited in the  research literature. Our intention here is to analyse access  patterns to a discussion forum within a VLE (virtual learning  environment) to determine a taxonomy of online learners and to  understand types of access pattern most conducive to learning.      2. ONLINE IDENTITY & COMMUNITY  2.1 Learning Theory  Social Learning Theory [4] sees environmental (i.e. social) and  psychological factors influencing behaviour with retention  (remembering what one observes), reproduction (recreating  that behaviour), and motivation (having good reason to  recreate that behaviour) as critical cognitive aspects.  Motivation in these terms resonates with the ideas of Whyte  [5] who advocated getting students to accept personal  responsibility for their own learning, and, as long as the  individual has some inherent ability, better academic progress is  made. However some form of guidance or scaffolding is  required for learners which needs to be built into the educational  infrastructure, or online learning environment.     Social Development Theory [6] argues that social interaction  precedes intellectual development, with consciousness and  cognition being the end product of social behaviour. Thus the  connections between people and the sociocultural context in  which they interact are critical for such development [7]. Thus  Vygotsky challenges the traditional instructionist model of  learning whereby teachers transmit information to students,  and offers in its place learning contexts in which students  themselves take an active role in enabling their learning.     In this analysis of social constructivism, Vygotskys Zone of  Proximal Development (ZPD) appears to chime to some extent  with the Community of Practice (CoP) concept [8] whereby  novices firstly undertake learning from a position of Legitimate  Peripheral Participation (LPP). Lave and Wenger argue that  learning is situated such that it is embedded within activity,  context and culture. However intentionality is not necessarily      Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise,   or republish, to post on servers or to redistribute to lists, requires prior  specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada    Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.       226    assumed here and can be an incidental or vicarious outcome of  the interactions occurring within a community of practice.     2.2 Empirical Evidence   Though proponents argue for the use of social media as an  educative tool, empirical studies are not plentiful. Heo et al. [9]  examine online interaction amongst a set of students engaged in  project-based learning and show that levels of academic  achievement do not always reflect quantity of online interaction,  while quality of interaction appears to be critical relationship to  academic outcomes. Social media as a distraction is noted by  Junco [10] who finds Facebook usage and scholarship to be  negatively correlated: the frequency of engagement with  Facebook has a negative correlation with the amount of time  spent by students preparing for class. In the same vein Junco and  Cotten [11] find that students who spend more time chatting  online than their peers report greater levels of academic  impairment. Such evidence shows social media holding back  rather than supporting student learning. This view chimes with  Kubey et al. [12] who find heavy recreational Internet use  highly correlated with impaired academic performance. Such  observations prompted us to investigate student access patterns  to an online discussion board and their apparent learning.      3. METHODOLOGY  The investigation took place over two years (2009-2011) on an  undergraduate course on Project Management.  Students on the  course were from different degree pathways so the class  included diverse academic interests as well as diverse  demographics (e.g. gender, race, age, etc.).  The task given to  the student cohort was to form teams of 4-6 people in order to  undertake a project assignment. Students were informed that the  group activity did not attract academic credit per se, but was  essential for a subsequent activity which did attract credit. This  follow-on activity required students on an individual basis to  produce short critical accounts reflecting on their team working  experience in the first activity. Thus, without sufficient  involvement in the group activity, students would find it difficult  to complete the reflective account (the second activity).  Communications amongst the team members in the group  activity were to be conducted through an online discussion  application (Blackboard). The first year of the study (2009/10)  involved 160 active students on the discussion forum, while the  second year (2010/11) involved 143 active students. Only those  students who submitted the second assignment were considered  to be active.     Analysis of the data derived from the discussion forum involved  a hierarchical cluster analysis to identify subgroups amongst the  class, as well as tests of difference amongst groups (t-tests) and  tests of association among factors (Pearson correlation).      4. RESULTS  An initial hierarchical cluster analysis (Wards linkage on  Euclidean distance) was performed on the data set for each year  separately.    Year 2010/11   This procedure distinguished several groups on dimensions  related to variables extracted from the analytics data: the  average interval between online sessions (AIn); and the average  duration of an online session (ADu). We see that there are five   major groups of students formed when differentiating on the  variables introduced above, with the primary differentiator being  ADu at a height of h=7.77 (i.e. a considerable difference  amongst the two top-level clusters).  The top-level clusters  consisted of n=87 students with average durations per session of  less than 9 minutes, and n=56 students with average duration per  session of greater than 9 minutes (the cut-off point of 9 minutes  was chosen from the dendogram by visual inspection).     The second level clusters were divided on the basis of average  interval between sessions. For the group with relatively shorter  ADu, a first division at h=6.94 was made, though a subsequent  division at h=4.05 serves to better define three distinct clusters.  The height of h=4.05 indicates a relatively high distinction  amongst clusters. The sizes of the sub-clusters were n=54, n=26  and n=7, divided at average interval per session of 1-5 days, 5- 12 days, and more than 12 days respectively. The alternative  group with comparatively longer ADu was divided at h=4.01,  with cluster sizes of n=41 for 1-5 days, and n=15 for more than  5 days. No further divisions were made due to small height  values which could not provide reliable sub-clusters.     We can define the five clusters on the basis of their average  interval between sessions and the average duration of a session.    Cluster 1, n=54: Short interval between sessions and short  duration per session  Cluster 2, n=26: Medium interval between sessions and short  duration per session  Cluster 3, n=7: Long interval between sessions and short  duration per session  Cluster 4, n=41: Short interval between sessions and long  duration per session  Cluster 5, n=15: Long interval between sessions and long  duration per session  Note that no clear cluster emerged with medium interval   between sessions and long duration per session.     A test of difference between means (independent samples t-test)  confirmed no significant difference between clusters formed on  final assignment mark. A one-way ANOVA test was conducted  on the three sub-clusters divided at h=4.05 (Levenes statistic  F=.515, p=.599 confirms homogeneity of variance). Results  indicate there is a statistically significant difference between  groups (F (2.77) = 5.878, p = .004). A Tukey post-hoc test  reveals that the final mark attained in the follow-up assignment  is statistically significantly higher for average interval per  session of between 1 and 5 days (1<AIn<5) (59.9  10.3  percent) compared to 5AIn<11 (53.5  11.8 percent, p = .047)  and AIn11 (46.87  13.2 percent, p = .010). There was no  statistically significant difference between clusters 5AIn<11  and AIn11 (p = .300).    Correlation analysis at the level of clusters indicates some  interesting associations.     Cluster 1: r = -0.387, p=0.01, n=54. Moderate indirect  correlation between access span (period between first and last  access) and average interval between sessions.  Cluster 2: r = 0.462, p=0.05, n=26. Moderate direct correlation  between assignment mark and the average number of messages  read per session.  Cluster 3: r = 0.875, p=0.01, n=7. Strong direct correlation  between assignment mark and access span.   227    r = 0.831, p=0.05, n=7. Strong direct correlation between access  span and average messages read per session.  Cluster 4: n=41. No significant correlations.  Cluster 5: r = 0.555, p=0.05, n=15. Moderate direct correlation  between average duration of a session and average number of  messages read per session.   Year 2009/10   The analysis procedure was repeated on the data for year  2009/10. Analysis revealed several groups distinguished on  dimensions related to variables extracted from the analytics data:  The average interval between online sessions (AIn); average  duration of an online session (ADu); and the average number of  discussion messages read during an online session (AMr). As  with Year 2010/11, we can define five clusters for Year 2009/10  on the basis of their average interval between sessions and the  average duration of a session, though the clusters were not as  highly distinct as for Year 2010/11. Cut-off height of h=2.09 to  achieve five clusters, but top level h=7.77, and as with Year  2010/11, the value of h (incidentally identical) indicates a  considerable difference between the two top-level clusters.   However, in contrast to Year 2010/11, the division at h=7.77 is  based on average interval per session (AIn) rather than average  duration per session (ADu).    A test of difference between means (independent samples t-test)  revealed no significant difference in final assignment mark  between clusters. A one-way ANOVA test was conducted on the  three sub-clusters divided at h=2.09 (Levenes statistic F=.975,  p=.423 confirms homogeneity of variance). Results indicate  there are no statistically significant differences between clusters  1, 2, and 3 on the basis of final assignment marks.     Analysis of analytics data over two years has revealed some  consistent results, yet there remain a number of issues that the  data are unable to resolve. First we should note that student  access patterns can be defined using the two parameters of (a)  average interval between sessions, and (b) average duration of  each session. A number of distinct clusters can be produced  using just these two parameters, though their exact inter- relationship is not clear from the analysis. In one year, average  interval was the primary differentiator, whereas for the  alternative year, average duration was predominant. The more  important concern, though, is the composition of clusters along  behavioural patterns, and the relationships between dimensions  of behaviour and attainment.    Commonalities across the two samples point to common  behaviour across years on some dimensions. Clusters 1, 3, and 5  are similar over the two studies, though clusters 2 and 4 describe  different behaviours. To help with the organisation of the  discussion, we introduce a framework composed of labels to  distinguish the different types of learners we observed over the  two years of the study. This is shown in Figure 1.    5. DISCUSSION  The application of Weiner's Attribution Theory [13] allows one  to make determinations of the reasons (causes) for the observed  behaviour of participants. Analytics data provide the  observations from which one can draw conclusions about the  students' motivations and intentions, given the assumption that  their behaviour is deliberate and, for the most part, stable. Thus,  the proceeding discussion is premised on the assumption that   observed behaviour is indicative of intention, which permits one  to make attributions of cause for the observations. Of course,  such attributions remain conjecture and unverified, wherein lies  a limitation of the work reported here.     Figure 1: Learner types based on access behaviour   patterns    An important determinant in the area of social learning is the  actions of others, which will in some great part impinge on the  experience had by a participant, and consequently, help  determine their response (i.e. remain online and observe, post  messages, engage in a chat, etc.). Attribution to external causes  (i.e. other participants) must feature prominently in an analysis  of observed behaviour within social media. It should also be  remembered that Social Development Theory [6] would place  the emphasis on social interaction for individual development,  while Bandura's social learning theory would highlight the role  of observation as a valuable component of a vicarious approach  to learning from others. Evidence of observation is shown in the  predominance of message reading over message posting, with  many of the learner types choosing to dip into and out of the  social artefacts in order to catch-up on things without significant  reciprocal contribution.     If we accept both views on learning (i.e. social development  and social learning) then there is a place for both interaction  and observation in a social learning process. However, as we see  from our data, preference for one view over the other seems  evident. Learner type 3 (inquisitive) and Learner type 7  (committed) are the two groups that demonstrate some level of  adherence to the principles of socially-constructed knowledge.  For these two groups of learners, attribution of causes for their  sustained levels of participation may be due to internal factors,  such as innate high levels of determination. Intrinsic motivation  assumes an attitude in the participant that places control over  attainment within oneself. Self-agency, therefore, is the driving  force that determines the extent to which committed and  inquisitive learners expend effort in influencing the group. By  contrast, it may be deduced that the remaining learner types are  not determined by a strong sense of self-agency within the  context of group work, and consequently, tend not to engage in  social development which would require active continual  interaction with others. At best, the remaining learner types  seem to adopt an observer role in efforts to learn vicariously.     One should not forget that social learning is not mediated by  observation after the fact (i.e. observation of historic activity),   228    but should be contemporaneous with the authentic activity being  learned. Thus, reading a trail of messages is not of itself  equivalent to participation in the construction process   primarily this is because such an approach is devoid of any  affordances for real-time exchanges with the active members of  the society being so observed.     With respect to the effectiveness of such illegitimate social  learning, an important consideration is cognitive loading [14],  which may impact on the observer's ability to comprehend the  considerable amount of accumulated knowledge in the form of  dozens or even hundreds of message exchanges. Reading alone  does not translate into learning, particularly in an electronically- created social environment that is far removed from the realities  of face-to-face team exchanges with their informationally-rich  cues (e.g. sounds, locations, visual and aural perceptions, etc.).  Without the combination of multimodal sensory inputs,  textually-based communications lack the potential for yielding  contextually-rich integrated knowledge that are conveyed  through visual and auditory channels. Thus social learning  conducted through a discussion board format consisting of  historic collections of message exchanges fails to live up to the  expectations and demands of an authentic social learning  experience in which learners are able to observe the visual as  well as the verbal cues of their peers (and experts), to process  those cues progressively without overloading their cognitive  capacities, and to re-affirm their understanding through  expression and feedback.     Although one may be able to recreate a community in a social  media environment, production of an authentic practice is  more problematic. Progression within the community of practice  is hard to facilitate therefore without continual authentic  exchanges intended to share experiences.  Those learners who  merely visit the community to observe would rarely be  considered to be participating in the practice, and therefore,  could not expect to benefit from the social nature of the media.      Whatever may be the drawbacks of an electronically-mediated  forum, learning can be an incidental product of participation in  online discussion, as evidenced by the committed learner type  (Figure 1). While their academic attainment did not noticeably  improve, the strengthening of their social ties and their  acceptance by the community as a valuable member of the  network are, arguably, important consequences of efforts to  enter the community through sustained contributions. Whether  the behaviour of the purposive, directed, strategic,  detached, and apathetic learner types can be explained by  their preference for an asocial approach to their personal  learning, or as being due to other moderating factors such as  introverted personalities, insufficient time, social reticence, low  level of motivation, non-acceptance in the community, or any  other form of action by others, is not clear from the data.   What we do learn is that any attempt to interpret behaviour  patterns from analytics data alone is fraught with vagaries  offering multiple competing explanations.     7. CONCLUSION  This study notes results that point to discernible VLE access  patterns, with interpretations of the intentions and motivations of  learners of several types. From this analysis a framework is  presented that identifies and relates seven types of learners on  the dimensions of access span and access duration. The  framework recognises learner types lying on a two-dimensional   spectrum that extends from purely instructivist to fully  constructivist. In reality, all learners lie in between the extremes  and demonstrate aspects of both learning approaches. The VLE  permits social interaction but interchange amongst participants  alone is inadequate as a means of raising performance levels.  Improvements in attainment are not clearly determined by  behaviour patterns, and intrinsic motivators are perhaps the  reason for variations in these patterns. In a context devoid of  adequate extrinsic motivators, emphasis must be on nurturing  intrinsic motivation in order to encourage more active  participation where the pedagogic goal is experience of the  process of social construction through interaction rather than  knowledge of the artefacts thus produced.    The results are preliminary in nature and must be considered  indicative rather than substantive.  Further work should include  the development of research instruments to test for the  substantive nature of the learner types, and to determine the  consequence of each type for learner attainment, group cohesion  and the viability of a social media-based learning environment.    REFERENCES  [1] Amichai-Hamburger, Y., Kaplan, H., & Dorpatcheon, N.  (2008), Click to the past: The impact of extroversion by users of  nostalgic website on the use of Internet social services,  Computers in Human Behavior, 24, 19071912.  [2] OReilly, T. (2005) What Is Web 2.0 Design Patterns and   Business Models for the Next Generation of Software   (http://www.oreillynet.com/pub/a/oreilly/tim/news/2005/09/30/  what-is-web-20.htmlpage=1)   [3] Cook, N, (2008), Enterprise 2.0, Aldershot: Gower  [4] Bandura, A. (1977), Social Learning Theory, General  Learning Press  [5] Whyte, C. (1980), An integrated counseling and learning  center, in K. Lauridsen (ed.) Examining the scope of learning  centers, San Francisco: Jossey-Bass, 33-43.  [6] Vygotsky, L.S. (1978), Mind and society: The development  of higher mental processes, Cambridge, MA: Harvard  University Press.  [7] Crawford, K. (1996), Vygotskian approaches to human   development in the information era, Educational Studies in  Mathematics, (31) 43-62.  [8] Lave, J. and Wenger, E. (1991), Situated Learning:  Legitimate Peripheral Participation, Cambridge, UK:  Cambridge University Press.  [9] Heo, H., Lim, K. and Kim, Y. (2010), Exploratory study on  the patterns of online interaction and knowledge co-construction  in project-based learning, Computers & Education, Vol. 55(3),  pp. 1383-1392.   [10] Junco, R. (2011), The relationship between frequency of  Facebook use, participation in Facebook activities, and student  engagement, Computers & Education, 58, pp. 162171.  [11] Junco, R. and Cotten, S. (2010), Perceived academic effects  of instant messaging use, Computers & Education, 56 (2), pp.  370378.  [12] Kubey, R., Lavin, M. and Barrows, J. (2001), Internet Use  and Collegiate Academic Performance Decrements: Early  Findings, Journal of Communication, June, pp. 366-382.  [13] Weiner, B. (1974), Achievement motivation and attribution  theory, Morristown, N.J.:  General Learning Press.  [14] Chandler, P. and Sweller, J. (1996), Cognitive Load While  Learning to Use a Computer Program, Applied Cognitive  Psychology, Vol. 10(2), pp. 151170.    229      "}
{"index":{"_id":"40"}}
{"datatype":"inproceedings","key":"Merceron:2012:ICG:2330601.2330656","author":"Merceron, Agathe","title":"Investigating the Core Group Effect in Usage of Resources with Analytics","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"230--233","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330656","doi":"10.1145/2330601.2330656","acmid":"2330656","publisher":"ACM","address":"New York, NY, USA","keywords":"association rules, confidence, learning management systems, usage of resources","Abstract":"In many educational institutions, face to face as well as on-line teaching is supported by the use of a Learning Management System (LMS). To be able to analyze better data stored by LMS, we have started developing a dedicated tool for this purpose. While analyzing usage data with teachers, we have noticed that the number of students attempting non self-tests decreases during the semester. Teachers were interested in investigating this pattern further to uncover the strategy adopted by students. In this paper, we explain our approach to investigate the core group effect in resources usage: given a set of resources, is a group of students emerging that continuously uses the resources or, on the contrary, are the resources used on an irregular basis by different students? We answer this question checking the confidence of what we call local rules and global rules. We show a case study conducted with our analysis tool as a first step to validate our approach.","pdf":"Investigating the Core Group Effect in Usage of Resources   with Analytics                        Agathe Merceron  Beuth University of Applied Sciences   FB VI, Luxemburger Strae 10  13353 Berlin   00 49 30 4504 5105   merceron@beuth-hochschule.de   ABSTRACT  In many educational institutions, face to face as well as on-line  teaching is supported by the use of a Learning Management  System (LMS).  To be able to analyze better data stored by LMS,  we have started developing a dedicated tool for this purpose.  While analyzing usage data with teachers, we have noticed that  the number of students attempting non self-tests decreases during  the semester. Teachers were interested in investigating this pattern  further to uncover the strategy adopted by students. In this paper,  we explain our approach to investigate the core group effect in  resources usage: given a set of resources, is a group of students  emerging that continuously uses the resources or, on the contrary,  are the resources used on an irregular basis by different students  We answer this question checking the confidence of what we call  local rules and global rules. We show a case study conducted with  our analysis tool  as a first step to validate our approach.   Categories and Subject Descriptors  H.4.2 [Information Systems Applications]: Types of Systems   decision support   General Terms  Measurement, Experimentation, Human Factors.   Keywords  Learning Management Systems, usage of resources, association  rules, confidence.   1. INTRODUCTION  In the Beuth University of Applied Sciences as in many  educational institutions worldwide, face to face and on-line  teaching are supported by the use of a Learning Management  System (LMS), Moodle [1] . A number of our industrial partners  use also an LMS for continuing education. The development and  maintenance of learning resources for an LMS requires some  effort.    Hence it is important to be aware of how learners learn with the  learning resources put for them on-line. Though they store users'  data, LMS have limited reporting and statistics facilities, which is  natural. Their purpose is teaching and learning, not analyzing  users' behaviors. To be able to analyze better data stored by LMS,  we have started developing a dedicated tool for this purpose [2,  3]. This software should be a Web Application that could be  personalized to serve different kinds of end-users such as  education providers, teachers, course designers, students and so  on. In that respect our tool is similar to AAT [4] or eLAT [5].   End-users are likely to be non proficient in Information  Technology, therefore it is essential for an analysis tool (i) to be  intuitive in its use,  (ii) to present results that end-users can easily  interpret and (iii) to use analysis techniques that are well  understood, as stressed in [6], and that have been validated in  order to inform properly stakeholders and in order to prevent  wrong decision making whose consequences are difficult to  predict. The present contribution is concerned with this last  aspect.     While analyzing usage data with teachers, we have come across   an interesting pattern like the one depicted in Figure 1 concerning  non-compulsory self-tests, here ex1 to ex7, that teachers make   available during the semester. This pattern indicates that the  number of students attempting these self-tests decreases during  the semester. Teachers were interested in investigating this pattern  further to uncover the strategy adopted by students: Are they  gradually giving up completely, which means that the students  who attempt self-test i is roughly a sub-group of the students who  attempted the preceding self-test Or are they eclectic in their  choice, which means students attempt randomly some self-tests  during the semester though they attempt them less as the semester  progresses In this contribution we explain how we handle these  questions.   The next section briefly introduces the tool we use for our  analysis. Then we expose how to analyze the core group effect  when usage of resources declines and, in the follower section,  present a case study as a first step to empirically validate our  approach. The conclusion ends this paper.    2. THE TOOL   As mentioned in the introduction, we design a dedicated tool for  analysis that is completely separated from the LMS. This means  that its data source is also independent of any LMS. In that  respect we follow the principles encountered in business  applications: the data warehouse used for data analysis is  independent of the transactional database used for daily business.     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April May 2, 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04 $10.00.      230    Therefore our tool is separated into two main parts: building the  data source and  querying/analyzing it.    The ExtractAndMap module, [2, 3] extracts all usage data stored  by an LMS and maps it to Mining-DB, a specially designed  database. Mining-DB has a form better suited for querying and  mining than the database of an LMS. Further analysis is  independent of any LMS. Indeed, the main class of  ExtractAndMap can be implemented to extract data from any  LMS. For the time being it has been implemented for Moodle. If  an institution uses different LMS, all usage data for analysis is in  one place, in Mining-DB. This module runs in the background  and regularly updates Mining-DB, which then contains current  and historical data. This feature makes our tool different from [4],  that can not handle data originating from different LMS at once or  from tools that are specific to a LMS like [7] or [8]. Another  feature of ExtractAndMap is that it renders users anonymous  complying with the principles on data privacy in our university.   Simple queries on  Mining-DB allow to answer a number of very  informative questions like: How many new courses have been  created in summer semester 2011, How many students have  subscribed the course Introductory Programming with Java and  the course formal basics of computer science, How many  students have completed self-test 1 in the course Introductory  Programming with Java , What is the average mark in the  exam formal basics of computer science of students who have  attempted self-test 6 and self-test 7, How many students have  accessed Resource X on April, 14 2011. A single query can  involve an arbitrary number of courses or resources.    Currently  queries are answered using the MySQL language [9] or  the Pentaho reporting tool [10]. The Pentaho reporting tool offers  the facility of predefining reports for different kinds of end-users.    3. ANALYSING THE CORE GROUP  EFFECT  We investigate the learning behaviour of students with respect to  the resources put in the LMS: how do they use them during the  semester First we explore how each resource is used, which is  done with simple queries. In the case we observe a decrease as  shown in Figure 1, we want to explore the following behaviour: is  a group of students emerging that continuously use the resources  or, on the contrary, are the resources used on an irregular basis by  different students   In the sequel we denote by |X|  the number of students that use  resource X and by |X, Y| the number of students who use both  resources X and Y.    We suppose first that a perfect core group emerges that  consequently keeps accessing the resources:  students who use X  form a subset of those who use some previous resource Y. In that  case the number of students who used resources X and Y is the   same as the number of students who used X, what is written |X, Y|   = |X|, or equivalently, |X, Y| / |X| = 1. In terms of association   rules [11], the quantity  |X, Y| / |X|  is the confidence of the rule  X    Y . This rule in our context is interpreted as if students  consulted X, they also consulted Y.   On the contrary, when resources tend to be  used on an irregular  basis, a number of students consulted resource X without   consulting some previous resource Y. In that case |X, Y|  |X|, or   equivalently, |X, Y| / |X| < 1. In the worst case, the group who used   X is completely distinct from the group who consulted Y and |X,  Y| = 0, thus |X, Y| / |X| = 0. The quantity |X, Y| / |X| measures the  proportion of the students who used Y among the students who   used X.    Summing up confidence of the rule X   Y allows to measure to  what extend the group of students who consulted some previous  resource Y forms a subgroup of those who consulted resource X.  In case of a real subgroup confidence is 1. From experience a  value of 0.8 or bigger seems adequate to denote a trend towards  subgroup.    In our context we have a set of n ordered resources Xi where 0 < i   < k  n means that  Xi has been put online earlier than Xk or Xi is  previous to Xk. In the case of a perfect core group, there are many   rules with a confidence of 1. First all rules with some previous   resource in the consequent: Xk, Xi for any i and k such that  0 <  i < k  n.. Then it is easy to see that rules with any set of previous  resources in the consequent will have also a confidence of 1:    Xk, S, where S is a set of resources and any resource in S has an  index smaller than  k. In other words, if a perfect core group  emerges, there is a rules deluge with a maximum confidence:  all rules with on the left side a resource that comes at a later date  than the resources on the right side.   We make the hypothesis that it is enough to check the confidence  of two kinds of association rules, what we call local rules and  global rules, for teachers to be aware and to follow the emergence   of a core group. Local rules have the form Xi+1   Xi. These rules   are concerned with what happens locally in time: use of a  resource and the preceding one. If a group of students starts  emerging that continuously uses the resources, this kind of rules  should have a high confidence, 0.8 or above. On the contrary  confidence should be low if different students irregularly use   these resources. Global rules have the form Xi+1 X1, X2 ,  ,Xi   and check globally the first i+1 resources: if students use resource   Xi+1 then they also use all earlier resources. Again, if a core group   starts emerging, confidence of these rules should be fairly high.      This hypothesis stipulates that in practice confidence of all  possible other rules as mentioned above with the rules deluge will  be very similar to the confidence of local and global rules and it is  not necessary to check them.      4. CASE  STUDY  We have investigated how students use self-tests in two courses  Formal Basics of Computer Science and Introductory  Programming with Java both taught in face to face teaching to  first semester students enrolled in the degree Computer Science  and Media at the Beuth University of Applied Sciences, Berlin,  during Winter Semester 09/10. 57 students were enrolled in  Formal Basics of Computer Science and 65 students were  enrolled in Introductory Programming with Java, 46 being  enrolled in both courses. Both courses proposed 7 self-tests  denoted  ex1, ex2 ... ex7 put online gradually during the semester.  These tests are not compulsory and students do not earn any mark  when they complete them. They are given as complementary  resources to support them further in their studies. However, the  Java course is quite difficult, especially for students with no  experience in programming. The failure rate of the Java course is  higher that the one of the Formal Basics course. The teacher in   231    charge of the Java course stresses the usefulness of the self-tests  more than the teacher of the Formal Basics course does. This case  study focuses on the 46 students enrolled in both courses and  investigates how they have attempted these self-tests.        Figure 1. Number of students attempting self-tests. Left:   Formal Basics course, right: Java course    Figure 1 shows how many students have attempted those tests.  One notices a similar pattern though more consistent for the Java  course: as the semester progresses less students attempt the self- tests. The last test takes place shortly before the exam.   Figure 2 shows the confidence of local association rules, 21  means if students attempt the second self-test, they attempt the  first one. One notices that confidence is quite high for the Java  course, except for the last rule, the smallest value being 0.77.  About 80% or more of the students who attempt an exercise have  attempted the preceding one. It is interesting to look at Figure 1  and 2 together.      Figure 2. Confidence of local rules. Bottom: Formal Basics   course, Top: Java course     Tests 2 and 3 of the Formal Basics course have been attempted  by 23 students as Figure 1 shows. Figure 2 tells us that about 40%  students who attempted test 2 did not attempt test 3. Altogether 32   students of the Formal Basics course were active with test 2 or  test 3 while it is 29 for the Java course. In the same way, 18  students from the Formal Basics course and 19 students of the  Java course were active at attempting exercise 6 or 7.    Global rules are shown in Figure 3. Here again for the Java  course, omitting the last test, about 70% or more of the students  who have attempted an exercise have attempted all preceding  ones.    Figure 3. Confidence of global rules. Bottom: formal basics   course, Top: Java course       Students in these two courses have followed a slightly different  strategy. Figure 2 and 3 show a trend for the emergence of a core  group till test 6 in the Java course, while from test 3 onwards  student tend to pick and choose in the Formal Basics course.    Then we have checked the confidence of all rules of the form jS   with j  6  and  S a set of self-tests occurring before j different  from the sets checked with local or global rules. With one  exception (0.71 for the rule 4  1, 2, 3), confidence is never  below 0.77 for the Java course and always well above 0.8 if S  contains only 1 resource. For the Formal Basics course  confidence of these rules varies between 0.18 and 0.65, many  values being around 0.30. These results are in the range given by   the local and global rules and thus confirm our hypothesis.    We have obtained very similar results when considering all  students, not only those registered in both courses. In that setting   we have investigated local and global rules from an association  rules point of view [11] and finish this section summarising the  findings First notice that we are not interested in support, a  measure often needed to extract association rules that occur often  enough in the data. Support of global and local rules varies  greatly, 0.06 (smallest value in the Formal Basics course) to more  than 0.54 (greatest value in the Java course). Global rules  particularly can be rare association rules [12]. Then we have  checked with three further measures the interestingness of our  rules: lift, cosine and correlation [13]. The values obtained rate  the local and global rules of the Java course as interesting,  whereas cosine and correlation rated most of the local and global  rules of the Formal Basics course as borderline. Further we have  checked the marks in the final exam in different settings: average  in general, average of students who attempted at least one  exercise, average of students who did not attempt any exercise,   ex1 ex2 ex3 ex4 ex5 ex6 ex7  0  5  10  15  20  25  30  35  40  Formal B.  Java  232    average of students who attempted exercise 1, average of students  who attempted exercise 2, and so on till average of students who  attempted exercise 7. In both courses the average of students who  attempted at least one exercise was higher than the general  average. A striking difference was that the highest average was  for students who attempted exercise 7 in the Java course [2]. Such  an effect was not visible for the Formal Basics course.    We have extracted local and global rules using queries only, not  using any association rule mining algorithm. There are two  reasons for that. First we know exactly which associations we are  looking for. There is no need for an algorithm that has to discover  possible associations. Second support is not important in our case  and can vary greatly. It would be difficult to adjust support to find  the desired local and global rules with some association mining  tool.   5. CONCLUSION  In this paper we explain our approach to investigate the core  group effect in resources usage. With core group effect we mean  the following: Given a set of resources whose usage decreases  over time, is a group of students emerging that continuously use  the resources or, on the contrary, are the resources used on an  irregular basis by different students We do not check all possible  dependencies between usage of resources but only those given by  what we call local rules and global rules. When confidence of  these rules is around or above 0.8, we interpret these rules as a  trend towards cor group effect.    The idea can be generalized when usage stays constant over time  and one wants to investigate whether resources are used by a  stable group of students.  When usage of resources increases,  reversing the rules should show whether a core group is growing.   For the teacher to intervene our experience shows that the  information given by local and global rules has to be completed  with more analysis, like past values about marks in the final  exam: Do students who attempt all exercises get better marks Do  students who attempt at least one exercise get better marks This  information has to be combined with the follow-up of use of  resources to intervene properly.   6. ACKNOWLEDGMENTS  This work is partly supported by the Institute fr Angewandte  Forschung Berlin and the European Social Fund for the Berlin  state  project LeMo .   7. REFERENCES  [1] Moodle, 2011. http://moodle.org/ (accessed on September   30, 2011).   [2] Krger, A,, Merceron, A. and Wolf, B.. 2010. A Data Model  to Ease Analysis and Mining of Educational Data.  Proceedings of the International Conference on Educational   Data Mining (Pittsburgh, USA, June 11   13, 2010),  EDM2010. 131-140.   [3] Krger, A,, Merceron, A. and Wolf, B.. 2010. Leichtere  Datenanalyse zur Optimierung der Lehre am Beispiel  Moodle.  In  Proceedings of the of the 8. e-Learning  Fachtagung Informatik Delfi  (Duisburg, Germany,  September 12 - 15, 2010). Lecture Notes on Informatics,  215-226.    [4] Graf. S., Ives, C., Rahman, N. and Ferri, A.. 2011. AAT  A  Tool for Accessing and Analysing Students Behaviour Data  in Learning Systems. In Proceedings of the Conference on  Learning Analytics & Knowledge (Banff, Alberta, Canada,  February 27  March 01, 2011), LAK2011, ACM New York,  NY, USA .   [5] Dyckhoff. A. L., Zielke, D., Chatti, M. A. and Schroeder, U..  2011. eLAT: An Exploratory Learning Analytics Tool for  Reflection and Iterative Improvement of Technology  Enhanced Learning. In Proceedings of the 4th International  Conference ob Educational Data Mining (Eindhoven, The  Netherlands , July 6   01, 2011), EDM2011, 355-356.    [6] Beck. J.. 2008. Difficulties in inferring student knowledge   from observations (and why you should care).In Proceedings  of Educational Data Mining workshop, in conjunction with   13 th   International Conference of Artificial Intelligence in   Education (Marina del Rey, CA. USA, July 07, 2008), 21- 30.   [7] Bader-Natal. A.. and Lotze, T.. 2011. Evolving a learning  analytics platform. In Proceedings of the Conference on  Learning Analytics & Knowledge (Banff, Alberta, Canada,  February 27  March 01, 2011), LAK2011, ACM New York,  NY, USA .   [8] Romero, C., Ventura. S., and Garcia, E. 2008. Data Mining  in Course Management Systems: Moodle Case Study and  Tutorial. Computers and Education 51 (2008), 368-384.   [9] MySQL open source database. http://www.mysql.com/, last  access 20.10.2011   [10] pentaho. Open source business intelligence.  http://www.pentaho.com/ last access 20.10.2011   [11] Han, J. W. and Kamber, M. 2006. Data mining: concepts  and techniques. Morgan Kaufmann Publishers.   [12] Koh, Y.S. and Rountree, N. (Eds.). 2010. Rare Association  Rule Mining and Knowledge Discovery: Technologies for  Infrequent and Critical Event Detection. IGI Global,  ISBN13: 9781605667546 .   [13] Merceron, A.  and Yacef, K. 2010. Measuring Correlation of  Strong Symmetric Association Rules in Educational Data. In  Handbook of Educational data Mining, C. Romero and all,  Ed. CRC Press, 245-256.             233      "}
{"index":{"_id":"41"}}
{"datatype":"inproceedings","key":"Roberge:2012:LTO:2330601.2330657","author":"Roberge, Daniel and Rojas, Anthony and Baker, Ryan","title":"Does the Length of Time Off-task Matter?","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"234--237","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330657","doi":"10.1145/2330601.2330657","acmid":"2330657","publisher":"ACM","address":"New York, NY, USA","keywords":"intelligent tutoring systems, learning analytics, off-task behavior","Abstract":"We investigate the relationship between a student's time off-task and the amount that he or she learns to see whether or not the relationship between time off-task and learning is a more complex model than the traditional linear model typically studied. The data collected is based off of students' interactions with Cognitive Tutor learning software. Analysis suggested that more complex functions did not fit the data significantly better than a linear function. In addition, there was not evidence that the length of a specific pause matters for predicting learning outcomes; e.g. students who make many short pauses do not appear to learn more or less than students who make a smaller number of long pauses. As such, previous theoretical accounts arguing that off-task behavior primarily reduces learning by reducing the amount of time spent learning remain congruent with the current evidence.","pdf":"Does the Length of Time Off-Task Matter  Daniel Roberge   Actuarial Mathematics  Worcester Polytechnic Institute   100 Institute Road  Worcester, MA 01609    droberge91@wpi.edu   Anthony Rojas  Mathematical Sciences   Worcester Polytechnic Institute  100 Institute Road   Worcester, MA 01609    acrojas@wpi.edu   Ryan Baker  Department of Social Science and   Policy Studies  Worcester Polytechnic Institute   100 Institute Road  Worcester, MA 01609    (508) 831-5355   rsbaker@wpi.edu      ABSTRACT  We investigate the relationship between a students time off-task  and the amount that he or she learns to see whether or not the  relationship between time off-task and learning is a more complex  model than the traditional linear model typically studied. The data  collected is based off of students interactions with Cognitive  Tutor learning software. Analysis suggested that more complex  functions did not fit the data significantly better than a linear  function. In addition, there was not evidence that the length of a  specific pause matters for predicting learning outcomes; e.g.  students who make many short pauses do not appear to learn more  or less than students who make a smaller number of long pauses.  As such, previous theoretical accounts arguing that off-task  behavior primarily reduces learning by reducing the amount of  time spent learning remain congruent with the current evidence.       Categories and Subject Descriptors  K.3.m [Computers and Education]: Miscellaneous.     General Terms  Human Factors     Keywords  Learning Analytics, Off-Task Behavior, Intelligent Tutoring  Systems     1. INTRODUCTION  Today, students are interacting with technology of various forms  more than ever during learning. One context where this is  occurring is in middle school and high school mathematics  classes, where learning increasingly occurs from students using  educational software in a classroom with a teacher present. One  such form of educational software which is used progressively  more in the United States is Cognitive Tutor software [11], where  student learning is individualized based on assessment of the  students current learning and the factors leading to a specific  student error.          Cognitive Tutors are now used in more than 6% of U.S.  secondary schools. Cognitive Tutors have been shown to enable  individual students to learn at their own pace, while empowering  teachers to spend instructional time in one-on-one teaching  episodes with the students who are struggling most [16].  Educational software like Cognitive Tutors provide extensive logs  of student performance [12], enabling not only more effective  learning for individual students, but supporting  analysis of  student learning over time, using methods from learning analytics  [17] or educational data mining [3].    In this paper, we study the relationship between a students  learning gain and his or her off-task behavior. One key model for  this relationship is Carrolls Time-On-Task hypothesis. This  hypothesis argues that off-task behavior reduces learning by  reducing the amount of time spent on-task [7]. However, there are  several factors that may complicate this relationship. In particular,  it is possible that taking a short break may improve cognition  afterwards [cf. 13]. Hence, short pauses may impact learning  differently than longer pauses. In addition, it is possible that a  qualitative difference may be seen between students who go off- task for a break, and students who are more fundamentally  disengaged; hence, students who are off-task for large proportions  of time may have greater reduction of learning than could be  anticipated through a simple linear model. For this reason, we  investigate the differences between several models of the  relationship between off-task behavior and learning, leveraging  both quantitative field observations of off-task behavior [cf. 2]  and an automated detector of off-task behavior [cf. 4] to measure  both overall prevalence of off-task behavior and the duration of  individual episodes.  We looked at the percent of time off-task  and the number of brief and lengthy off-task episodes to study the  relationship between these factors and learning gain.    Through the use of statistical analyses on medium-sized  educational data sets, a form of learning analytics [cf. 17], we can  better understand how off-task behavior influences learning and  under what conditions off-task behavior influences learning  differently. A learning analytics analysis using similar methods  includes research on student activities during writing [cf. 5].    Past studies conducted on students using educational software  have generally shown a negative correlation between off-task  behavior and learning during the use of the software [8, 9, 15]. A  similar pattern has been seen when studying these relationships  outside of technology, generally finding a negative relationship  between learning and off-task behavior (see [6] for an extensive  review of this literature). However, these studies have typically   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   234    Figure 1: The predicted post-test score (from the non-linear   model below) compared to the percent of time-off-task.   not explored non-linear relationships. A study done by Karweit  and Slavin, however, found that changing the length of  observation periods affected the strength of the relationships  between off-task actions and learning [10]. This supports the  notion that the length of off-task episodes may be predictive of  student learning, as well as the overall quantity of time spent off- task.      2. DATA  The students in this experiment used educational software in the  domain of scatterplots, a subject taught in the data analysis  portion of middle school mathematics in the United States.  Initially, students took a pre-test to determine how well they knew  the material at hand.  Afterwards, the students interacted with a  Cognitive Tutor lesson teaching this topic [1], for approximately  80 minutes apiece. Finally, a post-test assessment was given to  evaluate the students progress. Full details on the assessments are  given in [1]. 186 students completed the pre-test, the tutor  activity, and the post-test. These students were drawn from  multiple previous studies in separate years [cf. 8], but each used  the same tutor software under the same conditions (in some  studies, these students served as the control condition which was  compared to a modified version of the tutor  students using  modified versions of the tutor are not analyzed in this paper).     Data on student off-task behavior was gathered using two  methods. While students engaged in the cognitive tutor classroom  [11], two observers recorded students behaviors using  quantitative field observations [2].  The students were observed  using peripheral vision in order to decrease potential observer  effects, in a sequence of 20-second long observations cycled  across students. In each observation, the students behavior was  noted in terms of whether it involved off-task behavior [2, 4], in  order to compute the percentage of time each student was off- task. Off-task behavior was defined as any of the following: off- task conversation (talking about anything other than the subject  material), off-task solitary behavior (any behavior that did not  involve the tutoring software or another individual, such as  reading a magazine or surfing the web), and inactivity (such as  staring into space, or the student putting his/her head down on the  desk, for at least 20 seconds  brief reflective pauses by a student  actively using the software were not counted as off-task). Other  behaviors such as actively working in the software, collaborating  with other students, and gaming the system (intentionally  misusing the software in order to successfully complete problems  [cf. 2]) were not counted as being off-task.    The second method used an automated detector of off-task  behavior developed using data mining [4], built using the field  observations as ground truth. The off-task detector is a latent  response model used to infer exactly when off-task behavior  occurs, from features of individual student actions and recent  student behavior before those actions. The detector was shown to  achieve a correlation over 0.5 to the proportion of off-task  behavior observed, under student-level cross-validation. As such,  we use the detectors inferences as components in further  analysis, a process termed discovery with models [3]. As off- task episodes can sometimes be caught by the detector from the  behavior occurring shortly after the actual off-task episode (in  which case they are identified by very quick actions coming after  the a long pause) [4], we label each off-task pause with the length   of the longest pause in the sequence of 5 student actions  considered by the detector when making an inference.     3. ANALYSIS   3.1 Percent of Time Off-Task  The first relationship that we considered was the percentage of  time the student spent off-task while using the cognitive tutor.  Post-test score was the indicator used as an assessment of the  students eventual learning.      With the analyses in this section, we measure each students  proportion of off-task behavior, using the field observations, as  this is the most standard method for assessing off-task behavior,  used by researchers for decades [e.g. 2, 4, 6, 9, 10]. A students  proportion of off-task behavior, is statistically significantly  negatively correlated to their post-test score, r= -0.229, F(1,184) =  10.150, p<0.01. The students pre-test score was statistically  significantly positively correlated to the post-test as well, r=  0.299, F(1, 184) = 18.007, p<0.001.     In order to study the relationship between off-task behavior and  learning, we can analyze the relationship between off-task  behavior and the post-test, while controlling for the pre-test. The  best-fitting linear model of this relationship is:     Post = 0.273Pre - 0.394OT + 0.617    Within this model, the off-task term was significantly different  than chance, t(185)= -2.412, p=0.017.     We can also investigate a non-linear model (shown in Figure 1),  including the percentage of time off-task, squared, which  produced the best-fitting equation:     Post = 0.275Pre - 0.848OT 2  + 0.598         For the off-task squared term, t(185)= -2.295 and p=0.023.     Hence, both linear and quadratic models based on off-task  behavior are significant predictors of student off-task behavior. In   235    order to investigate whether one model is significantly better than  the other, we compare the models using the Bayesian Information  Criterion for Linear Regression, BIC [14].  BIC is a formula  used to see how well a specific model predicts the data given the  number of parameters (e.g. models with more parameters should  achieve better fit simply by chance). It can also be used to  compare two non-nested models of the same dependent measure,  so we can use it to compare the models predicting post-test using  the proportion of off-task behavior in a linear fashion, and the  proportion of off-task behavior, squared. In the first case, where  pre-test and percent of time off-task predict post-test, the BIC  produced a result of -12.685; while pre-test and percent of time  off-task squared produced a result of  -12.255. Although the  regression model that uses time off-task squared produces a  higher r value, the difference in the BIC of the two models is  only 0.430. This indicates that the two models are not statistically  different, which would be indicated by a difference of 6 or greater  [14]. Hence, there is not a strong justification for preferring a non- linear model of the relationship between off-task behavior and  learning, to a linear model, although there is a trend in that  direction.      3.2 Number of Brief/Lengthy Times Off-Task  A second question is whether lengthy pauses impact learning in a  different fashion than brief pauses. It has been shown that breaks  in the workplace can reduce mental fatigue and ultimately lead to  better employee performance on cognitive tasks [13]. Thus, it is  possible that the number of times a student has either brief or long  pauses affects their learning gain.      Within the analyses in this section, we use the automated detector  of off-task behavior rather than the field observations of off-task  behavior. The type of field observation used when the data was  first collected  round-robin observations of an entire classroom  by one or more field observers  gives a useful representation of  the total proportion of time each student is off-task, but it does not  shed light on how long individual episodes of off-task behavior  are. By labeling every student action across the entire session  (and gaps between actions) as to whether it is off-task or not,  automated detectors allow us to analyze the length of individual  episodes of off-task behavior.     An analysis predicting learning using the total number of times  off-task gave the following results. These two variables had a  correlation of 0.053, F(1,184)=0.519 and p=0.472.  When pre-test  was included as a covariate, there was not a substantial difference  for the term indicating the number of times off-task: t(185)=  0.027, p= 0.979. Hence, these results show that the total number  of off-task episodes as indicated by the detector is not predictive  of learning; however, when broken into brief and lengthy  episodes there may be a significant relationship.    To investigate the difference between lengthy and brief pauses,  we split the off-task episodes, as assessed by the detector, by their  length in two fashions. First, a median split was conducted in  terms of the length of an off-task episode. Episodes that were  shorter than the median 65.9 seconds were classified as brief  off-task episodes, whereas episodes that were longer than the  median were classified as lengthy off-task episodes. We also  conducted a quartile split, and compared the shortest-time quartile  (less than 26.0 seconds) of the off-task episodes to the longest- time quartile (longer than 124.9 seconds) of the off-task episodes.   In this manner, we can examine the difference in the correlation  between learning gain and the number of times a student spent  off-task, between brief off-task episodes and lengthy off-task  episodes.    We first analyze the median split. The relationship between the  number of off-task behavior episodes shorter than 65.9 seconds  and the post-test was not significant, r= -0.028, F(1,184)=0.142,  p=0.706. The relationship between the number of off-task  behavior episodes longer than 65.9 seconds and the post-test was  surprisingly also not significant, r= -0.056, F(1,184)=0.586,  p=0.445. These patterns remained non-significant even when  included the pre-test as a covariate. There was essentially no  difference between these two models, BIC = 0.     A similar pattern is seen when comparing the top quartile and  bottom quartile. The relationship between the number of off-task  behavior episodes shorter than 26.0 seconds and the post-test was  not significant, r= -0.101, F(1,184)=1.897, p=0.170. The  relationship between the number of off-task behavior episodes  longer than 124.9 seconds and the post-test was surprisingly also  not significant, r= -0.077, F(1,184)=1.098, p=0.296. This pattern  remained non-significant even when pre-test was included as a  covariate. The difference between these two models was not  significant, BIC = -1.501     4. DISCUSSION AND CONCLUSIONS  In general, this paper replicated previous findings showing a  negative relationship between off-task behaviors assessed using  field observations and learning. Our results showed that there was  a significant relationship between the percentage of time the  student spent off-task (assessed by the field observations) and  learning. There was also evidence for a relationship between the  proportion of off-task behavior squared, and learning. However,  the difference between the quadratic and linear models of this  relationship was not significant; suggesting that more complex  models than the model hypothesized by Carroll may not be  justified. There was also not strong evidence that brief off-task  episodes impact learning differently than longer off-task episodes.     One surprise in the findings was that the number of episodes  identified by the off-task detector was not predictive of learning,  either for brief episodes or lengthy episodes. Previous analyses  have found a significant relationship between the proportion of  off-task behavior identified by the detector and learning gains  [e.g. 3]. Those analyses were conducted in a broader data set,  including data from other versions of the same learning software,  and other tutor lessons. In this paper, we analyzed a more focused  data set, in order to explore different models in detail without  needing to consider this type of factor. But it is possible that  features specific to this sub-set of the data or the associated tutor  lesson led to the null result seen here. Therefore, it may be  valuable to replicate these analyses in a larger data set.     In summary, the data in this study appeared to accord with  Carrolls time on-task hypothesis [7]. Currently, there is not  sufficient evidence to suggest that a more complex relationship  exists between learning gain and off-task behavior, in terms of the  temporal aspects of off-task pauses. However, this issue may be  worth further investigation in additional data, before this result  can be considered conclusive.      236    5. ACKNOWLEDGEMENTS  We would like to thank Art Graesser for helpful comments and  suggestions, Angela Wagner for her help collecting the data re- analyzed here, and National Science Foundation, award           #SBE-0836012,  Toward a Decade of PSLC Research:  Investigating Instructional, Social, and Learner Factors in Robust  Learning through Data-Driven Analysis and Modeling .     6. REFERENCES  [1] Baker, R.S., Corbett, A.T., and Koedinger, K.R., Learning to   Distinguish Between Representations of Data: A Cognitive  Tutor That Uses Contrasting Cases. in International  Conference of the Learning Sciences, (2004), 58-65.   [2] Baker, R.S., Corbett, A.T., Koedinger, K.R., and Wagner,  A.Z., Off-Task Behavior in the Cognitive Tutor Classroom:  When Students  Game The System . in ACM CHI 2004:  Computer-Human Interaction, (2004), 383-390.    [3] Baker, R.S.J.d. and Yacef, K. The State of Educational Data  Mining in 2009: A Review and Future Visions. Journal of  Educational Data Mining, 1 (1), 3-17.   [4] Baker, R.S.J.d., Modeling and Understanding Students' Off- Task Behavior in Intelligent Tutoring Systems. in ACM CHI  2007: Computer-Human Interaction, (2007), 1059-1068.   [5] Blikstein, P., Using Learning Analytics to Assess Students'  Behavior in Open-Ended Programming Tasks. in Learning  Analytics Knowledge Conference (Banff, Canada, 2011).   [6] Caldwell, J.H., Huitt, W.G., and Graeber, A.O. Time Spent  in Learning: Implications from Research. The Elementary  School Journal, 82 (5), 171-180.   [7] Carroll, J. A Model for School Learning. Teachers College  Record, 64, 723-733.   [8] Cocea, M., Hershkovitz, A., and Baker, R.S.J.d., The Impact  of Off-task and Gaming Behaviors on Learning: Immediate  or Aggregate in 14th International Conference on Artificial  Intelligence in Education, (2009), 507-514.   [9] Gobel, Peter. Student Off-task Behavior and Motivation in  the CALL Classroom. International Journal of Pedagogies  and Learning, 4 (4), 4-18.    [10] Karweit, N. and Slavin, R.E. Measurement and Modeling  Choices in Studies of Time and Learning. American  Educational Research Journal, 18 (2), 157-171.   [11] Koedinger, K.R. and Corbett, A.T. Cognitive tutors:  Technology Bringing Learning Science to the Classroom. in  Sawyer, K. ed. The Cambridge Handbook of the Learning  Sciences, Cambridge University Press, 2006, 6178.   [12] Koedinger, K.R., Baker, R.S.J.d., Cunningham, K.,  Skogsholm, A., Leber, B., and Stamper, J. A Data  Repository for the EDM Community: The PSLC DataShop.  in Romero, C., Ventura, S., Pechenizkiy, M., and Baker,  R.S.J.d. eds. Handbook of Educational Data Mining, CRC  Press, Boca Raton, 2010, 43-56.   [13] Meijman, T.F. (1995). Mental Fatigue and the Temporal  Structuring of Working Times. in Human Factors and  Ergonomics Society Annual Meeting, (1995), 789-793.   [14] Raftery, A. Bayesian Model Selection in Social Research.  Sociological Methodology, 25, 111-163.    [15] Rowe, J., McQuiggan, S., Robison, J., and Lester, J. Off- Task Behavior in Narrative-Centered Learning  Environments. in 14th International Conference on Artificial  Intelligence and Education, (2009), 99106.   [16] Schofield, J.W. Computers and Classroom Culture.  Cambridge University Press, New York, 1995.   [17] Siemens, G. What are learning analytics, August 25, 2010.  Retrieved October 11, 2011, from Elearnspace:  http://www.elearnspace.org/blog/2010/08/25/what-are- learning-analytics/.                     237      "}
{"index":{"_id":"42"}}
{"datatype":"inproceedings","key":"Niemann:2012:CUH:2330601.2330659","author":"Niemann, Katja and Schmitz, Hans-Christian and Kirschenmann, Uwe and Wolpers, Martin and Schmidt, Anna and Krones, Tim","title":"Clustering by Usage: Higher Order Co-occurrences of Learning Objects","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"238--247","numpages":"10","url":"http://doi.acm.org/10.1145/2330601.2330659","doi":"10.1145/2330601.2330659","acmid":"2330659","publisher":"ACM","address":"New York, NY, USA","keywords":"attention metadata, clustering, higher-order co-occurrences, learning analytics, learning objects, usage data","Abstract":"In this paper, we introduce a new way of detecting semantic similarities between learning objects by analyzing their usage in a web portal. Our approach does not rely on the content of the learning objects or on the relations between the users and the learning objects but on usage-based relations between the objects themselves. The technique we apply for calculating higher order co-occurrences to create semantically homogenous clusters of data objects is taken from corpus driven lexicology where it is used to cluster words. We expect the members of a higher order co-occurrence class to be similar according to their content and present the evaluations of that assumption using two teaching and learning systems.","pdf":"Clustering by Usage: Higher Order Co-occurrences of  Learning Objects   Katja Niemann, Hans-Christian Schmitz,       Uwe Kirschenmann, Martin Wolpers    Fraunhofer Institute for Applied Information Technology  Schloss Birlinghoven   53754 Sankt Augustin, Germany   {katja.niemann, hans-christian.schmitz,  uwe.kirschenmann, martin.wolpers}   @fit.fraunhofer.de   Anna Schmidt, Tim Krones                                                                 Computational Linguistics Department  Saarland University   66041 Saarbrcken, Germany   {annas, tkrones}@coli.uni-saarland.de         ABSTRACT  In this paper, we introduce a new way of detecting semantic  similarities between learning objects by analyzing their usage in a  web portal. Our approach does not rely on the content of the  learning objects or on the relations between the users and the  learning objects but on usage-based relations between the objects  themselves. The technique we apply for calculating higher order  co-occurrences to create semantically homogenous clusters of  data objects is taken from corpus driven lexicology where it is  used to cluster words. We expect the members of a higher order  co-occurrence class to be similar according to their content and  present the evaluations of that assumption using two teaching and  learning systems.   Categories and Subject Descriptors  H.3.1 [Information Storage and Retrieval]: Content Analysis  and Indexing  linguistic processing; H.3.3 [Information Storage  and Retrieval]: Information Search and Retrieval  clustering;  J.1 [Administrative Data Processing]: education; K.3.1  [Computer Uses in Education] collaborative learning,  computer-assisted instruction (CAI), computer-managed   instruction (CMI), distance learning.   General Terms  Algorithms, Experimentation   Keywords  Attention metadata, usage data, clustering, higher-order co- occurrences, learning objects, learning analytics.   1. INTRODUCTION  In this paper we present a new way to cluster learning objects into  semantically homogenous groups without considering their  content or additional semantic metadata. We do so by just taking  the usage of the learning objects, i.e. the interaction of the learners  with the objects into account. For this purpose, we borrow the  technique of calculating higher order co-occurrences from corpus  linguistics where it is used to cluster semantically similar words  and apply it to the usage of learning objects.    In linguistics, first order co-occurrences of a word can be  calculated by taking the context of that word into account, e.g. the  sentences in which it occurs. Second order co-occurrences can be  calculated by taking the co-occurrences of the first order co- occurrences into account and so forth. For example: drink and  beer are co-occurrences, as well as drink and wine. Therefore,  beer and wine are co-occurrences in the first order co-occurrence  class of drink, this means beer and wine are second order co- occurrences. Higher order co-occurrence classes of words tend to  be semantically homogenous; this is to say they are similar  according to specific attributes, e.g. their direct hypernym.    While in linguistics the basic unit is a word that is used in  sentences, we transfer this approach to usage data and consider  learning objects as basic units that are used in sessions. Therefore,  the context of a learning object consists of all sessions the object  was accessed in and a session consists of all objects that were  accessed in that session. Thus, if two learning objects were used  in the same session, they are said to be co-occurrences.    In this paper we address the question whether higher order co- occurrence classes of learning objects become semantically  homogenous, like the analogue classes of words. This is a non- trivial question; it is by no means necessary that the semantic  convergence of words in higher order co-occurrence classes can  also be observed for entire learning objects. However, the  possibility to derive semantic similarity from usage is highly  promising for diverse applications of information retrieval in  learning settings and thus the question is worth solving. We  evaluate the potential of this approach using the usage data  collected in the systems MACE [1] and Travel Well [2].   The rest of the paper is structured as follows: In section 2 we  briefly report on the related work in the area of clustering  semantically similar data objects such as documents and pictures.  In section 3 we introduce higher order co-occurrences in corpus  linguistics and describe in section 4 how we adapt this idea to  calculate higher order co-occurrences of learning objects. We then  present the corresponding investigations using the MACE and the  Travel Well dataset as test-beds in order to illustrate our ideas in  section 5 and give a summary and an outlook in section 6.   2. RELATED WORK  Traditionally, representations describing the content of a data  object are needed to find semantically similar objects. There exist  several approaches to create such profiles and append semantic  features to different types of data objects. Some approaches are  based on the manual creation of such data, others on the automatic  extraction of semantic features.       Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00     238    A common approach to automatically extract semantic features  from text documents is based on the idea that the content of a text  can be represented by a list of characteristic keywords. Thus, by  extracting keywords one can construct a shallow semantic  representation of texts. A commonly applied measure for  extracting keywords is the TF-IDF [3] measure which is based on  the assumptions that on the one hand, the more often a term  occurs in a document the more representative it is, but that on the  other hand, the more often a term occurs in the entire collection of  all documents the less relevant it is to discriminate documents.  That is, keywords shall both be representative and discriminative.    However, a lot of learning object collections do not only contain  text documents but also images, videos or audio files. Thus,  additional automatic content extraction methods for different  media types are required. Pictures for instance can be analyzed  using content-based image retrieval (CBIR) methods [4] which  take into account the actual content of an image represented by  the features color, shape, texture or similar content-related  features. The information is extracted using automatic image  processing algorithms. Since the extracted information is on a  very low level, the results mostly contain only limited semantic  information not matching the users search queries. Given an  image of a specific person, CBIR methods can detect that a person  is shown in the picture but can usually not identify who it is. With  images showing complex scenes the identification and extraction  of semantic features often fails [5].    An alternative to the automatic extraction of semantic features is  manually created information. Traditional environments like  libraries use the expertise of librarians or archivists to create  metadata about their resources. These metadata typically contain  information such as title, author and further classifications. Since  this procedure requires manual creation of the metadata for all  resources, it is lengthy and time consuming and also requires a lot  of expertise. Further, maintenance of the data is problematic as  well.    One way to avoid such problems can be the use of social metadata  like ratings, tags or comments about learning objects which are  created by a community. The data can be used to create different  views on the resources, e.g. by filtering out content which is  tagged with a specific keyword or only displaying content which  is frequently used or highly rated. Tags in particular provide an  effective way to represent user interests and help the user to find  documents about a specific topic [6]. Thus, this approach can  effectively be used for different media types like images, videos  or audio files where it is still difficult to automatically extract the  appropriate semantic features [7]. A disadvantage of using such  social metadata is that it has to be added by a community and thus  often contains ambiguous or synonymic tags which make a  comparison difficult. Further, it is not assured that each tag is  assigned correctly which can lead to wrong results.    Therefore, it seems to be preferable to find a new way to cluster  semantically similar objects without considering the content but  only their usage. Collaborative filtering approaches [8] employ  this method by taking the relations between users and objects into  account using implicit and explicit feedback, e.g. if a user bought  a product or listened to a song. However, collaborative filtering  approaches are not suitable to cluster semantically similar data  objects but instead try to overcome semantic niches.   Our approach does not rely on the relations between users and  objects but on the relations between the objects themselves, i.e.  whether they are used in similar contexts. First approaches for   clustering objects based on their interrelations were mostly  conducted in the area of web mining. Rongfei et al. [9] create  object vectors containing the most significant co-occurrences of  the respective objects. Thereafter a DBSCAN algorithm is used to  cluster the objects based on these vectors.    Smith and Ng [10] follow a related approach. They use the  sessions an object occurred in to describe it. First, the sessions are  clustered into transaction groups. An object (which is represented  by an URL) is then characterized in terms of the transaction  groups, e.g. an URL was called in seven sessions that belong to  transaction group A and in five sessions that belong to group B.  The object vectors are then used as input for a self-organizing  map for clustering them.   In this paper we present an approach where not only the first order  co-occurrences of an object are taken into account but higher  order co-occurrences as well to cluster semantically related  objects.     3. BACKGROUND OF HIGHER ORDER  CO-OCCURRENCE CLUSTERING   3.1 Co-occurrences in Corpus Linguistics  If you want to know the meaning of a word, it is a good strategy  to look it up in a dictionary where in some cases you might find a  helpful definition. However, in most cases a definition (if there is  one at all) might not be sufficient to correctly understand the  words meaning as the word's context is often needed for  clarification. The words strong and powerful, for example, have  highly related meanings. However, we can say strong tea while  we cannot say powerful tea. Powerful drug though is acceptable  [11]. Definitions of the words meanings will most probably not  cover such differences. Therefore, dictionaries usually give  contexts in which a word typically occurs to illustrate the actual  word usage.    Context is considered to be significant for the meaning of a word.  Firth [12] says you shall know a word by the company it keeps.  (See [13] for an overview on the linguistic tradition related to  Firth.) The company a words keeps  its co-occurring words   contributes to its meaning. Two words might just co-occur by  accident. However, the co-occurrence might also be relatively  frequent and thus statistically significant. Statistically significant  co-occurrences reveal close relationships between the co- occurring words or their meanings, respectively: they are used to  detect multi-word expressions (New York), idioms (kick the bucket  [14]) or constructions with a milder idiomatic character  (international best practice [11]).   When calculating the co-occurrences of a word, one can consider  different definitions on how near two words must be to co-occur  depending on the purpose of the analysis. It is possible to only  consider the direct neighbors of a word, as in the examples above,  to use a static frame of words or to consider the whole sentence.   The significant co-occurrences form the co-occurrence class of the  respective word. For example, the co-occurrence class of the word  dog is made up of the words bark, growl and sniff among others.  It can then be examined whether words significantly co-occur in  co-occurrence classes. These words again form another co- occurrence class, namely a higher order co-occurrence class. For  instance, feed and dog are co-occurrences, as well as feed and cat.  Therefore, dog and cat are second order co-occurrences.    After some iterations the elements in the higher order co- occurrence classes become stable and semantic homogenous.    239    Heyer et al. [15] show this for the co-occurrences of IBM, among  other words. Their investigations are based on text corpora  collected for the portal wortschatz.uni-leipzig.de (concerning the  German treasury of words). The first co-occurrence class is rather  heterogeneous, containing words like computer manufacturer,  stock exchange, global and so on. After some iterations of  computing higher order co-occurrence classes, however, the  classes become more homogenous and stable. The co-occurrence  class of tenth order only contains names of other computer-related  companies like Microsoft, Sony etc.   3.2 Significance of Co-occurrences   3.2.1 Calculation of Significance Values   When calculating the significance of a co-occurrence, its  frequency is not sufficient as a measure, but the marginal  frequencies of the individual words also have to be taken into  account. For example, the bigram is to is one of the highest  recurrent word pairs in the Brown corpus with 260 occurrences.  However, the word is occurs about 10.000 times and the word to  occurs about 26.000 times in the corpus that contains altogether 1  Million words. Therefore, even if words were sequenced  randomly we would expect them to co-occur about 260 times  together and they are not considered as significant co-occurrences  [14]. We assume that the same holds true for the distribution of  objects in sessions and apply the same measures as in corpus  linguistics to calculate significant co-occurrences of learning  objects.   There are several measures that can be used to calculate how  strong words or objects are attracted by each other. These  measures can be divided into measures of effect size that calculate  how much the observed co-occurrence frequency exceeds the pre- defined expected frequency (e.g. MI, Dice, odds ratio) and  measures of significance that measure how unlikely the null  hypothesis is that the words or objects are independent (e.g. z- score, t-score, simple-ll, chi-squared, log-likelihood). For more  details see [16] where more than 30 different measures are  discussed.   In the following, we present a measure of significance that is  based on the Poisson distribution. We choose this measure as it  was already successfully applied in corpus linguistics for  calculating higher order co-occurrences of words, e.g. by Heyer et  al. [15]. The comparison by Bordag of the performance of  different significance measures for co-occurrences, namely DICE  coefficient, Mutual Information measure, Lexikographers Mutual  Information, t-score, z-score, two log-likelihood based measures  and two Poisson distribution based measures, supports this choice  [17]. Furthermore, a formal proof justifying the assumption of a  Poisson distribution for co-occurring words in a corpus, if the  frequency of most words is much smaller than the corpus size, is  given in [18].    Thus, following Heyer at al. we define the significance for the  learning objects A and B based on the Poisson distribution as:     ,  =           !           , with  =     !      (1)      a = frequency of contexts containing A  b = frequency of contexts containing B  n = frequency of all contexts     Under the assumptions   #   > 2.5 and k > 10, with k being the  frequency of all contexts containing A and B together, formula (1)  can be simplified:      ,                                                          (2)     3.2.2 Detection of a Suitable Threshold  There are various ways of deciding whether a co-occurrence is  significant or coincidental, e.g. by ranking or by using a threshold.  Ranking means that the co-occurrences are sorted by their  significance values and only the best n co-occurrences are  selected. When using a threshold, only co-occurrences with a  significance value higher than the threshold are selected.  However, there is no standard scale of measurement to draw a  clear distinction between significant and non-significant co- occurrences [14]. Therefore the calculation of a suitable n or a  suitable threshold (depending on the approach) is an exploratory  investigation.   4. HIGHER ORDER CO-OCCURRENCE  CLUSTERS  Sessions are used as input for the calculation of higher order co- occurrences. If timestamps are available, a session is made up of  all learning objects accessed by a user without a break longer than  an hour between two accesses. After a break of at least an hour, a  new session starts. Currently, there are no further lower or upper  limits for the size of a session. If only a date is stored for an  action, a session comprises all activities of a user at one day.    Duplicate learning objects in the sessions are deleted. For  example, in the session <A, B, B> the object A was accessed once  and the object B was accessed twice. Without deletion, we would  consider two co-occurrences between A and B that aroused from  one context and this would contort the further calculations.   After the sessions are created, they are taken as input to generate  significant first order co-occurrences. For calculating the  significance of the co-occurrences we use the formula given in  section 3.2.2. All significant co-occurrences of a learning object  together form its first order co-occurrence class. The first order  co-occurrence classes of all learning objects serve as input for the  calculation of the second order co-occurrence classes which then  form the input for the calculation of the significant third order co- occurrence classes and so forth.    To clarify this with an example, lets take the following sessions  S1, S2, and S3 as input:     S1 = <A, B, C, D>   S2 = <A, B, D>   S3 = <D, E, F>     The calculation of the first order co-occurrences results in the  following (temporary) classes for A, B, C, D, E, and F. The total  frequencies of the objects and their co-occurrences are given in  brackets, e.g. A and D co-occur 2 times, whereas B occurs 2 times  and D 3 times in total.          240    A (2)  B (2), C (1), D (2)   B (2)  A (2), C (1), D (2)   C (1)  A (1), B (1), D (1)   D (3)  A (2), B (2), C (1), E (1), F (1)   E (1)  D (1), F (1)   F (1)  D (1), E (1)   The given frequencies are used to calculate the significance values  for each co-occurrence based on the Poisson distribution (see  section 3.2.1). For demonstration we use exemplary significance  values in this example. (For successfully applying the presented  measure of significance, a large collection of sessions is required.)  The significance values of the co-occurrences are given in  brackets.   A  B (1.7), C (1.4), D (1.7)   B  A (1.7), C (1.4), D (1.7)   C  A (1.4), B (1.4), D (1.1)   D  A (1.7), B (1.7), C (1.1), E (1.1), F (1.1)   E  D (1.1), F (1.4)   F  D (1.1), E (1.4)   Given a threshold of 1.3, the final co-occurrence clusters can be  calculated by deletion of all co-occurrences with a significance  value lower than the threshold.   First order co-occurrence class for A: <B, C, D>   First order co-occurrence class for B: <A, C, D>   First order co-occurrence class for C: <A, B>   First order co-occurrence class for D: <A, B>   First order co-occurrence class for E: <F>   First order co-occurrence class for F: <E>   The generated classes can now be used to calculate the second  order co-occurrences. This is done the same way as calculating  first order co-occurrences but taking the first order co-occurrence  clusters as input and not the sessions. This leads to the following  second order co-occurrences for A, B, C, D (please note that the  second order co-occurrence classes for E and F are empty, so they  are not shown here). The total frequencies of the co-occurrences  and exemplarily significance values are given in brackets.    A (3)  B (2; 1.5), C (1; 1.3), D (1; 1.3)   B (3)  A (2; 1.5), C (1; 1.3), D (1; 1.3)    C (2)  A (1; 1.3), B (1; 1.3), D (2; 1.7)   D (2)  A (1; 1.3), B (1; 1.3), C (2; 1.7)   Using again the threshold of 1.3, the following second order co- occurrence classes arise:   Second order co-occurrence class for A: <B, C, D>   Second order co-occurrence class for B: <A, C, D>   Second order co-occurrence class for C: <A, B, D>   Second order co-occurrence class for D: <A, C, D>   These classes can now be used as input for the calculation of third  order co-occurrences and so forth. The calculation stops when the  classes get stable, i.e. they do not change anymore in further  iterations   5. CLUSTERING RESULTS   5.1 Testbeds   5.1.1 MACE  The MACE (Metadata for Architectural Contents in Europe)  project relates digital learning resources about architecture, stored  in various repositories, with each other across repository  boundaries to enable new ways of finding relevant information  [19]. While interacting with the MACE portal, users are  monitored and their activities are recorded as CAM  (Contextualized Attention Metadata, [20], [21]) instances.  Activities include search, access and metadata provision activities  like tagging and rating. The CAM instances used for the  evaluation were collected from September 2009 until April 2010  and comprise at least a timestamp as well as a user identifier and  an item identifier. The actions considered for a learning object to  be part of a session are goToPage, i.e. the user leaves the MACE  portal to access the original learning object, and  getMetadataForContent, i.e. the user accesses the metadata of a  learning object at the MACE portal, e.g. its ratings or user tags.  For the calculation of the higher order co-occurrence clusters, we  were thus able to take 46.641 events into account that took place  in 2.449 sessions. On average, about 11 learning objects were  accessed per session. Overall, 3710 distinct learning objects were  accessed [1].    MACE stores the metadata representations of the learning objects  on a central server. The representations base on the MACE  application profile which in turn is based on the Learning Object  Metadata (LOM) standard [22]. The MACE application profile  comprises several categories that are used to specify a learning  object in more detail, such as the general category where basic  information about the learning object is stored and the annotation  category where comments about a learning object's usefulness for  education and the comments origins can be stored.   We calculate the metadata-based similarity of all pairs of learning  objects to get a reference value for evaluating the higher order co- occurrence clusters. Each learning object holds one or more titles  and descriptions and is marked with the learning resource types it  comprises, e.g. a text containing a figure is marked with the  learning resource types narrative text and figure. As these terms  belong to a controlled vocabulary, they are comparable.    MACE also offers users and domain experts the possibility of  editing parts of the metadata, namely tags, classifications and  competencies. Tags are free text and can be assigned to learning  objects by logged in users. Classifications and competencies are  each defined in a controlled vocabulary and can only be set by  domain experts. The classification vocabulary is a taxonomy  consisting of 2884 terms. The competency vocabulary contains  107 terms to describe the suitability of learning objects for the  acquisition of special competencies, e.g. Knowledge of internal  environment control and Understanding interaction between  technical and environmental issues.    To compare the MACE learning objects, document vectors  describing them are generated by considering the following  assortment of available information: titles, descriptions, learning  resource types, user tags, classifications and competencies. Before   241    calculating the metadata similarity, the titles and descriptions are  pre-processed. After removing stop words the remaining words  undergo a stemming using the Snowball Stemmer [23].    The metadata-based similarity is then calculated using the cosine  similarity, i.e. measuring the similarity between two vectors by  calculating the cosine of the angle between them [24].   5.1.1 Travel Well  The dataset [2] was collected on the Learning Resource Exchange  (LRE) portal that makes open educational resources available  from more than 20 content providers in Europe and elsewhere.  These learning resources exist in multiple languages and conform  to a variety of national and local curricula. The registered users,  mostly primary and secondary teachers, come from a number of  different European countries.    The dataset contains information about the rating and tagging  behavior of 98 registered users over a period of six months  (August 2008  February 2009). For each user activity, the date,  user id, item id and the tag, respectively the rating is stored. As  there is no timestamp but only the date, a session comprises all  activities conducted by a user in one day. Overall, 14248 events  took place in 255 sessions where each session comprises 55  distinct learning objects on average. 75 users rated 1698 unique  objects on a scale of 1 to 5 for usefulness; each of these objects  was rated 1.3 times on average.  Additionally, 79 users tagged  1838 unique objects with 12041 tags in total; consequently each  object was assigned with 6.5 tags on average.   Information that is available about the users and learning objects  is e.g. mother tongue, spoken languages, and subjects the user is  interested in as well as title, metadata provider, language,  classification keywords, and intended end user age for the  learning objects.   Similar to the MACE dataset, we calculate the metadata-based  similarity of all object pairs to get a reference value for evaluating  the higher order co-occurrence clusters. We do so by taking the  classification keywords and the tags into account. Since an item  cannot be tagged more than once with the same keyword, we  created a binary vector for each item and used the Tanimoto  coefficient [25] for calculating the metadata-based similarity.   5.2 Usage-based Clustering: Results  Before clustering, we need to decide how to distinguish whether a  co-occurrence is significant or not. This can be done by ranking  and choosing the first n co-occurrences or by using a threshold.  Since in the given datasets some learning objects can have more  learning objects they are similar to than others, we decided to use  a threshold. The calculation of a suitable threshold is an  exploratory investigation. One possible indication for the quality  of a threshold without considering the content is the cluster size  and the amount of clusters. Another way to test the quality of a  threshold is to manually check some clusters for their semantic  consistency. Additionally, even if only available for some learning  objects, semantic metadata describing the objects can be used to  automatically train a suitable threshold.   Here, we used the MACE dataset including semantic metadata to  find the best fitting threshold to create meaningful and  semantically consistent clusters, which is 1.55. The use of higher  thresholds resulted in a lot of very small clusters and the use of  lower thresholds resulted in a few small and a few very big  clusters (more than 1000 learning objects). Additionally, we used  the semantic metadata to manually and automatically validate this  choice. We then applied this threshold directly on the Travel Well   dataset to test if the threshold is transferable or must be trained  new for each environment.   5.2.1 MACE  Using 1.55 as threshold, the higher order co-occurrence classes  became stable after the fifth iteration. This means that in further  iterations the classes did not change anymore. The calculations  resulted in 184 clusters that contain on average 63 learning  objects.      Figure 1. Cluster size distribution for MACE      The smallest cluster contains three learning objects and the  biggest one 719 learning objects. About 78% of the clusters  contain 30 learning objects at maximum and only 17% of the  clusters contain more than 100 learning objects (see Fig. 1).         Figure 2. Learning object distribution for MACE      The higher order co-occurrence based clustering is not a hard  clustering. Therefore, a learning object can belong to more than  one cluster, however, about 70% of the learning objects belong to  3 clusters at maximum and there is no learning object that belongs  to more than 9 clusters (see Fig. 2).   5.2.2 Travel Well  As for MACE, the usage classes became stable after the fifth  iteration using the threshold of 1.55. The 1838 learning objects  were clustered into 100 clusters that contain on average 142  learning objects, see Fig. 3.      86  41  17  2 3 5  2 1 0 3  31  0  10  20  30  40  50  60  70  80  90  100  1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-80 81-90 91-100 >100  n u  m b  e r   o f   cl u  st e  rs  cluster sizes  808  636  1160  201  344  490  55 21 5  0  200  400  600  800  1000  1200  1400  1 2 3 4 5 6 7 8 9  n u  m b  e r   o f   d a  ta  o  b je  ct s  number of clusters a data object belongs to  242       Figure 3. Cluster size distribution for Travel Well      The smallest cluster contains 6 learning objects and the biggest  one 420. It is noticeable that the clustering of the Travel Well  dataset, where each learning object is contained in about 7 clusters  on average (see Fig. 4), resulted in clearly bigger clusters than the  MACE clustering. This is due to the fact, that the sessions in  Travel Well which contain 55 objects on average are significantly  larger than in MACE, where a session only contains 11 learning  objects on average. The Travel Well sessions might be this large  as there are no timestamps in the dataset but only the date and  therefore all activities conducted by a user in a day form one  session independent of things such as potential learning breaks.      Figure 4. Learning object distribution for Travel Well     5.3 Evaluation  The following evaluation answers two questions: (a) Do all the  learning objects of the same clusters have a significantly different  relatedness than learning objects randomly drawn from the set of  all learning objects (b) If so, does the clustering lead to a  significantly lower or higher relatedness and what is the relation  of the significantly improved or worsened clusters These issues  will be resolved by applying two statistical methods, namely the  Kruskal-Wallis-Test [26] for the first question and a test for the  student-t-distribution [27] for the second one.   On a first glance the ANOVA [29] seems appropriate to check  whether the clustering does have an overall effect. Unfortunately,  this approach must be dropped as the ANOVA does have  preconditions on the data that were not met. For one, semantic  relatedness is not normally distributed as a Kolmogorov-Smirnov- Test revealed [28]; secondly the homogeneity of variances within   clusters is not given. Semantic relatedness tends to be power-law  distributed. Fig. 5 and Fig. 6 which show the distribution of the  pair-wise metadata-based similarities in MACE und Travel Well  support this claim.         Figure 5. Distribution of the pair-wise metadata-based   similarities in MACE      Because of these reasons, the non-parametric alternative for the  ANOVA, the Kruskal-Wallis-Test [26] was chosen. Kruskal- Wallis is based on ranked data and does not make such strong  assumptions as an underlying normal distribution or homogeneity  of variances.         Figure 6. Distribution of the pair-wise metadata-based   similarities in Travel Well      As for the second question and approach, i.e. whether the  clustering leads to an improvement of the semantic relatedness, it  is assumed that the means of the relatedness values within clusters  at least t-distribute around the overall mean-value of the  population of MACE respectively Travel Well objects. This is a  known fact based on the central limit theorem, according to which  the means of samples tend to be normally distributed around the  overall mean of the population if the sample size is equal to or  greater than 30. For smaller samples the means are student-t- distributed. The according values can be easily computed  according to formula 3:      14  9  7  5  16  6 6 7  11 10  8  0  2  4  6  8  10  12  14  16  18 n  u m  b e  r  o  f  cl  u st  e rs  cluster sizes  19 0  85  31  100  49  579  48  86  223  98 75 87  7 17 15 14 0 14 11  0  100  200  300  400  500  600  700  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 >19  n u  m b  e r   o f   d a  ta  o  b je  ct s  number of clusters a data object belongs to  5856201  398904  230059 187649  127437 60239  23868  11108 6638  3957 2434  1524 1005  702 516 365 236 156 114 509  0  1000000  2000000  3000000  4000000  5000000  6000000  7000000  n u  m b  e r   o f   o b  je ct   p a  ir s  metadata-based similarity  1654643  35641 3056713588  10155 10271  5129 594 3109 78  4205 203 372 177 31 104 507 8 1 638  0  200000  400000  600000  800000  1000000  1200000  1400000  1600000  1800000  n u  m b  e r   o f   o b  je ct   p a  ir s  metadata-based similarity  243                                     )  =  * ,-./0                                         (3)     1: Mean of the sample  2: Mean of the population  3: Sample size, n-1 are the degrees of freedom (df)  56*: Estimated standard error for population of means of size n    In this case we take advantage of the fact that the specific values  of the population, i.e. mean and standard deviation, can directly be  computed and thus do not have to be estimated. This leads to  formula 4:                                         )  =  * , /0                                          (4)      Here, the estimated standard error is replaced by the computed  standard error for the population.    To check whether several clusters deviate significantly from the  mean of the population the relevant t-value has to be calculated  and must then be checked against the t-value needed for  significance. As this is not a standard t-test, alpha-errors that  would bias the results by chance, do not occur [30].   5.3.1 Kruskal-Wallis-Test  The Kruskal-Wallis-Test evaluates whether the medians of certain  groups ranked data (dependent variable) differ systematically or  not. Therefore, the test distinguishes between the entire set of  objects and the cluster set of the higher order co-occurrence  clustering approach, i.e. it is tested whether the semantic  relatedness values of the respective clustering  all partition sets  taken together  differ significantly from the overall median of  relatedness of the entire MACE respectively Travel Well set. The  respective Null-hypothesis (H0) can be formulated as such:   H0: The learning objects are taken from the same population.  Higher order co-occurrence based clustering does not have an  effect on semantic relatedness.   Table I displays the result of the tests.  It shows that the clustering  does have a significant impact on semantic relatedness      Table 1. Kruskal-Wallis-Test   Similarity   Measure  Chi-squared df p   Cosine Similarity  for MACE   743619.8 184 <0.001   Tanimoto Similarity  for Travel Well   109288.9 100 <0.001      For both datasets, the H0 must be rejected, H1 accepted:   H1: The learning objects are not taken from the same population.  The higher order co-occurrence clustering has an effect on  semantic relatedness.   5.3.2 Test for Student-t Distribution  The Kruskal-Wallis-Test tests whether the entire clustering has an  effect on semantic relatedness. What remains is to compare the  individual cluster sets with the overall set of all objects. As in our  case the population of all MACE respectively Travel Well objects  is available, no additional post-hoc tests must be conducted. To  compare individual cluster sets with the overall set, it can simply  be tested whether the cluster means of semantic relatedness differ   significantly from the overall mean of the population. To this end,  the null hypothesis for each single cluster set with the same  sample size is formulated:   H0: The mean semantic relatedness of a specific cluster set does  not differ from the mean semantic relatedness of the whole  population of learning objects.   Respectively, H1 states:   H1: The mean semantic relatedness of a specific cluster set  systematically differs from the mean semantic relatedness of the  whole population.   The above described Student t-test to evaluate H0/1 was applied  to every cluster set of the clustering. We did a two-sided test on a  5%-significance-level for both datasets, see table II.     Table 2. t-distribution test   Similarity   Measure   Sig-level,   test   No.   Clusters  p+a p-b   Cosine  Similarity for   MACE   5%,         2-sided   184 80,4% 0,5%   Tanimoto  Similarity for  Travel Well   5%,         2-sided   100 96% 2%      a. Percentage of clusters significantly over-average (p<0.05)  b. Percentage of clusters significantly under-average (p<0.05)     The semantic density of the clear majority of cluster sets in the  MACE dataset as well as in the Travel Well dataset is  significantly higher than the semantic density of the overall  population (where semantic density is defined as the mean value  of semantic relatedness). This means, the chance that the included  learning objects show a higher similarity than at a random draw is  about 80% for MACE respectively 96% for Travel Well. This is a  very good result supporting our initial hypothesis. Furthermore  hardly any aggravation can be documented.   5.3.3 Discussion of the Results  We tested our approach on two different test data sets, MACE and  Travel Well. We admit that in both data sets the semantic  metadata are not perfectly accurate. Additionally, the definition of  a session is not straightforward in the Travel Well test set (in  MACE, it is). However, the test data are still comparatively good   large parts of both data sets have been hand-tagged by experts   and we do not find 'better' data sets available. Thus, a 'faute de  mieux' argument applies.   For the evaluation, we conducted the Kruskal-Wallis-Test to test  whether the semantic relatedness values of the higher order co- occurrence classes differ significantly from the overall median of  relatedness of the entire MACE respectively Travel Well dataset.  Additionally, we checked the cluster means on the student-t- distribution to see which higher order co-occurrence-based  clusters have an effect on semantic relatedness. The results clearly  indicate that the chosen approach is promising, i.e. semantically  similar learning objects are grouped together and learning objects  that are not semantically similar are separated. Calculation of  effect size is not needed as we compute the amount of significant  clusters directly with the given distribution of the population.    Effect size thus automatically is given by the significance level of  the student-t-distribution. Additionally, the tests show that the  threshold calculated for the MACE dataset is transferable to the   244    Travel Well dataset even if they deal with objects from different  domains. However, the datasets are still collected in the same  area, i.e. Technology Enhanced Learning, and it needs to be tested  if the threshold is also transferable to other areas such as web  browsing.   5.4 Tag Clouds Describing the Clusters  We have shown that the objects in the usage-based clusters are  semantically similar. Now the question arises what actually makes  them similar or  in other words  what are the features that make  them similar. Learning objects in MACE and Travel Well can for  example be similar according to their topic, their competence  level or their type.      Table 3. MACE cluster containing objects about energy   efficient heating in private buildings      As a first investigation to get a deeper impression about the  clusters, we calculate a tag cloud for each cluster. We do so, by  taking the user tags into account that were used most in the  respective cluster. In this subchapter, we present example clusters  for the MACE as well as for the Travel Well dataset.     Table 4. MACE cluster containing objects about Spanish   construction material      The first MACE cluster comprises 23 learning objects that were  assigned with 256 tags in total, whereas the 10 tags that were used   most are presented (see Table 3). As these are German tags, the  English translation is given in the second column. The third  column states how many of the learning objects in the cluster are  assigned with the respective tag. As can be seen all learning  objects in this cluster deal with energy efficient heating in private  buildings.   The second MACE cluster (see Table 4) comprises 190 learning  objects that were assigned with 2793 tags in total which are  mostly in Spanish. Most of the learning objects in that cluster deal  with construction material that was produced and used in Spain.      Table 5. Travel Well cluster containing objects dealing with   English learning material for children      The first example cluster from the Travel Well dataset contains  348 objects that are assigned with 2678 tags in different languages  and contains learning objects dealing with English learning  material for children (see Table 5).   It is noticeable that the Travel Well clusters are not as pure as the  MACE clusters. Although they are significantly semantically  dense, they often contain objects dealing with two topics. This  means, that two clusters were wrongly combined to one cluster.  The following cluster for example (see Table 6) contains 136  objects from the information and communication technology and  the geographical domain.      Table 6. Travel Well cluster containing objects from the ICT   and geographical domain        Tag  English   Translation   Frequency in   Cluster   kologisch ecological 23   Photovoltaikanlage  photovoltaic  facility   15   Wohnen living 15   Wrmerckgewinnung heat recovery 10   Solarthermie solar heat 10   Wrmetauscher heat exchanger 9   Passivhaus passive house 8   Holzfeuerungsanlage  wood combustion  plant   8   Wrmepumpe heat pump 7   Effizienzhaus efficient house 6   Tag English   Translation   Frequency in   Cluster   material en  construccion   construction  material   188   arquitectura architecture 188   construccin construction 101   empresa concern 86   cermica ceramics 85   elementos arquitectura architectural  elements   82   productos  products 62   acabados finishes 49   espaa Spain 45   revestimientos coatings 44   Tag  English   Translation   Frequency in   Cluster   English  English 134   Efl elf-english.com 74   vocabulary vocabulary 60   Interactive Interactive 60   angol  English 49   grammar  grammar 33   fun  fun 29   animals animals 24   worksheet worksheet 23   beginner  beginner 22   Tag English   Translation   Frequency in   Cluster   hardver  hardware 36   hardware hardware 31   perifria  periphery   18   szoftver  software 12   rengshullm seismic wave 9   kontinensvndorls continental drift 6   csatlakoz  electrical outlet 6   informatika information science 6   lemezmozgsok plate movements 5   ghajlat  climate 5   245    However, tests with varying thresholds show that the threshold  calculated for the MACE dataset is also the most suitable  threshold for the Travel Well dataset. Most probably the clusters  containing objects that deal with two topics arose from the  missing timestamps in the Travel Well dataset and the  consequential inexact session calculation.   6. Conclusion  We borrowed the technique of calculating higher order co- occurrences from corpus linguistics and applied this technique to  user sessions to group semantically similar learning objects  without considering their content. The significant co-occurring  objects form the co-occurrence class of the respective learning  object. It was examined whether objects significantly co-occur in  co-occurrences classes. These objects again form another co- occurrence class, namely a higher order co-occurrence class. We  investigated the assumption that - similar to words - the learning  objects in the higher order co-occurrence classes become stable  and semantic homogenous.   We clustered the learning objects contained in the MACE and  Travel Well datasets and evaluated the clustering using the  Kruskal-Wallis-Test and the student t-distribution test to answer  the questions whether all the learning objects of the same clusters  have a significantly different relatedness than learning objects  randomly drawn from the set of all learning objects. We then  asked: does the clustering lead to a significantly lower or higher  relatedness and what is the relation of the significantly improved  or worsened clusters Furthermore we created tag clouds  describing the clusters to identify the topics of the clusters. The  evaluation results clearly indicate that this approach is promising  and we will continue on this track.   We will further work on identifying an appropriate threshold to  separate significant and coincident higher order co-occurrences.  To this end, we will conduct additional experiments in other test  beds to explore domain-dependent distinctions that need to be  addressed.    In order to test the usefulness and quality of this clustering  approach in real world settings, we will embed the technique in  recommender systems. For example, within the MACE portal, a  recommender could provide resources thematically related to the  ones used in the current session. Furthermore the cluster results  can give information about the relationship between courses with  regards to content and support the teacher with her teaching  activities.   7. ACKNOWLEDGMENTS  The research leading to these results has received funding from  the European Community's Seventh Framework Program  (FP7/2007-2013) under grant agreement no 231396 (ROLE  project).   8. REFERENCES  [1] Niemann, K., and Martin, W. (2010) dataTEL challenge:   CAM for MACE. In 1st Workshop on Recommender  Systems for Technology Enhanced Learning (RecSysTEL  2010).  http://www.teleurope.eu/pg/pages/view/50649/   [2] Vuorikari, R. and Massart, D. (2010) dataTEL challenge:  European Schoolnet's Travel well dataset. In 1st Workshop  on Recommender Systems for Technology Enhanced  Learning (RecSysTEL 2010).  http://eqnet.eun.org/c/document_library/get_filefolderId=12 421&name=DLFE-304.pdf    [3] Baeza-Yates, R., Ribeiro-Neto, B. (1999). Modern  Information Retrieval. pp 29-30. Addison Wesley / ACM  Press, New York.   [4] Smeulders, A. W., Member, S., Worring, M., Santini, S.  (2000). Content-Based Image Retrieval at the End of the  Early Years. In: Analysis, 22(12), pp. 1349-1380.   [5] Greiner, F. (2009). Benutzerorientierte Evaluation von  Content Based Image Retrieval-Systemen mit automatischer  Beschlagwortung. Master Thesis. University of Regensburg.   [6] Zhao, Y. E. (2008). Tag-based Social Interest Discovery. In:  Social Networks, pp. 675-684.   [7] Duan, M., Ulges, A., & Breuel, T. M.: Style Modelling for  Tagging Personal Photo Collections. Style (DeKalb, IL).   [8] Burke, R. (2002). Hybrid Recommender Systems: Survey  and Experiments. User Modeling and User-adapted  Interaction, 12(4), pp. 331-370.   [9] Rongfei, J., Maozhong, J. Xiaobo, W. (2010). Web Object  Clustering Using Transaction Log. In Proc. Of 3rd  International Conference on Knowledge Discovery and Data  Mining, IEEE.   [10] Smith, K.A., Ng, A. (2003). Web page clustering using a  self-organizing map of user navigation patterns. In: Decision  Support Systems 35 (2003) pp. 245-256, Elsevier Sciences.   [11] Manning, C., Schtze, H. (1999). Foundations of Statistical  Natural Language Processing. MIT Press.    [12] Firth, J. (1957). A Synopsis of Linguistic Theory 193055.  In: Studies in Linguistic Analysis, Oxford: The Philological  Society.   [13] Stubbs, M. (1996). Text and Corpus Analysis. Blackwell  Publishers Ltd, Oxford, Malden.   [14] Evert, S. (2009) Corpora and collocations. In: Ldeling, A.,  Kyt, M. (eds.): Corpus Linguistics. An International  Handbook. Volume 2. De Gruyter, Berlin.   [15] Heyer, G., Quasthof, U., Wittig, T. (2006). Text Mining:  Wissensrohstoff Text. Konzepte, Algorithmen, Ergebnisse.  W3L-Verlag, Herdecke.   [16] Evert, S. (2004). The Statistics of Word Cooccurrences:  Word Pairs and Collocations. Dissertation, University  Stuttgart.   [17] Bordag, S. (2008). A Comparison of Co-occurrence and  Similarity Measures as Simulations of Context. In Proc.  CICLing 2008, LNCS 4919, pp. 52-63, Springer Verlag  Berlin Heidelberg.   [18] Holtsberg, A., Willners, C. (2011). Statistics for sentential  co-occurrence. Working Papers 48, pp. 135-148.   [19] Stefaner, M., Dalla Vecchia, E., Condotta, M., Wolpers, M.,  Specht, M., Apelt, S., Duval, E. (2007). MACE  enriching  architectural learning objects for experience multiplication.  In: Proceedings of the 2nd European Conference on  Technology Enhanced Learning (Crete, Greece, September  17  20, 2007). EC-TEL 2007. LNCS, vol. 4753, Springer,  Heidelberg, 2007, pp. 322-336.   [20] Wolpers, M., Najjar, J., Verbert, K., Duval, E. (2007).  Tracking Actual Usage: the Attention Metadata Approach .  In: Educational Technology & Society 10 (3), 2007, pp. 106- 121.   246    [21] Schmitz, H.-C., Wolpers, M., Kirschenmann, U., Niemann,  K. (2011). Contextualized Attention Metadata. In: Roda, C.  (eds.): Human Attention in Digital Environments, Cambridge  University Press, Cambridge, US.   [22] LOM IEEE Standard for Learning Object Metadata (2002).  IEEE Std 1484.12.1.   [23] Porter, M.-F. (1980). An algorithm for suffix stripping. In:  Program 14: 1980, pp. 130-137.   [24] Tan, P.-N. Steinbach, M., Kumar, V. (2005). Introduction to  Data Mining. Chapter 8, pp. 500. Addison-Wesley   [25] Salton, G., and McGill, M. J. (1983). Introduction to modern  information retrieval. New York: McGraw-Hill.    [26] Sawilowsky, S., Fahoome, G. (2005). KruskalWallis Test.  Encyclopedia of Statistics in Behavioral Science.   [27] Abramowitz, M.; Stegun, I. A., eds. (1965). Handbook of  Mathematical Functions with Formulas, Graphs, and  Mathematical Tables. Chapter 26, pp. 948. New York:  Dover.   [28] Massey F. J. (1951).  The Kolmogorov-Smirnov Test for  Goodness of Fit. Journal of the American Statistical  Association. Vol. 46, No. 253, pp. 68-78.   [29] Rosenbaum, P. R. (2002). Observational Studies. 2nd edn.,  pp. 40. New York: Springer-Verlag.   [30] Bortz, J. (2005). Statistik fr Human- und  Sozialwissenschaftler, 6th edition. Heidelberg: Springer,  Medizin Verlag.         247      "}
{"index":{"_id":"43"}}
{"datatype":"inproceedings","key":"Cobo:2012:UAH:2330601.2330660","author":"Cobo, Germ'an and Garc'ia-Sol'orzano, David and Mor'an, Jose Antonio and Santamar'ia, Eug`enia and Monzo, Carlos and Melench'on, Javier","title":"Using Agglomerative Hierarchical Clustering to Model Learner Participation Profiles in Online Discussion Forums","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"248--251","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330660","doi":"10.1145/2330601.2330660","acmid":"2330660","publisher":"ACM","address":"New York, NY, USA","keywords":"educational data mining, hierarchical clustering, learner behavior modeling, learning analytics, online discussion forums","Abstract":"Online discussion forums are a key element in virtual learning environments. The way learners participate in discussion boards can be a very useful source of indicators for teachers to facilitate their tasks. The use of a two-stage analysis strategy based on an agglomerative hierarchical clustering algorithm is proposed in this paper to identify different participation profiles adopted by learners in online discussion forums. Different parameters are used to characterize learners' activity (amount of posts, rhythm, depth of threads, crossed replies, etc). Participation profiles are identified and analyzed in terms of behavior and performance.","pdf":"Using agglomerative hierarchical clustering to model   learner participation profiles in online discussion forums   Germn Cobo   IMT Department, Universitat Oberta  de Catalunya (UOC)   Barcelona, Spain  +34 93 326 357   gcobo@uoc.edu      Eugnia Santamara  IMT Department, Universitat     Oberta de Catalunya (UOC)    Barcelona, Spain  +34 93 326 3743   esantamaria@uoc.edu   David Garca-Solrzano  IMT Department, Universitat Oberta   de Catalunya (UOC)   Barcelona, Spain  +34 93 326 3686   dgarciaso@uoc.edu      Carlos Monzo  IMT Department, Universitat  Oberta   de Catalunya (UOC)   Barcelona, Spain  +34 93 326 3895   cmonzo@uoc.edu   Jose Antonio Morn  IMT Department, Universitat Oberta   de Catalunya (UOC)   Barcelona, Spain  +34 93 326 3618   jmoranm@uoc.edu      Javier Melenchn  IMT Department, Universitat Oberta   de Catalunya (UOC)   Barcelona, Spain  +34 93 326 3508   jmelenchonm@uoc.edu      ABSTRACT  Online discussion forums are a key element in virtual learning   environments. The way learners participate in discussion boards   can be a very useful source of indicators for teachers to facilitate   their tasks. The use of a two-stage analysis strategy based on an   agglomerative hierarchical clustering algorithm is proposed in   this paper to identify different participation profiles adopted by   learners in online discussion forums. Different parameters are   used to characterize learners activity (amount of posts, rhythm,   depth of threads, crossed replies, etc). Participation profiles are   identified and analyzed in terms of behavior and performance.   Categories and Subject Descriptors  J.1 [Administrative Data Processing] Education; K.3.1   [Computer Uses in Education] Distance learning;   General Terms  Algorithms, Measurement, Performance and Experimentation.   Keywords  Learning analytics, Educational data mining, Learner behavior   modeling, Hierarchical clustering, Online discussion forums.   1. INTRODUCTION  Online discussion forums (or boards) are one of the most   common tools in web-based teaching-learning environments.   Online learner participation has been defined as a complex and   intrinsic part of online learning [6]. In fact, a high level of   interaction is desirable and increases the effectiveness of   distance education courses [5]. Thus, discussion boards can be a   relevant source of information in order to provide teachers with   useful indicators of learners activity and to facilitate their   monitoring, guidance and feedback tasks.   The purpose of the present work is to present a two-stage   analysis strategy in order to model and identify learners   participation profiles in online discussion forums. Since   clustering learners has proved to be a proper way to find similar   learning behaviors [11], learners with similar activity patterns   are clustered together in the first stage and resultant clusters are   combined in the second stage to identify participation profiles.   This paper is structured as follows. The working framework is   introduced in Section 2; the clustering algorithm used in the   experiments is proposed in Section 3; the data set is described in   Section 4; the modeling strategy to identify participation profiles   and the obtained results are shown in Section 5; and, finally,   conclusions and future work are presented in Section 6.   2. WORKING FRAMEWORK  Relevant contributions can be found in literature on modeling   learner behavior in online asynchronous environments. [1] deals   with identification of lurkers (in a discussion board, a lurker is   the one who reads but never writes). This kind of behavior   makes impossible a visible and active interaction both with other   learners and teacher in virtual environments. In order to   investigate lurking, [8] carried out a study on lurking using in-  depth semi-structured interviews with members of online groups.   The analysis reveals that lurking is a strategic activity involving   more than just reading posts and a model to explain lurker   behavior is proposed. Finally, three significant participation   patterns in accessing and contributing to an online discussion   board are defined in [10]: workers (proactive participants that are   continuously involved in discussions), lurkers (peripheral   participants that regularly access to the board and participate in   the discussions in read-only mode) and shirkers (parsimonious   participants that barely access to the board).     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00   248    A different approach is provided by social network analysis   techniques, in order to show interactions between learners in   online discussion threads (learner network) and evaluate their   participation [9]. Moreover, a great diversity of indicators (depth   of threads, rhythm, reciprocal readings, cross replies, etc.) is   used from this approach in order to define effective interaction   models capable of giving an immediate picture of the   effectiveness level of a collaborative group [2].   Finally, interesting contributions on participation profiles in   discussion boards of general topics not strictly educational can   be found as well. Several online forums of different topics are   classified in [3] regarding their predominant user roles rather   than their topics. Eight different user roles (popular initiators,   popular participants, joining conversationalists, supporters,   taciturns, grunts, elitists and ignored) are identified through an   analysis method based on PCA (the most dominant feature in the   largest component is selected in order to define three bands of   users and discard the lowest and middle ones marginal   participation profiles) and agglomerative hierarchical clustering   (the optimal number of clusters is selected after an inspection of   the solutions provided by different validation techniques).   3. CLUSTERING ALGORITHM  The modeling strategy in the present work is based on identifying   participation profiles from the different activity patterns   conducted by learners in online discussion forums. In order to   group learners with similar activity patterns together, a clustering   algorithm is used [11]. Due to the number of relevant patterns   (i.e., the number of relevant clusters) is a priori unknown, we use   an agglomerative hierarchical clustering algorithm [3].   The outcome of an agglomerative hierarchical clustering   algorithm is not a data partition, but a dendrogram-type graph   [7]. A dendrogram is a hierarchical tree structure formed by links   that join couples of clusters together in a new cluster (i.e., each   link defines a possible cluster of data) from the beginning   (singleton clusters i.e., one cluster per learner) to the end (a   unique cluster including the whole set of data i.e., all learners   grouped together) of the tree. The heights of the links   correspond to the distance between the couple of clusters joined   as a new cluster under the link. The similarity measure between   clusters depends on the linkage function defined in the algorithm:   the Single Link (nearest neighbor) and Complete Link (furthest   neighbor) are the most popular linkage functions [7].   A dendrogram is a useful tool for both visually exploring   similarities between data (data exploratory analysis) and   obtaining data partitions (clusters of data). Classical strategies to   get a data partition consist in cutting the dendrogram at any   defined threshold height and dismiss the links above the cut [7].   More interesting is to evaluate links in terms of their   inconsistency instead of their height and define a threshold   inconsistency in order to dismiss the most inconsistent links [12].   Finally, more versatile strategies try to isolate clusters separately   as the dendrogram grows, for the sake of flexibility and to be   able to detect both sparse and dense clusters [4].   The agglomerative hierarchical clustering algorithm used in this   paper combines the strategy of isolating clusters separately   (instead of getting a final data partition in one go by a single cut   in the dendrogram) with a modified version of the inconsistency   criterion defined in [12]. Our algorithm builds the whole   dendrogram and isolates its best cluster in terms of a consistency   criterion (the best cluster is the one defined by the most   consistent link). Once the best cluster is isolated, this process is   iterated until there is no remaining data to be isolated.   Thus, taking the gap concept (height increment between   consecutive links) proposed in [4], we define:   ( )[ ] ( )iiii nkuzc = max    as the consistency of the i-th link in the dendrogram, being:   i  ii i  gap z     =    ( ) ( ) ] [1,0,1ln1001     ==       TOT  n i  N enk i    where gapi is the gap above i-th link, i and i are the mean and   the standard deviation of the population formed by gapi and the   gaps above all the links nested under the i-th link (zi is the   standard score of gapi), ui is the set of standard scores of the gaps   above all the links nested under the i-th link, NTOT is the total   amount of elements (i.e., learners) in the data set, ni is the   amount of elements within the cluster defined by the i-th link   and k(ni) is an exponential correction applied to avoid isolating   too small size clusters (k(ni) is less than  when ni is less than   the  % of NTOT).   4. DATA SET  The experiments conducted in this paper analyze the activity   carried out by learners within the online discussion forums of   three different subjects in a virtual Telecommunications Degree   (Electronic Circuits, Linear Systems Theory and Mathematics)   and throughout three complete semesters (from February 2009 to   July 2010). All the courses took place in an asynchronous web-  based teaching-learning environment and the participation of   learners in discussion boards was not mandatory, but strongly   recommended. Thus, the whole dataset involves a total amount of   672 learners (NTOT) distributed in eighteen different virtual   classrooms and a total amount of 3842 posts. Total withdrawal   and passing rates are 36.31% and 52.23%, respectively.   5. MODELING ACTIVITY AND FINDING  PARTICIPATION PROFILES  The analysis strategy conducted in the present work consists of   two main stages. In the first stage, learners activity in online   discussion forums is characterized in two different domains   (writing and reading) and learners with similar activity patterns   are grouped together in each domain separately.   The activity carried out by learners is differently characterized in   each domain (different parameters are used depending on the   domain). In writing domain, each learner is characterized   according the following four parameters (all of them are ratios   over learners specific virtual classroom and semester): ratio of   threads weighted by their respective depths initiated by learner   over total amount of threads weighted, as well (depth), ratio of   reply posts written by learner over total amount of reply posts   (reposts), ratio of learners replied at least, once by learner over   total  amount of  learners (recross ) and ratio of  days when learner   249          Table 1. This table shows how the participation profiles are identified. Combining the resultant clusters from the first stage of   analysis, the final set of clusters is obtained (i.e., learners belonging to WRi and RDj clusters belong now to the new WRiRDj   cluster). In the table, final clusters are represented in rows, first column show the final clusters labels, N column (% over NTOT)   indicates the amount of learners per cluster, Withdrawal and Passing columns (% over N) indicate withdrawal and passing rates   at the end of semester, Average Representatives (Centroids) columns show each final clusters average representative (in terms   of the eight different parameters used to characterize learners activity), and the last two columns describe the participation   profiles represented by each final cluster (by identifying the different participation profiles proposed in [10]).   Final   Clusters  N Withdrawal Passing   Average Representatives (Centroids)  Participation Profiles   depth reposts recross wrrhythm rdposts rdthreads rdcross rdrhythm   WR3RD5 4.17 0 96.43 0.18 0.21 0.27 0.82 0.99 1 1 0.94   Workers   High-level   Workers WR3RD4 2.68 5.56 83.33 0.25 0.19 0.18 0.79 0.9 0.93 0.9 0.68   WR2RD5 7.29 4.08 89.8 0.03 0.04 0.07 0.22 0.99 1 1 0.95 Mid-level   Workers WR2RD4 29.76 19.5 65 0.03 0.03 0.05 0.19 0.8 0.84 0.9 0.56   WR2RD3 12.95 44.83 41.38 0.01 0.01 0.02 0.12 0.36 0.45 0.66 0.25 Low-level   Workers WR2RD2 5.51 45.95 37.84 0.01 0 0.01 0.07 0.1 0.17 0.31 0.09   WR1RD5 0.89 16.67 83.33 0 0 0 0 0.98 1 0.96 0.94   Lurkers   High-level   Lurkers WR1RD4 9.52 26.56 59.38 0 0 0 0 0.82 0.86 0.89 0.49   WR1RD3 9.23 64.52 27.42 0 0 0 0 0.31 0.36 0.66 0.16 Low-level   Lurkers WR1RD2 11.16 60 32 0 0 0 0 0.07 0.12 0.26 0.06   WR1RD1 6.7 93.33 2.22 0 0 0 0 0 0 0 0  Shirkers   WR2RD1 0.15 100 0 0 0 0 0.05 0 0 0 0   WR3RD3 0              WR3RD2 0             WR3RD1 0                Figure 1. This figure shows the resulting dendrograms from clustering learners in terms of (a) writing and (b) reading.   Each dendrogram legend indicates the assigned label to each cluster, which can be identified by its color. According to   the criterion defined by the algorithm, resultant clusters are nested under the most consistent links in the dendrogram.   Each cluster top height corresponds to the distance between the furthest learners within the cluster (Complete Link).   250    writes at least one post over total amount of days (wrrhythm).   Other four different parameters are used in the reading domain   (self-readings are excluded): ratio of posts read by learner over   total amount of posts (rdposts), ratio of threads where learner   reads at least one post over total amount of threads (rdthreads),   ratio of learners read at least, once by learner over total   amount of learners (rdcross) and ratio of days when learner read at   least one post over total amount of days (rdrhythm).   Learners are separately clustered in both domains by using the   agglomerative hierarchical clustering algorithm described in   Section 3 with the following configuration: Normalized   Euclidean Distance, Complete Link, 10=  and 9.0= .   Results obtained in both domains are shown in Figure 1.   Finally, the second stage of the analysis strategy consists in   grouping together those learners belonging to the same clusters   in both writing and reading domains. Thus, the final set of   clusters that completely defines the different activity patterns and   allows to identify the participation profiles of learners in online   discussion forums is obtained (see Table 1). Participation   profiles are mapped to final clusters by observing and comparing   the values of the parameters that characterize the learners   activity patterns in each cluster. Final clusters centroids allow to   confirm the suitability of this mapping and to describe and   characterize the participation profiles in more detail.   Some interesting remarks can be made from the obtained results.   Regarding the agglomerative hierarchical clustering algorithm   performance, it allows to find clusters of different size and   density in both different domains (e.g., in Figure 1 (a), WR2   cluster is larger and denser than the smaller and sparser WR3).   Participation profiles like the ones describe in [10] can be easily   identified by observing final clusters centroids (see Table 1):   shirkers (inactive learners) are grouped within WR1-RD1 and   WR2-RD1 clusters (centroids with no kind, or negligible, activity   at all); lurkers (only readers), within WR1-RD2//-RD5 clusters   (centroids with no kind of reading activity and different patterns   of writing activity); and workers (active learners), within WR2-  RD2//-RD5 and WR3-RD4/-RD5 clusters (different patterns of   both writing and reading activity). Furthermore, specific sub-  profiles for lurkers (low- and mid-level lurkers) and workers   (low-, mid- and high-level workers) have been defined depending   on differences between centroids values of reading and writing   parameters, respectively.   Empty possible combinations (WR3-RD1//-RD3) are also   useful to deduce some pretty logical conclusions: writing   involves reading, but not the other way around (reading does not   necessarily involve writing lurking behavior). Besides,   differences between centroids values can be useful to identify   other kinds of participation profiles as well (e.g., the different   user roles proposed in [3]: popular initiators, popular   participants, joining conversationalists, supporters, taciturns,   elitists, grunts and ignored).   Finally, some interesting conclusions regarding on performance   differences between profiles can be pointed out: the withdrawal   rates of shirkers and low-level lurkers are the top highest and the   passing rates of high-level lurkers are comparable with the ones   of high- and mid-level workers (which are logically the top   highest).   6. CONCLUSIONS AND FUTURE WORK  In this paper, a two-stage strategy in order to model learner   participation profiles in online discussion forums is proposed.   The presented agglomerative hierarchical clustering algorithm   successfully isolates the more relevant activity patterns in   different domains (writing and reading). The obtained final   clusters actually group learners with similar activity patterns and   allow to satisfactorily identify different participation profiles in   online discussion forums. In terms of future work, the number of   domains in first analysis stage will be increased (rhythm domain,   neighboring domain, etc.) and the impact of this increasing on   both the presented clustering algorithm suitability and the   identification of participation profiles accuracy will be checked.   7. REFERENCES  [1] Beaudoin, M.F. 2002. Learning or lurking Tracking the   invisible online student. The Internet and Higher   Education. 5, 2 (Jul. 2002), 147-155.   [2] Calvani, A., Fini, A., Molino, M. and Ranieri, M. 2009.  Visualizing and monitoring effective interactions in online   collaborative groups. British Journal of Educational   Technology. 41, 2 (Mar. 2010), 213-226.   [3] Chan, J, Hayes, C. and Daly, E. 2010. Decomposing  discussion forums and boards using user roles. In   Proceedings of the WebSci10: Extending the Frontiers of   Society On-Line (Raleigh, NC, USA, April 23 - 24, 2010).   [4] Fred, A.L.N. and Leito, J.M.N. 2003. A new cluster   isolation criterion based on dissimilarity increments. IEEE   Transactions of Pattern Analysis and Machine Intelligence.   28, 8 (Aug. 2010), 944-958.   [5] Fulford, C.P. and Zhang, S. 1993. Perceptions of   interaction: The critical predictor in distance education. The   American Journal of Distance Education. 7, 3 (1993), 8-21.   [6] Hrastinski, S. 2008. What is online learner participation A   literature review. Computers & Education. 51, 4 (Dec.   2008), 1755-1765.    [7] Jain, A. and Dubes, R. 1988. Algorithms for Clustering   Data. Prentice Hall. 1988.   [8] Nonnecke, B. and Preece, J. 2001. Why lurkers lurk. In   Proceedings of Americas Conf. on Information Systems   (Boston, MAS, USA, August 03 - 05, 2001).   [9] Rabbany, R., Takaffoli, M. and Zaane, O.R. 2011.  Analyzing participation of students in online courses using   social network analysis techniques. In Proceedings of 4th   International Conference on Educational Data Mining   (Eindhoven, The Netherlands, July 6  8, 2011).   [10] Taylor, J.C. 2002. Teaching and learning online: the   workers, the lurkers and the shirkers. Journal of Chinese   Distance Education. 9 (2002), 31-37.   [11] Vellido, A., Castro, F. and Nebot, A. 2010. Clustering   educational data. In Handbook of educational data mining,   CRC Press, 2010, 75-92.   [12] Zahn, C.T. 1971. Graph-theoretical methods for detecting   and describing gestalt clusters. IEEE Transactions on   Computers, 20, 1 (Jan. 1971), 68-86.   251      "}
{"index":{"_id":"44"}}
{"datatype":"inproceedings","key":"Siemens:2012:LAE:2330601.2330661","author":"Siemens, George and Baker, Ryan S. J. d.","title":"Learning Analytics and Educational Data Mining: Towards Communication and Collaboration","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"252--254","numpages":"3","url":"http://doi.acm.org/10.1145/2330601.2330661","doi":"10.1145/2330601.2330661","acmid":"2330661","publisher":"ACM","address":"New York, NY, USA","keywords":"collaboration, educational data mining, learning analytics and knowledge","Abstract":"Growing interest in data and analytics in education, teaching, and learning raises the priority for increased, high-quality research into the models, methods, technologies, and impact of analytics. Two research communities -- Educational Data Mining (EDM) and Learning Analytics and Knowledge (LAK) have developed separately to address this need. This paper argues for increased and formal communication and collaboration between these communities in order to share research, methods, and tools for data mining and analysis in the service of developing both LAK and EDM fields.","pdf":"Learning Analytics and Educational Data Mining: Towards  Communication and Collaboration    George Siemens  Technology Enhanced Knowledge Research Institute   Athabasca University   gsiemens@athabascau.ca   Ryan S J.d. Baker  Department of Social Science and Policy Studies   Worcester Polytechnic Institute    rsbaker@wpi.edu       ABSTRACT  Growing interest in data and analytics in education, teaching, and  learning raises the priority for increased, high-quality research  into the models, methods, technologies, and impact of analytics.  Two research communities  Educational Data Mining (EDM)  and Learning Analytics and Knowledge (LAK) have developed  separately to address this need. This paper argues for increased  and formal communication and collaboration between these  communities in order to share research, methods, and tools for  data mining and analysis in the service of developing both LAK  and EDM fields.   Categories and Subject Descriptors  H.2.8 [Database Applications]: Data Mining   General Terms  Algorithms, Human Factors, Measurements.   Keywords  Educational data mining, learning analytics and knowledge,  collaboration   1. INTRODUCTION  In education, the emergence of big data through new extensive  educational media, combined with advances in computation [1]  holds promise for improving learning processes in formal  education, and beyond as well. Increasingly, very large data sets  are available from students interactions with educational software  and online learning - among other sources - with public data  repositories supporting researchers in obtaining this data [2].   Two distinct research communities, Educational Data Mining  (EDM) and Learning Analytics and Knowledge (LAK), have  developed in response.    The first workshop on Educational Data Mining was held in 2005,  in Pittsburgh, Pennsylvania. This was followed by annual  workshops and, in 2008, the 1st International Conference on  Educational Data Mining, held in Montreal, Quebec. Annual  conferences on EDM were joined by the Journal of Educational  Data Mining, which published its first issue in 2009, with Kalina  Yacef as Editor. The first Handbook of Educational Data Mining  was published in 2010 [7]. In the summer of 2011, the  International Educational Data Mining Society (IEDMS)  (http://www.educationaldatamining.org/) was formed to promote   scientific research in the interdisciplinary field of educational data  mining, organizing the conferences and journal, and the free  open-access publication of conference and journal articles. The  EDM community brings together an inter-disciplinary community  of computer scientists, learning scientists, psychometricians, and  researchers from other traditions. A first review of research in  EDM was presented by Romero & Ventura [3], followed by a  theoretical model proposed by Baker & Yacef [4]. A very  comprehensive review of EDM research can be found in [6].    The Learning Analytics and Knowledge conference series was  initiated in early summer, 2010, with the development of global  steering and program committees (https://tekri.athabascau.ca/  analytics/node/5). The conference explicitly emphasized its role as  bridging the computer science and sociology/psychology of  learning in declaring that the technical, pedagogical, and social  domains must be brought into dialogue with each other to ensure  that interventions and organizational systems serve the needs of  all stakeholders. The first conference, held in Banff, Canada  attracted over 100 participants, with proceedings published in  ACM [5], validating interest in inter-disciplinary approaches to  analytics in learning. In summer of 2011, the Society for Learning  Analytics (SoLAR -- http://www.solaresearch.org/) was formed to  provide oversight for the conference, develop and advance a  research agenda in learning analytics, as well as advocate for, and  educate in the use of, analytics in learning.   With growing research interest in learning analytics and  educational data mining, as well as the rapid development of  software and analytics methods, it is important for researchers and  educators to recognize the unique attributes of each community.  While LAK and EDM share many attributes and have similar  goals and interests, they have distinct technological, ideological,  and methodological orientations. As schools, university, and  corporate learning and curriculum organizations begin to adopt  data mining and analytics, both LAK and EDM can benefit from  building off work occurring in the other community. This paper  details the overlap between these different communities and  discusses the benefits of increased communication and  collaboration.      2. SIMILARITIES BETWEEN  COMMUNITIES  The EDM and LAK communities are defined in relatively similar  ways. The International Educational Data Mining Society defines  EDM as follows: Educational Data Mining is an emerging  discipline, concerned with developing methods for exploring the  unique types of data that come from educational settings, and  using those methods to better understand students, and the settings  which they learn in.    The Society for Learning Analytics Research defines Learning  Analytics as:  the measurement, collection, analysis and      Permission to make digital or hard copies of all or part of this work for   personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy   otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK'12: 29 April  2 May 2012, Vancouver, BC, Canada.   Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.   252    reporting of data about learners and their contexts, for purposes of  understanding and optimizing learning and the environments in  which it occurs.    EDM and LAK both reflect the emergence of data-intensive  approaches to education. In sectors such as government, health  care, and industry, data mining and analytics have become  increasingly prominent for gaining insight into organizational  activities. Drawing value from data in order to guide planning,  interventions, and decision-making is an important and  fundamental shift in how education systems function. LAK and  EDM share the goals of improving education by improving  assessment, how problems in education are understood, and how  interventions are planned and selected. Extensive use by  administrators, educators, and learners of the data produced  during the educational process raises the need for research-based  models and strategies. Both communities have the goal of  improving the quality of analysis of large-scale educational data,  to support both basic research and practice in education.      3. KEY DISTINCTIONS BETWEEN  COMMUNITIES  The similarities between EDM and LAK suggest numerous areas  of research overlap. Additionally, the organizational deployment  of EDM and LAK requires similar data and researcher skill-sets.  However, these two communities have different roots and some  distinctions are important to note. Table 1 shows some of the key  differences between the communities. It is important to note that  these distinctions are meant to represent broad trends in the two  communities; many EDM researchers conduct research that could  be placed on the LAK side of each of these distinctions, and many  LAK researchers conduct research that could be placed on the  EDM side of these distinctions. By identifying these distinctions,  we hope to identify places where the two communities can learn  from each other, rather than defining the communities in an  exclusive fashion. Certainly, communities that grow organically  as these two communities have done will not have rigid edges  between what work appears in the two communities.    One key distinction is found in the type of discovery that is  prioritized. In both communities, research can be found that uses  automated discovery and research can be found that leverages  human judgment through visualization and other methods.  However, EDM has a considerably greater focus on automated  discovery, and LAK has a considerably greater focus on  leveraging human judgment. Even in research which combines  these two directions, this preference can be seen; EDM research  which leverages human judgment in many cases does so to  provide labels for classification, while LAK research which uses  automated discovery often does so in the service of informing  humans who make final decisions.    This difference is associated with another difference between the  two communities: the type of adaptation and personalization  typically supported by the two communities. In line with the  greater focus on automated discovery in EDM, EDM models are  more often used as the basis of automated adaptation, conducted  by a computer system such as an intelligent tutoring system. By  contrast, LAK models are more often designed to inform and  empower instructors and learners.    A third difference, and an important one, is the distinction  between holistic and reductionistic frameworks. It is much more  typical in EDM research to see research which reduces  phenomena to components and analyzing individual components   and relationships between them. The discovery with models  paradigm for EDM research discussed in [4] is a clear example of  this paradigm. By contrast, LAK researchers typically place a  stronger emphasis on attempting to understand systems as wholes,  in their full complexity. The debate between reductionist and  holistic paradigms has often paralyzed discussion between  education researchers from different camps; encouraging  discussion between EDM and LAK researchers is a key way to  prevent this common split from reducing what EDM and LAK  researchers can learn from one another.    Two other differences are in the most common origins and  methods of researchers in these two communities. Researchers  origins tend to drive the preferred approaches discussed above,  and these preferred approaches in turn drive preferred methods.  Greater detail on these issues is given in Table 1.      Table 1: A brief comparison of the two fields   LAK EDM  Discovery Leveraging human   judgement is key ;   automated discovery   is a tool to accomplish   this goal  Automated discovery    is key; leveraging   human judgment is a   tool to accomplish   this goal  Reduction &   Holism  Stronger emphasis on   understanding   systems as wholes, in   their full complexity  Stronger emphasis on   reducing to   components and   analyzing individual   components and   relationships between   them  Origins LAK has stronger   origins in semantic   web,  intelligent   curriculum,  outcome   p rediction, and   systemic   interventions  EDM  has strong   origins in educational   software and student   modeling, with a   siginficiant   community in   p redicting course   outcomes  Adapation &   Personalization  Greater focus on   informing and   empowering   instructors and   learners  Greater focus on   automated adap tion   (e.g. by  the computer   with no human in the   loop)  Techniques &   M ethods  Social network   analysis, sentiment   analysis, influence   analytics, discourse   analysis, learner   success prediction,   concept analysis,   sensemaking models  Classification,   clustering, Bayesian   modeling, relationship   mining, discovery   with models,   visualization           253    4. CALL FOR COMMUNICATION AND  COLLABORATION: EDM and LAK  There is a positive value to having different communities engaged  in how to exploit big data to improve education. In particular,  different standards and values for good research and important  research exist in each community, allowing creativity and  advancement that might not otherwise occur in a single,  monolithic research culture. For example, EDM researchers have  placed greater focus on issues of model generalizability (e.g.  multi-level cross-validation, replication across data sets). By  contrast, LAK researchers have placed greater focus on  addressing needs of multiple stakeholders with information drawn  from data. Each of these issues are important for the long-term  success of both fields, a key opportunity for the two communities  to learn from one another.    Friendly competition between the two communities will keep both  communities vigorous, and is generally beneficial for science.  This type of competition has occurred in the past, such as in the  split between the International Conference on the Learning  Sciences and the International Conference on Artificial  Intelligence in Education in 1992. Research networks are  increasingly global, as reflected by the multi-national executive  committees of IEDMS/EDM and SoLAR/LAK, but reflect  different nations to a significant degree. Hence, the existence of  both communities broadens the number of researchers working  and collaborating in the broader area of data-driven discovery in  education. At the same time, it is very important to keep  competition healthy. Healthy competition requires that both  communities disseminate their research to each other through their  respective conferences and journals to ensure awareness of  important ideas and advances occurring in the other community.  The two communities must communicate, in order to bring the  greatest possible benefits to educational practice and the science  of learning.       5. CONCLUSION  Given the overlaps in research interests, goals, and approaches  between the EDM and LAK communities, the authors of this  paper recommend that the executive committees of SoLAR and  IEDMS formalize approaches for dissemination of research and  enacting cross-community ties. A formal relationship will allow  each community to continue developing their specialized and  distinct research methods and tools, while simultaneously  increasing opportunities for collaborative research and sharing of  research findings between the communities.    This alliance would also strengthen our opportunities to influence  non-academic research and practice. A particular concern now  facing both EDM and LAK is the rapid development of analytics  and data mining tools by commercial organizations that do not  build off of either communitys expertise, algorithms, and  research results. To give one example, there is increasing  consensus in the EDM community that cross-validation needs to  be conducted at multiple levels (in particular the student level, but  also the classroom and lesson/unit levels). However, there is not   direct support for this goal in many of the data mining/analytics  tools now emerging. To the extent that EDM and LAK can jointly  articulate quality standards for research in this area, it may be  possible to more effectively communicate these standards to the  wider community of tool-developers and analytics practitioners,  as well as the broader research community. As such, both  communities would be facilitated in communicating their vision  for data-driven science and practice in the field of education.       Both the LAK and EDM communities anticipate that the impact  of data and analytics within education will be transformative at  primary, secondary, and post-secondary levels. An open,  transparent research environment is vital to driving forward this  important work. As connected, but distinct, research disciplines,  EDM and LAK can provide a strong voice and force for  excellence in research in this area, guiding policy makers,  administrators, educators, and curriculum developers, towards the  deployment of best practices in the upcoming era of data-driven  education.      6. ACKNOWLEDGMENTS  Our thanks to Jaclyn Ocumpaugh, and the anonymous reviewers  for their valuable input and assistance on this paper.   7. REFERENCES  [1].  Mayer, M. (2009) Innovation at Google: The physics of data   [PARC forum] (11 August, 2009: 3:59 mark). Available from  < http://www.slideshare.net/PARCInc/innovation-at-google- the-physics-of-data>   [2]. Koedinger, K.R., Baker, R.S.J.d., Cunningham, K.,  Skogsholm, A., Leber, B., Stamper, J. (2010) A Data  Repository for the EDM community: The PSLC DataShop.  In Romero, C., Ventura, S., Pechenizkiy, M., Baker, R.S.J.d.  (Eds.)Handbook of Educational Data Mining. Boca Raton,  FL: CRC Press, pp.43-56.   [3]. Romero, C., Ventura, S. (2007). Educational Data Mining: A  Survey from 1995 to 2005. Expert Systems with Applications,  33, 125-146.   [4]. Baker, R.S.J.d., Yacef, K. (2009) The State of Educational  Data Mining in 2009: A Review and Future Visions. Journal  of Educational Data Mining, 1 (1), 3-17.   [5]. Gasevic, D., Conole, G., Siemens, G., Long, P. (Eds). (2011)  LAK11: International Conference on Learning Analytics and  Knowledge, Banff, Canada, 27 February - 1 March 2011.   [6]. Romero, C., Ventura, S. (2010) Educational Data Mining: A  Review of the State-of-the-Art. IEEE Transaction on  Systems, Man, and Cybernetics, Part C: Applications and  Reviews. 40 (6), 601-618.   [7]. Romero, C., Ventura, S., Pechenizky, M., Baker, R. (2010)  Handbook of Educational Data Mining. 2010. Editorial  Chapman and Hall/CRC Press, Taylor & Francis Group.  Data Mining and Knowledge Discovery Series.            254      "}
{"index":{"_id":"45"}}
{"datatype":"inproceedings","key":"Roijers:2012:PEC:2330601.2330663","author":"Roijers, Diederik M. and Jeuring, Johan and Feelders, Ad","title":"Probability Estimation and a Competence Model for Rule Based e-Tutoring Systems","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"255--258","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330663","doi":"10.1145/2330601.2330663","acmid":"2330663","publisher":"ACM","address":"New York, NY, USA","keywords":"data mining, learning analytics, student model","Abstract":"In this paper, we present a student model for rule based e-tutoring systems. This model describes both properties of rewrite rules (difficulty and discriminativity) and of students (start competence and learning speed). The model is an extension of the two-parameter logistic ogive function of Item Response Theory. We show that the model can be applied even to relatively small datasets. We gather data from students working on problems in the logic domain, and show that the model estimates of rule difficulty correspond well to expert opinions. We also show that the estimated start competence corresponds well to our expectations based on the previous experience of the students in the logic domain. We point out that this model can be used to inform students about their competence and learning, and teachers about the students and the difficulty and discriminativity of the rules.","pdf":"Probability estimation and a competence model for rule based e-tutoring systems  Diederik M. Roijers, Johan Jeuring, Ad Feelders Department of Information and Computing Sciences  Utrecht University, Utrecht, The Netherlands  ABSTRACT In this paper, we present a student model for rule based e-tutoring systems. This model describes both properties of rewrite rules (difficulty and discriminativity) and of stu- dents (start competence and learning speed). The model is an extension of the two-parameter logistic ogive function of Item Response Theory. We show that the model can be ap- plied even to relatively small datasets. We gather data from students working on problems in the logic domain, and show that the model estimates of rule difficulty correspond well to expert opinions. We also show that the estimated start com- petence corresponds well to our expectations based on the previous experience of the students in the logic domain. We point out that this model can be used to inform students about their competence and learning, and teachers about the students and the difficulty and discriminativity of the rules.  Categories and Subject Descriptors J.1 [Administrative Data Processing] Education; K.3.1 [Com- puter Uses in Education] Collaborative learning, Computer- assisted instruction (CAI), Computer-managed instruction (CMI), Distance learning  Keywords Learning Analytics, Student Model, Data Mining  1. INTRODUCTION Students of natural sciences learn to solve various types of standard problems. Many of these problems are solved by rewriting some kind of formula or expression, step by step, until the problem is solved. Each problem domain (e.g. al- gebra, matrix calculus, or logic) has its own set of rewrite rules. Students start practicing these types of problems early in their school careers: already at primary school they learn how to calculate with fractions.  Learning to solve problems using rewriting is often a time- consuming and labor-intensive process, both for students  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 12, 29 April  2 May 2012, Vancouver, BC, Canada.  and teachers. Students have to practice a lot, and often require a lot of feedback, which a teacher may not always be able to give immediately. E-tutoring systems can be used to alleviate this situation by providing feedback automatically. In recent years the Open University the Netherlands has de- veloped an e-tutoring system based on rules and strategies: the ideas framework1. Heeren et al. [] show how to use rewrite rules and strategies in e-tutoring systems to provide feedback. Feedback can be provided for every (correct or incorrect) application of a rewrite rule, but not about how difficult and discriminative rules are, how competent stu- dents are when they start working, and how fast they learn.  In this paper we answer the question: How can we describe student behavior in a rule based e-tutoring system with a student model, and estimate the probability of a student ap- plying a rule correctly, the next time he/she tries to apply it The model describes the behavior of the students (whether or not a student applies the rewrite rules correctly) in rela- tion to the rules. Therefore, it includes the properties of a rewrite rule, as well as properties of a student.  Previous research on student models in the context of e- tutoring systems suggests that Item Response Theory (IRT) is a good starting point. Johns et al. [] apply IRT to an e-tutoring system that offers multiple choice questions for mathematics tests. However, multiple choice questions are single static questions, while we want to model recurring rewrite rules. We therefore have to adjust the model to incorporate rewrite rules, and to allow a student to apply the same rule multiple times.  Cen et al. []present a student model based on logistic re- gression, as we will do, but with more parameters than we use. They use a simplifying assumption that students learn at the same rate, which is of course invalid in general.  This paper is organised as follows. Section  discusses the method we use in our research. Section  presents our student model, and shows the results of testing this model by simulation and using real life data. Section  concludes and discusses what needs to be done to further evaluate this model, and how the model can be used in practice.  2. METHODS This research consists of two phases: a construction phase, in which we create a student model and test it on simulated  1http://ideas.cs.uu.nl/  255  root232 Text Box Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00    data, and a validation phase in which we use real-life data from students working on an e-tutoring system to learn the model parameters and compare these learned parameters to expert opinions.  To create a student model for rule based e-tutoring systems we use the IRT framework, and more specifically the two pa- rameter logistic ogive function (2PL), as a basis. One reason why we select 2PL, is because it is a well-established model in psychology and education (e.g. used to validate tests). An even more significant reason is that it operationalizes the de- sired properties both for rules (items in IRT) and student: difficulty and discriminativity, and competence. It relates these properties to the probability of a correct application of a rule (a positive response in IRT) [].  We have investigated four alternative models:  1. The start competence per student per rule may be dif- ferent, as well as the learning speed.  2. The start competence per rule may be different, but the learning speed is always the same for one student.  3. The start competence is the same for all rules for a student, but the learning speed may vary.  4. The start competence and the learning speed do not depend on the rule, but are properties of a student only.  Our domain experts discard option 3, because the learning speed is more likely to be a constant parameter over all rules for a single student than the start competence. We test the other options using simulated data. We run several simula- tions to discover how the model and the learning algorithms we use (see next section) behave in terms of: approximating the true parameter value, variance in the estimated param- eters, sensitivity to the amount of data (number of rules, number of students, number of instances per rule per stu- dent), and the accuracy with which the outcomes (applying a rule either correctly or incorrectly) are predicted.  For validating the model we collect data from students us- ing a rule- and strategy-based e-tutoring system to learn to rewrite boolean expressions into disjunctive normal form (DNF). This subject is often taught in Computer Science programmes, and we have a number of domain experts avail- able at the two universities where we collect data. We collect data at Utrecht University (UU) and Utrecht University of Applied Sciences (HU). We ask the domain experts to rank the rewrite rules necessary for the task twice: once for diffi- culty and once for discriminativity. We compare these rank- ings to the rankings based on the estimated values for the parameters from the student data returned by our learning algorithm. The rewrite rules for this domain can be found in Table .  We also compare what we know about the previous knowl- edge of the students to the output values of the starting competence parameter of the student model. There are three groups of student participants: 4 UU students with recent training in solving DNF problems, which we presume  to have most previous knowledge; 5 HU second-year tech- nical computer science students, who have had less training in manipulating logic expressions, but do have one and a half years of programming experience; and 5 HU first-year business informatics students, who have had little training in manipulating logical expressions and little programming experience, who we assume to have least previous knowl- edge.  3. RESULTS The results section is divided into two subsections: con- struction and validation. In the construction subsection we describe how the model is constructed, and how it performs on simulated data. In the validation subsection we show the results of applying the model to real data.2  3.1 Construction As a starting point for model construction we choose the 2PL IRT model. 2PL relates the probability of a correct answer for a number of items (e.g. problems on a test) for a number of students, to the latent variables of student competence and item difficulty and discriminativity, through the following probability function:  P (oi,j = 1|i, bj , aj) = eaj(ibj)  1 + eaj(ibj) (1)  where the binary outcome oi,j is the outcome for student i attempting item j, aj and bj are the discriminativity and difficulty of item j, and i is the competence of student i. The data is a binary matrix, containing the outcomes for each student for each exercise. The parameter values can be learned through an iterative scheme, alternating between optimizing the likelihood by changing aj and bj , and opti- mizing the likelihood by changing i, using gradient descent []. The 2PL model assumes that the competence is con- stant for each student.  The data we gather from an e-tutoring system however, is different from a binary matrix. A student does several ex- ercises in which each rule can be attempted several times, with the purpose to increase his or her competence. We obtain data in the form of a sequence of tuples of student ID, rule ID and the outcome (0 or 1). We therefore know how many times a student has attempted a rule before when (s)he applies a rule.  We extend 2PL, by adding a learning parameter. The cur- rent competence for a rule for a student is now the start competence plus the learning parameter times the number of times the rule has been attempted before. As described in the methods section we have to choose whether the starting competence and learning speed are parameters of the stu- dent, or whether they can also vary per rule. After testing the different models, we conclude that the variance in the learned parameters is too high to be used for prediction or to be informative to teachers and students when they depended on both the student and the rule. We therefore let the start- ing competence and the learning parameters be attributes of  2More details can be found in the Masters thesis of the first author available via http://www.staff.science.uu.nl/ ~jeuri101/homepage/Publications/ThesisDMRoijers. pdf  256    a student only. The following probability function describes our model:  P (or,s,t = 1) = ear(0,s+str,sbr)  1 + ear(0,s+str,sbr) (2)  where r is the rule ID, s the student ID, 0,s is the starting competence of the student, s the learning speed of the stu- dent, and tr,s the number of times student s attempted rule r before.  We run several simulations using this model. For n stu- dents, the starting competence 0,s is chosen randomly from a uniform distribution between 3 and 1, and the learning parameter s is chosen randomly from a uniform distribu- tion between 0 and 0.5. For m rules the parameter values are also drawn from uniform distributions, for ar between 0.8 and 1.5 and for bj between 3 and 3. We choose a value tmax for the number of outcomes per student per rule. Using the randomly chosen rule and student parameter values we simulate data, by drawing outcomes stochastically, with the probability of a positive outcome given by equation (). Using the simulated data we learn the parameters back: we generate parameter values, with these generated parameters as the original parameters we generate outcomes, and (ig- noring the original parameters) we estimate the parameters using the data. This process we call parameter recovery. When recovering the parameters for small data sets we can- not use gradient descent. Gradient descent cannot be ap- plied when there is linear separability of the data, or when there are either no positive or no negative outcomes for a rule. The probability that one of these issues occurs is high when the dataset is small. We therefore replace gradient de- scent by attempting a discrete set of parameter values (with intervals of 0.01) and determine which combination of item and student parameters yields the highest likelihood.  When we use parameter recovery simulations we find that our model and learning algorithms are unbiased by calcu- lating the average difference between the original randomly chosen parameter values, and the recovered parameter val- ues (which should be close to 0). We can also show that the learning is reliable by calculating the mean absolute dif- ference between the original and the recovered parameter values. For a simulation with 50 students, 25 rules, and 20 instances per rule, the average difference between the recov- ered and the original parameter values is around 1% of the parameter value range. The mean absolute differences be- tween the original and recovered parameters are: 0.015 for s, 0.15 for 0,s, 0.07 for ar, and 0.09 for br. These errors are acceptable.  The simulation closest to the real data obtained is 15 stu- dents, 23 rules, and 8 instances per rule. Here the mean absolute differences between the original and recovered pa- rameters are: 0.09 for s, 0.38 for 0,s, 0.21 for ar, and 0.34 for br. For 0,s and br, these errors are still acceptable, but for s it is almost one fifth of the parameter range, and for ar it is two fifth of the range. We therefore conclude that the parameter value estimates of s and ar are unreliable for this data size.  The average accuracy of predicting the outcomes is deter- mined as follows. We have original and recovered param-  eter values. We use the original rule parameters and new randomly drawn student parameters to generate more data. The data comes in as a stream of outcomes. At each point in time a random rule is selected, a prediction is made whether this outcome will be positive or negative, and then an out- come is generated with the right probabilities (based on equation ()). After each outcome the estimated student parameters are adjusted. Using all data until time t we make a prediction (0 or 1) for the outcome at time t+ 1. The ac- curacy is the number of correct predictions. We start with adding an outcome to all rules before starting prediction. The accuracy for different values of the student parameters (0,s and s) is between 0.75 and 0.80.  3.2 Validation To validate our model against reality, we use data from stu- dents working on solving problems in the domain of boolean algebra. As mentioned before there are 3 different groups of students. We estimate the parameters of the model on the basis of this data. The program produces the difficulty and discriminativity of the rules, and the starting competence and the learning parameter of the students as output.  As we know from our simulations, the parameter estimates for s and ar are unreliable. We can therefore only draw conclusions for the parameter value estimates of br and 0,s.  The estimates for starting competence can be compared to what we know about the different student groups. We ex- pect the UU students to have most previous experience, and therefore the highest start competence. The HU business in- formatics students (HU-BI) are expected to have the lowest start competence, and the HU technical computer science students (HU-TCS) are expected to be in between. The means and median competences learned from data are: UU mean 1.27 and median 1.36, HU-TCS mean 0.19 and me- dian0.50, and HU-BI mean0.90 and median1.20. This confirms our hypothesis.  The domain experts rank the items by putting them in their perceived order of difficulty. In Figure  these rankings are shown together with the ranking calculated from the difficul- ties estimated from data. We observe that the expert rank- ings look similar to each other, and to the ranking learned from data. The main surprise is rule number 10: rewriting something or true to true.  We use Spearmans rank correlation on the different rank- ings. The correlations between expert rankings 1, 2 and 3, and the ranking estimated from data can be found in table . The estimated model parameters of difficulty and the difficulty rankings provided by experts are highly correlated. Also, the expert opinions do not deviate much more from each other than from the estimated ranking. This means that the model makes a similar estimated ranking of the dif- ficulty of the rules as experts do. The model produces a this similar ranking, even though the amount of data is small.  Another important measure of the quality of the model is accuracy. We measure the accuracy by cross-validation. We set 1 student apart and estimate the rule parameters with the rest of the students. For the one student we set apart, we estimate the start competence and learning parameter  257    Table 1: The rewrite rules that can be applied while performing the DNF task, and the estimated diffi- culty rankings of these rules by experts 1 (Utrecht University of Applied Sciences), 2 (Utrecht Univer- sity of Applied Sciences) and 3 (Utrecht University) and by maximum likelihood estimation.  1 2 3 model  1 x 0.80 0.91 0.84 2 0.80 x 0.83 0.64 3 0.91 0.83 x 0.88  model 0.84 0.64 0.88 x  Table 2: The Spearmans rank correlations between experts 1, 2 and 3, and ranking based on the esti- mated model parameters.  after the first 25 outcomes. Then, for each outcome that follows, we predict whether it will be positive or negative, and determine whether our prediction is accurate. After the prediction we update the estimates for the start compe- tence and learning parameters, before we predict the next outcome. There are 13 students with enough outcomes to warrant this prediction procedure. The average of the total number of outcomes per rule for these students is 6.1. The average accuracy of prediction for the 13 students we pre- dicted the outcomes for is 0.78. For students with mostly correct outcomes the accuracy is not much higher than the percentage of correct answers. Around a proportion of cor- rect outcomes of 0.5, the algorithm performs better. A typ- ical example is student 12, with 52% correct answers, for who the accuracy was 0.71.  4. DISCUSSION The purpose of this research is to create a student model for rule based e-tutoring systems. This model describes and predicts student behavior: whether or not a student applies a rule correctly. For this purpose, we have constructed a probability model based on 2PL IRT (equation ).  We have used simulations to prove that the parameters of the model can be reliably learned; when we generate the data from a certain set of parameters, these parameters can be  adequately recovered. For a small data set of 50 students, 25 rules, and 20 instances per rule, we show the parameters can be estimated with small error. For a data set of 15 students, 23 rules, and 8 instances per rule however, the estimates for the discriminativity of a rule, and the learning parameter of a student become unreliable. The accuracy of prediction for this small simulation however, is still between 0.75 and 0.80.  Using data from students working on rewriting logic expres- sions to DNF, we estimate the student and rule parameters from data obtained from 14 students, 23 rules and an av- erage of 6.1 instances per rule. We compare the estimated difficulties of the rules to the rankings provided by domain experts, and show that the ranking based on the learning from data is as close to the rankings of the experts as the expert rankings are to each other, based on their Spearmans correlations. We show that the initial competence, 0,s, of the students as learned from data, is what we expect based on what we know about the different groups of students who participated.  We cannot draw any conclusions about the result values of rule discriminativity (ar) or student learning speed (s) be- cause the real-life dataset is too small. We know from simu- lations however, that all model parameters are recoverable. To have reliable estimates for all parameters, the dataset should be obtained from around 50 students who apply each rule around 20 times.  We selected the domain of rewriting logic expressions be- cause it is relatively well-known and often taught. In less well-known domains the model could be used to inform the teacher about the rule difficulty and, as hopefully further research will show, discriminativity. The model can also be used to report the competence to the students working on a rule based e-tutoring system.  We conclude that extended IRT is a promising model, which has the potential to provide more information to the users of rule based e-tutoring systems. More extensive tests with real-life data are required, but the results we obtained for the logic domain are promising.  5. REFERENCES [1] Frank B. Baker and Seock-Ho Kim. Item Response  Theory - Parameter Estimation techniques (2nd ed.). Taylor and Francis group LLC, Boca Raton, FL, USA, 2004.  [2] Hao Cen, Kenneth Koedinger, and Brian Junker. Learning factors analysis - a general method for cognitive model evaluation and improvement. In Proceedings of ITS-2006: the 8th International Conference on Intelligent Tutoring Systems, pages 164175, 2006.  [3] Bastiaan Heeren, Johan Jeuring, and Alex Gerdes. Specifying rewrite strategies for interactive exercises. Mathematics in Computer Science, 3(3):349370, 2010.  [4] Jeff Johns, Sridhar Mahadevan, and Beverly Woolf. Estimating student proficiency using an item response theory model. In Proceedings of ITS-2006: the 8th International Conference on Intelligent Tutoring Systems, pages 473480, 2006.  258      "}
{"index":{"_id":"46"}}
{"datatype":"inproceedings","key":"Barber:2012:CCU:2330601.2330664","author":"Barber, Rebecca and Sharkey, Mike","title":"Course Correction: Using Analytics to Predict Course Success","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"259--262","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330664","doi":"10.1145/2330601.2330664","acmid":"2330664","publisher":"ACM","address":"New York, NY, USA","keywords":"higher education, learning analytics, predictive analytics, predictive modeling, retention","Abstract":"Predictive analytics techniques applied to a broad swath of student data can aid in timely intervention strategies to help prevent students from failing a course. This paper discusses a predictive analytic model that was created for the University of Phoenix. The purpose of the model is to identify students who are in danger of failing the course in which they are currently enrolled. Within the model's architecture, data from the learning management system (LMS), financial aid system, and student system are combined to calculate a likelihood of any given student failing the current course. The output can be used to prioritize students for intervention and referral to additional resources. The paper includes a discussion of the predictor and statistical tests used, validation procedures, and plans for implementation.","pdf":"Course Correction:   Using Analytics to Predict Course Success  Rebecca Barber, Ph.D   Apollo Group  4035 Riverpoint Parkway   Phoenix, AZ 85040  1-602-557-7842   rebecca.barber@apollogrp.edu   Mike Sharkey  Apollo Group   4035 Riverpoint Parkway  Phoenix, AZ 85040   1-602-557-3532   mike.sharkey@apollogrp.edu      ABSTRACT  Predictive analytics techniques applied to a broad swath of student  data can aid in timely intervention strategies to help prevent  students from failing a course. This paper discusses a predictive  analytic model that was created for the University of Phoenix. The  purpose of the model is to identify students who are in danger of  failing the course in which they are currently enrolled. Within the  models architecture, data from the learning management system  (LMS), financial aid system, and student system are combined to  calculate a likelihood of any given student failing the current  course. The output can be used to prioritize students for  intervention and referral to additional resources. The paper  includes a discussion of the predictor and statistical tests used,  validation procedures, and plans for implementation.     Categories and Subject Descriptors  G.3 SPSS; H.2.3 SQL; H.2.4 Oracle; J.1 [Administrative Data  Processing] Education; K.3.1 [Computer Uses in Education]  Collaborative learning, Computer-assisted instruction (CAI),  Computer-managed instruction (CMI), Distance learning   General Terms  Management, Measurement, Experimentation   Keywords  Learning Analytics, Predictive Analytics, Predictive Modeling,  Higher Education, Retention.   1. INTRODUCTION  Whereas the holy grail of predictive models in higher education  would likely be one that could predict graduation at the time a  student applies for admission, the reality is that the elapsed time  between the start and end of college is years long, creating the  opportunity for a multitude of factors to interfere with a students  progress. A model to make such a prediction will take years to  develop and require data beyond the scope of what is available in  an institutions Student Information System (SIS) and LMS.   Nonetheless, there are known reasons students fail to graduate.  Work schedules, health problems, child care challenges,  transportation, and financial issues comprise some of the reasons  students drop out that are largely outside of the institutions   control [1]. One predictor of students decision to drop out that is  under the institutions purview, however, is a lack of preparation  or effort as reflected in their course grades.   Regardless of the reason, a student enrolled in a course will often  display signs of course failure before either formally withdrawing  or disappearing altogether. Failing to attend class (or in the case of  online courses, failing to participate in discussion forums), sloppy  or incomplete assignments, or a significant change in the students  behavior and academic performance are all warning signs that a  student may be on the verge of dropping out.   It is generally in both the students and the institutions best  interest that students remain enrolled or, if they must leave,  withdraw via the established process.  It is this concern that  precipitated the development of a predictive model. Whereas the  student may be wrestling with problems that range from personal  tragedy to time management or academic under-preparedness, the  University can monitor the students behavior in a course for  warning signals and increase that students priority for a call from  his or her academic advisor. The advisor can help the student by  pointing them to necessary resources, coaching them on time  management, or even advising early withdrawal.   This paper discusses the rationale for the model; the process  through which it was developed, revised and refined; and the  validation of the model by the operational team. The next section  will provide the context within University of Phoenix and further  exposition of the problem the model is intended to address.  Following this discussion is a brief overview of the extant  literature that guided some of the decisions made about variables  included in the model. This section is followed by a description of  the methods used to develop the model, including the data  elements found to be predictive and those found to be irrelevant to  prediction. We then discuss the process used to validate the model  within our operational environment. Finally, we offer a brief  discussion of next steps and plans for broader implementation.   2. INSTITUTIONAL CONTEXT  Founded in 1976, University of Phoenix is a regionally accredited,  degree-granting institution. Based in Phoenix, Arizona, the  University has over 200 campuses throughout the United States  and the largest (measured in student enrollment) online campus in  North America. In addition to holding regional accreditation,  University of Phoenix holds programmatic accreditation in  nursing, counseling, business, and education. As of August, 2011  the university enrolled more than 340,000 students in over 100  degree programs, ranging from associates through doctorates.   The university was founded with a focus on working adults who  wished to complete their degree. These non-traditional learners  remain the focus of the university, resulting in a more diverse      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that   copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.   LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.      259    student population than found in traditional institutions, in terms  of racial and ethnic demographics as well as the proportion of  first- generation college students [2]. Also, many non-traditional  students are employed while pursuing their education. In order to  help students complete their degrees in a time-efficient fashion,  University of Phoenix adopted a focused academic model in  which courses last between 5 and 9 weeks1.   Non-traditional learners may have been out of school for many  years before deciding to pursue a post-secondary degree. Adding  school responsibilities to busy schedules and refreshing study  skills are challenges that all returning students face.  We  constantly seek new ways to provide our students with the  services they need in order to progress academically, including  tutoring and coaching.  And, although a number of static triggers  (such as those that monitor attendance) exist to monitor students  for signs of trouble, there is continued interest in improving the  information available to academic advisors in order to direct  interventions.     3. THEORETICAL FOUNDATION  Garman used logistic regression to predict student success in an  online database course based primarily upon scores on a reading  comprehension assessment [3]. The only other input to the model  was the semester in which the student took the course. Whereas  this approach is interesting in that it supports the proposed  methodology for our model, the study found the semester variable  insignificant and the assessment score only minimally predictive.    Moore looked explicitly at course participation in both the  students current and prior courses [4]. This research indicated  increased participation to correlate highly with higher  performance in the course. Some other variables, such as student  expectations, high school rank, and entry exam scores (ACT)  were not significant predictors of student achievement.   The standard measure for monitoring participation in an online  course is student discussion postings, and prior research has found  final grades correlated with the number of postings both read and  written by students [5]. However, other research has found  postings to have an indeterminate relationship with course success  [6,7]. Ramos and Yudko found that total page hits were more  predictive than discussion board use of online course success [8].  The lack of agreement suggests including post counts in the model  until they can be definitively excluded.    Regarding demographic variables, Martinez found high school  GPA, age, sex, grade in last math class, highest level of math,  ethnicity, definite major choice, and work hours planned to  predict success in different levels of English courses [9]. In  addition, current credit hours, financial aid usage, and program  level were predictive of the likelihood of drop out [10]. Where  possible these variables were included.    Because the goal of this project was to work from an existing data  set, studies addressing variables that are unavailable (such as self- discipline, motivation, locus of control, and self-efficacy) were  not included in our literature review.    4. METHODS  Based on the literature, the variables in Table 1 were identified as  potentially useful and worth examining.  A number of key                                                                 1 Associates degree students take 2 9-week courses at a time. Bachelors   students take 1 5-week course at a time. Graduate students take 1 6-  week course at a time.  Additional weekly hours make these courses  equivalent to traditional semester-based courses.   variables were populated for less than 50% of the cases. Logistic  regression drops any records for which all of the fields are not  populated, resulting in too large a loss of data.  Therefore, despite  theoretical support for those fields inclusion, the decision was  made to exclude them from the analysis at this time.   Table 1: Variable Disposition   Field Status   Attendance /week Model 1-3  % cumulative course points /week Model 1-3  Prior credits earned Model 1 (replaced w/ratio)  Discussion post count /week Model 1-3  Late assignments Bad data quality  Gender Model 1  Age at program start Model 1  Unsubstantive post count <10% populated  Ethnicity <50% populated  Marital Status/Dependents <25% populated  Employment Status/Years  <25% populated  Household Income/Salary <25% populated  High school GPA <25% populated  Financial aid / Pell Grant recipient Model 2  Total student loans taken Model 2  Financial status (current/other) Model 2-3  Ratio of credits earned/attempted Model 2-3  Military status Model 2  Aattendance Model 2-3  Days into the course of 1st activity Model 2-3  Pct point delta to prior courses Model 2-3  Orientation participation Model 3  Inactive time since last course Model 3  Count of messages to instructor Model 3   4.1 Model Version 1  A consulting company using a limited data set created the initial  model. The data included a unique identifier; basic demographic  information from a voluntary survey completed by the student at  time of admission; and academic history within the University,  including number of transfer credits, number of courses taken, and  percentage of points earned in these courses. For each course,  information was also provided about discussion board postings,  points earned by week within the course, and whether the student  submitted assignments late. The consulting company used this  data to create a logistic regression model.   The analysis of the initial data exposed missing data and data  quality issues that would have compromised the final model.  Fields, such as submission timeliness and discussion board post  quality, were found to be either inaccurate or missing too many  records to contribute to the model. The final data set was reduced  to data reflecting transfer credits, prior academic activity at  University of Phoenix, and week-by-week activity (points earned  and discussion posts made) for each course. These data elements  were further recoded to create interpretable indicators.    The data set included all activity for approximately three months2,  organized by degree. The SPSS randomization algorithm selected  approximately 50% of the data as a hold-out sample, making the  remaining 50% available for model development. These data were  analyzed using logistic regression, with the outcome variable                                                                 2 The data set included more information for some elements, but October   to December 2010 was the most complete.   260    being an indicator of whether the student passed the course. The  model assessed student data through course week 4.   Separate models were developed for each degree level.  For  example, coefficients for bachelors degree students through week  2 were as follows:   Table 2: Coefficients for Model 1, Bachelors degree students    Week 0 Week 1 Week 2   <65% points in prior courses -1.44  -0.76  -0.52   >85% point in prior courses +0.46  +0.58  +0.82   Credits earned at Univ of Phx +0.07  +0.02  +0.02   Online Posts  +0.15  +0.18   Cumulative points Earned  +3.15  +4.22    * All results significant at p<.05 level.     These coefficients allow us to classify each student into one of  three tiers in week zero3: high risk, low risk, or neutral risk. Initial  percentages that comprised the neutral zone ranged from 41% to  54% of students.   Models in weeks 1 through 4 added discussion post information  and percentage of assignment points earned. This parsing  immediately (by the end of week 1) trimmed the range of the grey  zone to between 35% and 40%. By week 2 all masters degree  students were out of the neutral zone. By week 3 all bachelors  degree students were out of the neutral zone.  Results for students  not in the neutral zone were accurately predicted on average 94%  of the time, with no week below 85%. In other words the  prediction of pass (low risk) or fail (high risk) was accurate more  than 90% of the time.   4.2 Concerns with Model 1  The neutral zone was quite large initially, and the team felt that a  better way to categorize the students in that zone was to assigned  a score to each student, ranging from 1 (unlikely to pass)  10  (nearly sure to pass). That score would provide the prioritization  needed to make the output actionable.  Also, since some courses  are as long as 9 weeks, the time frame needed to be extended.   There was also concern about the reproducibility and actionability  of the model as developed. The data used came from a variety of  different databases and, as such, required significant manual  intervention to compile. At least one of the data sources required a  programmer to do an ad-hoc query to generate a data set that was  not directly accessible to the analytics team.    Enhancement of the model was brought in-house. The model was  replicated for validation purposes, and then, the process of  refinement and further development was started.   4.3 Model Version 2  One of the first problems addressed was that of data availability  and validity. As mentioned, there were critical data elements  around discussion board postings that were not easily available.  However between the initial data request and the start of model 2  a partial feed of the data was added to the enterprise data  warehouse environment. This advancement allowed post count by  week to be incorporated into the model. Further, the ability to  automatically update all data fields will make implementation  easier.     One limitation of the new data source is that it has only been  populated since June, 2011. Therefore data extraction processes                                                                 3 Week zero is the week prior to starting the course. There is no activity in   the course yet to use for the model.   were developed to use data from June through October, 2011, to  construct the updated model.  Additionally, some variables that  had not been included in model 1 were included in model 2. These  included military status and financial status.   As model 1 showed, some of the variables that the literature  suggested were relevant proved not to be when looking across all  programs and levels.  Gender, age, military status, pell grant  receipt and responsible party (whether the student was receiving  financial aid, paying through their employer, or paying directly)  were not significant or resulted in extremely low weights.  There  was also some variability by degree level.  For example, military  status was not significant for associates and bachelors degree  students, but was significant and negative for masters degree  student.  One interpretation is that masters-level military  personnel are more likely to be officers and therefore more likely  to have substantial responsibilities that could interrupt their  studies; however, other explanations could be considered.   Because of their lack of predictive power, most of these variables  were eliminated from the final version of Model 2.   Model 2 was built using a Nave Bayes algorithm in RapidMiner  and validated using 10x cross validation.  A sample of  information gain weights on a zero to one scale are in Table 3.   Table 3: Weights for Model 2, Bachelors Online students    Week 0 Week 1 Week 2   Cumulative points earned (%)  1.00 1.00  Financial Status not current 0.78 0.33 0.28  Ratio credits earned/attempted 0.95 0.36 0.28  Points earned - prior courses (%) 1.00 0.40 0.34  Online post count  0.25 0.21  Point delta - prior courses >10%  0.47 0.42  Sum of student loans taken 0.30 0.14 0.05  Number of concurrent courses 0.47 0.05 0.02  Attendance  0.12 0.08   * All results significant at p<.05 level. Contact 1st author for full table.   The new variables added to model 2 increased the predictive  accuracy considerably.  Model 2 accurately predicted 85% of all  students at week 0 (compared to 50% in model 1), rising to 95%  by week 3.     Specifically, the ratio of credits earned to credits attempted was a  substantial indicator of potential problems, as was a financial  status other than current.  As might be expected, cumulative  points earned remained the most powerful predictor. In addition,  whether the point delta between prior and the current course had  change by more than 10% was treated as a categorical variable  and was also influential.  This variable is less about the exact  point change than an indicator of a substantive change in  behavior.  Most of the other variables provided less predictive  power than these three, although enough to keep in the model.     4.4 Model Version 3  Development of model version 3 is awaiting the availability of  higher quality posting data. While version 2 allows direct access  to the total number of posts made by a student, it fails to provide  any distinction between posts made in response to discussion  questions, posts made in collaborative forums, and posts made to  the private forum provided for discussion with the instructor.  Because the literature suggests a link between passing and  engagement with the instructor [6], this differentiation between  post sources is necessary for the next phase of analysis.  Acquisition of this data is underway.   261    Additionally, per Ramos and Yudko [8], there is value in tracking  student page views within the learning environment. Currently,  only aggregate data is available, but these data are not useful from  a predictive analytics perspective. Accordingly, the technical team  is working to capture this data at the individual student level.   To complement these data elements around discussion board  activity, three variables will be added based upon research  conducted at another institution: major area of study, time since  last course, and participation in an orientation program. These  elements will be incorporated into the model while the discussion  data is being sourced.   One other potential modification will be a review of other types of  models.  Specifically support vector machines and random forest  models will be investigated for improved predictive accuracy.   5. MODEL VALIDATION  Validation of model 1 involved using the 50% hold-out sample.  The risk category percentage differences between estimation and  hold-out samples were within two percent, with the majority of  cases within one percent.  For model 2, a 10x cross-validation  procedure was used which provide that the model was highly  accurate at predicting students who would pass, but with some  room for accuracy improvement on those students who were  predicted to not pass.  These were under predicted in the model.   More important than validation of the model fit, however, is  validation of the models utility. In order to validate that the  model was indeed providing actionable information, a pilot has  been initiated to create scores that could be provided to academic  advisors. The academic advisors then use these scores for  prioritization, calling students with the highest risk score first,  even if that student would not normally have received a call, while  delaying calls to students who scored lower.   The initial pilot of the model is being conducted with only a few  academic advisors. These more experienced advisors are looking  at the model in terms of both its accuracy (does the information  the model provides align with what they learn by talking to the  student) and its utility (does it trigger contact with the right  students, and are those students then successful). Statistical  validity alone is insufficient; the model must provide actionable  information to front-line advisors in a form that can increase  student success in order for it to be seen as truly valid.   6. NEXT STEPS AND IMPLEMENTATION  Providing a score for students that can be used for prioritization is  helpful, but too many students remain in the neutral zone during  weeks 0 and 1.  Refinement will continue, with the objective of  accurately placing them at one end of the scale or the other.     The overarching goal for this project remains to provide valuable,  timely information to academic advisors. Once the pilot   completes, the utility will be evaluated and a decision will be  made as to whether to implement the model into the production  processes, making the results available to all academic advisors.  At this point, it is unclear as to whether this will be in the form of  a gradient categorization (e.g. red/yellow/green), a numeric score,  or a percentage chance of withdrawal. Accordingly, the model  will continue to be refined after initial implementation to best suit  advisors needs.   Future refinements will depend on the availability of additional  detail data from both the learning management platform and the  student information system. Concurrent work is proceeding to  substantially improve access to that data, making integration into  the model both technologically easier and substantially faster.   7. ACKNOWLEDGMENTS  Our thanks to Andrew Tubley for help with the title of this paper.   8. REFERENCES  [1] Cohen, A. and Brawer, F. 2009. The American Community   College. John Wiley and Sons, NY, ISBN: 9780470605486.  [2] University of Phoenix, 2010 Academic Annual Report.   http://www.phoenix.edu/about_us/publications/academic-  annual-report/2010.html   [3] Garman, G. 2010. A Logistic Approach to Predicting Student  Success in Online Database Courses. American Journal of  Business Education. 3(12), 1-5.   [4] Moore, R. 2007. Do Students Performances and Behaviors in  Supporting Courses Predict Their Performances and  Behaviors in Primary Courses Research and Teaching in  Developmental Education. 23(2), 38-48.   [5] Wang, A.Y. & Newlin, M.H. 2000. Characteristics of  students who enroll and succeed in Psychology web-based  classes. J. Educational Psychology. 92(1), 137-143.   [6] Reisetter, M. & Boris, G. 2004. What works: student  perceptions of effective elements in online learning.  Quarterly Review of Distance Education. 5(4), 277-291.   [7] Sadik, A. & Reisman, S. 2004. Design and implementation  of a web-based learning environment: lessons learned.  Quarterly Review of Distance Education. 5(3), 157-171.   [8] Ramos, C. & Yudko, E. 2008. Hits (Not Discussion  Posts) Predict Student Success in Online Courses: A Double  Cross-Validation Study. Computers & Education. 50(4),  1174-1182.   [9] Martinez, D. 2001. Predicting student outcomes using  discriminant function analysis. Paper presented at the 39th  Annual Meeting of the Research and Planning Group, Lake  Arrowhead CA.   [10] Morris, L., Wu, S. & Finnegan, C., 2005. Predicting retention  in online general education courses. American Journal of  Distance Education. 19(1), 23-26.     262      "}
{"index":{"_id":"47"}}
{"datatype":"inproceedings","key":"Gunnarsson:2012:PFC:2330601.2330665","author":"Gunnarsson, Bjorn Levi and Alterman, Richard","title":"Predicting Failure: A Case Study in Co-blogging","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"263--266","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330665","doi":"10.1145/2330601.2330665","acmid":"2330665","publisher":"ACM","address":"New York, NY, USA","keywords":"co-blogging, learning analytics, participation, predicting failure","Abstract":"Monitoring student progress in homework is important but difficult to do. The work in this paper presents a method for monitoring student progress based on their participation. By tracking participation we can successfully create a model that predicts, with very high accuracy, if a student is going to score a low grade on her current assignment before it is completed, thus enabling selective interventions.","pdf":"Predicting failure: A case study in co-blogging  Bjorn Levi Gunnarsson Brandeis University  Computer Science Department Waltham, MA 02454 USA  bjornlevi@cs.brandeis.edu  Richard Alterman Brandeis University  Computer Science Department Waltam, MA 02454 USA  alterman@cs.brandeis.edu  ABSTRACT Monitoring student progress in homework is important but difficult to do. The work in this paper presents a method for monitoring student progress based on their participation. By tracking participation we can successfully create a model that predicts, with very high accuracy, if a student is going to score a low grade on her current assignment before it is completed, thus enabling selective interventions.  Keywords Co-blogging, Participation, Predicting failure, Learning an- alytics  Categories and Subject Descriptors J.1 [Administrative Data Processing]: Education; K.3.1 [Computer Uses in Education]: Collaborative learning, Learner modeling, Predictive applications of data  1. INTRODUCTION Monitoring student progress on homework is important.  It is however time consuming and not always accessible  by definition homework is done at home, away from most mon- itoring methods. As early as possible, the instructor wants to detect those students who are not doing their homework, identify why, and hopefully help the student resolve the issue preventing her from successfully completing it.  One indicator that a student is doing well is her participa- tion in class. However, even if a student participates in class, she may be too busy or lack the motivation to apply herself to the homework. In-class exercises enable the teacher to do some observation of each students progress, but as class- size increases, finding struggling students becomes difficult  they get lost in the crowd. The ideal situation would be to somehow automatically monitor student progress on home- work and give timely feedback to the instructor or student about possible problems.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK 12, 29 April  2 May 2012, Vancouver, BC, Canada. Copyright 2012 ACM ACM 978-1-4503-1111-3/12/04 ...$10.00.  In a regular classroom, it is difficult to imagine how one could automatically monitor student progress. For students engaged in online learning activities, several options open up. For example, the work of Campell et al [3] develops data mining techniques, that look for students that are strug- gling or at-risk of failing, which trigger alerts of potential problems to instructors. This approach to predicting failure enables teachers to quickly sort out students who are hav- ing problems. While this kind of automatic assistance for early identification of struggling students is helpful it still falls short of what would be ideal; rather than waiting until the grades come in, the system should monitor and report progress while the students work on the assignment.  The work presented in this paper develops a model of stu- dent progress based on student participation in the an online collaborative learning activity. Participation and collabora- tion can be keys to online learning [8, 10] and tracking it could thus be a predictor of success or failure. Participa- tion can be directly tracked from an activity log of an online learning system. However, For some learning activities, a student can do a lot of work offline that is not visible to an online activity logger. What kind of activity is a good mea- sure of participation and a good predictor of performance In other words, what kinds of participation can I observe and are they important and predictive  This short paper will present a method for monitoring student effort while working on a homework assignment by tracking participation. A multiple regression model is pre- sented. It combines the current level of participation for an assignment and prior grades to predict if each student is on a path to succeed or fail with her current assignment. We describe student co-blogging as a source of data  during a semester-length course, students did their homework in a blogosphere.  2. BACKGROUND Information technology (IT) is widely used in the class-  room; in some communities students expect the classroom to include IT [16]. A range of technical inclusions can have im- pact, from the simple introduction of internet resources into the classroom to more advanced technology  like computer- supported intentional learning communities [12]  that ex- pand the learning context while enhancing lessons [11]. The most relevant content is available in the course material, but these other resources and modalities of learning have additional value. Availability does not necessarily translate into effective use. For example, searching the Internet for additional content requires skill [15].  263    The successful integration of an online learning environ- ment into the flow of a course depends on more than just technical skill. The payoff for students spending time on- line has to significantly exceed the costs of getting them on- line. Once the students are online, it is important to mon- itor their performance so that students who are not suc- ceeding can be identified. This problem exists offline too, but for an online learning activity alternate automatic or semi-automatic methods could be implemented to track stu- dent performance. Early detection of failure is an important problem for which a learning analytic approach may prove to be useful [1, 3].  Participation is a metric that can be used to gauge stu- dent progress. It is an important factor in collaborative on- line learning environments [6]; lack of participation is a risk factor for failure [7]; promoting participation is an effective method for improving student outcomes [11].  Learning environments that enable students to collabo- rate depend on grounding, mutual understanding and back- ground [2, 5]. In order to track user participation in an online learning environment we need reliable participation data. The collaborative environment is that platform be- cause its content is not available offline. Our study uses one type of collaborative learning environment [4], student co-blogging.  3. CASE STUDY  3.1 The co-blogging activity The data we present was collected from a course on Hu-  man Computer Interaction (HCI). There were about 50 stu- dents in the class: a mix of undergraduates and Masters students. In a student co-blogging community, each stu- dent had her own blog. During the semester, each student regularly posted to her blog. Students also browsed in the course blogosphere, read peer contributions and commented on them.  Homework assignments were weekly blog posts about var- ious methods used in HCI. Students, for example, did needs analysis, generated data gathering plans, or did expert re- views. The assignment was the same for all the students, but each student applied the methods to a different website, software, or device of her choice and posted about it in the course blogosphere.  Students were encouraged to read freely in the blogosphere throughout the semester. While working on an assignment, students were allowed to review the posted work of other students and revise their own posts up until the deadline. In this manner, the co-blogging environment is a platform for peer tutoring, peer assessment, and cooperative learning [14]. This has some similarity to peers getting together and discussing homework but then separately writing their own solution.  After the submission deadline, the TA assigned to each student two posts to critique; the critiques were then due a few days later. Students were also encouraged to do addi- tional commenting and respond to comments on their own posts. The critique part of the process is more about self- assessment [13]; a survey of the class suggested that students found writing critiques more useful than getting them.  There was incentive to create high quality posts in the blogosphere. Earlier posts could be used as a reference for later assignments. Learning how to apply the methods from  previous assignments saved time when doing a later assign- ment. By only having low quality earlier posts to go back to, much of the work would have to be done again. The student gains less by not adequately learning the material when they are first exposed to it, and later activities benefit less from blogosphere content because the quality is, as a result, not as good.  3.2 The technology The co-blogging system was developed by the primary au-  thor of this paper. Users can preview a post by hovering over its title and open a post to view its contents and any comments it has accrued  these are all different kinds of participation.  Students could filter blog posts by users (view all posts by the selected user), assignment tags (each homework assign- ment had a different tag associated with it) or view only the posts that were top-rated by the instructors. An author was notified if his post received comments. Most of the time stu- dents would browse the most recently updated posts. This meant that both good and bad posts were regularly read and were likely to receive comments regardless of their quality.  3.3 Grading Posts and critiques were graded by several TAs on the  scale of 0-3 for posts and 0-2 for critiques, where 0 means the assignment was not completed, 1 indicates not good , 2 is good, and 3 is exemplary work. The scale for critiques was 0 for not completed, 1 for not constructive, and 2 for a constructive and good critique.  4. PREDICTING FAILURE We regard reading content generated by a peer to be a  form of peer learning and as such it has tremendous ed- ucational value. There were many ways in which having access to the progress of other students could prove valu- able. By reading in the blogosphere, a student could get help in interpreting the homework requirements, she could get started on a difficult part of the assignment, she could look at formatting and presentation, or she could verify or check her answer. For these reasons, it was assumed that reading in the blogosphere was the most significant form of participation. It is also worth noting that a student cannot be offline and leverage peer content effectively. Doing so would require the student to save and maintain a synchro- nized version of the online content. The effort of opening a post and saving it would be logged as participation anyway. An automatic version of this process would still be more effort than simply accessing the content in the blogosphere itself. For these reasons reading in the blogosphere is a re- liable measure. Writing is not as good a predictor because, for example, some students draft their work outside of the blogosphere.  In what follows, grades are paired with participation. The relationship between average participation and average grades is explored for all assignments and each assignment. A mul- tiple regression model that combines participation with av- erage previous grades is presented.  The application of the model shows a significant positive relationship between participation and grades: the more a student participates (reads) the more likely it is that she will receive a high grade on the homework assignment. This relationship holds for all the assignments together, and any  264    individual assignment. Thus the relationship is positive, sig- nificant and consistent.  4.1 The Relevant Student Participation We tracked student activity while the students were writ-  ing their posts to predict success before their homework was turned in for grading. We used the simplest form of par- ticipation where we counted the number of times each user read a post created by another user. Regardless of the time spent reading that post (time until next link click happens) or how many times a particular post was read. Clicking a link to open a post written by a peer is just counted as one read in terms of participation in the blogosphere. User participation at any time is the total number of reads.  Reading enables a student to make use of content gener- ated by his peers. The user can discard the content as useless or accept it as helpful to whatever purpose the user had in mind when clicking the link. Posts can be previewed which means that users might have some idea about the content of the blog post before actually clicking the link. This preview is not counted as participation for the purpose of our model.  4.2 Pairing participation with grades The relationship of interest is between participation and  grades. We hypothesize that students who learn from the contributions of other students, as measured by their par- ticipation, produce higher quality solutions, as measured by their grade. Using this data, we then want to predict, before they are graded, if they are going to succeed or fail.  The data we explore is from the first six homework as- signments for the class. It is generated by scanning the co- blogging activity log which stores the user name associated to each action, the URL requested, and the date of request. The log is counted for reads by each user, in each homework assignment, for every post that has a matching homework tag, and was written by another user. This gives each user a participation number for every homework assignment that is paired with the grade of that same assignment. The dis- tribution of the participation numbers was skewed towards several very high activity users so the log transform of the number was taken to remove the outlier effect. The result- ing participation distribution was close to being normally distributed (mean=1.5, standard deviation=0.36).  5. CREATING A MODEL Participation is the number of times each student reads  a post that was written by another student. Average grade is the combined grade of post and comments for all assign- ments; the maximum grade for any assignment was 5, com- bined 3 for a post and 2 for a critique.  Average participation was highly positively related with average grade (r=0.7, p=0.00000003). Participation for in- dividual assignments was also significantly related with the grade for each assignment (lowest r=0.41 with p=0.0038) (see Figure 1).  6. APPLYING THE MODEL We use existing data to create a multiple linear regres-  sion model to predict failure for the homework the student is currently working on. We use linear models composed of current participation data and previous grades to calcu- late the expected grade. The hypothesis is that previous  Figure 1: Individual assignment model.  grades and the participation in the current homework can predict the grade received for that assignment. For example, if the student is working on homework number 3, the aver- age grades of the previous two assignments and the students participation for homework number 3 are used as predictors in the multiple linear regression models (see Figure 2).  We create one model for each homework. The models use current participation and the average of previous grades. Prior grades do not have a big impact in the first couple of regression models but by the third prediction model previous grades start to have predictive value. Participation remains an important part of the models throughout. It is important to note that no alarms about grades were sent to students.  If a notification would have been sent to students with a predicted grade of less than 3 out of 5, which is close to a failing grade of <2.5, then the intervention would have had the following effects:   Average alarm rate: 26%  Percentage of students that are predicted to score less than 3 out of 5. These are the number of students that would receive a noti- fication of possible homework failure.   False alarms: 10%  Students that would have been notified but even without the intervention, did not fail.   Average miss rate: 6%  Students that the model did not predict failing and without the intervention actually scored a low grade.   Unexplained miss rate: 0.83%  Out of 240 grade predictions only two students scored a low grade and would not have been notified.  The explained misses were students that only submitted a partial assignment (usually forgot to give critiques). There are different kinds of alarms that can go off for students nearing deadline without having a submission so we can only consider them to be possible misses by our model.  265    Figure 2: Grade and Participation model coefficients and p-value.  7. CONCLUDING REMARKS Participating in a collaborative learning environment en-  ables the exploration of peer created content. A student posts drafts in the blogosphere, reads drafts of the same homework by other students and has the option of leaving comments and marking those she especially likes. By explor- ing how peers approach the assignment, format the answer or solve a particular problem, she is contributing to a com- mon resource  a non-rivalrous resource [9]  that is usable later in the semester to solve different problems with the same method.  We have explored learning and preventing failure and found a promising method for notifying students before they get a bad grade. The study presented in this paper explored student participation in an online collaborative discourse co- blogging community while they worked on creating content that they could later use as a resource for doing their term project. By tracking participation and relating it to earlier grades, we successfully created a model that predicted, with very high accuracy, if a student was going to score a low grade on the assignment she was currently working on.  Further research could explore the use of peer assessment as a replacement for teacher grades in the model. The ap- proach to failure prediction developed in this work may also prove useful for reducing the size of the grading task for large classes. For a large class, weekly grading can be much too labor intensive. Spot checking and rotating through the class can provide some feedback, but the addition of a fail- ure prediction mechanism that works for each assignment as it is being done, could help the instructors to be more selective about which students to grade more closely. The approach to failure detection we have presented can poten- tially be reversed, in which case, the instructor would have an early detection mechanism for identifying students who are doing well on the current assignment. After verification, the instructor could choose to notify the rest of the class  of these examples of good work, so as to steer the weaker students in the right direction.  8. REFERENCES [1] P. Baepler and C. Murdoch. Academic analytics and  data mining in higher education. 2010.  [2] M. Baker, T. Hansen, R. Joiner, and D. Traum. The role of grounding in collaborative learning tasks. Collaborative learning: Cognitive and computational approaches, pages 3163, 1999.  [3] J. Campbell, P. DeBlois, and D. Oblinger. Academic analytics: A new tool for a new era. Educause Review, page 11, 2007.  [4] P. Dillenbourg. What do you mean by collaborative learning. Collaborative learning: Cognitive and computational approaches, pages 116, 1999.  [5] P. Dillenbourg and D. Traum. Sharing solutions: persistence and grounding in multi-modal collaborative problem solving. The Journal of the Learning Sciences, 15(1):121151, 2006.  [6] M. Driscoll. How people learn (and what technology might have to do with it). eric digest. Syracuse, NY: ERIC Clearinghouse on Information and Technology.(ERIC Document Reproduction Service No. ED 470032), 2002.  [7] J. Finn. School engagement and students at risk. Washington, DC: National Center for Education Statistics, 1993.  [8] S. Hrastinski. A theory of online learning as online participation. Computers & Education, 52(1):7882, 2009.  [9] L. Lessig. The future of ideas: The fate of the commons in a connected world. Random House Inc, 2001.  [10] J. Mason and P. Lefrere. Trust, collaboration, e-learning and organisational transformation. International Journal of Training and Development, 7(4):259270, 2003.  [11] K. Ruthven, S. Hennessy, and R. Deaney. Incorporating internet resources into classroom practice: pedagogical perspectives and strategies of secondary-school subject teachers. Computers & Education, 44(1):134, 2005.  [12] M. Scardamalia and C. Bereiter. Computer support for knowledge-building communities. Journal of the learning sciences, pages 265283, 1993.  [13] K. Topping. Peer assessment between students in colleges and universities. Review of Educational Research, 68(3):249, 1998.  [14] K. Topping. Trends in peer learning. Educational Psychology, 25(6):631645, 2005.  [15] C. Torrey, E. Churchill, and D. McDonald. Learning how: the search for craft knowledge on the internet. In Proceedings of the 27th international conference on Human factors in computing systems, pages 13711380. ACM, 2009.  [16] M. Wilson. Teaching, learning, and millennial students. New Directions for Student Services, 2004(106):5971, 2004.  266      "}
{"index":{"_id":"48"}}
{"datatype":"inproceedings","key":"Arnold:2012:CSP:2330601.2330666","author":"Arnold, Kimberly E. and Pistilli, Matthew D.","title":"Course Signals at Purdue: Using Learning Analytics to Increase Student Success","booktitle":"Proceedings of the 2Nd International Conference on Learning Analytics and Knowledge","series":"LAK '12","year":"2012","isbn":"978-1-4503-1111-3","location":"Vancouver, British Columbia, Canada","pages":"267--270","numpages":"4","url":"http://doi.acm.org/10.1145/2330601.2330666","doi":"10.1145/2330601.2330666","acmid":"2330666","publisher":"ACM","address":"New York, NY, USA","keywords":"college student success, early intervention, learning analytics, retention","Abstract":"In this paper, an early intervention solution for collegiate faculty called Course Signals is discussed. Course Signals was developed to allow instructors the opportunity to employ the power of learner analytics to provide real-time feedback to a student. Course Signals relies not only on grades to predict students' performance, but also demographic characteristics, past academic history, and students' effort as measured by interaction with Blackboard Vista, Purdue's learning management system. The outcome is delivered to the students via a personalized email from the faculty member to each student, as well as a specific color on a stoplight -- traffic signal -- to indicate how each student is doing. The system itself is explained in detail, along with retention and performance outcomes realized since its implementation. In addition, faculty and student perceptions will be shared.","pdf":"Course Signals at Purdue: Using Learning Analytics to  Increase Student Success   Kimberly E. Arnold  Purdue University   519 Young Hall, 155 S. Grant Street  West Lafayette, IN 47907 USA   kimarnold@purdue.edu   Matthew D. Pistilli  Purdue University   517 Young Hall, 155 S. Grant Street  West Lafayette, IN 47907 USA   mdpistilli@purdue.edu         ABSTRACT  In this paper, an early intervention solution for collegiate faculty  called Course Signals is discussed. Course Signals was  developed to allow instructors the opportunity to employ the  power of learner analytics to provide real-time feedback to a  student.  Course Signals relies not only on grades to predict  students performance, but also demographic characteristics,  past academic history, and students effort  as measured by  interaction with Blackboard Vista, Purdues learning  management system.  The outcome is delivered to the students  via a personalized email from the faculty member to each  student, as well as a specific color on a stoplight  traffic signal   to indicate how each student is doing. The system itself is  explained in detail, along with retention and performance  outcomes realized since its implementation. In addition, faculty  and student perceptions will be shared.   Categories and Subject Descriptors  J.1 [Administrative Data Processing]: Education    General Terms  Measurement, Performance    Keywords  Learning Analytics, College Student Success, Early  Intervention, Retention    1. INTRODUCTION  The first year of college is arguably the most critical with regard  to the retention of students into subsequent years of study [2, 3,  8, 9]. Noel and Levitz indicate that retention, or the lack of  attrition from college, is a by-product of student success [4].  Tinto has spent much of his career investigating the necessary  conditions for student success, and notes that academic support  is among the primary pieces necessary to ensure success. In his  1993 book, Leaving College, Tinto proposed three necessary  conditions for student persistence. First, an institution needed to  put programs into place that placed the welfare of the students  higher than that of the university. Second, programs and   solutions should be focused on all students at an institution, not  just a specific subpopulation. Finally, solutions implemented to  enhance student success, and therefore persistence, needed to  help integrate a student academically into the institution [6].   Helping a student become academically integrated to the  institution is key, as Course Signals helps to promote integration  in several ways. First, it allows faculty members to send  personalized emails to students that contain information about  their current performance in a given course. Second, faculty  members can encourage students to visit various help resources  on campus or office hours  activities that contribute to a  student becoming more fully integrated into the institution.  Third, it employs learner analytics to allow for the integration of  real-time data on student performance and interaction with the  LMS with demographic and past academic history information.  This combination creates an intentionally created environment  for the students that does not leave learning to chance,  something Tinto noted was necessary to ensure that a solution  would be broadly effective in helping students persist to  graduation [7]. The remainder of this paper will describe Course  Signals in detail, including its development and outcomes  realized as a result of its implementation. In addition, faculty  and student perceptions will be shared.    2. COURSE SIGNALS OVERVIEW  2.1 Description of Course Signals  Course Signals (CS) is a student success system that allows  faculty to provide meaningful feedback to student based on  predictive models.  The premise behind CS is fairly simple:  utilize the wealth of data found at an educational institution,  including the data collected by instructional tools, to determine  in real time which students might be at risk, partially indicated  by their effort within a course. Through analytics, large data sets  are mined and statistical techniques are applied to predict which  students might be falling behind. The goal is to produce  actionable intelligence in this case, guiding students to   appropriate help resources and explaining how to use them. [1]   A predictive student success algorithm (SSA) is run on-demand  by instructors. CS works by mining data from multiple  university sources and subsequently transforming the data into a  generated risk level with supporting information for each  student [1]. The algorithm that predicts students risk statuses  has four components: performance, measured by percentage of  points earned in course to date; effort, as defined by interaction  with Blackboard Vista, Purdues LMS, as compared to students  peers; prior academic history, including academic preparation,  high school GPA, and standardized test scores; and, student     Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK12, 29 April  2 May 2012, Vancouver, BC, Canada.  Copyright 2012 ACM 978-1-4503-1111-3/12/04$10.00.      267    characteristics, such as residency, age, or credits attempted.   Each component is weighted and pulled into the proprietary  algorithm, which then calculates a result for each student. Based  on results of the SSA, a red, yellow or green signal is displayed  on a students course homepage. A red light indicates a high  likelihood of being unsuccessful; yellow indicates a potential  problem of succeeding; and a green signal demonstrates a high  likelihood of succeeding in the course. Instructors then  implement an intervention schedule they create, possibly  consisting of:    Posting of a traffic signal indicator on a students  LMS home page;    E-mail messages or reminders;    Text messages;    Referral to academic advisor or academic resource  centers; or,    Face to face meetings with the instructor. [1]     With Course Signals, students are not placed at risk due to one  single factor; risk is determined by a contextualized landscape  that varies from student to student based on the data comprising  the four components of the SSA. The SSA transforms both static  and dynamic data points into a single score, improving the  reliability of the prediction. Since a course-specific risk  indicator is created for each student based on performance, peer- based behavior, and educational preparation data, instructors can  intervene early and give students a realistic opportunity to adapt  their behavior to be more specific in a given course.   2.2 History of Course Signals  Facing challenges of under prepared students, budget crises,  decreasing retention and longer graduation periods, higher  education is working to provide solutions to these challenges  while at the same time balancing the demands of providing  exceptional student service to foster student success. In an  attempt to ease these mounting pressures, CS was developed to  help identify students potentially at risk of not reaching their  full potential in a course. Once identified, instructors have the  ability to deliver meaningful interventions suggesting behaviors  a student may wish to change in order to improve her chances of  success. [1]   In 2007, Purdue University piloted Course Signals. According  to Pistilli and Arnold, the system was built from the ground up  using empirical data at every stage to ensure the most predictive  student success algorithm [5]. Course Signals became  automated in spring 2009 and partnered with SunGard Higher  education in October 2010 in order to help other institutions  harness the power of learning analytics. Today, nearly 24,000  students have been impacted by the CS project, and more than  145 instructors have used CS in at least one of their courses.   3. ACADMIC PERFORMANCE AND  RETENTION OUTCOMES   3.1 Impact on Academic Performance  Undeniably, one performance measure of student success is  final course grade. Research indicates that courses that  implement CS realize a strong increase in satisfactory grades,  and a decrease in unsatisfactory grades and withdrawals.  Individual courses see variable success with: an increase in As  and Bs ranging from 2.23 to 13.84 percentage points; a decrease  in Cs ranging from 1.84  to 9.38 percentage points; and a   decrease in Ds and Fs ranging from 0.59  to 9.40  percentage.   Combining the results of all courses using CS in a given  semester, there is a 10.37 percentage point increase in As and Bs  awarded between CS users and previous semesters of the same  courses not using CS. Along the same lines, there is a 6.41  percentage point decrease in Ds, Fs, and withdrawals awarded to  CS users as compared to previous semesters of the same courses  not using CS.   3.2 Impact on Student Retention  With increased student success in individual courses comes an  expected increase in retention to the University as well, and the  data indicate this nicely. Course Signals has been employed at  Purdue since 2007, and its use for each beginning cohort at the  institution is described in detail below.   3.2.1 Methodology   The fall 2007, 2008, and 2009 beginner cohorts were compared  to a master list of all Course Signals participants to determine  who from those entering cohorts took courses utilizing Course  Signals or not, and, if applicable, the number of times students  took courses with Course Signals. From there, students were put  through the retention module in the Universitys data reporting  system. They were analyzed based on the number of times they  had a course with CS  a number ranging from zero to five.   The beginner cohort for each year consists of the students who  are both first-time in college and carrying full-time credit loads.  The students who comprise each cohort is determined the  second week of each fall semester, and this data set is frozen;  once created, students do not leave the cohort for any reason.  Each semester, every student in each cohort has some form of  retention or leaving behavior  from simply remaining enrolled,  to graduating, to being academically dropped or withdrawing  voluntarily. The retention rate is calculated by adding those who  are still enrolled and who have graduated and dividing that sum  by the total number of first-time full-time students in the  original cohort.    3.2.2 Results  As indicated below, the students who began at Purdue in fall  2007 (Table 1), 2008 (Table 2), or 2009 (Table 3) and  participated in at least one Course Signals course are retained at  rates significantly higher than their peers who had no Course  Signals classes but who started at Purdue during the same  semester. Further, students who have two or more courses with  CS are consistently retained at rates higher than those who had  only one or no courses with Signals.  The analysis detailed in  Tables 1, 2, and 3 does not account for when a student had a  course with CS, only that at some point during their academic  career they did.  Tables 4, 5, and 6 examine that aspect.   For the 2007, 2008 and 2009 cohorts, instances of CS use across  successive semesters was compared to students retention  behavior for the following semester. This analysis asked if,  within a set of semesters, if a student had at least one course  with Course Signals. So, for example, for the 2007 cohort, the  first row looks at whether or not a student had CS in a course  during either the Fall 2007 or Spring 2008 semester, then  determines if they were retained into the Fall 2008 semester.  The comparison is against students who did not have a course  with CS during the same time period.  In short, there is a  noticeable impact on students having a course with CS early in   268    their academic career; basically, the earlier a student encounters  CS the better. Combined with the first analysis, the earlier and  the more occurrences, the greater the likelihood students will be  retained.   Table 1. Retention Rate for the 2007 Entering Cohort   Number    of CS   Courses   Cohort   Size   Year of Retention   1 Year  2 Year  3 Year  4 Year    No CS 5,134  83.44% 73.14% 70.47% 69.40%   At least 1 1,518  96.71% 94.73% 90.65% 87.42%   1 instance 1,311  96.57% 94.13% 89.70% 86.50%   2 or more 207  97.58% 98.55% 96.62% 93.24%      Table 2. Retention Rate for the 2008 Entering Cohort    Number   of CS   Courses   Cohort   Size   Year of Retention   1 Year  2 Year  3 Year    No CS     4,221  81.69% 75.08% 73.21%   At least 1     2,690  96.25% 89.55% 85.17%   1 instance  2,125  95.62% 88.00% 83.58%   2 or more       565  98.58% 95.40% 91.15%      Table 3. Retention Rate for the 2009 Entering Cohort   Number of   CS   Courses   Cohort   Size   Year of Retention   1 Year  2 Year    No CS          3,164  87.67% 81.89%   At least 1          2,962  90.34% 83.22%   1 instance          2,296  87.72% 80.87%   2 or more             666  99.40% 91.44%      Table 4. Analysis of Retention by Semester of Course Signals Use   for the 2007 Entering Cohort   Comparison to Students without CS in   Same Time Period    2 value P-value   CS in First Two Terms Retained to Third 18.57 1.64E-05   CS in First Three Terms Retained to Fourth 35.10 3.13E-09   CS in First Four Terms Retained to Fifth 131.95 < 2.2e-16   CS in First Five Terms Retained to Sixth 1073.18 < 2.2e-16   CS in First Six Terms Retained to Seventh 2.32 0.1278*   CS in First Seven Terms Retained to Eighth 725.57 < 2.2e-16   * Not significant        Table 5. Analysis of Retention by Semester of Course Signals Use   for the 2008 Entering Cohort   Comparison to Students without CS in   Same Time Period    2 value P-value   CS in First Two Terms Retained to Third 1.23 0.267*   CS in First Three Terms Retained to Fourth 2234.7 < 2.2e-16   CS in First Four Terms Retained to Fifth 131.95 0.5348*   CS in First Five Terms Retained to Sixth 1611.42 < 2.2e-16   * Not significant      Table 6. Analysis of Retention by Semester of Course Signals Use   for the 2009 Entering Cohort   Comparison to Students without CS in   Same Time Period    2 value P-value   CS in First Two Terms Retained to Third 309.67 < 2.2e-16   CS in First Three Terms Retained to Fourth 362.31 < 2.2e-16   In addition to the previous analyses, it should be noted that in  every case for the students from the 2007, 2008, and 2009  cohorts, students in courses with CS have a lower average   standardized test scores than those in non-CS courses. While  this aspect needs to be further investigated, early indications  show that lesser-prepared students, with the addition of CS to  difficult courses, are faring better with academic success and  retention to Purdue than their better-prepared peers in courses  not utilizing Course Signals.   4. FEEDBACK FROM STUDENTS AND  INSTRUCTORS  While the quantitative data provide evidence that there is an  impact on students grades and retention behavior, there exist  additional data that support the use of CS.  The instructors who  have employed CS, as well as students who have benefited from  the system, have provided information via surveys, focus  groups, and interviews that continue to warrant the usage of the  system.   4.1 Student Perception and Feedback  One of the major objectives of academic analytics is to identify  underperforming students and intervene early enough to allow  them the opportunity to change their behavior. For this reason  the Course Signals development team has closely tracked the  student experience with Signals since the pilot stage. At the end  of each semester, a user survey gathers anonymous feedback  from students, with more than 1,500 students surveyed across  five semesters. In addition, several focus groups have been held.   Students report positive experiences with Course Signals overall  (89% of respondents stated CS provided a positive experience  and 58% said they would like to use CS in every course). Most  students perceive the computer-generated e-mails and warnings  as personal communication between themselves and their  instructor. The e-mails seem to minimize their feelings of  being just a number, which is particularly common among  first-semester students. Students also find the visual indicator of  the traffic signal, combined with instructor communication, to  be informative (they learn where to go to get help) and  motivating (74% said their motivation was positively affected  by CS) in changing their behavior.    Of the roughly 1,500 student responses, only two wrote of  becoming demoralized by the constant barrage of negative  messages from their instructor. While this perception should not  be downplayed, negative feedback from instructors, especially  for students who might not be prepared for the rigors of higher  education, can be difficult to receive. Aside from these two  instances, however, the remainder of the negative feedback  concerned faculty use of the tool. For example, many students  spoke of over penetration (e-mails, text messages, and LMS  messages all delivering the same message), stale traffic signals  on their home pages (an intervention was run but not updated,  giving a false impression of a students status), and a desire for  even more specific information. This information  notwithstanding, the overwhelming response from the students  is that Course Signals is a helpful and important tool that aids in  their overall academic success at Purdue. Faculty believe this as  well, as indicated in the following section.   4.2 Faculty Perception and Feedback  While the student success algorithm predicts which students  might be in jeopardy of not doing as well as they could in a  course, it is the faculty and instructors who use the information   269    provided by Course Signals to intervene. It could certainly be  argued that these instructors, armed with the data provided by  learner analytics, are the most important weapons against  student under performance in the classroom. To wit, one  instructor asserted that  I want my students to perform well,  and knowing which ones need help, and where they need help,  benefits me as a teacher.   Faculty have easy access to CS data via the faculty dashboard.  Using learner analytics, faculty can provide action-oriented and  helpful feedback much earlier in the semester, which students  appreciate. This particularly benefits students early in their  academic careers, as they often are not fully aware of the  behaviors they must exhibit or actions they must take in order to  be successful.    Faculty also say that students tend to be more proactive as a  result of the Course Signals interventions. While students still  tended to procrastinate, they began thinking of big projects and  assignments earlier. Instructors and TAs also noticed that  students posted more questions about assignment requirements  well before the due dates. Because of the ability of academic  analytics to assess risk early and in real time, the instructors  consistently indicate that students are benefiting from knowing  how they are really doing in a course and, moreover, understand  the importance of completing assignments, and performing well  on quizzes and tests.   In general, faculty and instructors have a positive response to  CS but many approached the system with caution. Before using  Course Signals, faculty initially expressed concern about floods  of students seeking help; however, few actually reported  problems after they began using the system. The most  commonly reported issue being an excess of e-mails from  concerned students. In addition, faculty reported concerns about  creating a dependency in newly arrived students instead of the  desired independent learning traits. A final faculty concern was  the lack of best practices for using CS, demonstrating that  instructors and students share the same concern about the lack  of best practices.   There is little that can be done to mitigate the first concern,  since one of the goals of CS is to have students take a more  active role in their success.  The second concern is mitigated by  the strong retention results discussed in this paper. The final  issue was addressed by creating and posting best practice tips at  http://www.itap.purdue.edu/learning/tools/signals.    5. CONCLUSION  The use of learner analytics through the application of Course  Signals to difficult courses has shown great promise with regard  to the success of first and second year students, as well as their  overall retention to the University. To date, over 23,000 students  across 100 courses have been impacted by Course Signals and  over 140 instructors have utilized the system. Plans call for the  expansion of CS to include as many as 20,000 students a   semester within the next 18 months, and the upper  administration the institution is in strong support of this goal.     While this analysis is not without its limitations or areas for  continued improvement of the algorithm behind CS or the use of  CS, the outcomes are such that continued use of Course Signals  as a means of helping instructors provide detailed feedback to  their students, and to ultimately assist students in their academic  endeavors is highly warranted.   6. ACKNOWLEDGMENTS  Our thanks to Kyungmin Mike Ahn for his work analyzing  the data associated with the retention numbers.   7. REFERENCES  [1]. Arnold, K. E. 2010. Signals: Applying academic analytics.   EDUCAUSE Quarterly, 33, 1.    www.educause.edu/library/EQM10110    [2]. Barefoot, B. O., Gardner, J. N., Cutright, M., Morris, L. V.,  Schroeder, C. C., Schwartz, S. W., Siegel, M. J., Swing, R.  L. 2005. Achieving and sustaining institutional excellence  for the first year of college. Jossey-Bass, San Francisco,  CA.   [3]. Kuh, G. D., Kinzie, J., Schuh, J. H., Whitt, E. J., and  Associates. 2005. Student success in college: Creating  conditions that matter. Jossey-Bass, San Francisco.   [4]. Noel, L., Levitz, R., Saluri, D., and Associates. 1985.  Increasing student retention: Effective programs and   practices for reducing the dropout rate. Jossey-Bass, San  Francisco.   [5]. Pistilli, M.D., Arnold, K. E. 2010. Purdue Signals: Mining  real-time academic data to enhance student success. About   campus: Enriching the student learning experience, 15, 3,  22-24.   [6]. Tinto, V. 1993. Leaving college: Rethinking the causes and  cures of student attrition (2nd Ed.). University of Chicago  Press, Chicago.   [7]. Tinto, V. 2005. College student retention: Formula for  success. In:  A. Seidman, (ed.): College student retention:  Formula for student success, American Council on  Education and Praeger, Westport, CT.   [8]. Upcraft, M. L., Gardner, J. N., and Associates. 1989. The  freshman-year experience: Helping students survive and   succeed in college. Jossey-Bass, San Francisco.   [9]. Upcraft, M. L., Gardner, J. N., Barefoot, B. O., and  Associates. 2004. Challenging and supporting the first- year student: A handbook for improving the first year of   college. Jossey-Bass, San Francisco.      270      "}
