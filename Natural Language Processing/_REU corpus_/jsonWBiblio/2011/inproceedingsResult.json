{"index":{"_id":"1"}}
{"datatype":"inproceedings","key":"Ochoa:2011:LML:2090116.2090117","author":"Ochoa, Xavier","title":"Learnometrics: Metrics for Learning Objects","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"1--8","numpages":"8","url":"http://doi.acm.org/10.1145/2090116.2090117","doi":"10.1145/2090116.2090117","acmid":"2090117","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, learning object, learnometrics, metrics, repositories, reuse","Abstract":"The field of Technology Enhanced Learning (TEL) in general, has the potential to solve one of the most important challenges of our time: enable everyone to learn anything, anytime, anywhere. However, if we look back at more than 50 years of research in TEL, it is not clear where we are in terms of reaching our goal and whether we are, indeed, moving forward. The pace at which technology and new ideas evolve have created a rapid, even exponential, rate of change. This rapid change, together with the natural difficulty to measure the impact of technology in something as complex as learning, has lead to a field with abundance of new, good ideas and scarcity of evaluation studies. This lack of evaluation has resulted into the duplication of efforts and a sense of no ground truth or basic theory' of TEL. This article is an attempt to stop, look back and measure, if not the impact, at least the status of a small fraction of TEL, Learning Object Technologies, in the real world. The measured apparent inexistence of the reuse paradox, the two phase linear growth of repositories or the ineffective metadata quality assessment of humans are clear reminders that even bright theoretical discussions do not compensate the lack of experimentation and measurement. Both theoretical and empirical studies should go hand in hand in order to advance the status of the field. This article is an invitation to other researchers in the field to apply Informetric techniques to measure, understand and apply in their tools the vast amount of information generated by the usage of Technology Enhanced Learning systems.This paper will present the general goal of and inspiration for our work on learning analytics, that relies on attention metadata for visualization and recommendation. Through information visualization techniques, we can provide a dashboard for learners and teachers, so that they no longer need to drive blind. Moreover, recommendation can help to deal with the paradox of choice and turn abundance from a problem into an asset for learning.","pdf":"Learnometrics: Metrics for Learning Objects  Xavier Ochoa Escuela Superior Politcnica del Litoral  Via Perimetral Km. 30.5 Guayaquil, Ecuador  xavier@cti.espol.edu.ec  ABSTRACT The field of Technology Enhanced Learning (TEL) in gen- eral, has the potential to solve one of the most important challenges of our time: enable everyone to learn anything, anytime, anywhere. However, if we look back at more than 50 years of research in TEL, it is not clear where we are in terms of reaching our goal and whether we are, indeed, moving forward. The pace at which technology and new ideas evolve have created a rapid, even exponential, rate of change. This rapid change, together with the natural dif- ficulty to measure the impact of technology in something as complex as learning, has lead to a field with abundance of new, good ideas and scarcity of evaluation studies. This lack of evaluation has resulted into the duplication of efforts and a sense of no ground truth or basic theory of TEL. This article is an attempt to stop, look back and measure, if not the impact, at least the status of a small fraction of TEL, Learning Object Technologies, in the real world. The measured apparent inexistence of the reuse paradox, the two phase linear growth of repositories or the ineffective meta- data quality assessment of humans are clear reminders that even bright theoretical discussions do not compensate the lack of experimentation and measurement. Both theoreti- cal and empirical studies should go hand in hand in order to advance the status of the field. This article is an invi- tation to other researchers in the field to apply Informetric techniques to measure, understand and apply in their tools the vast amount of information generated by the usage of Technology Enhanced Learning systems.  Categories and Subject Descriptors K.3 [Computers and Education]: General  General Terms Learnometrics, Learning Analytics, Metrics  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK11 February 27-March 1, 2011, Banff, AB, Canada. Copyright 2011 ACM 978-1-4503-1057-4/11/02 ...$10.00.  Keywords Metrics, Learning Object, Repositories, Reuse  1. INTRODUCTION This dissertation presents the measurement of several char-  acteristics related to a learning object and the different pro- cesses that take place during its life cycle. We have called this initiativeMetrics for Learning ObjectsorLearnomet- rics for short. This denomination was chosen to reflect the similarity of this study goal and methodology with the In- formetric fields. For example, Bibliometrics, which is the scientific field focused on the measurement and analysis of texts and information [5], Scientometrics, which measures the scientific process [17] and Webometrics, that analyzes the behavior of the World Wide Web and the Internet [1].  Informetrics is focused on measuring and understanding processes that create, publish, consume or adapt informa- tion. Moreover, it is common that after the process has been analyzed, useful metrics are developed to summarize characteristics of the process and then used to create tools that can have a practical application to improve the studied or a related process. Scientometrics, for example, has stud- ied the scientific publication and citation processes. Exten- sive publication and citation data has been quantitatively analyzed. From these analyzes, it has been found that the number of publications per author and the number of cita- tions per journal follows the Lotka law [21]. Based on these findings, several scientists have suggested models that ex- plain the publication and citation process. Success breeds success [11] and Cumulative advantage [32] are two ways to express that the probability to publish a new scientific ar- ticle or receive a new citation is proportional to how many articles the author has published before or how many ci- tations the journal has already received. Practical metrics that have been extracted from these analyses are the Jour- nal Impact Factor [14] and the h-index [16]. Those metrics serve to summarize the scientific impact that a journal or a scientist has in a particular field. Moreover, these metrics, while not perfect [18], are often used as selection criteria in other scientific processes, such as when selecting a journal to publish research or selecting the most talented scientist to fulfill an academic position.  This Learnometric study follows a similar pattern. It quantitatively analyzes data from processes that take place during different points of the Learning Object life cycle. Based on that analysis, initial models are proposed to ex- plain the observed results. The study also proposes small calculations (metrics) that can covert the data available about  1    the learning objects into information that can be used to improve the effectiveness or usefulness of existing Learning Object end-user tools.  2. UNDERSTANDING THE PUBLICATION OF LEARNING OBJECTS  The first step to understand the Learning Object Econ- omy is to measure and understand how learning objects are offered or published. A literature review on the topic is very discouraging. The only serious work that tries to quanti- tatively measure the publication of learning objects is [22], which is, however, very superficial on its quantitative side and draws no conclusions from the results. This lack of re- search leads to an almost unexplored field with even the most basic questions unanswered.  This section will quantitatively analyze and compare dif- ferent types of publication venues for learning objects . These types include Learning Object Repositories (LORP), Learn- ing Object Referatories (LORF), Open Courseware Initia- tives (OCW), Learning Management Systems (LMS). To provide some type of comparison and because their con- tent can also be used for educational purposes, Institutional Repositories (IR) are also included in the study. For simplic- ity, we will refer to all these systems as repositories. The following subsections provide the analysis of data collected from this repositores to provide answer to several basic ques- tions.  2.1 What is the typical size of a repository This analysis measured the size distribution of different  types of repositories. This study included data from 24 LORPs, 15 LORFs, 34 OCWs, 2500 LMSs and 772 IRs. More details about the repository selection and data collec- tion could be found in [31]. Figure 1 shows the range of the obtained sizes (y-axis in log scale) for each type of repository. In general, individual learning object repositories seems to vary from hundreds to million of objects. Their average size depends of the type of repository. LORPs can be consid- ered to have few thousand of objects. LORFs are in the order of the tens of thousands. However, those numbers are small compared with multi-institutional IRs that can count hundreds of thousands and even millions of objects. OCWs and LMSs can have from hundreds to thousand of courses with a total of thousands or ten of thousands of individual resoures. However, the answer to this question is not that simple. The size is not Normally distributed, meaning that the average value cannot be used to gain understanding of the whole population. It is not strange to find repositories several orders of magnitude bigger or smaller than the aver- age. The distribution of learning objects among repositories seems to follow a Lotka or Power Law distribution with an exponent of 1.75. The main implication of this finding is that most of the content is stored in few big repositories, with a long, but not significant tail. Administrators of a big repository would want to federate [34]] their searches with other big repositories in order to gain access to a big propor- tion of the available content. On the other hand, it makes more sense for small repositories to publish their metadata [37] for a big repository to harvest it in exchange for the access to their federated search. It seems, through an initial reading of this finding, that a two (or three) tiered approach mixing federation and metadata harvesting is the most effi-  Figure 1: Size range of different type of repositories  cient way to make most of the content available to the wider audience possible using the current infrastructure.  Another important implications of thes findings is that if OCWs and LMSs are decomposed and converted into repos- itories, they can be considered very large LORPs. The fact that LMSs are a widely deploy technology [15] and that these systems are not accessible for external visitors make us think of the learning objects present in LORs as just the tip of the iceberg. The bigger part of learning resources is hid- den behind login pages. This finding validates the effort of the OCW Consortium and OER Commons [19]. If we want to create a really functioning Learning Object Economy, we must start opening the door of our LMSs.  2.2 How do repositories grow over time To measure the growth in the number of objects, 15 repos-  itories of different type were studied. They were selected based on how representative they are for their respective type in terms of size and period of existence. The first vari- able analyzed was the average growth rate (AGR), measured in objects inserted per day. It is interesting to compare the AGR of different types of repositories. LORPs, for example, grow with a rate of 1 or 2 objects per day. LORFs goes from 4 to 20 objects per day. OCWs and LMSs grow faster than LORPs, with an unexpectedly high value of circa 1 course published per day (in average 20 objects). IRs depending on their size could go from few objects to hundreds of ob- jects per day, depending on their size. The actual growth function for most respositories is linear (bi-phase linear).  This is a discouraging finding. Even popular and currently active repositories grow linearly. Even if we add them all to- gether, we will still have a faster linear, but no exponential. The main reason for this behavior is the contributor deser- tion. Even if the repository is able to attract contributors, it is not able to retain them long enough. The value propo- sition, that is the way how the contributor benefits from contributing to the repository, is still an unsolved issue in most repositories.  One anomaly in this study was the LORP Connexions. It grows at what seemed to be an exponential rate. Figure 2 shows this difference. Further investigagtion in [27] revealed that the social features present in Connexions that enable the formation of communities around the materials are the reason behind Connexions sucess. Based on these results,  2    it seems that the use of social engagement tools should be part of any new repository design.  2.3 How many learning objects does a con- tributor publish on average  To understand contributor behavior, full publication data from three LORPs (Ariadne, Connexions and Maricopa), one LORF (Merlot), one OCW site (MIT OCW), one LMS (SIDWeb) and three IRs (Queensland, MIT and Georgia Tech) was obtained. Each learning object was assigned ac- cording to the data to one contributor. If more than one contributor was listed, we counted the first author only. From the result of the distribution fitting, it is clear that the number of objects published per each contributor varies according to the type of repository. All LORPs and LORFs follow a Lotka distribution with exponential cut-off. Even high producing individual start loosing interest after pub- lishing many objects. Maybe one of the reasons behind this distribution is the lack of some type of incentive mechanism [6]. OCW MIT and SIDWeb present a Weibull distribu- tion. The finding of a weibull distribution means that for OCWs and LMSs there is an increased probability to pro- duce a certain amount of objects. This can be seen as the strong concavity in the curve compared with the flat Lotka. The mechanism behind this distribution is that there is an interest to produce courses with a given amount of learn- ing objects (maybe 1 object per session). The tail of the IRs are fitted by the pure Lotka distribution. The head of the distribution, users that have published 1 or 2 objects, have a disproportionately high value that cannot be fit by any of the tried distributions. This result suggests that the publication of documents in IRs have a different mechanism than the publication of learning objects in LORs, and maybe what we are measuring in the IRs tail is a by-product of the scientific publication process. These distributions could be seen in Figure 3.  Based on the finding of these heavy tailed distribution, it can be concluded that there is not such thing as an aver- age user [28]. The best way to describe the production of different contributors is to cluster them in classes similar to socioeconomic strata. If we adopt this approach we gain a new way to look at our results. In LORP and LORF, the repository is dominated by the higher-class. Most of the material is created by a few hyper-productive contribu- tors. the 10% of the users could easily have produced more than half of the content of the repository. In the case of OCWs and LMS, the Weibull distribution determines that the middle-class is the real motor of the repository. The low- and high-class are comparatively small. Finally, Uni- versity IRs, with Lotka with high alpha are dominated by the lower-class as more than 98% of the population produces just one object.  From a deper analysis on publishing rate and lifetime [31], it can be concluded that these different distributions are not caused by an inherent difference in the talent or capacity among the different communities, but by the difference in contributor engagement with the repository. It seems that the distribution of lifetime, the time that the contributor remains active, is different for this three observed reposi- tory types. In LORP and LORF, there is some time of novelty engagement that keep the contributor active at the beginning, but the chances of ceasing publication increases as more time is spent in the repository. For OCWs and  Figure 2: Comparation of Content Growth Function for Connexions, Ariadne and Merlot  3    Figure 3: Distribution of Contribution for the dif- ferent type of repositories  LMSs, there is a goal-oriented engagement that keeps the contributor productive until her task is finished (course is fully published). In the case of IRs, there is no engagement at all. The norm is just discrete contributions. Changes on the type of engagement should have an effect not only in the distribution of publications among users, but also in the growth and size of the repository.  3. UNDERSTANDING THE REUSE OF LEARN- ING OBJECTS  Although reuse is the reason why much of Learning Ob- ject Technologies exist, little is quantitatively known about the Reuse process. Beside small scale experiment in artifi- cial settings [33] [12] [38], there is practically no empirical data on how different factors affect the reusability of learn- ing objects. Again, with an almost unexplored field, this article proposes and aims to solve the some basic questions.  3.1 What percentage of learning objects is reused To perform a quantitative analysis of the reuse of learning  objects, this study uses empirical data collected from three different openly available sources. The sources were cho- sen to represent different reuse contexts and different object granularity.  Small Granularity: Slide Presentation Components. A group of 825 slide presentations obtained from the ARI- ADNE repository [10] were decomposed and checked for reuse using the ALOCOM framework [39]. From the de- composition of the slides 47,377 unique components were obtained. A component is considered reused if it is present in more than one slide.  Medium Granularity: Learning Modules. The 5255 learn- ing objects available at Connexions [3] at the time of data collection were downloaded. Some of these objects belong to collections, a grouping of a similar granularity as a course. 317 collections are available at Connexions. A module is considered reused if it is used in more than one collection.  Large Granularity: Courses. The 19 engineering curric- ula offered by ESPOL, a technical University at Ecuador, reuse basic and intermediate courses. When a new cur- riculum is created, existing courses, such as Calculus and Physics, are reused. On the other hand, more advanced courses, for example Power Lines in the case of Power En- gineering, are created and only used in the specific curricu- lum. Based on the published information, the 463 different courses were obtained. A course is considered reused if it is mandatory in more than one curriculum.  The results of the quantitative analysis (Table 1) seems to indicate that in common settings, the amount of learn- ing objects reused is around 20%. While relatively low, this result is very encouraging for Learning Object supporters. It indicates that even without support or the proper facili- ties, users do reuse a significant amount of learning materi- als. The multiplicative model also implicates that improving even one of the steps in the reuse chain, the others remaining equal, would improve the probability of reuse and, therefore, the amount of objects being reused. As mentioned above, Verbert and Duval, in [38], empirically found that facilitat- ing one of the steps, in this particular finding slide compo- nents, leads to a significant increase in the amount of reuse.  3.2 Does the granularity of a learning object affect its probability of reuse  4    Table 1: Percentage of reuse  Data Set Objects % Reuse  Small Granularity  Components in Slides (ALOCOM) 47,377 11.5%  Images (Wikipedia) 1,237,105 24.6%  Medium Granularity  Modules in Courses (Connexions) 5,255 22.6%  Soft. Libraries (Freshmeat) 2,643 20.4%  Large Granularity  Courses in Curricula (ESPOL) 463 19.9%  Web APIs (P.Web) 670 32.2%  The theory of Learning Objects affirms that higher granu- larity leads to lower reusability [40] . Results from the previ- ous study, however contradict this affirmation. The percent- age of object reuse was similar regardless of the granularity of the object. Courses were even reused more often than slide components. Merging the theory with the empirical finding leads to a new interpretation of the role of granular- ity in the reuse of learning objects. This new interpretation involves also the granularity of the context of reuse as the determining factor. Objects that have a granularity imme- diately lower than the object being built are easier to reuse than objects with a much lower or higher granularity. For example, when building a course, it is easier to reuse whole lessons than reusing complete courses or individual images. Also, when building a curriculum, it is easier to reuse com- plete courses than to reuse another complete curriculum or individual lessons. Empirical support for this new interpre- tation can be found in [38]. It was found that when building a slide presentation, the most reused component was by far individual slides. The reuse of text fragments and individual images represent just the 26% of the total reuse.  3.3 Is there a relation between the popularity of an object and its reuse  The objective of this analysis is to establish if the actual reuse of a learning object is linked to its relative popularity within the collection or repository. To perform this analysis, the Connexions and Freshmeat data sets were enriched with information about the number of times that the objects have been accessed. The popularity data was obtained from Web scraping These data sets were selected for this analysis be- cause they were the only ones with access information and have similar granularity.  The analysis consisted in obtaining the Kendalls tau cor- relation coefficient between the rank of the object in the reuse and popularity scales. Pearsons coefficient is not used because there is no guaranty that the values come from a bi-variate normal distribution. Also, scatter plots were cre- ated to visually analyze the relation between popularity and reuse. Figure 4 presents the data for Connexions. The corre- lation coefficient tau for the Connexions set was -0.02 (0.05 significant). This value means that there is absolutely no correlation between the popularity of the object and the times that it has been reused. For example, the most vis- ited object has only be reused in three collections, while the  Figure 4: Scatter plots of the Reuse vs. Popularity in Connexions  most reused object (8 times) has only received 25 visits. On the other hand, the Freshmeat set obtained a tau of 0.33 (0.01 significant). This result suggests that in the case of software libraries the popularity is slightly linked with the reuse. However, there are cases that have a large popular- ity but have a low track of reuse. For example, the DeCSS library [13], normally used to break DVD encryption, has a large popularity (circa 180.000 visits) but is only used in a small set of specialized DVD players for Linux (8 projects). These results suggest that the popularity of an object can- not always be used as a proxy for its reuse. A more counter- intuitive finding that can be obtained from this result is that a high level of reuse does not imply a high popularity. It would be usually expected that an object reused in several contexts is more findable and, therefore, more visited. The measurement indicates that it is not the case.  3.4 What is the distribution of reuse among learning objects  To gain more insight in the reuse process, the distribution of reuse among different objects was analyzed. The first step in this analysis was to obtain the total number of reuses for each object. Several distributions were fitted to the data to obtain the best fit. For all the data sets, the Log-normal distribution provided the best fit. As a visual aid, Figure 5 presents the size-frequency plot of the data [26].  The main implication of the finding of a Log-normal dis- tribution is that the Long Tail effect [2] applies to reuse. Few objects are reused heavily while most of the reused ob- jects are reused just once. However, the volume of reuse in the tail is at least relatively as important as the volume of reuse in the head. According to this result, federating repositories in order to provide a wider selection of objects is a good strategy to foster reuse. Objects present in small repositories have a high probability of being reuse at least once if they are exposed to a wider universe of users.  4. METRICS FOR LEARNING OBJECTS The main use that we can give to the information ex-  tracted from the analysis of the data created at the different processes of the Learning Object Economy is the creation of  5    Figure 5: Size-Frequency graphs of the reuse in ARI- ADNE Slides and the best fitting Log-Normal dis- tribution (line)  metrics to improve the tools used in those processes. This article will discuss two specific examples of these metrcis: 1) to estimate the quality of the learning object metadata and 2) to establish the relevance of learning objects for a given user and situation.  4.1 Quality Control for the Labelling Process The quality of metadata on learning objects stored in a  LOR is an important issue for LOR operation [4] and inter- operability [20]. Due to its importance, metadata quality assurance has always been an integral part of resource cat- aloging [36]. Nonetheless, most LOR implementations have taken a relaxed approach to metadata quality assurance. As repositories grow and federate, quality issues become more apparent. The traditional solution for quality assurance, manually reviewing a statistically significant sample of meta- data against a predefined set of quality parameters, similar to sampling techniques used for quality assurance of library cataloguing [7], fails to scale to increasing amounts of learn- ing objects being indexed manually or automatically. Some sort of automatical quality assurance mechanism should be created to cope with this problem.  In [30], the author propose and evaluate a set of metrics to automatically measure the quality of the learning object metadata instances. The main conclusions of this work are that some metrics correlate well with human reviews, spe- cially the Textual Information Content (Qtinfo). Also the metrics, when combined serves as a low quality metadata filter. Figure 6 present an application to evaluate the meta- data quality of whole repositories based on the proposed metrics.  4.2 Relevance Ranking to Improve the Selec- tion Process  In the early stages of the Learning Object Economy, LORs where isolated and only contained a small number of learning  Figure 6: Visualization of the Textual Information Content of the ARIADNE Repository. Red (dark) boxes indicate authors that produce low quality de- scriptions.  objects [25]. The search facility usually provided users with an electronic form where they could select the values for their desired learning object. The search engine then compared the values entered in the query with the values stored in the metadata of all objects and returned those which complied with those criteria. While initially this approach seems ap- propriate to find relevant learning objects, experience shows that it presents several problems, such as high cognitive load [23], mismatch between indexers and searchers [24], and low recall [35]. Given these problems with the metadata based search, most repositories provided a Simple Search approach, based on the success of text based retrieval exem- plified by Web Search engines [8]. In this approach, users only need to express their information needs in the form of keywords or query terms. This approach seemed to solve the problems of metadata based search for small reposito- ries. However, working with small, isolated repositories also meant that an important percentage of users did not find what they were looking for because no relevant object was present in the repository [23]. If this technique is applied to large repositories, or to federated collections of repositories, the user is no longer able to review several pages of results in order to select the relevant objects. While doing a stricter filtering of results (increasing precision at expense of recall) could solve the oversupply problem, it could also lead again to the initial problem of scarcity. A proven solution for this problem is ranking or ordering the result list based on its relevance. In this way, it does not matter how long the list is, because the most relevant results will be at the top and the user can manually review them.  In a previous work [29], the author describes a set of rel- evance ranking metrics for learning objects. These metrics try to implement the theoretical LearnRank [9]. This work found that the information about the usage of the learning objects, as well as the context where this use took place, can  6    Figure 7: Architecture for Metrics Services  be converted into a set of automatically calculable metrics to establish the relevance of a learning objects for a given user in a given situation. The evaluation of the metrics show that these metrics outperformed the ranking based on pure text- based approach. Figure 7 presents an architecture discussed in [29] to implement these metrics in real systems.  5. NOT CONCLUSIONS BUT FURTHER RE- SEARCH  As a first exploration of Learnometrics, this article, and its cited studies, raises more questions than it answers. Ample opportunities for further research are provided as the field of Learnometrics unfolds. The following is a list of what the author consider are the most interesting and urgent research questions seeking for answers and explanations.   What is the measurable effect that opennes have in the Learning Object Economy   How to integrate LMSs into the Learning Object Econ- omy   How to reformulate the Paradox of Reuse to consider more varibles apart from granualarity   Establishing a common data set to experiment with metrics and their usefulness  Answering these questions through quantitative analyses will increase our understanding of how the Learning Objects Economy works. This understanding can help us to cre- ate the right environment for this economy to flourish and provide its predicted benefits. The main task left for further work is to execute large empirical studies with full implemen- tations of the metrics in real environments. Once there is enough data collected, the user interaction with the system and the progress of the different metrics could be analyzed to shed light on these questions. We also hope that other researchers start proposing improvements to these initial ap- proaches.  6. REFERENCES [1] T. C. Almind and P. Ingwersen. Informetric analyses  on the world wide web: methodological approaches to SwebometricsS. Journal of Documentation, 53(4):404426, 1997.  [2] C. Anderson. The long tail. Hyperion, 2006.  [3] R. G. Baraniuk. Opening Up Education: The Collective Advancement of Education through Open Technology, Open Content, and Open Knowledge, chapter Challenges and Opportunities for the Open Education Movement: A Connexions Case Study, pages 116132. MIT Press, 2007.  [4] J. Barton, S. Currier, and J. M. N. Hey. Building quality assurance into metadata creation: an analysis based on the learning objects and e-prints communities of practice. In S. Sutton, J. Greenberg, and J. Tennis, editors, Proceedings 2003 Dublin Core Conference: Supporting Communities of Discourse and Practice - Metadata Research and Applications, pages 3948, Seattle, Washington, 2003.  [5] R. Broadus. Toward a definition of SbibliometricsT. Scientometrics, 12(5):373379, 1987.  [6] L. Campbell. Reusing Online Resources: A Sustainable Approach to E-Learning, chapter Engaging with the learning object economy, pages 3545. Kogan Page Ltd, 2003.  [7] A. Chapman and O. Massey. A catalogue quality audit tool. Library Management, 23(6-7):314324, 2002.  [8] H. Chu and M. Rosenthal. Search engines for the world wide web: A comparative study and evaluation methodology. In S. Hardin, editor, Proceedings of the 59th Annual Meeting of the American Society for Information Science, volume 33, pages 127135, Baltimore, MD, 1996. Softbound.  [9] E. Duval. Policy and Innovation in Education - Quality Criteria, chapter LearnRank: the Real Quality Measure for Learning Materials, pages 457463. European Schoolnet, 2005.  [10] E. Duval, K. Warkentyne, F. Haenni, E. Forte, K. Cardinaels, B. Verhoeven, R. Van Durm, K. Hendrikx, M. Forte, N. Ebel, et al. The ariadne knowledge pool system. Communications of the ACM, 44(5):7278, 2001.  [11] L. Egghe and R. Rousseau. Generalized success-breeds-success principle leading to time-dependent informetric distributions. Journal of the American Society for Information Science, 46(6):426445, 1995.  [12] K. Elliott and K. Sweeney. Quantifying the reuse of learning objects. Australasian Journal of Educational Technology, 24(2):137142, 2008.  [13] K. Eschenfelder and A. Desai. Software as Protest: The Unexpected Resiliency of US-Based DeCSS Posting and Linking. The Information Society, 20(2):101116, 2004.  [14] E. Garfield. The impact factor. Current Contents, 25(20):37, 1994.  [15] C. Harrington, S. Gordon, and T. Schibik. Course management system utilization and implications for practice: A national survey of department chairpersons. Online Journal of Distance Learning Administration, 7(4):13, 2004.  7    [16] J. Hirsch. An index to quantify an individuals scientific research output. Proceedings of the National Academy of Sciences, 102(46):1656916572, 2005.  [17] W. Hood and C. Wilson. The literature of bibliometrics, scientometrics, and informetrics. Scientometrics, 52(2):291314, 2001.  [18] P. Jacso. A deficiency in the algorithm for calculating the impact factor of scholarly journals: The journal impact factor. Cortex, 37(4):590594, 2001.  [19] A. Joyce. OECD Study of OER: Forum Report. Technical report, UNESCO, 2007.  [20] X. Liu, K. Maly, M. Zubair, and M. L. Nelson. Arc - an oai service provider for digital library federation. D-Lib Magazine, 7(4):12, 2001.  [21] A. Lotka. The frequency distribution of scientific productivity. Journal of the Washington Academy of Sciences, 16(12):317323, 1926.  [22] R. McGreal. A typology of learning object repositories. [pre-print]. Retrieved December 19, 2007 from http://hdl.handle.net/2149/1078, 2007.  [23] J. Najjar, J. Klerkx, R. Vuorikari, and E. Duval. Finding appropriate learning objects: An empirical evaluation. In A. Rauber, S. Christodoulakis, and A. M. Tjoa, editors, Proceedings of : 9th European Conference on Research and Advanced Technology for Digital Libraries. ECDL 2005, volume 3652 of Lecture Notes in Computer Science, pages 323335, Vienna, Austria, 2005. Springer Verlag.  [24] J. Najjar, S. Ternier, and E. Duval. User behavior in learning objects repositories: An empirical analysis. In L. C. . C. McLoughlin, editor, Proceedings of the ED-MEDIA 2004 World Conference on Educational Multimedia, Hypermedia and Telecommunications, pages 43734378, Chesapeake, VA, 2004. AACE.  [25] F. Neven and E. Duval. Reusable learning objects: a survey of lom-based repositories. In M. Muhlhauser, K. Ross, and N. Dimitrova, editors, MULTIMEDIA 02: Proceedings of the tenth ACM international conference on Multimedia, pages 291294, New York, NY, 2002. ACM Press.  [26] M. Newman. Power laws, Pareto distributions and Zipfs law. Contemporary Physics, 46(5):323351, 2005.  [27] X. Ochoa. Connexions: a social and successful anomaly among learning object repositories. Journal of Emerging Technologies in Web Intelligence, 2(1):1122, 2010.  [28] X. Ochoa and E. Duval. Quantitative analysis of user-generated content on the web. In D. De Roure and W. Hall, editors, Proceedings of the First International Workshop on Understanding Web Evolution (WebEvolve2008), pages 1926, Beijing, China, 2008. Web Science Research Initiative. ISBN: 978 085432885 7.  [29] X. Ochoa and E. Duval. Relevance ranking metrics for learning objects. IEEE Transaction on Learning Technologies, 1(1):15, 2008. in Press.  [30] X. Ochoa and E. Duval. Automatic evaluation of metadata quality in digital libraries. International Journal of Digital Libraries, 10(2):6791, 2009.  [31] X. Ochoa and E. Duval. Quantitative analysis of learning object repositories. IEEE Transactions on  Learning Technologies, 2(3):226238, 2009.  [32] D. Price. A general theory of bibliometric and other cumulative advantage processes. Journal of the American Society for Information Science, 27(5-6):292306, 1976.  [33] V. Schoner, D. Buzza, K. Harrigan, and K. Strampel.  Learning objects in use:SliteSassessment for field studies. J. Online Learning Teaching, 1(1):18, 2005.  [34] B. Simon, D. Massart, F. van Assche, S. Ternier, E. Duval, S. Brantner, D. Olmedilla, and Z. Miklos. A simple query interface for interoperable learning repositories. In D. Olmedilla, N. Saito, and B. Simon, editors, Proceedings of the 1st Workshop on Interoperability of Web-based Educational Systems, pages 1118, Chiba, Japan, 2005. CEUR.  [35] L. Sokvitne. An evaluation of the effectiveness of current dublin core metadata for retrieval. In Proceedings of VALA (Libraries, Technology and the Future) Biennial Conference, page 15, Victoria, Australia, 2000. Victorian Association for Library Automation Inc.  [36] S. E. Thomas. Quality in bibliographic control. Library Trends, 44(3):491505, 1996.  [37] H. Van de Sompel, M. Nelson, C. Lagoze, and S. Warner. Resource Harvesting within the OAI-PMH Framework. D-Lib Magazine, 10(12):10829873, 2004.  [38] K. Verbert and E. Duval. Evaluating the ALOCOM Approach for Scalable Content Repurposing. In E. Duval, R. Klamma, and M. Wolpers, editors, Creating New Learning Experiences on a Global Scale: Proceedings of the Second European Conference on Technology Enhanced Learning, volume 4753, pages 364377, Crete, Greece, 2007. Springer.  [39] K. Verbert, E. Duval, M. Meire, J. Jovanovic, and D. Gasevic. Ontology-Based Learning Content Repurposing: The ALOCoM Framework. International Journal on E-Learning, 5(1):6774, 2006.  [40] D. Wiley, S. Waters, D. Dawson, B. Lambert, M. Barclay, D. Wade, and L. Nelson. Overcoming the limitations of learning objects. Journal of Educational Multimedia and Hypermedia, 13(4):507521, 2004.  8      "}
{"index":{"_id":"2"}}
{"datatype":"inproceedings","key":"Duval:2011:APL:2090116.2090118","author":"Duval, Erik","title":"Attention Please!: Learning Analytics for Visualization and Recommendation","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"9--17","numpages":"9","url":"http://doi.acm.org/10.1145/2090116.2090118","doi":"10.1145/2090116.2090118","acmid":"2090118","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, recommendation, visualization","Abstract":"Who we learn from, where and when is dramatically affected by the reach of the Internet. From learning for formal education to learning for pleasure, we look to the web early and often for our data and knowledge needs, but also for places and spaces where we can collaborate, contribute to, and create learning and knowledge communities. Based on the keynote presentation given at the first Learning Analytics and Knowledge Conference held in 2011 in Banff, Alberta, this paper explores a social network perspective on learning with reference to social network principles and studies by the author. The paper explores the ways a social network perspective can be used to examine learning, with attention to the structure and dynamics of online learning networks, and emerging configurations such as online crowds and communities.","pdf":"Attention Please! Learning Analytics for Visualization and Recommendation  Erik Duval Dept. Computer Science  Katholieke Universiteit Leuven Celestijnenlaan 200A  B3000 Leuven, Belgium erik.duval@cs.kuleuven.be  ABSTRACT This paper will present the general goal of and inspiration for our work on learning analytics, that relies on attention metadata for visualization and recommendation. Through information visualization techniques, we can provide a dash- board for learners and teachers, so that they no longer need to drive blind. Moreover, recommendation can help to deal with the paradox of choice and turn abundance from a problem into an asset for learning.  Categories and Subject Descriptors K.3 [Computers and Education]: Computer Uses in Ed- ucation  General Terms Design, Human Factors, Theory  Keywords Learning Analytics, Visualization, Recommendation  1. INTRODUCTION Attention is a core concern in learning: as learning re-  sources become available in more and more abundant ways, attention becomes the scarce factor, both on the side of learners as well as on the side of teachers. (This is a wider concern, as we evolve towards an attention economy [10].)  Learners and teachers leave many traces of their attention: some are immediately obvious to others, for instance in the form of posts and comments on blogs, or as twitter mes- sages. These explicit traces are human readable, but can be difficult to cope with in a world of abundance [29]. Although some refer to information overload, we prefer Shirkys fil- ter failure as a way to think about the problem of dealing with this abundance [30]. In any case, human attention traces are extremely valuable, but do not scale very well.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK11 February 27-March 1, 2011, Banff, AB, Canada. Copyright 2011 ACM 978-1-4503-1057-4/11/02 ...$10.00.  In this paper, we will explain how machine readable traces of attention can be used to filter and suggest, provide aware- ness and support social links.  This paper is structured as follows: section 2 provides a brief background on the field of analytics in general. The section thereafter focuses on applications from the world of jogging, as these provide a particularly rich source of inspi- ration for our work. The general concept of goal oriented visualizations is at the core of learning dashboard applica- tions: that is why it is the topic of section 4. In order to scale up the current work on learning analytics and achieve broad adoption, it is imperative to establish a global open infras- tructure, as we briefly explain in section 5. The two next sections briefly present the two approaches weve explored so far to leverage learning analytics: learning dashboard (sec- tion 6) and learning recommenders (section 7). Before con- cluding the paper, we briefly mention exciting opportunities that learning analytics provides for data based research on learning in section 8.  2. BACKGROUND ON ANALYTICS The new field of learning analytics is quite related to sim-  ilar evolutions in other domains, such as Big Data [1], e- science [14, 9, 26], web analytics [7], educational data min- ing [25]. All of these have in common that they rely on large collections of quite detailed data in order to detect patterns. This detection of patterns can be based on data mining tech- niques, so that for instance recommendations can be made for resources, activities, people, etc. that are likely to be relevant. Alternatively, the data can be processed so that they can be visualized in a way that enables the teacher or learner rather than the software to make sense of them.  In fact, the research in my team gets much of its inspira- tion from tools like wakoopa (http://social.wakoopa.com) or rescuetime (http://www.rescuetime.com), that install tracker tools on the machine of a user and then automati- cally record all activities (applications launched, documents accessed, web sites visited, music played, etc.) by that user.  A typical illustration of the visualizations that such tools provide is figure 1, where a simple overview is presented of the software applications I used last week and last month and how their usage is distributed over time. (Tuesday- Thursday were travel days...) In this way, such an applica- tion can help a user to be more aware of her activities.  Moreover, based on these tracking data, the wakoopa tool can compare the activity of a user with that of other users and recommend software or contacts - see figure 2. It doesnt  9  http://social.wakoopa.com http://www.rescuetime.com   Figure 1: My wakoopa dashboard  require much imagination to see how similar visualizations could be useful in the world of learning, for instance to chart learning activities, tools used or recommended, and peer learners or suitable teachers in ones social network.  It is important to note that the tracking occurs without any manual effort by the user - although it is of course im- portant that the user is aware that her activities are being tracked. Actually, such tools typically also make it possible to pause tracking. Some applications allow users to set goals (spend less than 1 hour per day on emailor play computer games for less than 1,5 hour per day or write more than 3 hours per day) and will notify them when they are in danger of not meeting their goal, when they get close to the self-imposed limit - or signal them that they did reach their goal. Moreover, they provide quite detailed visualizations of all the activities of a user, so that she can analyze where most of her on-line activity takes place and make better in- formed decisions on how to manage these activities.  A similar tool is tripit (http://www.tripit.com): note- worthy about this tool is that when a user forwards flight or hotel reservations to a tripit email address, all the struc- tured data is extracted and a calendar is created with all the relevant information. This is an excellent example of auto- matic metadata generation [6] or information extraction [5], an essential technology if we want to collect metadata of resources, activities and people at scale. Note also that, if other people from the users network are near, tripit will mention that - see figure 3.  Of course, more mainstream tools like google offer similar functionality, such as for instance google history that pro- vides an overview of every search that a user ever submitted (when logged in) or that indicates who from a users social circle tweeted about an item included in a search result, etc.  3. INSPIRATION FOR OUR WORK A particularly inspiring set of applications comes from  the domain of jogging, and sports in general, where appli- cations like nikeplus (http://nikerunning.nike.com/, see figure 4) or runkeeper (http://runkeeper.com/) provide de- tailed statistics on how fast, far, often, etc. one runs.  What is particularly relevant in a learning context is that many running applications also help runners to set goals (run a marathon in november), develop a plan to achieve that goal, find running routes in a foreign town, locate other runners with a similar profiles, challenge them so as to main- tain motivation, etc. Sometimes, such tools even take a more pro-active role and send messages to users to enquire why they have stopped uploading activities, whether they need to re-define goals and plans, or want to be connected to other users that can help, etc.  Although there are few studies that show whether these special purpose social networks actually change user behav- ior, [16] did find that users weight changes correlated pos- itively with the number of their friends and their friends weight-change performanceand users weight changes have rippling effects in the Online Social Networks due to the so- cial influence. The strength of such online influence and its propagation distance appear to be greater than those in the real-world social network.. An early overview of how the combination of tracking and social network services can lead to a more patient-driven approach to medicine is provided in [32].  One assumption underlying our work is that similar ap- plications can be built to track learner progress, to assist in developing and maintaining motivation, to help define re- alistic goals and develop plans to achieve them, as well as connect learners or teachers with other learning actors, etc. In that way, they can help to realize a more learner-driven  10  http://www.tripit.com http://nikerunning.nike.com/ http://runkeeper.com/   Figure 2: My Wakoopa recommendations  Figure 3: My tripit dahsboard  11    approach to education, training and learning in general.  4. GOAL ORIENTED VISUALIZATIONS Many of these inspiring applications take a visual ap-  proach. Yet, if visualizations are to have any effect beyond the initial wowfactor, it would be useful to have more clar- ity on what the intended goal is and how to assess whether that goal is achieved. Many visualizations look good - and some are actually beautiful. But how we can connect visu- alization not only with meaning or truth, but with taking actions This is very much a quantified self approach (see http://quantifiedself.com/) [31], where for instance a vi- sualization of eating habits can help to lead a healthier life, or where a visualization of mobility patterns can help to explore alternative modes of transport, etc. Such visualiza- tions are successful if they trigger the intended behaviour (change). That can be measured, as in people smoke less when they use this visualization or people discover new publications based on this visualization (we are actually evaluating such an application) or people run more using this visualization etc.  It would be really useful if we could draw up some guide- lines to design effective goal oriented visualizations. As an example, it is probably kind of useful to be able to visualise progress towards a goal - or lack thereof. If you want to run further, a visualization can help you to assess whether youre making progress. Or if you want to spend less time doing email, a simple visualization can help. Another guideline could relate to social support, that enables you to compare your progress with that of others.  5. TECHNICAL INFRASTRUCTURE FOR LEARNING ANALYTICS  If we want to apply learning analytics at a broader scale, then it is imperative that we realize an infrastructure that can support the development of tools and services. Such an infrastructure will need basic technical agreement on com- mon standards and protocols [8].  A first question is how to model the relevant data. Our early work on Contextualized Attention Metadata (CAM) [19] [36] defines a simple model to structure attention meta- data, i.e. the interactions that people have with objects. The ontology-based user interaction context model (UICO) [24] focuses more on the tasks that people carry out while interacting with resources. Either we need to better un- derstand how to map and translate automatically between different such models, or we need to find a way to achieve broad consensus on and adoption of a common schema or a small set of schemas, as in the case of learning resources where nearly everyone has now adopted Learning Object Metadata (LOM) or Dublin Core (DC) [8].  Similar to the way we manage learning objects and their metadata [33], we will need a service architecture that can power a plethora of tools and applications. One interest- ing approach is to rely on technologies like widgets that enable the dynamic embedding of small application com- ponents - an approach at the core of Personal Learning Environments (PLEs), researched in the ROLE project on Responsive Open Learning Environments (see http://www. role-project.eu/) [15] [12]. Another approach is the Learn- ing Registry architecture that makes user data trails avail- able through a network of nodes that provide services to  publish, access, distribute, broker or administer paradata (see http://www.learningregistry.org/).  6. LEARNING DASHBOARDS For learners and teachers alike, it can be extremely useful  to have a visual overview of their activities and how they relate to those of their peers or other actors in the learning experience.  In fact, such visualizations can also be quite useful for other stakeholders, like for instance system administrators. Figure 5 provides an early example of such a visualization that displays the number of events in different widgets de- ployed in the ROLE context [27]. From the visualization, it is rather obvious that users were most active in the May- July period (towards the left of the diagram), that they enter chat rooms (top of area 4 on Figure 5) much more often than they post messages (third row of area 4 on Figure 5), etc. Such information can help a teacher to re-organize the ac- tivities or even to retract or add widgets that learners can deploy in their PLE.  Similarly, [28] describes a tool that includes a zeitgeistof action types (opening a document, sending a message, etc.) and specific user actions. By selecting a time period and the relevant action types, the user can control the visualization of relevant data (see also http://www.role-showcase.eu/ role-tool/cam-zeitgeist).  Following a similar visual approach, the Student Activity Monitor (SAM) supports self-monitoring for learners and awareness for teachers [13]: In area A on figure 6, every line represents a student in a course. The horizontal axis repre- sents calendar time and the vertical axis total time spent. If the line ascends fast, then the student worked intensely during that period. If the line stays flat, the student did not work much on the course. For example, student s1 started late and worked very hard for a very short time. Student s2 started early and then worked harder in about the same period as student s1. At the bottom, a smaller version of the visualization is shown with a slider on top to select a part of the period for analysis of data dense areas. Area 2 dis- plays global course statistics on time spent and document use. The colored dots represent minimum, maximum and average time spent per student and the time spent for the currently logged in user and for a user selected in one of the visualizations. The recommendation area in Box 3 enables exploration of document recommendations (see also section 7). The parallel coordinates in area B display  1. the total time spent on the course,  2. the average time spent on a document,  3. the number of documents used and  4. the average time of the day that the students work.  For example, the green line (the logged in user) works on average in the early evening and is spending an average time in line with the majority. He does not use so many different documents and on average looks at these for a short time. He scores the worst here. The average student of the class (in yellow) is also presented. This is a somewhat complex visualization, but our evaluation studies show that students considered the visualizations clear [13]. They rated the tools as usable, useful, understandable and organized.  12  http://quantifiedself.com/ http://www.role-project.eu/ http://www.role-project.eu/ http://www.learningregistry.org/ http://www.role-showcase.eu/role-tool/cam-zeitgeist http://www.role-showcase.eu/role-tool/cam-zeitgeist   Figure 4: My Nike Plus dashboard     At the top of the dashboard (label 1), there is the option of  filtering per application. The modification of this filter affects all  visualizations. The charts are also interlinked. Table 1 presents  which actions trigger updates of other visualizations.     Table 1 Actions overview   Section Action triggered Affected  visualizations   Sent Information   1 Selecting an application 2,3,4,5 Name of the widget   2 Restricting a period of  time   3,4,5 Starting date   Ending date   3 Selecting a day of the  week   2,4,5 Day of the week   4 Selecting a type of action 5 Type of Action   5 Selecting a type of item 4       5. USE CASE: XMPP CHAT BEHAVIOR    This use case describes the behavior of a specific widget in a PLE  environment, deployed during a course at RWTH Aachen  University during the period May to July 2010. After this period,  the environment was occasionally used in an informal way. In this  PLE, four widgets were used. The widgets use Open Social [13]  for their communication in a PLE.    ABC Testing widget. This widget was only used during the  first two weeks (this information is also displayed in the  dashboard).    Cam Widget. This widget tracks the Open Social  communication and translates this communication to  CAM. Users can deactivate or activate tracking of their  data.    Role Web 2.0 Knowledge Map. This widget allows to  search for articles by entering keywords.    XMPP Multiuser Chat. This widget enables chat  functionality between different users based on the  XMPP technology.        In this use case, we will focus on the XMPP Multiuser Chat  widget because it is the most active in terms of event  communication providing us more information about its particular  characteristics. We will now explain how we can derive the  conclusions from:  1. Detect changes on usage patterns:  When we select theXMPP   Multiuser Chat in part 1 one of Figure 2 and we obtain an  overview of the overall activity (Figure 3). The annotated time  line chart (Figure 3) enables us to see that the activity was  concentrated during the period from May to July 2010. After  this period, the activity was reduced considerably. In the  events per type of action chart (Figure 3), we can see that  people enter to room chats more than sending messages (if we   Figure 3 XMPP Multiuser Chat visualization   Figure 2 CAM Dashboard overview   Figure 5: The CAM dashboard [27]  13    A much more simple such visualization is edufeedr [21], where a matrix includes a row for every student that dis- plays his progress along a series of assignments. A nice fea- ture is that such progress can take place on the individual blog of the student, outside of the institutional Learning Management System (LMS), Virtual Learning Environment (VLE) or even institution provided PLE widgets. Rather, the coherence of the course is maintained through the track- back mechanism between the teacher blog and those of the students.  What these visualizations have in common is that they enable a learner or teacher to obtain an overview of their own efforts and of those of the (other) learners. This is the essence of our dashboard approach to visualizations for learning that remedy the blind driving that often oc- curs on the side of teachers and learners alike. Similar ap- proaches have proven to be beneficial in for instance software engineering [3] and social data analysis [18].  7. LEARNING RECOMMENDERS By collecting data about user behavior, learning analytics  can also be mined for recommendations, of resources, activ- ities or people [17]. In this way, we can turn the abundance of learning resources into an asset, by addressing lembarras du choix that is at the core of the paradox of choice [29]. Of course, similar approaches have been deployed for books, music, entertainment, etc. Yet, only by basing rec- ommenders on detailed learning attention metadata can they take into account the learning specific characteristics and re- quirements of our activities.  In one particular tool, we applied this approach to fil- ter and rank search results when a learner searches for ma- terial in YouTube (http://www.youtube.com/), SlideShare (http://www.slideshare.net/) and Globe (http://www.globe-info. org/): as figure 7 illustrates, every search activity in our tool is tracked in the form of attention metadata that are stored in a repository. The user can indicate whether search results are relevant or not and that feedback is also stored in the attention metadata repository. Search results are filtered and ranked based on earlier interaction by the user and by other users in her social network, as made available through OpenSocial. Although we need to do more user evaluations, the first results are very encouraging [11, 20].  8. DATA BASED RESEARCH ON LEARN- ING  On a meta-level, learning analytics provides exciting op- portunities to ground research on learning in data and to transform it from what is currently all too often a collection of opinions and impossible-to-falsify conceptualizations and theories [23].  As a precursor to making that happen, it is important that we agree on ways to share data sets, in an open science approach [26, 9, 14]. That is why a group of interested re- searchers has started an initiative around dataTEL (http: //www.teleurope.eu/pg/groups/9405/datatel/ [34]. The main objective is to promote exchange and interoperability of educational data sets.  9. CONCLUSION Of course, one of the big problems around learning an-  alytics is the lack of clarity about what exactly should be  measured to get a deeper understanding of how learning is taking place: typical measurements include time spent, number of logins, number of mouse clicks, number of ac- cessed resources, number of artifacts produced, number of finished assignments, etc. But is this really getting to the heart of the matter  Moreover, there are serious issues about privacy when de- tailed data of learner interactions are tracked [4]. An in- teresting early approach to deal with these issues was the proposal of the no longer active not for profit Attention- Trust [35]: their guiding principles included   property: the data about a persons attention remain the property of that person;   mobility: it should be possible to move data about a person out of one system and into another system - see also googles recent data liberation initiative (see http://www.dataliberation.org/);   economy: a person should be able to sell data about his attention;   transparency: it should always be clear to a person that she is being tracked.  Especially that last principle seems key: tools like ghostery (http://www.ghostery.com/) enable a user to know when she is being tracked on a web site. As we evolve towards a world where not only learning activities, but virtually every- thing will be tracked [2], this issue is likely to become even more important.  Some people are quite concerned about the filter bubble that personalization and recommendation engines may cre- ate [22]: we agree that there is a certain danger there, but we also believe that more advanced algorithms and ethical reflection can help us to address these issues.  In any case, we believe that learning analytics can be used to put the user in control, not to take control away in an Intelligent Tutoring Systems kind of way, by using attention to filter and suggest, provide awareness and support social links.  10. ACKNOWLEDGMENTS This research has received funding from the European  Community Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 231396 (ROLE) and no. 231913 (STELLAR). Much more importantly, the support, com- ments and feedback from my team and students have thought me much more than I will ever be able to teach them.  11. REFERENCES [1] C. Anderson. The End of Theory: The Data Deluge  Makes the Scientific Method Obsolete. Wired Magazine, 16(7), 2008.  [2] G. Bell and J. Gemmel. Your life, uploaded. Plume, 2010.  [3] J. Biehl, M. Czerwinski, G. Smith, and G. Robertson. FASTDash: a visual dashboard for fostering awareness in software teams. In Proceedings of the SIGCHI conference on Human factors in computing systems, pages 13131322. ACM, 2007.  14  http://www.youtube.com/ http://www.slideshare.net/ http://www.globe-info.org/ http://www.globe-info.org/ http://www.teleurope.eu/pg/groups/9405/datatel/ http://www.teleurope.eu/pg/groups/9405/datatel/ http://www.dataliberation.org/ http://www.ghostery.com/   Visualizing Activities for Self-reflection and Awareness 5  Fig. 1. The user interface with the 3 dierent visualizations.  The parallel coordinates in visualization B are a common way to display high-dimensional data [10]. On the vertical axes, we show (i) the total time spent on the course, (ii) the average time spent on a document, (iii) the number of documents used and (iv) the average time of the day that the students work. A student is represented as a polyline connecting the vertices on the vertical axes. The position of the vertex on the i-th axis corresponds to the i-th data point coordinate. For example, the green line (the logged in user) works on average in the early evening and is spending an average time in line with the majority. He does not use so many dierent documents and on average looks at these for a short time. He scores the worst here. The average student of the class (in yellow) is also calculated. This is a much more advanced visualization but can provide a good overview of the tendencies in the behavior of the students.  Figure 6: The Student Activity Monitor (SAM) [13]  include search results from content on your personal social networks (e.g. Twitter and Google Buzz). Bing collaborates with Facebook to provide similar functionalities: searching in the profiles of your friends and in content liked by your friends. IBM also added social features based on its em- ployee directory, tags, bookmarking behavior and ratings to its intranet to show more relevant blogs, wikis, forums and news to great success [2, 8]. Social search is also a popu- lar research topic. Haystaks [9] extends mainstream search engines with new features: users can create sharable search result folders and the content of these folders is used for rec- ommendations. I-SPY [10] allows people to create search communities and based on the queries and results in these communities it will adaptively re-rank the search results.  To enable search over multiple repositories, one can apply federated or harvested search, which collects all the meta- data from all repositories in a central repository for faster re- trieval [11]. When working with vast repositories of web 2.0 sources (e.g. YouTube), it is impossible to apply harvested search. As far as federated search is concerned, Ariadne [11] focuses on the interoperability between different reposito- ries. The search engine relies on the Standard Query Lan- guage (SQI) to ensure interoperability and to offer a trans- parent search over a network of repositories. Ariadne also provides harvesting. This is applied in the GLOBE network with 13 repositories3. MetaLib and WebFeat provide feder- ated search over scientific content [12]. WebFeat sends the query to all search engines and then shows the results in all the native user interfaces. In contrast with MetaLib that uses its own UI and communicates with the repositories over the standardized Z39.50 protocol. ObjectSpot [13] is originally a federated search widget for scientific publications, but it is now extended for web 2.0 sources. It uses a cover den- sity algorithm to rank the search results, which can also be manually re-ordered. Basic recommendations based on the user selection of search results are also provided. Extending ObjectSpot with social features was not trivial due to the use of incompatible technology. None of these federated search engines provides social features or social search results. Just as mainstream search engines are exploring social networks and attention metadata (voting & sharing), we adopted this strategy in our federated search engine.  THE DESIGN OF THE WIDGET In this section, the software architecture behind the search widget is explained and the user interface (UI) design and implementation is discussed.  Software architecture To enable search over multiple data sources, we employ a client-server architecture, as shown in Figure 1. When the user enters a search term, it is sent to the federated search service (step 1), which transmits it to all the different data sources concurrently (step 2). Currently it queries YouTube, SlideShare.net, Wikipedia, GLOBE and the OpenScout repos-  3The Global Learning Objects Brokered Exchange (GLOBE) al- liance, http://www.globe-info.org  Figure 1. The client-server architecture of the federated search and recommendation service.  itory4, but extra sources can be easily added to support dif- ferent learning scenarios. When all the results are returned from the data sources (step 3), the federated search service re-ranks the results (step 4) based on the metadata with the Apache Lucene library [14]. The ranked results are returned to the widget in the ATOM format [15] (step 5), which en- ables us in the future to make the service OpenSearch com- pliant [16]. OpenSearch allows search engines to publish search results in a standard and open format to enable syn- dication and aggregation. In future, we plan to adapt the service to return results every time the repositories return them to improve search speed. Once the widget receives the search results, they are presented to the user and the search result URLs are sent to the recommendation service (step 6). This service will return recommendations, based on the at- tention metadata stored in the database. The recommenda- tions are sent back to the widget, where they will be pre- sented to the user. The user can interact with the search re- sults, e.g. preview a movie inside the widget or (dis)like it. When some of these interactions happen, they are tracked and the attention metadata (basically the user, the URL of the search result and the action) is sent to the recommen- dation service (step 7). The service then stores the attention metadata in a database (step 8) to be able to calculate recom- mendations later. The next section describes the recommen- dation algorithm in more detail. The client-server architec- ture enables us to expose repositories not openly accessible by deploying the service inside the intranet.  User Interface Design The main design goal was to provide a simple, clean search interface with visually rich search results to enable better decision making while selecting a search result. The widget provides a simple Google-like search interface over multiple web 2.0 data sources. Although advanced search settings are available (see Figure 3), they are not visible by default. Morville et al. [2] advice this as well, because advanced search is often used by expert users. Figure 3 shows the ad- vanced search settings where the wanted media types, repos- itories and social recommendations can be configured. This can be operated with the wrench icon.  The search results are presented in a uniform way (see Fig- ure 2): basic metadata and tags together with a screenshot  4The OpenScout repository, http://www.openscout.net/ demo  2  Figure 7: Storing attention metadata in federated search [11]  15    [4] D. Boyd. Facebooks Privacy Trainwreck: Exposure, Invasion, and Social Convergence. Convergence: The International Journal of Research into New Media Technologies, 14(1):1320, 2008.  [5] C.-H. Chang, M. Kayed, M. R. Girgis, and K. F. Shaalan. A Survey of Web Information Extraction Systems. IEEE Transactions on Knowledge and Data Engineering, 18:14111428, 2006.  [6] N. Corthaut, S. Lippens, S. Govaerts, E. Duval, and J.-P. Martens. The integration of a metadata generation framework in a music annotation workflow. Oct. 2009.  [7] A. Croll and S. Power. Complete Web Monitoring. OReilly Media, Inc., 2009.  [8] E. Duval and K. Verbert. On the role of technical standards for learning technologies. IEEE Transactions on Learning Technologies, 1(4):229234, Oct. 2008.  [9] J. Fry, R. Schroeder, and M. den Besten. Open science in e-science: contingency or policy JOURNAL OF DOCUMENTATION, 65(1):632, 2009.  [10] M. H. Goldhaber. The Attention Economy and the Net. First Monday, 2(4), Apr. 1997.  [11] S. Govaerts, S. E. Helou, E. Duval, and D. Gillet. A Federated Search and Social Recommendation Widget. In Proceedings of the 2nd International Workshop on Social Recommender Systems (SRS 2011) in conjunction with the 2011 ACM Conference on Computer Supported Cooperative Work (CSCW 2011), pages 18, 2011.  [12] S. Govaerts, K. Verbert, D. Dahrendorf, C. Ullrich, S. Manuel, M. Werkle, A. Chatterjee, A. Nussbaumer, D. Renzel, M. Scheffel, M. Friedrich, J. L. Santos, E. Duval, and E. L.-c. Law. Towards Responsive Open Learning Environments : the ROLE Interoperability Framework. In ECTEL11: European Conference on Technology Enhanced Learning, Lecture Notes in Computer Science, 2011.  [13] S. Govaerts, K. Verbert, J. Klerkx, and E. Duval. Visualizing Activities for Self-reflection and Awareness. In Proceedings of the 9th international conference on Web-based Learning, pages 91100. Springer, 2010.  [14] T. Hey and A. E. Trefethen. Cyberinfrastructure for e-Science. Science, 308(5723):817821, 2005.  [15] U. Kirschenmann, M. Scheffel, M. Friedrich, K. Niemann, and M. Wolpers. Demands of Modern PLEs and the ROLE Approach. In M. Wolpers, P. Kirschner, M. Scheffel, S. Lindstaedt, and V. Dimitrova, editors, Sustaining TEL: From Innovation to Learning and Practice, volume 6383 of Lecture Notes in Computer Science, pages 167182. Springer, 2010.  [16] X. Ma, G. Chen, and J. Xiao. Analysis of An Online Health Social Network. In Proceedings of the 1st ACM International Health Informatics Symposium, pages 297306. ACM, 2010.  [17] N. Manouselis, H. Drachsler, R. Vuorikari, H. Hummel, and R. Koper. Recommender Systems in Technology Enhanced Learning. In F. Ricci, L. Rokach, B. Shapira, and P. B. Kantor, editors, Recommender Systems Handbook, pages 387415.  Springer US, 2011.  [18] M. McKeon. Harnessing the web information ecosystem with wiki-based visualization dashboards. IEEE transactions on visualization and computer graphics, 15(6):10818, 2009.  [19] J. Najjar, M. Wolpers, and E. Duval. Contextualized attention metadata. D-Lib Magazine, 13(9/10), Sept. 2007.  [20] X. Ochoa and E. Duval. Use of contextualized attention metadata for ranking and recommending learning objects. In CAMA06: Proceedings of the 1st international workshop on Contextualized attention metadata: collecting, managing and exploiting of rich usage information, pages 916, 2006.  [21] H. Poldoja. EduFeedr-following and supporting learners in open blog-based courses. In Proceedings of OpenEd2010, number 2010. Universitat Oberta de Catalunya, 2010.  [22] E. Pariser. The Filter Bubble: What the Internet Is Hiding from You. Penguin Press, 2011.  [23] K. Popper. The Logic of Scientific Discovery. Routledge, 1959.  [24] A. S. Rath, D. Devaurs, and S. Lindstaedt. UICO: an ontology-based user interaction context model for automatic task detection on the computer desktop. In Proceedings of the 1st Workshop on Context, Information and Ontologies, CIAO 09, pages 8:1-8:10, New York, NY, USA, 2009. ACM.  [25] C. Romero and S. Ventura. Educational data mining: A survey from 1995 to 2005. Expert Systems with Applications, 33(1):135146, July 2007.  [26] S. S. Sahoo, A. Sheth, and C. Henson. Semantic provenance for eScience - Managing the deluge of scientific data. IEEE INTERNET COMPUTING, 12(4):4654, 2008.  [27] J. L. Santos, K. Verbert, S. Govaerts, and E. Duval. Visualizing PLE Usage. In Proceedings of EFEPLE11: 1st Workshop on Exploring the Fitness and Evolvability of Personal Learning Environments. CEUR workshop proceedings, 2011.  [28] H. Schmitz, M. Scheffel, M. Friedrich, M. Jahn, K. Niemann, and M. Wolpers. CAMera for PLE. In U. Cress, V. Dimitrova, and M. Specht, editors, Learning in the Synergy of Multiple Disciplines, volume 5794 of Lecture Notes in Computer Science, pages 507520. Springer, 2009.  [29] B. Schwartz. The paradox of choice - Why more is less. HarperCollins, 2007.  [30] C. Shirky. Here Comes Everybody: The Power of Organizing Without Organizations. Penguin Press, 2008.  [31] E. Singer. The Measured Life. Technology Review, 2011.  [32] M. Swan. Emerging patient-driven health care models: an examination of health social networks, consumer personalized medicine and quantified self-tracking. International journal of environmental research and public health, 6(2):492525, Feb. 2009.  [33] S. Ternier, K. Verbert, G. Parra, B. Vandeputte, J. Klerkx, E. Duval, V. Ordonez, and X. Ochoa. The Ariadne Infrastructure for Managing and Storing  16    Metadata. IEEE Internet Computing, 13(4):1825, July 2009.  [34] K. Verbert, E. Duval, H. Drachsler, N. Manouselis, M. Wolpers, R. Vuorikari, and G. Beham. Dataset-driven Research for Improving TEL Recommender Systems. In 1st International Conference on Learning Analytics and Knowledge, Banff, Canada, 2011.  [35] M. Wolpers, J. Najjar, and E. Duval. Workshop report on the international {ACM} workshop on contextualized attention metadata: collecting, managing and exploiting rich usage information (cama 2006), June 2007.  [36] M. Wolpers, J. Najjar, K. Verbert, and E. Duval. Tracking actual usage: the attention metadata approach. Educational Technology and Society, 10(3):106121, 2007.  17    Introduction  Background on Analytics  Inspiration for Our Work  Goal Oriented Visualizations  Technical Infrastructure for Learning Analytics  Learning Dashboards  Learning Recommenders  Data Based Research on Learning  Conclusion  Acknowledgments  References     "}
{"index":{"_id":"3"}}
{"datatype":"inproceedings","key":"Haythornthwaite:2011:LNC:2090116.2090119","author":"Haythornthwaite, Caroline","title":"Learning Networks, Crowds and Communities","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"18--22","numpages":"5","url":"http://doi.acm.org/10.1145/2090116.2090119","doi":"10.1145/2090116.2090119","acmid":"2090119","publisher":"ACM","address":"New York, NY, USA","keywords":"education, learning analytics, online learning, social networks","Abstract":"Drawing on sociocultural discourse analysis and argumentation theory, we motivate a focus on learners' discourse as a promising site for identifying patterns of activity which correspond to meaningful learning and knowledge construction. However, software platforms must gain access to qualitative information about the rhetorical dimensions to discourse contributions to enable such analytics. This is difficult to extract from naturally occurring text, but the emergence of more-structured annotation and deliberation platforms for learning makes such information available. Using the Cohere web application as a research vehicle, we present examples of analytics at the level of individual learners and groups, showing conceptual and social network patterns, which we propose as indicators of meaningful learning.","pdf":"Learning Networks, Crowds and Communities  Caroline Haythornthwaite   School of Library, Archival and Information Studies,   University of British Columbia   Irving K. Barber Learning Centre   Suite 470- 1961 East Mall   Vancouver, BC V6T 1Z1    011-1-604-837-4790   c.haythorn@ubc.ca    ABSTRACT  Who we learn from, where and when is dramatically affected by  the reach of the Internet. From learning for formal education to  learning for pleasure, we look to the web early and often for our  data and knowledge needs, but also for places and spaces where  we can collaborate, contribute to, and create learning and  knowledge communities. Based on the keynote presentation given  at the first Learning Analytics and Knowledge Conference held in  2011 in Banff, Alberta, this paper explores a social network  perspective on learning with reference to social network principles  and studies by the author. The paper explores the ways a social  network perspective can be used to examine learning, with  attention to the structure and dynamics of online learning  networks, and emerging configurations such as online crowds and  communities.     Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education   General Terms  Management, Measurement, Design, Human Factors.   Keywords  Learning analytics, social networks, online learning, education.   1. INTRODUCTION We hear a lot about networks these days, from large scale maps of  the internet to social networking sites. There are knowledge  networks and social networks, political networks and  transportation networks, information exchange networks,  recommender networks and communication networks. There are  also webs  the web of science, food webs, and, of course, the  world wide web. Networks have great appeal in part because of  the visualizations that bring them to life and make visible the  invisible structures of human interaction   see, for example, Trier 2007, Lim forthcoming and  http://www.visualcomplexity.com/vc/.  In my research, I have been examining online interaction from a  social network perspective, addressing questions such as: How do  people work, learn and socialize together at a distance and   through computer media How do social network patterns of  interaction support information and knowledge sharing In  particular, a number of my studies have been based on examining  who talks to whom about what and via which media e.g.,  Haythornthwaite, 2002a, 2008. A number of these studies have  addressed online learning environments, for example, examining  in-class personal networks of students for their composition and  extent, how community is perceived and supported among online  learners, and how class requirements e.g., group projects affect  who talks to whom and the ramifications of this for network  structures.   This research provides the background for my orientation to  learning analytics. The social network perspective provides a  strong suite of social theory and analytic techniques that can  illuminate interaction processes and be particularly useful for  proactive and responsive teaching, learning, and education. While  we are at the very beginning of researching and understanding  learning networks, we have the opportunity to start from the  wealth of study done on social networks. This paper provides  background on my own studies of learning environments and  networks, and also describes social network research that provides  a groundwork for learning analytics in terms of both graph theory  and studies of social behavior. The kinds of questions we now  face include: How can networks can be used to examine learning  and education processes What needs to be done to build a  network analytic base for learning How can what is known in  social network research be used to jumpstart learning networks  research  The following discusses a social network analytic view of  learning, connecting this to aspects of learning and networks that  lend themselves to a research agenda for learning analytics.    1.1 Brief Introduction to Social Networks  Currently, the idea of social networks has permeated discussions  of online interaction, in part because of the adoption of  technologies known as social networking sites.  However, the  attention here is not on the phenomenon of social networking, but  on social network analysis SNA, a perspective, method, set of  techniques and vocabulary that can be applied to the study of  social phenomena, and in particular social relationships and  structures. A primary characteristic of social network analysis is  its emphasis on relational properties rather than aggregate  behavior. Relations are the interaction, transaction,  communication, or collaboration that ties two or more actors in a  network. Relations can be distinguished on characteristics such as  content, direction, and strength, e.g., in the difference between  conveying instructions, providing material vs. social support, or  collaborating in a formal partnership. Actors tied by these  relations may be individuals, groups or teams, centers,  organizations, countries, etc. The ties that result from the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February  27March 1, 2011, Banff, Alberta, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.      18    maintenance of relations may be distinguished by the set of  relations maintained in composition, multiplexity, importance,  etc., and in the way the relations are maintained, e.g., through  different media, with different frequency, intensity or intimacy.  Networks result from these kinds of direct interactions, but they  can also be inferred from indirect interaction. Thus, networks can  be demonstrated based on who interacts with whom, but also  around co-attendance or co-participation in events, e.g., as people  who have attended the same lecture, whether they met each other  or not, are tied through common experience, exposure to  information, or interest.    Networks are evident in many of our daily activities, and with  increasing frequency are easily visualized and seen in online  displays, particularly for internet-based phenomena. For example,  we can: see visualizations of twitter interactions  http://networkweaver.blogspot.com/2011_02_01_archive.html;  use ToughGraphs to look at internet linkages or book buying on  Amazon http://www.touchgraph.com/; look at linkages in  resource searching that creates a map of knowledge Bollen et al,  2009  Network analysis software is becoming increasingly available and  easier to use e.g., see lists on the   International Network of Social Network Analysts INSNA site:  http://www.insna.org/software/index.html, and is also being  integrated into analysis and presentation in learning environments  e.g., see the Social Networks Adapting Pedagogical Practice  SNAPP tool, Bakharia & Dawson, 2011, and  http://research.uow.edu.au/learningnetworks/seeing/snapp/index.h tml.  These developments create a readiness for production and  presentation of network results, increasing general understanding  of these kinds of results and perhaps also expectations around  their presentation. Thus, the time is appropriate to include  network analyses, results and displays into learning environments,  and to gain a greater understanding of how particular network  configurations correspond to different kinds of learning outcomes.   2. SOCIAL NETWORK RESEARCH  A lot of research has been done on social networks e.g., for recent  reviews and compilations, see Borgatti, Mehra, Brass & Labianca,  2009; Scott & Carrington, 2010; see also Wasserman & Faust,  1994; Carrington, Scott & Wasserman, 2005, with recent attention  appearing in relation to education and learning e.g., Daly, 2010;  Haythornthwaite & de Laat, in press. While a review is beyond  the scope of this paper, the following highlights some key areas  that relate to learning.   2.1 Knowledge Dissemination   From diffusion of innovation to trust in information sources, an  underlying theme in knowledge dissemination is the role of  different kinds of people in the adoption and uptake of new  information. Studies have repeatedly shown that a core of people  similar to us are important in our adoption of new practices, i.e.,  in the demonstration of new behavior that suggests learning has  happened. Whether of ideas or technologies, testimony of trusted  others and evidence in local practice makes a difference in what  we follow and incorporate in our own practices. The people we  most follow are those with whom we maintain strong ties, i.e., ties  that are sustained through a base of multiple relations and multiple  media, with frequent interaction, shared experience, mutual  interest, reciprocity in exchanges and self-disclosure. These  people are similar to us, travel in the same social circles, and are  exposed to similar information. This similarity facilitates   exchanges among us as we use the same language or short-hands  for communication that reduce the work we need to do to convey  information Clark & Brennan, 1991. We know more about each  other and this transactive memory helps us distinguish who  knows what across our network Hollingshead, Fulk & Monge,  2002; Wegner, 1987.   By contrast, those with whom we share weak ties, i.e., ties based  on fewer, more instrumental relations, with less frequent  interaction via fewer media, are more different from us. We know  less about them, trust their information less, and are less likely to  take up their ideas or innovations. This difference is, however, the  strength of weak ties Granovetter, 1973. Because they come from  different social circles, they are aware of and able to convey  information that is different from what is known among our strong  ties. While their ability to influence adoption may be low, weak  ties are important for creating awareness of new opportunities,  and bridging between social networks Putnam, 2000; Lin, 2008.  In the computing field, much effort has gone into describing and  designing for tightly connected working relationships associated  with strong ties. Areas such as computer-supported cooperative  work CSCW have detailed the tasks worked on by distributed  pairs in order to enhance systems that support such ties. Studies of  workplaces e.g., Engestrm & Middleton, 1996; Luff, Hindmarsh  & Heath, 2000 have been important in describing the details of  work life as a way of articulating close work tie relations.  Similarly, concerns about computer-mediated communication and  virtual communities have focused on how strong tie connections  are affected, creating social and technical design initiatives to  increase presence and the feeling of being there and/or of  being there with others. Along a similar line, research on trust  also explores the development of strong ties and/or strong-tie like  relationships e.g., brand loyalty and how this affects information  sharing in organizations, and engagement in online activities such  as e-commerce.   In the learning area, ideas of collaborative learning have  emphasized the importance of hearing from different others,  pooling and sharing different opinions as a way of enlarging the  educational and learning experienced Bruffee, 1993, i.e, in paying  attention to the role of weak ties. Yet, even in collaborative  learning environments, requirements for group projects often  bring the learner back to the need to achieve a close working tie  with others. Thus, from a network perspective, an argument can  be made for attention to both strong and weak ties for learning for  more on this, see Haythornthwaite, 2002b, and hence for support  and analytics that show how both kinds of ties and their respective  roles play out in learning and information sharing.   2.2 Network Configurations  Another area of network studies addresses the information  consequences of particular configurations of networks. A dense  network high density, for example, provides many links among  members of a network, increasing the likelihood that opportunities  will arise to interact and pass on information or gossip. By  contrast, a sparse network low density, provides few links among  network members, potentially increasing the time it takes for  information to reach all members of the network. Networks can be  structured around a highly central actor, i.e., configured in such a  way that all information must pass through a single network actor  before reaching others network star or broker, with a high  betweenness score; filling a structural hole, Burt 1992;  representing a cutpoint which, if removed, splits the network into  multiple networks. Networks may show a particularly long, path  length, so that information must pass through many actors to get   19    from one end of the network to another; they may show connected  cliques of actors; or they may reveal isolates who are not  connected at all to the general information flow in the network.  By itself, a dense or sparse network should not be considered  unquestionably good or bad for information exchange. Dense  networks facilitate exchange of both fact and fiction. They  promote constant visibility of others and their actions, creating  both the pro and con of a small village. Sparsity can help keep  information and actions private, allowing relative anonymity.  Network connections also require time to maintain. An individual  with lots of connections can be overwhelmed by the amount of  information arriving and/or the responsibility for forwarding this  information. Structures that help in knowing who should get what  kind of information and/or who to go to for specific information  acting as information sources and sinks  whether explicit or  learned as cognitive social networks  reduce the work the entire  network needs to do the keep track of important information. For  examples of learning networks from a personal network view,  looking at the number of ties and relations an individual learner  maintains, see Haythornthwaite, 2000, and from a whole network  view, see Haythornthwaite, 2002a, 2006; for further discussion of  networks and information exchange, see Haythornthwaite, 2010.  A question in learning environments is what kind of configuration  works best, and for what kind of outcomes. A goal of task  completion, e.g., for projects, may indicate a need for densely  connected cliques, whereas a goal of collaborative learning may  suggest aiming for high density across the whole network.  The  caveat is that simple measures like density may fail to capture the  nuances of relational content across networks. Thus, what kind of  relations tie actors becomes important for understanding what  comprises a learning tie, and what sustains a learning community,  both for providing new information and also for providing the  support that facilitates acquisition and understanding of new  and/or difficult concepts.   With this background, the following describes a few areas of my  research on learning networks as illustration of network effects  and how networks can be used to view learning.   3. LEARNING: FROM TIES TO  NETWORKS Before we can look at learning networks, we need to understand  what is meant by a learning tie  i.e., what is it that connects  people in a way that leads them to say I learned from this  person, while part of this class, as a member of a team, etc.    3.1 Relations and Ties  Two studies have tried to tease out the multiple threads that  connect people in work and learning relationships. In the first  study, interactions were studied among members of an academic  department referred to as Cerise Haythornthwaite & Wellman,  1998. Group members answered 24 questions about a variety of  their work and social interactions with  others in the group, as  well as about the type of work and friendship tie they maintained  with each person. A factor analysis revealed six dimensions of  work and social interaction. Two related to work practices:  Receiving Work and Giving Work each engaged in by 57% two  related to major work products : Collaborative Writing 32%, and  Computer Programming 56%; and two related to social support:  Sociability 86%, and Major Emotional Support 7%.   In a later study of three interdisciplinary research teams,  qualitative and semi-structured interviews provided data on  interaction and learning between the respondent and the five to  eight others with whom they interacted most frequently relating to   team work Haythornthwaite, 2006. Coding the data for learning  based on answers to questions on Who do you learn from or  receive help in understanding something from suggested nine  categories of learning. In terms of frequency of occurrence across  the teams, four major categories were found: Factual knowledge  about the discipline, Process how to knowledge, Method  knowledge, and engagement in Joint research. Minor areas of  exchange included Technology knowledge, Socialization,  Generation of new ideas, and Networking e.g., for jobs, with a  very few reporting a tie based on Administration.   The purpose in mentioning these studies for learning analytics is  to highlight that working and learning relationships are predicated  on a range of relations, some instrumental, some social. Looking  at relational dimensions shows the kinds of exchanges that will  need support socially, pedagogically, technically, and that can be  examined and addressed in support of learning.   The network data on these relations also shows who is connected  via which relations. For example, in the study of interdisciplinary  teams, it appears that factual knowledge across disciplines was  exchanged more among senior personnel, whereas method  knowledge tended to be exchanged between others involved in  methodological aspects of the work. This kind of finding  highlights another aspect of social networks analysis, which is the  identification of roles and positions based on finding repeated  patterns of social interaction and relational structure. Ideas of the  network star, broker or technological gatekeeper are already well  known from offline networks. Online environments are generating  new kinds of roles and positions such as wizards, newbies, and  even trolls. In online learning, a learner-leader role has been  identified Montague, 2006, and online roles have been awarded to  individuals who contribute in online learning communities  Preston, 2008.  Moreover, much of what is discussed about online  teaching and collaborative learning points directly to the impact of  change in roles, i.e., toward a network position of facilitator rather  than authority with regard to knowledge.    3.2 Ties and Media Use  The network data also revealed how exchanges are structured  across the group and across media. Data from the Cerise study,  and from studies of interaction among online class members,  showed that pairs who communicated infrequently and/or reported  a weak friendship or a work-only tie, maintained ties through  fewer media than others. Contrary to expectations of a task- medium fit, instead there was a tie-media relationship: with the  strength of the tie associated with the number of media used,  demonstrating media multiplexity effect Haythornthwaite &  Wellman, 1998.  The later studies of online learners also revealed another effect.  Studies of who talked to whom, about what, and via which media  in classes of online learners showed that the use of media  followed a unidimensional scale. Infrequent contacts used one  medium, and, in general, this was the medium established by the  teacher as the key meeting ground for the class  e.g., a live class  session involving chat, and/or required online discussion boards.  Other media were added to a pairs repertoire in a consistent  manner within each group: e.g., adding email, and phone onto  chat and discussion. In the Cerise group, which could meet face- to-face, the first means of contact were unscheduled face-to-face  meetings, then scheduled class or research meetings, then email,  phone, and other means.   The importance of the primary means of communication  established for such groups led to the proposition that a medium  set for such group interaction forms a latent tie structure on which   20    those who do not yet know each other well can build ties, first  weak and then strong Haythornthwaite, 2002a, 2005. What is  notable about this is that such structures can only be organized  and established by an authority other than the class or group  participants  because at the initiation stage the participants do not  know each other. However, once ties start forming, those with  stronger ties may then work out their own ways of adding media  onto the base medium, strengthening the options and opportunities  for contact and increasing the resilience of the tie in the face of  changes in base media or authorities in charge of these media.  This leads into my proposed view of online learning crowds and  community from a social network perspective.   3.3 Networks to Crowds and Communities  In recent times the power of crowds as means of soliciting and  acquiring knowledge has come to the fore. After many years of  attention to the strong ties of community, and seeing these  become possible and active online, crowds ask for attention to  weak ties. But crowdsourcing also brings to the fore the question  of motivation, and why individuals contribute to crowdsourcing  initiatives. This last section summarizes my network perspective  on crowds and communities, making the connection between  these two forms of organizing along a dimension of weak to  strong ties. The continuity between these collectives can be found  in the nature of social network ties, but also in the ideas of latent  ties and the role of authorities outlined above.   In brief, I view crowds as a lightweight collaborative structure,  where weight refers to the interpersonal connection involved not  to the importance of the production being undertaken. It is light  because crowdsourced projects do not require knowing others and  working with others directly as a requirement of engagement.  Instead, they often ask for a little bit of work, with minimal  learning requirements, and few barriers to entry. On the other  hand, heavyweight peer productions or collaborations require  knowing others, paying attention to both the purpose of the  project and the opinions and contributions of others. The  association among peers is intricate and involved, based on social  conventions, reputations and rewards, and mutual visibility. While  some may offer crowd-like contribution to the heavyweight  enterprise, it only survives with a committed core of participants.  By contrast, the lightweight enterprise could survive a complete  changes of participants because its operation is dependent on an  authority who sets and manages interaction activity.   The light- and heavyweight ideas are describe in more detail  elsewhere Haythornthwaite, 2009, 2011. In brief, the lightweight  model operates based on discrete, uniform contributions submitted  by unrelated individuals, e.g., in proofreading texts  http://www.pgdp.net. The lightweight model accepts contributions  that are all similar, requiring little learning or training for  submission, with weak or no association between contributions,  and minimal commitment to belonging and continuing with future  contributions. It depends on interest to the overall project, but not  an interest in other contributors.  By contrast, the heavyweight operates based on attention to others  and their contributions, with individuals adding their own  different contributions, e.g., as in peer-reviewed scholarly  communication. It depends on attention to both the purpose of the  peer production and to members and membership in the  community. The heavyweight model depends on contributions of  different sorts and sizes; it requires learning and apprenticeship  before contributing, a strong attention interconnection between  contributions and contributors; and commitment to belonging,  staying, and continued contribution.    Motivational factors for contribution in heavyweight enterprises  can be bound up in reputation and reward, but the case for  lightweight participation is less clear. From an individual  perspective, I argue that motivation for participation derives from  co-orientation to the crowdsourced project  such as its content,  its platform, etc. For example, in a study of motivation relating to  contribution to OpenStreetMap, Budhathoki 2010 found that  attachment to local regions and to open source principles support  contribution to this project. While individual career or future  rewards may be tied to this kind of contribution, an extreme  lightweight endeavor will need to depend on common interest to  draw participants, even as it adds means of creating reputation and  reward to keep participants.   Where learning enterprises now span the range from online help  sites of user-generated content, to virtual communities of learners  and knowledge production, these dimensions of light and  heavyweight suggest ways of organizing for these different kinds  of peer production. Even as emphasis is on heavyweight learning,  the lightweight structure provides ways for lurkers, novices and  new members to begin contribution before joining with a  heavyweight commitment. These two ends of collective  production suggest ways of modeling as well as monitoring online  interaction, e.g., to see if the resultant learning environment has  become a heavyweight, self-sustaining community, or whether it  has stalled on reaching that expression by rotating memberships,  heavy authority structures, or lack of reputation and reward  structures.     4. SUMMARY This brief account of learning from a social network perspective  can only stand as an idea generator for further research and  application to learning analytics, with ideas to be generated on  how to integrate and combine social network perspectives with  the wider range of learning analytic techniques. In summarizing, I  see that a network perspective provides the following ways that  learning provides the following ways that learning can be  represented and addressed:    Learning can be a relation that connects people    Learning can be the characterization of the tie, based on  multiple, contextually determined relations that support the  report of learning or evidence of learning    Learning relations can be taken as input for design, either  social or technical, e.g., when analysis of relations show the  mix of educational and social interaction that supports  learning, and when technology may be brought in to support  one or more of these relations    Learning can be a characterization of the outcome of  relations, e.g., when a group becomes a learning community    Learning as the network outcome of relations, e.g., the social  or learning capital of the network, and thus examined for  when interactions support the social capital of knowledge  and continued knowledge evaluation and acquisition    Learning as contact with ambient influence, e.g., as informal  and ubiquitous learning leads to interest and participation in  knowledge productions.   While much of learning analytics may focus on the class and/or  the educational institution, I also see there are many  transformations happening in learning on and through the web in  collaborative peer productions and mobile and ubiquitous learning  that are equally open to examination from a learning networks  perspective and a learning analytics perspective. In moving   21    forward on learning from a network and analytic perspective, I  look forward to further research that addresses learning as it  relates to social network ties, networks, crowds and communities.   5. REFERENCES  Bakharia, A. & Dawson, S. 2011. SNAPP: A birds-eye view of  temporal participant interaction. Paper presented at the Learning  Analytics and Knowledge Conference, Banff, Alta.  Bollen, J., Van de Sompel, H,. Hagberg, A., Bettencourt, L.,  Chute, R., Rodriguez, M.A., & Balakireva, L. 2009. Clickstream  data yields high-resolution maps of science. PLoS ONE, 43:  e4803. doi:10.1371/journal.pone.0004803.   Borgatti, S. T., Mehra, A., Brass, D., & Labianca, G. 2009.  Network analysis in the social sciences. Science, 323, 892-895.  Bruffee, K. A. 1993. Collaborative Learning: Higher Education,  Interdependence, and the Authority of Knowledge. Baltimore:  John Hopkins University Press.   Budhathoki, N. R. 2010. Participants' Motivations to Contribute  Geographic Information in an Online Community. Unpublished  doctoral dissertation, University of Illinois at Urbana-Champaign,  Champaign, IL.  Burt, R. 1992. Structural Holes: The Social Structure of  Competition. Cambridge: Harvard University Press.  Carrington, P.J., Scott, J. & Wasserman, S. Eds.2005. Models and  methods in social network analysis. Cambridge: Cambridge  University Press.  Clark, H. H. & Brennan, S. E. 1991. Grounding in  communication. In L. B. Resnick, J. M. Levine, & S. D. Teasley  Eds., Perspectives on socially shared cognition pp. 127-149.  Washington, D. C. : American Psychological Association.   Daly, A. J. Ed. 2010. Social network theory and educational  change. Harvard Education Press.   Engestrm, Y. & Middleton, D. 1996. Cognition and  communication at work. Cambridge: Cambridge University Press.  Haythornthwaite, C. 2000. Online personal networks: Size,  composition and media use among distance learners. New Media  and Society, 2, 2, 195-226.  Haythornthwaite, C. 2002a. Strong, weak and latent ties and the  impact of new media. The Information Society, 18, 5, 385 - 401.  Haythornthwaite, C. 2002b. Building social networks via  computer networks: Creating and sustaining distributed learning  communities. In K.A. Renninger & W. Shumar, Building virtual  communities: Learning and change in cyberspace pp.159-190.  Cambridge, UK: Cambridge University Press.  Haythornthwaite, C. 2005. Social networks and Internet  connectivity effects.  Information, Communication and Society, 8,  2: 125-147.  Haythornthwaite, C. 2006. Learning and knowledge exchanges in  interdisciplinary collaborations. Journal of the American Society  for Information Science and Technology, 57, 8, 1079-1092.   Haythornthwaite, C. 2008. Learning relations and networks in  web-based communities. International Journal of Web Based  Communities, 4, 2, 140-158.  Haythornthwaite, C. 2009. Crowds and communities: Light and  heavyweight models of peer production. Proceedings of the 42nd  Hawaii International Conference on System Sciences. Los  Alamitos, CA: IEEE Computer Society.  http//hdl.handle.net/2142/9457.  Haythornthwaite, C. 2010. Social networks and information  transfer. In M.J. Bates & M.N. Maack Eds., Encyclopedia  of Library and Information Sciences, third edition, 1, 1, 4837- 4847. NY: Taylor & Francis.  Haythornthwaite, C. 2011. Online knowledge crowds and  communities. In Knowledge Communities. Reno, NV: Center for  Basque Studies. http://hdl.handle.net/2142/14198.  Haythornthwaite, C. & de Laat, M. in press. Social network  informed design for learning with educational technology. In A.D.  Olofsson & J. O. Lindberg, Eds.. Informed Design of Educational  Technologies in Higher Education: Enhanced Learning and  Teaching. IGI Global.   Hollingshead, A. B., Fulk, J., & Monge, P. 2002. Fostering  intranet knowledge sharing: An integration of transactive memory  and public goods approaches. In P. Hinds & S. Kiesler Eds.,  Distributed work:  New research on working across distance  using technology pp. 335-355. Cambridge, MA: MIT Press.  Lima, M. forthcoming. Visual complexity: Mapping patterns of  information. NY: Princeton Architectural Press.  Lin, N. 2008. A network theory of social capital. In Dario  Castiglione, Jan W. van Deth & Guglielmo WollebEds.. The  Handbook of Social Capital pp. 50-59. Oxford: Oxford University  Press.  Luff, P., Hindmarsh, J. & Heath, C. 2000. Workplace Studies:  Recovering Work Practice and Informing System Design.  Cambridge, UK: Cambridge Univ. Press  Montague, R-A. 2006. Riding the Waves: A Case Study of  Learners and Leaders in Library and Information Science  Education. Unpublished doctoral dissertation. University of  Illinois at Urbana-Champaign, Champaign, IL.  Preston, C. J. 2008. Braided learning: an emerging process  observed in e-communities of practice, International Journal of  Web Based Communities, 4, 2, 220-243.  Putnam, R. D. 2000. Bowling Alone: The Collapse and Revival of  American Community. NY: Simon & Schuster.  Scott, J., & Carrington, P. Eds.. 2010. Handbook of social network  analysis. London: Sage.  Trier, M. 2007. Virtual knowledge communities - IT-supported  visualization and analysis. Verlag.  Wasserman, S. & Faust, K. 1994. Social network analysis.  Cambridge, MA: Cambridge University Press.  Wegner, D. 1987. Transactive memory: a contemporary analysis  of the group mind. In B. Mullen & G. Goethals Eds., Theories of  Group Behavior pp. 185-208. New York: Springer-Verlag.  22    "}
{"index":{"_id":"4"}}
{"datatype":"inproceedings","key":"DeLiddo:2011:DLA:2090116.2090120","author":"De Liddo, Anna and Shum, Simon Buckingham and Quinto, Ivana and Bachler, Michelle and Cannavacciuolo, Lorella","title":"Discourse-centric Learning Analytics","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"23--33","numpages":"11","url":"http://doi.acm.org/10.1145/2090116.2090120","doi":"10.1145/2090116.2090120","acmid":"2090120","publisher":"ACM","address":"New York, NY, USA","keywords":"argumentation, discourse analysis, discourse analytics, learning analytics, sensemaking, social network analysis, web semantics","Abstract":"We present an analysis of activity on iSpot, a website supporting participatory learning about wildlife through social networking. A sophisticated and novel reputation system provides feedback on the scientific expertise of users, allowing users to track their own learning and that of others, in an informal learning context. We find steeply unequal long-tail distributions of activity, characteristic of social networks, and evidence of the reputation system functioning to amplify the contribution of accredited experts. We argue that there is considerable potential to apply such a reputation system in other participatory learning contexts.","pdf":"Discourse-Centric Learning Analytics  Anna De Liddo  1 , Simon Buckingham Shum  1 ,    Ivana Quinto 1,2  , Michelle Bachler 1 , Lorella Cannavacciuolo  2  1 Knowledge Media Institute, The Open University   Walton Hall, Milton Keynes  MK7 6AA, United Kingdom   Telephone number, incl. country code   A.DeLiddo@open.ac.uk,  S.Buckingham.Shum@open.ac.uk   2 Dept. Business & Management Engineering   Universita degli Studi Napoli Federico II  Piazzale Tecchio 80, 80125, Napoli, Italy   Telephone number, incl. country code   Ivana.Quinto@unina.it,  Lorella.Cannavacciuolo@unina.it        ABSTRACT  Drawing on sociocultural discourse analysis and argumentation   theory, we motivate a focus on learners discourse as a promising   site for identifying patterns of activity which correspond to   meaningful learning and knowledge construction. However,   software platforms must gain access to qualitative information   about the rhetorical dimensions to discourse contributions to   enable such analytics. This is difficult to extract from naturally   occurring text, but the emergence of more-structured annotation   and deliberation platforms for learning makes such information   available. Using the Cohere web application as a research   vehicle, we present examples of analytics at the level of   individual learners and groups, showing conceptual and social   network patterns, which we propose as indicators of meaningful   learning.     Categories and Subject Descriptors  J.1 [Computer Applications]: Administrative Data Processing    Education.    General Terms  Measurement, Design, Experimentation, Human Factors,   Languages, Theory.   Keywords  Learning Analytics, Discourse Analytics, Discourse Analysis,   Argumentation, Sensemaking, Social Network Analysis, Web   Semantics.   1. INTRODUCTION: LEARNING AND  DISCOURSE  A key indicator of meaningful learning is the quality of   contribution to discourse. As proposed by Mercer [1], a   sociocultural perspective on learning highlights the possibility   that educational success and failure may be explained by the   quality of educational dialogue, rather than simply in terms of the   capability of individual students or the skill of their teachers.   The way in which learners engage in dialogue is an indicator of   how they engage with other learners ideas, how they compare   those ideas with their personal understanding, and finally how   they account for their point of view, which is an explicit sign of   the stance they hold in the conversation.    The analysis of accounts in text is the primary focus of discourse   analysis. We look at discourse as a key indicator for learning and   explore discourse analysis as a method to identify where and how   learning happens. This approach to analyzing dialogue, in search   of clues and indicators of learning, builds on the tradition of   scholarly and scientific discourse. Gilbert and Muckay [2]   pioneered this field by analyzing the way scientists construct   their claims and defend their positions in a scientific dispute.   Information and data, together with experimental procedures,   subsequent results and theoretical developments are not   univocally observable, and can present a variety of significant   interpretations which may be conflicting, while at the same time   being equally valid. Within this space of multiple   interpretations, appropriate forms of rhetorical move in language   are the means by which scientists make their claims to contribute   to scientific debate and advance knowledge.    Discourse analysis focuses explicitly on language as social   action, and in that, discourse and argumentation are the tools   through which people can compare their thinking, explore ideas,   shape agreement, and identify or solve disagreements. We use   discourse to co-evolve and think together. Many human   activities involve not just the sharing of information and the   coordination of social interaction, but also a joint, dynamic   engagement with ideas amongst partners. When working   together, we do not only interact, we interthink [3]. Mercer   focuses attention on the relationship of dialogue processes to   outcomes: if discourse is the tool through which learners think   collectively, then discourse outcomes and discourse analysis can   provide indicators to better understand the learning processes.   While Mercer and others originally focused on spoken dialogue   as a tool for thinking collectively, their work has been shown   subsequently to provide insight into textual discourse in online   learning [4], providing a bridge to the world of online learning   analytics for knowledge building.   Argumentation Theory is a second strand of work on which we   build, given the importance of reflection and critical thinking in   learning discourse. While argumentation and rhetorical theory   date back to the Greek philosophers, we draw particular   inspiration from the foundational work of Walton in articulating   many of the argumentation schemes that we encounter in   everyday discourse [5]. The formalization of such schemes by      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK11, February 27-March 1, 2011, Banff, AB, Canada.   Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.     23    Walton, et al. for computational modeling [6] provides the   technical bridge required for learning analytics.    The third strand of work informing our approach derives from   Rittels formative work on Issue-Based Information Systems and   his argumentative method for sensemaking deliberation in   complex problem spaces [7]. Its subsequent translation by   Conklin [8] into a hypertext data model supporting dialogue   visualization provides the third key bridge for learning analytics.   We build on these three research strands and argue that if   learning dialogues and their outcomes are representative   indicators to better scaffold the learning process [1,3], then   argumentation theory [5,6] and argumentation tools [7] can   improve the ways in which those processes can be analyzed and   understood.    2. THE MOVE TO SEMANTICALLY  STRUCTURED ONLINE DISCOURSE  Written discourse in online learning environments is expressed   in diverse corpora such as learning journals, discussions,   reviews, and essays, rendered via myriad tools such as threaded   forums, blogs, wikis and social networking sites. Each tool   constrains and shapes the ways in which users can articulate   their thoughts and dialogue with one another. Those aspects   makes online discourse and dialogue quite different from face-to-  face dialogue and a specific clarification needs to be done of   what type of online dialogue is being considered, how people can   dialogue in those environments and how discourse and discourse   analysis may be interpreted in the specific space of online   communication which is being investigated.    The most established online dialogue environments render   discourse chronologically, rather than logically, reflecting most   strongly the sequence of contributions rather than their   conceptual structure (for instance a Facebook dialogue, Figure   1).           Figure 1. Extract of a real Facebook Dialogue.   Subject-line threading, tagging, or anchoring commentary to a   document structure do provide mechanisms, albeit weak, to   navigate thematically. Such environments minimize the effort   required by the user to add a contribution (they just click   comment or reply), but the trade-off is that important   phenomena that can signify more reflective learning   conversations are hidden in the free-text content, making it   harder for both participants, and the software platform, to   understand questions such as: What are the key issues raised in   the conversation What are the emerging questions How much   support is there for this idea Who disagrees, and what evidence   do they use What kind of argument is made to support this One   must simply read the entire online conversation, along with all   the noise which cannot be filtered.      Figure 2. Shows how Cohere augment common online   dialogue text: the icons at the side of each post shows   rhetorical role of the post and the semantic connections the   rhetorical move between posts.   In order to address these limitations, we are now seeing the   emergence of robust tools for more structured deliberation and   argument mapping [9-11], prefigured by extensive CSCL   research [12-13]. These tools are now finding application in   many forms of knowledge work which require clear thinking and   debate, including learning, scenario planning, and policy   formulation. As part of our broader conception of Hypermedia   Discourse [14], we have developed a web application called   Cohere [15], providing a medium for engaging in structured   online discourse, or for summarising/analysing it, e.g. as a   moderator, educator or researcher. Following the approach of   structured deliberation/argument mapping, Cohere renders   annotations on the web, or a discussion, as a network of   rhetorical moves: users must reflect on, and make explicit, the   nature of their contribution to a discussion. A simple example   illustrates how a Cohere discussion is different from typical   social network interaction, such as in Facebook (Figure1). In the   example dialogue (Figure 1), Van has proposed a dinner for the   10th of October which Miriam cannot attend, while Anna and   Aurelie can make it. From how the dialogue is represented, it is   not evident at first that the central dinner-invitation post is the   second one, nor what the positions of other participants are.    24       Figure 3. Coheres environment in which online dialogue is represented as semantic network of posts.   The only way to make sense of the conversation is by reading it   whole. While this is feasible with just three participants and 5   posts, this does not scale for complex debates with many   participants.    Cohere aims to address this problem with two key extensions:    Adding icon types to the posts    Making semantic connections between posts (see Figure  2).   With Cohere, users can pick an icon to associate to their post,   which explain the rhetorical role of that post in the wider   conversation (e.g. Van is raising and idea/option to make a   dinner on Friday, Miriam presents a con to the proposal: since   she has guests that w-e, Anna present a pro in favor of Van   proposal: there will be no Lab dinner in that w-e, etc) (Figure 2).   Moreover with Cohere users can explicitly connect their post to   the post which is relevant to what they want to say. They can do   so by making a connection between posts, which explain the   rhetorical move they want to make in the conversation (i.e. Anna   and Aurelie agree with Vans idea while Miriam disagrees)   (Figure 2).   Cohere augments the online conversation by making explicit   information on the rhetorical function and relationship between   posts. Moreover users can browse the online dialogue not as a   linear text but as a semantic network of posts (Figure 3).   By structuring and representing online discourse as semantic   network of posts Cohere enables a whole new way to browse,   make sense of, and analyze the online discourse. In this paper we   discuss what it mean to use Coheres online dialogue   environment to monitor online learning activities and develop   useful learning analytics, by starting on the analysis of the online   discourse which learners are involved in.   In particular in the next section we describe how discourse   analytics can enable a deeper understanding of the online   discourse, of the participants to the discourse and the social and   learning dynamics.   3. LEARNING ANALYTICS ON  DISCOURSE ELEMENTS  In the previous sections we have described why discourse is a   key indicator for learning (section 1), and we have described the   specific type and technology for online discourse we will be   focusing on: Cohere (section 2). In the following we present   several examples of simple learning analytics based on discourse   elements. These examples are intended to work as a proof of   concept for the potential of discourse-centered learning analytics   that is to say a focus on learners' discourse as promising site to   identify patterns of meaningful learning.   Cohere introduces two main discourse elements in an online   conversation:   1. The post type: which is represented with an icon   and label, and expresses the rhetorical role played   by the post in the wider online conversation (Figure   2 and 3);   2. The semantic connection: that is represented by a   link and label, and expresses the rhetorical move   the author of the post wanted to make in the   conversation, and toward a specific post or   participant (Figure 2 and 3).   25    In the following we describe what kind of learning analytics can   be done on those two discourse elements and give concrete   examples of how Cohere can provide learning analytics per   learner and per group, to identify:    learners attention: what do learners focus on What  problems and questions they raised, what comments   they made, what viewpoints they expressed etc.     learners rhetorical attitude to discourse contributions:  With what and who do a learner agrees/disagrees   What ideas he supports What data he questioned      learning topics distribution: What are the hottest  learning topics, by who they have been proposed and   discussed     learners social interactions: How do learners act  within a discussion group What are the relationships   between learners    4. ANALYTICS PER LEARNER  Cohere provides two main types of learning analytics: analytics   per learner and analytics per group. In the following we discuss   the main analytics per learner which consist of two tables (Node   Type table and Link type table); and two lists of connections   (comparing thinking and information brokering connection list).   We will use as example data the statistic for Rebecca, a semi-  experienced Cohere user, which has been also involved in one of   the use cases described in the following section (section 5).    4.1 Analytics on Post Types: Analyzing  Rhetorical Roles   The table on post types (called node types) counts, shows the   variety of types of posts that the learner has added to the   conversation, and with how many posts of each type he has   contributed. Statistics on node types measure the rhetorical role   of the comment that the learners are making, (e.g. Does she ask   many Questions Does she contribute Data, or just Ideas) and   therefore provide an indicator of learners attention and   performance.   For instance higher the number of created posts, higher the   learners engagement in the discussion: for instance a higher   number of theory type posts may indicate a learners interest in   theoretical issues. Statistics on the node type can be also   interpreted as a way to classify the role of that learner in the   group. For instance, does he work as the person providing   answer He could be imagined as a point of reference-tutor.    If we look at a concrete example, in Figure 4, Rebecca has   contributed mainly with ideas, general opinions, and she has   offered several international perspectives on the conversation.   We may also notice that in just three posts she has raised   questions.   By looking at the node type table it is possible to evaluate   learners performance connecting the discourse outcomes with   the specific learning goal. For instance, there may be learning   scenarios in which the learning goal is to share online resources,   therefore in those cases the scores on Data posts type would   offer a useful figure on how the learner performed in this task.   With the post types table, Cohere draws a picture on the kind and   quantity of contributions to the conversation that the learner has   given and therefore, in different learning scenarios, it may enable   inferences on how the learner has performed in the specific   learning task.       Figure 4. Node types table: shows the different typologies in   which the learner classified his posts, and the counts of how   many times each type had been used in the conversation.   4.2 Analytics on Link Types: Analyzing  Rhetorical Moves   The table on link types (Figure 5) describes the rhetorical   moves that the learner has made in the conversation. It gives the   list of all link types, used by that learner, to express his ideas and   connect them with other peoples ideas. The list will be ordered   as descending on the number of time the semantic link has been   used.    Three main elements can be observed by looking at this table: the   language the learner uses to describe his thinking, his attitude   toward the discussed topic, and how this language and attitude   are similar or different from other learners in the same group.   In order to evaluate learners attitude Cohere classifies semantic   links type into three categories: Positive, Neutral and Negative   (Figure 5). Positive link types are represented with a green link   and label, and express positive rhetorical moves such as i.e.   supports, agrees with, improves on, is consistent with, predicts,   proves, solves the problem etc.      Figure 5. Link types usage statistics. Arrows show three   connections categories - Neutral, Positive and Negative      26    These categories provide indicators of the attitude a learner had   toward the learning task and within the conversation. In example,   in the figure shown (Figure 5), we can see that the example   learner has maintained a positive stance within the conversation.   In fact, in Figure 5 green links score quite high into the table and   the most of the learners contributions to the conversation have   been devoted to identify: consistencies, coherences, answers to   questions raised by herself or other learners etc.               More precise statistics can be easily obtained from this table. For   example, the percentage of positive contribution can be   calculated as:    %P  is equal to the summation of the number of times that the   learner used a green link ( Ngl ), extended to all the green links   (g=1,..G; where G is the number of different green links type),   divided for the total number of links he has created (T), and   multiplied by 100.   This percentage can be used as an indicator of the learners   positive attitude toward the learning task, and toward other   learners, within the online discourse.   The same calculation can be repeated for neutral and   negative link types. This would give the following analytics for   our example learner (Table 1).   These statistics can be extended to all learners in a class or in a   group of inquiry and therefore compare their attitude within the   online conversation.   Table 1. Example of Link Type Analytics for a learner      %P: Percentage of   Positive rhetorical   moves   %N: Percentage   of Negative   rhetorical moves   %Ntl: Percentage   of Neutral   rhetorical moves    47.1 9.5 43.4   The second observation, that can be made analyzing the link   types table (Figure 6), concerns the learners language used to   describe their rhetorical moves. Different learners may give   different meaning, or nuances of meaning, to the same concept   and this may mirror in using different terms to express the same   concept, or the same term while referring to quite different   meaning.    If we look at Rebeccas list of rhetorical moves, we can notice   that she classified as gray (neutral) rhetorical moves which refer   to descriptive turns in the conversation, aiming to: i.e. identify   similarities, illustrate, give examples and discover relationships.   And the classification is quite coherent, in the sense that all the   link types she has chosen convey the same descriptive meaning.   In the same way, the green links seem to represent positive   moves such as: improvement, support, problem solving etc. We   may notice though that she classified as positive the relationship   causes, which has a quite ambiguous interpretation. In fact   other learners may have classified causes as neutral link type   since causal moves may imply both positive and negative   consequences and therefore the term does not bring per se a   positive interpretation. Presented with link analytics of this sort,   an educator might ask questions such as: Why did Rebecca   classify this term as green How did other users interpret this   rhetorical move   Comparison between different learners link types table, and   different analytics on the data can support the understanding of   those deeper reflections on learners use of language. For   example, Figure 6 shows how four users who all participated in   the same investigation, used the five link types which were   calculated to have highest usage within the group.          Figure 6. Comparing four users usage of link types.   Usage is a relative metric, which may be constrained to specific   users group, or to specific users type (I.e experts, or non-expert).   For instance, if we want to calculate the usage of a semantic link   type by the most expert Cohere users, we can calculate the   weighted summation of the number of time the link type has   been used from the top 20 users, following the formula:   Where:    i is the semantic link type, and varies from i=1,..n;   where n is the number of all the different semantic   connections type used by the top 20 Cohere users.    u is the user and vary from 1,..20 for the top 20 Cohere  users    Nui with u=1,..20 is the number of times the user u  has used the connection i;     and the weights wi are obtained by calculating the  topological matrix of semantic-type/user and summing   the row elements to obtain the number of users which   used that same link type.    In other words, for each link type i, the weight wi is  proportional to the number of users that used that link type. The   weight is a measure of popularity of each link type within the top   20 expert Cohere users, and the more popular is a link type the   more it will score in the Usage calculation. By applying this   formula we choose popularity as the main factor to determine if a   semantic link type is used, in other words we make sure that if   %P =  Ngl gl=1  G    T *100    1)      U i = wiNui u=1  20   .   (2)      27    just one user has used a semantic link type many times, but   nobody else has, this link type will not score high in the link type   usage.    4.3 Learners Attitude to Compare Thinking  An important aspect of learning is the capability of the learner to   think critically and reflect on his personal point of view by   comparing it with others. When a learner decides to connect his   idea with another persons idea this could serve as an indicator   of reflective thinking, insofar as the learner is assumed to have   read the target node, and reasoned about the relationship with his   own idea, in order to select a link type.    Cohere counts and lists for each learner all the links in which she   has connected his opinion to another persons contribution to the   online conversation (Figure 7). In the picture we can see that for   each connection there are three user icons, which represent:  the   links author (at the center under the semantic connection label)   and the authors of the two posts that are being connected.   Compared thinking statistics counts the connections in which the   link author (at the center) is also author of one of the two   connected posts.      Figure 7. List of semantic connections in which learners have   compared their thinking with other learners.   4.4 Learners as Information Brokers   Another analytic afforded by semantic discourse of this sort   concerns the degree to which users act as information brokers   between others.       Figure 8. List of semantic connections in which a learner   acted as information brokers.   Since connecting is an explicit, reflective act in Cohere, it is   straightforward to count how many times learners create   semantic connections between nodes authored by others. Those   connections are then listed as in Figure 8.      5. DISCOURSE NETWORK ANALYSES  AND VISUALIZATION: ANALYTICS PER   ONLINE DISCUSSION GROUP     Cohere calculates different kinds of statistics on online data   generated and shared by group members. The group statistics   summary shows two main factors:    Discourse element statistics, such as the most popular  link type and the most popular node type     Discourse network statistics, such as i.e. the node type,  the post and the learner with highest degree   centrality.   Let us elaborate the properties of the discourse network. Its   structure consists of two superimposed networks that are   assumed to be strongly connected:    Concept network  which relates the nodes that  learners created.    Social network  which relates learners that participate  to Cohere discussions posting Ideas, Questions and   Arguments etc;   For data analysis we considered that, in the concept network, the   posts are the nodes, and the semantic relations among posts   indicate the edges; whereas the social network maps the pattern   of relationships among actors. In particular, we considered the   users as nodes and we measured the edge between two users by   counting the times that a user created a semantic connection that   targeted a post authored by another user.    In the following tables, we show the different meaning that each   Network Analysis metric has in the social and concept network   adapted to our context.   In order to provide some concrete examples of how the above   network metrics can be used to analyze learners activities in   online discussion groups we present two use cases.    In the first use case (OLnet team discussion) Cohere has been   used by a group of researchers to annotate the document of a   project proposal, and to reflect on which areas of the proposal   they were making a contribution. With Coheres Firefox sidebar   users can annotate the document and share their annotation in the   group discussion environments. These annotations are initially   presented as list of posts presented in reverse chronological order   within the discussion group. After this initial phase of reading   and annotating the document, participants were asked to have a   group discussion on the main research questions addressed by the   team, the main project achievements and how they related to the   project goal. In order to do this, they had to create new posts in   which they described more general reflections on research   questions, goals and activities and then they had to start creating   semantic connections between the document annotations and the   posts (for more info on the Coheres user interface and how to   build semantic connections and discourse networks through   Cohere please refer to [16,17]).   This resulted into a discourse network in which document notes,   open questions, ideas and other posts type are connected, and   28    nodes icons and links express the rhetorical role and move   played by each post into the online discourse.    Table 2. Ego-network measurements   Metric Social Network Concept Network   Outdegree   Measures most   active users. It   measure the   activity of a user in   terms of how many   posts s/he has   linked   Measures the   attractiveness of a   post in terms of how   many posts it has   been linked to   Indegree   Measures Prestige   and   Expertise of the   learner within the   network   Measures the   attractiveness of the   post in terms of how   many incoming links   it has. It identifies   key targeted posts.   Degree   Centrality   (undirected   graph)   A measure of the   total number of   links that a node   has. A greater   number of links   implies an higher   power within the   network   Most interesting   topics; hottest topics   (by summing   indegree and   outdegree)   Eigenvector   centrality   Measures a node   importance by   taking  into account   not only how many   connections a   vertex has (i.e., its   Degree), but also   the Degree of the   vertices that it is   connecting to.   It considers first   depth learners to   which a given   learner is   connected to, when   to calculate learner   influence within   the network   A measure of a   nodes importance. It   considers the kind of   node to which a post   is connected to. The   Eigenvector   Centrality metric   takes into   consideration not   only how many   connections a vertex   has (i.e., its Degree),   but also the Degree   of the vertices that it   is connecting to   Table 3. Network measurements   Metric Social Network Concept Network   Presence    of   networks   components   Assesses the degree to   which a network is   disconnected. A social   network which is fully   connected has only one   component   Assesses the   number of different   subtopics discussed   in a group   Link   distribution   Assesses the presence   of hub users   Assesses the   presence of hub   topics   In the second use case (COP15 discussion) four researchers have   used Cohere to collaborative annotate web news, documents,   blog posts etc about the United Nation Climate Change   Conference COP15. Results of the web annotations have then   been used to inform an online dialogue on the main issues   tackled during COP15, as reported by the press or as micro and   macro blogged by participants to the conference.    In order to have a specific focus for the discussion participants   choose to discuss one of the public's top questions that have been   suggested on a Open University Platform (see page:   http://www.open.ac.uk/platform/join-in/your-votes/question-by-  popular/Climate%20Change); that is:    How do we know that climate change is real and we're   not just experiencing a weather cycle   Participants were asked to explore and annotate key Open   Educational Resources (OER) and Social Media pages (such as   Blogs, Wikis, Twitter streams, and web pages in general) with   ideas to help answering the tackled question. Moreover they   were asked to make connections between their ideas and other   participants ideas. In this process the main driving question and   the identified relevant OERs have been used as evidences to base   claims/ideas. This resulted in a web of ideas and annotated   resources on the issues at stake, meaningfully connected into a   discourse network.   In the next paragraph we will describe how statistic on discourse   network can provide insights on the contents of the group   discussion and on the conceptual and social interaction between   group members.   To analyze both the online group discussions and to compute   some of above mentioned Network Analytics, we used UCINET   tool [18]; instead we used NodeXL tool [19] for both concept   network and social network visualization.    5.1 Concept Network Analysis and  Visualization    5.1.1 Link distribution: Is the network topology hub  and spoke or random  The first analysis that we conducted on the datasets of the two   use cases looks at links distribution to assess the presence of hub   users and hub topics. The existence of hubs indicates the   presence of hot topics/posts or key/most-active users. Networks   hubs are nodes with the highest degree centrality. From the   analysis of link distribution, emerges that both OLnet and Cop15   discussion groups are characterized by a power law distribution.   The power law tail indicates that the probability of funding posts   with a large number of links is rather significant; this means that   the network connectivity is dominated by few highly connected   posts [20].   As illustrated in the two histograms below (Figure 9), in both the   network it is possible to identify a hub with a highest degree   followed by smaller ones. From the analysis emerges that the hub   is a post labeled #COP15 and classified as idea type. The hub   post has been connected to many other posts, which present   annotations of various web resources. The learner who created   the post was in fact using the hub to cluster those resources   under the # tag COP15. This highlights a use of Cohere in   which the learner, more than dialoguing is rather mapping out   his notes on web resources and then sharing them with the group   within the online discourse.   29    A different case is the OLnet discussion group, which highlights   a use of Cohere as tool for collective inquiry. In fact the   discussion presents two hubs, both with degree equal to 8:    What motivates registered users to learn in socio-  collaborative ways on OpenLearn  which was   classifies by the author as idea type post; and     How can we build a robust evidence base to support   and enhance the design, evaluation and use of OERs   which has been classified as a question type post.   The first thing that we can notice is that the two hubs are both   posts which present an open question to the group. This seems to   suggest that within all posts types, questions have a higher   discourse power, in that they trigger learners participation and   interactions. Of course more systematic observations on wider   and different online discussion groups are needed to   appropriately test this hypothesis. Other considerations can be   also done by looking at the hub posts type. The learner who   authored the first hub did not correctly classify the rhetorical role   of his post within the wider conversation.         Figure 9. Link distribution histograms for COP15 and OLnet Team discussion groups.     The post clearly states a question but it has been classified as   idea. This may be due to misunderstanding of the learning task   or to less confidence in the use of the technology; in any case this   observation would alert a tutor on the learner performance. This   example, in the same line with the analysis done in section 4.1,   highlight the value of using posts type to classify the rhetorical   role of the posts within the online conversation.    5.1.2 Components analysis to compare the  struceture of OLnet and Cop 15: Are the networks   connected  The second analysis which has been conducted on the two   datasets consists in assessing the presence of components. A   component is a connected subset that composes a disconnected   network. Within networks components there are no links/paths   between the nodes belonged to different components. Therefore   network components identify isolated subsets of people or topics   within the discourse network.   If we look at the social network this analysis assesses the degree   to which a network is disconnected: for instance: a social network   which is fully connected has only one component. While if we   look at the concept network the analysis assesses the number of   different subtopics discussed in a group and at the same   time.From the analysis emerges that both networks present   several components and this implies that the networks are   weakly connected. In details, COP15 group presents 9   components but the bulk of nodes belong to two components.   OLnet group presents 10 components but the bulk of nodes   belong to one component. The presence of components in each   group can be interpreted as the emergence of different sub-  discussions independent among them. Analyzing the size of each   component, (number of node in each component) emerges that   not all the sub-discussions are developed by learners in the same   way. Bigger components can be interpreted as hot sub-topics   which attracts a greater interest than others. We can also notice   that the number of posts in the group discussion may have an   influence on how hub topics distribute. For instance if we   compare the two groups we can notice that in more developed   discussion groups, such as COP15 group, two components absorb   the bulk of nodes (161 out of 178). While in a group with less   posts, as OLnet group, the bigger component absorb less than   45% of the total nodes. This could indicate that at the beginning   of the discussion, learners try to explore a wider deliberation   space talking about different aspects of the same topic; then   gradually, they start to focus on few sub-topics and to deepen   them. This hypothesis would need to be proof/disproved by more   in depth analysis, but consideration on the line of these give an   example of how analyzing network metrics can inform the   understanding of group dynamics. Finally specific network   visualizations can be drawn to focus on the main network   analysis metrics (Fig. 10).   Figure 10 shows results of the concept network visualization for   the OLnet Team discussion group, done with NodeXL [19].    In particular in Figure 10:    Edge shape depends on link type (Positive=solid line;  Neutral=dotted line, Negative=dashed lines)    Edge width depends on the frequency of the  relationship    Vertex size depends on the degree centrality.   30      Figure 10. Concept Network Visualization of OLnet team discussion).      5.2 Social Network Analysis and Visualization  One approach to studying collaborative environment, as well as   collaborative network, has been the application of Social   Network Analysis.   The phrase social network refers to the set of actors and the   ties among them. The network analyst would seek to model these   relationships to depict the structure of a group. One could then   study the impact of this structure on behaviour of the group   and/or the influence of this structure on individuals within the   group [21].    Focusing on depicting the structure of the network, the paper   applies the main structural measures of SNA to Cohere   discourse network in order to analyze the typology of network   which emerges from the online discussions.     In the following we present the Social Network Analysis (SNA)   for the OLnet discussion group. The SNA measurements that we   consider in our analysis are: out degree and in degree centrality.   We adapted the meaning of these two measures to our case,   indeed:    Out degree measures the users activity level;    In degree is a sort of indirect measure of quality and  relevance of a users posts.   In the table (Table 5), we show the results that emerge from the   analysis of OLnet group social network.    The node more active is Learner 1. Her outdegree is equal to 11.   It means that she creates 11 links among different posts. While,   Learner 6 is the learner with the higher indegree value. Her   indegree is equal to 11. It means that L6s posts are considered   more interesting and/or relevant by the group.    By using the network metrics detailed in Table 2 the SN of the   OLnet Team discussion group can be represented as follows   (Figure 11, NodeXL tools visualization [19]). In particular in   figure 11:    Link width indicates the frequency of relationship  (reply).    Edge shape indicates the link type (positive: solid line,  neutral: dotted line, negative: dashed line). The final   shape depends on the prevalence of one of two link   type.    Vertex size depends on the in-degree centrality of each  users (bigger node have higher out-degree centrality).   Vertex colour depends on the out-degree centrality (more black   sphere have a higher in-degree value instead grey node have   lower indegree value   Table 5. Outdegree and Indegree values for the Olnet Team   discussion (11 participants)   Learners Outdegre  e Learners   Indegre  e   L1 11 L6 11   L2 7 L3 7   L3 6 L1 5   L4 4 L4 4   L5 4 L2 4   L6 3 L8 4   L7 3 L5 2   L8 2 L7 2   L9 2 L11 2   L10 1 L10 1   L11 0 L9 1      31       Figure 11. Social network representation of the OLnet Team discussion group.   6. CONCLUSIONS  Drawing on Mercers socio-cultural discourse analysis and   argumentation theory, we have motivated a focus on learners   discourse as a promising site for identifying patterns of activity   which correspond to meaningful learning and knowledge   construction. However, in order for online discourse to deliver on   the promise of learning analytics, software platforms must gain   access to qualitative information about the pragmatic dimensions   of conversational contributions, that is, the rhetorical dimensions   to a contribution.    We identified the emergence in recent years of more-structured   deliberation platforms on the Web, descended from hypertext   research systems in the 1980s.    Users of such tools make explicit certain classes of information   which is very difficult to extract from naturally occurring text,   although we are now investigating computational linguistics tools   for detecting rhetorical gestures [22].    In particular, we are interested in the rhetorical role that a users   contribution is making to a document or conversation (e.g.   identifying a problem, responding to a query, challenging or   supporting a viewpoint, contributing new data), and the nature of   the connection to other contributions using semantic   relationships. Using the Cohere system as an experimental   vehicle, we have presented examples of learning analytics at the   level of individual learners and groups to better understand:    learners attention: by analyzing the specific type and  quantity of contributions to the conversation that a   learner gives it is possible to measure several aspects   of the learner performance (section 4);    learners rhetorical moves within the online discussion:  By analyzing the semantic connections between posts   we can enhance our understanding  on the different   ways in which learners participate to the conversation.   Moreover we can make consideration on what attitude   they have toward the discussed topic and the role they   play within the group (section 4);    learning topics distribution: by applying concept  network analysis we can identify what are the hottest   learning topics and by who they have been proposed   and discussed. Moreover we can see how topic and   subtopic distribute within the online conversation   (section 5);    learners social interactions: by applying SNA we can  map how learners  act within a discussion group, what   are the relationships between learners, where and how   much they interacted with each other, and who is the   key learner and why (section 5).   These examples dont aim to present in depth analysis of the   collected use case data, they are rather meant to give a proof of   concept of the potential impact of discourse-centric learning   analytics in the study of CSCL.  Based on those examples we   argue that discourse-centric learning analytics such as these   enable more in-depth reflections on learners activities than   would be possible to achieve with quantitative analysis of lower   level actions (such as mining of logs on how many resources are   downloaded, how many times they have logged into a system,   how many comments they have made, how long they have spent   on a task, etc).    This is partially due to the nature of the analyzed data.   Discourse-centric learning analytics are based on data that makes   explicit the learners cognitive context (e.g. what kind of   rhetorical move the learner wanted to make with a comment,   what meaning he gave to a connection, what contrasting   viewpoints he detected etc). By analyzing more richly expressed   and structured data, discourse-centered analytics can augment the   level of accuracy and the cognitive depth of the inferences that   can be made on where and how learning happens.    32    7. ACKNOWLEDGMENTS  We gratefully acknowledge the support of the William & Flora   Hewlett Foundation, through their funding of the Open Learning   Network Project (www.OLnet.org).   8. REFERENCES  [1] Mercer, N. 2004. Sociocultural discourse analysis: analysing   classroom talk as a social mode of thinking. Journal of   Applied Linguistics. 1(2), 137-168.   [2] Gilbert, G.N., Mulkay, M. 1984. Opening pandora's box: A   sociological analysis of scientists' discourse. Cambridge:   Cambridge University Press.   [3] Mercer, N. 2000. Words and Minds. London, Routledge.   [4] Ferguson, R. 2009. The Construction of Shared Knowledge   through Asynchronous Dialogue. PhD thesis, The Open   University. Chapter 2. pp.44-51. http://oro.open.ac.uk    [5] Walton, D. 1996. Argumentation Schemes for Presumptive   Reasoning. Lawrence Erlbaum Associates: NJ   [6] Walton, D., Reed, C., Macagno, F. 2010. Argumentation   Schemes. Cambridge: CUP.   [7] Kuntz, W., Rittel, H. 1970. Issues as Elements of   Information Systems. Working Paper No. 131. Institute of   Urban and Regional Development. U. California at   Berkeley.    [8] Conklin, J., Begeman, M. L. 1988. gIBIS: A Hypertext Tool   for Exploratory Policy Discussion. ACM Transactions on   Office Information Systems, 6 (4), 303-331.   [9] Klein, M., Iandoli, L. 2008. Supporting Collaborative  Deliberation Using a Large-Scale Argumentation System:   The MIT Collaboratorium. Directions and Implications of   Advanced Computing. Conference on Online Deliberation.   U.C. Berkeley, 26-29 June, 2008.   [10] Kirschner, P., Buckingham Shum, S., Carr, C. Eds. 2003.   Visualizing Argumentation. Springer-Verlag: London.   [11] ODET 2010. Online Deliberation: Emerging Technologies   Workshop, Fourth Int. Conf. on Online Deliberation (Leeds,   30 June2 July, 2010), http://olnet.org/odet2010    [12] Andriessen, J. Baker, M., Suthers, D. 2006. Arguing to   Learn: Confronting Cognitions in Computer-Supported   Collaborative Learning Environments. Kluwer.   [13] Scardamalia, M., Bereiter, C. 1994. Computer support for   knowledge-building communities. Journal of the Learning   Sciences, 3(3), 265-83.   [14] Buckingham Shum, S. 2006. Sensemaking on the Pragmatic   Web: A Hypermedia Discourse Perspective. Proc.   PragWeb'06: 1st International Conference on the Pragmatic   Web. ACM Digital Library: http://dl.acm.org   [15] Buckingham Shum, S. 2008. Cohere: Towards Web 2.0   Argumentation. In 2nd Int. Conference on Computational   Models of Argument, Toulouse. IOS Press: Amsterdam.   [16] De Liddo, A., Buckingham Shum, S. 2010. Cohere: A   prototype for contested collective intelligence. In ACM   Computer Supported Cooperative Work Conference   (CSCW2010): Workshop on Collective Intelligence in   Organizations, Feb 6-10, 2010, GA.   http://oro.open.ac.uk/19554   [17] De Liddo, A. 2010. From Open Content to Open Thinking.   In: World Conference on Educational Multimedia,   Hypermedia and Telecommunications (Ed-Media 2010), 29   Jun, Toronto, Canada. Available at:   http://oro.open.ac.uk/22283/.   [18] Borgatti, S. Everett, M., Freeman, L. 2002. UCINET 6.0 for   Windows, MA: Analytic Technologies.   http://www.analytictechn.com   [19] Smith, M., Shneiderman, B., Milic-Frayling, N., Rodrigues,  E.M., Barash, V., Dunne, C., Capone, T., Perer, A., Gleave,   E. 2009. Analyzing (Social Media) Networks with NodeXL.   In C&T '09: Proceedings of the Fourth International   Conference on Communities and Technologies. Springer.   [20] Barabasi, A.L., Albert, R., Jeong, H. 2000. Scale-free  characteristics of random networks: the topology of the   world-wide web. Physica A: Statistical Mechanism and its   Applications, 281(1-4), 69-77.   [21] Wasserman, S. Faust, K. 1994. Social Network Ananlysis.  Cambridge: CUP.   [22] Sndor, A. 2007. Modeling metadiscourse conveying the  author's rhetorical strategy in biomedical research abstracts.   Revue Franaise de Linguistique Applique 200(2):97  109.     33      "}
{"index":{"_id":"5"}}
{"datatype":"inproceedings","key":"Clow:2011:IAP:2090116.2090121","author":"Clow, Doug and Makriyannis, Elpida","title":"iSpot Analysed: Participatory Learning and Reputation","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"34--43","numpages":"10","url":"http://doi.acm.org/10.1145/2090116.2090121","doi":"10.1145/2090116.2090121","acmid":"2090121","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics, participatory learning, power laws, reputation, reputation systems, social networks, social objects, wildlife","Abstract":"In the world of recommender systems, it is a common practice to use public available datasets from different application environments (e.g. MovieLens, Book-Crossing, or Each-Movie) in order to evaluate recommendation algorithms. These datasets are used as benchmarks to develop new recommendation algorithms and to compare them to other algorithms in given settings. In this paper, we explore datasets that capture learner interactions with tools and resources. We use the datasets to evaluate and compare the performance of different recommendation algorithms for learning. We present an experimental comparison of the accuracy of several collaborative filtering algorithms applied to these TEL datasets and elaborate on implicit relevance data, such as downloads and tags, that can be used to improve the performance of recommendation algorithms.","pdf":"iSpot Analysed: Participatory Learning and Reputation  Doug Clow   The Open University   Walton Hall, Milton Keynes   MK7 6AA, UK   d.j.clow@open.ac.uk   Elpida Makriyannis  The Open University    Walton Hall, Milton Keynes  MK7 6AA, UK      e.makriyannis@open.ac.uk  ABSTRACT  We present an analysis of activity on iSpot, a website supporting  participatory learning about wildlife through social networking. A  sophisticated and novel reputation system provides feedback on  the scientific expertise of users, allowing users to track their own  learning and that of others, in an informal learning context. We  find steeply unequal long-tail distributions of activity,  characteristic of social networks, and evidence of the reputation  system functioning to amplify the contribution of accredited  experts. We argue that there is considerable potential to apply  such a reputation system in other participatory learning contexts.    Categories and Subject Descriptors  K.3.1 [Computers and Education]: Computer Uses in Education.   General Terms  Design, Theory   Keywords  Participatory learning; social networks; social objects; analytics;  power laws; wildlife; reputation; reputation systems.   1. INTRODUCTION TO ISPOT      Figure 1. The iSpot home page, showing images of latest   observations (top left), observations from this time last year   (bottom left), and the sign up/search/log in panel (right).      iSpot [1] is a website that enables learners to post observations of  wildlife, and get help with identifying what they have seen from  experts and other users.  The aim is to encourage learners to  engage with the natural world, to enable them to identify species  and hence to learn about what they have seen.   The main activity on iSpot is the posting of observations (see  Figure 2). Learners upload photos of wildlife they have seen, and  log when they saw it and where. If they are able, they also add an  identification: the name of what it is they have seen.  To help with  the quality of the identification, iSpot can look up scientific  names from common names, and check spelling with the  dictionary of UK species curated by the Natural History Museum.  Other users of the site  whether experts or beginners  can then  click agree on the identification if they think it is correct, or post  an alternative identification if they think it is not.         Figure 2. An observation on iSpot, showing the user who   submitted it (top), the image captured (just below), and an   identification, the number of agreements with that   identification, and links to the Encyclopedia of Life and the   NBN (shaded box at bottom).      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.   Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.     34    A sophisticated but simple-to-use reputation system, described in  more detail in section 2.2, gives users of the site feedback about  the expertise of other users. The site is organised into eight broad  taxonomic groups: Amphibians and Reptiles; Birds; Fungi &  Lichens; Fish; Invertebrates; Mammals; Plants; and Other  organisms.   The UN declared 2010 to be the International Year of  Biodiversity, to celebrate the diversity of life on earth and of the  value of biodiversity for human lives [2]. However, expertise in  correctly identifying observations, for baseline recording of  biodiversity, is in such short supply that automation has been  proposed [3]. But social networking, as harnessed in iSpot, can  make the process more efficient, and spread taxonomic  knowledge [4]. The goal of iSpot is not to produce new data, but  new data recorders.1       Figure 3. The iSpot ecosystem, showing the iSpot website   (centre) surrounded by users, experts, formal learning,   broadcasts, iSpot Keys, and mentors (around).   The iSpot website does not stand alone. It is part of an ecosystem  of activity specifically designed to support and encourage  learning at scale (see Figure 3). In the UK2 there are many  (largely) voluntary expert organisations who are the repository of  expertise in identification and recording of particular taxa of  wildlife. There are also many millions of enthusiastic viewers of  nature programmes. With strong links with BBC TV and radio  broadcasts, iSpot helps motivate users to take their interest  further, providing an informal learning space. Learning pathways  that take users beyond the website were also designed in from the  start. There are the expert organisations, who are generally very  keen to welcome and nurture new members. There is also a  carefully-designed route through to formal learning, via a short  open-entry university-level course called Neighbourhood Nature,  offered by the Open University in distance mode, which could  form the first step on a journey through to a related degree.   The ecosystem also includes a network of biodiversity mentors,  who engage in outreach activities in their local regions, taking  pro-active steps, particularly to reach under-represented and  under-privileged groups.                                                                     1 However, useful data has been generated: some expert   organisations working with iSpot are very interested in  transferring records to formal recording schemes.   2 At the time of writing, iSpot is funded to operate only in the UK  and Ireland. No geographic restriction is in place on use of the  site, but as species  and expertise in identification  vary so  considerably around the world, we are working to set up  versions of iSpot tailored to other areas.    Finally, there are iSpot Keys, which are designed to provide a  new, more learner-friendly way to identify species than traditional  dichotomous keys.   A key strategy of the project was to aim to do one thing well  (social networking to support learning about identification of  wildlife), leaving other sites to do other tasks more effectively. So  apart from the iSpot Keys, the site does not itself provide  significant help with identification, but links instead to other  excellent resources that are available online, such as field guides.  Similarly, the site does not contain information about particular  species, but provides direct links to the Encyclopedia of Life  (EOL) [5], an online reference which aims to cover all 1.9 million  species known to science.   2. DESIGN RATIONALE  2.1 General  iSpot is an example of what the MacArthur Foundation  describes as participatory learning [6]. The learner is an  active participant, engaging in authentic, open-ended  creative activities, developing their interest and passion.  This type of participatory learning is an excellent  complement to traditional approaches to Open Educational  Resources (OER), providing a motivating link between the  worlds of informal and formal learning [7].   The design of iSpot was underpinned by the notion of shared  social objects (see e.g. [8][9][10][11][12]): there was a focus on  sociality, rather than pure functionality; and on social interactions  mediated by a social object (the observation), rather than a pure  social network. Both the object (the observation) and the social  conversation related to it (the identifications, agreements and  comments) are important.   In a community of practice [13], members learn from legitimate  peripheral participation [14]. Preece and Shneiderman [15]  categorise successive levels of social participation in online  communities as reading, contributing, collaborating and leading.  An analysis of participatory activity on 50 sites [16] suggests that  users connect, participate and collaborate around a shared object,  transferring information and pooling knowledge within and  between communities, in four different modes:   1. Browsing, gathering and sharing content  2. Giving and receiving feedback and expertise  3. Collaborating and jointly deciding about actions  4. Sharing control with other members over the   community and content  A users participation is influenced by several hidden elements  (e.g. reciprocity, identity, habits, real-world probes), which may  not be immediately evident in their actions, but can be inferred by  analysing user reactions. Engestrm [17] compares such  underground activities to mycorrhizae: symbiotic associations  between the fungus and roots of a plant. We liken this process to a  fairy ring, a circle or arc of mushrooms, where the mycelia of a  fungus grows invisibly beneath the surface, but its presence can  be inferred from occasionally wildly fruiting mushrooms, and also  from careful examination of the surface. Figure 4 shows what we  call a fairy ring of participation, which informed the design of  iSpot and the analysis presented below.      35       Figure 4. Fairy rings of participation, showing the visible   layer of participation operating in four different modes,   underpinned by an invisible network of interactions.   All activity on iSpot is visible in the sense that there are no  private or restricted spaces. This is to maximise the potential for  learning through legitimate peripheral participation, and to reduce  the potential for unpoliced abuse.    The development process of iSpot was underpinned by this  theoretical understanding, and followed a simplified version of  the socio-cognitive software design approach [18][19][20],  drawing on aspects of Agile development [21]: extensive  consultation and envisioning with stakeholders, followed by rapid  iterative release informed by feedback from users.   2.2 The Reputation System And Learning  Analytics  In formal learning, a teacher (loosely defined) arranges learning  activities for students, and motivates and tracks their progress  through formative and summative assessment. Indeed, it is widely  attested that assessment defines the de facto curriculum  [22][23]. The results from assessment can also be used to inform  improvement of the learning activities. This feedback is very  important for both the learners (to improve their own learning)  and the teacher (to improve their teaching). In informal learning,  there is no teacher as such, although there may be someone who  arranges learning activities for others. The participants do not  usually have the same access to external feedback about their  learning.  Additionally, a teacher in formal learning can often track student  progress through direct observation of learning situations, whether  face to face or online. Learning analytics can be useful in making  learning visible in ways it would not otherwise be. Learning  analytics promise to be particularly useful for informal learning  contexts, where direct observation is even more problematic.   The iSpot reputation system was designed to support learning on  the site by providing a form of external feedback, recognizing and  rewarding the activities that the team wanted people to engage in  on the site.   There is a large literature on reputation systems (e.g. [24] [25]  [26]), which has informed the design of the iSpot system. The  purpose of any reputation system is to facilitate trust between  users, by making actions and feedback transparent, and  encouraging reciprocity.    Particular attention was given to incentives for users to provide  honest, positive feedback. Some users find negative feedback  disproportionately demotivating; some might seek to escape it by  creating a new, separate account. So no facilities for negative  feedback are provided.   One concern is the potential problem of users colluding to rate  each other positively, perhaps using multiple sockpuppet  identities. This is known to be very hard to defend against without  a central, trusted authority [27] that can identify a trusted user or  users to be the source of reputational score [28].3 iSpot uses just  this approach, using the network of experts from expert  organisations.       Figure 5. Part of an iSpot profile for a single user, showing   social reputation (Social Points), and then, for each iSpot   group, a star rating of icons, with a count of observations   added, identifications made, agreements received, and   agreements given. Note a single icon for Fish (denoting score >   0), a gold icon for Fungi and Lichens (denoting an expert),   and three icons for Plants (denoting score > 10)  see Table 1.   The use of experts as a source of reputation does not depend on  a particular disciplinary epistemology: for subjects outside hard  Science, Technology, Engineering and Medicine, it might be  more appropriate to describe them as what Northedge [29] would  call fully functional users of the discourse within a particular  specialist discourse community.   There are two aspects to the iSpot reputation system, which are  displayed together on the users profiles (Figure 5): social points,  and scientific scores.   Social points are gained for engaging in activity on the site: the  number of observations posted, the number of identifications  made, and the number of agreements received and given.  Even  these social measures are not entirely separate from  technical/scientific approval: agree appears superficially similar                                                                     3 Or for each user to calculate reputation scores for themselves    which is mathematically equivalent to them treating themselves  as a trusted user. This is unwieldy in a social network, since  different users would see different reputational scores for the  same user.   36    to a social networks like functionality, but here it carries the  connotation agree that this identification is correct, an arguably  less subjective judgement.   Scientific scores are intended to be a rough measure of expertise  in accurate identification. Because the skills needed for accurate  identification vary between species, the scientific scores are  recorded separately for each of the eight groups.   The scores are accumulated as follows: experts (known to the  iSpot team to be such) are given a starting score of 1000,4 and all  other users start with 0. If user A agrees with an identification of  user B, user Bs score increases by user As current score divided  by 1000. So, for instance, if an expert agrees with user Bs  identification, user Bs score will increase by 1000/1000 = 1. If  user B  with a score of 1  then agrees with an identification, that  will increase the score of that user by 1/1000. For any particular  identification, a user can accumulate a maximum of 1 to their  score.   Table 1. Relationship between score and icons shown.   Score 0 >0 >2 >10 >75 >500  1000  expert   Icons  Shown   0 1 2 3 4 5 Gold   The scientific scores are not shown directly, but indicated by a  number of icons representing the score, calculated on an  exponentially-increasing basis5 as shown in Table 1. These icons  are displayed on a users profile page (Figure 5). The relevant  icons are shown next to a users name in any context where their  reputation in that group is pertinent. So, for instance, it is shown  next to their name on an observation (Figure 2), and next to the  list of users who have agreed with an identification (Figure 6).   The agree mechanism is designed to encourage and reward  reciprocity on the site: nearly anyone can agree with an  identification, regardless of their knowledge, and give it a virtual  thumbs up. However, the reputation score calculated on the  basis of these agreements is not so egalitarian.   The site displays badges for users who have been identified by  the iSpot team as being members of expert organizations. This  provides a reputational marker for those experts, and also  provides a way for those expert organisations to connect with  interested users (the badge is a link to their organisations web  page). The experts thereby have an incentive to contribute  constructively to the community.  In addition, a key source of  expert contributors to the community is the iSpot project itself,  through the expert team members and through the network of  mentors.                                                                     4 The factor of 1000 was chosen based on many factors,   including: providing a clear initial distinction between known  experts and novices; providing scope for reputation to grow  over time; and making it very tough but feasible to achieve the  maximum score.   5 An exponential was chosen over a linear approach in  expectation of the highly non-linear pattern of activity and  reputation that is observed  see p.11.      Figure 6. Part the list of users who have agreed with a   particular observation of a Bird on iSpot, showing the names   of users who have agreed with the identification, icons   indicating their reputation in the Bird group (blue bird icons,   gold-circled bird icon), and relevant badges for expert   organisations (e.g. black butterfly on yellow square).   3. GENERAL LEARNING ANALYTICS  Having explained the design and functionality of iSpot, this paper  now presents empirical data to illustrate how participatory  learning and reputation unfold in a real setting. There are three  main sources of data: Google Analytics, active on the site since  before the launch on 27 June 2009; the database of the site itself;  and qualitative analysis of the activity on the site. The dataset  analysed covers the period from the earliest online availability of  iSpot on 29 September 2008 up to 4 October 2010. There is very  little activity before the public launch on 27 June 2009.  The dataset contains 6,487 users, 27,493 observations, 47,355  images, 33,088 identifications, and 83,029 agreements. Users  have posted an average (mean) of 4.2 observations; observations  have an average of 1.7 images and 1.2 identifications; and  identifications have an average of 2.5 agreements. However, some  of these averages disguise very asymmetrical distributions, as will  be shown.   The number of visitors to the site (Figure 7) shows many  interesting patterns. The number of visitors increases over time,  but shows a series of very sharp spikes. There is also a clear  weekly pattern of activity below the big spikes: the site is much  more heavily used during the working week than at weekends.    The very sharp spikes occur at the same time as iSpot received  coverage in the mass media.    The first is on 13 October 2009, when a news story about a six- year-old girl spotting (via iSpot) a rare species of moth for the  first time in the UK received considerable coverage in the  national press.   The next spike occurs on 6 April 2010, when a national radio  programme (Saving Species on BBC Radio 4, weekly audience  over 1m [30]) mentioned iSpot at length, with a call to action.  iSpot has an ongoing relationship with the programme. There are  subsequent spikes in traffic, on 22 June 2010 and 29 June 2010,  when iSpot was again featured with interviews with iSpot users  and an explicit call to action.   There is a further spike on 18 May 2010, when a popular  television broadcast (Springwatch on BBC2, audience 2.1m [31])  mentioned iSpot on air.   There is a clear difference in the pattern of activity around these  spikes. The first spike, around the moth story, shows little  ongoing effect after the initial interest. But the traffic spikes from  the broadcasts show evidence that a significant proportion of   37    those visitors became ongoing, long-term visitors and  contributors. Indeed, Tuesdays (the day Saving Species is  broadcast) are the busiest day of the week on the site.      Figure 7. Visitors to iSpot over time, from Google Analytics.   The data from Google Analytics are matched by activity apparent  in the database. Figure 8 shows the number of observations posted  to iSpot per month. The broad shape is similar, but the pattern is  smoothed out. There is some evidence of an overall seasonal peak  of activity, matching the number of species active and the  desirability of field work. (Only the hardiest insects and  naturalists are active in mid-winter.)      Figure 8. Observations posted to iSpot, by month posted, from   database.   Table 2 shows the activity on the site, broken down by group.  Invertebrates is by far the most active group, followed by Plants,  Birds, and Fungi & Lichens. The other groups have very low  activity by comparison.    There are interesting patterns in the numbers of users who post  observations, identifications, and agreements. For Invertebrates,  Fungi & Lichens, and to a lesser extent Plants, these figures fall  off fairly sharply. For other groups, including Birds, the decline is  much less steep. This may be because identification of common  species in Birds is easier for non-experts than in other groups The  seasonality apparent in the overall numbers of observations  (Figure 8 above) is revealed in finer texture when the data is   disaggregated by group as in Figure 9, which shows the number  of observations per month by the date of observation.6      Figure 9. Observations posted by month observed (not date   added to iSpot) for selected groups.   The peak for Invertebrates appears to come in July/August, but  for Plants slightly earlier. Fungi & Lichens peaks much later in  the year. These peaks parallel the abundance and ease of  identification of the species: plants are most easily identified  when in flower, and it is almost impossible to identify fungi when  they are not fruiting without specialist equipment. The Fungi and  Lichens peak in November 2009 may be the result of a related  survey on air quality, which encouraged observations of lichens.  The Birds group appears to have two annual peaks: it may be that  the spring peak is associated with an abundance of migratory  species, and the winter one with ease of observation (lack of  foliage) and lack of other observation opportunities.   Thus, the analytics here reveal the texture of the learning  experience, with strong echoes of real-world activity beyond the  website itself.    In this and subsequent sections, the paper will focus mainly on the  Invertebrates group, the largest and most active group, for reasons  of space. Other groups are largely similar in overall character.   The activity on the site is far from evenly distributed. Figure 10  shows clearly that the pattern of posting of observations in the  Invertebrates group is very unequal. Twelve users have posted  more than 200 observations; 194 users have posted fewer than  ten. The pattern in other groups is very similar, even in groups  with much lower activity.                                                                      6 NB The date of observation is different from the date the   observation was added to iSpot. Users often upload  observations going back some time  there are even two  observations in the dataset with an observation date in the late  1980s.   38    Table 2. Activity on iSpot, broken down by group, from database.                                 Figure 10. Invertebrates: Observations per user, ordered by   number of observations.   This type of long tailed steep distribution has long been attested  as a feature of social networks [32]. Anderson argues [33, p126]  that it requires three factors: 1. Variety (there are many different  sorts of things); 2. Inequality (some have more of some quality  than others); 3. Network effects, such as word of mouth and  reputation, which tend to amplify differences in quality. It seems  highly likely that all three factors are present on iSpot: the  reputation system is precisely designed to amplify differences in  quality and reflect that in scores.   However, the precise nature of these steep distributions is not  immediately clear. Figure 11 shows the same data as in Figure 10,  but presented on a log-log scale with a power law drawn for  comparison. Even if the data were a perfect fit to this straight line,  considerable care would need to be taken before one could claim  with any certainty to have discovered a power law [34]. The  distribution is steep, and has fat tails, as does a power law, but is  not a power law, or at the very least it is not the same power law  over all of the data.         Figure 11. Invertebrates: Observations per user, ordered by   number of observations, log-log plot, showing power law   (dotted line) with exponent of -1.3.   4. QUALITATIVE ANALYSIS  This section presents, for context, comparison and triangulation, a  brief qualitative analysis of the activity on iSpot in general, and in  two of the groups.   4.1 General Activity   In iSpot, experts, mentors and citizen scientists form the core  community, while novices, students from the Neighbourhood  Nature course, and nature enthusiasts form the peripheral clusters.  The core community helps create links between the different  groups by helping out, encouraging and opening up social  conversation in the forums and comments section. This  conversation shows a transfer of knowledge within and between  the different groups, and also with external communities (e.g.  Wikipedia, BugLife, British Beetles etc).   Initial motivations for commenting, adding an identification or an  agreement include content quality of the observed species (e.g.  cool photograph!), uniqueness (very rare, I understand it only  appears for 3 weeks each year in May and June.) and relevance  (The hummingbird hawk has it's very own They Might Be  Giants song). However, the level of activity and mode of   Group  Observ- ations   Identif- ications   Distinct users posting   Observ- ations   Identifications Agreements   Other 269 312 199 129 65% 191 96%   Birds 4109 4618 669 612 91% 543 81%   Invertebrates 12571 15657 1404 959 68% 649 46%   Fish 169 202 88 84 95% 115 131%   Amphibians &  Reptiles   446 517 254 224 88% 201 79%   Mammals 707 751 313 293 94% 261 83%   Plants 6347 7482 800 623 78% 480 60%   Fungi &  Lichens   3079 3549 568 444 78% 234 41%   39    participation of the different users is highly dependent on the  invisible layer of underlying elements and motivations (see  section 2.2). These include reciprocity (e.g. a user adding content  will stop participating if their activity is not reciprocated, while  activity that is reciprocated encourages them to add more content;  this is especially true for the new members and novices in the  scientific field of the group); real-world probes (students asked to  post observations as part of the Neighbourhood Nature course);  and identity (experts knowing each other will discuss their  findings and collaborate to find the right identification for the  species).   There is considerable evidence of interaction with layers of  community beyond the site itself (e,g. Tompot Blenny Look at  my flickr pictures, there's a BBC story about 10 million  ladybirds descending on a farm in Somerset, all my colleagues  now know what a nettle weevil looks like).   4.2 Invertebrates  The Invertebrates group shows activity in all four modes of  the fairy rings of participation model: browsing,  gathering and sharing content; giving and receiving  feedback and expertise; collaborating and jointly deciding  about actions; and sharing control with other members over  the community and content  The rare moth story (the news story in section 3) illustrates this:  an initial observation (My daughter found this strange moth on  our windowsill) sparked increased interest and activity, growing  spectacularly when it was stated that a new species had been  discovered (this is indeed the first British record of the  Euonymus Leaf Notcher.), and this knowledge was transferred  into the wider global community (a colleague in Taiwan, within  the moth's native distribution, also confirms it), and the  discovery secured a place in the Natural History Museum  collection  after the girl had taken it to Show And Tell at school.    There are many examples of learning in a social yet scientific  environment. One Neighbourhood Nature student and iSpot  beginner starts with a shy observation of a micro-moth and  request for help from the experts; this grows into several  consecutive activities, with the learner making more accurate and  scientific descriptions of the observed species and showing  increased confidence about identifications of other users  observations.    4.3 Fungi And Lichens  The Fungi and Lichens group contains one single key expert, plus  nature enthusiasts and Neighbourhood Nature students. This is a  small group with considerable activity, with users helping each  other to identify species and providing many agreements  again,  working in all four modes.    One learner joined the group as a course requirement for  Neighbourhood Nature, with little knowledge. Her activity and  progress is extraordinary, uploading images and making  identifications on her own observations but also on other users  observations. She links information from external sites to support  her arguments, receiving feedback and many agreements from  experts and other users. She improves so much that when an  expert makes an observation (white fungus on log with teeth like  outgrowths), she makes a suggestion that it is something  different (Phlebia rufa  Small crust-like fungus growing on  dead wood) that makes the expert rethink and alter their  identification (after [the] suggestion of Phlebia  I was   wondering if this one might be a Phlebia instead since it has the  pores of a suitable shape ). A key element in this social  conversation is reciprocity in the community around the learner,  including the experts support.    5. REPUTATION LEARNING ANALYTICS  The reputation scores on iSpot are, we believe, unique as a  measure of activity on a social network: they are designed to be a  proxy measure of skill in a particular task, rather than a more  social measure of acceptability. The analysis presented in this  section is exploratory, rather than complete.   5.1  Reputation Received  The reputation received by users on iSpot in terms of icons (as per  Table 1 above)  is shown in Table 3, along with the number of  accredited experts (gold icons).7 Across all groups, there is a  clear tapering off, with large number of users having earning a  single icon, and increasingly smaller numbers having earned  more.      Figure 12. Invertebrates: log plot of reputation received,   ranked by reputation received, showing clear discontinuity at   reputation < 1.0.   The reputation score accumulated by individual users in the  Invertebrates group is shown (logarithmically) in Figure 12. As  with the number of observations, it is clearly a sharply unequal  distribution. There is also a sharp discontinuity in the curve. This  occurs between a reputational score of 1.0 or higher, between N =  618 and N = 619 in the ranked data. The users with a score below  1.0 have not had any expert agree with their identifications, since  a single experts agreement will add 1.0 to the score. It is perhaps  unsurprising that there is a difference in the dynamics of score  between those who have received some reputation from experts,  and those who have only received it from other non-experts.                                                                       7 Not all of these experts are actively engaged on the site in any   given period of time.   40    Table 3. Distribution of reputation icons earned by group   Icons Birds Invert-  ebrates   Fish Amphi-  bians &   Reptiles   Mam-  mals   Plants Fungi &   Lichens   1 326 322 29 127 158 283  171   2 131 233 6 56 77  148  67   3 50 106  1 2 2 35  8   4 - 26 - - - 1 -   5 - - - - - - -   gold 4 22 2 3 1 13 4      Figure 13. Invertebrates: reputation received, log-log plot,   first 618 users (reputation score  1.0), showing power law   (dotted line) with an exponent of -1.4.   The two sections of these data are shown separately in Figures 13   and 14. The shape of the graph for those with reputation  1.0  (Figure 13) is not a power law (i.e. not a straight line), but it is  very clearly long-tailed. For the population with reputation < 1.0,  yet to come to the attention of experts, a logarithmic relationship  appears to be a good fit (Figure 14). These differently shaped  distributions are consistent with the patterns of activity being  different between the two subpopulations.      Figure 14. Invertebrates  reputation received for users with   reputation < 1.0, showing logarithmic curve fit (solid line)   giving y = -0.197ln(x) + 0.8317, R2 = 0.98. NB Not log plot.   The data for the other groups are strikingly similar: there is a  distinct discontinuity between the population with scores  influenced directly by experts, and those without.    5.2 Reputation Given      Figure 15. Invertebrates: Reputation given ordered by   reputation given, log-log plot, showing power law (dotted line)   with exponent of -3.6.   The reputation given by a user is the sum of the amount by which  they have increased other users scores by clicking agree on an  identification. This quantity is not shown on the site in any way.  The sum total of reputation given will exceed the sum of  reputation gained, since an individual identification can only gain  the user a maximum of 1.0 point, but many experts may agree  with the same identification, which is counted as reputation given  of 1.0 points times the number of agreements made.    Figure 15 shows the distribution of reputation given for  Invertebrates. Once again, the data do not fit a power law, but  they are clearly very unequal. The sharpness of the fall-off is  much higher than for the distribution of observations and  reputation received. The data do not observe power laws, but the  exponent of a fitted power law can give a (very) rough indication  of the sharpness of the decay, yielding -1.3 for the observations  (Figure 11), -1.4 for the reputation received (Figure 13), and -3.6  for reputation given (Figure 15): a dramatically steeper curve.   The data for other groups are very similar.   In summary, the reputation given on the site follows a very, very  steeply unequal distribution, even by comparison with  observations posted and reputation gained. This is consistent with  the intended purpose of the reputation system in magnifying the  impact of known experts on the distribution of reputation.    41    5.3 Reputation Given vs Reputation Received        Figure 16. Agreements received against agreements given for   Invertebrates, log-log plot, showing fitted power law (dotted   line) with exponent 0.57 and R2 = 0.47.   Is there a relationship between the amount of reputation  given and received   Figure 16 shows a log-log plot of the number of agreements. The  data suggest that if there is a relationship, it is steep and very  widely scattered, particularly for lower numbers of agreements: a  power law explains less than half of the variance. For reputation  given and received (Figure 17), the picture is similar: such  relationship as there is is very non-linear, and the data are widely  spread and not well explained by a power law.   So there may be some correlation between agreements given and  received, and between reputation given and received, but the  relationships are different, not remotely linear, and very highly  variable: most users give far more agreements/reputation than  they receive, or vice versa.    6. DISCUSSION  This exploration of analytics data on iSpot reveals both the  gross, large-scale picture of participatory learning activity  on the site, and some of the more fine-grained, nuanced  texture of activity, illustrated by the qualitative sketches.  Patterns of activity are influenced by the broader context of  the site, as in the spikes of traffic from broadcasts, the  baseline growth of activity over time, and seasonality.  The distribution of activity on the site is highly unequal: that is, a  small number of users account for a very large amount of activity,  and a large number of users each account for only a very small  amount of activity. This pattern is consistent across types of  activity and across groups. It is strongly indicative of a  functioning social network. The focus on the shared social object  in iSpot enables social interaction in a scientific mode.      Figure 17. Reputation received against reputation given for   Invertebrates, log-log plot, showing fitted power law (dotted   line) with exponent 0.345 and R2 = 0.62.   The presence of experts has a noticeable and dramatic effect not  just on the quantity of reputation received, but fundamentally on  its distribution. The iSpot reputation system is amplifying the  contribution of accredited experts on the reputational scores of the  users of the site, without significantly inhibiting non-experts from  participating and agreeing with identifications.   The iSpot reputation system allows learners to track their own  abilities and that of other users, and appears to be extremely  powerful, motivating and useful.   There is considerable potential to apply a similar reputation  system in any participatory learning situation or domain,  providing: skilled, authentic judgements can meaningfully be  made by users on the data available; it is easy to indicate  agreement with the judgement; there is a degree of consensus  among an expert community about the correctness of  judgements; and the network of users can be seeded with  relatively uncontroversial experts who have an appropriate  incentive to help.   The dataset presented here warrants considerable further study. In  particular, we intend to probe the nature of the long-tailed  distributions more closely, to compare these data with social  network analysis, to explore the degree to which learning can be  identified more explicitly in the data, and to carry out deeper  qualitative research in to the activity on the site.   7. ACKNOWLEDGMENTS  We are grateful to the funders: iSpot is provided by The Open  University as part of the OPAL project (www.opalexplorenature),  which is funded by the UK National Lottery through the Big  Lottery Fund. We are also grateful to Jonathan Silvertown, the  inventor and leader of iSpot, and to the rest of the iSpot team,  including Janice Ansine, Mike Dodd, Richard Greenwood, Martin  Harvey, Richard Lovelock, Donal ODonnell, Jonathan  Silvertown, Jenny Worthington, and all others who have  contributed to the success of the project.   8. REFERENCES  [1] iSpot, http://www.ispot.org.uk   42    [2] UN Convention on Biological Diversity,  http://www.cbd.int/2010/welcome   [3] MacLeod, N., Benfield, M., Culverhouse, P.: Time to  automate identification. Nature 467, 154155 (2010)   [4] Silvertown, J.: Taxonomy: include social networking. Nature  467, 788-788 (2010)    [5] Encyclopedia of Life, http://www.eol.org   [6] Digital Media and Learning Competition: Reimaging  Learning,  http://www.dmlcompetition.net/reimagining_learning.php   [7] McAndrew, P., Scanlon, E., Clow, D.: An Open Future for  Higher Education. Educause Q. 33(1) (2010)   [8] Knorr-Cetina, K.: Objectual Practice. In: The practice turn in  contemporary theory, Eds. Theodore R. Schatzki, Karin  Knorr-Cetina, Eike von Savigny, pp. 175-188, London and  New York: Routledge (2001)   [9] Why some social network services work and others dont   Or: the case for object-centered sociality,  http://www.zengestrom.com/blog/2005/04/why-some-social- network-services-work-and-others-dont-or-the-case-for- object-centered-sociality.html   [10] Bouman, W., Hoogenboom, T., Jansen, R., Schoondorp, M.,  de Bruin, B., Huizing, A.: The Realm of Sociality: Notes on  the Design of Social Software. PrimaVera Working Paper,  University of Amsterdam (2007)    [11] Conole, G., Culver, J., Williams, P., Cross, S., Clark, P.,  Brasher, A.: Cloudworks: social networking for learning  design. In: Hello! Where are you in the landscape of  educational technology Proc. ascilite Melbourne (2008)   [12] Conole, G., Culver, J.: The design of Cloudworks: Applying  social networking practice to foster the exchange of learning  and teaching ideas and designs. Comput. Educ., 54(3), 679 692 (2010)   [13] Wenger, E.: Communities of Practice: Learning, Meaning  and Identity. Cambridge University Press, Cambridge (1998)   [14] Lave, J., Wenger, E.: Situated Learning: Legitimate  Peripheral Participation. Cambridge University Press,  Cambridge (1991)   [15] Preece, J., Shneiderman, B.: The Reader-to-Leader  Framework: Motivating Technology-Mediated Social  Participation. AIS T. HCI. (1) 1, 13-32 (2009).    [16] Makriyannis, E., Deliddo, A.: Fairy Rings of Participation:  The invisible network influencing participation in online  communities. In: Proc. 7th Conf. on Networked Learning.  University of Lancaster, UK (2010)   [17] Engestrm, Y.: From Teams to Knots: Activity Theoretical  Studies of Collaboration and Learning at Work. Cambridge  University Press, Cambridge (2009)   [18] Sharples, M., Taylor, J., Vavoula, G.: A theory of learning  for the mobile age. In: Andrews, R., Haythornthwaite, C.   (Eds.), The Sage handbook of e-learning research, pp. 221 247. Sage, London (2007)   [19] Vavoula, G.N., Sharples, M.: Future technology workshop:  A collaborative method for the design of new learning  technologies and activities. Int. J. CSCL. 2(4), 393419  (2007)   [20] McAndrew, P., Taylor, J., Clow, D.: Facing the challenge in  evaluating technology use in mobile environments. Open  Learning, 25(3), 233249 (2010)   [21] Manifesto for Agile Software Development,  http://agilemanifesto.org   [22] Elton, L.: Assessment for Learning. In: Bligh, D. (Ed):  Professionalism and flexibility in learning, pp. 106-135,  SRHE Leverhulme Programme, Surrey (1982)   [23] Rowntree, D.: Assessing Students: How shall we know them  Routledge, London (1987)   [24] Resnick, P., Zeckhauser, R., Friedman, E., Kuwabara, K.:  Reputation Systems: Facilitating Trust in Internet  Interactions. CACM 43(12), 45-48 (2000)   [25] Lampe, C., Resnick, P.: Slash(dot) and burn: distributed  moderation in a large online conversation space. In: Proc.  ACM CHI 2004 Conference on Human Factors in   Computing Systems, 543  550. ACM, New York (2004)   [26] Dellarocas, C. N., Fan, M., Wood, C. A.: Self-Interest,  Reciprocity, and Participation in Online Reputation Systems.  MIT Sloan Working Papers No. 4500-04 (2004)   [27] Douceur, J. D.: The Sybil Attack. In: Druschel, P.,  Kaashoek, F., Rowstron, A. (Eds.): IPTPS 2002. LNCS, vol.  2429, pp. 251-260. Springer, Heidelberg (2002)   [28] Cheng, A., Friedman, E.: Sybilproof Reputation  Mechanisms. In: Proc. ACM Workshop on Economics of  Peer-to-Peer Systems, pp. 128132. ACM, New York (2005)   [29] Northedge, A.: Organising Excursions In To Specialist  Discourse Communities: A Sociocultural Account of  University Teaching. In: Wells, G., Claxton, G. (Eds.),  Learning for Life in the 21st Century, pp. 252 -263. Wiley- Blackwell, Oxford (2002)   [30] Production team estimate based on RAJAR figures, private  communication.    [31] Broadcast Audience Research Board (BARB) Viewing  Figures,  http://www.barb.co.uk/report/weeklyTopProgrammesOvervi ew   [32] Power Laws, Weblogs, and Inequality,  http://www.shirky.com/writings/powerlaw_weblog.html   [33] Anderson, C.: The Long Tail: How endless choice is creating  unlimited demand. Random House, London (2006)   [34] Clauset, A., Shalizi, C.R., Newman, M.E.J.: Power-law  distributions in empirical data. SIAM Review 51(4), 661-703  (2009)      43      "}
{"index":{"_id":"6"}}
{"datatype":"inproceedings","key":"Verbert:2011:DRI:2090116.2090122","author":"Verbert, Katrien and Drachsler, Hendrik and Manouselis, Nikos and Wolpers, Martin and Vuorikari, Riina and Duval, Erik","title":"Dataset-driven Research for Improving Recommender Systems for Learning","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"44--53","numpages":"10","url":"http://doi.acm.org/10.1145/2090116.2090122","doi":"10.1145/2090116.2090122","acmid":"2090122","publisher":"ACM","address":"New York, NY, USA","keywords":"datasets, evaluation metrics, recommendation algorithms, technology enhanced learning","Abstract":"We consider the problem of predictive and causal modeling of data collected by courseware in online education settings, focusing on graphical causal models as a formalism for such modeling. We review results from a prior study, present a new pilot study, and suggest that novel methods of constructing variables for analysis may improve our ability to infer predictors and causes of learning outcomes in online education. Finally, several general problems for causal discovery from such data are surveyed along with potential solutions.","pdf":"Dataset-driven Research for Improving Recommender Systems for Learning  Katrien Verbert Department of Computer  Science, K.U.Leuven Celestijnenlaan 200A  B-3001 Leuven, Belgium katrien@cs.kuleuven.be  Hendrik Drachsler Open University of the Netherlands (OUNL)  P.O. Box 2960, 6401 DL Heerlen, The Netherlands  hendrik.drachsler@ou.nl  Nikos Manouselis Agro-Know Technologies,  Athens, Greece and  University of Alcala, Spain nikosm@ieee.org  Martin Wolpers Fraunhofer Institute for Applied Information Technology (FIT) Schloss Birlinghoven, 53754  Sankt Augustin, Germany martin.wolpers@fit.fraunhofer.de  Riina Vuorikari European Schoolnet (EUN)  Rue de Trves, 61 1040 Brussels, Belgium  riina.vuorikari@eun.org  Erik Duval Department of Computer  Science, K.U.Leuven Celestijnenlaan 200A  B-3001 Leuven, Belgium erik.duval@cs.kuleuven.be  ABSTRACT In the world of recommender systems, it is a common prac- tice to use public available datasets from different applica- tion environments (e.g. MovieLens, Book-Crossing, or Each- Movie) in order to evaluate recommendation algorithms. These datasets are used as benchmarks to develop new rec- ommendation algorithms and to compare them to other al- gorithms in given settings. In this paper, we explore datasets that capture learner interactions with tools and resources. We use the datasets to evaluate and compare the perfor- mance of different recommendation algorithms for learning. We present an experimental comparison of the accuracy of several collaborative filtering algorithms applied to these TEL datasets and elaborate on implicit relevance data, such as downloads and tags, that can be used to improve the performance of recommendation algorithms.  Categories and Subject Descriptors H.3.3 [Information Search and Retrieval]: Information filtering; K.3.m [computers and education]: Miscella- neous  Keywords Recommendation algorithms, Technology Enhanced Learn- ing, datasets, evaluation metrics  1. INTRODUCTION Recommender systems have been researched and deployed  extensively over the last decade in various application areas,  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK11 February 27-March 1, 2011, Banff, AB, Canada. Copyright 2011 ACM 978-1-4503-1057-4/11/02 ...$10.00.  including e-commerce and e-health. Several recommenda- tion algorithms, such as content-based filtering [28], collab- orative filtering [9] and their hybridizations [3] are widely discussed in the literature and in several surveys of the state-of-the-art. Also in the Technology Enhanced Learn- ing (TEL) domain, the deployment of recommender systems has attracted increased interest during the past years [19]. By identifying suitable learning resources from a potentially overwhelming variety of choices [32], recommender systems offer a promising approach to facilitate both learning and teaching tasks.  Whereas several recommender systems have been imple- mented for use in learning scenarios in recent years, only a few researchers have attempted to validate their recommen- dation algorithms based on data that have been captured in a real-life setting [19]. In many cases, small-scale experi- ments are conducted in which a few learners or teachers are asked to rate the relevancy of suggested resources in a con- trolled experiment. Whereas such experiments offer valuable insights into the usefulness and relevancy of recommender systems for learning, stronger conclusions about the validity and generalizability of scientific experiments could be drawn if researchers have the possibility of verification, repeatabil- ity, and comparisons of results based on large datasets that capture learner interactions in real settings [6]. Such a col- lection would enable researchers to create repeatable experi- ments to gain valid and comprehensive knowledge about how certain recommendation algorithms performed on a certain dataset and in certain learning settings.  To collect relevant TEL related datasets, the first data- TEL Challenge1 was launched as part of the first work- shop on Recommender Systems for TEL (RecSysTEL) [18], jointly organized by the 4th ACM Conference on Recom- mender Systems and the 5th European Conference on Tech- nology Enhanced Learning (EC-TEL 2010) in September 2010. In this call, research groups were invited to submit existing datasets from TEL applications that can be used for research purposes, among others for research on recom- mender systems for TEL. In this paper, we briefly present  1http://adenu.ia.uned.es/workshops/recsystel2010/datatel.htm  44    the collected datasets and evaluate the performance of sev- eral collaborative filtering algorithms on the datasets. The paper has three primary research contributions:  1. First, we present an analysis of datasets that capture learner interactions with tools and resources in TEL settings. These datasets can be used for a wide variety of research on learning analytics.  2. Second, the paper presents an experimental compar- ison of the accuracy of several collaborative filtering algorithms applied to TEL datasets.  3. Third, we research the extent to which implicit feed- back of learners, such as reading information, down- loads and tags, can be used to augment explicit rele- vance evidence in order to improve the performance of recommender systems for TEL.  The paper is organized as follows: Section 2 presents an analysis of datasets that capture learner interactions and that can be used for learning analytics. Section 3 presents an overview of existing recommendation algorithms, and in particular collaborative filtering algorithms, that can be ap- plied to these datasets to suggest relevant resources to learn- ers or teachers. Section 4 presents an overview of evaluation metrics that are commonly used to evaluate recommenda- tion algorithms. Then, we present our evaluation results of the application of these algorithms to TEL datasets. We evaluate algorithms based on both explicit rating data and implicit relevance data, such as tags and downloads, that are available in some datasets. Results and opportunities for future research in this area are discussed in Section 6. Conclusions are drawn in Section 7.  2. DATATEL CHALLENGE In this section, we present the objectives and results of  the first dataTEL challenge that was targeted to collect TEL datasets. These datasets capture user interactions with tools and resources in learning settings and can be used for vari- ous purposes in the learning analytics research area. In this paper, we focus on the application of these datasets to vali- date recommendation algorithms and to tackle challenges to support recommendation for learning.  2.1 Objectives In the world of recommender systems, it is a common prac-  tice to use public available datasets from different applica- tion environments (e.g. MovieLens, Book-Crossing, or Each- Movie) in order to evaluate recommendation algorithms. These datasets are used as benchmarks to develop new rec- ommendation algorithms and to compare them to other al- gorithms in given settings [6].  In such datasets, a representation of implicit or explicit feedback from the users regarding the candidate items is stored, in order to allow the recommender system to produce a recommendation. This feedback can be in several forms. For example, in the case of collaborative filtering systems, it can be ratings or votes (i.e. if an item has been viewed or bookmarked). In the case of content-based recommenders, it can be product reviews or simple tags (keywords) that users provide for items. Additional information is also required, such as a unique way to identify who provides this feedback (user identifier) and upon which item (item identifier). The  user-rating matrix used in collaborative filtering is a well- known example [9].  Although recommender systems are increasingly applied in TEL, it is still an application area that lacks such pub- licly available and interoperable datasets. Although there is a lot of research conducted on recommender systems in TEL, they lack datasets that would allow the experimental evaluation of the performance of different recommendation algorithms using comparable, interoperable, and reusable datasets. This leads to awkward experimentation and test- ing such as using datasets from movies in order to evalu- ate educational recommendation algorithms. This practice seems to lack the necessary validity for proving recommen- dation algorithms for TEL [17].  To this end, the dataTEL Theme Team of the STEL- LAR Network of Excellence2 launched the first dataTEL Challenge that invited research groups to submit existing datasets from TEL applications that can be used as input for TEL recommender systems. A special dataTEL Cafe event took place during the RecSysTEL 2010 workshop in Barcelona to discuss the submitted datasets and to facilitate dataset sharing in the TEL community.  2.2 Collected Datasets Seven datasets have been collected as a result of the first  dataTEL challenge. In this paper, we use datasets that include usage related data (such as ratings, tags, reads or downloads) as a basis to demonstrate and evaluate recom- mendation algorithms for learning. We present an overview of datasets that include such usage data, including infor- mation on the data elements that are available and basic statistics of the number of resources, users and activities that are stored.  Some of these datasets are already publicly available, whereas others are still under preparation and not yet publicly ac- cessible. An up-to-date overview of datasets is available at the dataTEL website3. We expect an increasing amount of learning related datasets in the upcoming year.  2.2.1 Mendeley dataset The first dataset was submitted by Mendeley [11] and in-  cludes usage data of papers that are available through the Mendeley scientific portal4. Mendeley is a research platform that helps users to organize research papers and collaborate with colleagues. In the context of learning, such a dataset provides useful data for recommender systems that suggest papers to learners or teachers, or suitable peer learners on the basis of common research or learning interests. Exam- ples of paper recommenders that have been evaluated in TEL settings are InLinx (Intelligent Links) [1], Papyres [25] and pioneering work on the application of recommender sys- tems in TEL conducted by Tang and McCalla [30]. Although research on paper recommenders has been elaborated more extensively in the Research2.0 domain that emerged in re- cent years, the dataset is currently one of the few available datasets that captures a very large number of user activities. This dataset can be used meaningfully for research on TEL recommender systems in contexts where papers are consid- ered as learning resources. Three files are included in the Mendeley dataset that capture data since 2009:  2http://www.teleurope.eu/pg/groups/9405/datatel/ 3http://www.teleurope.eu/pg/pages/view/50630/ 4http://www.mendeley.com/  45     Online article view log. The online article view set include a random sampling of 200.000 users that are extracted from usage logs. Time at which each view occurred is provided.   Library readership. The library readership set includes 41.220 user libraries that contain more than 20 arti- cles. From the 13.313.548 library entries, 2.655.578 (19.95%) have been read by users.   Library stars. The library stars set provides data on articles that have been starred by users. 186.976 (1.40%) of the 13.313.548 library entries have been starred.  Among others, this dataset is useful for research on (1) ex- traction of users interests, on the basis of articles that have been starred, read or added to libraries by users, and evolu- tions in these interests on the basis of time recordings, (2) identification of users who share common interests, on the basis of their usage behavior, and (3) identification of im- plicit quality/relevance indications of individual articles by analyzing their usage data.  2.2.2 APOSDLE-DS dataset The APOSDLE-DS dataset originates from the APOS-  DLE project [16], which ran from March 2006 to February 2010. APOSDLE is an adaptive work-integrated learning system that aims to support learning within everyday work tasks. It recommends resources (documents, videos, links) and colleagues who can help a user with a task.  The dataset captures 1500 user activities of 6 users dur- ing an evaluation period of 3 months. The activities cap- tured are perform task, view resource, edit annotation, per- form topic, selected learning goal, adapting experience level, adding resource to collection, browse data, being contacted, contacting person and creating new learning path. The dataset also includes 163 descriptions of documents and document fragments on which these activities were performed.  From the collected data, the adding resource to collection action can provide direct information about the relevance of a resource. This action occurred 581 times within the evaluation period. Creating a new learning path is consid- ered as an attempt to plan learning activities over a longer time period and can provide a solid basis for research on the recommendation of sequences of resources. Unfortunately, this action occurred only a few times (< 25). Also direct collaboration activities are rare: being contacted occurred 11 times and contacting person 69 times. Implicit data to cluster users who share similar interests or goals are avail- able more extensively (149 perform task, 861 perform topic and 414 select learning goal activities). Whereas the cur- rent collection contains data of only a few users and may be too small for statistical analysis, the dataset provides a good example of relevant learning activities to be captured in learning settings.  2.2.3 ReMashed dataset The ReMashed dataset was collected within the ReMashed  environment [8] that focuses on community knowledge shar- ing. The main objective of ReMashed is to offer personalized recommendations from the emerging information space of a community. The ReMashed dataset is based on aggregating contributions of the users in the ReMashed portal. This por- tal aggregates Web 2.0 contributions from a range of remote  services (delicious, Youtube, Flickr, Slideshare, blogs, and twitter) of the users. The data collection started in Febru- ary 2009 and is still ongoing. It includes information about interests (learning goals), bookmarks, tags, ratings and con- tents. Until now, 140 users are registered. In total, 23.000 tags and 264 ratings are given to 96.000 items.  The ReMashed dataset includes only publicly available contributions from users. Although, the data is publicly available, the dataset is not prepared yet for public access as it requires anonymization and the commitment of the users.  2.2.4 Organic.Edunet dataset The Organic.Edunet dataset was collected on a learning  portal for organic agriculture educators [20]. The portal pro- vides access to more than 10.500 learning resources from a federation of 11 institutional repositories. The portal mostly focuses on serving school teachers and university tutors and has attracted almost 12.000 unique visitors from more than 120 countries, out of which about 1.000 are registered users. This dataset contains data from the initial operational phase of the portal that took place in the context of the EC-funded Organic.Edunet project.  The dataset was collected from January 2010 until Septem- ber 2010 and includes information about 345 tags, 250 rat- ings and 325 textual reviews that these users have provided. The particularity of this dataset is the fact that ratings are collected upon three different dimensions/criteria: the use- fulness of a resource as a learning tool, the relevance to the organic thematic, and the quality of its metadata. This allows for the deployment of an elaborate multi-criteria rec- ommendation service within the portal.  2.2.5 MACE dataset The MACE dataset originates from the MACE project  [34], which ran from September 2006 to September 2009. The MACE portal provides advanced graphical metadata- based access to learning resources in architecture that are stored in different repositories all over Europe. Therefore, MACE enables architecture students to search through and find learning resources that are appropriate for their con- text. From 2007 until now, 1.148 users registered at the portal. The portal offers access to about 150.000 learning resources, from which 12.000 have been accessed by regis- tered users. These objects hold together about 47.000 tags, 12.000 classification terms and 19.000 competency values. Tags were assigned by logged in users and the classification and competency terms by domain experts.  Most user actions with the MACE portal were logged, in- cluding search activities, using facetted search, social tags, geographical locations, classifications and/or competencies, access of learning resources, download of resources, social tagging, including add tag, add comment and add rating, and access of user pages. The time of each user activity is recorded. The dataset provides useful and rich data for various research purposes. In addition to explicit rating feedback, access time, downloads, tags and comments can provide useful implicit indications that can be used to gain knowledge about user interests. The availability of a rela- tively large set of both explicit and implicit relevance data makes this dataset a potentially useful candidate for recom- mender research.  2.2.6 Travel well dataset  46    The Travel well dataset was collected on the Learning Re- source Exchange portal [33] that makes open educational resources available from 20 content providers in Europe and elsewhere. Most registered users are primary and secondary teachers who come from a variety of European countries. The dataset contains data from the pilot phase which was conducted during the EC-funded MELT-project. These data were collected from August 2008 until February 2009 on 98 users. The dataset includes explicit interest indicators that can be used to infer the relevance of a resource for the user. Users can rate resources on a scale of 1 to 5 for usefulness and add tags to resources. In total, 16.353 user activities were recorded on 1.923 resources.  The particularity of the dataset is that it contains infor- mation of the home country, mother tongue and spoken lan- guages of users. Additionally, it has metadata on the origin of the educational resource and its language. The dataset thus allows tracking the interests of users on travel well resources, indicating that the user and resource come from different countries and that the language of the resource is different from that of the users mother tongue. Additionally, this dataset is useful for research on extraction of teacher interests and identification of teachers who share common interests, on the basis of their tags and ratings. The avail- ability of a relatively large set of such explicit relevance indi- cators makes this dataset a potentially useful candidate for recommender research in TEL.  2.3 Summary Table 1 summarizes the details of the collected datasets,  including information on the number of users, items and ac- tivities that are captured and details on the data elements that are provided. The MACE, Organic.Edunet and Mende- ley datasets are the largest datasets that collected user data of 1.148, 1.000 and 200.000 users. The Travel well and Re- Mashed datasets contain ratings and tags of 98 and 140 users, respectively. The current sample of APOSDLE cap- tures data of relatively few users.  Of interest in this discussion are the data elements that are provided by the datasets. Explicit relevance feedback, such as ratings by users, are provided in the MACE, ReMashed, Organic.Edunet and Travel well datasets. These datasets provide ratings on a five point likert scale and are interesting datasets for evaluating recommender algorithms. Mendeley provides information on articles that are starred by a user (1 if the article has been starred and 0 otherwise), but the semantics of such stars in user libraries may be different for different users (i.e. a star can indicate relevance feedback, but may as well indicate that the user wants to read the article at a later stage). Therefore, the application of such data for recommendation is less straightforward.  In addition to ratings/stars, most datasets include ad- ditional user interactions, such as tags, downloads or the inclusion of a resource in a user library. In Section 5.2, we research the extent to which such activities can be used to improve the performance of recommendation algorithms. The APOSDLE dataset includes a wide variety of additional learner related activities, including tasks that are performed by a user, her learning goals and learning paths that she con- structed. Whereas the dataset may be too sparse to draw conclusions at this point, the capturing of such activities has a big potential for building recommender systems for learn-  ing. The application of this dataset for recommendation for learning is further discussed in Section 6.  The Mendeley, APOSDLE and Travel well datasets are openly available. For the Organic.Edunet, MACE and Re- Mashed datasets, legal protection rules apply. Details and contact information to obtain the datasets are included in the dataset descriptions. In the remainder of this paper, we report on experimental results with these datasets.  3. RECOMMENDER SYSTEMS Recommender systems apply data analysis techniques to  help users find items that are likely of relevance. Recom- mender algorithms are often categorized into three areas: collaborative filtering, content-based filtering and hybrid fil- tering. Collaborative filtering is the most widely imple- mented and most mature technology [3]. Collaborative rec- ommender systems recognize commonalities between users on the basis of their ratings or implicit relevance indications and generate new recommendations based on inter-user com- parisons. Content-based filtering matches content resources to user characteristics [28]. These algorithms base their pre- dictions on individual information and ignore contributions from other users. Hybrid recommender systems combine two or more recommendation techniques to gain better per- formance with fewer drawbacks [3].  In this paper, we evaluate the performance of collabora- tive filtering (CF) on TEL datasets. Similar experiments on TEL settings have been reviewed in Manouselis et al. [19]. The basic idea of CF-based algorithms is to provide rec- ommendations based on the opinions of other like-minded users. The opinions of users can be obtained explicitly from the users or by using implicit measures. Two approaches are distinguished for recommending relevant items to a user:   User-based collaborative filtering computes similarities between users to find the most similar users and pre- dicts a rating based on how similar users rated the item. In a first step, a user-based collaborative filter- ing algorithm searches users who share similar rating patterns with the active user. In a second step, rat- ings from these similar users are used to calculate a prediction for the active user.   Item-based collaborative filtering applies the same idea, but uses similarity between items instead of users. The approach was popularized by Amazon.com - i.e. users who bought x also bought y. In a first step, an item- item matrix is built that determines relationships be- tween pairs of items. In a second step, this matrix and the data on the active user are used to make a pre- diction. Once similar items are found, the prediction is then, for instance, computed by taking a weighted average of the target user ratings on similar items.  To enable empirical comparison of different approaches, we implemented different metrics to compute similarities between users and between items and different algorithms for computing predictions, including the standard weighted sum algorithm and simplified Slope One scheme [15]. The different approaches are presented briefly in this section. A more thorough review of various design options for collabo- rative filtering algorithms can be found in [17]. We report on experimental results in Section 5.  47    Table 1: Overview datasets Mendeley APOSDLE ReMashed Organic  Edunet Mace Travel well  Collection period 1 year 3 months 2 years 9 months  3 years 6 months  Number of users 200.000 6 140 1.000 1.148 98 Number of items 1.857.912 163 96.000 10.500 12.000 1.923 Number of activities 4.848.725 1.500 23.264 920 461.982 16.353  Publicly available + + - - - +  reads + + - - + - tags - (+) + + + + ratings (+) - + + + + download or add to collection  + + - - + +  search - + - - + - collaborations - + - - - - learning goal/task - + + - - - learning sequence - + - - - - competencies/ expe- rience level  - + - - + -  time + - - - + +  3.1 User-based Collaborative Filtering User-based collaborative filtering assigns weights to users  based on similarities of their ratings with that of the target user [5]. For calculating the similarity between a target user u and another user v, different similarity metrics can be used. We first present commonly used metrics. Then, we present the standard weighted sum algorithm for generating predictions based on these similarity computations.  3.1.1 Cosine similarity In this case, two users are thought of as two vectors in  the m-dimensional item-space. First, the set of items (Iuv) that both user u and user v have rated is selected. Then, similarity weights are calculated using the following formula  wuv =   iIuv rvirui iIuv r  2 vi   i r  2 ui  where rui is the rating of user u on item i and rvi is the rating of user v on item i. Basically, the cosine similarity between user u and user v is the angle between the ratings vector of user u and the ratings vector of user v.  3.1.2 Pearson correlation. In this case, similarity between two users u and v is mea-  sured by computing the pearson correlation between them using the following formula  wuv =   iIuv (rvi  rv)(rui  ru)  iIuv (rvi  rv) 2  i(rui  ru)2  where rv and ru denote the average ratings for users u and v, respectively. In essence, this similarity measure takes into account how much the ratings of other users for an item deviate from their average rating value.  3.1.3 Tanimoto-Jaccard  The Jaccard or Tanimoto Coefficient [31] measures the overlap degree between two sets by dividing the numbers of items observed by both users (intersection) and the number of different items from both sets of rated items (union). The similarity between two users u and v is defined as:  wuv = |Iu  Iv|  |Iu|+ |Iv|  |Iu  Iv|  where |Iu| and |Iv| represent the number of items that have been rated by user u and user v, respectively. This similarity metric considers only the number of items that have been rated in common and ignores rating values. The metric can be applied on binary datasets that do not con- tain rating values. In addition, studies have shown that the metric is advantageous in the case of extremely asymmetric distributed or sparse datasets [23].  3.1.4 Prediction Computation After computing similarity weights, top-K users with max-  imum weights are selected as experts. Suppose u is a test user and i is a corresponding test item. Let u be the set of experts who have rated i. The predicted rating rui is computed as:  rui = ru +   vu  wuv(rvi  rv) vu  wuv  Basically, the approach tries to capture how similar users rate the item in comparison to their average ratings. If u is empty, i.e. no expert has rated the test item i, then the average rating of the user is outputted as the prediction.  3.2 Item-based Collaborative Filtering Item-based collaborative filtering applies the same idea,  but uses similarity between items instead of users. Once similar items are found, predictions are computed by taking a weighted average of the target user ratings on these similar  48    items. We briefly describe the similarity computation and the prediction generation. The description is based on [29].  3.2.1 Item similarity computation The computation of similarities between items proceeds in  a similar way than computing similarities between users in user-based CF. The basic idea in similarity computation be- tween two items i and j is to first isolate the users who have rated both items and then to apply a similarity computa- tion technique to determine the similarity wij . We illustrate the approach using the cosine similarity metric. Alternative similarity measures such as pearson correlation (see previous section) are also commonly applied to calculate similarity between items.  To compute the cosine similarity, we first isolate the co- rated cases (i.e., cases where the users rated both i and j). Let the set of users who both rated i and j be denoted by U, then the cosine similarity is given by  wij = cos(~i,~j) =   uU ruiruj  uU r 2 ui   uU r  2 uj  where rui is the rating of user u on item i and ruj is the rating of user u on item j. Thus, this formulation views two items and their ratings as vectors, and defines the similarity between them as the angle between these vectors.  3.2.2 Prediction computation In the case of item-based predictions, a weighted sum tech-  nique computes the prediction of an item i for a user u by computing the sum of the ratings given by the user on items similar to i. Each rating is weighted by the corresponding similarity wij between items i and j. Formally, we can de- note the prediction of item i for user u as  rui =   allsimilaritemsj wij(rui) allsimilaritemsj wij  Basically, this approach tries to capture how the active user rates the similar items. The weighted sum is scaled by the sum of the similarity weights to make sure the prediction is within the predefined range.  3.2.3 Slope One scheme The Slope One scheme [15] is an alternative scheme to  compute item-based CF predictions that simplifies the im- plementation of standard item-based collaborative filtering algorithms. The scheme is based on a simple popularity differential. Let the set of users who both rated i and j be denoted by U. Given a training set c, and any two items j and i with ratings ruj and rui respectively by some user u in U, then the average deviation of item i with respect to item j is considered as:  devj,i =  uU  ruj  rui card(U)  The slope one scheme then simplifies the prediction formula to  rui = ru + 1  card(Rj)   iRj  devj,i  Details are presented in [15]. The advantage is that this implementation of Slope One does not depend on how the user rated individual items, but only on the user average rating and on which items the user has rated. Experimental results are presented in Section 5.  4. EVALUATION METRICS In this paper, we focus on the measurement of accuracy  and coverage of recommendation algorithms, which can be measured by offline analysis of data:   Accuracy measures how well the system generates a list of recommendations. Measures typically used are precision, recall and F1. Precision indicates how many recommendations were useful to the user, whereas re- call measures how many desired items appeared among the recommendations. F1 is the harmonic mean of pre- cision and recall - that is, (2precisionrecall)/(precision+ recall).   Predictive accuracy evaluates the accuracy of a system by comparing the numerical recommendation scores against the actual user ratings for the user-item pairs in the test dataset. Mean Absolute Error (MAE) be- tween ratings and predictions is a widely used metric. MAE is a measure of the deviation of recommenda- tions from their true user-specified values. The MAE is computed by first summing absolute errors of the N corresponding ratings-prediction pairs and then com- puting the average. The lower the MAE, the more accurately the recommendation engine predicts user ratings. Root Mean Squared Error (RMSE) and Cor- relation are also used as statistical accuracy metric.   Coverage is a measure of the percentage of items and users for which a recommendation system can provide predictions. A prediction is impossible to be computed in case that no or very few people rated an item or in case that the active user has zero correlations with other users.  A more comprehensive review of evaluation metrics for col- laborative filtering algorithms can be found in Herlocker et al. [10].  5. EXPERIMENTAL RESULTS In this section, we present our experimental results of ap-  plying collaborative filtering techniques to TEL datasets. We used the Apache Mahout5 framework for comparing the performance of different collaborative filtering algorithms on datasets. Apache Mahout is an open source framework that provides implementations of standard item-based and user- based collaborative filtering algorithms and implementations of different metrics to compute similarities between users and between items, including pearson, cosine and tanimoto measures.  First, we present results of collaborative filtering algo- rithms and the influence of different similarity metrics on datasets that contain ratings, including the MACE and Travel well datasets. We also compare these results with accuracy results of algorithms on the MovieLens dataset [5], that is  5http://mahout.apache.org/  49    often used by the recommender system community to eval- uate algorithms. Then, we present results of collaborative filtering algorithms applied to binary data without ratings, such as data of Mendeley. In this set of experiments, we used implicit relevance indications such as tags and downloads as a basis to generate recommendations.  5.1 Collaborative filtering based on ratings In a first set of experiments, we applied collaborative fil-  tering algorithms to datasets that contain rating data. First, we compare the influence of different similarity metrics on collaborative filtering. For this first set of experiments, we selected all users from the MACE and the Travel well col- lection who provided at least 5 ratings. User ratings were randomly split into two sets - observed items (80%) and held- out items (20%). Ratings for the held-out items were to be predicted. We used the Mean Absolute Error (MAE) as the evaluation metric for predictive accuracy in this experiment.  Results are presented in Figure 1. These results indicate that item-based CF based on tanimoto similarity outper- forms item-based CF based on pearson and cosine similar- ity measures for both the MACE and Travel well datasets. In contrast, the use of cosine and pearson measures on the MovieLens dataset improves predictive accuracy of item- based collaborative filtering. These results are consistent with previous experiments that demonstrate that the use of the tanimoto similarity measure on datasets that are very sparse, such as the MACE and Travel well datasets, is ben- eficial [23].  Figure 1: MAE of item-based collaborative filtering based on different similarity metrics  In a second experiment, we compared results of item- based, user-based and slope-one collaborative filtering schemes. For each dataset, we used the best performing similarity measure. Results are presented in Figure 2 and indicate that also the best choice of algorithm is dataset dependent. In the case of MACE, standard item-based collaborative fil- tering outperforms user-based and slope-one collaborative filtering. For Travel well data, user-based collaborative fil- tering outperforms the other schemes. The simplified Slope One scheme gives the most accurate results for the Movie- Lens dataset - which is consistent with findings reported in [14].  Whereas predictive accuracy results of the best perform- ing algorithms on MACE and Travel well data are compara- ble to reported results of collaborative filtering schemes ap-  plied to the MovieLens dataset, the major bottleneck of ap- plying these collaborative filtering schemes to the collected TEL data is the limited coverage of the approach. In MACE, only 113 of 1.148 users provided explicit relevance feedback in the form of ratings. In addition, only 1.706 of 12.000 ac- cessed resources were rated. In the Travel well dataset, more users have provided ratings (56 out of 98), but the number of resources that have been rated by multiple users is very small. In order to address these sparsity issues, we elabo- rate on the use of implicit relevance indicators and the use of binary data for collaborative filtering in the next section.  Figure 2: MAE of user-based, item-based and slope- one collaborative filtering  5.2 Collaborative filtering based on implicit rel- evance data  Implicit feedback techniques appear to be attractive can- didates to improve recommender performance in the TEL domain, where explicit feedback ratings are often sparse. Be- haviors most extensively investigated as sources for implicit feedback in other areas have been reading, saving and print- ing [12]. Morita and Shinoda [24] show that there is a strong tendency for users to spend a greater length of time reading those articles rated as interesting, as opposed to those rated as not interesting. This finding has been replicated by oth- ers in similar environments [13]. Other behaviors that have been explored include printing, saving, tagging and book- marking [27].  We explore the use of implicit relevance data in the Travel well, MACE and Mendeley datasets. In addition to explicit rating data, the Travel well dataset includes 11.943 tags that are provided by 76 users on 1.791 resources. In the MACE dataset, 48.004 tags are provided by 283 users on 6.673 re- sources. In addition, MACE includes: (1) information about the access of resources (resultViewed event), including the date and time when the user viewed the resource, (2) search terms that were used by the user, (3) information about downloaded resources (save event) and (4) comments that were added by the user (addComment event). The Mendeley dataset provides data about library readership and library stars.  In a second set of experiments, we used these data as implicit relevance indications. In this set of experiments, we predict a fixed number of top-N recommendations and not the ratings. In this case, implicit relevance data are used to rank items to the user in order of decreasing relevance. Suitable evaluation metrics are Precision, Recall and F1.  50    Similar to Sarwar et al. [29], our evaluations consider any item in the recommendation set that matches any item in the test set as a hit. The number of top-N items to be predicted was set to 10. The tanimoto similarity measure was used to compute similarities between users.  Figure 3: F1 of user-based collaborative filtering with increasing number of neighbors  Performance results of user-based collaborative filtering on the F1 measure are presented in Figure 3. As can be seen in this figure, the size of the neighborhood affects the quality of the top-10 recommendations. In general, the quality in- creases as we increase the number of neighbors. However, af- ter a certain point, the improvement gains diminish. Results indicate that implicit relevance indications can be used in a successful way. For Mendeley, we used library readership and starred articles as implicit relevance indications. Based on these data, a standard user-based collaborative filtering algorithm that predicts the top 10 most relevant items for a user has an F1 score of almost 30% - which is comparable to the application of user-based collaborative filtering on the MovieLens dataset (25%). Reasonable results were also obtained for the Travel well dataset. Similar to the low ac- curacy results of user-based collaborative filtering on MACE data that were presented in the previous section, accuracy results remain low (< 5%) when additional data about tags and downloads is incorporated. These results are consistent with previous studies of user-based collaborative filtering on extremely sparse datasets. To tackle this issue, part of our ongoing work is based on improving the performance based on alternative similarity measures [26]. We elaborate on useful extensions and future research directions for recom- mendation for learning in the next section.  6. DISCUSSION The goal of this kind of dataset driven research on recom-  mender systems is to gain deeper insights into both relevant similarity measures between users and between items and relevant data that can be taken into account to support rec- ommendation for learning. Results of our study show that the tanimoto similarity measure gives most accurate results on the current TEL datasets that are very sparse. The best choice of algorithm (i.e. user-based, item-based or slope- one) is dataset dependant. These results are consistent with previous findings that have been reported in [22]. The re- sults indicate that the successful operation of collaborative filtering in the context of real-life learning applications re- quires careful testing before their actual deployment.  It is important to note that the presented experiments serve only as a first step towards the understanding and appropriate specialization for recommendation for learning. This study has to be further complemented with experi- ments that will study the needs and expectations of the users, their information seeking tasks, and how recommended resources may be used in the context of their learning activ- ities [21]. In this study, only very generic collaborative fil- tering algorithms have been tested. In the learning domain, researchers have proposed the use of additional learner or teacher attributes in recommendation processes [2]. Exam- ples include knowledge or experience levels indicators, learn- ing interests, learning goals, learning and cognitive styles, affects and background information. In addition to inter- ests and preferences that are available in most datasets, the learning goal or competencies of a learner are often incor- porated as a basis for generating learning recommendations [4]. Data on competencies or experience levels is available in the MACE and APOSDLE datasets. In addition, APOS- DLE provides data on the learning goal of the learner when she is performing a task. Such data is useful to improve sim- ilarity measures between users and to find users who share similar goals, both as a basis to improve recommendation of relevant learning resources and to support recommendation of peer learners.  We aim to experimentally test the performance of varia- tion against several attributes of learners or teachers that are proposed in the literature. In order to create evidence driven knowledge about the effect of recommender systems on learners and personalized learning, more experiments like the presented one are needed. The continuation of additional small-scale experiments with a limited amount of learners that rate the relevance of suggested resources only adds lit- tle contributions to an evidence driven knowledge base on recommender systems in TEL. The key research question re- mains how generic algorithms need to be modified in order to support learners or teachers. To give an example, from a pure learning perspective, the most valuable resources for a learner could be the recommendation of different opinions or facts that challenge the learners to disagree, agree and rede- fine their point of view. In order to enable such experiments, the capturing of learner or teacher data is a key require- ment. Our ongoing research is focused on the development of a standardized data model that enables the uniform rep- resentation of both explicit and implicit relevance data of learners and teachers [7]. This data model will be standard- ized in collaboration with the CEN WS-LT Working Group on Social Data6.  7. CONCLUSION In this study, we presented datasets that capture learner  interactions with tools and resources and that can be used for learning analytics research. We successfully applied sev- eral variations of user-based and item-based collaborative fil- tering algorithms to these datasets. Challenges to be tackled include sparsity of data and require further research on both implicit relevance indicators as well as similarity measures to find relevant items and/or users. To tackle these chal- lenges, the further collection of sufficiently large datasets that capture learner interactions in different real-life learn- ing settings is a key requirement.  6https://sites.google.com/site/censocialdata/home  51    8. ACKNOWLEDGMENTS This work is supported by the dataTEL Theme Team and  the STELLAR Network of Excellence. Katrien Verbert is a Postdoctoral Fellow of the Research Foundation - Flanders (FWO). The work of Nikos Manouselis has been funded with support by the EU project VOA3R - 250525 of the CIP PSP Programme (http://voa3r.eu). This research used data op- erated by European Schoolnet (EUN). EUN does not guar- antee the accuracy of any data and cannot be held liable for any errors or omissions. The research leading to these results has also received funding from the European Commu- nity Seventh Framework Programme (FP7/2007-2013) un- der grant agreement no 231396 (ROLE).  9. REFERENCES [1] C. Bighini, A. Carbonaro, and G. Casadei. Inlinx for  document classification, sharing and recommendation. In IEEE International Conference on Advanced Learning Technologies, pages 9195. IEEE Computer Society, 2003.  [2] P. Brusilovsky and E. Millan. User Models for Adaptive Hypermedia and Adaptive Educational Systems. In P. Brusilovsky, A. Kobsa, and W. Nejdl, editors, The Adaptive Web, volume 4321 of Lecture Notes in Computer Science, chapter 1, pages 353. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007.  [3] R. Burke. Hybrid web recommender systems. In P. Brusilovsky, A. Kobsa, and W. Nejdl, editors, The Adaptive Web, volume 4321 of Lecture Notes in Computer Science, chapter 12, pages 377408. Springer Berlin Heidelberg, Berlin, Heidelberg, 2007.  [4] C.-M. Chen, H.-M. Lee, and Y.-H. Chen. Personalized e-learning system using item response theory. Computers & Education, 44(3):237255, April 2005.  [5] M. S. Desarkar, S. Sarkar, and P. Mitra. Aggregating preference graphs for collaborative rating prediction. In Proceedings of the fourth ACM conference on Recommender systems, RecSys 10, pages 2128, New York, NY, USA, 2010. ACM.  [6] H. Drachsler, T. Bogers, R. Vuorikari, K. Verbert, E. Duval, N. Manouselis, G. Beham, S. Lindstaedt, H. Stern, M. Friedrich, and M. Wolpers. Issues and considerations regarding sharable data sets for recommender systems in technology enhanced learning. Procedia Computer Science, 1(2):28492858, 2010.  [7] H. Drachsler, H. G. K. Hummel, and R. Koper. Personal recommender systems for learners in lifelong learning networks: the requirements, techniques and model. Int. J. Learn. Technol., 3(4):404423, 2008.  [8] H. Drachsler, D. Pecceu, T. Arts, E. Hutten, P. van Rosmalen, H. Hummel, and R. Koper. Remashed - an usability study of a recommender system for mash-ups for learning. International Journal of Emerging Technologies in Learning (iJet), Special Issue: ICL2009 on MashUps for Learning(5):711, January 2010.  [9] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst., 22(1):553, 2004.  [10] J. L. Herlocker, J. A. Konstan, L. G. Terveen, and J. T. Riedl. Evaluating collaborative filtering recommender systems. ACM Trans. Inf. Syst., 22:553, January 2004.  [11] K. Jack, J. Hammerton, D. Harvey, J. J. Hoyt, J. Reichelt, and V. Henning. Mendeleys reply to the datatel challenge. Procedia Computer Science, 1(2):13, 2010.  [12] D. Kelly and N. J. Belkin. Reading time scrolling and interaction: exploring implicit sources of user preferences for relevance feedback. In Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR 01, pages 408409, New York, NY, USA, 2001. ACM.  [13] J. A. Konstan, B. N. Miller, D. Maltz, J. L. Herlocker, L. R. Gordon, and J. Riedl. Grouplens: Applying collaborative filtering to Usenet news. Communications of the ACM, 40(3):7787, 1997.  [14] D. Lemire, H. Boley, S. McGrath, and M. Ball. Collaborative filtering and inference rules for context-aware learning object recommendation. International Journal of Interactive Technology and Smart Education, 2(3), August 2005.  [15] D. Lemire and A. Maclachlan. Slope one predictors for online rating-based collaborative filtering. Society for Industrial Mathematics, 05:471480, 2005.  [16] S. Lindstaedt, B. Kump, G. Beham, V. Pammer, T. Ley, A. Dotan, and R. De Hoog. Providing varying degrees of guidance for work-integrated learning. In Proceedings of the 5th European conference on Technology enhanced learning conference on Sustaining TEL: from innovation to learning and practice, EC-TEL10, pages 213228, Berlin, Heidelberg, 2010. Springer-Verlag.  [17] N. Manouselis and C. Costopoulou. Preliminary study of the expected performance of maut collaborative filtering algorithms. In M. D. Lytras, J. M. Carroll, E. Damiani, R. D. Tennyson, D. Avison, G. Vossen, and P. Ordonez De Pablos, editors, The Open Knowlege Society. A Computer Science and Information Systems Manifesto, volume 19 of Communications in Computer and Information Science, pages 527536. Springer Berlin Heidelberg, 2008.  [18] N. Manouselis, H. Drachsler, K. Verbert, and O. Santos, editors. Proceedings of the 1st Workshop on Recommender Systems for Technology Enhanced Learning (RecSysTEL 2010), volume 1 of Procedia CS. Elsevier, 2 edition, 2010.  [19] N. Manouselis, H. Drachsler, R. Vuorikari, H. Hummel, and R. Koper. Recommender systems in Technology Enhanced Learning. In R. L. S. B. Kantor P., Ricci F., editor, Recommender Systems Handbook: A Complete Guide for Research Scientists & Practitioners, pages 387415. Springer, 2010.  [20] N. Manouselis, K. Kastrantas, S. Alonso, J. Caceres, H. Ebner, and M. Palmer. Architecture of the organic.edunet web portal. International Journal of Web Portals, 1(1):7191, 2009.  [21] N. Manouselis, R. Vuorikari, and F. V. Assche. Simulated Analysis of MAUT Collaborative Filtering  52    for Learning Object Recommendation. In Workshop proceedings of the EC-TEL conference: SIRTEL07 (EC-TEL07), pages 2735, 2007.  [22] N. Manouselis, R. Vuorikari, and F. Van Assche. Collaborative recommendation of e-learning resources: an experimental investigation. Journal of Computer Assisted Learning, 26(4):227242, 2010.  [23] A. Mild and T. Reutterer. An improved collaborative filtering approach for predicting cross-category purchases based on binary market basket data. Journal of Retailing and Consumer Services, 10(3):123133, 2003.  [24] M. Morita and Y. Shinoda. Information filtering based on user behavior analysis and best match text retrieval. In Proceedings of the 17th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR 94, pages 272281, New York, NY, USA, 1994. Springer-Verlag New York, Inc.  [25] A. Naak, H. Hage, and E. Ameur. A multi-criteria collaborative filtering approach for research paper recommendation in papyres. In G. Babin, P. G. Kropf, and M. Weiss, editors, MCETECH, volume 26 of Lecture Notes in Business Information Processing, pages 2539. Springer, 2009.  [26] K. Niemann, M. Scheffel, M. Friedrich, U. Kirschenmann, H.-C. Schmitz, and M. Wolpers. Usage-based object similarity. Journal of Universal Computer Science, 16(16):22722290, 2010.  [27] D. W. Oard and J. Kim. Implicit Feedback for Recommender Systems. In AAAI Workshop on Recommender Systems, Madison, WI, pages 8183, July 1998.  [28] M. J. Pazzani and D. Billsus. Content-based recommendation systems. In The Adaptive Web: Methods and Strategies of Web Personalization, chapter 10, pages 325341. 2007.  [29] B. Sarwar, G. Karypis, J. Konstan, and J. Reidl. Item-based collaborative filtering recommendation algorithms. In WWW 01: Proceedings of the 10th international conference on World Wide Web, pages 285295, New York, NY, USA, 2001. ACM.  [30] T. Y. Tang and G. Mccalla. Smart recommendation for an evolving e-learning system. In Workshop on Technologies for Electronic Documents for Supporting Learning, International Conference on Artificial Intelligence in Education (AIED), 2003.  [31] T. T. Tanimoto. An elementary mathematical theory of classification and prediction. Internal Report IBM Corp, 1958.  [32] S. Ternier, K. Verbert, G. Parra, B. Vandeputte, J. Klerkx, E. Duval, V. Ordonez, and X. Ochoa. The ariadne infrastructure for managing and storing metadata. IEEE Internet Computing, 13(4):1825, 2009.  [33] R. Vuorikari and R. Koper. Ecology of social search for learning resources. Campus-Wide Information Systems, 26(4):272286, 2009.  [34] M. Wolpers, M. Memmel, and A. Giretti. Metadata in architecture education - first evaluation results of the mace system. Learning in the synergy of multiple disciplines, 5794:112126, 2009.  53      "}
{"index":{"_id":"7"}}
{"datatype":"inproceedings","key":"Suthers:2011:UFM:2090116.2090124","author":"Suthers, Daniel and Rosen, Devan","title":"A Unified Framework for Multi-level Analysis of Distributed Learning","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"64--74","numpages":"11","url":"http://doi.acm.org/10.1145/2090116.2090124","doi":"10.1145/2090116.2090124","acmid":"2090124","publisher":"ACM","address":"New York, NY, USA","keywords":"analysis, distributed learning, interaction analysis, networked learning, social network analysis, socio-technical networks","Abstract":"Learning and knowledge creation is often distributed across multiple media and sites in networked environments. Traces of such activity may be fragmented across multiple logs and may not match analytic needs. As a result, the coherence of distributed interaction and emergent phenomena are analytically cloaked. Understanding distributed learning and knowledge creation requires multi-level analysis of the situated accomplishments of individuals and small groups and of how this local activity gives rise to larger phenomena in a network. We have developed an abstract transcript representation that provides a unified analytic artifact of distributed activity, and an analytic hierarchy that supports multiple levels of analysis. Log files are abstracted to directed graphs that record observed relationships (contingencies) between events, which may be interpreted as evidence of interaction and other influences between actors. Contingency graphs are further abstracted to two-mode directed graphs that record how associations between actors are mediated by digital artifacts and summarize sequential patterns of interaction. Transitive closure of these associograms creates sociograms, to which existing network analytic techniques may be applied, yielding aggregate results that can then be interpreted by reference to the other levels of analysis. We discuss how the analytic hierarchy bridges between levels of analysis and theory.","pdf":"A Unified Framework for Multi-Level Analysis of  Distributed Learning   Daniel Suthers  Dept. of Information and Computer Sciences   University of Hawaii,  1680 East-West Road, POST 309   Honolulu, HI 96822, USA   suthers@hawaii.edu  Devan Rosen  School of Communications   Ithaca College  953 Danby Road,   Ithaca, NY 14850, USA  drosen@ithaca.edu  ABSTRACT  Learning and knowledge creation is often distributed across  multiple media and sites in networked environments. Traces of  such activity may be fragmented across multiple logs and may not  match analytic needs. As a result, the coherence of distributed  interaction and emergent phenomena are analytically cloaked.  Understanding distributed learning and knowledge creation  requires multi-level analysis of the situated accomplishments of  individuals and small groups and of how this local activity gives  rise to larger phenomena in a network. We have developed an  abstract transcript representation that provides a unified analytic  artifact of distributed activity, and an analytic hierarchy that  supports multiple levels of analysis. Log files are abstracted to  directed graphs that record observed relationships (contingencies)  between events, which may be interpreted as evidence of  interaction and other influences between actors. Contingency  graphs are further abstracted to two-mode directed graphs that  record how associations between actors are mediated by digital  artifacts and summarize sequential patterns of interaction.  Transitive closure of these associograms creates sociograms, to  which existing network analytic techniques may be applied,  yielding aggregate results that can then be interpreted by reference  to the other levels of analysis. We discuss how the analytic  hierarchy bridges between levels of analysis and theory.   Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Education   collaborative learning, distance learning.    General Terms Design, Analysis, Theory.   Keywords   Socio-technical networks, distributed learning, networked  learning, interaction analysis, social network analysis.    1. INTRODUCTION The rapid adoption of information and communication  technologies (ICT) in support of online, distributed, and   networked learning and knowledge creation activities [1], and  their blending with face-to-face venues [17] is well known to the  research community to which this paper is addressed. In this paper  we use learning as shorthand to include any enhancements of  individual or collective knowledge or skills, whether or not it  occurs in formal educational settings. We include in our scope of  interest learning in (for example) online university settings,  professional communities, and virtual organizations [2, 4, 9, 35].  We will refer to these collectively as socio-technical networks  [24]. A related trend is towards open learning communities.  Courses in formal educational settings need no longer isolate  participants from others in different courses, but can embed  courses in online communities of learners, for example supporting  transdisciplinary graduate education [12]. In corporate or other  work settings, professional learning communities similarly may  cross team contexts rather than being isolated in work teams [50].  The fundamental question of interest in all of these settings is how  learning takes place through the interplay between individual and  collective agency. All learning activity requires that individuals  take actions, but these individual actions are contingent on the  actions of others in their socio-technical network contexts, actions  that reflexively construct those contexts.    The first analytic challenge addressed by this paper is that  learning and knowledge creation activities in these networked  environments are often distributed across multiple media and  sites. As a result, traces of such activity may be fragmented across  multiple logs. For example, the networked learning environments  we study offer mixtures of threaded discussion, synchronous  chats, wikis, whiteboards, profiles, and resource sharing. Events  in these media may be logged in different formats and recorded in  databases and text files, disassociating actions that for participants  were part of a single unified activity. This disassociation is  exacerbated when activity is distributed across multiple virtual  sites or spread over time. Also, the granularity at which events are  recorded may not match analytic needs, and media-level events  may be the wrong ontology for analyses that begin with  relationships rather than individual acts. Translation from log file  representations to other levels of description may be required to  begin the primary analysis. As a result of these various issues, the  coherence of distributed interaction and phenomena that emerge  from this interaction are analytically cloaked.      Furthermore, understanding distributed learning and knowledge  creation requires multi-level analysis of the situated  accomplishments of individuals and small groups and of how  these local accomplishments give rise to larger phenomena in  networks such as the dissemination and transformation of ideas,  implicit coordination of the activities of many participants, and  the accrual of collective knowledge. Consider the question of how  the design of the virtual environment influences emergent   Permission to make digital or hard copies of all or part of this  work for personal or classroom use is granted without fee  provided that copies are not made or distributed for profit or  commercial advantage and that copies bear this notice and the  full citation on the first page. To copy otherwise, or republish, to  post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.  LAK11, February  27March 1, 2011, Banff, Alberta, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.   64    phenomena. Everything builds on the existence of multiple  successive moments in which an individual is experiencing some  presentation of the virtual environment, cares enough to act, and is  able to choose an appropriate action. Whether and how this action  has implications for network or community level phenomena  requires that some trace of the action be given persistent form that  other participants might later encounter in their experience of the  virtual environment [22]. Appropriate aggregation and availability  of such traces can drive dissemination of ideas, align participants,  and lead to accrual of collective resources of value. Critically, an  empirically grounded understanding of this emergence requires  analysis at both fine-grained and aggregate levels. The same can  be said for understanding the relationship between small group  interactions and larger scale phenomena.    In summary, since interaction is distributed across space, time,  and media, and the data comes in a variety of formats, there is no  single transcript to inspect and share, and the available data  representations may not make interaction and its consequences  apparent. To address these concerns (and to support the diverse  research in our laboratory), we have developed a framework  consisting of an abstract transcript representation that collects  relevant events into a single analytic artifact, and an analytic  hierarchy that supports multiple levels of analysis. This paper  describes the framework and discusses its potential roles in  unifying multiple sources of data and bridging between levels of  analysis and theory. We discuss how the framework addresses  several specific analytic needs, including: (a) scaling up  microanalysis of interaction to large data sets, (b) enabling the  translation of event logs into tie data appropriate for social  network analysis, and (c) interpreting results at one level in terms  of another (e.g., relating social network analytic results back to  their interactional settings). Throughout the paper, a simple  example drawn from our prior research illustrates many of the  features of the framework.    2. PREVIEW  The analytic hierarchy consists of several abstraction layers of  analytic representations that we have found to be useful,  summarized in Table 1. Process traces such as log files are  abstracted to domain models describing the actors, actions and  media objects involved in event models, which are collections of   temporally tagged events. These event models can be further  elaborated by installing directed graphs of empirical relationships  between events called contingencies. Contingencies can be any  observed relationship between events (e.g., two events are by the  same actor, involve the same object, are temporally contiguous or  proximal, or overlap in content). Contingencies situate  participants acts in relation to other eventshence the name  contextualized action model. The analytic utility of contingency  graphs is enhanced if focused on those contingencies that may be  interpreted as evidence of uptake: interaction and other influences  between actors. When such interpretations are made, contingency  graphs are abstracted into uptake graphs, representing generalized  interaction models. Interaction can be further abstracted to two- mode directed graphs, called associograms, which record how  associations between actors are mediated by their creation and  modification of and access to digital artifacts: hence the name  mediation models. Associograms also summarize sequential  patterns of interaction, making it easier to localize certain  patterns. Reduction of associograms by transitive closure into  direct ties between actors yields sociograms, representing tie  models. Existing network analytic techniques may be applied to  sociograms. The results of network analysis can then be  interpreted by reference to the other levels of analysis. Thus  associograms bridge between interaction analysis and network  analysis.   The analytic concepts (e.g., contingencies, uptake, mediated  associations, and ties) in this paper are not new. The value of the  framework relies on the fact that they are abstractions of concepts  commonly applied in existing analytic practice (e.g., adjacency,  edits, replies, etc.) as will be detailed later. Thus the framework is  offered to coordinate and augment rather than replace existing  analytic practices.    The layers are explained in more detail in the following  subsections. The process trace, domain, and event models and  transformations between them are likely to be familiar to readers:  brief sections on these layers are included for completeness and to  provide the foundation and examples for describing subsequent  layers. Contingency and uptake graphs and associograms are more  unique contributions, so are described in some detail heresee  also [45] for extensive discussion of motivations for contingency  graphs and examples of their use for uptake analysis. The most  abstract layer is covered substantially in the social network  analysis literature [e.g., 49], so is described here only in relation  to how it is derived from the layer below, and what that vertical  relationship enables that would not be possible with direct  measurement of ties. Throughout this presentation, applications to  the study of learning analytics are discussed. The methods  described in this paper have been applied in numerous analyses of  data from an online learning environment and from laboratory  studies of ICT mediated collaboration. At present we are using  these techniques in analyses of SRIs Tapped-In teacher  professional community [15, 39], a virtual organization that hosts  many thousands of education professionals annually in more than  800 user-created spaces that include IRC, threaded discussions,  shared files and URLs, and other tools to support collaborative  work.    3. PROCESS TRACES  Any analysis of interaction begins with a process trace, or record  of activity left in the environment and accessible to the researcher.  Examples include software log data (software application or  server logs), audio and video recordings, and textual transcripts.   Table 1. The Analytic Hierarchy   Models  Representations   Process Trace Log files, audio and video recordings, etc.  Domain  Entities and their relationships (types and   instances of both)  Event  Sets of events (described in terms of actors,   objects, time, etc.)  Contextualized  Action  Contingency graphs indicating empirical  relationships (contingencies) between events   Generalized Interaction    Uptake graphs (each arc corresponds to  bundles of contingencies that evidence  uptake)   Mediation  Associograms: two-mode directed graphs  relating actors to objects   Relationship  Subgraphs of the mediation model  consisting of all paths between two actors   Tie  Sociograms representing ties between actors   65    The analytic hierarchy described herein was originally designed to  support analysis of both software logs and video recordings,  sometimes in conjunction (e.g., we have analyzed application logs  and screen capture of the same application [31, 32]). For learning  analytic applications and to emphasize the potential for automated  analysis, this paper focuses on software logs, and does not touch  on issues of video analysis; see [18, 21] for discussion of such  issues.    The analytic hierarchy is illustrated throughout this paper by  building on a simplified example taken from one of our online  learning community applications, disCourse. The disCourse  environment provides threaded discussions, wiki pages, resource  sharing with searchable metadata, and user profiles, organized in a  workspace metaphor that collects together tools and resources  relevant to a given group, such as a class [43]. The lower portion  of Figure 1 shows excerpts (edited for anonymity and simplicity  of presentation) of an http server log1 from disCourse. The  example in this paper builds on these logs. See [45] for the full  text of the example.   4. ENTITY-RELATIONS: DOMAIN  MODEL  Prior to or concurrently with the construction of the event model  (next section), it is necessary to construct an ontology of the kinds  of entities involved in the application domain of interest. Classes  of entities and potential structural relationships between them are  defined (e.g., actors, discussions, and messages, related by  containment, threading and authoring relations). As the trace or  log file is processed, new instances of entities and their structural  relations are added to the domain model when they are  encountered, along with relevant attributes that are expected to be                                                                     1 disCourse logs events in a database. HTTP server logs of the same   events are shown in this example to illustrate the method using log  formats familiar to readers.  needed for analysis. This is undertaken in conjunction with  construction of the event model. For example, the right hand side  of Figure 1 illustrates a domain model fragment representing how  messages m1, ... m4 are created by participants P1, P2, P3 (shown by shading), related to each other by a threading relation,  and contained in a discussion forum. The content of messages are  also recorded in the domain model. Temporal information is  recorded in the event model, discussed next.   5. EVENTS: EVENT MODEL The process trace is transformed into a set of events that constitute  an analysts first commitments concerning the relevant units for  analyzing processes. This transformation involves the Exploratory  Sequential Data Analysis (ESDA) operations of chunking and  coding [37]. For example, the first three lines of the log of Figure  1 all are part of the process of posting a message in a system in  which each message is previewed before posting. These three  traces are chunked together and represented as the single event w1 in the event model, along with information about the actor (P2, indicated by grey), action taken (w for writing), object (message  m1), contents and location (recorded in the domain model), and  temporal scope of the action.  We call this layer the event model because the focus is on individual actions and other events by  nonhuman actants such as software display events. (Actant is  Latours [27] term for non-human entities that yet have agency in  networks of associations.) The events have not yet been put in  relation to each other, other than ordering along a timeline.    Events may be derived from distinct process traces that come  from different media, tools or sites, and are recorded in different  formats. For example, chat contributions, wiki edits, whiteboard  edits, file uploads, etc. can be merged into a single event stream.  (To remain faithful to the case example and avoid complicating  the figures, this capability is not illustrated in the figures, but it is  a simple extension.) A key concern is persistence of identity  across tools and sites: some work may be required to ensure that  each given actor is represented by the same identifier in the event   Figure 1. From process trace to domain and event models   66    model, and likewise for the identity of digital objects shared  across tools (ideally persistence of identity should be addressed in  mash-ups for the learners sake [25]). Once this has been  accomplished, the event and domain models taken together  provide an abstract transcript of the data that re-assembles in one  analytic artifact the diverse events that were for their actors a  single activity. If the transcription is complete with respect to the  needs of a given analysis, then it is not necessary to retain the  original process traces. However, we retain pointers to the original  process traces because it may not be possible to identify all needs  in advance, and because (in the case of video and audio) some  aspects of the data recording cannot be abstracted into computer  representations. We may need to recover other information from  the process trace. Also, any transcript includes initial theoretical  commitments [13, 34], which may turn out to be faulty,  necessitating a return to the original process traces.    A number of analyses can be undertaken on the event and domain  models without further analysis. In our research, this is the level at  which we answer basic questions about the distribution of activity  in the environment: who is participating with whom, in what  virtual sites or contexts, and involving what literal content. But to  analyze interaction and uncover ties between actors we must  relate events to each other.    6. CONTINGENCY GRAPH:  CONTEXTUAL ACTION MODEL   Contingency graphs are an empirically grounded elaboration of  the abstract transcript to make analytically relevant relationships  between events explicit. We originally called these relationships  dependencies, but have renamed them contingencies because they  capture relationships between events that may be merely  contingent or incidental to the situation, rather than being causal  or deterministic. The graph simply makes relationships that are  latent in the data more explicit, and does not constitute a  commitment concerning actors intentions. Human action can be  embedded in its context in many ways, including accidental  relationships, or opportunistic leveraging of contextual and  historical features as well as necessary antecedents for action [6,  27]. Thus, a contingency graph represents how action is  embedded in the context of other events. Examples of contingency  types we have used are listed in Table 1 not intended to be a  complete taxonomy). A detailed presentation of the motivations  and theory behind contingency graphs and their application to  interaction analysis may be found in [45].    Construction of a complete graph of the contingencies between  events in a process trace is not practical, as it would result in a  graph with a high signal to noise ratio that is too complex for  processing. (Imagine a graph in which each event is linked to  every other event involving the same actor, or the same object, or  that has overlap in lexical content, or occurred nearby in time, and  so on.) An analyst chooses those contingencies that are relevant  for specific analytic purposes as guided by explicit or implicit  theory. Therefore a contingency graph reflects further  commitments on the part of the analyst. However, even though a  contingency graph is theoretically selective, we always base  contingencies on empirically observable relationships between  events found in the event and domain models, preferably those  relationships that are unambiguous and can be detected  automatically. If this standard of evidence is followed, a  contingency graph can be treated as an abstract transcript that   makes the evidence for interaction or other phenomena of interest  manifest.   Contingency graphs can be constructed automatically from the  layers below it [see, for example, 32]. For example, for each event  in which an actor accessed an object we might scan back to find  the last event in which the object's contents were modified, and  install a media dependency. Contingencies can also be installed  from a given event to the most recent prior event involving the  actor, to prior events in which the actor accessed a media object  with similar inscriptions (e.g., lexical phrases or graphical  devices), or to temporally recent events in the same spatial site. A  challenge with algorithmic installation of contingencies is limiting  their number. Temporal or sequential proximity are useful (and  computable) heuristics for selecting relevant contingent events, as  they follow the local continuity of human attention and goal  directed behavior: what actors do at any given moment is likely to  be contingent upon their immediately prior act. We are using  temporal proximity in our analysis of quasi-synchronous chats in  Tapped In [44].   For example, Figure 2 shows the events of Figure 1 with  contingencies installed. The single arcs represent media  dependencies, and the double arcs represent multiple  contingencies, such as temporal proximity combined with same  actor and possibly inscriptional similarity. The act of reading a  message (r1, r2, etc.) is media-dependent on the act of creating  the message (w1, w2, etc.). The act of writing a message (e.g.,  w2) may be media-dependent on the act of creating the message  to which it is a threaded reply (e.g., w1) and is contingent on the  messages that the author has recently read (e.g., r1).  In this  example, the message created by w2 contained a noun-phrase in  common with that created by w1.  Once constructed, various kinds of analytic actions are possible on  contingency graphs. For example, suppose a particularly  productive session was identified in which participants made  significant ideational progress. One option is to examine the  interaction of the session participants more closely to identify the   Table 2. Examples of contingency types   E2 is contingent on E1 when  Media  Dependency    E2 operates on a media object or state of that  object that was created or modified by E1 (e.g., reply to a message; editing a shared  wiki).  Same Actor E2 and E1 were due to acts of the same actor.  Actor Address E2 adresses the actor of E1 by name.  Addressed Actor  E2 is generated by an actor addressed by name  in E1. (Whether or not E2 is intended as a  reply is left for interpretation.)    Inscriptional  Similarity   E2 creates inscriptions with visual attributes  similar to those of inscriptions created by E1. E2 creates inscriptions with lexical strings  identical to those in inscriptions created by  E1.  Temporal Proximity   E2 took place soon after E1, where soon  depends on the attentional properties of the  agents and persistency of the medium (e.g.,  proximal messages in chat).   Spatial  Organization   E2 operates on inscriptions in a spatial context  created by E1 (e.g., grouping a graphical  object by placing it near others).   67    relationship between group processes and their accomplishments,  and how participants appropriated the interactional affordances of  the available media for these purposes. We have used the  contingency graphs in several studies to support this kind of  microanalysis of interaction [30, 31, 32, 46]. Recurring patterns of  interaction so identified could be searched for in the overall  contingency graph to find other sessions that have similar patterns  of activity, to see whether they display similar productivity. Such  pattern matching techniques are similar to structural equivalence  metrics in social network analysis, which can be employed once  contingency graphs are converted into sociograms, as discussed in  section 9. Another option is to look outside the session to find  influences from or to other sessions. One can trace same-actor and  media-dependency contingencies, following the actors and actants  respectively. Tracing proceeds forward in time to see whether the  new ideas of the session were disseminated elsewhere, or  backward in time to identify possible predecessors of the  ideational advance. Such an analysis grounds the concept of  brokers in actual accomplishments, not relying solely on  structural relationships that do not guarantee such  accomplishments. At this writing we are constructing a  contingency graph of several years of data from Tapped-In in  preparation for application of methods such as those just  described.    7. UPTAKE GRAPH: GENERALIZED  INTERACTION MODEL  As discussed above, contingencies are so named because they can  include circumstantial relationships between acts with varying  degrees of relevance to interaction and other associations between  actors. Analytic interpretation is required to identify relationships  between events that are not merely circumstantial, but reflect  intentional acts. An act of uptake is one in which an actor takes   traces of one or more prior events as having certain significance  for an ongoing activity [45]. For example, a speaker takes up  some aspect of the prior speakers utterance, or a message poster  in a discussion forum can take up some aspect of the message  being replied to. Uptake is a generalization of all interactional  relationships used in analysis, such as comment, reply, elaboration. It includes these relationships, but also applies to  spatio-temporally distributed associations between actors in which  they may not even be aware of each other, let alone be directing  their actions towards each other, such as tagging, downloading,  etc. Uptake is more general than transactivity [5], which requires  other-directedness. Uptake is an appropriate unit of association  (generalizing interaction) in networked learning environments,  where individuals may benefit from each others presence without  conversing directly [10, 19]. The essential idea is that the trace an  actor's actions have left in the environment (e.g., chat  contribution, discussion posting, uploaded file, profile,  recommendation) is taken up by another actor in some manner.  Uptake of traces can result in stigmergic effects, i.e., implicit  distributed coordination of collective action [14, 36]. Illuminating  these stigmergic effects reveals the contingencies by which an  individuals actions are connected to information, actions, and  resources from sources that may otherwise not be known to that  individual, even if embedded within ones known social network.   An uptake graph is a generalized interaction model, as it describes  the interaction and other associations that the analysis claims is  taking place. Although all analytic artifacts from process traces on  up involve theoretical decisions, the move from contingency  graphs to uptake graphs is a move from primarily empirically  accountable representations to those more strongly determined by  analytic interpretations. Representationally, an uptake relation is a  subgraph of contingencies, as illustrated in Figure 3.  An analyst  collects contingencies that are considered to be analytically  meaningful: a number of contingencies between two or more acts   Figure 2.  Installing contingencies Figure 3. From contingency to uptake graphs  68    may corroborate the interpretation that the final act is an  intentional taking up of traces of the prior ones. For example, w2, in which P1 posts a reply to the message posted by P2 in w1, is  contingent on w1 in these ways: there is a media dependency (m2 is linked by threading to m1); lexical overlap (m2 contains  phrases also found in m1); and a chain of temporal proximity (w2 took place shortly after read event r1 by the same actor, and r1 is  media-dependent on w1 by virtue of reading m1). All of these  contingencies are taken as evidence for an intentional relationship  of w2 to w1, and collapsed into one uptake arc. Because of this  relationship between contingencies and uptake, an uptake graph  may be seen as an abstraction of a contingency graph, and many  of the same analytic moves (such as pattern matching and tracing  actions) apply to both.    Contingency and uptake graphs are described more fully in [45].  We have used contingency and uptake graphs to provide  interactional accounts of specific accomplishments of participants  [30, 31], to trace out information sharing [48], and to detect roles  of participants not visible in the final media trace [46]. For  example, examining only reply structure (the threading  relationship between messages in Figure 1) we might miss the fact  that m4 played an integrative role in this discussion. The uptake  graph of Figure 3 makes this integration explicit as a structure of  uptake converging on w4. (which is not taking up the message to  which it is linked by threaded reply). Integrative or convergent  acts are important to group learning processes such as  intersubjective meaning making [42] and community knowledge  building [38].    Contingency and uptake graphs represent process models: they  focus on how acts relate to each other and constitute a process of  interaction. Their basic unit is acts and other events: the actors and  entities through which interaction takes place are attributes of  these events. Now we turn to an alternative derived representation  that makes these actors and entities explicit, rather than the events.     8. ASSOCIOGRAMS: MEDIATION  MODEL  In the study of socio-technical networks, we are interested in how  the technological infrastructure enables and is utilized by the  social actors to interact with each other. The next layer of the  analytic hierarchy makes the objects of this technological  infrastructure explicit and shows how they mediate interaction  between participants. Analysis at this layer provides the mediation  model, and is represented by multi-modal bipartite graphs in  which participants are related to each other via the objects through  which they interact. We call these graphs associograms to  distinguish them from sociograms in a manner that honors  Latour's [27] concept of mediated associations that assemble a  social system. Associograms are multimodal because there may  be two or more types of nodesactors and the various types of  media through which they interactand they are bipartite because  they are divided into two partitions: actors in one partition and the  various types of media objects in the other. Directed arcs  represent state-influence (a weaker form of state-dependency):  they extend from an object to an actor if the state of the object is  influenced by some action of the actor (e.g., writing a message or  editing a wiki), and from the actor to the object if the state of the  actor has been influenced by accessing the object (e.g., reading a  message or wiki)    One can construct associograms from a set of events, whether  taken directly from the event model, or events of interest that were   selected from the contextualized action or interaction models   (contingency graphs or uptake graphs, respectively). A node in  any of these models represents an event, and actors and objects  are attributes of the node. This is largely reversed in an  associogram: actors and objects are nodes, and events are links  between nodes. For example, in Figure 4, w1the event of P2 writing m1becomes a directed association from m1 to P2 (m1s state is influenced by P1), and r1the event of P1 reading  m1becomes a directed association from P1 to m1.  An associogram can be constructed at different granularities.  Object nodes could be created for each individual object (e.g., one  node for each message, wiki page, chat, etc., as in Figure 4), or  they could be aggregated for object types (e.g., all associations via  messages aggregated into a single node, those via wikis in  another, etc.) in order to characterize how interaction is  distributed across types of media [7]. Some information is lost in  either case: all the events involving an actor and an object will fall  into the same two nodes and links between them. For example, if  P1 reads m1 multiple times there is still only one link from P1 to  m1, and if P2 edits a wiki multiple times, there is still one link  from the wiki to P2. Some of this information can be preserved  by weighting the links with number of occurrences, or by putting  backpointers to the originating event nodes. Temporal sequencing  is mostly lost, though it can be recovered by following these  backpointers to the contingency graph. This information reduction  is actually an advantage of associograms: they reduce the clutter  of interaction models to expose recurring patterns of mediation.  An example is given next.   8.1 Finding Interaction Patterns Associograms can help expose patterns of interest in contingency  or uptake graphs. For example, consider the question of finding   Figure 4. From events to associations  69    which participants are in dialogue with each other. Dialogue is  clearly a prerequisite for learning through argumentation [3],  intersubjective meaning-making [42] and group cognition [40]. A  key indicator of the presence of dialogue is what we call a round  trip: one participant makes a contribution that is accessed by  another participant who then makes a contingent contribution  (evidencing uptake) that the first participant then accesses [48]. In  a contingency graph one would need to trace out many paths from  each participant to find paths that go to another participant via a  read and then a write and then back to the first participant. In an  associogram one need only find cycles in the graph. If the links  are weighted with frequency counts, the minimum weight of the  path is taken as a measure of extent of dialogue. For example, in  Figure 4 there is a cycle  (following the arrows in reverse to trace  chronology rather than dependency) P2m1P1m2P2. This corresponds to the round trip in which P2 posts m1, P1 reads it and posts m2 in reply and P2 reads m2, completing the  round trip. Note that P2 need not post a reply to m2 to complete  the round trip: an analysis that looks only at the threading  structure of posted messages and does not include read events  would miss this round trip.    8.2 Characterizing Mediation Degree and path analysis of an associogram can reveal the roles  different media play in a socio-technical network. Media objects  or media types (in an aggregate associogram) that have high in- degree are accessed by many actors, and hence may be influential  sites where an educational intervention can reach many  participants in a socio-technical network. Those with high out- degree are modified by many actors, and hence may be sites  where ideas are aggregated or consolidated (potential roles as  community memory, or locus of knowledge building). In a  weighted associogram, heavily weighted links indicate that actors  visit the incident objects repeatedly. These measures may be  compared between different media types to assess their relative  roles. Additional roles can be identified, such as liaison roles,  where the media object or type connects other objects or actors  that would not otherwise be reachable. For example, we have used  associograms constructed from bridging events to assess the roles  of different media (discussions, wikis, resources and profiles) in  mediating bridging and supporting communities in socio-technical  networks [7, 43].    8.3 Characterizing Mediated Relationships  Associograms summarize how the afilliations between any given  two people [28] are reflected in media associations. The subgraph   of all paths of length two (direct mediation) between two persons  can be used in at least two ways to characterize the relationships  between those persons as mediated by the socio-technical system.  First, we can recognize defined patterns [23], four of which are  shown in Figure 5. Second, profiles of mediated interaction  between any two people can be represented as vectors of the  weights on paths of different types and directions (e.g., P1 to P2 via discussions, P2 to P1 via discussions, P1 to P2 via wikis,  etc.). Cluster analysis of these vectors can reveal recurring  patterns. These approaches are currently being investigated in a  dissertation by Kar-Hai Chu, under the authors direction.    9. SOCIOGRAMS: TIE MODEL Finally, we briefly note that associograms can be transformed to  conventional sociograms by transitive closure of the paths  between actors, or by other computations that interpret patterns of  mediated associations as ties. As shown in Figure 6, this results in  a directed graph or an asymmetric matrix representing the ties  between actors. Well established methods of social network  analysis (SNA) can then be applied [49], but with advantages that  would not be realized if one had merely constructed sociograms  directly from source data (e.g., surveys about ties). A tie in a  sociogram or sociomatrix is really shorthand for a complex  network of multi-mediated interactions that develop over time. If  suitable back-pointers to prior representations (the associograms  and, via them, the contingency graph) are maintained, then results  obtained via network analysis of ties can then be interpreted and  understood by expanding back to the mediation and interaction  models underlying those ties.    In fact, bidirectional construction and deconstruction of ties was  one motivation for the development of this analytic hierarchy. We  wanted to leverage the power of SNA for describing patterns of  relationships between actors in terms of the structural positions  they occupy in relational networks, but wanted to retain the  complex and artifact-mediated interactions that ties summarize. In  classic SNA research, ties are identified through manual  techniques such as interviews or questionnaires, eliciting Figure 5. Pairwise associations  Figure 6. From associogram to sociogram  70    subjective perceptions of relatedness to others. Some computer- mediated environments allow for a more automated data  collection, such as email networks, but those generally represent  explicit, intentional communication. Online environments,  through activity logging, offer the potential of automatically  computing ties from traces of behavior, but they log actions rather  than relationships. Therefore, methods to convert activity and  interaction into ties are of value. These traces may not reflect  subjective perceptions of relationships between persons, but have  the advantage that this conversion can be automated. The analytic  hierarchy supports such an automated translation from activity  logs to mediated interaction and ultimately ties to which SNA  methods can be applied. But the value of the analytic hierarchy  does not only lie in automating the gathering of tie data for SNA.  Analysis and interpretation can go in the other way as well. Once  analytic results are reached using summary representations of  tiesor using bipartite representations of artifact-mediated  relationshipsthese results can be interpreted by unpacking the  binary ties back into the interactional sequence that they  summarize.    Part of the unique power of social network analysis is in the  ability to identify systemic organization, providing a window into  social systems that is otherwise difficult to gain. Taking a  systemic approach to learning frames the learning process as a  group of individuals that, when interacting with each other via the  artifacts of the environment, create a whole greater than any  isolated individual [11]. If we are to analytically study learning  processes it is helpful to have frameworks for analysis that can  trace the patterns of interaction generated from learning processes,  and harness the information available from the structure revealed  by those patterns. The framework presented above provides a  missing piece in the analytic ability to extract structural indicators  (traces) of activity, both intentionally (e.g. discussion board  posts), and unintentionally cast off (e.g. accessing a digital  artifact), from individuals interacting and learning in mediated  environments   10. DISCUSSION  10.1 Status The framework discussed in this paper has been used extensively  in our research through mixtures of manual and ad-hoc automated  analysis, leading to implementation of a principled analytic  software toolkit. Research in our own laboratory is diverse: we  study technology-mediated interaction ranging from dyads to  online communities, and our methods draw on experimental,  conversation-analytic and network analytic traditions. We  developed the framework to make distributed interaction visible,  but also as a means of coordinating the various strands of our own  work. Initially we developed contingency and uptake graphs  through extensive use in manual analyses [30, 46]. Then we  implemented software tools for constructing contingency graphs  automatically from log files and developed rudimentary analytic  tools that leverage these representations [31, 32]; and we have  used associograms in ad-hoc computational analyses [43]. More  recently we developed a comprehensive Traces analytic toolkit  that is implemented in Java with Hibernate persistence and is  applicable to a variety of socio-technical networks. The Traces  design includes: (1) an Entity-Event-Contingency (EEC) core,  supporting the fundamental classes of Entity, Event, and  Contingency; and classes that are likely to be common to all  analyses (e.g., specializations of Entity into ArtifactBase,  ActorBase and IdeaBase abstract classes); (2) an Analytic Core   layer, including classes for Uptake, various types of  Contingencies, and Composites (CompositeEntity, e.g., discussion  forums; CompositeEvent, e.g., chat sessions; and  CompositeContingency, used to represent uptake); (3) a Domain  Core, with common domain objects such as Chat and  Contribution, Discussion and Message, etc. These cores are  extended for application to specific data sources such as Tapped- In, using (4) data source specific extensions to the Domain Core  such as Calendar Events (these may migrate to the core if they are  found to be useful across systems, and (5) classes that map the  data source databases and logs to the above.  Presently the Traces  toolkit does not include associograms: our analyses at the levels of  mediation and tie models are handled by exporting to other tools  available for the task at hand (e.g., Gephi, Jung, Pajek, and  UCINet). We have imported two years of data from Tapped-In,  and are conducting analyses to be reported in future publications.    10.2 Advantages for Communication and  Social Network Analysis  This paper outlines how the analytic framework supports  movement from log data to more abstract representations of  action, interaction, mediation and tie. This analytic framework  offers several advantages for the study of socio-technical systems  and network analysis. First, the framework facilitates the  transition from event data to communication and social network  data. This transition has been a source of methodological  difficulty for social network researchers, as the field is historically  based on self-report and observational data. Event log data exists  for much of the interaction across communication technology  platforms, particularly in online learning settings, but until  recently has remained largely untapped by learning and social  science researchers. Also, the framework can be applied to  integrate events distributed across a variety of media and their  logs (see also [20] for media multiplexity and [23] for cross  media SNA). Second, directional affiliation network analysis is  introduced. There is a growing body of research that represents  actors and group events as two modes of a network in a single  graph, but these affiliation networks generally do not account for  directionality between the two modes. The associogram captures  the direction of affiliation between the users and the media via  which they are associated. For example, one user may edit a  document and another may read it, resulting in a linear direction  between those participants that can be captured and preserved  when represented as an associogram. Third, temporality can be  introduced into a representation of a communication network. The  dynamic nature of events over time can be preserved in the  representation of contingencies, uptake and associograms.  Sociograms linking back to this derivation can recover the over- time nature of tie formation.   10.3 Multi-Level Analysis and Theoretical  Multivocality The analytic hierarchy is not just a data interpretation framework.  It is also intended to be a framework for connecting theorizing at  different levels. Developmentally, the framework arose out of our  own need to reconcile our research on small group interaction in  computer supported collaborative learning and online learning  contexts with our emerging research on online communities. It  was clear that studies of communication networks, and social  network analysis in particular, had something to offer, but the  ties of such analyses seemed to hide away the very processes  we were interested in: the interaction and how it was influenced   71    by and appropriated the media we were designing. Therefore we  constructed the framework as an explicit bridge between analyses  of local interaction and of larger social phenomena, with the  expectation that it would also guide our bridging between  theoretical explanations at these different levels.    This work is sympathetic to calls by Contractor and colleagues [8,  33] for multi-theoretical and multi-level (MTML) analyses and  models of communication networks, but is based on a different  conception of layering than MTML. The MTML approach calls  for examining (1) the properties of individual nodes  (incorporating attributed-based data); (2) properties of the  network under consideration (including dyadic, triadic and global  properties); and (3) relationships of this network to other relations  over the same network constituents or the same relations as they  change over time. These levels change the granularity or scope of  analysis, but stay within an ontology of structural relations  between a set of network constituents. Our hierarchical approach  adds a more vertical dimension, changing the ontology between  layers, from relationships between observed events, to mediated  associations, to direct ties between actors.   Marin & Wellman [29] contrast attribute based explanations,  which explain behavior in terms of attributes of individuals, with  the network analysis' position that causation is not located in the  individual, but in the social structure. Similarity of attributes is  explained by similarity of network positions, due to the similar  constraints, opportunities and perceptions created by these  similar network positions. We agree with their critique of  attribute-based explanations but wish to avoid the opposite  oversimplification: individuals similarities do not arise merely  out of static structures piping influences into the individuals from  without. A range of thinkers, including Garfinkel [16], Blumer [6]  and Latour [27] have argued (each in their own way) that social  regularities are constructed and sustained through interaction  between actors (whether strictly local interaction, as for Garfinkel,  or potentially mediated across time and space, as for Latour). To  fully understand social systems we must examine interaction.  Colleagues2 have offered the analogy that ethnomethodological  interaction is the quantum mechanics of social science. We can  ignore it when explaining social life at a Newtonian level, but to  really understand the origins of the social world we must dive in  and find how fluctuations in micro-phenomena can have an  influence on larger scale change. Latour [27] has made a similar  observation in claiming that the sociology of the social may  seem adequate for explanation of stable states of affairs, but  Actor-Network Theorys sociology of associations is needed to  understand rapidly changing networks.   Our position is that network structures are relevant because of  how they support interaction. The network structure is not  enough: to explain the origins of social life we must understand  the nature of the communication or interaction that takes place. In  socio-technical networks, this includes understanding how that  interaction is embedded in and exploits the resources of  technological infrastructures; i.e., how it is mediated. The present  work offers a conceptualization of how to map between these  different levels of theory and analysis, viz., structure, mediation,  and interaction; and also provides specific representations for  supporting analytic work with computational tools.    In addition to providing a unified analytic artifact and supporting  multiple levels of analysis, a third concern has motivated the work                                                                    2 Ravi Vatrapu, personal communication, July 28, 2007; David Sallach ,   personal communication, May 23, 2010  reported here. Researchers from multiple theoretical and analytic  traditions are studying distributed and networked learning, virtual  organizations, and similar socio-technical networks. This diversity  can mean balkanization, or it can be a strength. A single  integrated discipline of Learning Analytics may not be possible or  even desirable, but there must be some basis for dialogue and  coordination between the traditions. Shared instruments and  representations mediate the daily work of scientific discourse  [26], and advances in scientific disciplines are sometimes  accompanied with representational advances. Similarly,  researchers studying socio-technical networks could benefit from  shared ways of conceptualizing and representing distributed  interaction, or at least from boundary objects [41] that make  discourse between multiple analytic traditions possible. We offer  this framework as a potential basis productive discourse among  multiple analytic voices [47] in the study of socio-technical  networks such as networked learning by enabling the development  of shared conceptualizations, representations, and tools at a given  level of analysis and supporting bridging between different levels  of analysis.   Acknowledgements  This work was supported by NSF Award #0943147. The views  expressed herein do not necessarily represent the views of NSF.  The authors thank Nathan Dwyer, Richard Medina, Ravi Vatrapu,  and Kar-Hai Chu for discussions and collaborations through  which the present framework was developed refined, and Nathan  Dwyer and Kar-Hai Chu for software development.    References  [1] I. E. Allen and J. Seaman, Growing by Degrees: Online   Education in the United States, 2005, Alfred P. Sloan  Foundation, Needham, MA, 2005.   [2] T. Anderson, ed., The Theory and Practice of Online  Learning, Second Edition, Athabasca University Press,  Edmonton, Canada, 2008.   [3] J. Andriessen, M. Baker and D. Suthers, Argumentation,  Computer Support, and the Educational Context of  Confronting Cognitions, in J. Andriessen, M. Baker and D.  Suthers, eds., Arguing to Learn: Confronting Cognitions in  Computer-Supported Collaborative Learning Environments, Kluwer Academic Publishers, Dordrecht, The Netherlands,  2003, pp. 1-25.   [4] S. A. Barab, R. Kling and J. H. Gray, Designing for Virtual  Communities in the Service of Learning, Cambridge  University Press, New York, 2004.   [5] M. W. Berkowitz and J. C. Gibbs, A Preliminary Manual  for Coding Transactive Features of Dyadic Discussion  (unpublished manual), Ohio State University, 1979.   [6] H. Blumer, Symbolic Interactionism: Perspective and  Method, University of California Press, Los Angeles, 1986.   [7] K.-H. Chu, D. D. Suthers and D. Rosen, Uncovering multi- mediated associations in socio-technical networks, Proceedings of the Hawaii International Conference on the  System Sciences (HICSS-45), January 4-7, 2012, Grand  Wailea, Maui, Hawaii (CD-ROM), Institute of Electrical  and Electronics Engineers, Inc. (IEEE), New Brunswick,  2012.  [8] N. S. Contractor, S. Wasserman and K. Faust, Testing multi- theoretical multilevel hypotheses about organizational   72    networks: An analytic framework and empirical example, Academy of Management Review, 31 (2006), pp. 681-703.   [9] J. N. Cummings, T. Finholt, I. Foster, C. Kesselman and K.  A. Lawrence, Beyond Being There: A Blueprint for  Advancing the Design, Development and Evaluation of  Virtual Organizations  (Final report from workshops on  building effective virtual organizations) 2008.  [10] M. de Laat, Networked Learning, Politie Academie,  Apeldoorn, 2006.   [11] M. de Laat, V. Lally, L. Lipponen and R.-J. Simons,  Investigating patterns of interaction in networked learning  and computer-supported collaborative learning: A role for  Social Network Analysis, International Journal of Computer  Supported Collaborative Learning, 2 (2007), pp. 87-103.   [12] S. J. Derry and G. Fischer, Transdisciplinary Graduate  Education, American Educational Research Association, Montreal, Canada, 2005.   [13] A. Duranti, Transcripts, Like Shadows on a Wall, Mind,  Culture & Activity, 13 (2006), pp. 301-310.   [14] M. Elliott, Stigmergic collaboration: The evolution of group  work, M/C Journal, 9 (2006), pp. Retrieved 10 September  2010 from: http://journal.media-culture.org.au/0605/03- elliott.php.  [15] U. Farooq, P. Schank, A. Harris, J. Fusco and M. Schlager,  Sustaining a community computing infrastructure for online  teacher professional development: A Case Study of  Designing Tapped In, Computer Supported Cooperative  Work, 16 (2007), pp. 397-429.   [16] H. Garfinkel, Studies in Ethnomethodology, Prentice-Hall,  Englewood Cliffs, New Jersey, 1967.   [17] D. R. Garrison and H. Kanuka, Blended learning:  Uncovering its transformative potential in higher education, The Internet and Higher Education, 7 (2004), pp. 95-105.   [18] R. Goldman, R. Pea, B. Barron and S. J. Derry, Video  Research in the Learning Sciences, Lawrence Erlbaum  Associates, Inc., Mahwah, NJ, 2007.   [19] C. Haythornthwaite, Crowds and communities: Light and  heavyweight models of peer production, Proceedings of the  42nd Hawaii International Conference on the System  Sciences (HICSS-42), January 5-8, 2009, Waikoloa,  Hawaii (CD-ROM), Institute of Electrical and Electronics  Engineers, Inc. (IEEE), New Brunswick, 2009.   [20] C. Haythornthwaite, Social networks and internet  connectivity effects, Information, Communication &  Society, 8 (2005), pp. 125-147.   [21] B. Jordan and A. Henderson, Interaction Analysis:  Foundations and practice, The Journal of the Learning  Sciences, 4 (1995), pp. 39-103.   [22] S. Joseph, V. Lid and D. D. Suthers, Transcendent Communities, in C. Chinn, G. Erkens and S. Puntambekar,  eds., The Computer Supported Collaborative Learning  (CSCL) Conference 2007, International Society of the  Learning Sciences, New Brunswick, 2007, pp. 317-319.   [23] R. Klamma, M. Spaniol, Y. Cao and M. Jarke, Pattern- based cross media social network analysis for technology   enhanced learning in Europe, Proc. 1st European  Conference on Technology Enhanced Learning (EC-TEL  2006), LNCS 4227, Springer, 2006, pp. 242-256.   [24] R. Kling, Learning about information technologies and  social change: The contribution of social informatics, The  Information Society, 16 (2000), pp. 217-232.   [25] K. Koedinger, D. D. Suthers and K. Forbus, Component- based construction of a science learning space, International Journal of Artificial Intelligence in Education,  10 (1999), pp. 292-313.   [26] B. Latour, Drawing things together, in M. Lynch and S.  Woolgar, eds., Representation in Scientific Practice, MIT  Press, Cambridge, MA, 1990, pp. 19-68.   [27] B. Latour, Reassembing the Social: An Introduction to  Actor-Network-Theory, Oxford University Press, New York,  2005.  [28] C. Licoppe and Z. Smoreda, Are social networks  technologically embedded How networks are changing  today with changes in communication technology, Social  Networks, 27 (2005), pp. 317-335.   [29] A. Marin and B. Wellman, Social Network Analysis: An  Introduction, in P. Carrington and J. Scott, eds., Handbook  of Social Network Analysis, Sage, London, 2010.   [30] R. Medina, D. Suthers and R. Vatrapu, Inscriptions becoming representations, in C. O'Malley, P. Reimann, D.  Suthers and A. Dimitracopoulou, eds., Computer Supported  Collaborative Learning Practices: CSCL 2009 Conference  Proceedings, International Society of the Learning Sciences,  Rhodes, Greece, 2009, pp. 18-27.   [31] R. Medina and D. D. Suthers, Bringing Representational  Practice From Log to Light, in P. A. Kirschner, F. Prins, V.  Jonker and G. Kanselaar, eds., International Perspectives in  the Learning Sciences: Cre8ing a Learning World:  Proceedings of the Eigth International Conference for the  Learning Sciences (ICLS 2008), International Society of the  Learning Sciences Utrecht, 2008, pp. 59-66.   [32] R. Medina and D. D. Suthers, Using a contingency graph to  discover representational practices in an online  collaborative environment, Research and Practice in  Technology Enhanced Learning, 4 (2009), pp. 281-305.   [33] P. R. Monge and N. S. Contractor, Theories of  Communication Networks, Oxford University Press, Oxford,  2003.  [34] E. Ochs, Transcription as theory, in E. Ochs and B. B.  Schieffelin, eds., Developmental Pragmatics, Academic  Press, New York, 1979, pp. 43-72.   [35] K. A. Renninger and W. Shumar, Building Virtual  Communities: Learning and Change in Cyberspace, Cambridge University Press, Cambridge, 2002.   [36] D. Rosen and D. D. Suthers, Stigmergy and collaboration:  Tracing the contingencies of mediated interaction, Proceedings of the Hawaii International Conference on the  System Sciences (HICSS-44),rs, Inc. (IEEE), New  Brunswick, 2011.   73    [37] P. Sanderson and C. Fisher, Exploratory sequential data  analysis: Foundations, Human-Computer Interaction, 9  (1994), pp. 251-318.   [38] M. Scardamalia and C. Bereiter, Knowledge Building  Environments: Extending the Limits of the Possible in  Education and Knowledge Work, in A. DiStefano, K. E.  Rudestam and R. Silverman, eds., Encyclopedia of  Distributed Learning, Sage Publications Thousand Oaks,  CA, 2003, pp. 269-272.    [39] M. Schlager, J. Fusco and P. Schank, Evolution of an Online  Education Community of Practice, in K. Renninger and W.  Shumar, eds., Cambridge University Press, Building Virtual  Communities, 2002, pp. 129-158.   [40] G. Stahl, Group Cognition: Computer Support for  Collaborative Knowledge Building, MIT Press, Cambridge,  MA, 2006.   [41] S. L. Star and J. R. Griesemer, Institutional Ecology,  'Translations' and Boundary Objects: Amateurs and  Professionals in Berkeley's Museum of Vertebrate Zoology, Social Studies of Science, 19 (1989), pp. 387-420.   [42] D. D. Suthers, Technology affordances for intersubjective  meaning-making: A research agenda for CSCL, International Journal of Computer Supported Collaborative  Learning, 1 (2006), pp. 315-337.   [43] D. D. Suthers and K.-H. Chu, Identifying mediators of  socio-technical capital in a networked learning  environment, in L. Dirckinck-Holmfeld, V. Hodgson, C.  Jones, M. de Laat, D. McConnell and T. Ryberg, eds.,  Proceedings of the 7th International Conference on  Networked Learning, Aalborg, Denmark, 2010, pp. 387- 395.  [44] D. D. Suthers and C. Desiato, Exposing chat features  through analysis of uptake between contributions, Proceedings of the Hawaii International Conference on the  System Sciences (HICSS-45), January 4-7, 2012, Grand  Wailea, Maui, Hawaii (CD-ROM), Institute of Electrical  and Electronics Engineers, Inc. (IEEE), New Brunswick,  2012.  [45] D. D. Suthers, N. Dwyer, R. Medina and R. Vatrapu, A framework for conceptualizing, representing, and analyzing  distributed interaction, International Journal of Computer  Supported Collaborative Learning, 5 (2010), pp. 5-42.    [46] D. D. Suthers, N. Dwyer, R. Vatrapu and R. Medina,  Analyzing Interactional Construction of Meaning in Online  Learning, Proceedings of the 39th Hawaii International  Conference on the System Sciences (HICSS-39), January 4- 7, 2006, Poipu, Kauai (CD-ROM), IEEE Computer Society  Press, Poipu, Hawai'i, 2006.   [47] D. D. Suthers, K. Lund, C. Ros, G. Dyke, N. Law, C.  Teplovs, W. Chen, M. M. Chiu, H. Jeong, C.-K. Looi, R.  Medina, J. Oshima, K. Sawyer, H. Shirouzu, J.-W. Strijbos,  S. Trausan-Matu and J. van Aalst, Towards productive  multivocality in the analysis of collaborative learning, in H.  Spada, G. Stahl, N. Miyake, N. Law and K. M. Cheng, eds.,  Connecting Computer-Supported Collaborative Learning to  Policy and Practice: Proceedings of the 9th International  Conference on Computer-Supported Collaborative Learning  (CSCL 2011) International Society of the Learning  Sciences, Hong Kong, 2011, pp. 1015-1022.   [48] D. D. Suthers, R. Medina, R. Vatrapu and N. Dwyer,  Information sharing is incongruous with collaborative  convergence: The case for interaction, in C. Chinn, G.  Erkens and S. Puntambekar, eds., The Computer Supported  Collaborative Learning (CSCL) Conference 2007, International Society of the Learning Sciences, New  Brunswick, 2007, pp. 714-716.   [49] S. Wasserman and K. Faust, Social Network Analysis:  Methods and Applications, Cambridge University Press,  New York, 1994.   [50] E. Wenger, R. A. McDermott and W. Snyder, Cultivating  Communities of Practice: A Guide to Managing Knowledge, Harvard Business School Press, Boston, Mass., 2002.   74    "}
{"index":{"_id":"8"}}
{"datatype":"inproceedings","key":"Brooks:2011:WLC:2090116.2090128","author":"Brooks, Christopher and Epp, Carrie Demmans and Logan, Greg and Greer, Jim","title":"The Who, What, when, and Why of Lecture Capture","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"86--92","numpages":"7","url":"http://doi.acm.org/10.1145/2090116.2090128","doi":"10.1145/2090116.2090128","acmid":"2090128","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics, clustering, lecture capture, participation, recollect, student experience","Abstract":"Video lecture capture is rapidly being deploying in higher-education institutions as a means of increasing student learning, outreach, and experience. Understanding how learners use these systems and relating this use back to pedagogical and institutional goals is a hard issue that has largely been unexplored. This work describes a novel web-based lecture presentation system which contains fine-grained user tracking features. These features, along with student surveys, have been used to help analyse the behaviour of hundreds of students over an academic term, quantifying both the learning approaches of students and their perceptions on learning with lecture capture.","pdf":"The Who, What, When, and Why of Lecture Capture  Christopher Brooks, Carrie Demmans Epp, Greg Logan, Jim Greer  ARIES Laboratory, University of Saskatchewan, 176 Thorvaldson Building, 110 Science Place, Saskatoon, SK,   Canada   cab938@mail.usask.ca,   {c.demmansepp, greg.logan, jim.greer}@usask.ca       ABSTRACT   Video lecture capture is rapidly being deploying in higher-  education institutions as a means of increasing student learning,   outreach, and experience.  Understanding how learners use these   systems and relating this use back to pedagogical and   institutional goals is a hard issue that has largely been   unexplored.  This work describes a novel web-based lecture   presentation system which contains fine-grained user tracking   features.  These features, along with student surveys, have been   used to help analyse the behaviour of hundreds of students over   an academic term, quantifying both the learning approaches of   students and their perceptions on learning with lecture capture.   Keywords  Lecture capture, clustering, analytics, student experience,   participation, Recollect.   1. INTRODUCTION  Lecture video capture solutions (e.g., opencast [1], echo360   [2], epresence [3], virtproducer [4]) are rapidly being adopted by   traditional higher education institutions to increase the levels of   blended learning available to students. This adoption is driven in   part by the dramatic reduction in the costs of technology to   institutions and students, the high availability of broadband   internet access, the proliferation of media rich devices such as   smart phones and tablets, and an interest in repurposing the   traditional and widespread, sage on the stage model of   teaching for anytime anywhere learning.    Despite this adoption, very few studies have been done on the   ways students use lecture capture technology to assist in their   learning.  This paper furthers this area by considering explicitly   how students use the underlying technology, not necessarily its   effects on student marks or enthusiasm. Using a bottom-up   approach, we examine student interactions in the environment   and outline a model for user tracking. The most important result   from this work is the demonstration that low-level tracking data   collected from lecture capture systems can be used to describe   students around pedagogical goals.  We augment this   investigation with a more traditional top-down student survey on   perceptions and experiences. Using grounded theory, we distil a   variety of high-level student opinions about why they used the   lecture capture system the way that they did into a set of   categories that describe their system use.   2. SYSTEM & STUDY   2.1 Recollect Lecture Capture System  The Recollect system was developed in house at the University   of Saskatchewan as an automated lecture capture, processing,   and delivery system. It runs on commodity hardware inside of the   classroom and can record the projector signal, one or more video   cameras, and a single audio input. Students receive emails when   lectures have been processed and published online, and they   view lectures using an Adobe Flash-based web interface.   Recordings from various video devices are merged into single   streams based on declarative templates that are configured on a   class by class basis. For instance, some classes may have a   template that shows a single VGA feed with audio, while others   may include two smaller NTSC-based camera feeds to the left of   the VGA feed. Students cannot switch between feeds while   viewing the lecture; they must watch the video using the   template that the instructor has chosen for the class. Students   cannot download videos, and must watch them through the web   interface.   Recollect allows instructors to create a template for how they   would like the video to appear. All of the templates include a   thumbnail menu on the left-hand side of the screen. The   thumbnails in this menu are auto-detected segments of the   lecture (see [5] for more information) that can be used for   navigation. The traditional media scrubber widget is shown   underneath all videos, and allows for navigation and adjustment   of video parameters (e.g., sound levels).     Instructors are given several options when creating their   template. A typical classroom deployment can be seen in Fig. 1,   where the template chosen by the instructor shows the lecturer   next to his slides.   2.2 Situated Study  The Recollect lecture capture system has been deployed for   several years, and a study investigating its use was conducted   over one 15 week academic term in 2010. During this term,   students from professional colleges and a number of different   disciplines, including the sciences, social sciences, and   humanities, were invited to use the tool to augment their in-class   learning.  The tool was made available to every student enrolled   in the courses being recorded, as well as, depending on the      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that copies  bear this notice and the full citation on the first page. To copy otherwise, or  republish, to post on servers or to redistribute to lists, requires prior specific  permission and/or a fee.   LAK11, February 27-March 1, 2011, Banff, AB, Canada.   Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.     86    instructor, to students in similar courses taught at different times.    A total of 1,125 students participated in using the lecture capture   system, out of roughly 2,000 eligible students1. Students were   shown a brief five-minute introduction to the tool on the first day   of class, and were not given external motivation to use the tool   through payment or academic reward (e.g., marks for   participation). At the end of the term, students were invited to   fill out a 20 minute survey, on paper or online, describing their   experiences with the lecture-capture system for a chance to win a   gift certificate.   This study had two goals: to create a low-level semantic logging   framework that collected student interactions within the learning   environment, and to analyse student interaction and perception   data to form groups based on learning preferences.   The stateless, distributed, and often disconnected nature of web   systems makes user tracking difficult, and most content   management systems which include video content do so only at   the coarsest grain size.  For instance, kaltura [8] and blackboard   [9], only capture analytics at a high level, such as the number of   times a student viewed a video or page.   Unlike other forms of   web content, video usage data at such a high level is difficult to   use because the content changes rapidly and a single page   impression does not characterize how long or the way in which a   user interacted with the video. To address this, we built   Recollect with an event model where every interaction the user   has with the player (e.g., clicking on a button, seeing a popup                                                                  1 Exact numbers of eligible students are unable to be calculated   as instructors or other students could share lectures with   whomever they wished.   thumbnail, seeking to a new time position) sends a tracking   event to the server streaming the video. The player was also built   with a heartbeat mechanism where a new event was created for   every 30 seconds of continuous playback. Each event, regardless   of its originating action, identified the user and aspects of the   high-level video-player state, such as the current position in the   video and whether the video was playing or paused. Many events   also included specific details relevant to the action; for instance,   a seek in the video would include both where (the time offset) in   the video the user was when they started seeking and the position   in the video they chose to seek to. Over the four month period of   the study 3.4 million events were logged with the Recollect   system, 1.8 million of which were heartbeat events that we   analyse in the following section.   3. Quantitative Results  We hypothesised that students could be categorized into different   groups based on their access patterns. In particular, we formed   several sub-hypotheses for each kind of group that we predicted   would occur:    H1: There will be a group of minimal activity learners.   These students may have preferred methods of   achieving their learning goals and will investigate the   tool but not adopt it in any regimented fashion. Note   that we only consider students who try the tool in our   analysis, so this group will contain students who have   viewed at least single lecture through the recording   software.    H2: There will be a group of high activity learners.  These students may not watch all of each lecture, but   Figure 1. The Recollect system showing a typical classroom deployment, with VGA of the presenter desktop on the right-hand   side and a camera view of the instructor roughly in the centre.   87    0 1 2 3 4 5 7 8 9 10 11 12 13 14 15  c0 n n n n n n n n n n n n n n n 110 (47%)  c1 n n n n n n y y n n n n n n n 11 (5%)  c2 n n n n n n n y n n n n n n n 91 (39%)  c3 n n n n n n y y y y y y y n n 11 (5%)  c4 y y y y y y y y y y y y y n n 9 (4%)  Week in Academic T erm Total   ParticipantsCluster  will watch some content each week.  The key element   of this group is that they are embedding video lectures   in their learning routine.    H3: There will be a group of disillusioned learners.   These students will be keen enough to use the tool near   the beginning of the course but will stop using it   because they found it did not aid in their learning.    H4: There will be a group of deferred learners.  These  students will not use the tool at the beginning of the   course but began to use the tool closer to the end of the   course will exist. This could be because students are   leaving learning to the end of the course, or find latter   course content builds on early content thus requiring   more/deeper review.   To test these hypotheses, we inspected heartbeat data for each   student who used the tool for each week in the course. We   discarded the sixth week from each students data as an outlier   because the university was closed for holidays and accesses to   the lecture playback tool were minimal.   Using k-means clustering [10] with the Weka toolkit [11], we   aggregated data for a large class participating in our study2. We   limited our investigations to a single class to control for class-  specific effects such as the timing of assignments and exams, or   the cancelation of class due to holiday or instructor illness. We   changed student access data into nominal values of y indicating   that the student watched at least 10 minutes of lecture video that   week or n to represent that they didnt watch 10 minutes or   more video. Only students who had accessed the system were   included.   The question of the number of clusters to choose when using k-  means is always an issue, with fewer clusters generally seen as   better since the introduction of each new cluster can lead to over-  fitting the model to the data. We chose a number of clusters   equal to our hypotheses plus one as an initial metric. The   addition of the fifth cluster was to account for a group that we   believed would exist but would be hard to classify; those   students who used the tool intermittently to catch up on classes   they may have missed. The results of k-means clustering with   five categories are shown in Table 1.                                                                  2 The class we picked to analyse, a second year Chemistry class,   was chosen because it had a high number of participants. Data   from other classes was omitted due to space constraints.   The Weka-based k-means clustering showed strong support for   three of our four hypotheses. In particular, c0 corresponds well   with H1, c4 corresponds with H2, and c3 corresponds with H4.   Notably, H3, the hypothesis that students would start using the   tool at the beginning of the term but drop off as the term   progressed, was not verified.   The most surprising result was the formation of two clusters   around watching the video only in week eight or the combination   of week seven and week eight (clusters c1 and c2 respectively),   the former being extremely large.  Referencing the course   syllabus, the end of week eight corresponded to the placement of   the midterm exam.  Thus we present a new hypothesis backed by   this data:    H5: There are a group of just-in-time learners. These  students use the tool only for midterm exam review,   though midterm review may stretch over several weeks   of academic lecture time.   Despite a good fit to our initial hypotheses, we experimented   with both more and fewer clusters. The most interesting result   was running k-means with a cluster size of 6 (shown in Table 2).   This data shows students can be clustered well into all of the   hypothesis previously given, including moderate support for H3   through cluster c5. Drop off in engagement could be related to   the tool, the content, or other factors.  As the cluster includes an   increase of viewership before the midterm examination its   unlikely that students dropping the class is the main   characteristic of this cohort.  Further investigations into why   students quit using the tool are being planned, including an   active monitoring of course registrations.  It should be noted that   the increase in number of clusters results in a greater potential   for data over fitting, especially as some clusters become quite   small.    Students who miss a few lectures and review them strategically   are not well represented by these clusters. Evidence from student   surveys and data analysis suggests that, while this group is small,   it does exist. This may be because the clustering algorithm   considers all attributes weighted evenly; as a result, it over   specifies when more generic higher-level attributes might be a   better indicator of the presence of this group. Further, the effects   that other assessment mechanisms in the course have on viewing   is not clear; the course had several assignments which are not   represented in this data, but the sheer number of students   identified as just-in-time learners suggests that this warrants   deeper investigation.      Table 1. Results of k-means clustering with five categories versus the 15 weeks of the academic term. The vast majority of   students fall into the first cluster representing minimal or no accesses to the video playback tools.   88    0 1 2 3 4 5 7 8 9 10 11 12 13 14 15  c0' n n n n n n n n n n n n n n n 104 (44%)  c1' n n n n n n y y n n n n n n n 11 (5%)  c2' n n n n n n n y n n n n n n n 89 (38%)  c3' n n n n n n y y y y y y y n n 11 (5%)  c4' y y y y y y y y y y y y y n n 9 (4%)  c5' y y y y y n n y n n n n n n n 8 (3%)  Week in Academic Term Total   Part icipantsCluster  It was surprising to see that during the time between the last   week of classes (week 13) and the final exam (end of week 15)   no clusters included viewership of lecture material.  This   indicates that student learning strategies with respect to lecture   capture vary between when class is in session and when formal   instruction has ended.  This begs for further investigation; our   initial expectation was that students would have more time and   thus be more likely to use recorded content right before the final   examinations.   4. Qualitative results  At the end of the study period we conducted a survey to collect   both qualitative and quantitative data. The qualitative data   collection was intertwined with the quantitative by inserting   open-ended questions into the appropriate sections of the survey.   We then used grounded theory [12] to extract themes by coding   each participant's responses to open-ended questions about   system features and use.    As a result of the open-ended nature of the questions, it was   possible for students to express multiple ideas, which could   result in a response being coded as belonging to multiple themes.   Even though we collected student opinions about specific system   features, we focus this paper on participant responses to system   use questions since their responses regarding features were   limited to technical details of those features rather than the   motivation behind their overall system use or lack thereof.   There were several students who did not use Recollect. When   students were asked about why they chose to forego using   Recollect they were able to select a reason from a predefined list   or explain their reasons. Of the students who responded to this   question, 81 selected a response from the list. They selected  I   thought the recoded lectures were not valuable  most frequently   (52), but other reasons were also chosen (19 were unaware that   Recollect was available and 10 had limited computer and   internet access). The explanations given by 176 students who   expanded on their behaviour helped us to understand their   approach to how a video capture system, such as Recollect,   should fit into a class at the university level. We identified   several themes from their responses that we combined into the   following categories:    Logistical & Technical Problems: Scheduling or  technical limitations that prevented students from using   the system. This includes system avoidance because of   previous bad experiences and the negative reports of   others.    Unknown Resource: Students were unaware of  Recollect's features or existence.    Don't Want/Need it: Students who thought that their  current resources and learning efforts were sufficient or   better than those provided by Recollect.    Anti e-Learning & e-Support: Students who were  uncomfortable with or against the idea of online   learning materials as well as those who did not want to   help students who had been absent.    Will Only Use When Needed: Students who only used  or would only use Recollect when they missed class or   needed to review a concept.    Haven't Studied Yet: Students who claimed that they  would use Recollect to study for their final exam.    In Fig. 2 (a), we can see that the primary reason for students   abstaining from Recollect use was that they did not feel that they   needed the provided support. Many students  only used it   when [they] missed class  or  as a back-up reference .   The second most common response involved their lack of use of   Recollect because of scheduling or technical constraints, such as   the inability to read what was written on the chalkboard or hear   student questions.   While many students did not use Recollect, many others did; we   asked these students what they liked best about Recollect. We   analyzed their 207 open-ended responses and identified themes   that were grouped into the following categories:    Logistics & Scheduling: Recollect allowed for anytime  anywhere access to lecture material, which meant that   students could maintain the schedules that fit their   lifestyles and did not need to interrupt class or bother   others to obtain course material. This also includes   positive changes to the classroom environment that   resulted from Recollect's use, e.g., only having   interested students in class.    Class Attendance Not Required: Students were happy  that they could miss class when ill or because of   unforeseeable events.    Review & Notetaking: Students could review materials  that they had missed or failed to understand, including   verifying and completing personal lecture notes.    Having It: Students expressed a desire or appreciation  for having the video, which includes positive      Table 2. Results of k-means clustering with six categories. Very similar to the results with five categories shown in Table 2, this   clustering now includes moderate support for H3 where we expected students to use the tool initially but gradually reduce their u   89    statements about specific system features and feature   requests.    Helps Understanding: Recollect in some way facilitated  student understanding of course material. This includes   accommodating learning styles and allowing students   to focus on the message being delivered in class rather   than on taking notes.   As Fig. 2 (b) shows, students appreciated being able to miss   class when they needed to. Recollect enabled them to stay at   home when they were sick without sacrificing their ability to   receive a similar educational experience as they would have had   in class. Students also felt that Recollect  helped when [they]   missed class or couldn't hear  what their instructor was saying.   One nursing student said that she  was able to get a missed   lecture and take better notes  by using Recollect's pause and   play functionality.    The second most appreciated functionality provided by Recollect   relates to this latter student comment since the system allows   students to review materials.  The flexibility that a lecture   capture system provides by enabling anywhere, anytime learning   is phenomenal:  [Recollect] reduced stress if [I was] forced to   miss a class for whatever reason.... Students appreciated that   they  didn't have to worry about contacting [their] classmates   to know what [they] missed .   One student who felt particularly challenged by his workload   said that [he] enjoyed using it for the particularly difficult  a)     b)       Figure 2. a) An analysis of why students, across all courses, did not use the video lecture capture system. While most felt it was   unnecessary, the majority were not against its use. b) What students, across all courses, liked most about having video lectures av  concepts in the course, and it was also really handy when [he]   was being destroyed by a multitude of assignments and midterms   within the span of a week[He] was able to use the time [he]   would be in lecture to attempt to pass all of those things, and [he]   got caught up in class using recollect without being one of those   mass-email jerks that ask for notes in every class.   90    Students also liked having recorded lectures because it allowed   them to listen to their instructor's explanation, which allowed   them to focus on understanding so that they could ask questions   in class and fill in the details in their notes later. Some students   even went as far as never taking notes during class:  I'd listen to   lecture and not take notes, then watch video later and then take   notes .    The argument that attendance is reduced when using lecture   capture technologies is one of the principal issues facing   adoption. While we did not collect metrics to measure   attendance, one computer science students comment indicated   that a lack of attendance is not always a negative when   supporting classroom learning:    [Recollect] also played a part in lowering the attendance of   students who were largely disruptive in class, improves the   classroom environment.    Unlike other lecture capture solutions, Recollect allowed users to   take notes, within the system, while watching the video. Notes   are indexed by the relative slide position using automated   indexing. Students can convert these notes into printable PDFs   which include copies of the slides, and notes are automatically   shared with other students in the class through the note-taking   interface.   Despite the student demand for using lecture capture for note-  taking, very little activity was observed in the provided note-  taking tools included in Recollect. Most students used the videos   to take their own notes on paper.    Student responses to lecture capture were far from uniformly   positive; some students were ambivalent to the learning benefits   it might bring or even directly hostile to the opportunities it   might present to others. One chemistry student stated that I go   to the lectures, so I don't bother watching videos to relearn what I   already did that day. One of the more hostile responses came   from a biosciences student who resented people who did not   attend class. He said:     I showed up for class. Student's who don't show up for class   should not be rewarded with lecture videos. Lecture podcasts   should only be used for off campus education. Showing up for   work is a reality of life, and students should get used to showing   up for commitments they have made.   5. Conclusions & Future Research  The adoption of lecture capture is increasing, with many   institutions  making selected lectures available on the web for   free viewing through initiatives like the OpenCourseWare   (OCW) consortium [13], portals like iTunes U [14], or YouTube   Edu [15]. Lecture capture is particularly well suited for   traditional higher education institutions that want to leverage   their faculty and classroom experiences in increasingly connected   online learning environments.    This paper has made contributions to the understanding of what   kinds of students use lecture capture systems, when those   students engage in reviewing content online, and why they are   motivated to use this technology. Analytics in learning systems   can be used to provide both auditing and interventions in student   learning. While we intend neither of these explicitly with this   work, we aim to scaffold support for them by demonstrating how   a low-level video logging tool can use automated clustering   techniques to group students into pedagogically motivated   cohorts. This form of analytics has been largely unexplored when   it comes to lecture capture, and fits well with the heritage of   intelligent and adaptive learning systems described in the user   modelling and artificial intelligence in education communities.   Building intervention tools to take advantage of these clusters,   such as intelligent content recommendation or help seeking tools   is a natural next step.   What motivates students to use lecture capture is a broad   question. We explored this by asking students a mixture of   closed and open-ended questions. Students provided candid   feedback and, while the diversity of opinion on how and even   whether lecture video should be used varies, the opportunity to   review a lecture and make notes seem central to the learning   process.   Our analysis limits itself to a single cohort for quantitative   measures and to a single semester of data for qualitative   measures.  Tying these two data sets together into a single model   is difficult; collecting qualitative data is expensive, and the   diversity of teaching approaches in different courses makes   collapsing usage data into one coherent set non-trivial.    Nonetheless, even a surface analysis as we have done results in   interesting and pedagogically useful results.  Armed with the   knowledge of how students use lecture recordings, we can begin   to build intervention tools and strategies to increase student   learning and satisfaction in rich media education environments.   6. References  [1] opencast community project,   http://www.opencastproject.org/    [2] Echo360 Inc., http://www.echo360.com/    [3] ePresence, http://epresence.tv/    [4] Mertens, R., Ketterl, M., Vornberger, O.: The virtPresenter   lecture recording system: Automated production of web   lectures with interactive content overviews. Int. J. of   Interactive Technology and Smart Education (ITSE) 4(1),   55--66 (2007)   [5] Brooks, C., Amundson, K., Greer, J.: Detecting Significant   Events in Lecture Video using Supervised Machine   Learning. In: International Conference on Artificial   Intelligence in Education (AIED), Brighton, UK (2009)   [6] McCalla, G.: The Ecological Approach to the Design of E-  Learning Environments: Purpose-based Capture and Use of   Information About Learners. J. of Interactive Media in   Education 2004(1), (2004)   [7] Najjar, J., Wolpers, M., Duval, E.: Attention Metadata:   Collection and Management. World Wide Web Conference   (WWW) Workshop on Logging Traces of Web Activity: The   Mechanics of Data Collection (2006)   [8] Kaltura open source video, http://corp.kaltura.com/    [9] Blackboard, http://www.blackboard.com/    [10] Witten, I., Frank, E.: Data Mining Practical Machine   Learning Tools and Techniques. Morgan Kaufmann, San   Francisco (2005)   [11] Weka, http://www.cs.waikato.ac.nz/ml/weka/    91    [12] Lazar, J., Feng, J., Hochheiser, H.: Research Methods in   Human-Computer Interaction. John Wiley & Sons Ltd,   Glasgow (2010)   [13] OpenCourseWare Consortium,   http://www.ocwconsortium.org    [14] iTunes U, http://www.apple.com/education/itunes-u/    [15] YouTube EDU, http://www.youtube.com/educationb=400       92      "}
{"index":{"_id":"9"}}
{"datatype":"inproceedings","key":"Vatrapu:2011:TVA:2090116.2090129","author":"Vatrapu, Ravi and Teplovs, Chris and Fujita, Nobuko and Bull, Susan","title":"Towards Visual Analytics for Teachers' Dynamic Diagnostic Pedagogical Decision-making","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"93--98","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090129","doi":"10.1145/2090116.2090129","acmid":"2090129","publisher":"ACM","address":"New York, NY, USA","keywords":"computer supported collaborative learning (CSCL), learning analytics, learning sciences, multivocality, teaching analytics, visual analytics","Abstract":"The focus of this paper is to delineate and discuss design considerations for supporting teachers' dynamic diagnostic decision-making in classrooms of the 21st century. Based on the Next Generation Teaching Education and Learning for Life (NEXT-TELL) European Commission integrated project, we envision classrooms of the 21st century to (a) incorporate 1:1 computing, (b) provide computational as well as methodological support for teachers to design, deploy and assess learning activities and (c) immerse students in rich, personalized and varied learning activities in information ecologies resulting in high-performance, high-density, high-bandwidth, and data-rich classrooms. In contrast to existing research in educational data mining and learning analytics, our vision is to employ visual analytics techniques and tools to support teachers dynamic diagnostic pedagogical decision-making in real-time and in actual classrooms. The primary benefits of our vision is that learning analytics becomes an integral part of the teaching profession so that teachers can provide timely, meaningful, and actionable formative assessments to on-going learning activities in-situ. Integrating emerging developments in visual analytics and the established methodological approach of design-based research (DBR) in the learning sciences, we introduce a new method called Teaching Analytics and explore a triadic model of teaching analytics (TMTA). TMTA adapts and extends the Pair Analytics method in visual analytics which in turn was inspired by the pair programming model of the extreme programming paradigm. Our preliminary vision of TMTA consists of a collocated collaborative triad of a Teaching Expert (TE), a Visual Analytics Expert (VAE), and a Design-Based Research Expert (DBRE) analyzing, interpreting and acting upon real-time data being generated by students' learning activities by using a range of visual analytics tools. We propose an implementation of TMTA using open learner models (OLM) and conclude with an outline of future work","pdf":"Towards Visual Analytics for Teachers' Dynamic  Diagnostic Pedagogical Decision-Making   Ravi Vatrapu, Chris Teplovs, Nobuko Fujita  Computational Social Science Laboratory (CSSL)   Department of IT Management  Copenhagen Business School   vatrapu@cbs.dk  Susan Bull  Electronic, Electrical and Computer Engineering   University of Birmingham   s.bull@bham.ac.uk  ABSTRACT The focus of this paper is to delineate and discuss design  considerations for supporting teachers dynamic diagnostic  decision-making in classrooms of the 21st century. Based on the  Next Generation Teaching Education and Learning for Life  (NEXT-TELL) European Commission integrated project, we  envision classrooms of the 21st century to (a) incorporate 1:1  computing, (b) provide computational as well as methodological  support for teachers to design, deploy and assess learning  activities and (c) immerse students in rich, personalized and  varied learning activities in information ecologies resulting in  high-performance, high-density, high-bandwidth, and data-rich  classrooms. In contrast to existing research in educational data  mining and learning analytics, our vision is to employ visual  analytics techniques and tools to support teachers dynamic  diagnostic pedagogical decision-making in real-time and in actual  classrooms. The primary benefits of our vision is that learning  analytics becomes an integral part of the teaching profession so  that teachers can provide timely, meaningful, and actionable  formative assessments to on-going learning activities in-situ.  Integrating emerging developments in visual analytics and the  established methodological approach of design-based research  (DBR) in the learning sciences, we introduce a new method called  Teaching Analytics and explore a triadic model of teaching  analytics (TMTA). TMTA adapts and extends the Pair Analytics  method in visual analytics which in turn was inspired by the pair  programming model of the extreme programming paradigm. Our  preliminary vision of TMTA consists of a collocated collaborative  triad of a Teaching Expert (TE), a Visual Analytics Expert  (VAE), and a Design-Based Research Expert (DBRE) analyzing,  interpreting and acting upon real-time data being generated by  students learning activities by using a range of visual analytics  tools. We propose an implementation of TMTA using open  learner models (OLM) and conclude with an outline of future  work  Categories and Subject Descriptors H.5.3 Group and Organization Interfaces: Theory and models, Asynchronous interaction Collaborative computing, Evaluation/methodology; H.1.2 User/Machine Systems: Software  Psychology.  General Terms Design, Human Factors, Theory    Keywords Visual analytics, learning analytics, teaching analytics, learning  sciences, computer supported collaborative learning (CSCL),  multivocality.   1. INTRODUCTION Learning analytics is the use of intelligent data, learner-produced  data, and analysis models to discover information and social  connections, and to predict and advise on learning.1 The LAK  2011 conference call for papers defines learning analytics as the  measurement, collection, analysis and reporting of data about  learners and their contexts, for purposes of understanding and  optimizing learning and the environments in which it occurs. In  this paper, we present our vision of leveraging learning analytics  tools and techniques to support teachers dynamic diagnostic  pedagogical decision-making in actual K-12 classroom settings.  Our vision seeks to extend the current state-of-the-art in learning  analytics in at least four directions, to apply learning analytics in  the primary and secondary education formal classroom settings  compared to tertiary education settings, focus on real-time use of  learning analytics by teachers for technology enhanced formative  assessment, apply an extended version of the pair analytics  method in visual analytics, and finally, to review and build on  current work in the learning sciences and the method of design- based research. The primary contribution of our paper is the  presentation of the preliminary triadic model of teaching analytics  (TMTA).  The remainder of this paper is organized as follows. In section 2,  we briefly review two strands of research on analyzing learning  data from computer supported collaborative learning (CSCL) and  higher education. In section 3, based on the Next Generation  Teaching Education and Learning for Life (NEXT-TELL)  European Union integrating project proposal, we present the new  demands faced by teachers in classrooms of the 21st century.  Section 4 introduces the concept of teaching analytics and  presents the preliminary triadic model of teaching analytics  (TMTA). In section 5, we conclude the paper with the  identification of several challenges and directions for future work.                                                                     1 http://www.elearnspace.org/blog/2010/08/25/what-are-learning-  analytics  Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM ISBN 978-1-4503-1057-4$10.00.   93    2. RELATED WORK  We present below two selective reviews of recent empirical work  in learning analytics from computer supported collaborative  learning (CSCL) and the Learning Analytics in higher education.    2.1 Computer Supported Collaborative  Learning (CSCL)  Various researchers [23, 24] in CSCL have considered the role of   productive multivocality  in the analysis of collaborative  learning2.  Multivocality refers to fact that CSCL researchers  take diverse theoretical, methodological, and analytical  approaches to the empirical study of how technology enhanced  interaction supports learning processes and leads to learning  outcomes. Multivocality can either be a source of strength (in the  diversity of perspectives on a complex phenomenon such as  learning) or a symptom of weakness (incoherency, divergence of  empirical findings, incommensurability of perspectives and so  on). Productive multivocality can be achieved only if the  voices share sufficient objects to reach some degree of  coherence in the discourse of the field3.  Through a series of workshops, a group of CSCL scholars have  sought to bring together different researchers, who brought with  them a variety of data sets and analytic tools and approaches.   Part of the motivation in doing so was to determine the degree to  which there was commonality to support dialog between the  various players and reach some degree of coherence in their  discourse. The workshop participants were asked to consider  analytic efforts across five dimensions: purpose of analysis, unit  of interaction, data and analytic representations used, analytic  manipulations, and theoretical orientation. Suthers et al. [22] have  extended some of these ideas further by developing what they call  an  uptake analysis framework  to help conceptualize, represent,  visualize, analyze and interpret distributed interactions.   However, in CSCL, one side effect of the symmetrical socio- technical configurations of students, equitable division of labor,  shared conception of the problem, and shared task goals is the  displacement of the  teacher from the analytical center and a  delimitation of the teachers role to that of a facilitator at worst  and a curriculum designer/learning architect at best.  Moreover,  there exists a gulf of relevance between the emerging results of  learning analytics work in CSCL and the professional practice of  teachers. Creating solutions and generating implications for the  professional practice of teachers has been a topic of interest and  importance within CSCL [e.g., 14, 15, 18] and we seek to re- engage with that.   2.2 Learning Analytics in Higher Education  Networked learning analytics were first proposed for Learning  Management Systems (LMS) such as Blackboard and Moodle  with the objective of collecting data from learners in a non- intrusive, unobtrusive and automatic ways in order to trace the  trajectory of the learning process and for appraisal and assessment  of the effectiveness of online and blended courses [19].                                                                     2 http://engaged.hnlc.org/story_comments/list/13 3 CSCL 2009 Workshop: Common Objects for Productive   Multivocality in Analysis  http://engaged.hnlc.org/story_comments/list/13  Emerging empirical results indicate that Learning Analytics can  help predict student performances with respect to learning across  a variety of courses and academic programs in higher education.  The use of academic analytics generated actionable intelligence  for designing early interventions for freshman students at-risk of  not returning for the sophomore year at the University of  Alabama from 1999-2001 [7]. Another example is the Signals  program [2] at the Purdue university mined institutional data from  campus IT systems, analyzed the collected data, identified at-risk  students and generated actionable information for designing  educational interventions. Results show a significant  improvement of student learning performance and subjective  satisfaction  [2].     Prior findings also show that monitoring and predicting the key  performance indicators (KPI) of students with the help of learning  analytics can help in designing, tailoring and targeting highly  effective student interventions [9, 12].   Further, current results show the benefits of using learning  analytics for performance monitoring and outcomes prediction for  student populations in general at higher education institutes  beyond the at-risk student segment [10, 16, 21, 29, 31]   2.3 Summary and Critique  An overarching observation is that the voice of the researchers  and administrators in many of these approaches and studies comes  through loud and clear. What is less prominent is the voice of the  teacher or practitioner. We have evidence that the voice of the  teacher can be very powerful when it comes to learning analytics.  Some studies [25] have suggested that the sorts of detailed  information that have typified analytic feedback have been useful  to researchers, a more intuitive, user-friendly, and visually  sophisticated representation is more powerful for use by teachers  for just-in-time assessment.   Knowledge building systems, with formative assessment, can be  conceptualized as a cybernetic system with feedback loops  serving to drive the system in new directions [20]. To optimize  performance, feedback must be relevant and timely. Analysis of  discourse from computer-supported collaborative learning  environments is common but, as noted in [13], relatively little  attention has been paid to the formative, embedded, and  transformative aspects of assessment in collaborative inquiry.   We offer two scenarios based on real anecdotes suggesting new  ways in which teachers, researchers, and analysts can interact to  support rapid feedback.   2.3.1 Scenario #1  Students engaged in online knowledge building often appear to be  collaborating but the extent to which they are doing so is not often  apparent.  Are students really working together to build  knowledge  What evidence can we garner that that is happening   One fourth-grade teacher was facing exactly those questions, and  she was able to use a graphical social network analysis tool to  show the sociograms that resulted from looking at who was  interacting with whom in the online database.  She used this tool  to help her understand the extent to which students were  interacting.  At one point, a group of teachers from another school  district visited her classroom and posed similar questions.  She  immediately started the social network analysis tool, and showed  the visitors what she thought were unimpressive results:  the data  showed that all students were interacting.  Of course, the visitors  were anything but unimpressed.  They were stunned by four   94    things:  that the students were interacting to such an extent, that  the data to support such a claim were readily available; that the  tools existed to provide simple representations of complex  phenomena, and that she was able to use and demonstrate the tool  so effectively.   2.3.2 Scenario #2  An experienced teacher was working with her 10-12 year old  students on a module about electricity.  The students were very  engaged and had spent considerable time working through  interesting problems. They had contributed a considerable number  of notes to the online database that they used to track their  inquiries and the unit had already gone on for several weeks.  But  were they covering the mandated curriculum topics How could  she obtain objective verification that her students had covered the  curriculum even if she believed they had A visual analytics  expert had devised a tool that allowed a user to visualize the  degree to which the curriculum had been covered.  By literally  lining up the curricular expectations on one side of the screen and  the students' traces on the other side and examining the links  between them the visual analytics expert was able not only to  reassure her that her students were well on track, but to also allow  her to see the few remaining curricular expectations that needed  to be covered.  The teachers' feedback on the visualization led the  visual analytics expert to improve the visualization tool to make  the same sorts of comparisons easier in the future.   Though these may seem far-fetched or perhaps, unique scenarios,  we argue that they are both representative of learning and  teaching situations encountered in formal learning settings.  Particularly, when we consider the new demands being made on  teachers in the 21st century classroom.   3. NEXT-TELL: NEW DEMANDS ON  TEACHERS IN THE 21TH CENTUERY  CLASSROMS According to Peter Reimann and colleagues of the Next  Generation Education, Teaching and Learning for Life (NEXT- TELL)4 integrating project recently funded under the European  Commissions Seventh Framework Programme, the following are  the new demands that teachers face in the 21st century classrooms  (NEXT-TELL Consortium, 2010).    Develop 21st Century competencies in addition to subject- matter specific Knowledge, Skills and Attitudes (KSAs)5   Personalize learning by planning lessons and learning  activities for the individual student6   Teach adaptively in the classroom, making good use of ICT  [8, 17]    Provide evidence-based accounts for selected learning  activities and assessments                                                                     4 Peter Reimann et.al, www.next-tell.eu 5 European Reference Framework: Key competences for lifelong   learning. 6 Harnessing Technology for Next Generation Learning: Children,   schools and families Implementation Plan 2009-2012.  Downloadable from BECTA:  http://publications.becta.org.uk/display.cfmresID=39547   Be accountable towards stakeholders (students, parents,  policy makers).   As the NEXT-TELL project consortium says:    In order to deal with these demands, teachers need to  rapidly capture an ever-increasing amount of  information about students learning, interpret this  diverse body of information in the light of students  development, appraise it in light of curricular goals, and  make reasoned decisions about next learning steps.  However, in comparison with most other professionals  from whom clients expect rapid decisions in a  dynamically changing environment, presently teachers  often do not get the information they need for decision  making in a timely fashion and in an 'actionable'  format. This is particularly a challenge in technology- rich settings (the school computer lab, the laptop  classroom) with high content and communicative  density, where students engage with learning software  and tools that teachers can only partially follow at any  point in time. However, as technology increasingly is  permeating all schools and all classrooms, the challenge  is there for all to face. (Peter Reimann et al., 2010)   Drawing on this, we propose that learning analytics research  should focus on providing both computational and  methodological support for teachers in real-time and in-situ classroom settings. Towards this end, we sought to integrate  emerging developments in visual analytics and the established  methodological approach of design-based research (DBR) in the  learning sciences. The results of this integrative exercise are the  approach called Teaching Analytics and a model of teaching  analytics, termed triadic model of teaching analytics (TMTA),  discussed next.   4. TRIADIC MODEL OF TEACHING  ANALYSIS (TMTA)  Our model of teaching analytics seeks to adopt and extend the  model of pair programming from the software engineering  paradigm of Extreme programming.  We propose an extensible  triadic model. More specifically, teaching analytics adapts the  Pair Analytics method [1] in visual analytics [26]. The Pair  Analytics method was inspired by the Pair Programming7 model  in the Extreme Programming8 software engineering approach.  In  pair programming, all code to be sent into production is created  by two people working together at a single computer8. Our  vision can be outlined as below:   To empirically explore the effectiveness, efficiency and  satisfaction in fundamentally transforming the teaching profession  from a lone ranger model to the collaborative model where  teachers, analysts and researchers with complementary expertise  collaboratively leverage their knowledge, skills and aptitudes  towards enhancing learning in high-performance/high-bandwidth/ high-density classrooms of the 21st century.   However, the dyadic configuration of driver and navigator in  pair programming and pair analytics creates a bootstrapping  problem for learning settings: can we really throw a Visual                                                                     7 http://www.extremeprogramming.org/rules/pair.html 8 http://www.extremeprogramming.org/  95    Analytics Expert (VAE) and Teaching Expert (TE) together into a  classroom setting and expect them to work productively without  explicit facilitation, intelligent scaffolding, and guided design   Facilitating interaction is a role that can be fulfilled by a Design- Based Research Expert (DBRE).  As such, we adapt and extend  the dyadic model of pair analytics in visual analytics to a Triadic  Model of Teaching Analytics (TMTA) as shown in Figure 1:   Figure 1. Triadic Model of Teaching Analytics (TMTA)  At its core, our model sees collaborative knowledge building  between teachers, analysts and researchers. Each has a  complementary role in the teaching analytics setting.   Eliciting criteria for Teaching Analytics involves a collocated  collaborative triad of a Teaching Expert (TE), a Visual Analytics  Expert (VAE), and a Design-Based Research Expert (DBRE)  analyzing, interpreting and acting upon real-time data being  generated by students learning activities by using a range of  visual analytics tools.  We think of the relationships between the TE, VAE and DBRE as  a dynamic socio-technical system.  The design considerations are  about creating feedback loops between the three individuals, such  that each one drives the other two to higher levels of performance  on the positive side (with the cost of anxiety in the negative case).  That is, feedback from the teacher inspires the VAE to create  new, better visualizations and for the researcher to better  understand the ongoing teaching and learning processes while  feedback from the VAE  perhaps in the form of visualization  artifacts  allows the teachers to better understand what is going  on in the classroom from a learning activity design perspective  and the research to hypothesize, test and predict student learning  trajectories and performance outcomes. All in all, these feedback  loops should culminate in the teacher providing timely,  meaningful actionable, customized and personalized feedback to  students. The key point here is that each member of the  triumvirate of TE, VAE, and DBRE can gain from the other two,  not that each partner's role is to highlight deficiencies of the other  two. Therefore, TMTA involves a close collaboration between the TE,  VAE, and the DBRE. It includes teaching practitioners in the  design process and invites them to contribute significantly to the  innovation of the visual analytics tools. This allows these learning  analytics tools to address pedagogical issues as they arise and  evolve in real classrooms. In the next section, we outline an  approach to TMTA based on open learner models (OLM).   5. TMTA AND OPEN LEARNER MODELS An obvious starting point for developing the TMTA approach is  to base it around the existing work in Artificial Intelligence in  Education, on open learner models. A learner model holds   information (usually) about an individual learner, and the model  is automatically and dynamically updated during the user's  interaction with a computer-based/online educational  environment. The learner model typically includes data about the  learner's knowledge state, which may include specific difficulties  and misconceptions; and it can also have data on other aspects of  the learning process (e.g. representation, content, teaching style  preferences; motivational, social, affective attributes). The learner  model is then used by the educational environment to adapt its  teaching to the specific needs of the individual learner (the  environment 'understands' the user's understanding). An  open  learner model  is a learner model that can also be externalised to  the user [4]. This externalised (open) learner model may be  simple or complex in format using, for example: text, skill meters,  concept maps, hierarchical structures, animations [3].  Normally the user who accesses the learner model is the learner.  Common purposes of externalising the learner model to learners  are to promote metacognitive activity such as awareness-raising,  reflection, self-assessment and planning [5]. Some learner models  have, however, also been made available to teachers [6, 11, 30].  Teacher access to the learner models of their students can help  them to better understand learners' needs as individuals and as a  group, and can therefore enable teachers to adapt their teaching.  Of particular interest in NEXT-TELL is the possibility of open  learner models to support the routine but dynamic decision- making that teachers need to perform in the classroom.  While the above describes the typical situation of open learner  models, it is easy to envisage this being extended for use in  TMTA. A range of visualisations or externalisations of the learner  model have been explored (e.g. Bull et al., 2010), and these could  be further extended to support the synthesis of work between  teaching experts, visual analytics experts and design-based  research experts, as required for the proposed TMTA approach.  6. DISCUSSION As mentioned in the prior section, we conceive of the Triadic  Model of Teaching Analytics (TMTA) as a socio-technical  system. Such systems are characterized by socio-technical  interactions. The design considerations are to develop, deploy and  evaluate the use and impact of the perception and appropriation of  socio-technical affordances in the TMTA socio-technical system.  Affordances are action-taking possibilities and meaning-making  opportunities in an actor-environment system relative to the  competencies of the actor and the capabilities of the system [28].  Based on the theory of socio-technical interactions in technology  enhanced learning environments developed in [27, 28], we  propose that design dimensions based on affordance classes [39]  can help inform realize the idea of TMTA. Future work will  consist of a systematic exploration and exploitation of the  affordance classes in different socio-technical configurations of  TMTA. In conclusion, we would like to highlight the similarity between  the TMTA and the productive multivocality framework  mentioned in the introduction.  Whereas the productive  multivocality framework focuses on relationships between  researchers, the TMTA extends that multivocality to include  teachers, design-based researchers, and visual analytics experts.  Each voice in the system shares the goal for sustained innovation  in leveraging the design of affordances of visual analytic tools to  support teachers' dynamic diagnostic pedagogical decision  making.  96    7. ACKNOWLEDGEMENTS This work is supported by the NEXT-TELL - Next Generation  Teaching, Education and Learning for Life integrated project  co- funded by the European Union under the ICT theme of the 7th  Framework Programme for R&D (FP7).   8. REFERENCES [1]  Arias-Hernandez, R., Kaastra, L.T., Green, T.M. and Fisher, B   Capturing Reasoning Processes in Collaborative Visual  Analytics. Proceedings of Hawai'i International Conference  on System Sciences 44 January 2011, Kauai, Hawai'i, 2011, CD-ROM.  [2]  Arnold, K. E. Signals: Applying Academic Analytics.  EDUCAUSE Quarterly, 33, 1, 2010,  http://www.educause.edu/EDUCAUSE+Quarterly/EDUCAU SEQuarterlyMagazineVolum/SignalsApplyingAcademicAnal yti/199385.  [3]  Bull, S., Gakhal, I., Grundy, D., Johnson, M., Mabbott, A. and  Xu, J. Preferences in Multiple View Open Learner Models. Springer, City, 2010.   [4]  Bull, S. and Kay, J. Student Models that Invite the Learner In:  The SMILI Open Learner Modelling Framework.  International Journal of Artificial Intelligence in Education  17, 2 2007, 89-120.   [5]  Bull, S. and Kay, J. Metacognition and Open Learner Models. City, 2008.   [6]  Bull, S. and McKay, M. An Open Learner Model for Children  and Teachers: Inspecting Knowledge Level of Individuals and  Peers. Springer, City, 2004.   [7]  Campbell, J., DeBlois, P. and Oblinger, D. Academic  Analytics:A New Tool for a New Era of Educational  Research. EDUCAUSE Review 42, 4 2007),  http://net.educause.edu/ir/library/pdf/erm0742.pdf.  [8]  Crawford, V., Schlager, M., Penuel, W. and Toyama, Y.  Supporting the art of teaching in a data-rich, high- performance learning environment. Teachers College Press,  City, 2008.   [9]  Dawson, S., Heathcote, L. and Poole, G. Harnessing ICT  potential: The Adoption and Analysis of ICT Systems for  Enhancing the Student Learning Experience. International Journal of Educational Management, 24, 2 2010, 116128.   [10]  Donald Norris, L. B., Joan Leonard, Louis Pugliese, and Paul  Lefrere Action Analytics: Measuring and Improving  Performance That Matters in Higher Education. educause Review, 43, 1 2008,  http://www.educause.edu/EDUCAUSE+Review/EDUCAUSE ReviewMagazineVolume43/ActionAnalyticsMeasuringandIm p/162422.  [11]  Eyssautier-Bavay, C., Jean-Daubias, S. and Pernin, J.-P. A Model of Learners Profiles Management Process. IOS Press,  City, 2009.   [12]  Finnegan, C., Morris, L. V. and Lee, K. Differences by Course  Discipline on Student Behavior, Persistence, and Achievement  in Online Courses of Undergraduate General Education.  Journal of College Student Retention, 10, 1 2008-2009, 39-54  [13]  Lee, E., Chan, C. and van Aalst, J. Students assessing their  own collaborative knowledge building. International Journal  of Computer-Supported Collaborative Learning, 1, 2, 2006,  277-307.  [14]  Lockhorst, D., Admiraal, W., Pilot, A. and Veen, W. Design  Elements for a CSCL Environment in a Teacher Training  Programme. Education and Information Technologies, 7, 4,  2002, 377-384.   [15]  Lund, K. and Baker, M. Teachers collaborative  interpretations of students computer-mediated collaborative  problem-solving interactions. IOS Press, City, 1999.   [16]  Nichols, W. and Dayton, K. A Technology-Based, Flexible  Delivery of Instruction. Conference on Information  Technology (CIT 2007), 2007,  http://www.league.org/2007cit/special_sessions.cfm.  [17]  Penuel, W., Roschelle, J. and Abrahamson, L. Research on  classroom networks for whole-class activities. Proceedings of  the IEEE International Workshop on Wireless and Mobile  Technologies in Education, Los Alamitos, CA, IEEE, (2006),  222-229.  [18]  Resta, P., Christal, M., Ferneding, K. and Puthoff, A. K.  CSCL as a catalyst for changing teacher practice. Proceedings of the 1999 conference on Computer support for collaborative  learning1999, 60.   [19]  Retalis, S., Papasalouros, A., Psaromiligkos, Y., Siscos, S. and  Kargidis, T. Towards Networked Learning AnalyticsA  concept and a tool. Networked Learning 20062006), http://www.networkedlearningconference.org.uk/past/nlc2006/ abstracts/pdfs/P2041%2020Retalis.pdf.  [20]  Roos, B. and Hamilton, D. Formative assessment: a cybernetic  perspective. Assessment in Education: Principles, Policy &  Practice, 12, 1 2005, 7-20.   [21]  Sloan, B. and Search, S. Integrated Learning Support: Greater  Than the Sum of its Parts. Innovations 2010, http://www.league.org/iStreamSite/content/ppt/INV2010.  [22]  Suthers, D., Dwyer, N., Medina, R. and Vatrapu, R. A  framework for conceptualizing, representing, and analyzing  distributed interaction. International Journal of Computer- Supported Collaborative Learning, 5, 1 2010, 5-42.   [23]  Suthers, D., Law, N., Lund, K., Rose, C. and Teplovs, C.  Common objects for productive multivocality in analysis. In  Proceedings of the Proceedings of the 9th international  conference on Computer supported collaborative learning -  Volume 2 (Rhodes, Greece, 2009). International Society of the  Learning Sciences.  [24]  Suthers, D. and Lund, K. Productive multivocality in the  analysis of collaborative learning. In Proceedings of the  Proceedings of the 9th International Conference of the  Learning Sciences - Volume 2 (Chicago, Illinois, 2009).  International Society of the Learning Sciences.    [25]  Teplovs, C., Donoahue, Z., Scardamalia, M. and Philip, D.  Tools for concurrent, embedded, and transformative  assessment of knowledge building processes and progress. In  Proceedings of the Proceedings of the 8th iternational   97    conference on Computer supported collaborative learning (New Brunswick, New Jersey, USA, 2007). International  Society of the Learning Sciences.    [26]  Thomas, J. and Kielman, J. Challenges for visual analytics.  Information Visualization, 8, 4 (Win 2009), 309-314.   [27]  Vatrapu, R. Toward a Theory of Socio-Technical Interactions  in Technology Enhanced Learning Environments. Springer- Verlag, City, 2009.   [28]  Vatrapu, R. Explaining Culture: An Outline of a Theory of  Socio-Technical Interactions. Proceedings of the 3rd  International Conference on Intercultural Collaboration (ICIC  2010), 111-120.   [29]  White, B. and Barnery, P. Reaching Students Through  Analytics. Innovations 2006, http://www.league.org/i2006/program/index.cfm.  [30]  Zapata-Rivera, D., Hansen, E., Shute, V. J., Underwood, J. S.  and Bauer, M. Evidence-Based Approach to Interacting with  Open Student Models. International Journal of Artificial  Intelligence in Education, 17, 3 2007, 273-303.   [31]  Zhang, H., Almeroth, K., Knight, A., Bulger, M. and Mayer,  R. Moodog: Tracking Students' Online Learning Activities. AACE, City, 2007.   98      "}
{"index":{"_id":"10"}}
{"datatype":"inproceedings","key":"Ferguson:2011:LAI:2090116.2090130","author":"Ferguson, Rebecca and Shum, Simon Buckingham","title":"Learning Analytics to Identify Exploratory Dialogue Within Synchronous Text Chat","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"99--103","numpages":"5","url":"http://doi.acm.org/10.1145/2090116.2090130","doi":"10.1145/2090116.2090130","acmid":"2090130","publisher":"ACM","address":"New York, NY, USA","keywords":"educational dialogue, exploratory dialogue, instant messaging, learning analytics, synchronous dialogue, text chat","Abstract":"While generic web analytics tend to focus on easily harvested quantitative data, Learning Analytics will often seek qualitative understanding of the context and meaning of this information. This is critical in the case of dialogue, which may be employed to share knowledge and jointly construct understandings, but which also involves many superficial exchanges. Previous studies have validated a particular pattern of 'exploratory dialogue' in learning environments to signify sharing, challenge, evaluation and careful consideration by participants. This study investigates the use of sociocultural discourse analysis to analyse synchronous text chat during an online conference. Key words and phrases indicative of exploratory dialogue were identified in these exchanges, and peaks of exploratory dialogue were associated with periods set aside for discussion and keynote speakers. Fewer individuals posted at these times, but meaningful discussion outweighed trivial exchanges. If further analysis confirms the validity of these markers as learning analytics, they could be used by recommendation engines to support learners and teachers in locating dialogue exchanges where deeper learning appears to be taking place.","pdf":"Learning Analytics To Identify Exploratory Dialogue within  Synchronous Text Chat   Rebecca Ferguson  IET, The Open University   Walton Hall  Milton Keynes   +44 (0)1908 654856   r.m.ferguson@open.ac.uk   Simon Buckingham Shum  KMi, The Open University   Walton Hall  Milton Keynes   +44 (0)1908 655723   s.buckingham.shum@gmail.com  ABSTRACT While generic web analytics tend to focus on easily harvested  quantitative data, Learning Analytics will often seek qualitative  understanding of the context and meaning of this information.  This is critical in the case of dialogue, which may be employed to  share knowledge and jointly construct understandings, but which  also involves many superficial exchanges. Previous studies have  validated a particular pattern of exploratory dialogue in learning  environments to signify sharing, challenge, evaluation and careful  consideration by participants. This study investigates the use of  sociocultural discourse analysis to analyse synchronous text chat  during an online conference. Key words and phrases indicative of  exploratory dialogue were identified in these exchanges, and  peaks of exploratory dialogue were associated with periods set  aside for discussion and keynote speakers. Fewer individuals  posted at these times, but meaningful discussion outweighed  trivial exchanges. If further analysis confirms the validity of these  markers as learning analytics, they could be used by  recommendation engines to support learners and teachers in  locating dialogue exchanges where deeper learning appears to be  taking place.   Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Education   collaborative learning, distance learning.    General Terms Theory.   Keywords educational dialogue, text chat, instant messaging, exploratory  dialogue, learning analytics, synchronous dialogue.   1. INTRODUCTION Learning resources are being uploaded to the Internet at such a  rate that is increasingly likely that individuals will find themselves  adrift in an ocean of information [1, p136]. Resources that  extend over time, such as conference recordings, videos and real- time dialogue capture are difficult to scan or assess quickly and so  learners and teachers must rely on basic, often misleading, cues   such as title, keyword and producer when deciding whether to  make use of a resource. Analytics are therefore needed to  distinguish between resources that extend over time, and to  identify those that support learning. This paper investigates the  use of key words and phrases to identify sections of Elluminate  online conference sessions that have inspired participants to  engage in knowledge building through dialogue in the associated  synchronous text chat.  In other contexts, various approaches have been used to identify  and classify forms of learning dialogue and academic dialogue but  these are typically dependent on the use of grammatically correct,  carefully punctuated and formally structured text [2,3].  Synchronous textual dialogue is likely to be more akin to speech  than to formally constructed prose [4]. It is therefore relevant to  look at how people build knowledge together through speech. In  face-to-face settings, Mercer and his colleagues [5-9] have  distinguished three social modes of thinking used by groups of  learners: disputational, cumulative and exploratory. Of the three,  exploratory dialogue is the type considered most educationally  desirable by teachers [10]. Mercer and Littleton [8, p62] provide a  clear description of its use in a school environment:  Exploratory talk represents a joint, coordinated form of co- reasoning in language, with speakers sharing knowledge,  challenging ideas, evaluating evidence and considering options in  a reasoned and equitable way. The children present their ideas as  clearly and as explicitly as necessary for them to become shared  and jointly analysed and evaluated. Possible explanations are  compared and joint decisions reached. By incorporating both  constructive conflict and the open sharing of ideas, exploratory  talk constitutes the more visible pursuit of rational consensus  through conversation.  Exploratory dialogue is a form of discourse that may be found in  both online and offline learning environments [4,11], where it can  be taken as an indication that learning is taking place and that  learners are going beyond a simple accumulation of ideas. The  research reported here therefore asks: Could the identification of  exploratory dialogue within the synchronous textual chat  associated with online resources help to identify resources and  sections of resources that support learning   2. DATA COLLECTION & PREPARATION  In order to investigate these questions, data were collected from  Elluminate, a web conferencing tool that supports chat alongside  video, slides and presentations. The focus was on the synchronous  discussion related to a two-day online teaching and learning  conference. The Elluminate text chat in four conference sessions,  each between 150 and 180 minutes in length (24,530 words in  total) was investigated. During these four sessions, 233  participants logged in to the Elluminate sessions at one or more  times. The majority of these participants were higher education   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.   99    researchers and practitioners from around the world, although  most were based in the UK. Analysis presented in this paper  focuses mainly on the afternoon session of 22 June, when 120  people logged in to the Elluminate discussion and 67 actively  participated in the Elluminate synchronous text chat. Of these  participants, 47 were female (26 contributed to the text chat), 54  were male (34 contributed to the text chat) and the gender of 19 is  unknown (7 contributed to text chat). The conference timetable  was used to subdivide the four main conference sessions into  smaller units, including pre-session chat, post-session chat,  conference introduction, groups of short talks, longer talks,  moderated discussion and keynotes.   The four conference sessions were all archived and made public  by the organisers. Sociocultural discourse analysis [12] was used  to identify words that could be indicative of exploratory dialogue.  These included:  Challenges eg But if, have to respond, my view  Critiques eg However, Im not sure, maybe  Discussion of resources eg Have you read, more links  Evaluations eg Good example, good point  Explanations eg Means that, our goals  Explicit reasoning eg Next step, relates to, thats why  Justifications eg I mean, we learned, we observed  Others perspectives eg Agree, here is another   Ninety-four words and phrases were identified in this way. Some  words, phrases and punctuation, which initially appeared to be  good indicators, were discarded because they were often used for  finding out more about the conference, its tools and participants,  rather than its content. For example, interrogatives and question  marks were often associated with comments such as Can you still  hear or whats everyone doing for coffee Once exploratory  markers had been identified, the Elluminate chat was pasted into  Microsoft Word, where a simple find and replace Apple Script  program was used to highlight the key words and phrases. The  data was then transferred to Excel for more detailed analysis.  Table 1 gives an example of a section of data in which six of nine  consecutive postings were coded as exploratory. By way of  contrast, Table 2 gives an example of nine consecutive postings in  the data, none of which was coded as exploratory.   Table 1. Dialogue coded as exploratory (real names removed  from all data samples). Each row represents one contribution.   Words in bold have been highlighted by the analysis.  what about quality control Not all authors are as skilled as  [named individual]... ie could get some dreadful blunders in public!   If youre doing it in public and people are following, that wil  help in terms of quality control for egregious errors. However, if youre writing it in a vacuum, then that's not going to work  so well, I guess.  Also, not everyones blog has the traffic [named individual]s  has Is it only skills that are needed, is it Theres something to do  with attitude (to criticism, to mistakes, etc)  [named  individual] (sorry, no mic)  Shouldnt, in theory, course authors be writing carefully in the  first place, because its going to be seen by hundreds or  thousands of students  I would just note that a course team of 6 or 7 people can feel  plenty public enough when you are trying to form thoughts -  this is back to my point about there being stages where its   good to be quite closed while you evolve an idea and approach   @[other conference participant] - yes, I think that educators  will get a lot more out of this scary paradigm shift if they have  set up some peers who also want to learn  [Another conference participant] Definitely or even interested  amateurs.  Table 2. Dialogue not coded as exploratory. Each row represents   one contribution.  like the idea of a virtual poster session   shame - sadly I cant make Friday   >embarrassed< @[named participant]  itll be asynchronous so drop in  any time Audio dropped out   Uhoh  Sound cut out   Weve lost you [named participant]   Once key words had been highlighted, the postings were divided  according to the timings on the official conference timetable, and  the use of exploratory dialogue in each section was calculated. As  postings are short and clearly delineated, the posting was taken as  the unit of analysis, and so an entire posting containing one or  more markers of exploratory dialogue would be coded as  exploratory.   The conference included two morning sessions and two afternoon  sessions. The first phase of analysis focused on identifying the  periods containing the greatest concentration of exploratory  markers. The four main sessions were first rated on the amount of  turns in the conversation rated as exploratory (those containing  one or more of the words/phrases indicating exploratory talk). The  total number of exploratory turns was divided by the number of  people contributing to the posting dialogue, to give an average  number of exploratory posts per person.   Total exploratory turns            in dialogue             = Average no. of exploratory  No. of people contributing                     posts per person        to posted dialogue   In addition, the number of words in the turns considered  exploratory were totaled and divided by the number of people  contributing to the posted dialogue, to give an average number of  exploratory words per person. The results are shown in Table 3.           Total words in          exploratory turns             = Average no. of exploratory  No. of people contributing                     words per person        to posted dialogue   100    Table 3. Comparing exploratory turns per person in main  conference sessions   22 June  am   22 June  pm  23 June  am   23 June  pm  Average no. of  exploratory  turns per   person  1.6 3.5 2.3 1.9   Average no. of  exploratory  words per   person  32.7 59.1 41.4 39.3   In both cases the afternoon session of 22 June pm, which  contained one of the two keynote sessions, appears to have  inspired the most exploratory dialogue. On the other hand, the  morning session that day appears to have inspired the least  learning dialogue. As the four conference sessions differed in  length, it is possible that these differences were related to having  more or less time in which to post.   Table 4 therefore indicates the average number of exploratory  turns posted per minute, and the average number of words in  exploratory turns posted per minute.   Table 4. Comparing exploratory turns per person in the main  conference sessions   22 June  am  22 June  pm  23 June  am  23 June  pm  Average no. of  exploratory turns   per minute 0.7 1.1 0.7 0.5   Average no. of  exploratory  words per   minute  14.8 19.2 12.8 11.1   Once again, the afternoon of 22 June contained the most  exploratory dialogue. However, the least exploratory dialogue  now appears to have taken place on the afternoon of 23 June.  Further analysis is required in order to investigate which of these  measures is most relevant.   As all measures indicate that the afternoon of 22 June contained  the highest concentration of exploratory markers, the following  analysis concentrates on that session. During that afternoon, the  Elluminate session was divided into four sections: a set of short  talks, moderated discussion, keynote, and then chat between the  scheduled end and the actual close of the Elluminate session.   Table 5 presents a summary of analysis of that afternoons  Elluminate chat. As the length of the sessions ranged from 8 to 75  minutes, contributions were first classified by time. This showed  that the most posts per minute took place during the informal chat  session, whereas the most exploratory posts were contributed  while the keynote was in progress. The series of short talks at the  beginning of the afternoon appeared to be associated with the  lowest levels of talk, whether exploratory or not.   On the basis of markers of exploratory dialogue, it therefore  appears that conference participants engaged in the highest levels  of knowledge construction on the afternoon of 22 June, and that  these levels were highest during the 75-minute keynote  discussion. It is not possible to represent the whole Elluminate  session here, but Table 7 provides a one-minute extract. If a   recommendation engine were to use these markers to identify  sections of the 12-hour conference where knowledge construction  took place, it would recommend sections like those shown in  Table 6.   This minute contains 6 exploratory turns (mean for the keynote  was 1.6 per minute), it contains 192 words (mean for the keynote  was 67.5 per minute) and it contains 81 words in the exploratory  turns (mean for the keynote was 31.1 per minute). Using the  markers has clearly not identified all the exploratory turns, but it  has successfully identified a section including challenge (line 5),  critique (line 7), discussion of resources (lines 2 and 5), evaluation  (line 12), explicit reasoning (line 4) and consideration of the  perspective of others (line 9, among others). The discussion  moves fast, contains contributions from nine different  participants, and is grounded by references to two online  resources as well as to examples drawn from three separate higher  education establishments. It clearly relates to the presentation that  is taking place in the audio channel, which is referenced in lines 1  and 2, and it moves this discussion on by posing questions and  relating the discussion to personal experience.   Table 5. Comparing contributions to the synchronous  Elluminate text chat during one continuous afternoon   conference session (22 June)  Short  talks  60 mins   Discussion  45 mins   Keynote  75 mins   Chat 8 mins   Posts per minute Mean no. of   posts per  min   2.4 4.6 5.8 8.8   Mean  wordcount  per min  21.3 47.8 67.5 57.6   Mean no. of  exploratory   posts per  min   0.4 1.2 1.6 0.9   Mean  wordcount exploratory   posts per  min   5.2 18.6 31.1 11.8   Posts per contributor Mean no. of   posts per  contributor   5.4 5.6 10.1 3.3   Mean word  count per   contributor  47.3 58.2 117.7 22.0   Mean  exploratory   posts per  contributor   0.9 1.5 2.8 0.3   Mean word  count of   exploratory  posts per   contributor   11.4 22.6 54.2 4.5   101    As Elluminate identifies who has posted each comment in the text  chat, it was also possible to consider the postings of individuals  (only the participants who made some contribution to the live chat  were included in this analysis). Once again, a large amount of  exploratory activity was evident during the keynote on 22 June,  whereas the many contributions during the informal chat were  found to be short and lacking in exploratory talk. When analysed  in this way, participants were seen to be contributing longer, more  thoughtful posts during the short talks at the beginning of the  afternoon but, once again, the exploratory dialogue was less  evident during these short talks than during the moderated  discussion or the keynote.   Table 6. One minute of Elluminate text chat, with exploratory  markers highlighted in bold   1 aaagh the imaginary wife again [speaker]  2 [The speaker]s argument is very congruent with John  Seely Brown et als arguments on the shift from Push to  Pull http://en.wikipedia.org/wiki/Pull_Platforms   3 @[online participant] the value is in what they learn   4  the problem is there is not one suitable analogy as the  role is still very complex  its easier to understand it in  regards to what we do not do anymore (e.g.,  supply/provide all content)   5  Get rid of the notion of space and place... -- not sure  about that there is a well developed notion in architecture  of a Place as a meaningful Space, which has since been  translated to collaborative systems  PDF  http://bit.ly/bM6Xr2   6 Who is going to generate the knowledge then   7 what about learning goals and learning outcomes they stay  8 What about the research into teaching agenda   9 I agree that a common lamentation from students asked  to construct their own way is What am I paying for you  for  10 it;s also about inspiration take this backchannel for example  11 Students come to my institution to spend time with the great minds   12 At the OU, it is the ALs who are the biggest luddites, in my experience.  Table 7 summarises analysis of the activity by individuals during  the keynote talk and compares the contributions of the five people  who posted the most contributions during this session. The  moderator (M) was very active, posting 32 times.   For all these individuals who posted a large number of posts, more  than a fifth of their words were in contributions containing  exploratory markers. However, there are notable differences  within these groups, and C stands out as a high-volume poster  with 75% of her total words in posts containing exploratory  markers.   These figures were typical of those in other sessions  the  moderator was consistently one of the most active contributors.  Although individuals interest and attention clearly fluctuated  according to session, C was often among those with the highest  percentage of exploratory posts in a session.   A recommendation engine using markers of exploratory dialogue  to search for people engaged in learning would therefore have  highlighted C. Her contributions, such as the one below,  incorporated important elements of knowledge building, including  explanation, explicit reasoning and discussion of resources. In  addition, many of her contributions, like this one, were addressed  to a named participant in the conference, indicating that she was  engaging in dialogue, rather than adding didactic comments or  personal reflection.   Learning should be an active process, [named participant].  Lectures, like television, tend to be more passive. Not all lectures  are like that, but most are. Recording them and putting them on  the web doesnt make them any better. Thats why social knowing  (John Seely Brown) is so much better, where people come  together to construct knowledge through their conversations and  interactions with each other. Alternatively, engaging in projects  or experiments can be useful.   Table 7. Analysis of the contributions of the five individuals  who contributed the most posts during the keynote session   Contributor   A B C D M   Posts (Mean = 5.6) 16 16 17 27 32   Wordcount (Mean = 58) 111 183 297 210 253  Exploratory posts  (Mean = 1.5) 3 3 11 6 7   Exploratory wordcount  (Mean = 22.6) 43 38 224 75 105   Exploratory posts as % of  personal posts 19% 19% 65% 22% 22%   Exploratory wordcount as  % of personal wordcount 39% 21% 75% 36% 42%   3. DISCUSSION  Preliminary analysis of the data suggests that markers of  exploratory dialogue can be used to distinguish meaningfully  between Elluminate sessions and to support evaluation of those  sessions. The markers proved to be a more nuanced tool than  generic analytics, such as simply counting the numbers signed in  for an Elluminate session, or contributing to the text chat. Peaks  of posting activity were associated with the end of Elluminate  sessions, when many participants were thanking speakers and  saying goodbye, while others were discussing what they had  learned. Peaks of exploratory activity, on the other hand, were  associated with periods set aside for discussion and keynote  speakers. Fewer individuals posted at these times, but meaningful  discussion outweighed trivial exchanges.   Exploratory markers indicate the importance of context when  assessing learning dialogue. When several speakers were  presenting in close succession, posting activity was relatively low,  but increased as the presentations came to an end. However, when  speakers had time to engage in discussion as part of their allotted  timeslot  as was the case with the keynote speaker  meaningful  exchanges peaked. Unscheduled chat at the beginning of  Elluminate sessions tended to be primarily social in nature, while  unscheduled chat was likely to include many more exploratory  exchanges.   102    This has implications for those scheduling online conferences   clearly flagged discussion sessions related to presentations will be  easier to find in the archives than discussions that overrun into  other sessions. Discussion continues after scheduled sessions, so it  could prove useful to leave Elluminate sessions open for chat for  some time after the end of the scheduled presentation.   Not all exploratory dialogue related to conference content  there  was considerable discussion of online conferences and of social  issues. A future set of exploratory markers should identify  keywords such as mike, sound and how are you that would  signal a move away from discussion of content. At this stage,  though, analysis suggests that time needs to be set aside for these  exchanges, to avoid distraction or cognitive overload when  presentations begin.   4. AREAS FOR INVESTIGATION  Data analysis covered two complete days of online conference,  and only a representative sample can be presented in a paper of  this length. However, the analysis to date is clearly limited in its  scope and there is a pressing need for evaluation of the reliability  and validity of these presumed markers of exploratory dialogue   both individually, and as a set. If this set, or an amended set, of  markers can be shown to be reliable and valid it will be important  to attend to both context and practicalities. Exploratory dialogue is  not necessarily focused on learning about content  individuals  and groups are also likely to be learning about the tools they use  (such as Elluminate) and the people with whom they are  interacting. This type of learning dialogue is of less interest for  people participating after the event, as they are neither using the  same tools in the same way nor interacting with the same people.   From a practical perspective, the current analysis is mainly carried  out manually and in future it will be necessary to investigate how  this process can be carried out automatically in order to benefit  both learners and educators. It will also be useful to investigate  the relationship between the text chat and the audio and video  channels.   Compared to other computational linguistics approaches to text  analysis, the approach presented in this paper is very simple; we  are testing the limits of the simple exploratory dialogue markers  described. In parallel, however, we are also beginning to test more  complex forms of computational rhetorical analysis as described  by Sndor [13,2], as a way to detect linguistic phenomena  associated with the making of knowledge-level claims around  open educational resources, on which we hope to report in future  work.   5. CONCLUSION  Although the conference sessions studied here are freely available  as open online resources, they are both difficult and time- consuming for users to navigate. The published timetable of the  conference gives some guidance, but is limited because a few  sessions were reorganized, started late or overran. Some provoked  little debate, whereas others inspired discussion which extended  far beyond the scheduled time period. The conference also  included set-up sessions and breaks, during which talk turned to  the practicalities of microphone use, and the absence of virtual  biscuits. There is therefore a need for analytics that will allow  learners to locate sections of an Elluminate session that clearly  support learning.   At the same time, both learners and educators can benefit from  tools that allow them to use Elluminate and other, similar,  resources more effectively. Analytics can be used to distinguish  different types of contribution to text chat, and to support learners  who wish to engage in more fruitful learning discussion. They can  be used to help educators schedule events in order to support  discussion, and to model exploratory dialogue within that  discussion.  6. REFERENCES  [1] Roach, S. S. 1988. Technology and the services sector:   America's hidden competitive challenge. National Academies  Press, Washington DC.   [2] Sndor, . and Vorndran, A. 2009. Detecting key sentences  for automatic assistance in peer review research articles in  educational sciences. In Proceedings of the 2009 Workshop  on Text and Citation Analysis for Scholarly Digital  Libraries, ACL-IJCNLP 2009 (Suntec, Singapore).    [3] Whitelock, D. and Watt, S. 2007. Open Mentor: supporting  tutors with their feedback to students. In Proceedings of the  11th CAA International Computer Assisted Assessment  Conference, 10/11 July 2007 (Loughborough).   [4] Ferguson, R. 2009. The Construction of Shared Knowledge  through Asynchronous Dialogue. PhD, The Open University,  Milton Keynes. http://oro.open.ac.uk/19908/  [5] Mercer, N. 1995. The Guided Construction of Knowledge:  Talk amongst Teachers and Learners. Multilingual Matters  Ltd, Clevedon.   [6] Mercer, N. 2000. Words & Minds. Routledge, London.  [7] Mercer, N. 2002. Developing dialogues. Blackwell   Publishers, Oxford.  [8] Mercer, N. and Littleton, K. 2007. Dialogue and the   Development of Children's Thinking. Routledge, London and  New York.   [9] Mercer, N. and Wegerif, R. 1999. Is 'exploratory talk'  productive talk Routledge, London.   [10] Wegerif, R. 2008. Reason and dialogue in education. Cambridge University Press, Cambridge.   [11] Ferguson, R., Whitelock, D. and Littleton, K. 2010.  Improvable objects and attached dialogue: new literacy  practices employed by learners to build knowledge together  in asynchronous settings. Digital Culture and Education, 2,  1, 116-136.   [12] Mercer, N. 2004. Sociocultural discourse analysis: analysing  classroom talk as a social mode of thinking. Journal of  Applied Linguistics, 1, 2, 137-168.   [13] Sndor, ., Kaplan, A. and Rondeau, G.  2006. Discourse  and citation analysis with concept-matching., Caen, France.  Web publication  www.unicaen.fr/services/puc/article.php3id_article=686  103    "}
{"index":{"_id":"11"}}
{"datatype":"inproceedings","key":"Fournier:2011:VLA:2090116.2090131","author":"Fournier, H'el`ene and Kop, Rita and Sitlia, Hanan","title":"The Value of Learning Analytics to Networked Learning on a Personal Learning Environment","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"104--109","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090131","doi":"10.1145/2090116.2090131","acmid":"2090131","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics tools, big data, educational research, learning analytics, massive open online courses","Abstract":"Some might argue that the analytics tools at our disposal are currently mainly used for boring purposes, such as improving processes and making money. In this paper we will try to define learning analytics and their purpose for learning and education. We will ponder on the best possible fit of particular types of research methods and their analysis. Methodological concerns related to the analysis of Big Data collected on online networks as well as ethical and privacy concerns will also be highlighted and a case study of the use of learning analytics in a Massive Open Online Course explored.","pdf":"The Value of Learning Analytics to Networked Learning on  a Personal Learning Environment   Hlne Fournier PhD.  National Research Council of   Canada 100, rue des Aboiteaux, suite 1100  Moncton, NB, E1A 7R1, Canada   1 5068610957  Helene.Fournier@nrc-cnrc.gc.ca  Rita Kop PhD.  National Research Council of   Canada 100, rue des Aboiteaux, suite 1100  Moncton, NB, E1A 7R1, Canada   1 5068610965  Frederika.Kop@nrc-cnrc.gc.ca  Hanan Sitlia   National Research Council of   Canada 100, rue des Aboiteaux, suite 1100  Moncton, NB, E1A 7R1, Canada   1 5068610957  hsitlia@gmail.com  ABSTRACT Some might argue that the analytics tools at our disposal are  currently mainly used for boring purposes, such as improving  processes and making money. In this paper we will try to define  learning analytics and their purpose for learning and education.  We will ponder on the best possible fit of particular types of  research methods and their analysis. Methodological concerns  related to the analysis of Big Data collected on online networks as  well as ethical and privacy concerns will also be highlighted and a  case study of the use of learning analytics in a Massive Open  Online Course explored.   General Terms Design, Human Factors, Theory.   Keywords Learning Analytics, Analytics Tools, Massive Open Online  Courses, Educational Research, Big Data.   1. INTRODUCTION Conference10, Month 12, 2010, City, State, Country.   The Internet and its recent tools and Web developments have  added new research and evaluation tools to the arsenal of the  educational researcher [2][3]. As educational practice and the  settings in which learning takes place have changed with the  proliferation of the Internet and its available tools, careful thought  about these tools and considerations of the processes and means  with which data is being collected and analyzed is once again  required [4]. Lazer et al. [4] stress that social scientists have  lagged behind researchers in other fields, for instance in fields  such as biology and physics, and that it is unavoidable for  analytics to become part of social science research. Moreover,  they emphasize the urgency for a data-driven computational  social science to develop based in an open academic  environment, rather than in the domain of private companies   such as Google and Yahoo, and government agencies who are  currently the main players in the analytics field.  They answer the  question: What value might a computational social science   based in an open academic environment  offer society, by  enhancing understanding of individuals and collectives [4,  p.721]. We would like to add to this question one in the context of  this paper: what would it offer stakeholders in the evaluation and  improvement of the learning process: educators, researchers,  administrators and learners themselves In this paper we will  illustrate the research methods used in exploring networked  learning on a Massive Open Online Course.   2. DEFINING LEARNING ANALYTICS  Analytics of web environments have been around for a while. The  first reports we could find were from the mid 90s, and relate to  the analysis of market trends using web logs and browser tags  online [5].  Boyd highlights the Big Data development on the  Internet, which has created unprecedented opportunities for  people to produce and share data, interact with and remix data,  aggregate and organize data. . . . [3, p1]. Educause [6] highlights  features of analytics tools:  provide statistical evaluation of rich  data sources to discern patterns that can help individuals at  companies, educational institutions, or governments make more  informed decisions. There is a clear contradiction in this  sentence: statistical evaluation of rich data sources. It seems that  statistical evaluation is the perfect tool not for rich data sources,  but for a multitude of data sources. Some researchers in the  qualitative tradition might argue that rich data sources would be  better analyzed through qualitative methods as these would be  better at capturing the depth and richness than statistical analysis  could do.   Norris et al. [7] have a slightly different emphasis on the use of  analytics; they would like analytics to be used to measure,  compare and improve the performance of individuals, not just to  better the experience but also to facilitate better outcomes to the  activity. In the more specific context of education and learning  some interesting distinctions in ideas and definitions on analytics  have been proposed.  Most analytics are related to the  introduction of Learning Management Systems (LMSs) and are  sometimes called Academic Analytics [8][9][10]. With the  introduction of LMS came the back office functionality that  would provide traces of participant activities on the system and  this data was then used to aid the management and effectiveness  of institutional teaching and learning. Dawson et al. [9] added that  the analysis of this data might be used to improve the student  learning experience, which would not only require a quantitative   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.   104    analysis, but also a qualitative one, or at least a qualitative  interpretation of findings. The interpretation would have to  include a value judgment on peoples use of the environment: not  only counting who uses the environment for what, but also  judging what might be a good and what might be a bad  experience, and offering suggestions for moving on the  continuum from one to the other. The conference organizers  provided us with their definition of learning analytics, which  seems to express most of the above: Learning analytics is the  measurement, collection, analysis and reporting of data about  learners and their contexts, for purposes of understanding and  optimising learning and the environments in which it occurs.                         3. DIFFERENT TYPES OF ANALYSIS FOR  DIFFERENT PURPOSES  It follows that not only quantitative data should be used, but also  qualitative as data collection would not only relate to the increase  of the effectiveness of learning, for instance by showing trends in  use, but also with possible changes in the learning process.  As  highlighted by Downes [10]: There are different tools for  measuring learning engagement, and most of them are  quantificational. The obvious ones [measure] page access, time- on-task, successful submission of question results  things like  that. Those are suitable for a basic level of assessment.  You can  tell whether students are actually doing something. Thats  important in certain circumstances. But to think that constitutes  analytics in any meaningful sense would be a gross  oversimplification. There is a whole set of approaches having to  do with content analysis. The idea is to look at contributions in  discussion forums, and to analyze the kind of contribution. Was it  descriptive Was it on-topic Was it evaluative Did it pose a  question [10]  Parry [11] and Kop [12] highlight possibilities to take this one  step further, and suggest that analytics could be used not only to  provide managers of learning and possibly educators and learners  with information that they can use to improve learning, but also to  provide learners with recommendations in their learning based on  earlier learning activity. This would, however, require that data is  used in a different way than in academic analytics. It would entail  that data is not just analyzed with steps being taken by people to  improve the performance in formal education but rather that  technological means are being used to link data and use it to  improve learning. This would change the realm of analytics and  move it outside academic management and perhaps back onto the  open wider Web, as people dont only learn in a formal academic  environment but also outside it. We are most interested in  analytics on online networks as our research interest is in Personal  Learning Environments (PLEs).   4. METHODS USED IN LEARNING  ANALYTICS OF ONLINE NETWORKS Research in the intricacies of learning taking place on online  networks is one of the means for our research into the design and  development of a PLE. If people are encouraged to move away  from the institution for their learning, it is important to find out if  the informal (online) networks in which people do find their  information and where they might develop understandings, are  valuable to their learning experience. A network in the context of  this paper would be an open online space where people meet, as  nodes on networks, while communicating with others and while  using blogs, wikis, audio-visuals and other information streams   and resources. De Laat [13] highlighted the complexity of  researching networked learning and emphasized as key problems  the issues of human agency and the multitude of issues involved,  such as the dynamics of the network, power-relations on the  network, and the amount of content generated. Effective analysis  would require a multi-method approach. He suggested the use of  computer-generated content analysis to explore what people are  discussing. In addition, interviews with an emphasis on critical  event recall focusing on the experiences of participants to find out  why they are talking as they do and Social Network Analysis to  find out the dynamics of the network to see who communicates  with whom [13, p. 110]. This seems a viable choice of research  methods. Social Network Analysis would be a form of learning  analytics, and a quantitative method, and could clarify who the  central nodes on the network were, in other words which people  on the network performed vital roles of connecting to the  otherwise un-connected. It could also provide information on the  importance of connectors to other networks, which would be  important in finding out who the innovators on the network were,  i.e. the ones to link vital information streams [14].   We would argue for the use of additional qualitative methods and  that virtual ethnography would be the most appropriate method of  qualitative research on learning networks.  Researchers in this  tradition work towards research data analysis that reflects as  closely as possible what is happening in the chosen setting. The  researcher is interested in the processes taking place, the  perspectives and understandings of the people in the setting, the  details, context, emotion and the webs of social relationships that  join persons to one another [15, p. 55]. Hine [16] highlighted that  in a technologically rich environment, such as the Internet, the  technology itself and the artifacts it produces should be taken into  consideration in the online ethnography as well, as these are part  of the research setting and might influence the human interactions  researched.    As vast amounts of data are being generated in networked  learning in an open environment, computational tools for analysis  and interpretation will have to play a role in the research. Some  argue for a mixed-method approach in educational research as  the theories we hold, and the training we have received, critically  affect the data we collect and the lenses we choose in looking at  such data [17, p. 30]. They argue that the use of more than one  method in research will increase its robustness [15]. Boyd [3, p.2- 5], a social scientist researching Big Data highlights some other  methodological concerns especially when analyzing Big Data  collected on online networks: 1) bigger data is not always better  data than obtained in other research as reliability will very much  depend on the sampling strategies being used; 2) caution needs to  be taken as not all data are created equally; 3) what people do is  of limited importance unless you also ask people why they did  what they did; 4) she argues that qualitative researchers are not  the only ones interpreting data, that also quantitative researchers  do this; dispelling the myth that it is qualitative researchers  [who] are in the business of interpreting stories and quantitative  researchers [who] are in the business of producing facts.  Interpretation as part of analysis is the hardest of any data  analysis, big or small. Boyd [3] would like to see computer  experts working together with social scientists to avoid fallacies  in interpretations.   105    5. RESEARCHING A MOOC: ANALYTICS,  DATAMINING OR QUALITATIVE  ANALYSIS 5.1 Background of the research The research in this paper was carried out during The Personal  Learning Environments Networks and Knowledge (PLENK2010   http://connect.downes.ca) course in the fall of 2010.  It was a  free Massive Open Online course which lasted for 10 weeks. In  total, 1641 participants were registered. The course was a joint  venture between the National Research Council of Canadas  (NRC) Institute for Information Technology, Learning and  collaborative Technologies Group, PLE Project, The Technology  Enhanced Knowledge Research Institute (TEKRI) at Athabasca  University, and the University of Prince Edward Island. Four  facilitators, highly visible and knowledgeable in the field of  study, were active on the course and would find resources,  speakers and participate in all aspects of the course.   PLENK2010 did not consist of a body of content and was not  conducted in a single place or environment. It was distributed  across the web. This type of learning event is called a  connectivist course and is based on four major types of activity:  1) Aggregation: access to a wide variety of resources to read,  watch or play, along with a newsletter called The Daily, which  highlighted some of this content; 2) Remixing: after reading,  watching or listening to some content, it was possible to keep  track of that somewhere-i.e., by creating a blog, an account with  del.icio.us and creating a new entry, taking part in a Moodle  discussion, or using any service on the internet  Flickr, Second  Life, Yahoo Groups, Facebook, YouTube, iGoogle, NetVibes; 3)  Repurposing: participants were encouraged to create something of  their own. In the PLENK2010 the facilitators suggested and  described tools that participants could use to create their own  content. The job of the participants was to use the tools and just  practice with them. Facilitators demonstrated, gave examples,  used the tools themselves, and talked about them in depth. It was  envisaged that with practice participants would become  accomplished creators and critics of ideas and knowledge; and 4)  Feed Forward: participants were encouraged to share their work  with other people in the course, and with the world at large.  Participants were able to work completely in private, not showing  anything to anybody if they wished to do so. Facilitators  emphasized that sharing would always be the participants choice.  In addition, a tag would be used to identify anything that was  created in relation to the course, using the course tag  #PLENK2010. That is how content related to the course was  recognized, aggregated, and displayed in The Daily newsletter  for the course.  If participants decided to use a tool such as Blogger, Flickr, or a  discussion group they were asked to share the RSS feed. A  separate post on how to produce and include their own RSS feed  to the Daily was offered for those who did not know how to do  this. All postings to a blog or forum would apply the  #PLENK2010 tag. That is how information was recognized as  being related to this particular course. When a connectivist course  is working really well, one can see a great cycle of content and  creativity begin to feed on itself, people in the course reading,  collecting, creating and sharing.  5.2 Research methods and tools used The NRC research team decided to use a mixed methods approach  and a variety of research techniques and analysis tools to capture  the diverse activities and the learning experiences of participants  on PLENK2010. Learning analytics tools were used as a  quantitative form of Social Network Analysis to clarify activity  and relationships between nodes on the PLENK network. Three  surveys were carried out at the end of the course and after it had  finished to capture learning experiences during the course: End  survey (N=62); Active producers survey (N= 31); Lurkers  survey (N=74).   In addition, qualitative methods in the form of virtual  ethnography have been used. A researcher was an observer during  the course, collecting qualitative data through observation of  activities and engagement and also carried out a focus group in  the final week of the course to gain a deeper understanding of  particular issues related to the active participation of learners. As  vast amounts of discursive data were generated and collected,  analysis and computational tools have been used to represent  large networks of activity in the PLENK, to identify themes in the  data and for analysis and interpretation of the qualitative research  data (e.g., SNAPP, Pajek, NetDraw and Nvivo).   6. FINDINGS The professional background of participants on the PLENK  course, were mainly employed in education, research and design,  and development of learning opportunities and environments.  They were teachers, researchers, managers, mentors, engineers,  facilitators, trainers, and university professors.   6.1. Analyzing and visualizing participation on  the course  When the course started 846 had registered, which steadily  increased to 1641 at the end of the course, as shown in Figure 1.  People took part in the twice weekly meeting sessions that were  hosted on Elluminate, once a week with an invited speaker and  once as a discussion session amongst the group and facilitator(s).  Actual presence at these synchronous sessions decreased over the  weeks from 97 people in week two, when attendance was the  highest, to 40 in the final week and there was a similar trend in  the access of the recordings.    Figure 1.  Participation during PLENK  106    Global participation and multiple time zones influenced who were  present and who accessed the Elluminate recordings. A high  number of blog posts were generated related to the course (949)  and an even higher number of Twitter contributions (3459) as a  means of connection participants inside and outside the course  (see Figure 2.).     The #PLENK2010 identifier facilitated the easy aggregation of  blog posts, Del.icio.us links and Twitter messages produced by  participants, which highlighted a wide number of resources and  links back to participants blogs and discussion forums, and thus  connecting different areas of the course. Although the number of  course registrations was high, an examination of contributions  across weeks (i.e., Moodle discussions, blogs, Twitter posts  marked with #PLENK2010 course tag, and participation in live  Elluminate sessions) suggested that about 40-60 individuals on  average contributed actively to the course on a regular basis by  producing blog posts and discussion posts, while others visible  participation rate was much lower.   652 participants used Twitter and were linked to other #tag  networks as suggested in figure 3.  Figure 3 Twitter networks that participants were also linked  to with #tags   In the Moodle Forums for PLENK2010, general trends in posting  behaviors indicate that there was a peak in activity in Week 2 in  Moodle forums, with a slight upward trend in Blog and Twitter  posts as well (Figure 4.). This was followed by a sharp decline in  the number of posts in all three mediums (Moodle, Blogs, and  Twitter) in Week 3, a slight increase in Week 4, and a steady  decline again in Weeks 5 and 6.    Interestingly, the number of posts by course facilitators follows  similar trends (Figure 5.), with the number of posts by facilitators  peaking in Week 2, then showing a steady decline in Weeks 5 and  6. The facilitator(s) played an important role in triggering  discussion, questioning, providing feedback, and sustaining  interaction amongst participants.     Social Networks Adapting Pedagogical Practice (SNAPP) uses  information on who posted and replied to whom, and what major  discussions were about, and how expansive they were, to analyze  the interactions of a forum and display it in a Social Network  Diagram. Figures 6. provides a visual depiction of all interactions  occurring among students and facilitators in PLENK2010 for  Week 1-Discussion on PLE/PLNs. The social network diagram  provides an aggregate visual representation of the connections  that occurred between 69 participants for this particular  discussion and is an aggregate visual representation of the  interactions among participants but is not very comprehensive in  describing the nature of the interaction (i.e., the quality).   Figure 2. Twitter activity in PLENK   Figure 4. Postings across six weeks of PLENK   Figure 5. Facilitator posts in PLENK   107    Some people with experience in learning on a MOOC were very  active and involved in the course, producing a Google map for  PLENK participant place of residence, another created a concept  map to represent their PLE, while others produced Wordles to  visualize the content of a paper. Not all participants contributed in  a visibly active way. A high number of people accessed resources  but were not engaged in producing blog posts, videos or other  digital artifacts; they seemed to be consumers rather than  producers on the course. Only a small percentage of participants  engaged in the production of digital artifacts. Between 40 and 60  were active producers, the other 1580 were not visibly active.  This was unexpected to the course organizers as before the start  they saw the production phase as vital to the learning on a  networked environment.  After all, as some participants  mentioned in the discussion, if nobody is an active producer, it  limits the resources that all participants can use to develop their  ideas, to discuss, think, and be inspired by in their learning.  Analyzing the word frequency in the Moodle discussion forum for  Week 1 using Nvivo 9 highlighted the importance of personal  agency in maintaining engagement, participation, and interaction  with others. Keywords such as learning, me, network,  question, and exploration in Week 1 discussions were  focused on PLE/PLNs concept map activities. Connections  between participants were made which supported the learning  process as interactions generated many resources, including 49  links and 17 suggestions for useful tools.  The use of the #PLENK2010 hash tag made it possible to  aggregate blog post and Twitter messages and visualize and  organize them into the Daily newsletter and a Twitter newsletter.  The end of course survey results confirm that although the Daily  newsletter and Moodle helped 45% of participants understand the  course content, learners needed a common space to create  artifacts and connect back to their blog, such as Amplify for  example, for social sharing and bringing together various media  and resources. One participant commented in the survey: I would have liked to see a thread each week called, How Can We  Help You allowing the community to answer many of the  questions and offer more support, mentoring, and evaluations.  7. DISCUSSION AND CONCLUSIONS  This paper has highlighted some of the possible uses of current  analytics tools in providing useful information to participants and   facilitators about their participation and social connections within  the Massive Open Online Course and outside it, but also the  limitations of visualization, knowledge representation, and  virtualization in providing meaningful information about learning.  The mixed method approach used has highlighted the  effectiveness of combining both quantitative and qualitative  methods to achieve breadth and depth of data analyses.  Quantitative analyses have exposed  a basic level of assessment  and reporting on learner activity, on whether participants are  actually doing something, in this case either inside the Moodle  environment and corresponding activity outside the environment  including Blog and Twitter activity being tracked with the  #PLENK2010 course tag. Qualitative tools and approaches (e.g.,  SNAPP, Nvivo, NetDraw) demonstrated how deep exploration of  content can reveal the types of contributions made, as well as the  knowledge, ideas, thinking, information, tools, and experience  that promote learning along the way. But still, the need for human  analysis and interpretation has also become apparent.  From a research point of view, the time and efforts needed to  conduct various analyses on two forum discussions was  prohibitive, but yielded a detailed view of what actually occurred  in one discussion, including the processes, the learning, and  important outcomes. The use of tags in the Moodle environment  would have been helpful in linking various contents across weeks,  allowing participants to search for relevant content and to see how  they were connected to various content and people with similar  interests.  Analytics can be applied to structure mining (link information),  content mining (including text, images, audio files), usage mining  and transaction data. Structure mining is often more valuable  when it is combined with content mining of some kind to interpret  the hyperlinks contents. Vast amounts of data were being  generated in this example of networked learning in an open  environment, so much so that facilitators, participants, and  researchers could not possibly attend to all the details but needed  to focus on the most relevant information efficiently and  effectively in order to encourage better outcomes of activity.  Intelligent and automatic data analysis with powerful  computational tools for analysis and interpretation should be  explored as a valid option for informing learning in MOOC in a  connectivist-type course.   One of the limitations of the analytics approach, however, was the  narrow scope of the analyses, as focused on one snapshot of the  MOOC experience for Week 1 and 2 related to the topic of  PLE/PLNs. It did expose what was occurring within the course  when there was a peak in activity and when data was plentiful but  did not expose gaps in the data that capture experiences that were  lacking or nonexistent, and missing data, for instance in the case  where people were lurking and their activity related to the  course invisible. Qualitative methods will be applied to give more  meaning to the experiences of those on the periphery, the non- active participants, and those who were perhaps lacking the skills  or mechanisms for engaging wholeheartedly in the course.  Interviews with participants who were either not connected, not  visible on the PLENK2010 social network or on the fringes will  provide them with a voice as the analytics used in our analysis  could not capture their learning stories and made them into a  marginalized, invisible group.  Another limitation of analytics has been their inability to capture  contextual nuances in data. Analytics can provide a view of what   Figure 6. Social network and connections   108    is happening, but it has problems representing the nature of  connections between data sets and people. Human interpretation  or artifical intelligence capacity will be necessary to achieve this.  Facilitators and participants themselves were exposed to analytics  tools within the PLENK2010 course and in one of the discussions  concluded that although they provided a global view of social  networks in the MOOC they lacked sufficient detail to be really  informative. A greater understanding of how learners  communicate, complete tasks and construct new knowledge in a  Moodle environment, combined with blogs, and Twitter activity  will inform the design and development of optimal learning  experiences.   Further analyses will be undertaken after the course has been  completed with more options for analyses of data involving  interpretation of meanings and human actions rather than a focus  on the numbers. This work is expected to be completed in time for  presentation at the conference on Learning Analytics and  Knowledge, February 27-March 01, 2011.   The use of learning analytics is only in its infancy, but from our  use of the tools it seems that they can be powerful in giving  meaning to interactions and actions in a learning environment  such as was used on this MOOC, providing scope for  personalized learning and the creation of more effective learning  environments and experiences. Personalization and analysis of  user interaction data is a key approach to overcoming the  problems related to the overpowering plethora of information  available and generated through technology in an open networked  learning environment. More in depth analyses of the data from  PLENK will feed into the development of support structures of  optimal learning experiences in Personal Learning Environments.  Learning analytics tools have clearly provided scope for  information filtering and visualization, as promising technologies  to support people in clarifying and relating information, peer  learners and digital artifacts and in doing so supporting people in  pursuing their learning.   8. REFERENCES [1] Anderson, T. and Kanuka, H.: E-Research: Methods,   Strategies, and Issues, Allyn & Bacon, Boston, New York  (2003).  [2] Boyd, D.: Privacy and Publicity in the context of Big Data,  available at  http://www.danah.org/papers/talks/2010/WWW2010.html ,  Raleigh, North Carolina, April 29th 2010 (Accessed, 16th  October, 2010).   [3] Lazer, D., Pentland, A., Adamic, L., Aral, S., Barabasi, A,  Brewer, D., Christakis, N., Contractor, N., Fowler, Gutmann,  M., Jebara, T., King, G., Macy, M., Roy, D., and van  Alstyne, M.: Computational social Science, in Science, Vol  323, 6 February 2009, Www.Sciencemag. org (Accessed  12th October, 2010).   [4] Ballardvale Research (n.d.).: Market Trends - Web  Analytics:  History and Future, available from  http://www.ballardvale.com/free/WAHistory.htm (Accessed  14th October 2010).   [5] EDUCAUSE Learning Initiative.: 7 Things you should know  about ANALYTICS, Available from  http://net.educause.edu/ir/library/pdf/ELI7059.pdf  (Accessed, 3rd November 2010).   [6] Norris, D., Baer, L. & Offerman, M.: A National Agenda for  Action analytics, National Symposium on Action Analytics ,  available from  http://www.edu1world.org/PublicForumActionAnalytics/  (Accessed 14th October 2010), (2009).   [7] Campbell, J. & Oblinger, D.: Academic analytics, Educause  (2007).  [8] Dawson,S., Heathcote, L. & Poole, G.: Harnessing ICT  potential: The adaptation and analysis of ICT systems for  enhancing the student learning experience. International  journal of Educational Management, 24, 2 (2010).   [9] Dell cloud Services.: Collaboration, Analytics, and the LMS:  A Conversation with Stephen Downes,  in Learning  Management that fits, Dell cloud services, available from  http://bit.ly/938t1o   (Accessed 14th October 2010).   [10] Parry, M.: Like Netflix, New College Software Seeks to  Personalize Recommendations, The chronicle of Higher  Education, available from  http://chronicle.com/blogs/wiredcampus/like-netflix-new- college-software-aims-to-personalize- recommendations/27642 (2010).   [11] Kop, R.: The Design and Development of a Personal  Learning Environment: Researching the Learning  Experience, European Distance and E-learning Network  annual Conference 2010, June 2010, Valencia, Spain, Paper  H4 32 (2010).   [12] Laat, de, M.: Networked Learning, PhD Thesis, Instructional  Science, Utrecht Universiteit, The Netherlands (2006).   [13] Krebs, V.: Social Network Analysis, A Brief Introduction,  OrgNet.com, Available from  http://www.orgnet.com/sna.html (Accessed 16th October  2010), (2007).   [14] Hammersley, M., Gomm, R., Woods, P., Faulkner, D., Swan,  J., Baker, S., Bird, M., Carty, J., Mercer, N., and Perrott, M.:  Research Methods in Education Handbook, Milton Keynes,  Open University (2001).   [15] Hine, C.: Internet Research and the Sociology of Cyber- Social-Scientific Knowledge, The Information society, 21,  pg. 239-248 (2005).   [16] Sloane, F., and Gorard, S.: Exploring Modeling Aspects of  Design Experiments, Educational Researcher, 32, 1, 29-31  (2003).  109    "}
{"index":{"_id":"12"}}
{"datatype":"inproceedings","key":"Blikstein:2011:ULA:2090116.2090132","author":"Blikstein, Paulo","title":"Using Learning Analytics to Assess Students' Behavior in Open-ended Programming Tasks","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"110--116","numpages":"7","url":"http://doi.acm.org/10.1145/2090116.2090132","doi":"10.1145/2090116.2090132","acmid":"2090132","publisher":"ACM","address":"New York, NY, USA","keywords":"automated assessment, constructionism, educational data mining, learning analytics, logging","Abstract":"There is great interest in assessing student learning in unscripted, open-ended environments, but students' work can evolve in ways that are too subtle or too complex to be detected by the human eye. In this paper, I describe an automated technique to assess, analyze and visualize students learning computer programming. I logged hundreds of snapshots of students' code during a programming assignment, and I employ different quantitative techniques to extract students' behaviors and categorize them in terms of programming experience. First I review the literature on educational data mining, learning analytics, computer vision applied to assessment, and emotion detection, discuss the relevance of the work, and describe one case study with a group undergraduate engineering students","pdf":"Using learning analytics to assess students behavior in  open-ended programming tasks                       !     #$% &'( )*#+#(( *(,-+$   !./     There is great interest in assessing student learning in unscripted,  open-ended environments, but students work can evolve in ways  that are too subtle or too complex to be detected by the human  eye. In this paper, I describe an automated technique to assess,  analyze and visualize students learning computer programming. I  logged hundreds of snapshots of students code during a  programming assignment, and I employ different quantitative  techniques to extract students behaviors and categorize them in  terms of programming experience. First I review the literature on  educational data mining, learning analytics, computer vision  applied to assessment, and emotion detection, discuss the  relevance of the work, and describe one case study with a group  undergraduate engineering students              K.3.2 [            ]: Computer Science Education.        Algorithms, Measurement, Performance, Language.     !  Learning Analytics, Educational Data Mining, Logging,  Automated Assessment, Constructionism.    # $%&%$ Researchers are unanimous to state that we need to teach the so- called 21st century skills: creativity, innovation, critical  thinking, problem solving, communication, and collaboration.  None of those skills are easily measured using current assessment  techniques, such as multiple choice tests, open items, or  portfolios. As a result, schools are paralyzed by the push to teach  new skills, and the lack of reliable ways to assess them. One of  the difficulties is that current assessment instruments are based on  products (an exam, a project, a portfolio), and not on processes  (the actual cognitive and intellectual development while  performing a learning activity), due to the intrinsic difficulties in  capturing detailed process data for large numbers of students.    However, new data collection, sensing, and data mining   technologies are making it possible to capture and analyze  massive amounts of data in all fields of human activity. These  techniques include logs of email and web servers, computer  activity capture, wearable cameras, wearable sensors, bio sensors  (e.g., skin conductivity, heartbeat, brain waves), and eye-tracking,  using techniques such as machine learning and text mining. Such  techniques are enabling researchers to have an unprecedented  insight into the minute-by-minute development of several  activities. In this paper, we propose that such techniques could be  used to evaluate some cognitive strategies and abilities, especially  in learning environments where the outcome is unpredictable such  as a robotics project or a computer program.   In this work, we focused on students learning to program a  computer using the NetLogo language. Hundreds of snapshots for  each student were captured, filtered, and analyzed. I will describe  some prototypical coding trajectories and discuss how they relate  to students programming experience, as well as the implication  for the teaching and learning of computer programming.   '# ()%&*% Two examples of the current attempts to use artificial intelligence  techniques to assess human learning are text analysis and emotion  detection. The work of Rus et al. [13], for example, makes  extensive use of text analytics within a computer-based  application for learning about complex phenomena in science.  Students were asked to write short paragraphs about scientific  phenomena  Rus et al. then explored which machine learning  algorithm would enable them to most accurately classify each  student in terms of their content knowledge, based on  comparisons with expert-formulated responses. However, some  authors have tried to use even less intrusive technologies; for  example, speech analysis further removes the student from the  traditional assessment setting by allowing them to demonstrate  fluency in a more natural setting. Beck and Sison [4] have  demonstrated a method for using speech recognition to assess  reading proficiency in a study with elementary school students  that combines speech recognition with knowledge tracing (a form  of probabilistic monitoring.)   The second area of work is the detection of emotional states using  non-invasive techniques. Understanding student sentiment is an  important element in constructing a holistic picture of student  progress, and it also helps enabling computer-based systems to  interact with students in emotionally supportive ways. Using the  Facial Action Coding System (FACS), researchers have been able  to develop a method for recognizing student affective state by  simply observing and (manually) coding their facial expressions  and applying machine learning to the data produced [11].  Researchers have also used conversational cues to detect students  emotional state. Similar to the FACS study, Craig et al. designed  an application that could use spoken dialogue to recognize the   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.   LAK11, February  27March 1, 2011, Banff, Alberta, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.     00$    states of boredom, frustration, flow, and confusion. They were  able to resolve the validity of their findings through comparison to  emote-aloud activities (a derivative of talk-aloud where  participants describe their emotions as they feel them) while  students interacted with AutoTutor.   Even though researchers have been trying to use all these artificial  intelligence techniques for assessing students formal knowledge  and emotional states, the field is currently benefiting from three  important additions: 1) detailed, multimodal student activity data  (gestures, sketches, actions) as a primary component of analysis,  2) automation of data capture and analysis, 3) multidimensional  data collection and analysis. This work is now coalescing into the  nascent field of Learning Analytics or Educational Data Mining  [1, 3], and has been used in many contexts to measure students  learning and affect. However, in Baker and Yacefs review of its  current uses, the majority of the work is focused on cognitive  tutors or semi-scripted environments [2]. Open-ended tasks and  unscripted learning environments have only been in the reach of  qualitative, human-coded methods. However, qualitative  approaches presents some crucial shortcomings: (1) there is no  persistent trace of the evolution of the students artifacts  (computer code, robots, etc.), (2) crucial learning moments within  a project can last only seconds, and are easy to miss with  traditional data collection techniques (i.e., field notes or video  analysis), and (3) such methodologies are hard to scale for large  groups or extended periods of time. The cost of recording,  transcribing and analysis data is a known limiting factor for  qualitative researchers.   At the same time, most of previous work on EDM has been used  to assess specific and limited tasks  but the 21st century skills  we need to assess now are much more complex, such as creativity,  the ability to find solutions to ill-structured problems and navigate  in environments with sparse information, as well as dealing with  uncertainty. Unscripted learning environments are well-known for  being challenging to measure and assess, but recent advances both  data collection and machine learning could make it possible to  understand students trajectories in these environments.   For example, researchers have attempted to automate the  collection of action data, such as gesture and emotion. Weinland  et al. [15] and Yilmaz et al. [17] were able to detect basic human  actions related to movement. Craig et al. [10] created a system for  automatic detection of facial expressions (the FACS study). The  technique that Craig et al. validated is a highly non-invasive  mechanism for realizing student sentiment, and can be coupled  with computer vision technology and biosensors to enable  machines to automatically detect changes in emotional state or  cognitive-affect.   Another area of active development is speech and text mining.  Researchers have combined natural language processing and  machine learning to analyze student discussions and writing,  leveraging Independent Component Analysis of student  conversations  a technique whose validity has been repeatedly  reproduced. The derived text will is subsequently analyzed using  Latent Semantic Analysis [13]. Given the right training and  language model, LSA can give a clearer picture of each students  knowledge development throughout the course of the learning  activity.   In the realm of exploratory learning environments, Bernardini,  Amershi and Conati [6] built student models combining  supervised and unsupervised classification, both with log files and  eye-tracking, and showed that meaningful events could be   detected with the combined data. Montalvo et al. [11], also using a  combination of automated and semi-automated real-time coding,  showed that they could identify meaningful meta-cognitive  planning processes when students were conducting experiments in  an online virtual lab environment.   However, most of these studies did not involve the creation of  completely open-ended artifacts, with almost unlimited degrees of  freedom. Even though the work around these environments is  incipient, some attempts have been made (see 7, 8). Another of  such examples is the work Berland & Martin [5], who by logging  data found that novice students' developed successful program  code by following one of two progressions: planner and tinkerer.  Planners found success by carefully structuring programs over  time, and tinkerers found success by accreting programs over  time. In their study, students were generally unsuccessful if they  didn't follow one of those paths.   In this paper, I will present one exploratory case study on the  possibility of using learning analytics and educational data mining  to inspect students behavior and learning in project-based,  unscripted, constructionist [12] learning environments, in which  traditional assessment methods might not capture students  evolution. My goal is to establish a proof of existence that  automatically-generated logs of students programming can be  used to infer patterns in how students go about programming, and  that by inspecting those patterns we could design better support  materials and strategies, as well as detect critical points in the  writing of software in which human assistance would be more  needed. Since my data relies in just nine subjects, I dont make  claims of statistical significance, but the data points present  meaningful qualitative distinctions between students.    +# ,-%$ To collect the programming logs, I employed the NetLogo [16]  programming environment. NetLogo can log to an XML file all  users actions, such as key presses, button clicks, changes in  variables and, most importantly, changes in the code. I developed  techniques and custom tools to automatically store, filter, and  analyze snapshots of the code generated by students.   The logging module uses a special configuration file, which  specifies which actions are to be logged. This file was distributed  to students alongside with instruction about how to enable  logging, collect the log-files, and send those files back for analysis   Nine students in a sophomore-level engineering class had a 3- week programming assignment. The task was to write a computer  program to model a scientific phenomenon of their choice.  Students had the assistance of a programming teaching  assistance, following the normal class structure. The teaching  assistant was available for about 3-4 hours a week for each  student, and an individual, 1-hour programming tutorial session  was conducted with each of the students on the first week of the  study.   158 logfiles were collected. Using a combination of XQuery and  regular expression processors (such as grep), the files were  processed, parsed, and analyze (1.5GB and 18 million lines of  uncompressed text files). Below is a summary of the collected  data (in this order): total number of events logged, total number of  non-code events (e.g., variable changes, button presses), percent  of non-code events, and actual coding snapshots.     000      #$   .           $  .   $/  $/ 0   Chuck 258036 257675 99.9% 361   Che 5970 928 15.5% 5042   Leah 2836 525 18.5% 2311   Liam 4044723 4041123 99.9% 3600   Leen 253112 241827 95.5% 11285   Luca 92631 86708 93.6% 5923   Nema 3690 649 17.6% 3041   Paul 218 15 6.9% 203   Shana 4165657 4159327 99.8% 6330     11'213+ 1311333 44#20 +1542  The overwhelming majority of events collected were non-coding  events, such as variable changes, buttons pressed, and clicks.  These particular kinds of event takes place when students are  running or testing models  every single variable change gets  recorded, what accounts for the very large number of events  (almost 9 million.) Since the analysis of students interactions  with models is out of the scope of this paper, all non-coding  events were filtered out from the main dataset, so we were left  with 1187 events for 9 users.   For further data analysis, a combination of techniques was used.  First, I developed a series of Mathematica scripts to count  meaningful events within the dataset, such as number of  characters, keywords used, code compilations, and types of error  messages. Then, I used the resulting plots to look at particular  snapshots where seemingly atypical coding activity took place   inflection points, plateaus, and sharp decreases or increases. To  examine the snapshots, I developed a custom software tool,  Event Navigator (Figure 1). The software enables researchers to  go back and forth in time, frame-by-frame, tracking students  progression and measuring statistical data.    6  #  7  7 .  $.   ! 8!77 !   7   9  7  8  97!             #  :# $;< For the analysis, I will first focus on one student and conduct an  in-depth exploration of her coding strategies. Then, I will compare  her work with other students, and show how differences in   previous ability and experience might have determined their  performance.  :#        4.1.1 Luca Luca is a sophomore engineering student and built a scientific  model in her domain area. She had modest previous experience  with computers, and her grade in the class was also around the  average, which makes her a good example for an in-depth analysis  of her log files.  Figure 2 is a visualization of Lucas model building logs. The  continuous (red) curve represents the number of characters in her  code, the (blue) dots (mostly) underneath the curve represent the  time between two code compilations (secondary y-axis to the  right), (green) dots placed at y=1800 represent successful  compilations, (orange) dots placed at y=1200 represent  unsuccessful compilations (the y coordinates for those two data  series are arbitrary and were chosen just for visualization  purposes.) In the following paragraphs, I will analyze each of the  6 regions of the plot. The analysis was done by looking at the  overall increase in character count (Figure 2), and then using the  Code Navigator tool (Figure 1) to locate the exact point in time  when the events happened.   6 '# = 8    !   8    8  ;>   The following are the main coding events for Luca:    # Luca started with one of the exemplar programs seen in the tutorial. In less than a minute, she deleted the  unnecessary code and ended up with a skeleton of a new program (see the big  drop in point ).   '# She spent the next half-hour building  her first procedure. During this time,  between  and , she had numerous  unsuccessful compilations (see the  orange dots), and goes from 200 to 600  characters of code.   00#    +# The size of the code remains stable for 12 minutes (point ), until there is a sudden jump from 600 to 900 characters (just before point . This  jump corresponds to Luca copying and pasting her own code: she duplicated her first procedure as a basis for a second one. During this period, also, she opens many of the sample programs within NetLogo.   :# Luca spends some time making her new duplicate procedure work. The frequency of compilation decreases (see the density of orange and green dots), the average time per compilation increases, and again we see a plateau for about one hour (point ).   @# After one hour in the plateau, there is another sudden increase in code size, from 900 to 1300 characters (between  and ). Actually, what Luca did was to open a sample program and copy a procedure that generated something she  needed for her model. Note that code  compilations are even less frequent.   2# After making the recycled code work, Luca got to her final number of 1200 characters of code. She then spent about 20 minutes beautifying the code, fixing the indentation, changing names of variables, etc. No real  changes in the code took place, and there are no incorrect compilations.   Lucas narrative suggests, thus, four prototypical modeling  events:       ! an existing program as a starting  point.   Long plateaus of no coding activity, during which      !  7    (or their own code) for  useful pieces.        7      8 when students  import code from other programs, or copy and paste  code from within their working program.    A 7  in which students fix the formatting of  the code, indentation, variable names, etc.   4.1.2 Shana, Lian, Leen, and Che  Using the character count time series it is possible to examine  logfiles from other students in search of similarities. In the  following, I show plots from four different students (Luca, Che,  Leen, and Shana, Figure 3), which include all of their activity  (including opening other modelsthe spikesnote that the plot  in Figure 2 did not show all of Lucas activities, but only her  activities within her model, i.e., excluding opening and  manipulating other models).   6 +# = .         A78;8 7 8;  #7 9 7!  !77         # First, lets examine Shanas logfiles. After many spikes and  almost no change in the baseline character count, there is a sudden  jump (at time=75) from about 200 to 4,000 characters of code. A  closer, systematic examination revealed that Shana employed a  different approach than Luca. After some attempts to incorporate  the code of other programs into her own (the spikes), she gave up  and decided to do the opposite: start from a ready-made program  and add her code to it. She then chose a very well-established  sample program (provided as part of the initial tutorial) and built  hers on top of it. The sudden jump to 4,000 characters indicates  the moment when she loaded the sample program and started to  make it her own by adding procedures. She seamlessly  integrated the pre-existing code into her new one, adding  significant new features.  Leen, on the other hand, had yet another coding style. He did open  other sample programs for inspiration or cues, but did not copy   20 40 60 80 100 120  1000  2000  3000  4000  5000  6000  shana  20 40 60 80 100 120  1000  2000  3000  4000 luca  50 100 150 200  1500  2000  2500  3000  3500 che  100 200 300 400 500  1000  2000  3000  4000  leen  00+    and paste code. Instead, he built his procedures in small  increments by trial-and-error. In Table 2 we can observe how he  coded a procedure to sprout a variable number of white screen  elements (the action lasted 30-minutes). The changes in the code  (diffs) are indicated with the (red) greyed-out code.    '#;  >     !       to Insert- Vacancies  to Insert-Vacancies   sprout 2      [ set breed  vacancies       set color  white ] ]  end  Initial code   to Insert- Vacancies  ask patches   [ sprout 2      [ set breed  vacancies       set color  white ] ]  end  to Insert-Vacancies     ask one-of patches   [ sprout 2      [ set breed  vacancies       set color  white ] ]  end  9 7  is  introduced, and  then  /  to Insert- Vacancies    ask one-of  patches   [ sprout 1     [ set breed  vacancies       set color  white ] ]  end  to Insert-Vacancies     ask 5 patches    [ sprout 2     [ set breed  vacancies       set color  white ] ]  end  Leen  experiments with         7 (1, 5,  3)  to Insert- Vacancies    ask patches- from   [ sprout 2      [ set breed  vacancies       set color  white ] ]  end  to Insert-Vacancies  loop    [    ask one-of patches   [ sprout 2      [ set breed  vacancies       set color  white ] ] ]  end  Tries  7 /  and then  introduce a    to Insert- Vacancies while    [    ask one-of  patches   [ sprout 2      [ set breed  vacancies       set color  white] ] ]  end  to Insert-Vacancies    while n < Number- of-Vacancies   [    ask one-of  patches   [ sprout 2      [ set breed  vacancies       set color  white ] ] ]  end  Tries another  loop approach,  with a !7  command   to Insert- Vacancies    ask one-of  patches   [ sprout 2      [ set breed  vacancies       set color  white ] ]  end  to Insert-Vacancies     ask 35 of patches   [ sprout 2      [ set breed  vacancies       set color  white ] ]  end  Gives up  looping, tries a  B     7   to Insert- Vacancies    ask n-of Number-of- Vacancies patches   [ sprout 2      [ set breed  vacancies       set color  white ] ] end   Gives up a  fixed number,  creates a   , and introduces  /  Leen trial-and-error method had an underlying pattern: he went  from simpler to more complex structures. For example, he first  attempts a fixed, hardcoded number of events (using the sprout  command), then introduces control structures (loop, while) to  generate a variable number of events, and finally introduces new  interface widgets to give the user control over the number of  events. Leen reported having a high familiarity with programming  languages (compared to Luca and Shana), which might explain his  different coding style. He seemed to be much more confident  generating code from scratch instead of opening other sample  programs to get inspiration or import code.  Che, with few exceptions, did not open other models during  model building. Similarly to Leen, he also employs an  incremental, trial-and-error approach, but we can clearly detect  many more long plateaus in his graph. Therefore, based on these  logfiles, seven canonical coding strategies can be inferred:   1. Stripping down an existing program as a starting point.  2. Starting from a ready-made program and adding ones own  procedures.  3. Long plateaus of no coding activity, during which students  browse other sample programs (or their own) for useful code.  4. Long plateaus of no coding activity, during which students  think of solutions without browsing other programs.  5. Period of linear growth in the code size, during which  students employ a trial-and-error strategy to get the code right.   6. Sudden jumps in character count, when students import code  from other programs, or copy and paste code from within their  working program.  7. A final phase in which students fix the formatting of the  code, indentation, variable names, etc.   Based on those strategies, and the previous programming  knowledge of students determined from questionnaires, the data  suggest three coding profiles:    Copy and pasters: more frequent use of a, b, c, f, and g.   Mixed-mode: a combination of c, d, e, and g.    Self-sufficients: more frequent use of d, e.   The empirical verification of these canonical coding strategies and  coding profiles has important implications for design, in  particular, learning environments in which engage in project- based learning. Each coding strategy and profile might demand  different support strategies. For example, students with more  advanced programming skills (many of which exhibited the self- sufficient behavior) might require detailed and easy-to-find  language documentation, whereas copy and pasters need more  working examples with transportable code. In fact, it could be that  more expert programmers find it enjoyable to figure the solutions  themselves, and would dislike to be helped when they are  problem-solving. Novices, on the other hand, might welcome  some help, since they exhibited a much more active help-seeking  behavior. The data suggests that students in fact are relatively  autonomous in developing apt strategies for their own expertise  level, and remained consistent. Therefore, echoing previous work  on epistemological pluralism, the data suggests that it would be  beneficial for designers to design multiple forms of support to  cater to each style (see, for example, [14]).    4.1.3 Code compilation  Despite these differences, one behavior seemed to be rather  similar across students: the frequency of code compilation. Figure   00-    4 shows the moving average of unsuccessful compilations (thus,  the error rate) versus time, i.e., the higher the value, the higher the  number of unsuccessful compilations within one moving average  period (the moving average period was 10% of the overall  duration of the logfileif there were 600 compilation attempts,  there period of the moving average would be 60).             6 :#      .       C    For all four students, after we eliminate the somewhat noisy first  instants, the error rate curve follows an inverse parabolic shape. It  starts very low, reaches a peak halfway through the project, and  then decreases reaching values close to zero. Also, the (blue) dots  on top of y=0 (correct compilations) and y=1 (incorrect  compilations) indicate the actual compilation attempts. Most of  them are concentrated in the first half of the activity approximately 2/3 in the first half to 1/3 in the second half. This  further confirms the data from the previous logfile analysis, in  which we observed that the process of learning to program and  generating code is not homogenous and simple, but complex and  comprised of several different phases. In the case of code  compilations, we can distinguish three distinct segments: an initial  exploration characterized by few unsuccessful compilations,  followed by a phase with intense code evolution and many  compilation attempts, and a final phase of final touches and  smaller fixes, with a lower error rate.   @# %$;&%$ This paper is an initial step towards developing metrics  (compilation frequency, code size, code evolution pattern,  frequency of correct/incorrect compilations, etc.) that could both  serve as formative assessments tools, and pattern-finding lenses  into students free-form explorations in technology-rich learning  environments. The frequency of code compilations, together with the code size  plots previously analyzed, enables us to trace a reasonable  approximation of each prototypical coding profile and style. Such  an analysis has important implications for the design of project- based learning environments.  First, to design and allocate support resources, moments of greater  difficulty in the programming process should be identified. The  data indicate that those moments happens mid-way through the  project and not towards the end, as I initially suspected (given the  deadline crunch anecdotally reported by many students). The  proposed metrics can be calculated during the programming  assignment and not only at the end, so instructors and facilitators  could monitor students in real time and offer help only when the  system indicates that students are in a critical zone. These zones  might be detected when, for example, several incorrect  compilations occur with few changes in character count, or an  atypical error rate curve is identified, or when the code is  changing in size too much for a long period of time.  Second, support materials and strategies need to be designed to  cater to diverse coding styles and profiles. A self-sufficient   00     coder might not need too many examples, but will appreciate  good command reference. Similarly, novices might benefit more  from well-documented, easy to find examples with easy-to-adapt  code. By better understanding each students coding style and behavior;  we also have an additional window into students cognition.  Paired with other data sources (interviews, tests, surveys), the data  could offer a rich portrait of the programming process and how it  affects students understanding of the programming language and  more sophisticated skills such as problem solving.  However, the implications of this class of technique are not  limited to programming. Granted, programming offers a relatively  reliable way to collect project snapshots, even several times per  hour. But such approaches could be employed with educational  software, or even with tangible interfaces, with the right computer  vision toolkit.   2# ;,%$$6&&*% Due to the low number of participants, the current study does not  make any claims about statistical significance. Also, because of  the length of the assignment (3 weeks), some students lost part of  their log files and their data could not be considered. For future  studies, we will be using a centralized repository that would avoid  the local storage of the log files, increasing their reliability and  reduce lost data. Another limitation is that I do not log what  students do outside of the programming environment, so I might  mistakenly take a large thinking period with a pause.   3# 6$ [1] Amershi, S., & Conati, C. (2009). Combining Unsupervised   and Supervised Classification to Build User Models for  Exploratory Learning Environments. Journal of Educational  Data Mining, 1(1), 18-71.   [2] Baker, R. & Yacef, K. (2009). The State of Educational Data  Mining in 2009: A Review and Future Visions. Journal of  Educational Data Mining, 1(1).   [3] Baker, R. S., Corbett, A. T., Koedinger, K. R., & Wagner, A.  Z. (2004). Off-task behavior in the cognitive tutor classroom:  when students game the system. Paper presented at the  Proceedings of the SIGCHI conference on Human factors in  computing systems.    [4] Beck, J., & Sison, J. (2006). Using knowledge tracing in a  noisy environment to measure student reading proficiencies.  International Journal of Artificial Intelligence in Education, 16(2), 129-143.   [5] Berland, M. & Martin, T. (2011). Clusters and Patterns of  Novice Programmers. The meeting of the American  Educational Research Association. New Orleans, LA.   [6] Bernardini, A., & Conati, C. (2010). Discovering and  Recognizing Student Interaction Patterns in Exploratory  Learning Environments. In V. Aleven, J. Kay & J. Mostow  (Eds.), Intelligent Tutoring Systems (Vol. 6094, pp. 125- 134): Springer Berlin / Heidelberg.   [7] Blikstein, P. (2009). An Atom is Known by the Company it  Keeps: Content, Representation and Pedagogy Within the  Epistemic Revolution of the Complexity Sciences. Ph.D.  PhD. dissertation, Northwestern University, Evanston, IL.      [8] Blikstein, P. (2010). Data Mining Automated Logs of  Students' Interactions with a Programming Environment: A  New Methodological Tool for the Assessment of   Constructionist Learning. American Educational Research  Association Annual Conference (AERA 2010), Denver, CO.   [9] Conati, C., & Merten, C. (2007). Eye-tracking for user  modeling in exploratory learning environments: An empirical  evaluation. Knowledge-Based Systems, 20(6), 557-574.   [10]Craig, S. D., D'Mello,S., Witherspoon, A. and Graesser, A.  (2008). 'Emote aloud during learning with AutoTutor:  Applying the Facial Action Coding System to cognitive- affective states during learning', Cognition & Emotion, 22: 5,  777  788.   [11]Montalvo, O., Baker, R., Sao Pedro, M., Nakama, A., &  Gobert, J. (2010) Identifying Students Inquiry Planning  Using Machine Learning. Educational Data Mining  Conference, Pittsburgh, PA.   [12]Papert, S. (1980). Mindstorms : children, computers, and  powerful ideas. New York: Basic Books.   [13]Rus, V., Lintean, M. and Azevedo,R. (2009). Automatic  Detection of Student Mental Models During Prior  Knowledge Activation in MetaTutor. In Proceedings of the  2nd International Conference on Educational Data Mining (Jul. 1-3, 2009). Pages 161-170.   [14]Turkle, S., & Papert, S. (1990). Epistemological Pluralism. Signs, 16, 128-157.   [15]Weinland, D., Ronfard, R., and Boyer, E. (2006). Free  viewpoint action recognition using motion history volumes.  Comput. Vis. Image Underst. 104, 2 (Nov. 2006), 249-257   [16]Wilensky, U. (1999, updated 2006). NetLogo [Computer  software]. Evanston, IL: Center for Connected Learning and  Computer-Based Modeling.   [17]Yilmaz, A. and Shah, M. (2005). Recognizing Human  Actions in Videos Acquired by Uncalibrated Moving  Cameras. In Proceedings of the Tenth IEEE international  Conference on Computer Vision (ICCV 05) Volume 1 -  Volume 01 (October 17 - 20, 2005). ICCV. IEEE Computer  Society, Washington, DC, 150-157.     001    "}
{"index":{"_id":"13"}}
{"datatype":"inproceedings","key":"Vatrapu:2011:CCL:2090116.2090136","author":"Vatrapu, Ravi","title":"Cultural Considerations in Learning Analytics","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"127--133","numpages":"7","url":"http://doi.acm.org/10.1145/2090116.2090136","doi":"10.1145/2090116.2090136","acmid":"2090136","publisher":"ACM","address":"New York, NY, USA","keywords":"behavior, cognition, communication, computers, general terms, human-computer interaction (HCI), learning analytics, online learning culture","Abstract":"In Westerman's [12] disruptive article, Quantitative research as an interpretive enterprise: The mostly unacknowledged role of interpretation in research efforts and suggestions for explicitly interpretive quantitative investigations, he invited qualitative researchers in psychology to adopt quantitative methods into interpretive inquiry, given that they were as capable as qualitative measures in producing meaning-laden results. The objective of this article is to identify Westerman's [12] key arguments and apply them to the practice of Learning Analytics in educational interventions. The primary implication for Learning Analytics practitioners is the need to interpret quantitative analysis procedures at every phase from philosophy to conclusions. Furthermore, Learning Analytics practitioners and consumers must critically examine any assumption that suggests quantitative methodologies in Learning Analytics are inherently objective or that Learning Analytics algorithms may replace judgment rather than aid it. Lastly we propose a method for making observational data in virtual environments concrete through nested models.","pdf":"Cultural Considerations in Learning Analytics  Ravi Vatrapu   Computational Social Science Laboratory (CSSL)  Department of IT Management  Copenhagen Business School   vatrapu@cbs.dk  ABSTRACT  This paper discusses empirical findings demonstrating cultural  influences in social behavior, communication, cognition,  technology enhanced learning and draws implications for learning  analytics.   Categories and Subject Descriptors H.5.3 Group and Organization Interfaces: Theory and models, Asynchronous interaction Collaborative computing, Evaluation/methodology; H.1.2 User/Machine Systems: Software  Psychology.  General Terms Design, Human Factors, Theory   Keywords Learning analytics, online learning culture, behavior,  communication, cognition, computers, human-computer  interaction (HCI).General Terms   1. INTRODUCTION According to George Siemens, learning analytics is the use of  intelligent data, learner-produced data, and analysis models to  discover information and social connections, and to predict and  advise on learning.1 The LAK 2011 conference call for papers  defines learning analytics as the measurement, collection,  analysis and reporting of data about learners and their contexts, for  purposes of understanding and optimizing learning and the  environments in which it occurs. This paper presents empirical  findings demonstrating cultural influences in social behavior,  communication, cognition, technology enhanced learning and  draws implications for learning analytics.  The remainder of this paper is organized as follows. Section 2  presents prior empirical work documenting cultural influences in  online learning settings. Section 3 first presents cultural                                                                     1 http://www.elearnspace.org/blog/2010/08/25/what-are-learning-  analytics  differences in teaching and learning in formal classroom settings,  communication styles and cognitive processes and then discusses  the implications for online learning and learning analytics. Section  4 concludes the paper with the identification of several challenges  and directions for future work.   2. CULTURE: CONCEPT, HISTORY, AND  DEFINITIONS   The concept of culture has a checkered intellectual history.  Raymond Williams [45] has termed culture one of the two or  three most complicated words in the English language (p.87).  Williams attributes this complexity of the concept to its  complicated historical development in many European languages  and its subsequent adoption in to a plurality of academic  disciplines. Further complications with the concept of culture arise  from the slippage of meaning between the academic usages of the  term and the popular usages of the term like in high culture vs.  pop culture. As the ever eloquent Williams [44, p. 11] puts it  the concept at once fuses and confuses the radically different  experiences and tendencies of its formation. The Victorian  Ethnologist, Edward Tylor, is widely credited for providing the  first definition of culture in anthropology. According to Tylor,  culture or civilization, taken in its wide ethnographic sense, is  that complex whole which includes knowledge, belief, art, morals,  law, custom, and other capabilities and habits acquired by man as  a member of society [38, p. 64].  A compiled list of over 200  different definitions of culture can be found in Kroeber and  Kluckhohns [23] critical review of the concept of culture. Table  2.1, generated from Kroeber and Kluckhohns [23], summarizes  the different analytical emphases of different definitions of  culture.   Table 1. Definition groups of the concept of culture [23]  Culture Definitions Definition Emphasis  Descriptive  Enumeration of content (p.81)   Historical Social heritage or tradition (p.89)   Normative Rule or way (p.95)   Psychological Culture as a problem solving device  (p.105)   Structural Patterning or organization of culture  (p.118)   Genetic Culture as a product or artifact (p.125)   Tracing the intellectual evolution of notion of culture from the  German kultur and its tensions with the notion of civilization,  Kroeber and Kluckhohn [23] categorize the different definitions of  cultures into 6 groups: descriptive, historical, normative,   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.   127    psychological, structural and genetic. To this grouping of  definitions of culture, it is fruitful to add notions of culture as  identity and also generative notions of culture. The psychological  definitions of culture have emphasis on adjustment, on culture as  a problem-solving device [23, p. 105] and also have an emphasis  on culture as learning. Two other definitions of culture are listed  below:  Culture is learned and shared human patterns or  models for living; day-to-day living patterns.  These patterns and models pervade all aspects of  human social interaction. Culture is mankind's  primary adaptive mechanism [7, p. 367].  Culture is the shared knowledge and schemes  created by a set of people for perceiving,  interpreting, expressing, and responding to the  social realities around them [24, p. 9].   In the above two definitions, culture is defined as patterns or  schemas. Culture is shared and is a factor in an individuals social  interactions. The definitions also emphasize the adaptiveness  implicit in the notion of culture and the importance of culture as  an active notion and not a passive one. Culture conceived of as  adaptations to the environmental demands towards some  instrumental ends has been a feature of cultural ecology strands of  anthropology. For example, according to White [43, p.339], all  life is a struggle for free energy and culture is a means in that  struggle toward the ends of survival. As Redfield [33] defines it:  culture is the shared understandings manifest in act and artifact  [Cited by Triandis, H. in the Foreword to 18].   2.1 Definition of Culture This paper uses the following definition of culture:   the collective programming of the mind which  distinguishes the members of one group or  category of people from another [17, p. 5].    The emphasis in Hofstedes definition is that culture is learnt in  nurture and not inherited by human nature. The collective  programming of the mind highlights culture as a collective  activity that is to be conceived as a dynamic process rather than a  passive state. The other part of the definition which distinguishes  the members of one group or category of people from another  points out the individual and group identity formation and  sustenance aspects of enculturation in social institutions like  family, school and work. Culture comes from a perceived  similarity of individuals within a cohort group (be it a linguistic  community, an ethnic group or a scientific community) and in that  sense it is collective. This similarity is not intended to be exact;  neither does it imply essentialist homogeneity. In a multicultural  society, culture is about a collective particularity.   Hofstede [17, p.18] relates collective programming to  Bourdieus [1] notion of habitus. As mentioned earlier,  Hofstedes definition is interpreted in this paper from the  Vygotskian socio-cultural perspective of the social formation of  the mind [42]. Collective programming is not to be understood  as an external imposition but an active social and ecological  composition in which the particular individual plays the  protagonist. Taken together, social formation and collective  programming of the mind indicate a cognitive schema.   Cognitive sciences have highlighted the role of scripts and  schemas and models in the mundane activities of everyday life   [35]. In this paper, culture is conceptualized in the cognitive  scientific sense of a cultural schema. Cultural schemas are putative  structures; they are properties of an individuals mind. Cultural  schemas are not things; neither are they substances. Cultural  schemas are the historical products of the processes of collective  programming of the mind. Thus cultural schemas are properties of  an individuals biography. This biography includes the interactive  effects of the geography of that individuals upbringing (ecology) and the formative experiences of the collective programming of  the mind (history). Cultural schemas are etic categories (scientific  constructs) of cognitive analysis and not emic conceptions  (experiential) of individual subjective experience [see 15 for more  on the emic vs. etic distinction and also a defense of his  positivistic cultural materialism].   In the next three sections, three separate lines of empirical findings  from the fields of social behavior, communication and cognition  are presented and implications for online learning are discussed.    3. CULTURE AND BEHAVIOR  Cultural models can be used to identify the differences in cultures  that affect the computer supported collaborative learning  environments. There are two kinds of cultural models: models that  use typologies and models that use dimensions. Typologies  describe a number of ideal types each easy to imagine.  Dimensional models group together a number of phenomena in a  society that were empirically found to occur in combination into  dimensions. Typologies are difficult to adopt in empirical research  as real cases very rarely correspond to one single ideal type.    3.1 Hofstedes Cultural Dimensions Model  Hofstedes seminal work on cultures in organizations formulated a  framework of four dimensions of culture identified across nations.  Each dimension groups together phenomena in a society that were  empirically found to occur in combination. In this section,  Hofstedes definitions for these original four cultural dimensions  are listed followed by a discussion of each dimension with respect  to online learning. Hofstedes cultural dimensions model indicates  what reactions are likely and understandable given ones cultural  background.  3.2 Low Power Distance vs. High Power  Distance Power distance is the extent to which the less powerful members  of institutions and organizations within a country expect and  accept the power that is distributed unequally [17, p.28]. People  in large power distance cultures are much more comfortable with a  larger power/status differential than small power distance cultures.  Table 2.1, adapted from Hofstede [16, p. 313], outlines the effects  of power dimension that have implications for online learning  environments. It is important to note that Hofstedes conception of  power distance is not a bi-directional one; it is conceived as a  subordinates expectation and acceptance of unequal distributions  of power in a social setting.  If online education is offered as an alternative to traditional  schooling then it is important to investigate how students perceive  the social affordances of the virtual learning institutions. In the  context of collaborative problem solving, students co-constructing  concept maps are provided information attributed to scientists who  have authority by virtue of their expertise and experience.  Arguments from authority are valued in the scientific enterprise if  those authorities themselves adhere to the scientific method. The  point here is not whether the issues of power distance will show   128    up in online classrooms but rather how does this dimension help  understand the interactional behavior in an online learning setting.  Table 2. Power distance dimension in traditional classrooms  Small Power Distance   Societies  Large Power Distance  Societies   Student-centered education   (premium on initiative)   Teacher-centered education   (premium on order)   Teacher expects students to  initiate communication   Students expect teacher to  initiate communication   Students may speak up in  spontaneously in class   Students speak up in class only  when invited by the teacher   Students allowed to contradict  or criticize teacher   Teacher is never contradicted  nor publicly criticized   Effectiveness of learning  related to amount of two-way  communication in class   Effectiveness of learning  related to excellence of the  teacher   Outside class, teachers are  treated as equals   Respect for teachers is also  shown outside class   Power distance becomes an important dimension to consider in  collaborative problem solving involving students from lower and  higher ends of the relatively ordered dimensional scale.  Collaborative learning does away with the traditional instructional  role of a teacher. Collaborative learning replaces the teacher- student dyad with a student-student dyad, replacing a didactic  pedagogical approach with a social constructivist one. In the case  of high power distance cultures, this reconfiguration in learning  results in a replacement of the more hierarchical power structures  with flatter ones. Do students take advantage of this in an online  learning setting From a cognitive standpoint, it is interesting to  investigate how cultural schemas adapt in this reconfigured  learning setting. For example, what would be the role of  confirmation bias in problem solving in intercultural collaborative  learning environments Do students from high power distance  cultures conform to the expert opinion even in cases where it  explicitly contradicts the available evidence What role does the  collaborative other play in these learning interactions These are a  few of the questions that become relevant when the power  distance dimension is considered in intercultural online learning  settings.  3.3 Individualism vs. Collectivism  Individualism pertains to societies in which the ties between  individuals are loose: every one is expected to look after himself  or herself and his or her immediate family. Collectivism as its  opposite pertains to societies in which people from birth onwards  are integrated into strong, cohesive in-groups, which throughout  peoples lifetime continue to protect them in exchange for  unquestioning loyalty [17, p.51]. This dimension describes the  degree to which a culture emphasizes an individuals reliance on  the self or the group. Table 2.2, adapted from Hofstede [16, p.  312], outlines the effects this dimension that have implications to  online environments.   This dimension is of particular interest to the social constructivist  theories of learning given the small group size emphasis. In inter- cultural online learning groups, dynamics of in-group and out- group memberships might affect how certain technology  affordances are appropriated as social affordances. They might   also affect the perception of other students in the online learning  environment. The notion of face-saving is of important when it  comes to subjective perceptions and evaluation of the user  interface, online interaction and instructional elements of an online  course. Based on sociocognitive conflict theory [28], collaborative  learning effectiveness is thought to be influenced by the extent  that students jointly identify and discuss conflicts in their  knowledge beliefs [4]. This works well in an individualist culture  but in collectivist cultures consensual forms of intersubjective  meaning making processes may be more prevalent.    Table 3. Collectivism vs. individualism dimension in  traditional classrooms   Collectivist Societies Individualist Societies  Students expect to learn how to  do  Students expect to learn how to  learn    Individual students will only  speak up in class when called  upon personally by the teacher    Individual students will speak  up in class in response to a  general invitation by the  teacher   Individuals will only speak up  in small groups    Individuals will speak up in  large groups   Large classes split socially into  smaller cohesive subgroups  based on particularistic criteria   (e.g. ethnic affiliation)    Sub-groupings in class vary  from one situation to the next  based on universalistic criteria   (e.g. the task  at hand )   Formal harmony in learning  situations should be maintained  at all times    Confrontation in learning  situations can be salutary;  conflicts can be brought into  the open   Neither the teacher nor any  student should ever be made to  lose face   Face-consciousness is weak   3.4 Femininity vs. Masculinity  Masculinity pertains to societies in which the gender roles are  clearly distinct; femininity pertains to societies in which the  gender roles overlap [17, p. 82]. This dimension refers to  expected gender based division of labor in a culture. The cultures  that score towards what Hofstede refers to, in a confusing choice  of category labels, as  masculine  tend to have very distinct  expectations of male and female roles in society. The more   feminine  cultures have a greater ambiguity in what is expected  of each gender. Table 2.3, adapted from Hofstede [16, p. 315],  summarizes the implications of this dimension for online learning  environments. Collaborative learning is often distinguished from cooperative  learning by the argument that collaboration involves joint activity  or an effort to maintain a joint conception [34] whereas  cooperation involves a mere joining of individual activities [10].  Collaboration is often conceived of as being beyond a basic  division of labor and more of an enterprise involving parties with  equal stakes.   129    Table 4. Femininity vs. masculinity dimension in traditional  classrooms   Feminine Societies Masculine Societies  Teachers avoid openly praising  students  Teachers openly praise good  students  Teachers use average student  as the norm    Teachers use best students as  the norm   System rewards students' social  adaptation    System rewards students'  academic performance   Students practice mutual  solidarity    Students compete with each  other in class   Students try to behave  modestly   Students try to make  themselves visible   3.5 High Uncertainty Avoidance vs. Low  Uncertainty Avoidance  The extent to which the members of the culture feel threatened by  uncertain or unknown situations[17, p. 113]. This dimension  refers to how comfortable people feel towards ambiguity. Low  uncertainty avoidance cultures feel much more comfortable with  the unknown. High uncertainty avoidance cultures prefer formal  rules and any uncertainty can express itself in higher anxiety.  Table 2.4, adapted from Hofstede [16, p. 314], summarizes the  effects this dimension that have implications to online learning  environments.  Table 5. Uncertainty avoidance dimension in traditional  classrooms   Weak Uncertainty  Avoidance Societies   Strong Uncertainty  Avoidance Societies   Students feel comfortable in  unstructured learning  situations: vague objectives,  broad assignments, no  timetables    Students feel comfortable in  structured learning situations:  precise objectives, detailed  assignments, strict timetables   Teachers are allowed to say  I  don't know     Teachers are expected to have  all the answers   Students are rewarded for  innovative approaches to  problem solving   Students are rewarded for  accuracy in problem-solving   Teachers are expected to  suppress emotions (and so are students)    Teacher are allowed to behave  emotionally  (and so are students)   Teachers interpret intellectual  disagreement as a stimulating  exercise   Teachers interpret intellectual  disagreement as personal  disloyalty   The dimension of uncertainty avoidance can affect how students  perceive social affordances of the online learning environment.  Also of importance are the effects of culture on the interpretation  or an acknowledgement of the ambiguous data and judgment of  the relevance of data in the unfolding interactional sequence.    4. CULTURE AND COMMUNICATION  Besides Hofstedes cultural dimensions model the dimension of  low-context vs. high-context cultures introduced by Hall [14]  are important in the contexts and situations of intercultural  communication. According to Hall [14], in high-context cultures,   usually the cultures with high power distance, a member needs to  be explicitly asked to respond to elicit behavior that is a deviation  from the norm. Table 2.5 lists patterns of Halls cultural  communication context dimension. Hall characterizes speaking as  an art in high-context cultures, with an emphasis on the emotional  aspect. High-context cultures privilege social motivation. In low- context cultures, by contrast, members expect to influence others  to act by explicitly pointing out pertinent information. The  information provided implicitly enables the communicating other  to take the desirable decision. Low-context cultures privilege  rational information.   Table 6: Low-context vs. high-context cultural communication   styles  High-context communication Low-context communication  Communication is aimed at  emotions and rhetorical  persuasion  Communicative focus is  rational information   Speech is unhurried and long,  as persuasion takes time   Information is desired in  quantity and expected to be  delivered at once    The main emphasis is not laid  on the transmission of  information, as most of it lies  in the context   Decisions are taken on the  basis of information   Both speaking and listening  are something to be  thoroughly enjoyed   Speaker errors carry social  costs as they blur information    Ambiguous interpretation is  allowed   Unequivocal Interpretation is  desired   If the communicative context varies across cultures than it  becomes a variable of interest in the learners interactional  accomplishment of problem solving.    5. CILTURE AND COGNITION  According to Nisbett and Norenzayan [30, p. 1], mainstream  psychology in general had made four basic assumptions about  cognition. Adapting from them, the four foundational  psychological assumptions regarding human cognition are:     Universality: Basic cognitive processes of sensation,  perception, attention and memory are universal. In other  words, basic cognitive processes are invariant across cultures  and communities.    Content Independence: Basic cognitive processes are  invariant across contents. In other words, cultural differences  in content do not affect the nature and structure of the basic  cognitive processes.     Environmental-Sufficiency: Cognitive processes of general  learning and interference operate upon environmental  contents to equip the child for functional survival. The  environment provides content to cognitive processes without  the need for cultural or social interventions. In other words,  cultural differences in cognitive processes are due to different  environmental influences and not social influences.    Infinite Cultural Variance: Since the universal basic  cognitive substrate is content independent and  environmentally-sufficient, the range of cultures is a function  of the variance in environmental conditions. In other words,   130    cognition places no constraints on the possible evolutionary  design space of cultures.   All in all, these four assumptions have led to a belief in a  fundamental dissociation between cognition and culture. One  consequence of this was that psychology and anthropology  evolved into independent academic disciplines with mostly non- overlapping research agendas. However there were some  exceptions to this dualist view of cognition and culture. These  exceptions include in psychology, Lev Vygotsky and colleagues  [40, see 42 for a biographical and historical treatment of this  influential research movement]. In cognitive anthropology, most  notably D'Andrade [6]; and in cognitive sociology, Dimaggio [11].  Recent empirical results have shown that culture and cognition are  mutually implicated and are not disassociated as traditional  psychology has postulated.    5.1 Nisbett and Colleagues Cross-Cultural  Psychology Findings  Richard Nisbett and colleagues have embarked upon an  experimental cross-cultural psychology research program to  systematically investigate cognitive differences across cultures.  Table 2.6, compiled from Nisbett [29, pp.xix, 44-45] and Nisbett  and Norenzayan [30, pp. 21-25], presents a concise summary of  above discussion along with empirical evidence from the  literature.   The cultural difference in attention to field vs. object is highly  relevant to collaborative knowledge map learning environments.  East-Asian learners might pay attention to a meaningful group of  interrelated knowledge map objects whereas Western learners  might attend to individual objects and evidential relational links.  The cultural difference in attention can vary the ways in which  referencing and deixis are carried out in collaborative discourse.  East-Asian learners might make more references to regions of the  concept maps and groups of related concept map objects (i.e., to  fields of interest), whereas Western learners might reference  individual objects in their collaborative discourse. This translates  into a socio-technical design hypothesis that given the choice of  referencing regions of concept map areas and individual objects in  the concept map, East Asian learners will appropriate the  affordances for referencing fields. On the other hand, Western  learners will appropriate the affordances for referencing individual  objects. The implications from the cultural difference in perception is that  Western learners by virtue of being more susceptible to primacy  effect might favor earlier perceptions of information related to a  collaborative learning task. East-Asian learners might perceive  more relationships between the information in concept map and  instructional materials leading to a greater number of evidential  relation links in the concept map.   The cultural difference in causal inferencing processes implies that  East Asian learners might be more inclined to reason-giving that  prioritizes situational factors when compared to the dispositional  attributions of Western learners. One particular implication would  be the cultural effect on collaborative argumentation. Also, East- Asian learners perception of their collaborative partners might  follow this same trajectory. This might manifest as East-Asians  giving higher ratings for their collaborative peers due to situational  attributions explaining any perceived unpleasant performance.  Western learners might perceive their collaborative peers for their  dispositional competence. This cultural difference in cognitive  processes might manifest as East-Asian learners preferring a  highly inclusive final conclusion in intercultural collaborative   problem solving tasks. Western learners might argue for more  differentiated analytical hypothesis that seems logically the most  viable.  Table 7. Cognitive differences between East-Asians and  Westerners   Cultural Profile   Cognitive  Process   Westerners East-Asians Empirical  Evidence   Attention Object  Westerners tend  to attend to  individual objects in the  perceptual field   Field East-Asians  tend to attend  to whole fields  rather than  individual objects  [25]  Field  Dependence [46]  Perception Object-oriented   Westerners have  lesser difficulty  in detaching  objects from  their perceptual  contextual fields   Relation- oriented   East-Asians  have difficulty  in disentangling an object from  its perceptual  surroundings  [20]  [26]    Causal Inference  Dispositional  Westerners susceptible to  overlooking of  situational  factors on  observed behavior   Situational   East-Asians  susceptible to  hind-sight bias    [32]  [2]  Knowledge Organization   Categorical  Reliance on  categories of  objects/events   Relational  Reliance on  relationships between  events/objects   [3]  Reasoning Analytical   Application of  formal logical  rules and  analytical  procedures that  emphasize non- contradiction in  hypothesis   Holistic   Willingness to  simultaneously  entertain  several  contradicting  hypotheses    [31]  [30]  131    6. CULTURE AND TECHNOLOGY  ENHANCED LEARNING   Vatrapu [39] found that despite differences between American and  Chinese cultural group participants on (a) how they used the tools  and resources of the learning environment (appropriation of  affordances) and (b) how they related to each other during and  after their collaborative learning interactions (technological  Intersubjectivity),  there were no significant individual learning  outcomes differences. Kim and Bonk [9] report cross-cultural  differences in online collaborative behaviors of the US, Finnish  and Korean participants in their study. Daniels, Berglund and  Petre [8] found cultural differences in international projects in  undergraduate CS education. McLoughlin [27] based on her  experiences with developing web-based instruction for Australian  Indigenous education calls for a culturally responsive technology.  Iivonen, Sonnenwald, Parma, and Poole-Kober [19] found  culturally influenced differences in language and communication  styles in a library and information studies course taught over the  Internet in Finland and US. Walton and Vukovics [41] work with  south African students from disadvantaged backgrounds found  that cultural differences make it difficult for the students to make  the transition to the web use. Crump [5] explored the effects of  computing learning environment on the newly arriving  international students at universities in New Zealand. The author  reports that the cooperative and collaborative learning  environment was an issue of concern to the students. The author  says it is likely due to the oversimplification of social structure of  groups, individual and group goals and the diverse nature of  knowledge construction in the collaborative learning  environments. Duncker [12] conducted an ethnography of the  usability of a library metaphor used in digital libraries in the  cultural context of the Maori who are the indigenous population of  New Zealand. Duncker says that metaphors and metaphorical  thinking are strongly rooted in culture. The Maori found the digital  libraries interesting but difficult to use due to the breakdown of the  library metaphor caused by a number of cultural misfits. Keller,  Prez-Quiones and Vatrapu [22] outlined cultural issues and  opportunities in computer science education.   7. DISCUSSION    Taking the existing body of research on cultural effects on social  behavior, cognitive processes, online pedagogies and HCI,  learning analytics should consider both appropriation of  affordances (tool use) as well as technological intersubjectivity   (how learners and teachers relate to, interact with, and form  impressions of each other in technology enhanced teaching and  learning settings). Given that both seminal networked learning  research (Hiltz, 1994) and current online learning best practices  prescriptions (Moore, 2006) emphasize student collaboration, and  since these aspects vary across cultures in traditional classroom  settings (Hofstede, 1986) as well as online learning settings  (Edmundson, 2007), learning analytics should critically examine  mono-cultural design assumptions. Learning environments and  learning analytics solutions that do not incorporate diverse  alternates for action might not achieve the best results in terms  of student learning processes, outcomes and satisfaction.  In order to exhaustively study the potential effects of culture on  the appropriation of potentials for action and the negotiation of the  meaning of those actions, one needs to analyze individual actions  in the context of their interactional sequences [21]. Therefore,  learning analytics tools should support micro-genetic analysis of   learners interactions apart from aggregating behavioral outcomes  for statistical testing.   Although the cognitive embeddedness of discourse and  knowledge-building have been theorized and empirically  evaluated [36, 37], social engagement and cultural embeddedness  aspects of these design implementations have remained  unexamined so far. Learning analytics designers need to consider  ways of facilitating the varying degrees of social and cognitive  embeddedness. Increasingly, issues are being identified in the  cross-cultural implementation of online learning or e-learning  systems which are primarily designed, developed, and evaluated in  North America and/or Western Europe contexts [13]. To help  remedy this situation, future work could investigate three models  of cultural influence in online learning and learning analytics: (1)  culture-specific, (2) culture-comparative, and (3) culture- interactional. Culture-specific work studies learning analytics in a  specific cultural context where learners have a shared sense of  identity. Culture-comparative studies investigate learning analytics  processes and products across cultures. In culture-interactional  studies, learning analytics in intercultural settings is the primary  focus.  8. ACKNOWLGEMENT   The presentation of this paper was supported by the Comparative  Informatics networking grant under the Second International  Network Programme of the Danish Ministry of Science, Technology  and Innovation.   9. REFERENCES  [1]  Bourdieu, P. Structures, Habitus, and Practices. Cambridge   University Press, City, 1977.  [2]  Choi, I. and Nisbett, R. Situational Salience and Cultural   Differences in the Correspondence Bias and Actor-Observer  Bias. Personality and Social Psychology Bulletin, 24, 9 1998),  949.   [3]  Choi, I., Nisbett, R. and Smith, E. Culture, category salience,  and inductive reasoning. Cognition, 65, 1 1997), 15-32.   [4]  Constantino-Gonzlez, M. and Suthers, D. A Coached  Collaborative Learning Environment for Entity-Relationship  Modeling. Proceedings of the 5th International Conference on  Intelligent Tutoring Systems2000), 324-333.   [5]  Crump, B. J. New Arrival students: mitigating factors on the  culture of the computing learning environment. Proceedings of  the sixth conference on Australian computing education -  Volume 302004), 49 - 56.   [6]  D'Andrade, R. G. The Development of Cognitive  Anthropology. Cambridge University Press, New York, 1995.   [7]  Damen, L. Culture Learning: The Fifth Dimension on the  Language Classroom1987), Page 367.   [8]  Daniels, M., Berglund, A. and Petre, M. Reflections on  International Projects in Undergraduate CS Education.  Computer Science Education, 9, 3 1999), 256-267.   [9]  Davison, R. Technical opinion: Cultural complications of  ERP. Communications of the ACM, 45, 7 2002), 109 - 111.   [10]  Dillenbourg, P., Baker, M., Blaye, A. and OMalley, C. The  evolution of research on collaborative learning. Learning in  Humans and Machines1995), 189-205.   [11]  Dimaggio, P. Culture and Cognition. Annual Review of  Sociology, 231997), 263-287.   132    [12]  Duncker, E. Digital library communities and change: Cross- cultural usability of the library metaphor. Proceedings of the  second ACM/IEEE-CS joint conference on Digital  libraries2002), Pages: 223 - 230.   [13]  Edmundson, A. Globalized E-learning Cultural Challenges. Information Science Publishing, City, 2007.   [14]  Hall, E. Beyond Culture. Anchor Press, New York, 1976.  [15]  Harris, M. The Epistemology of Cultural Materialism. From   Cultural Materialism: The Struggle for a Science of Culture.  AltaMira Press. Broadview Press, City, 1998/1966.   [16]  Hofstede, G. Cultural Differences in Teaching and Learning.  International Journal of Intercultural Relations, 10, 3 1986),  301-320.   [17]  Hofstede, G. Cultures and Organizations: Software of the  Mind, Intercultural Cooperation and its Importance for  Survival. McGraw-Hill, 1997.   [18]  House, R. J., Hanges, P. J., Javidan, M., Dorfman, P. W. and  Gupta, V. Culture, Leadership and Organizations: The  GLOBE study of 62 societies. Sage Publications, Newbury  Park, CA, 2004.   [19]  Iivonen, M., Sonnenwald, D. H., Parma, M. and Poole-Kober,  E. Analyzing and understanding cultural differences:  Experiences from education in library and information studies.  ,. City, 1998.   [20]  Ji, L., Peng, K. and Nisbett, R. Culture, control, and perception  of relationships in the environment. Journal of Personality and  Social Psychology, 78, 5 2000), 943-955.   [21]  Jordan, B. and Henderson, A. Interaction Analysis:  Foundations and Practice. The Journal of the Learning  Sciences, 4, 1 1995), 39-103.   [22]  Keller, B., Prez-Quiones, M. A. and Vatrapu, R. Cultural Issues and Opportunities in Computing Education. City, 2006.   [23]  Kroeber, A. L. and Kluckhohn, C. Culture: A Critical Review  of Concepts and Definitions. Harvard University Peabody  Museum of American Archeology and Ethnology, 1952.   [24]  Lederach, J. P. Preparing for peace: Conflict Transformation  Across Cultures. Syracuse University Press, Syracuse, NY,  1995.   [25]  Masuda, T. and Nisbett, R. Attending holistically versus  analytically: Comparing the context sensitivity of Japanese  and Americans. Journal of Personality and Social Psychology, 81, 5 2001), 922934.   [26]  Masuda, T. and Nisbett, R. Culture and change blindness.  Cognitive Science, 302006), 1-19.   [27]  McLoughlin, C. Culturally Responsive Technology Use:  Developing An On-Line Community Of Learners. British Journal of Educational Technology, 30, 3 1999), 231-243.   [28]  Mugny, G. and Doise, W. Socio-cognitive conflict and  structure of individual and collective performances. European  Journal of Social Psychology, 8, 2 1978), 181-192.   [29]  Nisbett, R. The Geography of Thought: How Asians and  Westerners think differently . . . and Why. Free Press, New  York, 2003.   [30]  Nisbett, R. and Norenzayan, A. Culture and Cognition. City,  2002.   [31]  Nisbett, R., Peng, K., Choi, I. and Norenzayan, A. Culture and  systems of thought: Holistic versus analytic cognition.  Psychological Review, 108, 2 2001), 291-310.   [32]  Norenzayan, A. and Nisbett, R. Culture and Causal Cognition.  Current Directions in Psychological Science, 9, 4 2000), 132- 135.   [33]  Redfield, R. Introduction to Malinowski, B. Magic, science  and religion. Beacon Press, Boston, 1948.   [34]  Roschelle, J. and Teasley, S. The construction of shared  knowledge in collaborative problem solving. Springer-Verlag,  City, 1995.   [35]  Sternberg, R. J. Cognitive Psychology. Wadsworth, 2006.  [36]  Suthers, D., Vatrapu, R., Medina, R., Joseph, S. and Dwyer, N.   Beyond Threaded Discussion: Representational Guidance in  Asynchronous Collaborative Learning Environments.  Computers and Education, 50, 4 2008), 1103-1127.   [37]  Suthers, D. V., R;Joseph, S, Dwyer, N. and Medina, R.  Representational Effects in Asynchronous Collaboration: A  Research Paradigm and Initial Analysis. Institute of Electrical  and Electronics Engineers, Inc. (IEEE), City, 2006.   [38]  Tylor, E., B Primitive Culture: The Science of Culture. McGraw-Hill, City, 1903/1988.   [39]  Vatrapu, R. Cultural Considerations in Computer Supported  Collaborative Learning. Research and Practice in Technology  Enhanced Learning, 3, 2 2008), 159-201.   [40]  Vygotsky, L. Mind in society. Harvard University Press,  1930/1980.   [41]  Walton, M. and Vukovic, V. HCI in the developing world:  Cultures, literacy, and the web: dimensions of information   scent . interactions, 10, 2 2003), 64 - 71.   [42]  Wertsch, J. Vygotsky and the social formation of mind. Harvard University Press, Cambridge, MA, USA, 1985.   [43]  White, L. A. Energy and the evolution of culture.  American  Anthropologist. Reprinted. Mayfield Pub. Co, City,  1943/1996.   [44]  Williams, R. Marxism and literature. Oxford University Press,  Oxford, 1977.   [45]  Williams, R. Keywords: A Vocabulary of Culture and Society. Oxford University Press, New York, 1983.   [46]  Witkin, H. A. and Goodenough, D. Field dependence and  interpersonal behavior. Psychological Bulletin, 84, 4 1977),  661-689.   133    "}
{"index":{"_id":"14"}}
{"datatype":"inproceedings","key":"Rosen:2011:SSN:2090116.2090137","author":"Rosen, Devan and Miagkikh, Victor and Suthers, Daniel","title":"Social and Semantic Network Analysis of Chat Logs","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"134--139","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090137","doi":"10.1145/2090116.2090137","acmid":"2090137","publisher":"ACM","address":"New York, NY, USA","keywords":"internet relay chat (IRC), learning analytics, semantic network analysis, social network analysis, virtual environments","Abstract":"The University of Phoenix understands that in order to serve its large population of non-traditional students, it needs to rely on data. We have created a strong foundation with an integrated data repository that connects data from all parts of the organization. With this repository in place, we can now undertake a variety of analytics projects. One such project is an attempt to predict a student's persistence in their program using available data indicators such as schedule, grades, content usage, and demographics.","pdf":"Social and Semantic Network Analysis of Chat Logs Devan Rosen   School of Communications  Ithaca College   953 Danby Road  Ithaca, NY 14850, USA   1-607-274-5100  drosen@ithaca.edu  Victor Miagkikh  Information and Computer Sciences   University of Hawaii  1680 East-West Road   Honolulu, HI 96822, USA  1-808-956-7420  miagkikh@hawaii.edu  Daniel Suthers  Information and Computer Sciences   University of Hawaii  1680 East-West Road   Honolulu, HI 96822, USA  1-808-956-3890  suthers@hawaii.edu  ABSTRACT  Multi-user virtual environments (MUVEs) allow many users to  explore the environment and interact with other users as they  learn new content and share their knowledge with others. The  semi-synchronous communicative interaction within these  learning environments is typically text-based Internet relay chat  (IRC). IRC data is stored in the form of chatlogs and can generate  a large volume of data, posing a difficulty for researchers looking  to evaluate learning in the interaction by analyzing and  interpreting the patterns of communication structure and related  content. This paper describes procedures for the measurement and  visualization of chat-based communicative interaction in MUVEs.  Methods are offered for structural analysis via social networks,  and content analysis via semantic networks. Measuring and  visualizing social and semantic networks allows for a window  into the structure of learning communities, and also provides for a  large cache of analytics to explore individual learning outcomes  and group interaction in any virtual interaction. A case study on a  learning based MUVE, SRIs Tapped-In community, is used to  elaborate analytic methods.   Categories and Subject Descriptors  H.5.3 [Information interfaces and presentation]: group and  organization interfaces  evaluation/methodology.  General Terms  Algorithms, Measurement, Human Factors, Theory.   Keywords  Social network analysis, semantic network analysis, learning  analytics, virtual environments, Internet relay chat (IRC)  1. INTRODUCTION Multi-user virtual environments (MUVEs) are used for many  different purposes in a number of contexts, but the interaction  within these environments can often lead to learning outcomes  and resource sharing, and there is an increase in their use for  learning communities. Communicative interaction within these  environments is commonly conducted via Internet relay chat  (IRC). Text boxes displaying IRC has been a successful tool for   allowing for communicative interaction. However, IRC poses a  difficulty for researchers seeking to analyze and interpret the  communicative interaction since data is stored in the form of  chatlogs, which can produce large volumes of text data. This  paper discusses and applies procedures for the representation and  analysis of chat interaction in learning based MUVEs, or learning  taking place in any type of MUVE, as social and semantic  networks. A description of the social and semantic network  approach to human communication is presented followed by a  review of parallel methodological techniques. Elaboration of  methods presented in this research is covered along with sample  outputs from a case study on SRIs Tapped-In (tappedin.org)  community [1]. Finally, applications and future research  possibilities are offered.   2. CONCEPTS 2.1 Communicative Interaction in MUVE  Although MUVEs have a wide array of uses, communicative  interaction within the environment is often conducted through  Internet Relay Chat (IRC). IRC is conducted in a semi- synchronous way, where comments posted appear almost  instantly for other users to view and respond to. IRC is a much  more real time mode of computer-mediated communication than  listserv messages, bulletin boards, and email. Much like instant  messaging (IM), IRC allows users to select a username that  appears before each comment they post, allowing multiple users  to comment and maintain conversational interaction.   IRC interaction is conducted within a chat-box that displays all  users comments along with their username in a log file. In  addition to IRC interaction being semi-synchronous, it is also  persistent. The persistence of these interactions allow for the  storage of all data as chatlogs, which can in turn be used for  analyses of the users interaction.  However, the nature of  chatlogs as a dynamic, non-threaded interaction introduces some  methodological hurdles regarding analysis. Posts to IRC  conversations are generally quite short, usually a few words to  several lines allowing the IRC interaction to allow for multi- participant semi-synchronous interaction, similar to face-to-face  (FtF) conversation in the sense that many people can interact in  the same communicative space [2]. However, IRC interaction  departs from FtF in how interactional coherence is achieved, and  users adapt to CMC interaction in interesting ways [3]. One of the  principle differences is adjacency relevance, where an utterance  in FtF interaction is generally relevant to the one before it. In  IRC, this constraint is loosened: an utterance may be relevant to  the one that appeared several lines before it. Herring [3] showed  that users invent devices to manage interactional coherence in  spite of the fact that IRC contributions can arrive out of sequence,  violating conventions from in face-to-face conversation      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.   134    concerning the sequential organization of contributions. Since we  cannot assume that adjacent contributions are relevant to each  other, as they may be relevant to contributions a little while back,  we need an estimate of this relevance. Estimates of who was  talking to who are generated from the concept of chat proximity:  these people are co-present, and contributions may be relevant to  any of these things said recently. The constraint that contribution  C_i is relevant to C_(i-1) is loosened to a temporal window. The  algorithm introduced in the Methods section below is motivated  by this unique interaction, and uses a temporal window to capture  such non-adjacent utterances.  Smith, Farnham, & Drucker [4] investigated the social life of  small graphical chat spaces by analyzing Microsofts V-Chat  systems. The VChat research illustrates the usage patterns of  graphical chat systems, illuminating the ways physical proxemics  are translated into social interactions in online environments.  Krikorian, Lee, Chock, and Harms [5] developed methods to  study user proximity in graphical chat rooms, and found that  various perceived demographics influenced the social distance  of avatars in the graphical chat environment.  In addition to the spatial analysis, there have also been  methodological advancements regarding the communicative  content of virtual environments. Sack [6] generated conversation  maps of newsgroup postings and described very large  conversations by visualizing large amounts of interaction in  newsgroups. Suthers, Dwyer, Medina, and Vatrapu [7] developed  a framework for representing and analyzing distributed interaction  within multi-user virtual environments, including some structural  representation of interaction in sequential records of events.  Rosen, Woelfel, Barnett, and Krikorian [8] explicated a  methodology for semantic network analyses of IRC interaction in  virtual worlds. Rosen & Corbit [9] developed network analytic  techniques for the measurement and representation of the  structure of networks from IRC interaction.   Understanding the structure and content of the interaction  provides an in-depth and unique window into MUVEs along  several lines. First, network position can be used to identify  network roles, similar to Turner et al. [10], identifying roles such  as answer person and question person. Second, network analytic  techniques can be employed on the subsequent data. Network  visualizations can be generated allowing for visual and  representational analyses, elements that have traditionally  important to community research [11]. Finally, network analytics  and representations can be used in cohort with semantic network  analysis for a more complete understanding of the learning  environments and interactions.   2.2 Social Networks  Social network perspectives focus on the structure of social  systems. Individual characteristics are only part of the story:  people influence each other, and ideas and materials flow  throughout the network [13]. From the network perspective, the  social environment can be expressed as patterns or regularities in  relationships among interacting units. This section elaborates  some of the network concepts and terminology used in the  subsequent methods for the analysis of MUVEs.  The form of network that will be utilized herein is a  communication network. Communication networks are generally  defined as the patterns of contact that are created by the flow of  messages among communicators through time and space [see 2].  However, these flows are not clear in IRC interaction from an   adjacency approach, and the algorithmic solution presented in this  paper is way of deal with this problem. Communication network  analysis identifies the communication structure, or  communication flow. Relation ties (linkages) between actors are  channels for the transfer (flow) of either material or nonmaterial  resources, or for an association between actors. The ties that exist  between the nodes can vary along several elements, including  direction, reciprocity, and strength.  Ties between actors can be measured as being either directional,  or non directional. Ties that are directional indicate the movement  from one point to another, such as the number of phone calls one  person makes to another, or the degree of liking one person has  for another. Additionally, these links can also be symmetrical or  asymmetrical.  If the link is directional and the relation has  different values in each direction then the link is asymmetrical  and lacks reciprocity. Non-directional links simply indicate an  association of two actors in a shared partnership, such as two  students being part of the same class. There are many measures of  centrality for individual nodes, as well as how connected the  entire network is; select measures are discussed below.   2.2.1 Degree Centrality  The degree measure of centrality is calculated by counting the  number of adjacent links to or from an actor in a network [12].   Freeman [14] conceptualized this measure as an indicator of  individual activity, yet it does not capture system-wide properties  of the network like density and centralization, discussed below.  It  does, however, represent the number of alternatives available to  an individual in the network. While a relatively straightforward  measure, degree centrality provides insight into individual  contributions to the interconnectedness of the overall network  [14].   2.2.2 Betweenness Centrality  Betweenness centrality measures the relative brokerage of an  individual node i by indicating the number of nodes j that need to  go through i to get to other nodes k that could otherwise not be  reached. Betweenness centrality is calculated by the proportion of  all geodesics linking j & k that pass through i,  for all nodes.    2.2.3 Density Density is used to measure the completeness of the relations in a  network, also called connectedness.  Measured as the ratio of total  links to possible links, density can identify networks as being  sparse (relatively disconnected) or dense (relatively well  connected).   2.2.4 Centralization Centralization measures the disparity, or variation of the  individuals centrality (which can be betweenness or degree  centrality) in a given network. The higher a networks degree  centralization is the more likely it is that few individuals are well  connected while others are less connected.  Conversely, the more  decentralized a network is, the more equal the members  centrality scores are.   2.3 Semantic Networks  In semantic network analysis, a specific text is analyzed to  generate a measure of the degree to which words are associated.  The association is then used to infer something about their  meaning or the meaning of the context they were used in. One of  the more common approaches is to generate the amount of co- occurrence between word-pairs within a particular set of text.   135    Then, the co-occurrence measure of relatedness across a particular  set of words can be used to group, cluster, or scale the words (or a  specific subset, such as frequently occurring words. The groups or  clusters can be used for analysis, or used to obtain additional  measures for use in other analyses, or bases for formal content  analysis [15, 16].    3. METHOD 3.1 Social Network Analysis  The structure of the communicative interaction within a MUVE  may be examined through network analysis. Network analysis is a  set of research procedures for identifying structures in social  systems based on the relations among the systems components,  and is the methodology used to operationalize the network  approach to interaction, discussed above.  The basic network data set is an n x n matrix S, where n equals  the number of nodes in the analysis. A node is the unit of  analysis; in the current research a MUVE participant will be  considered a node. Each cell, Sij, indicates the strength of the  relationship, which would typically represent the amount of  communication from person i to person j. Since there is no  inherent direct communicative relationship between individuals in  IRC interaction, the relationship used herein assigns relational  strength by capturing temporal proximity of contributions in IRC.  Relationships in networks are analyzed as directional when  possible, and in the current study direction is established based on  the ordering of contributions within a temporal window in IRC.  This method provides the directional differences between all  analyzed parties, representing the communication matrix.  Network mapping procedures are used to generate sociogram  maps that visually represent the networks created using above  procedures. These will allow for the visual analysis of other  network data, as well as elaborate cliques and network roles that  can remain cloaked when only analyzing numerical outputs.    3.1.1 MUVE Communication Matrix Formation  To generate the n x n matrix used in the analysis of MUVE  interaction, a process was developed that extracts the strength of  the relationship between each cell, Sij. Since IRC is logged  temporally based on the sequential comments of participants,  methods can be used to generate relational strength based on  proximity in the interaction. The algorithm includes several   parameters to generate relational data from IRC interaction. Using  the time stamp that accompanies all posts, a temporal parameter  was used to help insure that a user is not considered connected to  all users that posted after their post. This parameter can be set for  use based on the context, as some interactions are faster moving  than others; the current study used a limit of 120 seconds before a  users connection was reset. See Table 1 for Pseudocode of the  algorithm used.  The algorithm is O(n), where n is the number of records in the  chat dataset. Each record contains timestamp, userid, and  contribution. The algorithm assumes that chat records in the  dataset are sorted by timestamps. The algorithm was implemented  in the Java programming language. The window size parameter  was set to 120 seconds.    3.2 Semantic Network Analysis  The method used in this study adapts and implement neural-based  content analysis software to observe Internet communication  patterns in chat rooms [8]. This implementation uses Catpac  [17], a developed and proven semantic network analysis package  that has the capability to extract word patterns and clusters.  Sliding a text-window through the text and associating each word  in the window with a neuron in an artificial neural network extract  clusters. Using a proprietary variation of an interactive activation  and competition algorithm, connection strengths or weights are  generated as a function of the coactivation patterns among the  neurons. These weights in turn serve as the basis of cluster  analysis and Galileo mapping [18].  Catpac has been used for the study of traditional text [20], such as  articles and long response questionnaires. It has been successful in  revealing clusters of associated words in text that provided helpful  quantitative data to support qualitative interpretations.  One of the most important aspects of the method used in the  procedure discussed in this paper is the ability to analyze data  based on set parameters. For this, an algorithm has been  developed that parses chat data into separate and interrelated files  used to determine individual, group, and systematic  organizational patterns over time. This becomes useful when  combined with a qualitative analysis where the researcher has an  ethnographic understanding of the community members, whereas  there is a  name file  that allows for directed analysis and the  labeling of contributions. For example, if the learning community  were associated with a large undergraduate class, the teacher  would have the ability to observe semantic clusters extracted from  any designated groups communication (e.g. freshmen, non- majors, etc.). If the analysis was on a mentor-based learning  community one could observe the difference between  communication originating from mentors/teachers as compared to  student users. Other uses bridge to industry, where virtual task  groups' general learning interactions could be parsed, revealing  both potentially positive and negative trends in the interaction.   4. OUTPUTS Outputs below were generated from SRIs Tapped In [1], a virtual  organization that hosts the content and activities of many  thousands of education professionals annually in more than 8,000  user-created spaces that include IRC, threaded discussions, shared  files and URLs, and other tools to support collaborative work.  Education agencies and institutions of higher education use  Tapped In to meet the needs of their students and faculty. Also,  approximately 40-60 community-wide activities per month are   Table 1. Pseudocode of the algorithm used for communication  matrix formation   InitializeaqueueQoftimestampsanduseridpairs; Foreachchatrecordindatasetdo: oldestMessageTime=timestampwindowSize; RemoveallrecordsfromthebackofQwhicharepast oldestMessageTime; SearchusersinQforrecordofthisuser; Ifearlierrecordofthisuserexists  Replacetherecordwithanewtimestamp, useridpair; EndIf Incrementcurrentusersdirectedweighttoallusersin Q; EndFor  136    explicitly designed to help connect members, and groups are often  formed after members meet in these activities. Analytic outputs  below were generated from a single chat session of a Tapped In  user group that deals with the use of wikis in the classroom. The  session was 1 hour long with 62 participants.   4.1 Social network analytics and visualization   The analytics employed in the current methodological explication  are Degree Centrality, Betweenness Centrality, Density, and  Centralization. The degree centrality for the users in the Tapped- In group can be found in Table 2, and the visualization of the  network can be found in Figure 1. Network centralization is  6.372% (Outdegree) and 4.104% (Indegree), indicating a  decentralized network. Network density is 46.20, indicating a  fairly dense network.  The centrality measures presented in Table 2 offer interesting  insights into the user interaction within the chat. First, the degree  centrality indicates the number of incoming and outgoing  connections via chat posts. The values have been normalized  relative to the number of users. The in-degree measures indicate  the number of message posts within the time window that were  pointed back to that user, and the out-degree indicates the number   of messages that user posted pointing back to other users within  the window. A few users were the most active, with some slight  differences between their in- and out-degree values. However,  since the centralization measure was very low, the distribution of  interaction was indeed spread through the network, without a core  group of users that were substantially outweighing a periphery.  The betweenness centrality scores indicate that there are indeed  several users that have very high scores, and thus act as bridges of  information in the network. These users connect other individuals  that could not otherwise reach many of the people in the network.   This bridging role can be seen in the visualization of the network  in Figure 1. It is clear that user 7, 44, 37, and 27, who have the  highest betweenness centrality, are structurally positioned in the  network between many other users that would otherwise be  disconnected. The sociogram also reveals several cliques (i.e.  clusters) with in the network, as well as a few people that are not   very well connected, often connected only to one other user, such  as 14 and 56.  Using network analytic measures, such as centrality and density,  provide for structural analyses of IRC interaction based on chat  proximity. These measures and visualizations can thus be used to  decipher effects along different levels of granularity; at the micro  level, user roles, such as bridging or leader roles, can be  identified, at the meso level one can identify group formation  through clique detection [see 21 for elaboration], and at the macro  level overall network centralization and density can be calculated.  These measures, however, represent only a small subset of  possible analytic approached afforded by network analysis, and  measures should be chosen that help explain the phenomena being  explored.   4.2 Semantic network visualization  Using the neural network engine in the CATPAC package allows  for the semantic analysis of parsed content. The plot in Figure 2,  produced using ThoughtView [19], contains a multi-dimensional  scaling representation of the top 40 words from the entire 1-hour  of IRC from the group analyzed. Usernames were automatically  stripped from the data for the analysis of the complete interaction.  There are some contextual issues regarding automated textual  analysis, like the occurrence of errors in user typing,  abbreviations, and icons. Additional outputs available, but not  included here, include dendograms, frequency lists, and two- dimensional plots [19]. Additional parsing of content into  individual text files is also possible with the algorithm, allowing  for analysis of specific user content based on other parameters  such as demographics, individual network metrics (e.g.  centrality), learning outcomes, etc. Extracting semantic clusters  from user activity in IRC can allow for further exploration of  contributions from specific users identified in the social network  analysis as relevant to research agendas. For example, there may  be interest in analyzing the content of contributions from very  central participants, or from participants that became more central   Table 2. Degree and Betweenness centrality rankings for  users. Degree centralites below 1.0 and betweenness   centralities below 0.5 have been removed from table. User  IDs correspond to sociogram in Figure 1, and have been   anonymized from original user logon names   User  ID   Normalize d In- Degree   Normalize d Out- Degree    User  ID   Normalized  Betweennes s   15 6.974 4.702  7 42.783   37 3.48 3.86  44 35.314   7 2.614 2.652  37 25.462   41 1.927 0.903  27 23.44   58 1.725 0.921  42 9.023   4 1.688 1.767  8 3.448   27 1.436 1.336  6 0.705   2 1.308 1.478  32 0.583   22 1.255 1.162  16 0.583   5 1.249 1.018  4 0.583   55 1.153 0.919  22 0.583   9 1.123 1.192  15 0.583        Figure 1. Sociogram of chat-based user interaction generated   using algorithm in Table 1. Thickness of lines indicated tie  strength and arrows indicate direction of flow.   137    over time (if longitudinal data is used). The individualized outputs  are not included in this paper due to space limitations.       Figure 2. Multi-dimensional plot of word clusters.   5. CONCLUSION This paper presented two approaches for the analysis of learning  communities using IRC, and learning interaction in IRC. A social  network approach for structural analysis is paired with a semantic  network approach for content analysis. An algorithm was  introduced for the formation of network matrices from IRC  interaction. Similar versions of the social and semantic  approaches discussed above have been introduced separately in  earlier papers [8, 9], but the algorithm used for the network  analysis is introduced here for the first time, as well as  implications of combining the two procedures.  One of the shortcomings of using the proposed algorithm to create  social network ties in IRC from a temporal approach is that ties  are an abstraction from chat interaction, rather than the traditional  bilateral connections between actors. Unfortunately, some online  learning environments offer little other evidence of social  connectivity, and the chat proximity analysis offers a window into  the social structure of chat interaction. One of the strengths of the  technique is that latent or informal networks can be discovered  from the interaction that may have otherwise been cloaked from  analysis. Evidence of social network ties in learning communities  can exist (such as direct messaging), but these connections  represent intentional ties where users are choosing to be  connected to each other. There is potential utility in uncovering  informal network connections that may represent bridging  between otherwise disconnected social groups, as well as pivotal  moments where an idea or discussion has migrated across  community boundaries. Communities can exist informally, and  future research should employ clustering analysis and clique  detection to enable automatic community detection.   6. ACKNOWLEDGMENTS This work was supported by NSF Award #0943147. The views  expressed herein do not necessarily represent the views of NSF.   7. REFERENCES [1] M. Schlager, J. Fusco and P. Schank. 2002. Evolution of an   Online Education Community of Practice, in K. Renninger   and W. Shumar, eds., Cambridge University Press, Building  Virtual Communities, pp. 129-158.   [2] Rogers, E. M. and Kincaid, L. D. 1981. Communication  networks: Toward a new paradigm for research. New York:  Free Press.   [3] Herring, S. C. 1999. Interactional coherence in CMC.  Journal of Computer Mediated Communication, 4(4).   [4] Smith, M., Farnham, S., and Drucker, S. 2000. The social  life of small graphical chat spaces. In Proceedings of the  2000 ACM SIG CHI Conference. New York: ACM.   [5] Krikorian, D., Lee, J., Chock, T. M., and Harms, C. 2000.  Isn't that spatial: Distance and communication in a 2-D  virtual environment. Journal of Computer Mediated  Communication, 5(4).   [6] Sack, W. 2000. Conversation map: An interface for very  large scale conversations. Journal of Management  Information Systems, 17(3), 73-92.   [7] Suthers, D. D., Dwyer, N., Medina, R., and Vatrapu, R. A.  2010. framework for conceptualizing, representing, and  analyzing distributed interaction. International Journal of  Computer Supported Collaborative Learning, 5(1), 5-42.   [8] Rosen, D., Woelfel, J., Krikorian, D., and Barnett, G. A.  2003. Procedures for analyses of online communities.  Journal of Computer Mediated Communication, 8, 4.   [9] Rosen, D, and Corbit, M. 2009. Social network analysis in  virtual environments. In Proceedings of ACMs Hypertext  Conference. New York: ACM.   [10] Turner, T., Smith, M., Fisher, D., and Welser, H. 2006.  Picturing Usenet: Mapping computer-mediated collective  action. Journal of Computer-Mediated Communication,10, 4.   [11] Preece, J., and Maloney-Krichmar, D. 2005. Online  communities: Design, theory, and practice. Journal of  Computer-Mediated Communication, 10(4), article 1.   [12] Brass, D. J. and Burkhardt, M. E. Centrality and power in  organizations.  In N. Nohria & R. G. Eccles (Eds.), Networks  and organizations: Structure, form, and action (pp. 191-215) .  Boston: Harvard Business School Press (1992)   [13] Marin, A., and Wellman, B. 2010. Social Network Analysis:  An Introduction. In P. Carrington & J. Scott (Eds.),  Handbook of Social Network Analysis. London: Sage.   [14] Freeman, L. C. 1979. Centrality in social networks:  Conceptual clarification. Social Networks, 2, 215  239.   [15] Rice, R. E. 2005. New media/Internet research topics of the  Association of Internet Researchers. The Information  Society, 21, 285-299.   [16] Matsuzawa, Y., Oshima, J., Oshima, R., Nihara, Y., and  Sakai, S. 2009. KBDeX: A Platform for Exploring Discourse  in Collaborative Learning. In Procedia - Social and  Behavioral Sciences, Volume 2, Issue 4, The 1st  Collaborative Innovation Networks Conference (K. Riopelle,  P. Gloor, C. Miller and J. Gluesing, Eds.).   [17] Woelfel, J., and Woelfel, J. 1997a. Catpac version 2.0,  Galileo Corporation.   [18] Woelfel, J. Artificial neural networks in policy research: A  current assessment. Journal of Communication, 43(1), 63-80  (1993)   138    [19] Woelfel, J., and Woelfel, J. 1997b. ThoughtView version  2.0, Galileo Corporation.   [20] Doerfel, M. L., and Barnett, G. A. 1996. The use of  CATPAC for textual analysis. Cultural Anthropology  Methods, 8, 4-7.   [21] Wasserman, S. and K. Faust. 1994. Social Network Analysis:  Methods and Applications, New York: Cambridge  University Press.    139    "}
{"index":{"_id":"15"}}
{"datatype":"inproceedings","key":"Palavitsinis:2011:AAL:2090116.2090138","author":"Palavitsinis, Nikos and Protonotarios, Vassilios and Manouselis, Nikos","title":"Applying Analytics for a Learning Portal: The Organic.Edunet Case Study","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"140--146","numpages":"7","url":"http://doi.acm.org/10.1145/2090116.2090138","doi":"10.1145/2090116.2090138","acmid":"2090138","publisher":"ACM","address":"New York, NY, USA","keywords":"agriculture, learning repositories, log analysis, web portal","Abstract":"This paper discusses empirical findings demonstrating cultural influences in social behavior, communication, cognition, technology enhanced learning and draws implications for learning analytics.","pdf":"Applying Analytics for a Learning Portal:  the Organic.Edunet Case Study  Nikos Palavitsinis, Vassilios Protonotarios, Nikos Manouselis  Greek Research and Technology Network (GRNET),  56 Messogeion Av., 115 27, Athens, Greece  palavitsinis@grnet.gr; vprot@aua.gr; nikosm@ieee.org   ABSTRACT Learning portals are education-oriented Web portals, which  provide access to a variety of educational material, usually  coming from various sources.  In order to explore how they can  support their users during an educational activity (e.g. preparation  of teaching a course), it would be interesting to study the behavior  of their visitors, focusing on the particular context in which  specific actions are taking place. For example, user activities may  be analyzed during specific learning events, when activities are  more focused. This paper discusses the case study of the  Organic.Edunet Web portal (www.organic-edunet.eu), a learning  portal for organic agriculture educators that provides access to  more than 10,500 learning resources from a federation of 11  institutional repositories. The portal mostly focuses on serving  school teachers and university tutors and has attracted until today  almost 42.200 unique visitors from more than 160 countries, out  of which about 2.600 have registered to the portal. An effort is  made to study the users behavior, focusing in tutors and  educators in both schools and universities, in relation to specific  training events in which we know that they have been involved.  Therefore, we analyze logs of user activities that took place on  specific dates and geographical locations, in order to potentially  identify notable changes in their normal visiting behavior.   Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Education  General Terms Design, Theory.   Keywords Web portal, log analysis, learning repositories, agriculture     1. INTRODUCTION Initially the term web portal was used to refer to well-known  Internet search and navigation sites that provided a starting point  for users to explore and access information on the World Wide   Web [17]. The term Internet portal or Web portal began to be  used to describe mega-sites (such as Yahoo!, Excite, AOL, MSN,  Netscape Netcenter, and others) that many Web visitors used as a  starting point for their web surfing. Since that time, Web portal  have significantly expanded and matured, and a diverse range of  portal types have been developed and used in different contexts  [10]. Nowadays, Web portals are generally defined as gateways to  information and services from multiple sources [14]. One  important component is the organization, navigation, labeling and  indexing of their content in order to facilitate searching of  information and services, so that users can search, identify and  access the most appropriate resources for their needs.     Learning portals, Web portals that offer learners or educators  with a large selection of learning resources, are essential to the  further integration of information technologies and learning [7].  The purpose of a learning portal is not simply indexing and  delivery but to facilitate actual reuse and sharing [5]. Thus, the  expected usage of services and resources found in learning portals  could be considered as different compared to the usage of other  types of portals (such as entertainment, information or  commercial ones). In order to explore more about the way that  users of learning portals interact with the portal services and the  indexed content, it is often useful to engage analytics - usually by  studying the log files of the portal. Such a log analysis can take  place in a systematic, repeatable but also practical way, allowing  portal owners to explore the actual usage of their learning portal  and to identify potentially interesting patterns of use [9][11].    In this paper, we examine the case of a Web portal that supports  users in finding digital learning resources to support and enrich  their teaching activities. It is the case of the Organic.Edunet portal  (www.organic-edunet.eu) that aggregates resources on organic  agriculture and agro-ecology, allowing educators to find and  retrieve this content from a single point of access. We particularly  examine how the portal has been used before, during and after the  organization of training events for potential users (i.e. educators)  in several European countries. This analysis focuses both on the  general usage statistics of the portal, but it also looks at the user  level, trying to identify changes on the typical user profile of the  portal due to the training events that were organized.  Additionally, a comparison is attempted, looking at the same  statistics over an extended period of 7 months, when the portal  was supported by community events organized such as the Open  Days and the statistics for the next 7 months, when the project  was completed and the portal was functioning independent of  project related events.   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.   140    2. BACKGROUND The Organic.Edunet Web portal aims to facilitate access, usage  and exploitation of digital educational content related to Organic  Agriculture (OA) and Agroecology (AE). Organic.Edunet aims to  support stakeholders producing content about OA & AE in order  to publish it in an online federation of learning repositories and  describe it according to multilingual, standard-complying  metadata. The portal front-end features a multilingual user  interface translated into fourteen (14) languages, providing access  to almost 11.000 learning resources from a federation of eleven  (11) institutional repositories. The targeted audiences of  Organic.Edunet portal are mainly educators: school teachers  looking for resources to help them prepare their relevant teaching  activities (e.g. how to set up a school garden to support hands-on  environmental education); and academics (professors, teaching  staff or researchers) in agriculture and life science topics, looking  for resources to help them with their teaching, learning and  research activities (e.g. writing a case study report on a particular  crop or farm type). Potential users also include the stakeholders  producing content, such as agricultural libraries and academic  publishers. The portal has attracted, until the end of June 2011,  54.300 visits and 209.700 page views from 42.200 unique visitors  from approximately 170 countries, out of which more than 2.600   have registered to the portal. These numbers assure that the portal  has been used from a wide audience, thus allowing an analysis of  the users behavior to reach safe conclusions as far as the portal  usage is concerned.  In order to introduce the potential users to the portal, a series of  Open Days have been organized in various countries, featuring  validation events where educators had the chance to work with the  Organic.Edunet portal. These events were organized based on  specific guidelines and also provided clear instructions to all the  participants (http://virtuelleschule.bmukk.gv.at/projekte- international/eu-projekte/organicedunet/open-days/). From its  launch, the Organic.Edunet Portal has been linked to a Google  Analytics (http://www.google.com/analytics) account in order to  track the visits and be able to document its usage. Another  envisaged use for this account was to try and evaluate the portal  through analyzing focused statistics having to do with a range of  parameters, like the day or time of visit, the geographic areas of  the visitors, but also the origins of their visits. The visitors log  files have been collected and analyzed using the Google Analytics  tool in order for observations to be made in relation to the way the  behavior of users was affected by the organization of such  training events.   Table 1. Overview of the Open Days organized in Schools & Universities   No Location Date Audience No of Participants   1 Tartu, Estonia 3-5/2/10 Academics / Researchers 12  2 Thessaloniki, Greece 23/3/10 Academics / Researchers 10  3 Vienna, Austria 23/3/10 School Teachers 15  4 Vienna, Austria 6/5/10 School Teachers 16  5 Budapest, Hungary 28/5/10 School Teachers 8  6 Thessaloniki, Greece 25/5/10 Academics / Researchers 10  7 Budapest, Hungary 31/5/10 Academics / Researchers 18  8 Tallin, Estonia 17/8/10 School Teachers 10  9 Harjumaa, Estonia 18-19/8/10 School Teachers 12  10 Essen, Germany 30/8/10 Academics / Researchers 8  11 Dresden, Germany 11/9/10 School Teachers 14  12 Budapest, Hungary 14-15/9/10 Academics / Researchers 12  13 Vienna, Austria 22/9/10 School Teachers 15     TOTAL 160  3. METHODOLOGY A brief outlook on existing literature on evaluating learning  portals has showed that there are different ways in which a portal  can be evaluated. One option would be to provide users with  questionnaires or web-based evaluation tools and analyze the  feedback provided [12][16]. This evaluation has already been  carried out in Organic.Edunet with an online questionnaire  (http://www.ieru.org/organicsurvey/). A second option would be  to apply an analytics approach, by analyzing the log files of the  users in order to study their visiting behavior [2][3][6]. Hybrid  methods using both approaches can also been found in the  literature [13].   3.1 Context of Study  The analysis carried out in the context of this paper, particularly  focuses on studying how the usage of the Organic.Edunet portal  was affected by the organization of the focused training events  (i.e. Open Days). Overall, thirteen (13) of the Open Days that  have been organized in five (5) different countries are examined.  These Open Days had 160 participants comprising mainly from  school teachers and academic staff. Brief information on these  Open Days is presented in Table 1:   141    3.2 Steps Taken and Data Used  In order to study the portal statistics before, during and after the  Open Days, some variables have been defined and some  assumptions were made. More specifically, when looking at the  data on Table 1, its apparent that the Open Days took place on  two distinct periods and not throughout the whole period of  reference. The first group of Open Days generally took place  during March & May 2010 (the fact that one Open Day took place  during early February it is not considered to affect results so  much) whereas the second group of Open Days took place on  August-September 2010. Our analysis focuses on the period  before the two groups of Open Days (Pre-OD), the actual Open  Days (OD) and finally on the period after the Open Days (Post- OD), more specifically:   a) The time before the first group of Open Days is  symbolized as Pre-OD, whereas the first period of  Open Days is symbolized in all graphs as OD1. This  time following the first group of Open Days is  symbolized as Post-OD1. Respectively, the second  Open Days period is called OD2 and the period  following the second group of events took place is  symbolized as Post-OD2,   b) The portal-related variables selected to be examined  were: Visits for the portal, Page Views, Unique Visitors,  Traffic Sources and Most Popular Pages. These  statistics are calculated for all the periods defined in (a),   c) The user profile that is also examined in this study, is  comprised by a set of statistics that try to focus on the  user-level behavior that can possibly lead to new  insights on the use of a thematic portal with a focused  community around it. More specifically, the variables  measured here include: Average Time on Portal,  Average Time on Page, Pages Accessed per Visit,  Visitor Loyalty and Depth of Visit   Definitions Deep visitor: Deep visitor would be the visitor that in one visit  would open more pages than the average pages per visit of all the  visitors put together for the same period of time.  Statistics for this metric were calculated using the Depth of  Visit available in Google Analytics, showing the percentage of  users that visited one page per visit, two pages per visit, three  pages, etc. up to the class of 51-100 pages. Bounces: Any visit during which the user views only one page and  then exits the portal, is considered as a bounce  Visitor Loyalty: Loyalty of each visitor shows the times per time  period (i.e. day, week, and month) that a user visits a website.  Data post-processing and analysis was carried out using Microsoft  Excel where exported CSV files from Google Analytics where  processed. The results are presented in the form of tables and  figures that focus on the variables that are examined. Column  charts were chosen to present focused quantitative data whereas  tables were mainly used to provide overviews of the data per  region. The main research question explored in this initial analysis is:   How do the usage statistics both on the portal level but also on  the individual user level, change, after a series of training events  on the portal is organized E.g. do educators start using more the  Organic.Edunet Web portal How is that reflected on the user- level statistics   4. INITIAL RESULTS   4.1 Portal-level Statistics  In this section, we present the statistics related to the portal in  general, trying to identify the way in which they change, if so,  after the organization of an Open Day.   Table 4, shows the most famous pages of the portal measuring the  visits per day that each page attracted. As it was expected, most of  the visits to the portal originate from the homepage, so this is the  highest ranking page of all. Simple search (text-based search) is  also a very popular destination within the portal, but in this case,  the visitation rates fluctuate less than the ones for the homepage.  Despite that, comparing all the search functionalities offered, text- based search ranks on the top. Help on how someone can use  these functionalities is also visited pretty often, with high  numbers during the Open Day periods, also. Browsing through  the resources based on specific criteria (educational level,  language, difficulty, etc.) is also a widely used search method in  the portal which follows a steady but decreasing usage. Browsing  is followed by the Semantic Search which involves searching  through the terms of a domain specific ontology deployed as a  tree of terms and concepts.     Table 2. Overview of portal statistics before, during & after the Open Day periods defined  Visits per   day  Page Views   per day  Visitors per day Direct  Search  Engines Refer.  Bounces  per day   PRE-OD 14,5 121,6 6,9 65% 21% 14% 5,3  OD1 33,4 194,1 19,7 50% 32% 18% 12,7  POST-OD1 32,6 159,9 19,4 43.7% 25.7% 30.6% 14,9  OD2 54,2 250,6 31,9 44.1% 26% 29.9% 23,1  POST-OD2 46,9 182 36,9 24.8% 16.5% 58.7% 26,4  142    Semantic searching seemed to gain some ground before and  during the second Open Day period, but again it fell to the levels  of visitation before the Open Days.   Looking at Table 2, it would have been expected to get higher  values for the periods of the Open Days in all metrics, when  compared to the respective Pre and Post Open Day periods.  So, for example, from 32.6 visits per day before the second Open  Day period, we moved to a 54.2 visits per day during the Open  Day period, which dropped to a 44.9 after the Open Days ended.  Its interesting to note that even though the numbers drop after the  Open Days, they do not decrease to the same extent, showing  some proof that the Open Day actually brought upon a change to  the portal usage. One exception to this rule is the case of Page  Views per day for the second period of Open Days.   Examining the sources of visit, one could argue that time given,  the users of the portal are using it more and more, and therefore it  could be safe to assume that they would also visit the portal  directly and not through search engines or other referencing  websites. This assumption is contradicted by findings, as direct  visits drop from a 65% before all Open Days, gradually to a  23.9% after their end. Visits generated by search engines do not  seem to follow and clear pattern (ranging from 18.5% to 32%)  whereas visits from referencing sites gradually increased from a  mere 14% to almost 60%! The explanation to this comes from a  factor outside the portal itself. The Organic.Edunet portal was  mostly referenced in websites of consortium members in the  beginning of the period examined but was also promoted through  a network of affiliated partners which was developed during the  course of the project, and mainly during the last quarter of 2010.  In total, 83 affiliated partners put links to Organic.Edunet on their  websites, which can explain the rise of visits coming from  referencing websites.  The interpretation of bounces is also interesting. Overall, bounces  seem not to be affected by the Open Days organized, in the sense  that they do not fluctuate at all before and after the Open Days,  but they show a steady increase throughout this period. Bounces  are of course included in portal visits and relating these two  numbers it is clear that the visits that did not bounce off the  portal, went into great depth. For example, for Post-OD2  period, only 18.2 visits per day did not bounce off the portal, but  these visits generated 127 page views per day, which are almost  seven pages per visit on an average.  To see if this effect lasted and to which extent, after the  completion of all Open Days, the period that followed was also  examined. As it is apparent in Table 3, most of the statistics  improved, along with a switch in the way that people find the  portal. More find it through search engines and less directly,  which makes sense, since during the Open Days people were   given the URL in advance. Bounces per day are alarmingly high  and its characteristic that they rise more than the respective visits  per day. Meaning that the portal attracts more visits but the  biggest part of them leaves the website after looking at one page.   Table 4, shows the most famous pages of the portal measuring the  visits per day that each page attracted. As it was expected, most of  the visits to the portal originate from the homepage, so this is the  highest ranking page of all. Simple search (text-based search) is  also a very popular destination within the portal, but in this case,  the visitation rates fluctuate less than the ones for the homepage.  Despite that, comparing all the search functionalities offered, text- based search ranks on the top. Help on how someone can use  these functionalities is also visited pretty often, with high  numbers during the Open Day periods, also. Browsing through  the resources based on specific criteria (educational level,  language, difficulty, etc.) is also a widely used search method in  the portal which follows a steady but decreasing usage. Browsing  is followed by the Semantic Search which involves searching  through the terms of a domain specific ontology deployed as a  tree of terms and concepts. Semantic searching seemed to gain  some ground before and during the second Open Day period, but  again it fell to the levels of visitation before the Open Days.    Educational Scenarios is a portal page that offers content  specifically designed for use in the classroom of either schools or  universities, through elaborated scenarios on various topics  related to Organic Agriculture and Agroecology. Overall, this  page shows that despite a promising start it represents the least  popular page, close to the tag-based search. Finally, tag-based  search remained largely unused throughout the period examined.   All these statistics, related to ways in which users search for  content can greatly influence the interface design of the portal.   Looking again at the page popularity for the two aforementioned  periods, it seems that text-based search is becoming more and  more used, whereas all the other pages visitation is declining up  to 53%. In addition, an interesting finding came through  examining the top-50 pages for the whole period (1/2/2010   30/6/2011). Amongst them, 11 specific resources (and not portal  pages per se) related to organic cultivation of vegetables were  found. More specifically, the resources accessed, contained  videos/documents, explaining the organic method to cultivate:  strawberries (1.704 visits), potatoes (1.373 visits), parsley (497  visits), dill (458 visits), gumbo (422 visits), spinach (357 visits),  chicory (307 visits), broccoli (303 visits), eggplant (295 visits),  peas (289 visits) and cauliflower (288 visits). All these pages  summed up to 6.293 visits, which if examined as one group of  similar pages, have 12.2 visits per day.    Table 3. Overview of portal statistics during the whole Open Day period, compared to the 8 month period following the Open Days Visits per   day  Page Views   per day  Visitors per   day Direct Search  Engines Refer.  Bounces  per day   Open Days Period (1/2/2010-30/10/10) 68.4 380.3 42.5 44.3% 31.2% 24.5% 30.8  After Open Days (1/11/10-30/6/11) 144.9 422.7 126.5 12% 78.1% 9.9% 89.1  Difference +111% +11% +197% -73% +150% -60% +189%   143    Table 4. Most popular pages (visits per day) before, during & after the Open Day periods defined   Visits per day Pre-OD OD1 Post-OD1 OD2 Post-OD2  Homepage 102 138 118 136 81  Text-based Search 51 61 49 73 59  Educational Scenarios 15 5 7 5 6  Browse 12 14 7 9 5  Semantic Search 6 9 14 14 17  Tag search 4 4 2 2 2  Table 5. Most popular pages (same as Table 4) during the whole Open Day period, compared to the 8 month period following the Open  Days   Visits per day Open Days period After Open Days Difference   Homepage 117.2 83.5 -29%   Text-based Search 57.4 85.5 +49%   Educational Scenarios 10.2 6.3 -38%   Browse 9.8 4.9 -50%   Semantic Search 26.2 12.6 -52%   Tag search 3 1.4 -53%   4.2 User-level Statistics  In this section, we present the statistics related to the users in  specific, trying to identify the way in which their behavior  changes, if so, after the organization of an Open Day.   Looking at Table 6, it seems that the average time spent on the  Organic.Edunet portal by each user is dropping throughout the  period, starting from approximately six minutes, down to almost  three minutes. So, overall, more visitors came to the portal as it  was depicted in the portal level statistics, spending all and all, less   time on the portal as it is shown here. Someone may have  expected that this would also be reflected in the average time  spent on each page, this is not the case. Although this time is also  decreased, the decrease is both not that big and it also fluctuates  from 59 to 73 seconds. Pages viewed per visit are dropping  gradually, which was also reflected on the most popular pages.  Looking at the depth of visit, it seems that the depth in which the  visitors use the portal, is more or less steady.    Table 6. Overview of portal statistics before, during & after the Open Day periods defined   Average time on portal (sec)  Average time on  page (sec)  Pages per visit  (PpV)  Depth of Visit   (%>=PpV)  Loyalty  (%>=1 visit per   week)  PRE-OD 467 73 8.39 26.85% 5.02%  OD1 269 59 5.8 27.15% 14.97%  POST-OD1 229 63 4.9 26.80% 14.63%  OD2 229 68 4.62 27.03% 21.51%  POST-OD2 188 61 3.42 26.13% 6.44%  User loyalty was measured as the percentage of visitors that came  back to the portal one or more times during a week. This limit was  defined in the context of this study to measure the loyalty of the  users of educational portals, as no such information could be  retrieved in other studies. Starting from the pre-Open Day period,  only about 5% of the users would return to the portal more than  once a week. As expected, during the first period of Open Days,  this number of loyal visitors tripled to 15% which more or less  was sustained during the period that followed the first set of Open  Days. As expected, the second set of Open Days enhanced visitor  loyalty, with one fifth of the visitors coming back to the portal at   least once a week. As with most of the stats that were analyzed so  far, user loyalty seems to have been helped by the Open Days,  increasing during them and returning to a lower level after them,  which is marginally above the initial loyalty, that was measured  before any Open Day took place.  Examining differences in the aforementioned statistics on the long  run, most of them are declining significantly. Overall, average  time spent both on a portal level and on the page level, decline as  pages per visit and loyalty do. Only the depth of visit has risen,  but when looked at in comparison to pages per visit, it seems that  this increase is not significant as only 16.5% of visitors visit more   144    than 3 pages, whereas before 13.3% visited more than 5.6 pages  per visit.   Table 7. Overview of portal statistics during the whole Open Day period, compared to the 8 month period following the Open Days  Average time on   portal (sec)  Average time on   page (sec)  Pages per  visit  (PpV)  Depth of Visit    (%>=PpV)  Loyalty  (%>=1 visit per   week)  Open Days Period  (1/2/2010-30/10/10) 326 58 5.6 13.3% 5.8%  After Open Days  (1/11/10-30/6/11) 115 40 2.9 16.5% 1.7%  Difference -65% -31% -48% +24% -71%   5. CONCLUSION  This paper presented an initial analysis of the log files of the  Organic.Edunet portal, a Web portal that supports educators in  finding digital learning resources to support and enrich their  teaching activities. It particularly examined how the portal has  been used before, during and after the organization of focused  training events for potential users in several European countries.  It tried to study any notable changes in both the portal statistics  overall and also study any changes in the behavior of the users,  before, during and after the training events, both on a small scale  (periods of approx. 70 days) but also on a larger scale, examining  two 7-month periods. The present paper reached the following conclusions:    - The portal statistics retained some of the dynamics that  were created during the Open Days, two months after  their completion. On the long term, visits were  increased but most of the statistics declined for the  period following the end of the project,  - Bounces continued to rise throughout the period  showing no direct relation to the Open Days organized.  In general, looking at the greater period of almost 1 year  and a half, bounces seem to rise hand in hand with the  visits attracted to the portal,   - It seems that traditional search functionalities worked  better for the users that largely did not use tag-based  search. Semantic search and browsing per topic were  used, but still not as much as expected,   - The typical user before the Open Days would not return  to the portal more than once a week and would spend  more than six minutes in the portal in total in each visit.  This user would occasionally visit more pages than the  portals average, taking a deep visit. After the end of  the Open Days, almost 7 months after the last Open  Day, the same user rarely returns to the portal more than  once a week and spends significantly less time on it,   - Many project-related, outside of the portal scope,  parameters affected the statistics on the portal (i.e.  affiliation strategy of the project, press releases  circulated) by drawing visits from a wide scope of  people that will influence the usage of the portal   As far as the analysis of the Open Days in groups is concerned,  one can argue that the results must be compared before and after  each Open Day, but this was not chosen for two reasons. First of  all, a preliminary analysis showed that the results were more or  less as expected showing no research interest when analyzing  Open Days per country; i.e. traffic is heavier during Open Days,  with more visits or that time spent on the portal increases due to  the structured exercises given during the Open Days. The second  reason had to do with selecting a bigger sample of users by  grouping the Open Days together, from which safer conclusions  could be drawn, whereas the limited sample of 15 participants per  Open Day could not provide that.   Another issue has to do with the fact that there is no standard  threshold that can characterize a user as loyal or otherwise,  because this number is more or less affected by the nature of the  website examined, the services it offers, etc. In a recent survey by  Chitika Inc.1, related to Facebook and other popular websites, a  loyal visitor was the one that came back to the website four or  more times per week. In the case of an Educational Portal, this  number might be too high of an expectation, so for the purpose of  this study, it was decided to set forth our own threshold in  characterizing someone as loyal.   Future work should focus on delving deeper, trying to elaborate  the generic research question into a set of research hypotheses  that will be statistically explored. Through such an analysis, it can  be decided if a measurable and confident change in the users  behavior is observed. A more detailed analysis can also take place  focusing on data such as the sections of the portals that are used  more during and after the training events (i.e. do the users spend  more time on new features/services that fit their needs). Finally,  we are interested to view results in the light of the interaction of  different audiences with the portal (e.g. academics/researchers,                                                                     1 http://insights.chitika.com/2009/digg-facebook-loyal-readers/   145    teachers, public, other). This could also indicate if separate portal  interfaces are needed to better serve each community.     6. ACKNOWLEDGMENTS The work presented in this paper has been funded with support by  the European Commission, and more specifically the project  250525 Virtual Open Access Agriculture & Aquaculture  Repository: Sharing Scientific and Scholarly Research related to  Agriculture, Food, and Environment of the ICT Policy Support  Programme (ICT PSP), Theme 4 - Open access to scientific  information.  7. REFERENCES [1] Belcher, M., Place, E. & Conole, G. (2000). Quality   assurance in subject gateways: creating high quality portals  on the Internet. Quality Assurance in Education, 8 (1), 38- 47  [2] Buckley, B.C., Gobert, J.D. and Horwitz P. (2006). Using  log files to track students' model-based inquiry. In  Proceedings of the 7th international conference on Learning  sciences (ICLS '06). International Society of the Learning  Sciences 57-63.   [3] Carr, L., Brody, T. and Swan, A. (2008) Repository  Statistics: What Do We Want to Know In: Third  International Conference on Open Repositories 2008, 1-4  April 2008, Southampton, United Kingdom   [4] Dub, L., Bourhis, A., & Jacob, R. (2003). Towards a  typology of virtual communities of practice, Retrieved on  November 2nd, 2010 from:  http://citeseerx.ist.psu.edu/viewdoc/downloaddoi=10.1.1.98 .2079&rep=rep1&type=pdf    [5] Duncan, C. (2002). Digital Repositories: the back-office of  e-Learning or all e-Learning, in Proc. of ALT-C 2002, Sunderland, 9-11 September.   [6] Hasan, L., Morris, A. and Probets, S., (2009). Using Google  Analytics to Evaluate the Usability of E-Commerce Sites.  Human Centered Design. In Proceedings of the 1st International Conference, HCD 2009, Held as Part of HCI  International 2009, San Diego, CA, USA, July 19-24, 2009   [7] Holden, C. (2003). From Local Challenges to a Global  Community: Learning Repositories and the Global Learning  Repositories Summit. Version 1.0, Academic ADL Co-Lab,  November 11.   [8] Katz, R. N. (2002). Web Portals and Higher Education:  Technologies to Make IT Personal. San Francisco, CA: John  Wiley & Sons, Retrieved on November 1st, 2010 from:  http://www.iskme.org/knowledge- bank/copy_of_articles/information-and-knowledge- management/pub5006d.pdf  [9] Piearrakos, D. Paliouras, G. Papatheodorou, C. and  Spyropoulos, C. (2003). Web Usage Mining as a Tool for  Personalisation: A Survey. User Modeling and User Adapted  Interaction. Vol. 13, 2003, p.p. 311-372.   [10] Portals Community Fundamentals (2001). Portal Definition  and Types of Portals. PortalsCommunity. Retrieved August  23, 2001,  http://www.portalscommunity.com/library/fundamentals.cfm .  [11] Santos Machado, C. and Becker, K. Distance Education: a  Web Usage Mining Study for the Evaluation of Learning  Site, In Proc. of the 3rd IEEE Int. Conference on Advanced  Learning Technologies (ICALT03), Athens, Greece. June  2003.  [12] Silius, K. & Tervakari, A-M. (2003). The usefulness of  web-based learning environments. The Evaluation Tool into  the Portal of Finnish Virtual University. International Conference on Network Universities and e-learning. 8-9  May 2003. Valencia. Spain.   [13] Stacey, E., & Rice, M. (2002). Evaluating an online learning  environment. Australian Journal of Educational Technology, 18(3), 323340.   [14] Tatnall, A.(2005). Web portals  the New Gateways to  Internet Information and Services. Hershey, PA: Idea Group  Publishing.  [15] Van Baalen, P., Bloemhof-Ruwaard, J. & Van Heck, E.  (2005). Knowledge sharing in an emerging network of  practice: The role of a knowledge portal. European Management Journal, 23(3), 300-314   [16] Van der Heijden, H. (2002). Factors influencing the usage of  websites: The case of a generic portal in The Netherlands.  Information Management 40(6) 541549.   [17] Winkler, R. (2001). Portals  The All-In-One Web  Supersites: Features, Functions, Definition, Taxonomy. SAP  Design Guild, Edition 3. Retrieved August 23, 2001,  http://www.sapdesignguild.org/editions/edition3/overview_ edition3.asp.  146    "}
{"index":{"_id":"16"}}
{"datatype":"inproceedings","key":"Teplovs:2011:GPM:2090116.2090139","author":"Teplovs, Chris and Fujita, Nobuko and Vatrapu, Ravi","title":"Generating Predictive Models of Learner Community Dynamics","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"147--152","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090139","doi":"10.1145/2090116.2090139","acmid":"2090139","publisher":"ACM","address":"New York, NY, USA","keywords":"analysis, game theory, information visualization, latent semantic analysis, learner models, social network analysis","Abstract":"Multi-user virtual environments (MUVEs) allow many users to explore the environment and interact with other users as they learn new content and share their knowledge with others. The semi-synchronous communicative interaction within these learning environments is typically text-based Internet relay chat (IRC). IRC data is stored in the form of chatlogs and can generate a large volume of data, posing a difficulty for researchers looking to evaluate learning in the interaction by analyzing and interpreting the patterns of communication structure and related content. This paper describes procedures for the measurement and visualization of chat-based communicative interaction in MUVEs. Methods are offered for structural analysis via social networks, and content analysis via semantic networks. Measuring and visualizing social and semantic networks allows for a window into the structure of learning communities, and also provides for a large cache of analytics to explore individual learning outcomes and group interaction in any virtual interaction. A case study on a learning based MUVE, SRI's Tapped-In community, is used to elaborate analytic methods.","pdf":"Generating Predictive Models of Learner Community Dynamics   Chris Teplovs1,2, Nobuko Fujita1,2 and Ravi Vatrapu2 1 University of Toronto 2Copenhagen Business School   chris.teplovs@gmail.com, nobuko.fujita@utoronto.ca, vatrapu@cbs.dk  ABSTRACT In this paper we present a framework for learner modelling that  combines latent semantic analysis and social network analysis of  online discourse.  The framework is supported by newly  developed software, known as the Knowledge, Interaction, and  Social Student Modelling Explorer (KISSME), that employs  highly interactive visualizations of content-aware interactions  among learners.  Our goal is to develop, use and refine KISSME  to generate and test predictive models of learner interactions to  optimise learning.   Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Education  General Terms Design, Theory, Analysis.   Keywords Information visualization, latent semantic analysis, social network  analysis, learner models, game theory.   1. INTRODUCTION The nascent field of Learning Analytics focuses on  the  measurement, collection, analysis and reporting of data about  learners and their contexts, for purposes of understanding and  optimising learning and the environments in which it occurs 1. One approach to learning analytics is social network analysis,  which examines the patterns of interaction among learners.   Social network analysis of, in particular, e-learning is facilitated  by the availability of digital data that are amenable to such  analysis.  Considerably less attention has been paid to the content  of the artifacts around which the learners are interacting.  Content  analysis is time-consuming, pain-staking, and detailed work.  Without content analysis, however, claims about the nature of the  dynamics among learners are left wanting.  Understanding  learning, it seems, requires digging deeply into the data that are  available.  In this paper we introduce a framework that interweaves social  network analysis, semi-automated content analysis, information  visualization, and applied economic theory to help us understand  and optimise learning.  We are interested in investigating research    questions such as:  Can we predict when particular interactions  will result in learning What are some characteristics of  interactions of effective learning   This paper begins with a brief introduction and survey of relevant  literature using social network and latent semantic network  analysis (LSA) to analyze online discourse. Next, a description of  the prototypic software environment (the Knowledge Space  Visualizer or KSV) on which the new software (the Knowledge,  Interaction and Semantic Student Model Explorer, or KISSME) is  being developed is presented. The use of LSA in the generation of  student models suitable for studies of collaborative learning is  then proposed. Finally, we present a theoretical framework for  understanding the dynamics of collaborative learning in terms of  examining the outcomes of social and semantic interactions  among participants.   2. BACKGROUND Wasserman1and Faust [1] describe social network analysis (SNA)  as a methodology that focuses on relationships and patterns of  relationships.  As such it requires a set of methods and analytic  concepts that are distinct from the methods of traditional statistics  and data analysis (p. 3).  They cast SNA in the broader list of  topics that have been studied using network analytic methods,  including community [2], group problem solving [3-5], diffusion  and adoption of innovations [6-8], and cognition [9, 10].  No  matter what the objective of the study, though, network analysis  focuses on the relations between units.   Studies have explored the application of SNA to explore learning  and knowledge construction in Networked Learning/Computer- Supported Collaborative Learning (NL/CSCL) environments.  However, researchers have yet to achieve consensus on what  methods to use. For example, de Laat, Lally, and Lipponen, [11]  used content analysis, critical event recall and SNA to study  interaction patterns.  They suggest that SNA can be used to  complement content analysis [12, 13] to describe and understand  patterns of interaction in NL/CSCL. Of the various network  metrics that are available (see [1]), these researchers focus on  density and centrality. In contrast, Reffay and Chanier [14]  applied SNA to determine the cohesion of groups engaged in  CSCL. They argue that embedding tools that perform such  analyses in the design of the learning environment itself may be  more effective than time-consuming content analysis to support  teaching and learning.  The importance of time-based analyses has also been noted  [15][16]  The study by de Laat et al [11] was the first application  of using SNA to illustrate how patterns change over time and the   relationship of those patterns to teaching and learning.  An  important generalization from the literature is that the essential                                                                     1 https://tekri.athabascau.ca/analytics/call-papers   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.   147    features to conduct SNA are two or more units, usually learners  and the elucidation of the relationship between them. But there is  another equally important type of network analysis to be  considered in learning analytics and knowledge work: the  network of ideas. Ideas, unfortunately, are difficult to delineate.   2.1 Latent Semantic Analysis  Latent semantic analysis (LSA) represents both a statistical  technique and a model of human knowledge acquisition.  Landauer and Dumais [17] propose LSA as a model that could  answer the question, how do individuals know so much given as  little information as they get  This problem is variously known  as Platos Problem, the Problem of Induction , the poverty of  the stimulus , or the problem of the expert . (Platos solution  was that individuals possess innate knowledge and only need  some stimulation to reveal it.)   LSA provides a high-dimensional representation of the  associations between words and the documents containing those  words. The final output from LSA is a series of measures that  describe the relationships between units such as words,  documents, or words-and-documents. In LSA, each document or  word is represented by a vector in high-dimensional latent  semantic space.  The vector is calculated by examining patterns of  co-occurrence of words in a term-by-document matrix, which is  subsequently simplified using Singular Value Decomposition  (SVD). Thus, each document is represented by a vector of  numbers, typically numbering between 100 and 300 elements.   Whereas dimensions resulting from the application of SVD to  data can typically be interpreted (e.g. the dimensions from  Principal Components Analysis), the dimensions resulting from  LSA are not typically interpretable.  This limitation has made the  interpretability of LSA-based analyses difficult in the past.    Information visualization techniques seem to be a natural next  step in interpreting LSA, and can be used to create meaningful  representations of ongoing learning processes. Visualization of  LSA-derived similarities may be problematic, though, due to an  unacceptable reduction of dimensionality to two or three  dimensions suitable for visualization from that which is optimal  for LSA (typically around 300) [18].    3. SOFTWARE In this section we describe software designed to support the  visualization of learner models based on social and  semantic networks.  We present a description of the  Knowledge Space Visualizer (KSV), a prototypic software  system on which our new software, KISSME, is based.   3.1 The Knowledge Space Visualizer (KSV)  KISSME extends the Knowledge Space Visualizer, which  was developed by the first author for his doctoral  dissertation.  The KSV was designed to allow researchers  to use computer-assisted two-dimensional visualization of  learner-generated contributions to an online discourse  space.  In its simplest form this generates a graph in which  nodes are contributions and links are relationships between  those contributions such as  reply ,  reference  and   annotate  (see Figure 1).    These explicit relationships between contributions are  based on the behaviours of the contributors.  A learner, for   example, can intentionally choose to make a contribution  that is a reply to another learner's contribution.  In the  resulting graph the links are based on these behavioural  relationships.  Content is not considered.    In addition to the explicit linkages defined by behaviours  such as replying, referencing and annotating there exist  implicit linkages between contributions to the discourse  space.  These implicit linkages concentrate on the similarity  of the content of the contributions.  Whereas human raters  can evaluate the similarity between documents reliably and  with good validity, it is very tedious and time-consuming  work.  There are a variety of automated and semi- automated techniques that can be used to determine the  similarity of text-based contributions.  One powerful  technique is LSA, described above.    Figure 1. Structural relationships between contributions.   Blue lines indicate  build-on  or  reply-to  relationships.    Magenta lines indicate  reference  links   The preceding examples are based on the use of a force-directed  layout algorithm to position the nodes in to respect the strength of  the ties between them while minimizing the distortion of the  network of the relationships between the nodes.  Other types of  layouts are also possible.  For example, other researchers [19]  have highlighted the importance of chronology when studying the  dynamics of learning communities.  The KSV supports this sort of  inquiry by facilitating the positioning of notes chronologically.   More generally, the KSV supports the use of any categorical,  ordinal, or continuous variable from the data set to define either  of the axes for the display.  So in addition to the use of a  continuous chronological scale to define the horizontal axis,  authorship can be used to define the vertical axis.  An example of  the resulting learner-time display is shown in Figure 2.     Once contributions are positioned on whatever set of  operationally defined axes the analyst has chosen, links between  nodes can be overlaid without affecting the positioning of the  nodes.  For example, the behavioural links can be overlaid on the  learner-time display to show how patterns of interaction change  over time.  An example of this overlay is shown in Figure 3.     In a similar way, links between contributions based on latent  semantic analysis can be overlaid on the same learner-time  display to show the degree to which contributions are similar over  time and authorship.  More computationally intensive measures   148    can also be visualized.  For example, one can determine which  contributions were opened (and possibly read) by a learner within  some specified time interval before that contributor added a new  contribution to the discourse space.  An example of this sort of   recency influence  diagram is shown in Figure 4.   Figure 2. Chronological-authorial layout of contributions   Perhaps some of the most interesting diagrams that can be  produced using the KSV are based on the superposition of  different link types on the same layout.  For example, one can  overlay links of LSA-based semantic similarity atop those based  on  recency influence  to investigate the degree to which the  content of recently opened (read) contributions is reflected in new  contributions.  The KSV also allows the user to constrain the analysis by  specifying beginning and end dates for the analysis.  Rather than  specifying the dates a priori, the user can manipulate the  beginning and end dates with specially designed slider.  In  addition to being able to manipulate the beginning and end dates  independently of one another, the user can manipulate both dates  simultaneously, effectively providing time slices of the network  graph.  Figure 3. Chronological-authorial layout of contributions  overlaid with structural links   One of the key innovations of the KSV was the use of flexible  thresholds in the creation of network representations.  This is  what allowed us to create visualizations of LSA-based  representations of texts.  Rather than attempting to provide a two-  dimensional layout based on the first few dimensions resulting  from the matrix decomposition used in LSA, our approach has  been to determine the similarities between documents based on  the cosines between the vectors representing documents.  A graph  is then created in which the nodes correspond to the documents  and the edges correspond to the LSA-based similarities between  them.  A force-directed layout algorithm is then applied to the  graph such that the positions of nodes in the two-dimensional  representation minimize the distortion of the (very low  dimensional) representation.  This representation of a maximally  connected graph typically lacks clarity, and in typical cases where  there are tens or hundreds of nodes the graph is essentially  unintelligible due to the large number of edges.   Figure 4. Chronological-authorial layout with overlaid with  structural and recency links   This problem of overly connected graphs also presents a  conceptual problem: does it make sense to connect two document  nodes if their LSA-based similarity is very low  Other  researchers [20] have attempted to address the  threshold  problem  but heir research suggests that no typical value of cosine  threshold for determining document similarity exists.  Our  approach to tackle this problem is to provide the end user with  control over the choice of threshold to use.  We do so by  providing a slider control in the software that allows the user to  specify the cosine value below which edges are not drawn  between document nodes.  The dynamic nature of this control  allows the user, for example, to examine patterns of cluster  formation as the similarity threshold is varied.  This provides an example of how visual approaches to learning  analytics can provide solutions to previously intractable problems.   The answer to the question of  when are two documents (or ideas)  different  is typically  it depends on what you're looking for .   Given a collection of documents generated by students on, for  example, the physics of light.  At the most permissive level of  similarity threshold, all documents are related by virtue of being  in the same language.  This corresponds to a similarity threshold  of zero.  At a value slightly higher than zero, one could imagine  the documents cluster into two groups:  one about colours of light  and one about reflection.  As one raised the threshold higher yet  one could imagine the colours cluster fragmenting into smaller  clusters of related notes about topics such as rainbows,  wavelength, and so on.  The interactive nature of being able to  manipulate the threshold supports this broad range of possibilities   149    for determining the diversity of ideas that are present in discourse  space.   The Knowledge Space Visualizer, while providing powerful  visualizations of multi-dimensional networks, has several  limitations.  First, it relies on the end user having an functional  installation of a recent version of Java.  Recent advances in  browser-based technology -- specifically the widespread adoption  of HTML5 -- has enabled the production of highly interactive  browser-based visualizations.  Perhaps more significantly, the  KSV was limited by its focus on document-based networks.  The  KSV enables the visualization of relationships between  documents, based on both explicit and implicit linkages, but other  than examining patterns of authorship and co-authorship it was  not particularly good at generating visualizations of author-based  networks.  We are working on creating next-generation software  that will facilitate the examination of networks of authors. In its  earliest versions, the KSV was highly tuned to data from  Knowledge Forum.  The KSV was recently enhanced to allow the  importation of data from almost any data source that provided  indications of authorship, chronology and content.  The KSV was  released as open source code and is maintained on Google Code  at http://code.google.com/p/ksv.  3.2 Visualizing Student Models: The  Knowledge, Interaction and Semantic Student  Model Explorer (KISSME)  Recent work has led to the implementation of a learner model  based on interactions with other learners.  The functionality of the  KSV, in terms of being able to manipulate the threshold at which  two nodes are considered similar enough to be joined by visible  edges, was extended from document nodes to learner nodes.  Put  another way, a learner model based on social network analysis  was created in the KSV and the implementation of a flexible  threshold (based on the intensity of the interaction between any  two learners) allowed researchers to investigate patterns of  interaction.   The KSV allowed the analyst to exercise  considerable control over various parameters such as the intensity  of interaction necessary to establish a social link between  participants, as well as the date at which the social network was  analysed.  The ability of the analyst to vary these parameters  allowed the detection of patterns of interaction that were  previously obscured [21].  However, the network between authors  was based solely on their patterns of interaction.  No information  about the content of their contributions was used in the generation  of the graphs.   The ability to model students or other participants and then to  visualize those models in an interactive visualization environment  offers the potential to gain insights into the nature and outcomes  of interactions between learners.  In the work with the STEF lab  we constrained our analyses to focus on the social networks that  formed among learners.  While this approach revealed interesting  patterns of interaction, we felt the results were incomplete  because no attention was paid to the content of the learners'  contributions to the online discourse space.   Other researchers have conducted studies that meld automated  interaction analysis with manual content analysis [11, 16].   However, manual content analysis represents the rate-limiting  step in this sort of analysis.  Because manual content analysis  takes so long it is incommensurable with real-time analysis,   which is one of our goals.  Therefore, we are interested in using  some sort of automated or semi-automated content analysis.  For  reasons specified earlier we have chosen to use latent semantic  analysis to help us conduct automated content analysis.  For our  purposes, all that we are using LSA for is to generate  mathematical representations of the participants' contributions to  the discourse space.  We can then use those mathematical  representations in a variety of ways.  LSA uses a vector  representation of text.  One characteristic of these vectors is that  they are additive:  the vectors of two documents can be added  together to get the vector of the combined documents. We can  extend this property to generate latent semantic models of  participants by adding together the vector representations of all  their contributions to the discourse space.   This is not the first application of LSA to student modelling.   Other researchers [22-24] have used LSA in student modelling  but they have not focused on the collaborative nature of learning.   Still others have extended techniques from earlier research on  LSA to apply to e-learning contexts [25-27].  Zampa and  Lamaire's recent work [23] builds on the notion of matching  students to text based on the Vygotsky's Zone of Proximal  Development.  However, theirs is an individualistic model:  the  selection of  stimuli  is meant to effect individualized  optimization of learning.   Our approach is somewhat different:  we are interested in  combining information about patterns of interaction among  participants with information about the content of those  contributions.  We too take a Vygotskian approach:  that optimal  learning will take place when interactions occur between  individuals who are neither too similar nor too dissimilar from  each other, based on the semantics of what they have written.   This approach of combining social network analysis and latent  semantic network analysis is an example of the sort of  multi- dimensional  network championed by Noshir Contractor [28].   Our current work includes the implementation of software that  will allow us as researchers to examine the interplay of  interactions between learners and the latent semantic models of  those learners.  We are interested in testing the Vygotskian  hypothesis that uptake [29] is most likely to occur when the  semantic relatedness of the corresponding contributor models is  neither too high nor too low.  We are also interested in  simulations of learner interactions that take into consideration  both interactions and semantic relatedness.  This, we believe,  would allow us to generate models of community dynamics in  collaborative learning.  Once we have simulation data that  incorporates interaction and content we can make inferences  about the characteristics result in the success (broadly defined) of  some learning communities.   4. GAME THEORETICAL APPROACHES  TO UNDERSTANDING THE LEARNERS  GROUP DYNAMICS  Our approach to understanding community dynamics is based on  understanding the nature of the interaction between members of  that community.  We are examining a variety of theoretical  approaches but one that seems particularly promising is the  application of game theory [30] to interactions between users.   This approach requires us to consider the outcomes of interactions  between users in terms of  payoffs  to each player.  Of course,  different players can employ different strategies.  We consider   150    this to be part and parcel of learning:  our hypothesis is that as  learners gain expertise, they enhance their repertoire of learning  strategies, and through experience they learn when to employ  particular strategies.  5. SUMMARY We have proposed a framework that combines social network  analysis and latent semantic analysis of online discourse.  The  proposal is speculative:  previous work with latent semantic  analysis has yielded promising results that may help us  understand the nature of interactions among learners. Examining  those interactions using a framework such as game theory may  allow us to gain insight into the nature of community dynamics.   6. REFERENCES [1] Wasserman, S., Faust, K.: Social network analysis: Methods   and applications. Cambridge University Press, Cambridge,  UK (1997)   [2] Wellman, B.: The community question: The intimate  networks of East Yorkers. American Journal of Sociology  84, 1201-1231 (1979)   [3] Bavelas, A.: Communication patterns in task-oriented  groups. Journal of the Acoustical Society of America 22, 271-282 (1950)   [4] Bavelas, A., Barrett, D.: An experimental approach to  organizational communication. Personnel 27, 366-371 (1951)  [5] Leavitt, H.J.: Some effects of communication patterns on  group performance. Journal of Abnormal and Social  Psychology 46, 38-50 (1951)   [6] Coleman, J.S., Katz, E., Menzel, H.: The diffusion of an  innovation among physicians. Sociometry 20, 253-270 (1957)  [7] Coleman, J.S., Katz, E., Menzel, H.: Medical Innovation: A  diffusion study. Bobbs-Merrill, Indianapolis (1966)   [8] Rogers, E.M.: Network analysis of the diffusion of  innovations. In: Holland, P.W., Leinhardt, S. (eds.)  Perspectives on Social Network Research, pp. 137-164.  Academic Press, New York, NY (1979)   [9] Freeman, L.C., Romney, A.K., Freeman, S.C.: Cognitive  structure and informant accuracy. American Anthropologist  89, 310-325 (1987)   [10] Krackhardt, D.: Cognitive social structures. Social Networks  9, 109-134 (1987)   [11] de Laat, M., Lally, V., Lipponen, L., Simons, R.-J.:  Investigating patterns of interaction in networked learning  and computer-supported collaborative learning: A role for  Social Network Analysis. International Journal of Computer- Supported Collaborative Learning 2, 87-103 (2007)   [12] Henri, F.: Computer conferencing and content analysis. In:  Kaye, A.R. (ed.) Collaborative learning through computer  conferencing. Springer, London (1992)   [13] Hara, N., Bonk, C.J., Angeli, C.: Content analyses of on-line  discussion in an applied educational psychology course.  Instructional Science 28, 115-152 (2000)   [14] Reffay, C., Chanier, T.: How social network analysis can  help to measure cohesion in collaborative distance-learning.   In: Designing for change in networked learning. Proceedings of the international conference on Computer  Supported Collaborative Learning 2003., pp. 343-352.  Kluwer Academic Publishers,  (Year)   [15] Haythornthwaite, C.: Exploring multiplexity: social network  structure in a computer-supported distance learning class.  The Information Society 17, 211-226 (2001)   [16] Martnez, A., Dimitriadis, Y., Rubia, B., Gomez, E., de la  Fuente, P.: Combining qualitative evaluation and social  network analysis for the study of classroom social  interactions. Computers & Education 41, 353-368 (2003)   [17] Landauer, T.K., Dumais, S.T.: A solution to Plato's problem:  The latent semantic analysis theory of the acquisition,  induction, and representation of knowledge. Psychological  Review 104, 211-240 (1997)   [18] Landauer, T.K., Laham, D., Derr, M.: From paragraph to  graph: Latent semantic analysis for information  visualization. PNAS 101, 5214-5219 (2004)   [19] Reimann, P.: Time is precious: Variable- and event-centred  approaches to process analysis in CSCL research.  International Journal of Computer-Supported Collaborative  Learning 4, 239-257 (2009)   [20] Penumatsa, P., Ventura, M., Graesser, A.C., Louwerse,  M.M., Hu, X., Cai, Z., Franceschetti, D.R.: The right  threshold value: What is the right threshold of cosine  measure when using latent semantic analysis for evaluating  student answers International Journal on Artificial  Intelligence Tools 15, 767-778 (2006)   [21] Reffay, C., Teplovs, C., Blondel, F.-M.: Productive re-use of  CSCL data and analytic tools to provide a new perspective  on group cohesion.  CSCL2011, Hong Kong (submitted)   [22] Dessus, P., Mandin, S., Zampa, V.: What is teaching  Cognitive-based tutoring principles for the design of a  learning environment. In: Tazi, S., Zreik, K. (eds.) Common  innovation in e-learning, machine learning and humanoid  (ICHSL.6) pp. 49-55. Europa/IEEE, Paris (2008)   [23] Zampa, V., Lemaire, B.: Latent Semantic Analysis for User  Modeling. J. Intell. Inf. Syst. 18, 15-30 (2002)   [24] Dessus, P.: An Overview of LSA-Based Systems for  Supporting Learning and Teaching.  Proceeding of the 2009  conference on Artificial Intelligence in Education: Building  Learning Systems that Care: From Knowledge  Representation to Affective Modelling. IOS Press (2009)   [25] Kintsch, E., Caccmise, D., Franzke, M., Johnson, N.,  Dooley, S.: Summary Street: Computer-guided summary  writing. In: Landauer, T.K., McNamara, D.S., Dennis, S.,  Kintsch, W. (eds.) Handbook of Latent Semantic Analysis.  Lawrence Erlbaum Associates, Mahwah, NJ (2007)   [26] Rehder, B., Schreiner, M.E., Wolfe, M.B., Laham, D.,  Landauer, T.K., Kintsch, W.: Using latent semantic analysis  to assess knowledge: Some technical considerations.  Discourse Processes 25, 337-354 (1998)   [27] Wolfe, M.B., Schreiner, M.E., Rehder, B., Laham, D., Foltz,  P.W., Kintsch, W., Landauer, T.K.: Learning from text:  Matching readers and text by latent semantic analysis.  Discourse Processes 25, 309-336 (1998)   151    [28] Contractor, N.: The emergence of multidimensional  networks. Journal of Computer-Mediated Communication  14, 743-747 (2009)   [29] Suthers, D., Dwyer, N., Medina, R., Vatrapu, R.: A  framework for conceptualizing, representing, and analyzing  distributed interaction. International Journal of Computer- Supported Collaborative Learning 5, 5-42 (2010)   [30] Rasmusen, E.: Games and information: An introduction to  game theory. Blackwell, Malden, MA (2007)   152    "}
{"index":{"_id":"17"}}
{"datatype":"inproceedings","key":"Lockyer:2011:LDL:2090116.2090140","author":"Lockyer, Lori and Dawson, Shane","title":"Learning Designs and Learning Analytics","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"153--156","numpages":"4","url":"http://doi.acm.org/10.1145/2090116.2090140","doi":"10.1145/2090116.2090140","acmid":"2090140","publisher":"ACM","address":"New York, NY, USA","keywords":"learning analytics, learning design, pedagogical models, social network analysis, university teaching","Abstract":"Learning portals are education-oriented Web portals, which provide access to a variety of educational material, usually coming from various sources. In order to explore how they can support their users during an educational activity (e.g. preparation of teaching a course), it would be interesting to study the behavior of their visitors, focusing on the particular context in which specific actions are taking place. For example, user activities may be analyzed during specific learning events, when activities are more focused. This paper discusses the case study of the Organic.Edunet Web portal (www.organic-edunet.eu), a learning portal for organic agriculture educators that provides access to more than 10,500 learning resources from a federation of 11 institutional repositories. The portal mostly focuses on serving school teachers and university tutors and has attracted until today almost 42.200 unique visitors from more than 160 countries, out of which about 2.600 have registered to the portal. An effort is made to study the users' behavior, focusing in tutors and educators in both schools and universities, in relation to specific training events in which we know that they have been involved. Therefore, we analyze logs of user activities that took place on specific dates and geographical locations, in order to potentially identify notable changes in their normal visiting behavior.","pdf":"Learning Designs and Learning Analytics  Lori Lockyer   University of Wollongong  Faculty of Education   Wollongong, NSW Australia  +61242215511  llockyer@uow.edu.au  Shane Dawson  University of British Columbia   Faculty of Arts  Vancouver, BC, Canada   +1 604 8220978   shane.dawson@ubc.ca  ABSTRACT Government and institutionally-driven reforms focused on quality  teaching and learning in universities emphasize the importance of  developing replicable, scalable teaching approaches that can be  evaluated. In this context, learning design and learning analytics  are two fields of research that may help university teachers design  quality learning experiences for their students, evaluate how  students are learning within that intended learning context and  support personalized learning experiences for students.  Learning  Designs are ways of describing an educational experience such  that it can be applied across a range of disciplinary contexts.   Learning analytics offers new approaches to investigating the data  associated with a learners experience. This paper explores the  relationship between learning designs and learning analytics.   Categories and Subject Descriptors H.5 Information Interfaces and Presentation   General Terms Measurement, Design, Human Factors    Keywords learning design, pedagogical models, learning analytics, social  network analysis, university teaching.   1. INTRODUCTION Within the higher education sector internationally there is  mounting interest in developing replicable and scalable exemplars  and templates of effective teaching practice. This, in part, has  arisen from increased accountability and quality assurance  expectations in terms of the demonstration of teaching quality.  Bodies such as the Quality Assurance Agency for Higher  Education (QAA) in the UK and the Australian Universities  Quality Agency (AUQA) in Australia have been charged  assessing and reporting on the quality of the education and  student services that are provided by the higher education  institutions in their countries. Whether driven by national accountability expectations or  institutional initiatives to support quality, the field of learning  design has received much attention in the education sphere.  This   interest may be due to the perceived potential for learning designs  to address concerns about quality. Learning designs, (or as they  may be termed pedagogical models or pedagogical patterns) are  representations of teaching practice.  Learning designs articulate a  sequence of learning activities, what resources are used by  learners engaged in those activities, and how the teacher or others  participants in the experience supports the learning activities.  A  learning design can act as a model or template that teachers may  replicate across a range of educational contexts. In essence,  learning designs help frame the intent of and process for the  pedagogical experience. Learning design research and its up-take in educational practice  has come in many forms.  At its basis is the concept that it is  important to describe an educational experience in such a way  that it can be replicated then applied across a range of disciplinary  contexts.  Essentially, the focus is on the design and planning of  the educational experience.  In practice, learning designs are  predominately directed at the designer/teacher of the educational  experience.  Learning designs provide a way for a teacher to  document their pedagogical intent.  When shared amongst  teachers, learning designs provide stimulus or guidance for other  teachers to design quality educational experiences.   Learning analytics, on the other hand, are largely directed at the  student.  In essence learning analytics allow us to use the data  associated with a learners interactions to make pedagogically  informed decisions and evaluations. Learning analytics as an  emerging interdisciplinary field draws upon and integrates  research related to data mining, social networks, data  visualisation, machine learning, semantics, e-learning and  educational theory and practice.  Seemingly, learning analytics  takes up where learning design finishes in the educational  experience continuum  implementation and outcomes.  This paper explores the relationship between learning design and  learning analytic approaches.  The paper considers the context in  which learning design research emerged and highlights one strand  of its development in Australia as a way of consider how it may  be compatible with learning analytics.  It also considers work in  social network analysis as an example subset of learning analytics  to demonstrate how this field complements learning design.  2. BACKGROUND Although the terminology and the scope of research and  application of both learning design and learning analytics  approaches have evolved in recent years, their underlying basis is  not new. Essentially these research fields are concerned with  documenting and reporting on educational practice and  experience.  The interest in both learning designs and learning  analytics has gained momentum within the context of two  compatible worldwide trends within the higher education  environment  calls for improved teaching and increasing use of   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.   153    technology.  In the early 90s, commentators and policymakers  began to argue for increased expectations of the quality of the  teaching that was delivered in universities, and accountability for  that quality[1].  This was met, in many countries around the  world, with a wave of professional development programs aimed  to improve the quality of teaching and funding for innovation in  teaching (such as the Australian Learning and Teaching Council,  Higher Education Funding Council for England). Soon after this focus on quality teaching began, the increasingly  accessible and user-friendly Web provided universities with an  opportunity to expand their student markets and offer flexible  forms of programs.  There was increasing expectations for, and  interest among, university teachers to integrate technology within  their teaching practice.   From those early days of the Web,  research and teaching scholarship explored the opportunities that  technology provided education  particularly in terms of  improving pedagogy [2].  The increasing emphasis on and  expectations towards technology integration created complexity  within the teaching environment in terms of its design,  implementation and evaluation process.  As a result, professional  development and teaching innovation funding was directed at  projects aimed at improving teaching using technology.   Underlying the support for such projects was the premise that the  project outcomes, resources and products would be disseminated  amongst the higher education community and adopted at the  institutional and individual teacher levels.  The emergence and  subsequent developments in the area of learning designs  exemplify these principles of resource creation, sharing and reuse  that were at the heart of this higher education funding.   2.1 Research in Learning Designs  For over a decade now, researchers around the world, have been  interested in the educational and technical issues associated with  how teachers and instructional designers could create document,  share, adapt and implement, educational design ideas.   This  involved developing a technical language (IMS LD [3]),  developing tools in which learning designs could be created  and/or used [4, 5], and investigating how teachers interpret and  use different learning design representations [6, 7].   In Australia, learning designs first emerged through a project  funded by then Australian Universities Teaching Committee.   Initiated in 2000, entitled Information and Communication  Technologies and Their Role in Flexible Learning, the project  established a framework for evaluating high quality learning in  higher education [8], identified and evaluated educational  examples against the framework, documented these as exemplars  and generic learning designs, and disseminated these through the  project website (http://www.learningdesigns.uow.edu.au/).  These  initial resources were termed learning designs and comprised a  visual representation with supporting textual descriptions (see  Figure 1 for example) [9].   Figure 1. Example learning design from  http://learningdesigns.uow.edu.au  The visual representation depicted a sequence of tasks that  learners engage in as rectangles in the centre of the diagram.   Resources (e.g., readings or equipment) that are required for each  learning task are represented as triangles to the left of relevant  tasks and the supports that teachers or other participants provide  (e.g., facilitating a discussion, peer-feedback) are represented as  circles to the right of relevant tasks.  The supporting textual  description allows teachers to identify intended learning outcomes  of the design and provide more fulsome guidance on each  component (tasks, resources, and supports) within the design. The exemplar and generic learning designs developed through this  Australian project have been the basis of a number of subsequent  research studies.  The learning design framework has been found  to be useful as method for teachers to interpret and then apply the  ideas in the design of their own teaching [10] and as a tool that  that allows extraction of teaching and learning processes for  analysis [11]. The learning design framework has proven to be  useful for teacher interested in how to integrate learning objects  into their teaching  where learning objects are consider the  resources within the learning designs [12]. In researching their  dissemination, these learning designs have been reportedly used  by staff in educational development units and individual  university teachers to support the process for designing courses  and also as a way of reflecting on the implementation once a  course has been delivered [13].   Compatible frameworks have been researched yielding similar  results.  Some approaches are a simple as the use of a concept  map to identify and link key learning components [6].  The more  complex pedagogical patterns approach, developed from the  architectural frame, has shown promise for articulating  collaboration focused teaching and learning and incorporating  open educational resources (or learning objects) [14].  This combined work suggests that a learning design framework of  some sort is a useful mechanism to support the design and  planning aspect of the educational experience and also as a way  of sharing and stimulating educational ideas amongst university  teachers.  Thus, this research basis gives us a range of mechanism  to define pedagogical intent.  The next step in the process is being  able to understand the process and outcome of these intentions.   While traditional course assessment provides one mechanism, the   154    growing availability of data through online education systems  allows other opportunities.   3. THE POTENTIAL OF LEARNING  ANALYTICS While learning designs may provide theoretical, practice-based,  and/or evidence-based examples of sound educational design,  learning analytics allow us to test those assumptions with actual  student interaction data in lieu of self-report measures such as  post hoc surveys.  In particular, learning analytics provides us  with the necessary data and tools to support the accountability  that has been called for in higher education. As with learning designs, learning analytics takes a range of  forms and foci.  One sub-set of the learning analytics domain is  social network analysis (SNA). SNA provides a way to  understand the educational experience and outcomes for learners  engaged in online communication activities.  Much of the  aforementioned take-up of technology in higher education has  been the associated with the the discussion forum  particularly  within learning management system course sites [15, 16].  There  are countless examples in the literature that describe the design  and implementation of educational programs that use discussion  forums.  Where these cases are aligned with a theoretical premise,  it is of the social cognitive aspect of learning and the benefits of  collaboration (e.g., [17].  Thus it follows that, social network  analysis provides a methodology for teachers to begin to  understand the impact of their implemented learning design on the  student learning experience and outcomes.  However, this  analysis needs to be readily accessible and interpretable for all  educators regardless of expertise in network analyses; information  and communication technologies or educational theory.   To address this issue of user uptake and interpretation, Dawson  [18] and colleagues developed an analytical tool named Social  Networks Adapting Pedagogical Practice (SNAPP  available at  http://research.uow.edu.au/learningnetworks/seeing/snapp/index.h tml).  SNAPP seamlessly integrates with learning management  system discussion forums to extract data and provide teachers  with real-time visualizations of the discussion forum activity (see  figure 2).  Recent evaluations of the use of SNAPP by university  teachers demonstrated that the tool was seen to be particularly  effective for retrospective analysis of an education  implementation after a course was completed [19].  However,  despite the ease in generating ongoing visualizations of student  discussion interactions and relationship development, teachers did  not take the opportunity to analyze data during actual course  implementation. This process would have allowed for  modification of the learning design on the fly if the intended  experience or outcomes were not being realized.  While this  learning analytics tool theoretically supports the implementation  stage of that educational continuum - to date it has been largely  used as a reflective tool. Further research is required to investigate  how educators can be better prompted when student interactions  indicate a deviation from the intended learning outcomes. For  example, an educator observes minimal student-to-student  interactions arising despite the implementation of learning  activities designed to establish a learning community.   Figure 2. SNAPP generated sociogram of the student network  relationships evolving from an online discussion forum   (student names have been removed).   4. THE CHALLENGE  Essentially, the learning design approach is founded on a case- based reasoning premise. A documented learning design provides  a case that is abstract enough such that teachers can imagine how  it might be applied to their own context.  However, investigation  of the use of learning designs suggests that teachers also benefit  from understanding the context of others and the environments in  which they teach and thus cases should not be so abstract as to  lose a connection with the original context [20].    Regardless of  the level of abstraction, the notion is that teachers learn or refine  their ideas about design from the examples of other teachers.  The dissemination and adoption of innovative teaching ideas in  higher education  whether those ideas are expressed through  learning design representations or other types of tools or  resources, recognizes the social processes of being a teacher.   University teachers report that their ideas about teaching come  from interacting with colleagues and other teachers [21, 22]. In  this context, teachers are less likely to seek information from  external resources or online sources.  This research (and that of  others investigating educational change) frequently highlights  time as a barrier to the adoption of new ideas into ones teaching  practice.  Academics in higher education institutions are stretched  across their teaching and research responsibilities.  They are more  likely to invest time to try new ideas if they are convinced,  through evidence, that the innovation will have an impact for  them and their learners that is both time and workload efficient.    Learning analytics has the potential to provide this evidence.  The  research challenge is identifying effective and efficient ways to  make learning design and learning analytics useful and relevant  for teachers. The integration of research related to both learning  design and learning analytics provides the necessary contextual  overlay to better understand observed student behavior and  provide the necessary pedagogical recommendations where  learning behavior deviates from pedagogical intention.  5. ACKNOWLEDGMENTS The authors wishes to acknowledge collaborators in Learning  Design research, Dr Shirley Agostinho, Associate Professor Sue  Bennett, and Emeritus Professor Barry Harper.   155    6. REFERENCES [1] Ramsden, P. (1992) Learning to Teach in Higher   Education. Routledge, London  [2] Owston, R.D.: The World Wide Web: A technology to   Enhance Teaching and Learning Educational Researcher  26 (1997) 27-33   [3] Koper, R., Olivier, B.: Representing the learning design of  units of learning. Educational Technology & Society 7  (2004) 97-111   [4] Conole, G., Fill, K.: (2005)A learning design toolkit to  create pedagogically effective learning activities. Journal  of Interactive Media in Education 8, 2   [5] Griffiths, D., Blat, J., Garcia, R., Vogten, H., Kwong, K.:  (2005) Learning design tools. Learning Design 109-135   [6] Falconer, I., Beetham, H., Oliver, R., Lockyer, L.,  Littlejohn, A. (2007) Mod4L final report: Representing  learning designs. Final project report for Joint Information  Systems Committee (JISC) Design for Learning program,  Glasgow: Glasgow Caledonian University   [7] Agostinho, S.(2009 Learning design representations to  document, model, and share teaching practice. In:  Lockyer, L., Bennett, S., Agostinho, S., Harper, B. (eds.):  Handbook of Learning Design and Learning Objects:  Issues, Applications, and Technologies, Vol. 1. IGI  Global, Hershey) 1-19   [8] Boud, D., Prosser, M. (2001) Appraising New  Technologies for Learning: A Framework for  Development. Educational Media International 39, 237 -  245  [9] Agostinho, S., Harper, B., Oliver, R., Hedberg, J., Wills,  S. (2008) A visual learning design representation to  facilitate dissemination and reuse of innovative  pedagogical strategies in university teaching. In: Botturi,  L., Stubbs, S. (eds.): Handbook of Visual Languages for  Instructional Design: Theories and Practices. Information  Science Reference, IGI Global, Hershey PA 380-393   [10] Bennett, S., Agostinho, S., Lockyer, L. (2005) Reusable  learning designs in university education. Proceedings of  the IASTED International Conference on Education and  Technology 102-106   [11] Lockyer, L., Kosta, L., Bennett, S. (2009 An analysis of  learning designs that integrate patient cases in health  professions education. In: Lockyer, L., Bennett, S.,  Agostinho, S., Harper, B. (eds.): Handbook of Research in  Learning Designs and Learning Objects. IGI Global,  Hershey, New York ) 777-791   [12] Bennett, S., Lockyer, L., Agostinho, S. (2004)  Investigating how learning designs can be used as a  framework to incorporate learning objects. Beyond the  Comfort Zone: Proceedings of the 21st ASCILITE  Conference 116-122   [13] Agostinho, S. (2006)  The use of a visual learning design  representation to document and communicate teaching  ideas. In: Markauskaite, L., Goodyear, P., Reimann, P.  (eds.): Who's Learning Annual Conference of the  Australasian Society for Computers in Learning in  Tertiary Education. Sydney University Press, Sydney 3-7   [14] Dimitriadis, Y., McAndrew, P., Conole, G. &  Makriyannis, E. (2009). New design approaches to  repurposing open educational resources for collaborative  learning using mediating artefacts. In Same places,  different spaces. Proceedings ascilite Auckland 2009. http://www.ascilite.org.au/conferences/auckland09/procs/ dimitriadis.pdf  [15] Macfadyen, L., Dawson, S. (2010) Mining LMS data to  develop an early warning system for educators: A proof  of concept. Computers & Education 54, 588-599   [16] Dawson, S., McWilliam, E. (2008) Investigating the  application of IT generated data as an indicator of  learning and teaching performance.: A report on the  Australian Learning and Teaching Council funded  project., Canberra  [17] Lockyer, L., Patterson, J., Harper, B. (2001) ICT in higher  education: Evaluating outcomes for health education.  Journal of Computer Assisted Learning 17, 275-283   [18] Dawson, S., Bakharia, A., Heathcote, E.  (2010)  SNAPP:  Realising the affordances of real-time SNA within  networked learning environments. In: Dirckinck- Holmfeld, L., Hodgson, V., Jones, C., de Laat M,  McConnell, D., Ryberg, T. (eds.): Proceedings of the 7th  International Conference on Networked Learning 125-133   [19] Dawson, S., Bakharia, A., Lockyer, L., Heathcote, E.  (2011)  Seeing  networks: Visualising and evaluating  student learning networks. A report prepared for the  Australian Learning and Teaching Council funded project.  [20] Fincher, S., Utting, I (2002). Pedagogical patterns: their  place in the genre. Proceedings of the 7th annual  conference on Innovation and technology in computer  science education. ACM ,199-202   [21] McKenzie, J., Alexander, S., Harper, C., Anderson, S.  (2005): Dissemination, adoption & adaptation of project  innovations in higher education:. A report for the Carrick  Institute for Learning and Teaching in Higher Education  [22] Bennett, S.J., Agostinho, S., Lockyer, L., Kosta, L.K.,  Jones, J., Harper, B.M. (2008) Understanding university  teachers' approaches to design. In: Luca, J., Weippl, E.  (eds.): Proceedings of ED-MEDIA 2008 World  Conference on Educational Multimedia, Hypermedia &  Telecommunications AACE, Chesapeake, VA 3631-3637   156    "}
{"index":{"_id":"18"}}
{"datatype":"inproceedings","key":"Richards:2011:RFE:2090116.2090141","author":"Richards, Griff and DeVries, Irwin","title":"Revisiting Formative Evaluation: Dynamic Monitoring for the Improvement of Learning Activity Design and Delivery","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"157--162","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090141","doi":"10.1145/2090116.2090141","acmid":"2090141","publisher":"ACM","address":"New York, NY, USA","keywords":"cohort-paced, e-learning instructional design, formative evaluation, learning activities, online learning","Abstract":"In this paper we present a framework for learner modelling that combines latent semantic analysis and social network analysis of online discourse. The framework is supported by newly developed software, known as the Knowledge, Interaction, and Social Student Modelling Explorer (KISSME), that employs highly interactive visualizations of content-aware interactions among learners. Our goal is to develop, use and refine KISSME to generate and test predictive models of learner interactions to optimise learning.","pdf":"Revisiting Formative Evaluation: Dynamic Monitoring for  the Improvement of Learning Activity Design and Delivery   Griff Richards         Irwin DeVries  Thompson Rivers University   Open Learning  Kamloops, Canada   1.250.852.6866  griff@sfu.ca   idevries@tru.ca   ABSTRACT Distance education courses have a tradition of a formative  evaluation cycle that takes place before a course is formally  delivered. This paper discusses opportunities for improving online  and blended learning by collecting formative data during course  presentation. With a goal of overall improvement in instructional  effectiveness and identification of promising practices for  inclusion in a learning activities design library, we propose the  immediate and on-going monitoring of the effectiveness of  learning activities, tutor facilitation and learner satisfaction during  the course presentation. This has implications for constructively  involving the learners and facilitators in the course improvement  process. While originally conceived to reduce the time for pilot  evaluation of new courses and learning activities, the proposed  system could also be extended to individualized and blended  learning environments, and if implemented using semantic web  technologies, for research into the effectiveness of learning  activity patterns.   Categories and Subject Descriptors  H.5.m [Information systems]: Information Interfaces  and  Presentation. Miscellaneous.  General Terms  Algorithms, Management, Measurement, Documentation, Design,  Economics.   Keywords  Formative evaluation, Learning activities, Cohort-paced, Online  learning, e-learning Instructional Design.   1. INTRODUCTION Distance education has a long tradition of conducting formative  evaluation of instructional materials and learning activities before  the ongoing delivery of a course. The feedback from pilot testing   and expert evaluation enables course designers to catch and  correct any weaknesses detected. The lessons learned can be  incorporated into the professional design heuristics of the course  designers enabling promising design practices to be reused in new  courses, and disappointing practices to be redesigned or rejected.  In recent years there has been an influx of traditional face to face  delivery institutions to the online environment. Sometimes,  indeed often, they expect the instructor of a face-to-face course to   convert their course (or certain activities of their blended  courses) to on-line delivery with a minimum of instructional  design support, and there is little provision for observing which  learning activities work and which require improvement.  Typically course evaluation takes place at the end of the course,  after the final marks have been submitted, but before learners  receive grades. This delayed process does not capture immediate  responses and reflections in time to provide meaningful formative  evaluation that might enhance the learning experience of a course  in session. Thus for both blended and distance courses a case can  be made for a system for improving formative evaluation.   This paper looks at the potential for embedding formative  evaluation tools in both online and blended course delivery. Our  goal is to improve the quality of online and blended learning  experiences, along with facilitation and instructional design  practice, by stimulating ongoing reflective practices among  course designers and course facilitators such as professors,  instructors, mentors or tutors. We recognize that there may be  pitfalls to openly soliciting feedback from learners during a  course, and there may be governance and collective agreement  issues arising from providing feedback on the effectiveness of  learning activity facilitation. The proposals contained here are  work in progress, and an opportunity to open dialogue and critical  reflection on this topic. We are fully aware that every on-line  cohort establishes informal back channels where the learners  actively blog their opinions  possibly the only ones not in the  conversation on instructional effectiveness are the faculty  presenting the course.    2. FORMATIVE EVALUTION  Scrivens [1] coined the terms formative and summative  evaluation to distinguish between evaluation of educational  materials during their development and at the end of the  instructional cycle. Formative evaluation is intrinsic to  instructional systems design models [2][3] and it has been  ingrained into the development cycle of most distance learning  organizations that produce instructional media or course  packages. During the big media phase [4] when distance      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February  27March 1, 2011, Banff, Alberta, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.      157    education was dominated by centralized production facilities  turning out television shows and print packages, formative  evaluation was a key part of ensuring quality before printing  hundreds of copies for the warehouse or broadcasting on  television. Distance learning was in a sense asserting its rightful  place, and the best way to counter criticism of traditional  universities was to demonstrate the quality of the courses was as  good as if not better than the traditional offerings. Indeed, what  most distance learning courses lost in presence they more than  made up for in a systematic approach to development, the  alignment of course materials to instructional objectives, and the  thoroughness of content delivery and student assessment [5].   Formative evaluation was also an essential part of multimedia  development [6] and carried into web site development [7]. It was  evident in early on-line course development, again in response to  concerns about the quality of courses that simply shoveled content  onto the web [8][9] Various formative evaluation approaches  were suggested by Reigeluth and Frick [10] with the intent of  improving instructional design theories that then would translate  into improved theories and instructional design. However, as  online delivery became mainstream and blended with classroom  instruction, formative evaluation seemed to lose its earlier  attention in the literature. Perhaps course designs and instructional  activities became somewhat standardized, but probably the real  reason was that increasingly in the 2000s, web delivery had  become accepted as a credible indeed essential extension of the  academy. With the volume of courses to be transferred to the web  there were insufficient instructional design resources to conduct  formative evaluations. This period of adjustment was  characterized by the downsizing of resources for centralized  distance learning departments as faculties set up their own  distance programs, the growth of learning management systems  making it easier for individual instructors to load content online,  the rise of the dual mode university and in Canada the reduction  in the number of single mode distance universities [11] [12].  Traditionally neither formative nor summative evaluation has  seen a comfortable fit in the face-to-face classroom [13]. Courses  were taught by faculty who were expected to get the bugs out  in  two or three terms. As this same expectation is creeping into the  practice of online education, formative evaluation in online  learning has not seen a high profile in practice over the past  decade. Yet formative evaluation can strengthen both the  implementation of a program and the knowledge gained within it  [14].   3. CHALLENGES IN ONLINE      EDUCATIONAL PACTICE  In addition to changes in instructional development models for  online courses, the past twenty years of educational practice have  seen a change from objectivist philosophies and paradigms to  increasingly constructivist views [15] [16]. The internet is  increasingly seen less as a medium of delivery and more as a  medium of communication in which interactions can take place  among learners and instructors, and learners and the content [17].  Traditional models of distance education offered individual  delivery of content-based instructional materials. Alongside the  growth of channels for interaction, cohort paced courses have  been implemented. These require learners to interact in many  ways and to create new knowledge together. The resulting learner  engagement can promote both achievement and retention.   The need for formative evaluation of learning activities in online,  paced cohort courses is important in view of this shifting role of  the learner. Learners are active participants with rich and complex  experiences. The learners engagement in the collaborative  activities places them in the position of co-creators of knowledge  within the learning environment, as well as self-organizers of  their learning [18]. As described by Parrish, While IDs  [instructional designers] work to tame instruction into a  manageable, replicable process that begins by predetermining  outcomes to be measured through properly aligned assessments,  engagement describes that wild aspect of the process in which the  learner is as much or more in control of the activities as the  ID[19]. The situatedness of the learners and the contexts in  which they find themselves become meaningful realities in the  learning environment [20]. Development of community, shared  practices and reflection are important parts of learning activities.  The application of cooperative learning techniques to the design  of learning activities for cohort-paced e-learning can produce  engaging discussion, reflection and deeper processing of the  content. With instructor-facilitated cohort/collaborative  approaches providing such positive results, distance learning  course providers are abandoning investments in comprehensively  detailed content packages and elaborate instructional designs.  Institutions notice that these changes make a difference, and  cohort-paced distance learning courses have lower drop-out rates  than their self-paced counterparts, about 85% retention versus  65% for individualized delivery [21][22]. In a review of literature  Means, Toyama, Murphy et al. [23] noted significant effect sizes  for facilitated and collaborative online learning when compared  with individualized delivery for the same content, although they  were careful not to attribute this as a media effect noting that the  cohort modes often involve different activities and increased time  on task. At the same time, it is clear from the research that the  many of the types of activities included may be of little value. For  instance, they also noted that the provision of extra video clips  and chapter quizzes contributed little to student achievement,  while activities that provoke reflection and engage the learners  metacognitive processes can yield improvements in learning.  Richards [24] observed that trivial learning activities such as  knowledge level multiple-choice quizzes, or forum directions to  post your thoughts and reply to the thoughts of two other  learners led to a superficial understanding of the course content.  It is therefore important to continue formative evaluation in these  dynamic new learning environments, in order to determine which  activities are both valued and valuable and those that are little  more than  make work  projects. Feedback from learners in these  environments is necessary in gaining a better understanding of  these activities.  In this paper we strongly advocate for careful design of such  activities for cohort-paced e-learning, and suggest that if  formative evaluation is no longer conducted before the delivery of  a course, then it should be embedded into the course delivery.  This should be simple to implement. Finally, since the purpose of  formative evaluation is to inform practice and improve delivery,  the process should promote reflection on the part of learners,  instructors and designers as all have a role to play within the  learning experience, and all might benefit from an open  discussion on improving the learning environment.   158    4. OTHER BENEFITS OF FORMATIVE EVALUATION  Eijkman raises a series of questions we as educators need to  consider in our use of web-based learning and social  environments. For instance, what practices, habits, and patterns  of use emerge  and What changes need to occur in institutional  policies and technological practices in order to integrate the social  Web effectively into the educational mainstream [25].  Documentation or other forms of visualization of learning  activities and designs can help to capture emerging innovative and  expert practices [26] [27], and to gain a deeper understanding of  the user experience.    While not a primary focus of this paper, research and  development around reusability or adaptation of learning  activities and designs along with educational policy and practice  can also benefit from formative evaluation. If formative  evaluation of activities leads to their improvement over time due  to the use of this feedback in updating and maintaining courses,  these activities can be added to design libraries for re-use and  sharing. Further, analysis of the broader emerging patterns may be  incorporated into strategic and operational planning. The goal in  the end of improvement of learning activities and designs is  improved quality of instruction [28].     5. A SIMPLE MICRO MODEL OF AN  ONLINE LEARNING ACTIVITY   Fig. 1 diagrams three nested levels of the design and delivery of  an online learning activity. Level 1 is the Instructional Design  Level  the level at which instructional goals are aligned with  learning activities that are appropriate for the learners. Level 2 is  the Facilitation Level  and encompasses those roles, activities  and resources that come together during the conduct of a learning  activity. Level 3 represents the Learner Experience. Note that  each level has been allocated three phases of preparation,  enactment, and reflection. It is our belief that this is the simplest  depiction possible for our purposes and we recognize that learning  environments and learning activities may become extremely  crowded with multiple roles, players and resources. We fully  anticipate that other evaluators may want to expand this depiction  to be more explicit or to compact the phases to be more specific.  In some settings the design and facilitation roles may involve the  same individual(s). In some settings the facilitator may also be  consulted in the design process, while in others, the facilitator  may become involved years after the initial design, after a course  has run several times.    Online learning activities evolve to meet the needs of content,  audience and the constraints of the instructional system. The  model looks at a single activity, whereas a course is a strategy  of intentionally sequenced progression through a series of  learning activities. Some activities such as a lecture are well  structured, and others like a reading assignment are loosely- structured. It is also possible that parallel learning activities such  as study groups may be autonomously initiated and conducted by  a learner or group of learners as they form a learning community.   Whether these should be included in the scope of the Dynamic  Evaluation Model is left to the discretion of those conducting the  evaluation. Similarly, there may be others external to the  instructional process having a bearing on the results. While  Garrison and Anderson [29] only identify instructors, peers and   content in their interaction model for online learning the actual  educational environment may include professional faculty  developers, mentors, peers, friends, family and others  anyone  who influences the decisions and performance of any of the key  roles.      Fig. 1. A Conceptual Model for Dynamic Evaluation of  Learning Activities   6. ALIGNING THE MODEL WITH  ISNTRUCTIONAL DESIGN  METHODOLOGY  As discussed earlier, in a cohort-paced constructivist learning  activity not all learner activity is predictable since learners bring  their own experiences and contexts to the learning situation.  While situated in the design and execution of intentional learning  activities, the model also takes into account learners own  experiences of the activity. We use the term learning activity to  avoid confusion with the more technical terms learning design  and lesson plan which are expressions of learning activities.  The term learning activity encompasses any activity that brings  learners into planned contact with content, other learners, and  experiences that promote acquisition of skills, knowledge and  attitudes. This broad definition is congruent with similar  definitions [23]. While traditionally instructional design does not  include accidental nor incidental learning activities, in more open  ended learning environments learners might influence the learning  environment in unpredictable ways, and in their search for  alternate explanations may discover materials useful to others.   To the extent that instructional design is an intentional and  iterative process, we look at preparation (planning and alignment  of goals with activities), the design itself, and reflection on the  outcomes of the design. Preparation is included as part of  facilitation because so much success depends on the facilitators  skills and knowledge of facilitation techniques, their  understanding of the activity and their role in and commitment to  its success. Preparation is also important for learners in terms of  both prerequisite skills and knowledge and in terms of adequate  direction to participate the learning activity. We believe that  reflection is a part of all processes and, in terms of improving the  system, early reflection catches errors before they can become  deeply embedded in the teaching-learning system.      7. PRACTICAL ISSUES  The goal of formative evaluation is to improve the learning  experience. If evaluation of the learning activities is not   159    conducted until the end of the course or beyond, then no  remediation can take place if there is a problem. We propose the  following guidelines:     1. Formative evaluation should take place during or at the  end of each learning activity.   2. Formative evaluation should seek data and reflections  from learners, facilitators and designers.   3. Formative evaluation to seek both quantitative and  qualitative data.   4. The results of the formative evaluation should be open  to all participants.   5. If error correction is required, it should be considered  immediately    6. If activity re-design is required it should be embarked  upon so that it can be revised for the next course  offering.   7. If learning activities are to be evaluated for several  courses, then investment in an evaluation system to  gather and analyze the data should be considered.   8. The results of dynamic formative evaluations may have  value in explaining the findings of end of course  evaluations, and in the evaluation of generic learning  activity designs, including the training of facilitators,  and the directions provided to learners.     For purposes of brevity, we have not described the importance of  linking such a system to descriptive ontologies such as Learning  Object Context Ontology [30]. However, we believe that semantic  tagging will enhance the ability of researchers and designers to  better understand the patterns that may emerge from the data  collected, and raise the importance of both instructional activity  design and evaluation as part of organizational learning.      8. PROPOSED IMPLEMENTATION  Richards [24] embedded questions on the efficacy of cohort  learning activities into the Moodle site for a graduate distance  course in Instructional Design at Athabasca University. For each  activity, the learners were asked eight questions to rate their  experience (along a five point Likert scale) and provided an  opportunity to comment. This rating has been conducted a  number of years and Fig. 2 shows a typical result.   As the evaluation was constrained to a single course, a  questionnaire was used to present the questions and collate the  data. Unfortunately, with Moodle the raw data were not available  so neither is further analysis - even simple statistics such as the  standard distribution or the maximum and minimum values are  unavailable. For a more robust system capable of handling  multiple courses we propose to implement the dynamic evaluation  system external to the learning management system so that we  can have greater control over the data, and the results would be  then returned to the course participants through a web service.  The course instructors and course designers would also have  access to the participant comments. In a course with several class  sections or perhaps teaching assistants, additional questions could  be developed to link into each section and to pass back the  appropriate identifiers to and from the LMS.   If the function of the embedded questionnaires is to improve the  learning activities, then the most important question is what  suggestions the participant offers to improve the learning activity.   For research purposes, it will be useful to ferret out other  additional information for example on the role of the facilitator in  animating a learning activity. While it would be appropriate to  ask the learners if  the facilitator/ instructor/tutor contributed to  the success of this activity , it could only be interpreted in light of  facilitators own reflections about their preparation for the  activity, the amount of time devoted to the activity, and other  such factors.   Similar questions might be asked of the course designers when  they review the results of the activity. It is important to note that  it may be very easy or very difficult to pin down why a cohort  activity works or does not work. For example the questions used  in Fig. 2 take for granted that many preparatory steps had already  gone correctly: learners had the appropriate prerequisites, text  books had arrived, individuals had read the prescribed materials,  there were no untimely interruptions in internet services, and  other such assumptions. These are extrinsic factors. Intrinsic  factors are more within the realm of the course developers and  facilitators  was the activity relevant, was the group size  appropriate (what about the group make up), was the time  appropriate. The outcomes are the feeling of connectedness  (which is a vector on group cohesion), that all members of the  group contributed equally is in part a function of the balance  between individual and group accountability  group projects  generally do not work if there is no positive interdependence [31].  Finally, the achievement worthiness is important: was the activity  worthwhile and did the activity help with learning We can well  imagine learning activities that are well intentioned but involve  superficial treatment of the content and thus provoke little or no  deep learning and have little long-term effect on understanding or  retention.         Fig. 2. Typical results of a learning activity evaluation in  MDDE604   Dynamic formative evaluation seeks to gather data to ascertain  the effectiveness of a learning activity, if required remediate with  the current learners, and make adjustments as required in the  activity before the next class. The adjustments may be with the  content and materials, the directions to the facilitator role, or  directions to learners. However, as noted earlier, a significant  value of dynamic formative evaluation may be in generalizing the  lessons learned and formalizing the expertise so that it can be   160    shared with other course developers. This loftier purpose requires  the design of a data base that is semantically enriched so that  pattern description of the activities and the roles can be  generically described with ontologies such as the Learning Object  Context Ontology (LOCO) and extensions [30].  The semantic  tags will enable pattern analysis across several courses, initially to  allow course developers to locate and view how winning activities  are embedded in existing courses, but also in the long run to  identify and extract patterns into a library of successful practices.  This then brings us close to the ideals of Koper [28] in  documenting successful learning designs that can be reused in a  pragmatic manner. Before closing it is important to note that a  key implementation issue will be acceptance of the system by all  users. In distance delivery student response to end of course  questionnaires administered by administrative and marketing  groups can be as low as 10 per cent, while Richards [24] found  embedding the questions as part of the course brought a 100 per  cent response rate. For dynamic formative evaluation to be  effective it needs to be an active part of the learning experience   the questions should provide feedback to the learners on how their  perceptions and experience compared with that of others, and  there should be an active response to problem areas identified.   Going further, the dynamic evaluation system could also solicit  suggestions to update the learning resources that might be used to  help others learn  moving from a prescriptive to a constructive  learning environment has been a successful strategy in the  corporate learning context of the IntelLEO Project [32]. Similar  benefits should be obvious for facilitator and course designers in  improving the quality and efficiency of the online learning  experience.     9. SUMMARY In summary, the purpose of this paper was to provoke discussion  about the need to revisit formative evaluation of e-learning  activities and course designs. If e-learning and blended models  are the new reality of distance education, then formative  evaluation is more important than ever. Because of the  proliferation of distance education, much of it developed without  the assistance of an instructional design team, and the complexity  of constructivist learning design in cohort-paced courses, in many  cases formative evaluation needs to take place during early course  delivery. A dynamic process for formative evaluation on the  success of learning activities (whether designed or not) is  important in the creation of an informed community of practice.  Currently, because of back channel communications among the  learners, the only ones out of the feedback loop are the instructors  and course designers. A dynamic learning activity evaluation  system will help to close that gap.     10. REFERENCES [1] Scrivens, M. 1967. The Methodology of Evaluation. In:   Tyler, R.W., Gagne, R.M., Scrivens, M. (Eds.) Perspectives  of Curriculum Evaluation. Rand McNally & Co.    [2] Dick, W., Carey, L. 1978. The Systematic Design of  Instruction. Scott, Foresman, Glenview, IL.    [3] Reigeluth, C. 1983. Instructional Design: What Is It and  Why Is It In: Reigeluth, C. (Ed.): Instructional-Design Theories and Models: An Overview of their Current Status,  pp. 4-36. Laurence Erlbaum, Hillsdale, NJ.   [4] Schramm, W. 1962. Mass Communication. Annual Review of  Psychology 13, 251-284.   [5] Moore, M., Kearsley, G. 1996. Distance Education: A  Systems View. Wadsworth Publishing Company, Belmont  CA.   [6] Flagg, B.N. 1990. Formative Evaluation in the Performance  Context. Lawrence Erlbaum, Hillsdale, NJ.   [7] Preece, J., Benyon, D. 1993. Open University: A Guide to  Usability: Human Factors in Computing,  Addison-Wesley  Longman Publishing Co., Inc., Boston, MA.   [8] Harasim, L. 2000. Shift Happens: Online Education as a  New Paradigm in Learning. The Internet and Higher  Education 2, 41-61.   [9] Nobel, D. October 1997. Digital Diploma Mills.  http://www.handshake.ca/noble.html   [10] Reigeluth, C., Frick, T.W. 1999. Formative Research: A  Methodology for Creating and Improving Design Theories.  In: Reigeluth, C. (Ed.) Instructional Design Theories and  Models: A New Paradigm of Instructional Theory, vol. 2, pp.  633-652. Lawrence Erlbaum Associates, Inc., Publishers, NJ.   [11] Abrioux, D. 2001. Trends in Canadian Distance Education:  An Institutional Perspective. ICDE World Conference,  Dsseldorf.   [12] Abrioux, D. 2006. Strategic Issues in Single- and Dual-mode  Distance Education. Consultant's report,  http://www.col.org/resources/publications/consultancies/Pag es/2006-singleDualMode.aspx   [13] Cavanagh, R. 1996. Formative and Summative Evaluation in  the Faculty Peer Review of Teaching. Innovative Higher  Education 20, 235-240.   [14] Brown, J.L., Kiernan, N.E. 2001. Assessing the Subsequent  Effect of a Formative Evaluation on a Program. Evaluation and Program Planning 24, 128-143.   [15] Jonassen, D.H. 1990. Objectivism versus Constructivism: Do  We Need a New Philosophical Paradigm Educational Technology 30, 32-34.   [16] Cooper, P. 1993. Paradigm Shifts in Designed Instruction:  From Behaviorism to Cognitivism to Constructivism.  Educational Technology 33, 12-19.   [17] Garrison, D. R., Anderson, T., & Archer, W. 2000. Critical  inquiry in a text-based environment: Computer conferencing  in higher education. Internet and Higher Education, 2(2-3),  87 - 105.    [18] Rohse, S., Anderson, T. 2006. Design Patterns for Complex  Learning. Journal of Learning Design 1, 82-91.   [19] Parrish, P. 2007. Plotting a Learning experience. In: Botturi,  L, Stubbs, T. (Eds.) Handbook of Visual Languages for  Instructional Design: Theories and Practices. Information  Science Reference  Imprint of IGI Publishing, Hershey, PA,  91-111.   [20] Luckin, R., Cook, J., Clark, W., Day, P., Garnett, F.,  Ecclesfield, N., Whitworth, A., Hamilton, T., Akass, J.,  Robertson, J. 2011. Learner-Generated Contexts: A  Framework to Support the Effective Use of Technology for  Learning. In: Lee, M., McLoughlin, C. (Eds.): Web 2.0- Based E-Learning: Applying Social Informatics for Tertiary   161    Teaching. Information Science Reference (an Imprint of IGI  Global), 70-84.   [21] Anderson, T. 2009. Keynote: Beyond Learning  Management: Open Learning Support and Inspiration.  Canada MoodleMoot, Edmonton.   [22] Jakubec, M., Harrison, M., Enstrom, E. 2010. Evolving  Online: Perspective on Collaborative Learning. Canadian  eLearning Conference.   [23] Means, B., Toyama, Y., Murphy, R., Bakia, M., Jones, K.:  2010. Evaluation of Evidence-Based Practices in Online  Learning: A Meta-Analysis and Review of Online Learning  Studies. Revised September 2010. US Department of  Education, Office of Planning, Evaluation, and Policy  Development, Policy and Program Studies Service.   [24] Richards, G.  Moisey, S. 2009. Embedding collaborative   online learning activities. Moodle Moot. Edmonton.   [25] Eijkman, H. 2011. Dancing with Postmodernity: Web 2.0+  as a New Epistemic Learning Space. In: Lee, M.,  McLoughlin, C. (Eds.): Web 2.0-Based E-Learning:  Applying Social Informatics for Tertiary Teaching.  Information Science Reference (an Imprint of IGI Global),  343-364.    [26] Conole, G. 2008. Using Compendium as a Tool To Support  the Design of Learning Activities. In: Okada, S., Shum, B.,  Sherborne, T. (Eds.), Knowledge Cartography: Software  Tools and Mapping Techniques, London, Springer, 199-222.   [27] Dalziel, J. 2008. Learning Design: Sharing Pedagogical  Knowhow. In: Iiyoshi, T., Kumar, M. S. (Eds.) Opening Up  Education: The Collective Advancement of Education   through Open Technology, Open Content, and Open  Knowledge, pp. 375-388. The MIT Press, Cambridge, MA.   [28] Koper, R., Tattersall, C. 2005. Preface to Learning Design: A  Handbook on Modelling and Delivering Networked  Education and Training. Journal of interactive Media in  Education 18, 1-7.   [29] Anderson, T. 2003. Getting the mix right again: An updated  and  theoretical rationale for interaction.  The International  Review of Research in Open and Distance Learning, 4 (2).   [30] Knight, C., Gaevi, D., Richards, G. 2006. An Ontology- Based Framework for Bridging Learning Design and  Learning Content. Educational Technology & Society 9, 23- 27.   [31] Johnson, D., Johnson, R., Holubec-Johnson, E. 1994. The  New Circle of Learning: Cooperation in the Clasroom and  School: Alexandria, VA: Association for Supervision and  Curriculum Development.   [32] Siadaty, M., Jovanovi, J., Gaevi, D, Jeremi, Z.,  Holocher-Ertl, T. 2010. Leveraging Semantic Technologies  for Harmonization of Individual and Organizational  Learning. In  Martin Wolpers, Paul A. Kirschner, Maren  Scheffel, Stefanie Lindstaedt and Vania Dimitrova (Eds.)  Sustaining TEL: From Innovation to Learning and Practice  5th European Conference on Technology Enhanced  Learning, EC-TEL 2010, Barcelona, Spain, September 28 -  October 1, 2010. Proceedings 340-356.  http://www.springerlink.com/content/978-3-642-16019- 6#section=781059&page= 11& locus=15        162    "}
{"index":{"_id":"19"}}
{"datatype":"inproceedings","key":"Pardo:2011:SOB:2090116.2090142","author":"Pardo, Abelardo and Kloos, Carlos Delgado","title":"Stepping out of the Box: Towards Analytics Outside the Learning Management System","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"163--167","numpages":"5","url":"http://doi.acm.org/10.1145/2090116.2090142","doi":"10.1145/2090116.2090142","acmid":"2090142","publisher":"ACM","address":"New York, NY, USA","keywords":"adaptation, learning analytics, learning management system, user monitoring, virtual machines","Abstract":"Government and institutionally-driven reforms focused on quality teaching and learning in universities emphasize the importance of developing replicable, scalable teaching approaches that can be evaluated. In this context, learning design and learning analytics are two fields of research that may help university teachers design quality learning experiences for their students, evaluate how students are learning within that intended learning context and support personalized learning experiences for students. Learning Designs are ways of describing an educational experience such that it can be applied across a range of disciplinary contexts. Learning analytics offers new approaches to investigating the data associated with a learner's experience. This paper explores the relationship between learning designs and learning analytics.","pdf":"Stepping out of the box. Towards analytics outside the Learning Management System  Abelardo Pardo University Carlos III of Madrid  Avenida Universidad 30 28911 Legans (Madrid) Spain  abel@it.uc3m.es  Carlos Delgado Kloos University Carlos III of Madrid  Avenida Universidad 30 28911 Legans (Madrid) Spain  cdk@it.uc3m.es  ABSTRACT Most of the current learning analytic techniques have as starting point the data recorded by Learning Management Systems (LMS) about the interactions of the students with the platform and among themselves. But there is a tendency on students to rely less on the functionality offered by the LMS and use more applications that are freely available on the net. This situation is magnified in studies in which stu- dents need to interact with a set of tools that are easily installed on their personal computers. This paper shows an approach using Virtual Machines by which a set of events occurring outside of the LMS are recorded and sent to a central server in a scalable and unobtrusive manner.  Keywords Learning analytics, Learning management system, virtual machines  Categories and Subject Descriptors K.3 [Computing Milieux]: Computers and Education  General Terms Learning analytics, user monitoring, adaptation  1. INTRODUCTION The field of Learning Analytics is emerging as a combi-  nation of business intelligence, business logic, educational data mining and action analytics [8] where data is collected, analyzed and interpreted to derive so called actuators to op- timize a learning experience. Much the same way in which a regular web site monitors the operations performed by the users to then infer patterns to suggest or modify the ex- perience of future users, in the context of learning, students usually interact with a Learning Management System (LMS) that records all the operations. This wealth of data can also  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK11 February 27-March 1, 2011, Banff, AB, Canada. Copyright 2011 ACM 978-1-4503-1057-4/11/02 ...$10.00.  be analyzed and transformed into useful information to ulti- mately infer and apply changes to the current environment aiming at improving the process for the student, the instruc- tor and/or the institution.  There are numerous key developments that are behind the emergence of this new field. LMSs, both commercial and open source, include modules that automatically regis- ter every event taking place in the platform. The higher the percentage of course activity that takes place in the LMS, the more detailed information is stored. For example, if a learning experience contains support for on-line quizzes, for- mative assessment, a chat room and its own email service, the platform may easily keep record of who did what, when and with whom. When combined with the well established techniques in the area of web analytics, having a detailed account of the interaction between students and instructors, or among students becomes a reality.  Once the data is obtained from the tools used in a learning experience there are multiple objectives that can be tackled. For example, The Signals project a Purdue University [4] is an example of how learning analytics as described in [10] are applied at an institutional level to create an early detection system for student failure. The system uses the data already collected by the institutional LMS to detect in real time the type of events that, based on previous information, have a high probability of leading to student failure. Detecting these patterns translates then into a set of measures that are taking to anticipate the problem and thus reduce student failure rates.  Interaction has been shown to be an important factor in- fluencing student success [3]. The amount of interaction a student has with peers has a positive correlation with the academic performance [12]. As a consequence, having a de- tailed account of the interactions that occur in a learning experience will likely offer a good predictor of academic per- formance, which itself is one of the most important aspects of an educational institution. With these new tools, a learning community can be seen in ways that were never considered before [7].  But in this scenario, there are several challenges that need to be overcome. Campbell and Oblinger [5] characterized the process of learning analytics as an engine with five stages: capture, report, predict, act, and refine. The first step al- ready faces the challenge of having an adequate observation capability. A detailed account of any event that takes place in a learning scenario is the first requirement to have a solid foundation upon which to build the reporting, predicting and acting mechanisms. After the data has been obtained, it  163    should be reported in meaningful forms to all stakeholders. Several visualization techniques have been applied specifi- cally to data gathered in courses (see for example [15, 14]). The next challenge is to determine which factors are truly significant to achieve accurate predictions. In [13] a detailed analysis is performed considering initially a set of 22 vari- ables recorded by the LMS (BlackBoard Vistatm1. Out of these 22 factors, only 13 were found to have a positive sig- nificant correlation with student final grade. A final multi- variable linear model is proposed with only three of these factors accounting for 33% of the variability. These factors are: total number of discussion messages posted, total num- ber of mail messages sent, and total number of assessments completed.  Once a model has been created, the following steps face the challenges of inferring relevant interventions, and finally, to design a feedback process to refine the overall mechanism. There are already several approaches that close this cycle. In [11] a system is presented in which all the interactions of the students with the course material and among each other are recorded and made available to the instructors when modifying the course content. The system uses Semantic Web techniques to translate LMS logs into resource anno- tations that are then inserted into the editing tool used by the instructors to create content.  1.1 The challenge of recording the interaction But recording the interactions that take place in a learning  environment is becoming more difficult. The initial model used during the early stages of LMS deployment in edu- cational institutions could be called LMS-centric. There were numerous analogies between LMSs and conventional knowledge management tools. But with the advent of the Web 2.0, the LMS-centric model has failed [6]. Although the latest LMSs offer an increasing set of features, students are beginning to reach the educational institutions with solid experience on how to interact with their peers in ways that are not covered by the LMS. The main consequence is that a significant part of the interaction during a learning expe- rience is beginning to take part outside of the LMS in what it could be called a de-centralized approach.  Even the LMSs themselves have contributed to this de- centralization. For example, most LMSs offer the option of receiving email notifications when new messages are posted in forums. The chances of students using an email client outside the LMS are increasingly large. Email support is another example. LMSs offer internal an email account to each user, but they are no competition in terms of features other platforms available on the Net.  This tendency is more exacerbated when a learning expe- rience contains a significant amount of activities that cannot be embedded by any means in an LMS. In experimental sci- ences, students typically require the use of special resources for procedural activities. The extreme case of this tendency is in ICT education where most of these special resources are applications that can be installed in the student per- sonal computer. Furthermore, some studies are beginning to confirm that students use conventional ICT tools to ac- cess an increasing number of resources outside the institu- tion LMS [21].  The main consequence of this tendency is that in order to maintain the effectiveness of learning analytics, new tech-  1www.blackboard.com  niques are required to extend beyond the LMS-centric ap- proach and adapt to the Web 2.0 style.  1.2 Observation to support assessment Another factor that is changing the educational landscape  is the transition from a purely expository instructional method to a learner-centered approach [17] where the tutor adopts a more supportive role, and the learner explores, participates and is more active during the learning process. This change of philosophy is having numerous ramifications within the academic world. Entire degree programs are re-organized in order to accommodate the new role of the student. Teach- ing staff needs to adapt their pedagogical techniques to a, sometimes, totally new approach.  Together with these changes, numerous accreditation in- stitutions have emerged with the objective of assuring that educational institutions embrace quality assurance and sus- tained innovation techniques. For example, ABET (Accred- itation Board for Engineering and Technology) is an insti- tution that provides accreditation for degrees in the area of applied science, computing and engineering education. The focus of the accreditation process is on what is learned rather than what is taught2.  The approach described in this document is being de- ployed in an engineering degree that currently pursuing ac- creditation by ABET. The institution describes what stu- dents are expected to know and be able to do by the time they graduate. Again, in the specific case of engineering ed- ucation, some of these outcomes have a strong procedural nature. For example: (k) an ability to use the techniques, skills, and modern engineering tools necessary for engineer- ing practice. [1].  In order to assure that on graduations students are capa- ble of using modern engineering tools, they need to practice with them through activities. This is an example of the type of new outcomes that are being requested from applied science degrees that are difficult to accommodate by con- ventional LMS. At most, LMSs may cover this aspect of the learning process by supporting on-line quizzes, but as an indirect measuring tool.  A second example of the limitations of LMSs is highlighted by another program outcome: (d) an ability to function on multi-disciplinary teams. Teamwork requires a high degree of student-student interaction. There are studies that rely on interaction through forums hosted in an LMS to gain insight on the level of collaboration within teams [2]. But if students are already used to communicate using a variety of Web 2.0 type of tools, it is highly unlikely that when immersed in a collaborative setting, they would use an LMS for these tasks.  From the previous observations, there are several ques- tions that lay ahead in the area of learning analytics:   How much do they rely on interaction taking place in the LMS   How can they cope with new forms of interaction   How are they affected when analyzing interaction in collaborative environments  This document describes the approach to obtain learning analytics in a concrete scenario of collaborative activities  2www.abet.org  164    within a course of an engineering degree. Although still in the preliminary stages, we believe there are several ob- servations that can help shed some light on the previous questions.  2. APPROACH The approach was deployed in the face-to-face courseSys-  tems Architecture, which is part of the degree in Telecom- munication Engineering. The total number of students that initially signed for the course was 248 and were divided into five sections groups. The course contained the following learning outcomes:  1. Design and development of applications in the C Pro- gramming Language.  2. Use proficiently the tools for application development.  3. Apply team working techniques to develop an applica- tion for a mobile device.  4. Use of self-learning techniques.  Outcomes 3 and 4 refer to generic methodological aspects. Team work was used during the second half of the course (six weeks) in which groups of four students were created by the instructors to work in a project. Several documents about team dynamics were requested as readings and a class session was devoted to discuss teamwork, agree on a team contract and discuss the different type of conflicts that may arise. The measures to achieve outcome 4 were applied throughout the entire course. Each session had two sets of activities, previous and in-class. The set of previous activities required an objective that would be reviewed in the following class. Students found this methodology significantly different to those used in other courses.  The course followed a continuous evaluation scheme. Five partial examinations spread along the semester were com- bined with small exercise submissions. The goal was to en- gage students to regularly work in the course. The final course grade was simply the sum of all these partial scores; no final exam was given.  2.1 Providing a fully configured development environment  The main complication from the point of view of analyz- ing the interaction derived from outcomes 2 and 3. In order assure that students use proficiently the tools for applica- tion development, they required a development environment fully configured and, most importantly, with high availabil- ity (to promote off-class work and do not overload computer rooms). This type of environment was clearly beyond the reach of the institutional LMS, and therefore, the possibil- ity of observing the interaction with these tools was initially non-existent.  The adopted solution was based on the use of a virtual machine. Lately, virtualization has been considered in edu- cation in order to easily facilitate students fully configured machines that can execute with barely any configuration steps in their personal computers [9]. The use of this this ap- proach had several benefits. First, all students had initially the same exact set of tools properly configured which greatly simplified the design of activities to use them. Second, the machine was configured so that students could access the  files stored in their regular personal computers. Third, the virtual machine (although including a fully configured op- erating system) was portrayed to the students as the appli- cation to use when working on the course material. And last, but most importantly, the machine included a system to record the events occurring with respect to the installed tools.  More precisely, the monitoring mechanism was capable of recording the following events:   Power-up and shutdown of the machine.   Invocation of a previously selected set of tools.   Internal commands used by the students in some of the development tools   Historic data about the sites visited with the included browser  With this mechanism, a wide variety of interaction events that otherwise would be ignored, were recorded and stored in the virtual machine.  2.2 Support for a shared folder The use of the virtual machine was combined with support  for a web-based folder shared among the team members in which students could store any files they needed related to the course. More precisely, a first folder was created for each pair of students during the first half of the course, and a second shared folder was created for the teams formed in the second half of the course.  Instructors had access to the shared folders of all the teams under their supervision. This addition turned to be a powerful communication channel not only among students, but also between students and instructors to solve problems, check errors, and generic consultations.  During the configuration phase of the virtual machine de- scribed in 2.1, the shared folder was configured as the repos- itory where all the recorded events were stored. A non- intrusive procedure would be in charge of sending the recor- ded events whenever the students submitted a new version of the files in the web-based folder.  In order to comply with the current legislation, the vir- tual machine was downloaded only by those students that agreed with the terms of use described in a document. Fur- thermore, the machine was configured to boot up with the browser open and showing a page explaining the recording mechanism, the steps to disable it, and the contact person to exercise the rights over the collected data (delete, query and modify). Figure 1 shows a screen capture of the initial desktop of the virtual machine.  2.3 Support for actuators With the configuration described in the previous session,  the virtual machine can be thought of as the application that students need to use when working on the course material. But being outside of the LMS not only poses a challenge to record and collect data, but also to the step of acting.  Although the experience is only at the first stages, and as such, only the data collection aspect has been deployed, a solution has been considered and configured to be able to act on this environment. A widget displaying the content of a pre-defined folder has been installed in the desktop of the machine. Initially, the folder contains the terms of use  165    Figure 1: Initial screen of the virtual machine  for the machine (that the student agreed to). The widget is shown in the upper left corner of the desktop illustrated in Figure 1. Using a technique similar to the one to send the recorded events, a new set of files can be uploaded to this special folder such that their corresponding icons appear in the desktop widget.  With the described configuration, students would see how the folder shown in their desktop keeps changing its con- tent. The type of resources that can be added range from files (documents, audio, video) to URLs to access remote resources.  2.4 Encoding the events The events recorded in the student machines offer a very  detailed account of the procedures followed as well as the tools that were invoked. The captured information has been encoded using the CAM (Contextualized Attention Meta- data) format [20, 22, 18]. CAM provides a data model for representing user activity together with contextual informa- tion. Educational application of the CAM framework are discussed in [19], where the tool CAMera for monitoring and reporting on learning behavior is described. CAMera collects usage metadata from diverse various applications, represent these metadata with CAM and reports them to the learner. More complex applications in the scope of adap- tation and web-semantics have also been built based on this format [16, 16].  3. INITIAL RESULTS The initial objective of this approach is to explore ways  to extend the data-gathering phase of learning analytics be- yond the LMS and into environments in which a significant amount of interaction is taking place. The initial conditions were also to deploy the data gathering in a scalable and non-intrusive way.  The virtual machine was made available at the begin- ning of the course. Out of the 248 students that signed out for the course, a total of 220 downloaded the machine  (88.71%). Out of the remaining 28 students (11.29%), most of them opted to use their own configured environment. The large percentage of students that decided to use the machine shows its acceptance as the course tool.  The number of downloads, though turned out to be not a good estimation of the true activity carried out by the students in those machines where the recording mechanism was not disabled. The events received in the first half of the course (in which students worked in all the activities in pairs) were 48, 342 for a total of 115 students (an average of 420 events per student).  3.1 Activity outside the LMS An important side-effect of placing the data-gathering phase  outside of the LMS and into a fully configured environment was to be able to measure the percentage of URLs that were related to the LMS. In other words, by exploring the events encoding a visit to a URL we can have a first look at the percentage of traffic that goes to other sites.  Out of the almost 49, 000 events, 15, 507 (32.07%) were events in which a URL was opened with the browser. When counting the number of unique URLs, this number falls down to 8, 669. Out of these, only 2, 471 (28.51%) pointed to the LMS. An initial interpretation (pending a more thorough analysis) seems to suggest that students interact with a large number of resources that are outside of the LMS.  4. DISCUSSION AND FUTURE WORK In this paper a context has been described in which in  order to assess the degree of interaction that students are having with a previously detected set of tools and among themselves, the LMS offers a very poor coverage. The con- text is derived from the adoption of learning outcomes that require procedural activities with tools and functionality be- yond the scope of a conventional LMS>  The described approach proposes extending the scope of the data-gathering techniques to include a fully configured virtual machine containing all the required tools as well as  166    a mechanism to record a subset of the most representative events. A detailed description of the terms of use of the machine with instructions on how to disable the recording mechanism, as well as how to check, modify or delete the information, was included with the machine. The machine is also configured to establish a bidirectional communication channel with a central server to send the recorded events and receive new resources that are shown in a desktop widget as actuators on the learning environment. The received data has been encoded using the CAM format and is being pre- pared to perform a more sophisticated algorithm to detect special patterns to detect early which students are not using properly the given tools.  The work is still in its preliminary stage in the sense that only the data-gathering stage has been successfully de- ployed. Still, the approach has been shown to be scalable (more than 200 students) and in-obtrusive (students do not sense that the events are being recorded). The obvious line for future work is to identify those variables of the recorded events are more suitable to make predictions of those stu- dents that are not using properly the tools included in the machine.  A second line of work has also been conceived to combine the recorded events and the information extracted from the LMS to detect potential anomalies in the collaborative part of the course. The challenges in this context are bigger be- cause teams are suppose to meet regularly on face-to-face meetings in which there is no type of event recording.  Acknowledgment Work partially funded by the Learn3 project, Plan Nacional de I+D+I TIN2008-05163/TSI, the Best Practice Network ICOPER (Grant No. ECP-2007-EDU-417007), the Accion Integrada Ref. DE2009-0051, and the Emadrid: Investi- gacion y desarrollo de tecnologas para el e-learning en la Comunidad de Madrid project (S2009/TIC-1650).  5. REFERENCES [1] Criteria for accrediting computing programs.  Technical report, ABET Accreditation Board for Engineering and Technology, 2007.  [2] A. R. Anaya and J. G. Boticario. Application of machine learning techniques to analyse student interactions and improve the collaboration process. Expert Systems with Applications, 38(2):11711181, Feb. 2011.  [3] T. Anderson. Getting the Mix Right Again: An updated and theoretical rationale for interaction Equivalency of Interaction. The International Review of Research in Open and Distance Learning, 4(2), 2003.  [4] K. Arnold. Signals: Applying Academic Analytics. EDUCAUSE Quarterly, 33(1):10, 2010.  [5] J. Campbell, P. DeBlois, and D. Oblinger. Academic Analytics: A New Tool for a New Era. Educause Review, 42(4):4057, 2007.  [6] M. Chatti, M. Jarke, and D. Frosch-Wilke. The future of e-learning: a shift to knowledge networking and social software. International journal of knowledge and learning, 3(4):404420, 2007.  [7] S. Dawson. Seeing the learning community: An exploration of the development of a resource for  monitoring online student networking. British Journal of Educational Technology, 41(5):736752, Sept. 2010.  [8] T. Elias. Learning Analytics : Definitions , Processes and Potential, 2011.  [9] A. Gaspar, S. Langevin, W. Armitage, and M. Rideout. March of the (virtual) machines: past, present, and future milestones in the adoption of virtualization in computing education. Journal of Computing Sciences in Colleges, 23(5):123132, 2008.  [10] P. Goldstein and R. Katz. Academic analytics: the uses of management information and technology in higher education, chapter 1, pages 112. Educause, 2005.  [11] J. Jovanovic, D. Gasevic, C. Brooks, V. Devedzic, M. Hatala, T. Eap, and G. Richards. Using Semantic Web technologies to analyze learning content. IEEE Internet Computing, 11(5):4553, 2007.  [12] R. Light. Making the most of college: Students speak their minds. Harvard Univ Pr, 2001.  [13] L. P. Macfadyen and S. Dawson. Mining LMS data to develop an early warning system for educators: A proof of concept. Computers & Education, 54(2):588599, Feb. 2010.  [14] R. Mazza. A graphical tool for monitoring the usage of modules in course management systems. In L. 4370, editor, Visual Information Expert Workshop, VIEW 2006, pages 164172. Springer, 2006.  [15] R. Mazza and V. Dimitrova. Visualising student tracking data to support instructors in web-based distance education. In Proceedings of the 13th international World Wide Web conference on Alternate track papers & posters, pages 154161. ACM, 2004.  [16] P. J. Munoz Merino, A. Pardo, C. D. Kloos, M. Munoz organero, M. Wolpers, K. Niemann, and S. Augustin. CAM in the Semantic Web World. In International Conference on Semantic Systems, page Accepted as Poster, 2010.  [17] D. A. Norma and J. C. Spohrer. Learner-centered education. Communications of the ACM, 39(4):2427, Jan. 1996.  [18] M. Scheffel, M. Friedrich, M. Jahn, U. Kirschenmann, K. Niemann, H. Schmitz, and M. Wolpers. Self-monitoring for Computer Users. Informatik, 2009.  [19] H. Schmitz, M. Scheffel, M. Friedrich, M. Jahn, K. Niemann, and M. Wolpers. CAMera for PLE. Learning in the Synergy of Multiple Disciplines, pages 507520, 2009.  [20] H.-C. Schmitz, M. Wolpers, U. Kirschenmann, and K. Niemann. Contextualized Attention Metadata, volume 13, chapter 8. Cambridge University Press, Sept. 2007.  [21] J. Waycott, S. Bennett, G. Kennedy, B. Dalgarno, and K. Gray. Digital divides Student and staff perceptions of information and communication technologies. Computers & Education, 54(4):12021211, May 2010.  [22] M. Wolpers, J. Najjar, K. Verbert, and E. Duval. Tracking actual usage: the attention metadata approach. Journal of Technology Education & Society, 10(3):106121, 2007.  167      "}
{"index":{"_id":"20"}}
{"datatype":"inproceedings","key":"Bakharia:2011:SBV:2090116.2090144","author":"Bakharia, Aneesha and Dawson, Shane","title":"SNAPP: A Bird'S-eye View of Temporal Participant Interaction","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"168--173","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090144","doi":"10.1145/2090116.2090144","acmid":"2090144","publisher":"ACM","address":"New York, NY, USA","keywords":"analytics, computer supported collaborative learning, discussion forum, evaluation, graph theory, inferring social networks, learning analytics, learning management system, network learning, social network analysis, visualisation","Abstract":"Distance education courses have a tradition of a formative evaluation cycle that takes place before a course is formally delivered. This paper discusses opportunities for improving online and blended learning by collecting formative data during course presentation. With a goal of overall improvement in instructional effectiveness and identification of promising practices for inclusion in a learning activities design library, we propose the immediate and on-going monitoring of the effectiveness of learning activities, tutor facilitation and learner satisfaction during the course presentation. This has implications for constructively involving the learners and facilitators in the course improvement process. While originally conceived to reduce the time for pilot evaluation of new courses and learning activities, the proposed system could also be extended to individualized and blended learning environments, and if implemented using semantic web technologies, for research into the effectiveness of learning activity ","pdf":"SNAPP: A Birds-Eye View of Temporal Participant  Interaction     AneeshaBakharia  Centre for Educational Innovation   and Technology, University of Queensland  Australia   aneesha.bakharia@gmail.com        Shane Dawson  Faculty of Arts, University of   British Columbia, Canada   Faculty of Education, University   of Wollongong, Australia   shane.dawson@ubc.ca     ABSTRACT  The Social Networks Adapting Pedagogical Practice (SNAPP)  tool was developed to provide instructors with the capacity to  visualise the evolution of participant relationships within  discussions forums. Providing forum facilitators with access to  these forms of data visualisations and social network metrics in  real-time, allows emergent interaction patterns to be analysed  and interventions to be undertaken as required. SNAPP essentially  serves as an interaction diagnostic tool that assists in bringing the  affordances of real-time social network analysis to fruition. This  paper details the functional features included in SNAPP 2.0 and  how they relate to learning activity intent and participant  monitoring. SNAPP 2.0 includes the ability to view the evolution  of participant interaction over time and annotate key events that  occur along this timeline. This feature is useful in terms of  monitoring network evolution and evaluating the impact of  intervention strategies on student engagement and connectivity.  SNAPP currently supports discussion forums found in popular  commercial and open source Learning Management Systems  (LMS) such as Blackboard, Desire2Learn and Moodle and works  in both Internet Explorer and Firefox.   Categories and Subject Descriptors  H.3.4 [Social Networking]: Social network analysis.   General Terms  Social network analysis, analytics, network learning, learning  management system, graph theory, visualisation, computer  supported collaborative learning, evaluation, discussion forum.   Keywords  Learning Analytics, Inferring social networks.   1. INTRODUCTION  Socio-constructivist theorists advocate for the use of collaborative  learning activities as a process for promoting student  understanding. Although traditionally, collaborative learning  activities were conducted in on-campus settings, there has been an  increasing shift for these practices to be facilitated through the  online context. This has been driven, in part, by increasing student  diversity and a demand for greater course flexibility. As such, the   adoption of online learning activities for both on and off campus  instruction is now commonplace within contemporary education  practice.    At many universities the Learning Management System (LMS) is  seen as the primary vehicle for enabling online collaborative  learning activities. Commercial and open source LMS such as  Blackboard, Desire2Learn, Sakai and Moodle are generally  centrally run systems that are made available to faculty. Despite  the vast array of pedagogical benefits these technologies bring,  teachers frequently note that the online environment lacks the  student learning cues that are readily obtained in more traditional  modes of education delivery (face to face). For instance, the  classroom cues that assist teachers in identifying which students  require support, are actively engaged or have cognitively  checked out of the learning activities. These types of formative  feedback mechanisms are critical for instructors to better adapt the  flow, language, and structure of the lesson in real-time to  maintain engagement and better promote understanding. While  there exists a vast array of learning cues in the LMS and other  integrated student online systems, there is a disconnect between  the student tracking and reporting processes and subsequent  instructor pedagogical interpretations.   Instructors and tutors require the ability to gauge student activity  and interaction so that they are able to better optimize and adapt  the learning activities that take place within the online  collaborative learning environment. However, current versions of  LMS tend to only contain basic interaction statistics such as the  number of times a page was viewed, or the number of messages  posted or read. These forms of basic content access statistics do  not provide the necessary evidence of learner participation and  engagement within the learning network. Thus, there are minimal  informed opportunities for instructors to identify students  requiring learning support, at risk of attrition, or dis-engaged from  the learning network. The development of rich student  participation and interaction data married with effective and easily  interpretable visualization processes, are critical to aid online  instructors in their teaching tasks. In this context, Social Network  Analysis (SNA) provides a framework for merging both complex  group and individual data sets with easily interpretable visual  representations.   Numerous case studies have demonstrated the value of SNA as a  means to assess participant interaction within online learning  environments. For instance, case studies conducted by De Laat  and Aviv et al.,  used SNA to address research questions related to  the level of participant activity, identification of central  participants and network density. SNA has also been used to assist  instructors in identifying isolated students , provide evidence of  group cohesion  and creativity . While there has been increasing      Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.   Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.     168    applications of SNA for educational purposes, the full potential of  SNA for providing real-time data remain largely un-realised.  This is especially pertinent for online collaborative learning  environments .   The importance for providing instructors with real-time data  related to relationship establishment is critical to inform  instructors of any necessary modifications to activities to better  scaffold learning content  and promote diversity of engagement.  There is strong evidence to suggest that within well-structured  activities, knowledge construction processes reach higher levels of  critical thinking and that students are able to establish and sustain  cohesive groups. This evidence serves to substantiate the need for  automated SNA tools that are able to deliver real-time  analytics for tutors and instructors.    The Social Networks Adapting Pedagogical Practice (SNAPP)  tool aims to deliver real-time social network analysis and  visualisation within LMS discussion forums. The discussion  forum is one of the most frequently used collaborative tools  within an LMS. The manner in which forum threads are displayed  however, makes it difficult for instructors to perceive  conversation dynamics, and determine whether participants are  actively engaged or merely peripheral on-lookers. The first  version of SNAPP released in 2008 extracted post-reply data from  forum threads, inferred relationships and allowed the SNA data to  be exported for further analysis in tools such as NetDraw. During  this time, many instructors that used SNAPP were found to import  the SNA data into NetDraw for the purpose of scaling nodes and  edges according to post and interaction frequency for display on a  sociogram. SNAPP proved useful but numerous technical steps  were also involved in the export and import of data. In 2009, the  Australian Learning and Teaching Council (ALTC) funded a  research grant to further research and develop SNAPP for the  higher education context. This lead to the development of SNAPP  1.5 which embedded an interactive sociogram within an LMS  based discussion forum. SNAPP 1.5 however, only stored the  aggregate of interactions between participants up to the date the  analysis was performed. During trials of SNAPP 1.5, one of the  key features that instructors requested was the ability to filter the  social graph by date. SNAPP 2.0 includes the ability to display  animations of network evolution. This is achieved by storing the  date of each participants interaction. The interactive sociogram  serves as an alternate representation of the threaded forum that  provides insight into participant interaction, emerging patterns and  network density. The SNAPP interface is illustrated in Figure 1.  SNAPP includes controls that allow the user to interactively filter  the sociogram visualisation by date and relationship strength.  Node size and edge widths can also be scaled according to post  frequency and relationship strength accordingly. The Java  Universal Network Framework (Jung) library is used to render the  sociograms.   2. IMPLEMENTING SNAPP AS A CLIENT- SIDE BROWSER EXTENSION  There are a diverse set of challenges confronting the development  of any analytic solutions for learning and teaching practice. For  instance, ensuring that any technical solution is cross-platform,  cross browser and compatible with the myriad of LMS available.  The LMS landscape comprises both commercial vendors such as  Blackboard (including WebCT) and Desire2Learn and open  source products such as Moodle and Sakai. Unfortunately, to date  there is no common server-side development platform in existence  that affords any extensions to target all systems uniformly. The   introduction of the Learning Tools Interoperability (LTI)  standards may begin to address this issue. However, LTI adoption  among vendors and developers is still in its infancy. Presently,  each LMS has their own Application Programmable Interface  (API), extension framework and is developed in a specific  programming language. As the initial version of SNAPP was  required to extract social network data for comparative analyses  across multiple universities, each using a different mandated  LMS, cross-LMS integration was a critical feature to address.   Although, the various LMS extension frameworks allow for new  tools to be added, they do not necessarily provide for existing  LMS tools to be enhanced. For instance, developing  enhancements for LMS based discussion forums or synchronous  chat. As a result, developers will often focus on providing an  extension in lieu of a direct code modification. Although, this  context may differ for the Open Source LMS, there are  disadvantages to any code modification in terms of the ability to  upgrade to later versions. Thus, a unique feature of SNAPP is the  manner in which it seamlessly integrates with the discussion  forum and provides the user with interactive sociograms as a  visual representation of the student interactions and group  dynamics emerging from the implemented learning activities.  Without this direct discussion forum tool enhancement, SNAPPs  usefulness as an embedded real-time diagnostic tool would be  lost.   The SNAPP design team sought to address these development  challenges by using client-side browser based techniques. The  bookmarklet technique was chosen because it worked in multiple  Web browsers, enabled forum data extraction from multiple LMS  and allowed the sociogram visualisation to be embedded directly  within a forum. Other client-side techniques include native  browser extensions and GreaseMonkey userscripts. Both of these  techniques however, require administrator access for installation   a privilege not afforded to all academics at universities that  mandate the use of a Standard Operating Environment (SOE).  Bookmarklet installation merely requires a link to be dragged on  to a toolbar (Firefox) or added to a favourites list (Internet  Explorer). Thus, the use of the bookmarklet for dissemination, and  adoption is not limited or impeded by end-user IT related  permissions.   3. FORUM DATA EXTRACTION   SNAPP infers participant relationships from the post-reply data.  Discussion forum threads are stored in a database table with each  row containing all of the information related to a single participant  post. The post title, description, author and date are stored in the  database. If the post is not the commencement point of a thread, a  reference back to the parent thread in the form of a unique  identifier is also stored. Forum posts are essentially stored in this  hierarchical manner so that the parent-child relationship between  posts and replies can be captured and displayed visually as a  threaded tree of messages. Retrieving this information directly  from a database using a series of SQL queries is a routine task.  However, at the time of SNAPPs conceptualization, not all LMS  vendors made available the captured student interaction data via  an API or Web service. However, all LMS do display the required  network interaction data when a forum is displayed as a threaded  tree with indents inserted to structure the hierarchy of posts and  replies. SNAPP uses the Javascript client-side scripting language  to retrieve the post-reply data from the threaded tree view of the  forum. SNAPP uses the attributes of each post, including the  author and date, to produce sociogram and social network metrics  within any specified timeframe.    169    The sociogram produced is in fact an alternate visual  representation of the activity in a forum. The threaded forum tree  view, displays interaction in chronological order but it is difficult  for instructors to rapidly gauge the strength and diversity of the  relationships evolving between participants. Recent research  involving SNAPP indicates that the use of visualisations such as  sociograms provides an easily interpretable interface for  instructors . These additional pedagogical insights were  previously, neither easily obtained nor obvious from the  discussion forum view or student data tracking tables. The  sociogram is not a replacement for the threaded tree view. SNAPP  is a complementary tool that further aids the analysis and  interpretation of the captured interactions and observed social  patterns.   4. MAPPING SNAPP FEATURES TO  PEDAGOGICAL INTENT  Although the current suite of LMS include student tracking data,  business intelligence or learning analytics functionality, the  captured and reported data are often presented to instructors in a  complex format that is isolated from the specific learning context.  The poor reporting and visualization techniques associated with  current LMS has resulted in minimal uptake of the included  tracking and reporting tools . Broadly speaking, the available  analytical tools are most likely to be used by University  Administrators seeking information related to adoption rates for  return on investment analyses or institutional technology reviews.  The translation of interaction data from analysis to informed  pedagogical action is for the vast majority of teaching faculty, a  complex and potentially labour intensive process . The following  sections illustrate how SNAPP attempts to reveal the underlying  pedagogical response and action to observed patterns of behaviour  evolving from the student interactions.   4.1 Identification of Isolated and Highly  Interactive Participants  There is an observed correlation between an individuals  connectivity to peers and their overall academic success. As  numerous commentators have previously noted, in the world of  online learning, attrition rates are frequently reported as higher  than their on-campus counterparts. This has in part, been  attributed to a lack of connectivity that is both social and  academic, with fellow learners and the institution. Thus, any aids  that can be afforded to forum facilitators to more accurately  identify students that have not connected or have dis-engaged  with the learning network early in their academic study, may  assist with addressing concerns related to online attrition.    SNAPP has been developed to provide forum facilitators with  rapid and easy identification of a participants overall level of  engagement with the student learning network. In this instance,  early identification can provide an opportunity for instructors to  intervene before students become disenfranchised with the  learning process. Isolated students in SNAPP appear as nodes  with no connections. This indicates that the participant has  submitted a post but no other participants have responded. It can  be difficult to identify participants at either end of the interaction  spectrum on sociograms especially if the forum contains a large  number of users. SNAPP therefore, provides the capacity for users  to filter nodes based upon the number of posts. Filtering removes  nodes above or below a threshold value. Filtering nodes above a  threshold value reveals the participants that are central to a  discussion. SNAPP scales nodes based upon the number of posts  made by a participant. Connections between nodes are also   weighted according to the number posts and replies made between  the participants. The reciprocal directionality of the interaction  can also be interrogated by passing a mouse over a connection.   4.2 Identification of Patterns and Structural  Holes  SNAPP includes various graph layout algorithms to help users  discover and interpret emerging network structures. The  Fruchterman-Reingold, Kamada-Kawai, Spring and Circle layout  algorithms are included to assist with the detection of network  patterns for alignment with the teaching intent. For instance, the  facilitator-centric pattern  arises when there are direct interactions  between individual students and facilitator with minimal student  to student activity (Figure 1). This pattern has a distinctive star  shape and would commonly occur in an FAQ style forum, where  the facilitator directly responds to student queries. However, if the  intent of the forum is to promote knowledge sharing and  construction among participants, then the emergence of the  facilitator-centric pattern would be interpreted as undesirable.  Identification of this pattern early in the course provides an  opportunity for facilitators to introduce alternate learning  interventions and then monitor any changes in network  composition.         Figure 1. Facilitator centric pattern (Actor names removed).      The presence of structural holes within a network indicates the  development of actor sub-groups or cliques. The development of  sub-groups may indicate strong bonding among a core set of  students. However, the formation of these strong cliques can be to  the detriment of other actors attempting to engage. The formation  of these groups can also limit the diversity of engagement with  peers. For instance, Dawson observed that in large class forums  students will form cliques based on perceived academic potential.  In essence, high performing students flocked to other high  performers to the exclusion of all other potential participants. In  these situations, an effective strategy may include assigning  participants to new groups and establishing additional group  specific discussion forums. Another strategy is to encourage  participants to interact across multiple cliques (i.e. bridge  structural holes) to foster intergroup idea sharing.   170    4.3 Ego Network Exploration  A sociogram provides a visual representation of the relational  interconnectivity across the entire network and highlights where  dense, reciprocal and transitive connections exist. Within large  participant cohorts it becomes difficult to gauge the relationship  strength and reciprocality at an individual level by looking the  sociogram as a whole. As much as Instructors require the capacity  to view the entire social network, they also require the ability to  analyse the social structures that surround a participant. This is  commonly referred to as ego network analysis. An ego network  consists of the selected actor and includes all other actors directly  linked and their associated relationships with other connected  participants. SNAPP incorporates functionality to aid with the  exploration of ego networks. Clicking on a participants node in  SNAPP highlights all of the nodes that comprise the immediate  ego network (Figure 2).      Figure 2. Ego-network illustrating highlighted nodes (Actor   names removed).      Ego networks with several strong ties are often considered to be  homophilous in nature. In these instances, actors with strong  relationship ties frequently share common attributes or interests.  While strong ties promote a sense of community they can also  reduce the degree of diversity an individual is exposed to. In  certain educational contexts (collaborative learning), the  development of a heterogenous network is more in line with the  overall pedagogical intent. Heterogeneous networks tend to  promote a greater number of weak ties and therefore increased  access to knowledge and resources. Weak ties are shown to  introduce novel knowledge into the network while participants  that share strong ties usually have access to the same information .  Thus, it can be viewed as advantageous to embed specific learning  activities that directly foster interaction between participants from  different discipline areas and groups e.g. medical, dental and  nursing students enrolled in a mandatory clinical ethics course, to  promote the development of weak ties.   Ego network analysis also provides an effective means to evaluate  the role an instructor or tutor plays in a network. A sociogram is  able to reveal whether an instructor or tutor is central or peripheral  to the network, but with ego analysis the types of students that an  instructor or tutor interacts with is able to be evaluated. In a study  conducted by Dawson, tutors primarily interacted with high  performing students despite isolated and low performing students  making several unrequited posts. It is common for facilitators to   be a necessarily central and dominate actor in the network at the  start of a course to establish relationships and promote  communication exchanges. However, as the course unfolds the  facilitator will gradually move to the periphery of the network to  play a more mediating role.   4.4 Monitoring Network Evolution and  Discussion Continuity  In the first release of SNAPP, post-reply relationships between  participants were aggregated with no date information stored. This  limited the sociogram to be a representation of the network, at the  last time participants were active in the forum. The temporal  nature of interaction was therefore lost. In SNAPP 2.0, individual  post-reply interactions are stored with their relevant date and time  stamps. In terms of functionality this means that users can filter  activity by date and view the resulting visualisation and social  network metrics at a specified point in time. An animated view of  network evolution is also incorporated to allow for the  identification of events leading to the emergence of an interaction  pattern. All of the available graph layout algorithms can be  applied during the playback of network evolution to aid with the  discovery of patterns.    The storage of temporal interaction data also enables the analysis  of forum activity over time. Post frequency distribution over the  duration of forum activity can be plotted to a graph. This assists  facilitators in identifying periods of increased forum activity and  in determining the events (e.g. examination period) and  interventions responsible for triggering the activity.   SNAPP has been developed to aid with the analysis and  interpretation of interaction patterns as a course progresses and  class relationships and interactions form. For example, SNAPP  can be triggered at any time to display a visualisation of current  participant interaction  based on these early analyses alternate  interventions can be designed to engineer more pedagogically  desirable user engagement activity. SNAPP promotes the use of  SNA as a real-time diagnostic tool providing instructors with the  insight they require to moderate a forum effectively. Included in  SNAPP 2.0 is the ability for instructors to document intervention  strategies using the annotations functionality. Annotations are  textual descriptions that are stored with a date and time stamp.  These annotations are potentially useful for instructors when for  reflecting on the impact of implemented moderation strategies.   4.5 Evaluating and Comparing Multiple  Forums   Multiple forums are often used over the duration of a course to  address a variety of learning objectives. Individual forums are set  up to cover different topics and cater for diverse learning needs.  Forums may also be setup to encourage online collaboration  within groups where access to the forum is restricted to group  members. SNAPP is able to conduct analyses across multiple  forums as well as individual forums. Viewing the resulting social  network visualisation of all activity over the duration of the course  provides a high level indication of the network depth, relationship  strength and also allows for the identification of central and  peripheral participants. The position of instructors and tutors  within the resulting network pattern is also crucial to the  evaluation. These forms of analyses address questions relating to  the interaction characteristics of the facilitators (e.g. a central or  peripheral role). Additionally, the analysis of multiple forums  provides an indication of which moderation techniques were  successful and how these techniques can be improved.   171    The analysis of where and how students interact in various class  forums can reveal much about an individuals motivations and  learning preferences. For example, Dawson, et al.,  observed that  students were predisposed to contribute to either learning or  administrative focused forums depending on their individual  learning disposition. Additionally, in terms of class, and small  group work contributions, Marcos-Garcfa et al., compared student  interaction within generic whole of class forums and small group  specific forums. The authors noted that while a sub-set of students  were able to make significant contributions at the class level, they  failed to initiate any interaction within the small group work  forum. In these cases the students interactions in the small group  work forums were limited as a result of personality conflicts and  general dissent. The capacity to monitor both individual and  multiple forums provides instructors with a more holistic picture  of both the broader class structures and individual student  learning.   4.6 Future Directions and Conclusion  Although, SNAPP provides a valuable analytical resource, there  remain numerous areas for further development and expansion.  SNAPP currently performs social network analysis and produces  easily interpreted visualisations of discussion forum activity.  However, learning activity design is not an isolated process. The  online and offline learning design process involves the integration  and coordination of multiple tools both collaborative and  individual (for example, assessment, synchronous discussions,  academic literacy). In recent years commercial and open source  Learning Management Systems have begun to introduce  additional tools such as blogs and wikis either as native  applications or extensions. Future releases of SNAPP require the  ability to perform analysis within these additional applications as  well as across the broad range of collaborative tools that are used  within an online learning activity. The future development of  learning analytic applications broadly should be guided by an  imperative to aggregate from diverse data sets. For instance  SNAPP will commence the incorporation of algorithms that are  able to infer social relationships originating from blog  commenting as well as the knowledge co-construction that occurs  within collaborative editing environments such as a wiki.    In addition to social network analysis and visualisation, SNAPP  also provides basic metrics of individual participation in the form  of the number of posts submitted. Connection strength is currently  an aggregate of the number of times participants have actively  responded to each other. Passive participation also occurs within a  discussion forum where participants read or browse messages but  dont respond. The incorporation of passive activity will allow  lurkers to be identified and help instructors to compare active and  passive participation. Passive participation within forums is not  currently tracked within many LMS. There is however scope to  implement such tracking within open source systems, Moodle  being a prime candidate. SNAPP does not analyse message  content and as a result neglects to incorporate references made to  other participants within text. The use of computational linguistic  techniques such as Named Entity and Anaphoric Resolution need  to be incorporated to further improve the accuracy of the inferred  social structure.    Due to the complex nature of interactions that occur within  collaborative learning environments there is a need for  implementing multiple learning evaluation techniques . In this  context, there have been numerous frameworks proposed for  evaluating computer supported collaborative learning. In  particular, Weinberger, et. al,  described a multi-dimensional   framework involving - participation, epistemic, argumentative and  social mode dimensions. While SNAPP produces visualisations  and metrics to assist with the evaluation within the participation  and social mode dimensions, it presently lacks analytics specific  to the epistemic and argumentative dimensions. These later  elements can only begin to be addressed through content analysis.  In essence, evaluations of the perceived quality of the discussion  are frequently overlooked as a result of the labour intensive nature  of the process. It is therefore important for automated content  analysis techniques to be incorporated within learning analytic  tools. The merging of SNA techniques with automated content  analysis will provide instructors with a more complete assessment  of the individual and group dynamics evolving from the  implemented learning design.   5. REFERENCES  [1] Aviv, R., Z. Erlich, G. Ravid, and A. Geva, (2003).Network   analysis of knowledge construction in asynchronous learning   networks. Journal of Asynchronous Learning Networks. 7(3):  p. 1-23.   [2] Bakharia, A., E. Heathcote, and S. Dawson, Social network  adapting pedagogical practice: SNAPP, in ASCILITE 2009:  Same places, different spaces. 2009, ASCILITE: Auckland.   [3] Borgatti, S.P., (2002), NetDraw: Graph visualization  software. 2002, Harvard: Analytic Technologies.   [4] Brooks, C., W. Liu, C. Hansen, G. McCalla, and J. Greer,  (2007). Making Sense of Complex Learner Data. in  Workshop of Assessment of Group and Individual Learning   Through Intelligent Visualization (AGILEeViz). AIED 2007.  [5] Dawson, S. and E. McWilliam, Investigating the application   of IT generated data as an indicator of learning and teaching   performance. 2008, Australian Learning and Teaching  Council: Canberra.   [6] Dawson, S., E. McWilliam, and G. Poole, (2008).  Monitoring student creative capacity: Using network   visualisation to evaluate pedagogical practice. in Creating  value between commerce and commons. Brisbane, Australia:  Centre for Excellence for Creative Industries and Innovation.   [7] Dawson, S., E. McWilliam, and J. Tan, (2008). Teaching  Smarter: How mining ICT data can inform and improve   learning and teaching practice. in Hello where are you in the  landscape of educational technology. ASCILITE 2008.  Melbourne, Australia.   [8] Dawson, S., L. Macfadyen, and L. Lockyer, (2009). Learning  or performance: Predicting drivers of student motivation. in  Ascilite 2009 Conference: Same places, different spaces.  Auckland, New Zealand.   [9] Dawson, S., (2010).'Seeing' the learning community: An  exploration of the development of a resource for monitoring   online student networking. British Journal of Educational  Technology. 41(5): p. 736752.   [10] Dawson, S., A. Bakharia, and E. Heathcote, (2010). SNAPP:  Realising the affordances of real-time SNA within networked   learning environments. in Networked Learning Conference  2010. Aalborg, Denmark.   [11] Dawson, S., A. Bakharia, L. Lockyer, and E. Heathcote,  'Seeing' Networks: Visualising and evaluating student   learning networks. 2010: Canberra.  [12] Dawson, S., E. Heathcote, and G. Poole, (2010).Harnessing   ICT potential: The adoption and analysis of ICT systems for   enhancing the student learning experience. International  Journal of Educational Management. 24(2): p. 116-128.   172    [13] De Laat, M., (2002). Network and content analysis in an  online community discourse. in Computer-Supported  Collaborative Learning. Boulder, Colorado.   [14] De Laat, M., V. Lally, L. Lipponen, and R.J. Simons,  (2006).Analysing student engagement with learning and  tutoring activities in networked learning communities: a   multi-method approach. International Journal of Web Based  Communities. 2(4): p. 394-412.   [15] Dillenbourg, P., M. Baker, A. Blaye, and C. O'Malley,  (1996). The evolution of research on collaborative learning,  in Learning in Humans and Machine: Towards an  interdisciplinary learning science., E. Spada and P. Reiman,  Editors. Elsevier: Oxford. p. 189-211.   [16] Fischer, F., J. Bruhn, C. Grasel, and H. Mandl,  (2002).Fostering collaborative knowledge construction with  visualization tools. Learning and Instruction. 12(2): p. 213- 232.   [17] Garrison, D.R. and T. Anderson, (2003), E-learning in the  21st century: A framework for research and practice. 2003,  London and New York: RoutledgeFalmer.   [18] Granovetter, M., (1983).The strength of weak ties: A network  theory revisited. Sociological theory. 1(1): p. 201-233.   [19] Gunawardena, C.N., C.A. Lowe, and T. Anderson,  (1998).Analysis of a global online debate and the  development of an interaction analysis model for examining   social construction of knowledge in computer conferencing.  Journal of Educational Computing Research. 17(4): p. 397- 431.   [20] Haythornthwaite, C. and M. Twidale, Visualization of  Conversationally Constructed Social Networks, in CHI 2002.  2002, Citeseer: Minneapolis, MN.   [21] Henri, F., (1992).Computer conferencing and content  analysis. Collaborative learning through computer  conferencing: The Najaden Papers. 90: p. 117-136.   [22] Hmelo-Silver, C.E., (2003).Analyzing collaborative  knowledge construction: multiple methods for integrated   understanding. Computers & Education. 41(4): p. 397-420.  [23] Lapadat, J., (2002).Written interaction: A key component in   online learning. Journal of Computer Mediated  Communication. 7(4).   [24] Light, R.J., (2001), Making the most of college: Students  speak their minds. 2001, Cambridge, Mass.: Harvard  University Press.   [25] Macfadyen, L. and S. Dawson, (2010).Mining LMS data to  develop an early warning system for educators: A proof of   concept. Computers & Education. 54(2): p. 588-599.  [26] Marcos-Garcfa, J.A., A. Martinez-Mones, Y. Dimitriadis, R.   Anguita-Martinez, I. Ruiz-Requies, and B. Rubia-Avi,   (2009). Detecting and Solving Negative Situations in Real  CSCL Experiences with a Role-Based Interaction Analysis   Approach., in Intelligent Collaborative E-Learning Systems  and Applications T. Daradoumis, S. Caball, and J.M.  Marqus, Editors. Springer: Berlin, Germany.   [27] Martinez, M. (2003). High Attrition Rates in e-Learning:  Challenges, Predictors and Solutions. The eLearning  Developers Journal  2003  [cited 2009 3 July]; Available  from: http://www.elearningguild.com/pdf/2/071403MGT- L.pdf.   [28] McDonald, B., N. Noakes, B. Stuckey, and S. Nyrop, (2005).  Breaking down learner isolation: How social network   analysis informs design and facilitation for online learning.  in AERA. Montreal, Canada.   [29] Picciano, A.G., (2002).Beyond student perceptions: Issues of  interaction, presence and performance in an online course.  JALN. 6(1): p. 21-40.   [30] Pozzi, F., S. Manca, D. Persico, and L. Sarti, (2007).A  general framework for tracking and analysing learning   processes in computer-supported collaborative learning   environments. Innovations in Education and Teaching  International. 44(2): p. 169-179.   [31] Reffay, C. and T. Chanier, How social network analysis can  help to measure cohesion in collaborative distance learning.  2003. p. 343-352.   [32] Rovai, A.P. (2002). Building sense of community at a  distance. International Review of Research in Open and  Distance Learning  2002  [cited 2005 25 January]; Available  from: http://www.irrodl.org/content/v3.1/rovai.html.   [33] Tinto, V., (1993), Leaving college: rethinking the causes and  cures of student attrition. 2nd ed. 1993, Chicago: University  of Chicago Press.   [34] Weinberger, A. and F. Fischer, (2006).A framework to  analyze argumentative knowledge construction in computer-  supported collaborative learning. Computers & Education.  46(1): p. 71-95.   [35] Welser, T., E. Gleave, D. Fisher, and M. Smith,  (2007).Visualizing the signatures of social roles in online  discussion groups. The Journal of Social Structure. 8(2).   [36] White, S., J. OMadadhain, D. Fisher, and Y.B. Boey.  (2004). JUNGJava Universal Network/graph Framework.   2004; Available from: http://jung.sourceforge.net/.   [37] Willging, P.A., (2005).Using Social Network Analysis  Techniques to Examine Online Interactions. US-China  Education Review. 2(9).                    173      "}
{"index":{"_id":"21"}}
{"datatype":"inproceedings","key":"Graf:2011:ATA:2090116.2090145","author":"Graf, Sabine and Ives, Cindy and Rahman, Nazim and Ferri, Arnold","title":"AAT: A Tool for Accessing and Analysing Students' Behaviour Data in Learning Systems","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"174--179","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090145","doi":"10.1145/2090116.2090145","acmid":"2090145","publisher":"ACM","address":"New York, NY, USA","keywords":"academic analytics, data extraction and analysis, learning systems","Abstract":"Most of the current learning analytic techniques have as starting point the data recorded by Learning Management Systems (LMS) about the interactions of the students with the platform and among themselves. But there is a tendency on students to rely less on the functionality offered by the LMS and use more applications that are freely available on the net. This situation is magnified in studies in which students need to interact with a set of tools that are easily installed on their personal computers. This paper shows an approach using Virtual Machines by which a set of events occurring outside of the LMS are recorded and sent to a central server in a scalable and unobtrusive manner.","pdf":"AAT  A Tool for Accessing and Analysing Students  Behaviour Data in Learning Systems   Sabine Graf  School of Computing and Information   Systems,   Athabasca University, Canada   +1 (780) 752-6836  sabineg@athabascau.ca  Cindy Ives, Nazim Rahman  Centre for Learning Design and   Development, Athabasca University, Canada   +1 (780) 675-6957  cindyi@athabascau.ca,   nrahman@athabascau.ca  Arnold Ferri  Project Management Office,    Athabasca University, Canada  +1 (780) 421-5866   aferri@athabascau.ca  ABSTRACT In online learning environments, teachers and course designers  often get little feedback about how students actually interact with  and learn in online courses. Most of the learning systems used by  educational institutions store comprehensive log data associated  with students behaviours and actions. However, these systems  typically reveal or report on very general and limited information  based on this data. In order to provide teachers and course  designers with more detailed and meaningful information about  students behaviour and their use of learning resources within  online courses, an analytics tool has been developed. The tool  incorporates functionality to access and analyse data related to  students behaviours in learning systems. This tool can provide  valuable information about students learning processes allowing  the identification of difficult or inappropriate learning material,  and can therefore significantly contribute to the design of  improved student support activities and resources.     Categories and Subject Descriptors H.4.2 [Information Systems Applications]: Types of Systems   decision support.  K.3.m [Computers and Education]: Miscellaneous.   General Terms Design, Human Factors.   Keywords Academic analytics, learning systems, data extraction and  analysis.   1. INTRODUCTION Educational institutions need ways of responding to internal and  external pressures for accountability. Designers of instruction  need feedback about how successfully the teaching materials and  learning activities support student success. Academic analytics is  a relatively recent response to both these requirements in higher  education. According to EDUCAUSE, analytics marries large  data sets, statistical techniques, and predictive modeling [1] to   better understand the wealth of data produced by interactions and  transactions in organizational systems with a view to informing  action. Campbell et al. [1] describe how the techniques and tools  of early institutional-level analytics efforts in administrative areas  such as enrolment management and fundraising have evolved to  include analyses of factors that support student learning and  success. Administrative systems, registration systems and learning  management systems (LMS) together provide large amounts of  data that can be combined to provide understanding of student  engagement and performance. Advanced data analysis skills,  integrated information systems and multifunctional collaborative  teams are necessary to extract and interpret the evidence for  decision making in various academic areas. For example, LMS  data reveal student effort measures through participation in  discussions, time on task, quiz results, and log files that register  click patterns. These and other data can be combined to inform  the development of a variety of interventions  from simple early  alert systems to customized learning environments to personal  learning plans. Some of the factors for success of academic  analytic projects include careful attention to ethical and privacy  issues, stewardship for systemic implementation plans, attention  to faculty perspectives, broad-based collaboration and information  sharing, and adequate resources and skills.   Universities that have reported on their academic analytic projects  include McGill University in Canada [2], University of Fairfax in  the United States [3], and the University of Cordoba in Spain [4].  In 2003, a cross-functional team reporting to McGills Chief  Information Officer used the data from one LMS to understand  student and faculty preferences in order to establish criteria for  the choice of a replacement LMS [2]. In 2009, a team in the  Office of the Dean of Academic Affairs at Fairfax used the  community of inquiry model [5] to interpret patterns of faculty  interactions with students that were extracted from the enterprise  course management system with goals of assessing the  relationships between student satisfaction and instructor  involvement, and of preparing for a long-term trend analysis  project. In 2008, researchers at Cordoba reported on their data  mining project in Moodle [6], describing the emerging  discipline as a way of combining complex student usage data  and applying the results to elearning problems such as assessing  students performance, adapting courses based on learning  behaviours, evaluating learning materials and courses, providing  feedback to instructors, administrators and students, and  identifying at-risk students [4]. They described the use of  statistics, visualization, clustering, classification, and association  rule mining in an iterative continuous empirical evaluation  approach to course development for online learning. Further,  Morris and Finegan [7] suggested using tools available to track  student behaviour, collecting aggregate data over time to   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM 978-1-4503-1057-4/11/02$10.00.   174    understand course norms for success and early indicators for  failure to persist. They described efforts to predict student  retention through analysis of relationships among academic and  demographic data types (achievement, locus of control, financial  assistance, attitude, motivation), and identified other relevant  success factors such as instructor presence and multiple roles,  feedback, explicit learning support, and technical assistance  available.  These projects and others are also informed by the notion of  formative evaluation of courses, which is central to instructional  design theories and models [8]. Formative evaluation, a  methodology for improving the effectiveness and efficiency of  instructional resources [9, 10], is a key phase in most instructional  systems design models, which are typically used in the design of  online learning environments [9]. Reigeluth and Carr-Chellman  [11] proposed that in the information age, design of learner- centred instructional strategies for student success depends on  information about learning behaviours and activities; the  gathering of this information is facilitated by the technology of  the LMS. Baker and Herman [12] provided distance learning  guidelines that emphasize adaptation of materials based on  students needs. Activities that enhance motivation and social  support lead to student engagement and help them sustain their  commitment and persistence. Thus, formative evaluation leads to  data-driven design decisions about suggested improvements,  whether directly from students to the developer or through system  data. They recommended focusing on specific elements of the  learning environment to identify weaknesses and recommend  revisions.  Building on both lines of thinking above, Athabasca University  (AU)  Canadas open, distance university   intends to use data  from its enterprise LMS (Moodle) to enhance its online courses  through learning interventions and other support mechanisms. By  tracking student activities in courses and then linking those data  to other data sources we will better understand the effectiveness  of the learning environment. This understanding will lead to  revisions to course materials and structure that should enhance  student learning, motivation and/or persistence. The project is  building on past work on student persistence and completion rates  at AU, and on studies from other universities on the success of  interventions to decrease course dropout, student engagement  analysis, and learning design improvements. This is one of several  institutional projects designed to support AU's capacity for  teaching and learning using digital technologies. Called Moodle  Analytics, it involves cross-functional teams of faculty and staff  with special expertise in learning design, analytical approaches,  institutional research, information technology applications and  programming.  In this paper, the Academic Analytics Tool (AAT) developed  within the Moodle Analytics project is introduced. First, its  objectives and the design decisions in developing the tool are  explained. Subsequently, the architecture of the tool and its  functionalities are presented. The last section summarises the  paper and discusses the potential contributions of the tool to  improve academic course design.   2. OBJECTIVES AND DESIGN DECISIONS  FOR THE ACADEMIC ANALYTICS TOOL  The Academic Analytics Tool (AAT) is a software application  that allows users to access and analyse student behaviour data in   learning systems; it enables users to extract detailed information  about how students interact with and learn from online courses in  a learning system, to analyse the extracted data, and to store the  results in a database and/or CSV/HTML files. AAT is primarily  developed for learning designers who want to get feedback about  how students use and learn in courses, but it can also be used by  teachers. In order to use AAT, users need to know the courses  they aim at investigating well in order to interpret the results  correctly.   While several prototype tools exist that extract particular data  from a learning systems database and analyse these data in  different contexts [e.g., 13, 14], AAT allows users to decide and  specify what data they are interested in and what analysis they  want to perform with this data. Furthermore, the data and  information that can be extracted and analysed through AAT go  far beyond the statistics and activity reports provided in some  LMSs, which show limited information predefined by their  developers (e.g., information about when a student logged in the  last time or accessed a certain activity). Instead, AAT provides  comprehensive and customized information to its users, allowing  them not only to select from predefined types of information but  also to specify what information they are interested in.    Furthermore, most LMS statistics and activity reports are only  based on the data from individual courses rather than from a set of  courses hosted in a learning system. Similarly, most prototype  tools aim at analysing data from one particular course. AAT is  designed for academic analytics in educational institutions and  therefore aims at flexibility with respect to the choice of courses,  allowing, for example, the capture and analysis of data from all  courses offered by the educational institution, courses of one or  more departments/centres, a single course or a purposefully  chosen combination of courses. Furthermore, distinctions can be  made between the level of courses (i.e., undergraduate courses,  graduate courses, 200-level courses, etc.).   Another objective and design decision was to develop the tool in a  generic way so that it could be applicable for different learning  systems. Therefore, AAT can be used independently of the  learning system used by the educational institution. Furthermore,  most educational institutions use LMSs such as Moodle [6], Sakai  [15], and Desire2Learn [16], systems which are frequently  updated with new versions released regularly. By making the tool  applicable for different learning systems, updates to newer  versions of the same learning system can be handled easily.   In addition, the tool aims at being easily extendable, for example,  with respect to adding sophisticated analysis techniques such as  artificial intelligence algorithms, different data sources such as  data about students demographics, marks, etc., and any other  kind of functionality that users require to conduct effective  academic analyses.   From a technical point of view, the tool is implemented as a web  application using PHP as programming language.   3. ARCHITECTURE OF AAT  The architecture of the tool is based on the architecture of DeLeS  [14], a tool for identifying learning styles from the behaviour of  students in online courses. While DeLeS also aims at being  applicable for different learning systems, several extensions in the  architecture have been made for AAT in order to fulfil all the  objectives described in the previous section.   175    Figure 1 shows the architecture of AAT. AAT uses input data  from one or several databases of a learning system, extracts and  analyses the data that are specified by users, and stores these data  within the Academic Analytics database or outputs CSV/HTML  files with the results.  In order to fulfil the objectives described in the previous section,  four design elements have been used: a framework of types of  learning objects, patterns, templates, and profiles. In the following  paragraphs, these elements are described.  AAT is based on the assumption that each course consists of  learning objects, which are digital resources that students interact  with and learn from. Learning objects can be, for example,  learning material, forum postings, questions of a quiz, the outline  of the course as well as video and audio files. Since AAT mainly  focuses on analysing the behaviour of students in relation to such  learning objects, the consideration of these learning objects is of  particular importance.  Learning objects have an inherent pedagogical purpose. However,  learning objects of the same type can be used for different  pedagogical purposes. For example, quizzes can be used for  training or testing, and forums can be used for discussions or  announcements. An analytics investigation on two learning  objects of the same type used for different pedagogical purposes  could lead to erroneous interpretations of results. For example,  when analysing students participation in discussion forums,  including forums for announcements would lead to aberrant  results. Therefore, a framework of types of learning objects has been  introduced that distinguishes between general types of learning  objects and pedagogical types of learning objects. General types  of learning objects refer to types of learning objects without  regard to their pedagogical use (e.g., quiz, forum, resource). Each  general type of learning object can be related to one or more  pedagogical types of learning objects, which refer to a type of  learning object associated with its pedagogical use or educational  purpose (e.g., a quiz that is graded and a quiz that can be  performed as self-assessment; a forum for announcements and a  forum for discussions). By distinguishing between general types   and pedagogical types of learning objects, mixing data that are  based on learning objects with different pedagogical purposes can  be avoided and misinterpretations due to such a mix of data can  be prevented.  Patterns are based on types of learning objects and specify what  data the user is interested in and therefore, what data should be  extracted from the database(s). A pattern can be a query that  extracts specific data, or a formula supported by a query where  the tool performs calculations on extracted data. Patterns can be,  for example, the average amount of time each student spent on  quizzes, the number of times a discussion forum has been visited  by students, etc.  Templates aim at making the tool applicable for different learning  systems and can be seen as the interface between the tool and the  databases. While patterns specify what data should be extracted  from a database, templates specify where (i.e. what tables and  columns) the respective data resides within the database of a  particular learning system, considering the version of the system  (e.g., Moodle 2.0). Different templates are developed for different  learning systems (and different versions) and are then used for  extracting respective data from the database of these learning  systems.  A profile can be seen as an experiment for extracting and  analysing particular information. In a profile, a user specifies  which learning system is used (through selecting a template), how  to connect to the data (through selecting and setting up database  connections), which courses, learning objects and time spans  should be investigated (through selecting the data set), and which  data the user is interested in (through selecting patterns). AAT  guides the user through this specification process. Once the  profile is created, it can be used to extract and analyse the  specified information.   4. FUNCTIONALITIES OF AAT  AAT is an easy-to-use and powerful tool that allows users to  study student behaviour in online courses. It allows users to  execute predefined and customized queries against any learning  system that stores its data in an SQL accessible database. Users   Figure 1. Architecture of AAT   176    can also chain together queries to make more sophisticated  compound queries. More importantly it allows users to  progressively improve the analytical capabilities of the tool with a  simple to use graphical user interface (GUI). Figure 2 shows a  screenshot of AAT, demonstrating the first step in creating such  queries.  During the installation of AAT, the administrator specifies  database connectivity information and selects a suitable template  for the LMS. Based on this information, AAT automatically finds  courses and learning objects and makes predefined patterns  available to the users. After the installation process, the users can  change the selected settings, such as changing the template and  adding/removing databases using the GUI.   In the following sections, the main functionalities of AAT are  explained.  4.1 Profiles Users of AAT perform analytical investigations by creating and  executing profiles. To create a profile, the user needs to choose a  data set (courses and learning objects) and a set of analytics  operations to be performed on the data set. Analytics results are  generated when a profile is executed. These results can be stored  in the Academic Analytics database, displayed on screen, and  saved as HTML and/or CSV files.   4.2 Choosing a Data Set  Before users run an analytics query, they need to be able to  precisely define the data set they wish to analyse. Using a GUI,  users can select the data set they are interested in analysing from  the identified pool of courses and their associated learning  objects. Functionality for selecting groups of courses is also  provided. Since online courses are not restricted by time constraints, some  universities, such as AU, use a continuous enrolment model. In  order to make AAT applicable for courses with semester-based   enrolment as well as courses with continuous enrolment, AAT  allows users to specify the exact periods of time they wish to  analyse.    4.3 Choosing Analytics Operations  Once a data set has been specified, users need to define the  analytical operations they wish to perform on the data. They can  choose from an extensive set of predefined patterns (e.g., overall  activities of students in a course, the number of visits of particular  types of learning objects, the amount of time spent on particular  types of learning objects). Furthermore, users have the option of  creating their own custom patterns. The ability to create custom  patterns allows users to get answers to questions they need to ask.  Multiple patterns can be applied to a data set. Patterns can be  chained (i.e., the output of one pattern can be used as input into  another pattern). Thus, powerful and complex queries can be  constructed incrementally and progressively. The entire process  of creating and chaining patterns is performed using a simple  GUI, where users can either use an SQL editor that guides them  step by step through the process or users can directly input SQL  queries. For example, if a user wishes to identify quiz questions that are  difficult to answer for students, he/she can build a pattern that  extracts data about the average performance of students on  questions within quizzes. On top of this pattern, the user can  create another pattern that outputs all quiz questions where the  average performance of students is lower than, for example, 70%.  Using the results of this pattern, the user can create another  pattern that investigates the question types (e.g., multiple choice,  true/false, matching, etc.) of the questions that were difficult to  answer for students and output a distribution of these types.  Furthermore, a user can investigate the learning material that is  associated with the questions that were difficult to answer and  can, for example, create a pattern that looks into the time students  spent on this learning material and compare this time with the  average time students spent on all learning materials.   Figure 2. Creating Patterns/Queries in AAT   177    4.4 Pedagogical Types of Learning Objects  While general types of learning objects (e.g., forum, quiz) are  identified automatically by AAT when database connection and  learning system information is available, specifying the  pedagogical purpose of learning objects requires the users  intervention. To address the issue of pedagogical purpose, AAT  allows users to define pedagogical purposes for each general type  of learning object, using user-defined controlled vocabulary.  Controlled vocabulary schemes mandate the use of pre-selected  terms which have predefined definitions. Subsequently, users are  supported by AAT to annotate learning objects through a semi- automatic approach, using the defined pedagogical purposes. It is  up to the user to interpret the meaning of the pedagogical purpose  and thus, it is important for the user to consider the meaning of  the pedagogical purpose when defining and/or using a pattern in  order to perform data extraction and statistical analysis in  alignment with that interpretation.   Using the example given in the previous section, a user can define  two pedagogical purposes for quizzes to distinguish between  marked quizzes and self-assessment quizzes. Using only marked  quizzes to analyse distributions of question types that are difficult  to answer for students and learning material that is associated  with these difficult questions, will result in more accurate  understandings, since some students might not take self- assessment quizzes as seriously as marked quizzes and may  choose to take the quiz before reading the learning material.   4.5 Working with Databases  AAT requires read-only access to the database(s) of the learning  system it is to analyse. Therefore, the user or administrator must  provide database connectivity information. AAT can connect to  multiple instances of a learning systems database. In addition,  AAT allows users to perform analytics on data from several  different instances of a database (of the same learning system)  simultaneously. Since it is not uncommon for universities to  distribute course data across several databases, AAT is capable of  working with such complex database configurations.  4.6 Working with Different Learning Systems  There are many different learning systems available and new  versions of learning systems are introduced frequently over time.  To make AAT applicable for different learning systems as well as  to allow upward compatibility with future versions of learning  systems, AAT uses templates to define how to find specific pieces  of information from a specific version of a learning system. AAT  comes pre-packaged with templates for several learning systems /  versions of learning systems. Therefore, a user simply needs to  select the right template in order to specify the learning system in  AAT. If a template is not available, for example, for a newly  released version of a learning system, administrators of the AAT  instance, who know the database of the new learning system well,  can create new templates. These new templates can then be shared  within the community and made available to administrators of  AAT systems.   4.7 Extending the Tool  AAT has been created in a modular fashion and many design  features have been inspired from the content management system  Drupal [17]. Administrators have the option of coding new  modules to extend functionality of AAT. Writing AAT modules is  as easy as writing Drupal modules.    4.8 Other Features  In addition to the above-mentioned features, AAT provides strong  data security and access control features, support for Smarty  templates, embedded help files, a SQL editor, a GUI SQL query  generator, history, and backup and recovery features.  Furthermore, the designers of the tool have made every effort to  make it user-friendly and user-centred without compromising the  design principles or its functionality.   5. CONCLUSIONS In this paper, the Academic Analytics Tool (AAT) is introduced.  AAT is a powerful and easy-to-use tool designed to allow users to  perform simple and complex analytical queries on students  behaviour in online courses. In the following paragraphs, the  possible benefits this tool can bring to educational institutions are  described, discussing the plans of using the tool at Athabasca  University (AU).   The data that can be retrieved through the use of AAT on how  students are currently using the learning objects in their AU  courses will be evidence for the formative evaluation of those  courses. The data will be analysed as part of our regular course  revision process. Combined with students evaluations of the  courses and professor and tutor recommendations for changes,  these data will inform the work of our learning designers, who  with subject matter experts will adapt and extend resources that  are generating successful learning and revise materials that are of  less direct value to students. Engaging learning objects will be  shared across disciplines as appropriate, generating interest in  new pedagogical approaches within the academy. Once it is  possible to integrate data from administrative systems with data  from the learning system, we will be well positioned to identify  factors affecting student success. The infrastructure to be  established will facilitate the extraction and transformation of data  required for improved operational reporting across a number of  different aspects of the teaching and learning environment at AU.  Eventually it will be possible to generate automated interventions  to enhance student retention, motivation and/or learning, and to  generate customized dashboards for sharing progress information  with tutors and students, thereby meeting institutional goals of  quality and access.   The importance of analysing student activities in Moodle has  increased over the last couple of years as we have moved from  simple course conversion to complete course re-design for the  online environment. The direct value of the results of our analyses  can be understood in two ways. We will have data about which  learning activities students are completing and which ones they  are not. This will build on course evaluation data and help inform  improvements to individual courses. It will also allow us to  evaluate and revise the standards we are setting for excellence in  AU online courses. Future work will deal with conducting a study where learning  designers will test AAT with respect to its usability and  usefulness. Furthermore, we plan to release AAT as an open  source product in order to allow other educational institutions to  benefit from AAT as well. In addition, the indirect value of the  project will be realized in a methodology for further activities in  academic analytics.   178    6. ACKNOWLEDGMENTS The authors acknowledge the support of the Knowledge  Infrastructure Program through the Open Knowledge  Environment Project funding.   7. REFERENCES [1] Campbell, J.P., DeBlois, P.B., and Oblinger, D.G. 2007.   Academic Analytics: A New Tool for a New Era.  EDUCAUSE Review 42, 4 (Jul./Aug. 2007), 40-57.   [2] Finkelstein, A.B.A., Masi, A.C., and Winer, L.R. 2004. My  LMS Gets 1,000,000 Hits a Day: Supporting Your Strategic  IT Decisions with Log Analysis Data from your LMS.  Presentation at the EDUCAUSE Conference, Denver CO,  October, 2004   [3] Orcutt, J.M. 2010. Using Enterprise Reporting to Assess  Instructor Involvement in Online Classes. Presentation at  Pearson CiTE Conference, Denver CO, April, 2010.   [4] Romero, C., Ventura. S., and Garcia, E. 2008. Data Mining  in Course Management Systems: Moodle Case Study and  Tutorial. Computers and Education 51 (2008), 368-384.   [5] Garrison, D. R., Anderson, T., and Archer, W. 1999. Critical Inquiry in a Text-Based Environment: Computer  Conferencing in Higher Education. The Internet and Higher  Education, 2, 2-3 (Spring 1999), 87-105.   [6] Moodle, 2011. http://moodle.org/ (accessed on July 31,  2011).  [7] Morris, L.V. and Finegan, C.L. 2008. Best Practices in  Predicting and Encouraging Student Persistence and  Achievement Online. Journal of College Student Retention 10, 1 (2008), 55-64.   [8] Reigeluth, C.M. and Frick, T.W. 1999. Formative Research:  A Methodology for Creating and Improving Design   Theories. In Instructional Design Theories and Models, Reigeluth, C.M., Ed., Vol. I, Lawrence Erlbaum Associates,  Mahwah, NJ, 633-651.   [9] Smith, P.L. and Ragan, T.J. 1999. Instructional Design (2nd ed.). Merrill, Upper Saddle River NJ.   [10] Tessmer, M. 1998. Planning and Conducting Formative  Evaluations: Improving the Quality of Education and  Training. Kogan Page, London.   [11] Reigeluth, C.M. and Carr-Chellman, A.A (eds.). 2009.  Instructional-design Theories and Models: Volume III.  Building a Common Knowledge Base. Routledge, New York.    [12] Baker, E.L. and Herman, J.L. 2003. A Distributed Evaluation  Model. In Evaluating Educational Technology, Gaertel, G.,  Means, B., Eds., Teachers College Press, New York, 95-119.   [13] Mazza, R. and Milani, C. 2008. Exploring usage analysis in  learning systems: gaining insights from visualisations. In  Proceedings of the AIED Workshop on Usage Analysis in  Learning Systems at the International Conference on  Artificial Intelligence in Education (AIED 2005), Springer.   [14] Graf, S., Kinshuk, and Liu, T.-C. 2009. Supporting Teachers  in Identifying Students' Learning Styles in Learning  Management Systems: An Automatic Student Modelling  Approach. Educational Technology & Society 12, 4 (Oct.  2009), 3-14.   [15] Sakai, 2011. http://www.sakaiproject.org/portal (accessed on  July 31, 2011).   [16] Desire2Learn, 2011. http://www.desire2learn.com/ (accessed  on July 31, 2011).   [17] Drupal, 2011. http://drupal.org/ (accessed on July 31, 2011).   179    "}
{"index":{"_id":"22"}}
{"datatype":"inproceedings","key":"Bader-Natal:2011:ELA:2090116.2090146","author":"Bader-Natal, Ari and Lotze, Thomas","title":"Evolving a Learning Analytics Platform","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"180--185","numpages":"6","url":"http://doi.acm.org/10.1145/2090116.2090146","doi":"10.1145/2090116.2090146","acmid":"2090146","publisher":"ACM","address":"New York, NY, USA","keywords":"collaborative learning, learning analytics platform, web-based learning","Abstract":"The Social Networks Adapting Pedagogical Practice (SNAPP) tool was developed to provide instructors with the capacity to visualise the evolution of participant relationships within discussions forums. Providing forum facilitators with access to these forms of data visualisations and social network metrics in 'real-time', allows emergent interaction patterns to be analysed and interventions to be undertaken as required. SNAPP essentially serves as an interaction diagnostic tool that assists in bringing the affordances of 'real-time' social network analysis to fruition. This paper details the functional features included in SNAPP 2.0 and how they relate to learning activity intent and participant monitoring. SNAPP 2.0 includes the ability to view the evolution of participant interaction over time and annotate key events that occur along this timeline. This feature is useful in terms of monitoring network evolution and evaluating the impact of intervention strategies on student engagement and connectivity. SNAPP currently supports discussion forums found in popular commercial and open source Learning Management Systems (LMS) such as Blackboard, Desire2Learn and Moodle and works in both Internet Explorer and Firefox.","pdf":"Evolving a learning analytics platform  Ari Bader-Natal Grockit  San Francisco, CA USA ari@grockit.com  Thomas Lotze Grockit  San Francisco, CA USA thomas@grockit.com  ABSTRACT Web-based learning systems offer researchers the ability to collect and analyze fine-grained educational data on the per- formance and activity of students, as a basis for better un- derstanding and supporting learning among those students. The availability of this data enables stakeholders to pose a variety of interesting questions, often specifically focused on some subset of students. As a system matures, the number of stakeholders, the number of interesting questions, and the number of relevant sub-populations of students also grow, adding complexity to the data analysis task. In this work, we describe an internal analytics system designed and devel- oped to address this challenge, adding flexibility and scala- bility. Here we present several examples of typical examples of analysis, discuss a few uncommon but powerful use-cases, and share lessons learned from the first two years of itera- tively developing the platform.  Categories and Subject Descriptors K.3 [Computers and Education]: Computer Uses in Ed- ucation  General Terms Design, Human Factors, Theory  Keywords Web-based Learning, Collaborative Learning, Learning An- alytics Platform  1. INTRODUCTION The purpose of learning analytics is to better understand  and more effectively foster learning. The approach is heav- ily data-driven, as it is based on the collection, analysis, and interpretation of collected educational data. Software-based systems have the ability to easily record and analyze very fine-grained data, and web-based software systems have the  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. LAK11 February 27-March 1, 2011, Banff, AB, Canada. Copyright 2011 ACM 978-1-4503-1057-4/11/02 ...$10.00.  added benefit of compiling this disparate data into a sin- gle centralized location. This allows for analysis of data in the aggregate, allowing population-wide patterns to be ob- served and leveraged. Two years ago, we began building one such system for Grockits web-based collaborative learning platform. Since then, the growth of the platform, the size of the datasets, and the evolving needs of our learners and decision-makers have demanded and driven the continued evolution of this system. In this work, we discuss the it- erative development of the tool, with a particular focus on techniques we have used to make it more flexible, powerful, and scalable.  Grockit aims to build a learning system that is simulta- neously scalable, effective, and engaging, by leveraging syn- chronous collaborative learning dynamics and game dynam- ics [2, 3]. Behind the development of the social and game- based aspects of the system is a strong reliance on data analysis, allowing us to understand and improve on the ef- fectiveness of the platform: Do students who work in groups spend more time on task than those who choose to work alone Which of the various interventions available within Grockit lead to the largest learning gains Do peer-awarded points motivate more discussion in small group settings Are video explanations more effective than written explana- tions How does an item response theory model compare to a knowledge tracing model, in terms of predictive response accuracy How does frequency and duration of discussions during group study sessions differ between high school stu- dents and post-college learners By being able to rapidly ask, answer, visualize, and disseminate findings from these questions, we can inform decisions around the design of an increasingly effective learning environment.  The challenges in creating an analytics platform for a web application  including learner analytics platforms  is in balancing the power and flexibility of an approach with the efficiency and scalability limitations of that approach. As the size of a dataset grows, this balance often shifts.  2. CREATING AN ANALYTICS PIPELINE After a few weeks of treating each question that we wished  to answer as a entirely new analysis project, patterns in the process began to emerge, and we began to create infrastruc- ture to support the most common parts of the workflow. At a high level, the Grockit analytics system is now based on a standard pipeline consisting of (a.) data collection, (b.) selection, (c.) analysis, (d.) visualization, and (e.) distri- bution. The goal of collection is to instrument the system to record relevant data points for later reference, generally  180    done with application code. The goal of selection is to draw together all of the relevant data to answer a particular ques- tion, generally done with SQL queries. The goal of analy- sis is to use that compiled data to answer a specific ques- tion, most frequently done using the R statistical package [8]. The goal of visualization is to create an effective way to convey that analysis, also generally done using R. The goal of distribution is to organize and disseminate specific analyses to only those stakeholders to whom each is relevant, which is done via an internal web-based system in which dif- ferent stakeholders are subscribed to different self-updating reports.  By creating a common framework for analytics, the ease of adding a new analysis is greatly improved. One can cre- ate a new analysis simply by creating two files: an SQL file to retrieve the data and an R file to analyze and display it. Because the infrastructure takes care of setting up the environment, retrieving the data, passing it to the appro- priate steps, and collecting and distributing the results, the only thing which needs to be created for each analysis is the part which is different: the actual data retrieval and analy- sis. The overhead is minimal, allowing us to quickly look at many different analyses.  We can illustrate this workflow with a specific question: Are the difficulty levels of our questions appropriate for the ability levels of our students Based on what we might find, we would choose to ask our content authors to focus on cre- ating more difficult questions or more easy questions. To answer this, we start with a SQL database query to list each question identifiers, grouped by subject (e.g. Algebra I), along with the primary parameter that we use to character- ize the difficulty of that item (its IRT location parameter, .) Similarly, we collect a list of all student unique identi- fiers, along with the ability estimate (IRT person parameter, ) for the student in that subject. The resulting database tables are accessed through an R script, which is used to calculate and display back-to-back histograms to create a Wright Map of the data [5]. This analysis is then automat- ically disseminated to the content authoring team and the student modeling team. When we first ran this analysis last year, we found that of the ten domains for which we had developed IRT models, the quantitative questions that we had in place for the GMAT group could better reflect the ability level of the student population. Based on this, our content authoring team subsequently focused on developing additional content at the upper end of the difficulty scale.  The value in creating a common analytics pipeline is that new analyses can be performed rapidly, past analyses can be archived for future access, and common usage patterns can be easily identified and codified.  3. ASKING THE SAME QUESTIONS USING DIFFERENT DATASETS  When Grockit expanded from a single student popula- tion (post-college learners studying for a business school entrance exam) to other groups of learners, including high school students, we found ourselves wanting to ask the same questions of different subsets of learners. Beyond simply segmenting students in our database, we started wanting to ask questions based on a variety of other criteria: spe- cific time windows, a particular teachers class, all students in the treatment group of some controlled experiment, only  Figure 1: A Wright Map compares the distribution of IRT person parameters of all students to item pa- rameters of all questions in the GMAT Verbal sec- tion in Grockit. This plot suggests that the stu- dent abilities are slightly above question difficulties, which informed our decision to author more chal- lenging questions for this domain.  the questions most recently added to the system, only stu- dents who have acted as peer tutors, only students who have worked with a specific instructor, excluding response data from teachers/tutors/administrators/authors, or any num- ber of other data restrictions. In order to avoid constantly modifying (or duplicating) queries, we chose to add support for on-the-fly redefinitions of these sorts of constraints. As the data that we work with is highly relational, the goal was for these restrictions to cascade seamlessly through the model (e.g. if we exclude a particular game, we necessarily wish to exclude all questions answered in that game, any re- views of that game, all explanations read during that game, etc.) Online analytic processing (OLAP) was designed to address this sort of challenge, and the solution that we built shares several qualities with Relational OLAP (ROLAP) systems [4].  We chose to use a lightweight view-based solution, with our MySQL database. Data selection queries were modified to work with (non-materialized) views of the database tables rather than with the tables themselves. These view defini- tions form a directed acyclic graph (DAG), so most views are defined based on other views. Based on this DAG of non-materialized views, a single view redefinition (such as students who answered questions yesterday) effectively re- defines all other views dependant on that one. Records in tables that are dependent on more than one other table are therefore only included in the associated view if all upstream records are considered.1 In addition to this cascading inter-  1For example, a database row recording a student having  181    0.0 0.2 0.4 0.6 0.8 1.0  IRTbased probability of response accuracy  Av er  ag e   tim e   ta ke  n  to   a ns  w er   (i n   se co  nd s)  0 30  60 90  Response accuracy:  Total Response accuracy:  Correct Response accuracy:  Incorrect  0.0 0.2 0.4 0.6 0.8 1.0  IRTbased probability of response accuracy  Av er  ag e   tim e   ta ke  n  to   a ns  w er   (i n   se co  nd s)  0 30  60 90  Response accuracy:  Total Response accuracy:  Correct Response accuracy:  Incorrect  Figure 2: Average time to answer a question, as a function of the IRT-based probability of response accuracy (used as a proxy for subjective difficulty). GMAT Quantitative data on left, ACT English data on right.  section, a question can be asked of a composition of multiple non-overlapping view definitions. This means that we can easily ask questions about item responses to Geometry ques- tions among students who have been active during the past 30 days. As these analyses are run on replicas of the appli- cations production database rather than the database itself, long-running queries can process near-realtime data without degrading the performance of the application server.  One question we were interested to understand earlier this year: Do students generally spend more time or less time to answer a question that they find easy That they find difficult Does this vary by subject matter To understand the relationship between subjective item difficulty, accuracy, and time taken, we used our item difficulty parameters to estimate the probability that each item response would be correct. We view this probability as a subjective difficulty metric, based on the assumption that students who have a very low probability of answering a question correctly will find that question difficult, and students who have a very high probability of answering a question correctly will find it to be easy. One notable benefit of separating the question definition from the data definition is that stakeholders can often access the results of existing queries for new subsets of students without requiring any new code to be written. New entries just need to be added to the report-scheduling queue:  QUERY=probability_time_accuracy VIEW=GameGMATQuantitative+UserStudyingForGMAT  QUERY=probability_time_accuracy VIEW=GameACTEnglish+UserStudyingForACT  Fig. 2 displays the output of this query for different sub-  answering a question is only included in the view of that table if records for both that student and that question are included in views of their tables, respectively.  sets of data. For GMAT Quantitative questions, students are, on average, spending less time answering questions that are subjectively very easy or very difficult than they spend on questions with are in between, a finding that supports Kosters theory of the connection between challenge appro- priateness and learner engagement [7]. Interestingly, a differ- ent trend is seen in ACT English responses, where students generally spend less time the easier they find the questions.  With this flexibility, a common use-case for the analytics platform, asking the same question of a different set of data, no longer requires any code changes, and could be made ac- cessible to non-technical stake-holders.  4. RECORDING EVENTS TO ENABLE EX- PERIENCE ANALYSIS  We found that some of the questions that we sought to answer required knowledge about events that were not al- ready being recorded in the database. Rather than create new models and relational tables for each such event type, we opted to add support for a simple experience logging facility. These are currently written to the database to al- low analytics queries to join them with other relational data, but they could theoretically be logged elsewhere initially and only merged into the analytics environment at analysis-time.  Questions that we have been very interested in become possible to answer by annotating adding a few such event experiences. Did a student ever see their study plan Did they start viewing an available video explanation Did a student ever access their performance analytics page These event experiences played a role in a bigger-picture question that weve been exploring recently: Of all of the many op- portunities for learning within Grockit  individual prob- lem solving, small peer-group study, instructor-led lessons, skill-based video explanations, private tutoring sessions, and skill-customized practice, among others  which of these is  182    InitialQuestions Should new students be given easier questions when playing alone,  until they answer at least 5 questions correctly This test is being assigned and evaluated for new users only.  EasyFirst Normal Subsequent activity (1657 persons) (1721 persons) (difference)  Participated in further study 73.2% (1213) 67.5% (1161) (2.6% to 8.9%) ** Participated in group study 31.4% (520) 27.0% (464) (1.3% to 7.5%) *  Logged in again later 30.8% (510) 28.4% (489) Participated in group discussion 22.1% (366) 18.4% (316) (1% to 6.5%) *  Reviewed past questions 13.9% (230) 12.8% (221)  Table 1: A reformatted portion of the automatically-produced output of a controlled experiment testing the effect of the difficulty of the first few questions presented to a new student. Difference indicates a confidence interval around the difference between the percentages from the two groups. A single * indicates a p-value that is significant at the  = 0.05 level, and a double ** indicates significance at the  = 0.01 level (two-tailed).  most effective Understanding this could inform decisions ranging from the study plans that we offer students, the spe- cific activities that we encourage at various points in time, and even decisions about removing certain activities alto- gether. When we first sought to ask this question, we found that we hadnt recorded all of the data that we were inter- ested in examining (including information on partial or en- tire video explanations watched). Instrumenting the system with a few logging records was sufficient to collect the ad- ditional information. Details can be found in Bader-Natal, Lotze, and Furr [1].  An event-logging facility within a web application allows data analysts to record data necessary for analysis that are not otherwise persisted, often in a single line of code.  5. TESTING NEW HYPOTHESES WITH RAN- DOMIZED CONTROLLED EXPERIMENTS  Where OLAP-style analysis allows us to understand and describe trends that we see in collected student data, this ap- proach does not directly let us test out new hypotheses. As with other types of web applications, web-based educational software provides an ideal environment for running random- ized controlled experiments [6]. Our goal with Grockits implementation of a framework for doing so was to (a.) min- imize the amount of work necessary to introduce a new ex- periment, and (b.) minimize the amount of work necessary to analyze an experiment. We illustrate below how just a few lines of application code automatically generate an anal- ysis of the differences among treatment and control groups over a fixed set of outcome measurements (used for all such analyses). For a recent experiment, we sought to under- stand the affect that the difficulty of the first few questions presented to a student had on subsequent participation and retention rates. This code randomly assigns students (with equal probabilities) to the treatment or control groups, and provide a different user experience based on that assignment:  experiment = {  InitialQuestions  => [ EasyFirst ,  Normal ] } if (SplitTest.find_or_create_assignment(self,  InitialQuestions )  ==  EasyFirst ) show_easy_first  else show_regular  end  The first line defines a unique identifier for the new exper- iment, which, combined with the unique identifier for a par-  ticular student (based on self, in the second line), is the basis for a random assignment to one of the groups. By adding this application code, the analysis in Table 1 is automati- cally generated and distributed daily. Here, students were assigned to the treatment group with a p = 0.5 probability. The null hypothesis is that the two populations of students have the same true proportions, and the alternative is that the proportion is different in one of the populations. In the case of the analysis in Table 1, we found that the EasyFirst question selection strategy resulting in an increase in the rate of subsequent study, group study, and participation in group discussions.  A built-in infrastructure for introducing and evaluating randomized controlled experiments allows for a powerful and valuable new class of analysis: hypothesis testing.  6. DISTRIBUTION: FROM PUSH TO PULL Originally, the completed reports were emailed as PDF  files to specific recipients. As the number of reports and students increased, emailing many large files became infea- sible as a way of distributing the results of the reports. In place of this, we now have a centralized, web-based report- ing system. All past reports are archived, and the source data and intermediate data files are persisted to allow for later analysis, comparisons over time, or replications of past results. Furthermore, by using the same user authentication as the main Grockit site, we can also provide teachers with access to reports on their classes. Finally, by providing a common way of accessing the reports, it allows stakeholders to discuss individual reports simply by sharing a link to the report under discussion.  Access control, search, and data archiving can all be sim- plified by means of a centralized, web-based repository for the output of an analytics reporting system.  7. FOCUSING ON PERFORMANCE, SCAL- ABILITY, AND STABILITY  As the number of students in the system has increased, the size of data to be analyzed has become much larger, forc- ing us to consider how to scale the workflow and analyses to deal with more reports on more data. The first and simplest answer is increasing the power of the computers running the analyses, in our case by performing the analyses on Amazon EC2 instances with additional RAM and faster processors.  183    Figure 3: For each student, we provide access to a simple report showing their accuracy in each of several skill areas. This allows them to visualize their strengths and focus on the areas they are weak in. The student can select various difficulty ranges and time ranges to view their analytics over.  One useful tool for making use of these powerful remote ma- chines in developing analyses is the ability to run analyses as though they were local, by sending the request to the remote machine, having it perform the data selection, analysis, and visualization, and then retrieving the results for display as though they had been run locally.  Additionally, since we do not want to slow down our pro- duction web-server or database with intensive analysis, we replicate the databases. Originally, this was done daily at the beginning of the set of analyses to be run; but as the data size increased, copying over all of the data became infeasible. Instead, we continuously keep the reporting database near- realtime by using MySQL replication against the production database, ensuring that our reports are run on up-to-date data. This automatic replication process also allowed us to begin to distribute reporting across multiple machines; instead of having a single machine run all reports and wait- ing for each report to finish before running the next one, we have multiple machines, each with an up-to-date copy of the database and workflow code, which can run through the entire reporting process in parallel. The next step in the process it to have a pool of ready reporting machines, along with a central job distribution system, such that any report request, whether run periodically or by an analyst trying to answer a new question, can be sent directly to the next available machine. Providing a central job distribution process information about priority and the state of running jobs would allow all job requests to be integrated, and also allow for any user viewing a report on the web-based system to initiate report jobs on-demand.  As the analyses become valuable on a recurring basis, rather than simply being a one-time answer to a question, we need to ensure that they are stable in the face of continual changes to the data and application, so that the stakehold- ers relying on the analysis can rely on their being ready and available. While we have automated tests that ensure that reports are successfully generated, we wish to also have a  simple capacity to ensure that the results of the analysis are still reliable and accurately reflect the answer to the ques- tion. This additional capacity can be provided by having a single test file for each report, that verifies correct re- sults from a pre-specified testing dataset. This can be done without slowing down the process for rapid one-off analy- ses, which can be performed without creating or providing automated tests.  As the dataset grows and results become mission-critical, issues of performance, scalability, and reliability become in- creasingly important.  8. PROVIDING ANALYTICS DIRECTLY TO LEARNER AND TEACHERS  As teachers and students become more data-oriented about their learning process, several analyses initially prepared us- ing the analytics pipeline have since been moved to a user- facing location within the application itself. Analytics re- ports that were distributed weekly to teachers in classroom pilots were subsequently incorporated into the teachers dash- board in Grockit. Skill-grained student performance report- ing, illustrated in Fig. 3, was also first created within the analytics pipeline, which allowed for rapid iteration and re- finement before it was transitioned into the production ap- plication. Once this transition is made, computation time becomes critical. In order to minimize page load speeds, we proceed by precomputing and caching, leveraging a Hadoop infrastructure.  However, as the number of games and questions in the system increases, directly computing this when the student requests it began to take several seconds, especially for more active students. This is particularly true given that the student can select not just to view their all-time analyt- ics results, but also select from 35 different combinations of problem difficulty and response recency to analyze their per- formance. To solve this, we began precomputing portions of  184    the students analytics information. For each day the stu- dent was active, for each skill to be analyzed, we compute their total number of questions answered, number correct, and total time taken. This can be done using Hadoop for all users in just a few minutes, allowing for frequent updates of the precomputed data. Generating the percentage correct and average time for each individual skill or track becomes a simple matter from this data. This can then quickly be retrieved as a single row from an indexed table, and sent to the client browser for rendering. Overall, we saw between a 10-25x improvement in response time. As we provide more analytics within the application, increasing the amount of precomputing and cloud-based parallelization will allow us to provide feedback without sacrificing performance.  Students and teachers may benefit from more direct ac- cess to the results of a learning analytics system. Doing so may require aggressive performance optimizations to provide immediate access to near real-time analysis.  9. CONCLUSION To take full advantage of the rich data available from  computer-based learning systems, creating a pipeline for processing and presenting advanced analysis can be a sig- nificant boon for learning about students behavior and per- formance. We have described many of the advantages to be gained by doing so, as well as methods which we have used to achieve them. We hope that our system may serve as an example of what is possible by automating this pipeline. In making the analytics more readily available, scalability and stability must be addressed; we have described these challenges as well as approaches we are developing to ad- dress them. Finally, the future of analytics is one where the results are available not only to researchers and system de- signers, but also directly to students and teachers. In order to do this, we must not only make the process of developing new analytics as easy as possible, but also reliable and ac- cessible to the teachers and students that is meant to inform and help.  10. REFERENCES [1] Ari Bader-Natal and Thomas Lotze and Daniel Furr. A  Comparison of the Effects of Nine Activities within a Self-Directed Learning Environment on Skill-Grained Learning. In Biswas, G. and Bull, S. and Kay, J. and Mitrovic, A., editors, Proceedings of the 15th International Conference on Artificial Intelligence in Education, Lecture Notes in Computer Science, 2011, Volume 6738/2011, Springer.  [2] Ari Bader-Natal. Incorporating game mechanics into a network of online study groups. In Scotty D. Craig and Darina Dicheva, editors, Supplementary Proceedings of the 14th International Conference on Artificial Intelligence in Education, volume 3, Intelligent Educational Games workshop, pages 109112, July 2009. IOS Press.  [3] Ari Bader-Natal. Interaction synchronicity in web-based collaborative learning systems. In Theo Bastiaens, Jon Dron, and Cindy Xin, editors, Proceedings of World Conference on E-Learning in Corporate, Government, Healthcare, and Higher Education 2009, pages 11211129, Vancouver, Canada, October 2009. AACE.  [4] S. Chaudhuri and U. Dayal. An overview of data warehousing and OLAP technology. ACM Sigmod record, 26(1):6574, 1997.  [5] Gerhard H. Fischer and Ivo W. Molenaar, editors. Rasch Models: Foundations, Recent Developments, and Applications. Springer-Verlag, New York, 1995.  [6] R. Kohavi, R.M. Henne, and D. Sommerfield. Practical guide to controlled experiments on the web. In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 959967. ACM, 2007.  [7] Raph Koster. A Theory of Fun for Game Design. Paraglyph Press, 2004.  [8] R Development Core Team. R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria, 2010. ISBN 3-900051-07-0.  185    "}
{"index":{"_id":"23"}}
{"datatype":"inproceedings","key":"Sharkey:2011:AAL:2090116.2090135","author":"Sharkey, Mike","title":"Academic Analytics Landscape at the University of Phoenix","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"122--126","numpages":"5","url":"http://doi.acm.org/10.1145/2090116.2090135","doi":"10.1145/2090116.2090135","acmid":"2090135","publisher":"ACM","address":"New York, NY, USA","keywords":"Hadoop, academic data, data modeling, integrated data, learning analytics, predictive analytics","Abstract":"In online learning environments, teachers and course designers often get little feedback about how students actually interact with and learn in online courses. Most of the learning systems used by educational institutions store comprehensive log data associated with students' behaviours and actions. However, these systems typically reveal or report on very general and limited information based on this data. In order to provide teachers and course designers with more detailed and meaningful information about students' behaviour and their use of learning resources within online courses, an analytics tool has been developed. The tool incorporates functionality to access and analyse data related to students' behaviours in learning systems. This tool can provide valuable information about students' learning processes allowing the identification of difficult or inappropriate learning material, and can therefore significantly contribute to the design of improved student support activities and resources.","pdf":"Academic Analytics Landscape at the University of Phoenix  Mike Sharkey, Director of Academic Analytics,   University of Phoenix   ABSTRACT The University of Phoenix understands that in order to serve its  large population of non-traditional students, it needs to rely on  data.  We have created a strong foundation with an integrated data  repository that connects data from all parts of the organization.   With this repository in place, we can now undertake a variety of  analytics projects.  One such project is an attempt to predict a  students persistence in their program using available data  indicators such as schedule, grades, content usage, and  demographics.  Categories and Subject Descriptors K.3.1 [Computers and Education]: Computer Uses in Education  General Terms Design, Theory    Keywords Learning analytics, academic data, integrated data, data modeling,  predictive analytics, Hadoop   1. INTRODUCTION TO THE UNIVERSITY  OF PHOENIX  The University of Phoenix is a regionally accredited degree- granting institution founded in 1976 by Dr. John Sperling.  Based  in Phoenix, Arizona and with over 200 locations throughout the  United States and a strong online campus, the University of  Phoenix is the largest private university in North America.  As of  May 2010, over 470,000 students were enrolled at the University  of Phoenix.   1.1 History In 1976, Dr. John Sperling, a Cambridge-educated economic  historian and professor, founded University of Phoenix on an  innovative idea: making higher education accessible for working  adults.  In the early 1970s, while a tenured professor at San Jose State  University in California, Dr. Sperling and several associates  conducted field-based research on new teaching and learning  systems for working adult students. From this research, Dr.   Sperling realized that the convergence of technological,  economic, and demographic forces would herald the return of  working adults to higher education. He saw a growing need for  institutions that are sensitive to the learning requirements, life  situations, and responsibilities of working adults. These beliefs  resulted in the creation of University of Phoenix.1  1.2 Students Served  In the past, the University of Phoenix focused on degree   completion for non-traditional adult learners.  Over the years,  changing demographics have seen students with little to no credits  starting at the university with the goal of completing their entire  programs.  The university also expanded in 2006 when it started  offering associates degrees in addition to bachelors, masters,  and doctoral degrees.   The demographics of the non-traditional learners at the University  of Phoenix differ from students at traditional post-secondary  institutions.  Non-traditional students tend to be older, largely  female, and tend to come from more diverse socio-economic and  racial backgrounds.  Many non-traditional learners also have jobs  and family obligations as opposed to 18-22 year-old residential  students at a traditional institution.  This different demographic  changes the motivators and drivers behind the students actions.   1.3 Structure Key aspects of the universitys organizational structure  differentiate it from traditional degree-granting institutions and  community colleges.  These aspects have a significant effect on  determining the direction of technological initiatives.   Academically, the university has a central administrative unit.   All programmatic and curricular decisions are made by the central  Academic Affairs unit and then distributed throughout each of the  campus locations.  This process holds true for the creation of new  programs, the modification of existing programs, and the  development of course curricula.   Courses are taught by practitioner faculty; the faculty members  have experience working in the field that is related to their course.   Curriculum is centrally designed by faculty members, content  experts, and instructional designers. University of Phoenix faculty  members have the academic freedom to enhance the standard  curriculum with their expertise, content and theoretical  knowledge, and the practical experience they gain as a result of  working in the fields in which they teach.                                                                     1 Taken verbatim from our institutions website at:   http://www.phoenix.edu/about_us/media_relations/just-the- facts.html.  For a detailed history of Dr. Sperling and the  University of Phoenix, refer to the book Rebel With a Cause (John Wiley & Sons, 2000).   Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.  Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.   122    Each campus is an independent entity and receives operational  support from the centralized organization (curriculum, marketing  guidelines, operational requirements).  Over two thirds of the students take courses online while the  remaining students attend in a face-to-face modality.  Because  many students take one hundred percent of their courses online,  there is a campus dedicated solely to online students.  This is the  largest of the University of Phoenix campuses and it is also based  in Phoenix, Arizona.  Students enrolled at physical campuses can  choose to take their courses in local classrooms or they can attend  online classes as long as the programmatic requirements are met.   One last point is that the universitys academic calendar does not  follow a semester, trimester, or quarter model.  For the most part,  courses are taken serially throughout a students entire program.   At the associate level, students take two complementary nine- week courses concurrently.  At the undergraduate and graduate  levels, students take one five-week or six-week course in serial  (respectively).  A student may continue to take one course after  another until the program is complete, or a student might choose  to take time off between courses if there are scheduling conflicts  due to personal or work issues.   1.4 Unique Needs of the Institution  Before diving into the details of how data are utilized by the  University of Phoenix, it is important to emphasize how the  structure of the organization lends itself toward a set of needs that  differ from traditional universities.   One common thread throughout the institution is scale.  While the  university maintains a class size in the mid-teens, there are  hundreds of courses in progress affecting thousands of students on  any given day.  Students have scheduling flexibility because there  is a good chance that the desired course is starting on any week.   We need to think in terms of tens of thousands for course design,  content, facilitation, tutoring, licensing, data collection, or any  other variable function (or terabytes when it comes to data).   Another aspect of our institution is operational efficiency.  The  sheer size of the business combined with the number of campuses  and the centralized administration means services must be simple  and as efficient as possible.  This is common when it comes to  areas such as applications and registrar functions.  However, it  must also be viewed from an academic perspective.  A good  example is in the licensing of software.  Some academic software  products require an instructor to upload a course roster to allow  students to log into the product.  Due to the flexibility of  University of Phoenix scheduling, course rosters might, and  frequently do, change right up to the time class starts. Instructors  do not usually have the roster information in advance.  In this  case, administrators work with the content partners to provide a  single sign-on feature that allows students to automatically login  from our learning management system to the vendor system.   2. OUR APPROACH TO DATA  As an institution, we are very aware of the fact that data will  make or break our ability to educate our students effectively in   the future.  Although we have the advantage of a proprietary  online learning system, we realize that we have not come close to  tapping the potential of the data stored in our systems.   To that end, we have spent a significant amount of time and effort  over the past few years to ensure that data has its place in the  foundation of the organization.  2.1  The Move Toward a Data Driven Culture  While it might sound trite, it is vital that any change starts with  the people who make up the organization.  We started a concerted  effort to stress the importance of data at every rung of the  organizational ladder.   One basic step we took involved messaging.  After a restructuring  of the product and engineering groups in 2009, our new  management focused on three areas of performance:    Site Up    Data   User Experience   The fact that data was one of the three focal areas of the group  was a testament to our commitment to a data-driven culture.  We  followed up on the messaging with key hires in the data arena.   This included bolstering our technical capacity and bringing on  board staff with experience in analytics, cognitive science, and  data-driven consumer products businesses.  The investment in  staff who can move, align, and interpret data is something that  will pay dividends in the future.   2.2 Applications of the Data  Data are different from information.  Data are atomic units; they  set the foundation for capabilities that can have a deep impact on  learning and business.  We ask ourselves, What can we do with  the data once this foundation is set  The following diagram  illustrates our answer to that question:   Reporting and business intelligence is the base of the pyramid.   Although commonplace, we do not want to underestimate the  impact of basic business intelligence.  A good, integrated data  structure can provide simple answers to many questions.   Predictive analytics goes to the next level.  It allows us to answer  the tougher business questions and use data to look ahead.  Data- driven learning is where we apply the data not only to  business/operational questions but to the core activity of our  institution - learning.  These areas will be discussed in more detail  in Section 4.   2.3  Data as a Strategic Advantage  The University of Phoenix is in a unique position as compared to  traditional universities and colleges.  Because we are a for-profit  entity, we need to address business and financial implications in  addition to the implications of learning.  One of the largest advantages we have in the higher education  space is our size.  With over 400,000 students, we have the ability  to use data and analytics that would produce accurate and    123    Figure 1. Pyramid showing the different applications of our data foundation  reproducible results.  We are not limited to testing a new learning  tool on a class of 25 students.  We can test with hundreds or  thousands of students, so long as the trials do not negatively  impact the students ability to learn.  From a data standpoint, that  means that we have more than enough data points to support the  efficacy of the tool.   3. TOPOLOGY It should not be surprising that we have data strewn in different  databases across the entire student lifecycle.  This scattering of  data reduces the efficacy of our analytic capabilities. To combat  that, we set up a replication system where all data flows into a  single integrated repository (see Figure 2).   3.1 Source Systems  The first step in the workflow is to replicate all source systems.   We use a commercial replication tool called Golden Gate to  accomplish this (Golden Gate was acquired by Oracle in 2009).   Golden Gate is used on any of the source systems we want to  move to the integrated repository  these include both Oracle and  SQL databases.   One of the benefits of the replication is that we alleviate the  problem of destructive data.  Normally, if a field in one of the  source systems is overwritten, we lose the older data forever.   With replication, we effective-date the tables so that any  overwrites are saved.  This helps with older systems that  inadvertently destroy data due to a poor/outdated design.   The table below is a listing of some of the source applications that  we replicate to our staging systems.  It is not an exhaustive list.   Figure 2. The flow of data` to our integrated data repository   124    Table 1.  A partial listing of source data systems replicated to a single staging area.   Source system Description   Aptinet Lead/marketing data Apply web Students application to the institution  Osiris Students, courses, schedules, course grades (SIS)  Galaxy Student contact data (CRM)  Gradebook Assignment grades  OLS Online Learning System (discussion forums)  Application Log Content usage tracking  HEAT Tech support issue tracking system   3.2 Integrated Data Repository  The key to our data foundation is the integrated data repository  (IDR).  The IDR is a unified, normalized data structure of all data  elements across the entire company.  Replication copies each  source table to a staging area.  The next step is to travel from  staging to the IDR.  To do this, we needed to rationalize every  field that we moved over.  As an example, we looked at the  students home zip code.  We may have collected that zip code  when the student first contacted the university, we may have the  collected it on an application, and the campus may also have  collected it in the course registration process.  We now have three  instances of the zip code in our systems and regardless of whether  they are the same or not, a student should only have one current  zip code on file.  This is where the data modeling comes into play   Modeling. We have a team of data modelers who work to create a  normalized physical model of all of the data elements.  In our zip  code example, the first thing the modelers do is create the proper  data schema.  Instead of having a Marketing_Person,  Application_Person, and Registrar_Person table, we only have a  single Person table.  The next step is to determine which source  table has the right zip code.  We may determine that the zip code  stored in the registrars database is the proper one to use, so we  designate that field for transport.   Extract, Transform, Load (ETL). After the entire integrated  schema is mapped out, the next step is to populate it.  The ETL  team writes jobs to move the data from staging into the proper  place in the IDR.  A significant part of the ETL process is quality  control.  As data are moved over, we check for inconsistencies  and errors and do what we can to address them.  The IDR is  known as the single source of truth, so consistency and quality are  vital characteristics.   Data Marts. One other facet of the architecture is a data mart.   The IDR is large and normalized; this is not a good combination  for efficient querying.  In order to have a data structure that is  built for fast querying of complex data, we need to de-normalize  and index the data.  There may be multiple data marts in  existence.  One may be a series of tables dealing with students  progression throughout their programs, while another may focus  on financials and accounting.   3.3 Hadoop Due to the size of our institution, we know that we would be  running into issues with the sheer volume of data in some of the  tables.  For example, the discussion forums databases have a  record for each post for every student and faculty in every class.    If you multiply the posts by students (and faculty) by week and  by course, we can see millions of records in a week or even a day.   Mining tables of this size in an efficient manner calls for a  different solution.   The solution we have adopted is an open source product called  Hadoop.  Hadoop was inspired by work at Google and extended  through usage at Yahoo!, Facebook, and other prominent Internet  companies.  Hadoop addresses the problem of large datasets by  using distributed parallel processing.  A Hadoop cluster is made  up of many commodity server nodes - the benefit is to use a large  number of cheap servers instead of a small number of expensive  ones.  The University of Phoenix product group built a 40-node  cluster in 2009, and we are continuing to develop its capabilities.   Hadoop is used to solve specific problems with our data.  It is  most applicable in two cases.  First, it helps to digest large  datasets.  Whether it is the discussion forum tables or raw web  usage logs, Hadoop can process the data, create summarized  tables, and send the summaries back to the IDR.  Second, Hadoop  can help with detailed data analysis on non-fielded data.  The  actual discussion forum posting is a block of text.  In a traditional  database, that block of text is lumped into one field and that  makes it hard to mine the text beyond the use of simple query  statements or regular expressions.  With Hadoop, we can run  cycles of queries or code against the non-fielded data, continually  reducing the problem into smaller chunks.  When we have derived  the information we are looking for, we can write that summarized  information back to the IDR for use with traditional queries.   3.4 Analytics Tools  The last mile of analytics includes any data reporting, analysis,  or visualization tool used to turn data into information.  Following  the mantra of using the right tool for the right job, there are a  number of tools being used within our institution.  A tool like  Microsoft Excel is always available as a failsafe, but we rely on  other products for more specialized needs.   Tableau. Tableau is a commercial data visualization tool whose  strength lies in its ability to figuratively paint many different  kinds of pictures.  Unlike traditional visualization tools where one  might start by selecting the desired type of visualization (e.g.  scatter plot, bar graph), Tableau lets the user start by just adding  measures and dimensions to a palette.  As the user adds fields, one  can either try different visualizations to see how it looks or use  suggested types from Tableau.  The product helps the user paint  the picture that will tell the desired story in the best way possible.   125    R. If the need is to perform statistical calculations or correlations,  R is the right tool for the job.  It is a powerful open source  software product that can complete a myriad of statistical  functions.  PL/SQL. Many times, the need is to simply explore the data in  order to zero in on whatever answer, question, or anomaly one is  looking for.  Our IDR is an Oracle database and a simple SQL  querying tool such as PL/SQL Developer will often be the right  tool for the job.   4. ANALYTICS APPLICATIONS  The data foundation described in this overview is just that  a  foundation.  In and of itself, it has no value.  One must apply the  data towards an end goal such as answering a business question.   The pyramid in Figure 1 shows three levels by which we can  categorize the application of data across our organization.   4.1 Business Intelligence (BI)  The BI team works like many traditional reporting teams.  The  goal is to provide reporting services to the areas of the company  where it is needed.  The kinds of services provided depend on the  needs and capabilities of the requestor.  At its most basic level,  we have the reporting tool.  We use the commercial Business  Objects tool to provide reporting to all areas of the business.  The  departments might author reports on their own or they may put in  a request and have a central reporting team develop the report on  their behalf.   Another variation on reporting is dashboards.  Our development  group can create simplified visual dashboards that answer a few  key business questions in an easy-to-understand manner.  If  reports are good for departments that need to make operational  decisions, dashboards might be better for high-level overviews of  a business process.   4.2 Predictive Analytics  The analytics team at the University of Phoenix is set up to focus  in on the more difficult questions that cannot be easily answered  with a single report.  We just changed the curriculum in a certain  courseis it a change for the better  Is one campus location  doing a better/worse job than another in its ability to deliver  instruction  How many weeks does it take for an MBA student to  get to their fifth course  These are complex questions that require  complex analysis.     There is more than just answering the question, though.  We want  to be able to use data to predict future outcomes so we can stay  ahead of the impending trends.  Predictive analytics can be used  to predict different outcomes including student learning, student  success, marketing channel efficacy, or financial outcomes.   Student Persistence. The University of Phoenix is currently  looking at one specific predictive channel focused on persistence  in a program.  The approach is similar to an actuarial table, but  instead of predicting how long an insured person will live, we  want to look at how long a student will progress through their  program.  As an institution serving non-traditional learners with  competing factors like a job or a family, we know that external  factors can hinder a students ability to stay in the program.  We   may not be able to avoid these factors, but if we see signs of them  coming, we can help the student handle the change in a more  productive manner.   The goal with student persistence is to include as many factors as  possible in a correlation model.  By analyzing past data, we might  be able to determine what factors indicate a high probability that  the student is preparing to withdraw from the program.  The IDR  contains static information such as demographics and active  information such as schedules and grades.  It is our belief that  some of these factors will have a high correlation with a students  intention to withdraw.  Therefore, we will be able to rate the  withdrawal risk and give some sort of a persistence score.   The obvious next question is, So what do we do with this  information  If we are able to predict persistence/withdrawal  with some level of accuracy, then we can proactively help the  students with their decision-making.  All students have an  academic advisor who has the job of assisting the student  throughout their program.  It is our intent to provide the advisors  with up-to-date persistence scores so that the advisors can  intervene and help the student find the best course of action.  We  do not know if the best course of action is remediation, taking a  break in scheduling, or some other solution.  To that end, our  intent is to focus on human intervention (with the advisors)  instead of some automated remediation path.   The student persistence analytics project started in October 2010  and we hope to share results as the project matures.   4.3 Adaptive Learning Engine  The top level of the data application pyramid is an adaptive  learning engine.  This is a longer-term project aimed at the heart  of our institution.  Our goal as a university is to help students  achieve the learning outcomes as specified by the program.  A  project such as student persistence might help to keep the student  enrolled in the program, but it does not address the learning.   We have the desire to leverage all of the student data we have to  help students traverse that optimal learning path.  Through a  combination of data analysis, learner profiling, and a learning  platform that supports multiple paths to achieving the same  outcome, we believe we can guide students down the path that  best fits their individual needs as a learner.   5. CONCLUSION  THE CURRENT STATE  OF ANALYTICS  It has taken the University of Phoenix many months to  establish and populate the IDR, our foundation for analytics.  As  of this writing, the IDR is still not complete and new tables are  continually being added.  However, we are not waiting for it to be  one hundred percent complete.  There is enough data to initiate  reporting and analytics projects that can both provide value to the  company and set the stage for future research.  We are fortunate  to be able to dedicate multiple teams to different aspects of the  analytics and we will continue to share outcomes with  communities such as the Learning Analytics and Knowledge  group as results become available.   126    "}
{"index":{"_id":"24"}}
{"datatype":"inproceedings","key":"Atkisson:2011:LAI:2090116.2090133","author":"Atkisson, Michael and Wiley, David","title":"Learning Analytics As Interpretive Practice: Applying Westerman to Educational Intervention","booktitle":"Proceedings of the 1st International Conference on Learning Analytics and Knowledge","series":"LAK '11","year":"2011","isbn":"978-1-4503-0944-8","location":"Banff, Alberta, Canada","pages":"117--121","numpages":"5","url":"http://doi.acm.org/10.1145/2090116.2090133","doi":"10.1145/2090116.2090133","acmid":"2090133","publisher":"ACM","address":"New York, NY, USA","keywords":"educational intervention, hermeneutics, interpretive inquiry, learning analytics, operationalism, positivism, quantitative inquiry","Abstract":"Web-based learning systems offer researchers the ability to collect and analyze fine-grained educational data on the performance and activity of students, as a basis for better understanding and supporting learning among those students. The availability of this data enables stakeholders to pose a variety of interesting questions, often specifically focused on some subset of students. As a system matures, the number of stakeholders, the number of interesting questions, and the number of relevant sub-populations of students also grow, adding complexity to the data analysis task. In this work, we describe an internal analytics system designed and developed to address this challenge, adding flexibility and scalability. Here we present several examples of typical examples of analysis, discuss a few uncommon but powerful use-cases, and share lessons learned from the first two years of iteratively developing the platform.","pdf":"Learning Analytics as Interpretive Practice: Applying  Westerman to Educational Intervention  Michael Atkisson   Brigham Young University  150 MCKB - BYU  Provo, UT 84602   011 (801) 422-5097   michael.atkisson@gmail.com      David Wiley  Brigham Young University   150-E MCKB- BYU  Provo, UT 84602   011 (801) 422-7071   david.wiley@byu.edu     ABSTRACT  In Westermans [12] disruptive article, Quantitative research as  an interpretive enterprise: The mostly unacknowledged role of  interpretation in research efforts and suggestions for explicitly  interpretive quantitative investigations, he invited qualitative  researchers in psychology to adopt quantitative methods into  interpretive inquiry, given that they were as capable as qualitative  measures in producing meaning-laden results. The objective of  this article is to identify Westermans [12] key arguments and  apply them to the practice of Learning Analytics in educational  interventions. The primary implication for Learning Analytics  practitioners is the need to interpret quantitative analysis  procedures at every phase from philosophy to conclusions.  Furthermore, Learning Analytics practitioners and consumers  must critically examine any assumption that suggests quantitative  methodologies in Learning Analytics are inherently objective or  that Learning Analytics algorithms may replace judgment rather  than aid it. Lastly we propose a method for making observational  data in virtual environments concrete through nested models.     Categories and Subject Descriptors  E.0 [Data]: General   General Terms  Measurement, Theory   Keywords  Learning Analytics, Interpretive Inquiry, Quantitative Inquiry,  Educational Intervention, Operationalism, Positivism,  Hermeneutics   1. INTRODUCTION  In traditional cognitive science inquiry, measurement almost  always involves significant levels of abstraction away from the  phenomena of interest. Usually, observable behavior is of interest  because it is assumed to indicate cognitive phenomena. For  example, in learning measurement, the factors of interest are   unobservable, so behavior observation is used as a proxy. In  online learning environments, however, even greater abstraction  is required in order to conduct inquiry. Behavior, in most online  learning scenarios, is not directly observable. So, second-order  proxies that represent directly observable behavior must also be  constructed. Furthermore, these second-order proxies are typically  encumbered by a severely impoverished vocabularya language  of actions consisting almost exclusively of mouse clicks and  keystrokes.   Hence, online- and blended-learning environments present  situations for inquiry that, in most cases, require even more  examination of assumptions and methodology than is required in  traditional cognitive science inquiry. Despite the potential  advantages of scale in Learning Analytics to investigate learning  phenomena, great care must be taken in order to account for the  philosophy and human judgment behind the measures and  constructs employed in a study in order to interpret results in a  reasonable manner.   1.1 Westermans Interpretive Inquiry and  Learning Analytics  The arguments of Michael Westerman [12], though directed  towards critics of traditional psychology who eschewed  positivism and quantitative methodology, have particular  relevance to Learning Analytics. Westerman invited qualitative  researchers in psychology to adopt quantitative methods into their  practice, which was no small invitation. Most qualitative  researchers avoid quantitative research methods because of their  traditional association with positivist philosophy, which purports  that control, prediction, objectivity, and universal models are the  end-goal of science [12]. Qualitative researchers in social science  are usually interested in questions that address the meaning of  psychological phenomena more than how to replicate them.  Westerman argued that quantitative methods, however, are not  tied to positivism, and in fact are fundamentally interpretive and  meaning-laden. Consequently, researchers interested in questions  that address meaning should adopt quantitative methods into their  repertoire of inquiry tools.   The implications of quantitative methods lacking default  objectivity, requiring interpretation, and addressing questions of  meaning are a watershed for the practice of Learning Analytics.  Given the multiple levels of abstraction involved in identifying  and interpreting behavior in online settings, we contend that  Westermans arguments regarding interpretive quantitative  inquiry have particular relevance to Learning Analytics practice.       Permission to make digital or hard copies of all or part of this work for  personal or classroom use is granted without fee provided that copies are  not made or distributed for profit or commercial advantage and that  copies bear this notice and the full citation on the first page. To copy  otherwise, or republish, to post on servers or to redistribute to lists,  requires prior specific permission and/or a fee.  LAK11, February 27-March 1, 2011, Banff, AB, Canada.   Copyright 2011 ACM ISBN 978-1-4503-1057-4/11/02$10.00.     117    2. THE MISTMATCH BETWEEN  POSITIVSM AND SCIENTIFIC INQUIRY   Mainstream social and physical science are usually associated  with positivism [12]. Positivism, as referred to in this article, is a  philosophy of science in the tradition of Isidore Auguste Marie  Franois Xavier Comte (Auguste Comte) that assumes that  determinate, value-free, causal accounts of phenomena can be  made through objective methodology, hypothesis testing, and  operational definitions [1-2, 10]. A good example of the type of  scientific inquiry that a positivist philosophy is likely to produce  comes in the words of the well-known physicist Stephen  Hawking, who wrote, If one takes the positivist position, as I do,  one cannot say what time actually is. All one can do is describe  what has been found to be a very good mathematical model for  time and say what predictions it makes [8].   In one of Westermans [12] central arguments, he questioned the  appropriateness of employing positivism in the inquiry of  psychological phenomena, given its assumptions of objective  methodology and value-free ontology. In the light of Learning  Analytics, positivist philosophy also poses a formidable  contradiction between its assumptions and the science that  educational interventionists attempt to conduct.   2.1 Positivism and Operational Definitions  So what is the trouble with a philosophy that provides the  rationale for objectivity, causality, and prediction in scientific  inquiry The challenge rests on the central assumption that  objectivity exists in the practice of science. In social science,  operational definitions play a prominent, mediating role that  define how phenomena are observed, measured, and analyzed,  which can hardly be called objective. Westerman explained that  Notwithstanding nearly ubiquitous references to  operationalizing variables and hypotheses about relations  between variables, quantitative research procedures as they are  actually employed do not objectively translate theoretical ideas  about constructs and processes into meaning-free language about  procedures [12].   The challenge that operational definitions pose in research can  also be illustrated by taking a closer look at how they create  abstractionscaricatures of actual behaviors or psychological  phenomena. Similar to many definitions of the term, the Center  for Teaching and Learning at the University of Texas defines  operational definition as, a specific statement about how an  event or behavior will be measured to represent the concept under  study [9]. The language to represent is key here. Operational  definitions do not actually define concepts or observable  behavior, but act as abstracted mediators of how behaviors and  concepts are measured and interpreted. Westerman went on to  say, In fact, what instruments of this kind provide by way of so- called operational definitions are natural language  explanations of each category and examples. Such definitions are  very useful, but they are anything but exhaustive. Indeed, they are  useless if not employed by a coder with a wealth of background  knowledge about the concepts, interpersonal behavior in our  culture, and family life as we are familiar with it [12].   A good example of how operational definitions provide a  challenge in online environments occurred during Fast  Companys Influence Project [2] in the summer of 2010. The  magazine asked its readers to participate by creating a profile on  the projects website. Participants gained influence by how   many people clicked on their profiles. As the project came to a  close, it became apparent that defining influence by the number of  clicks a profile received was not the best measure. A lot of people  tried to game the system, so judgment was required in order to  define what constituted a valid click. The project organizers felt  in the end that influence would have been better defined by how  many participants were able to, not only persuade individuals to  click on their profiles, but also to convert people who clicked on  profiles into creators of their own profiles. Even with the latter  definition of influence, however, many nuanced variations of  online influence could not have been discovered if such a reduced  meaning of influence were used as the sole definition and data  point. From the report, we find that Fast Company used other  means besides click counts in order to distinguish among six types  of online influence: large existing networks, static advertisements,  commoditized celebrity appeal, overt ideology, grass roots  activism, and ability to convince others to participate, which is a  much richer account of profile relationships than a definition as  either number of clicks or number of converts.   The point here is not to avoid systematic inquiry in scientific  observation and analysis of data, but rather to appreciate that,  unlike positivist assumptions of objectivity, interpretation is  required at the most fundamental level of scientific inquiry, given  the inseparable part that human judgment plays in defining the  constructs that researchers examine. Learning analytics  practitioners, particularly should avoid placing confidence in the  idea that observational data collected through web-analytics  measurement tools objectively map on to the constructs they are  investigating through the lens of operational definitions.   2.1.1 Positivism, Operational Definitions, and  Learning Analytics.    Even seemingly simple constructs of potential interest to Learning  Analytics researchers like time on task must first be constructed  (hence the name) before they can be recognized, recorded, and  analyzed. When a student sits silently in a comfortable chair  looking intently at the page of a book, this behavior indicates that  he might be attending to the books contents. However, he might  also be daydreaming about his girlfriend, or worrying about his  fathers illness, or essentially thinking about an infinite list of  other things. Looking intently at the page is only a proxy for  reading.   In online settings, second-order proxies are further abstracted  away from the true subject of a researchers interest. As an  example, take a researcher who is interested in the amount of time  an online student spends on task. Rather than observing a  student sitting silently in a comfortable chair looking intently at  the page of a book, the researcher may have access to a page  load event and a page unload event in a web server log. These  events outline a rough window of time. But was the browser  window containing the text the researcher hoped the student  would read even in focus between the two events Was the  student even in front of the computer while the browser window  was in focus Was the student looking at the browser window, or  texting, or reading a magazine Technological tricks may be able  to help us answer these questions. And when we overcome these  many obstacles, we have only arrived back at the original level of  uncertainty present in direct observation.   Could reasonable people create meaningfully different  operationalizations of the construct time on task in online  settings Could different operationalizations of the construct   118    applied to the same data produce different answers to research  questions If the answer to both these questions is yes, as we  believe it is, the purportedly objective process of conducting  Learning Analytics research is built on a foundation of  subjectivity.   2.2 Positivism and Methodology  Another way in which positivist philosophy diverges from the  practice of science is in the assumption that methods and  instruments objectively display what is being measured;  structured observations do not provide a way to examine  hypothesized associations in a transparent manner [12]. As  Westerman went on to explain, Interpretation plays a role when  it comes to measurement, which lies at the heart of quantitative  research. This point is obvious regarding research based on  clinical judgments about such global constructs as irritable or  submissive. Research of this sort may employ the technical  machinery of Likert-type scales or Q-sort procedures, but it  clearly is based on rich appreciation of the meanings of human  behavior [12]. Just because a concept is associated with a  number does not mean that the association is objective. Judgment,  and hence subjectivity, is always involved when assigning a  metric to phenomena in the real world.   Failing to recognize the nature of human judgments in scientific  methodology has significant consequences regarding the validity  of research conclusions. Joseph Rychlak illustrated these  consequences well in his treatment of Philipp Franks  Philosophy of science: the link between science and  philosophy. The obvious lesson is that science is not only a  methodological endeavor. Constant attention must be given to  theoretical considerationsor, as they might be called,  metaphorical or philosophical considerations...[In] Newtonian  science, the uncritical acceptance of empirical data without  sophisticated study of assumptions lead to a theorization of  scientific method that is, the assumptions of the method were  projected onto the world as a necessary characteristic and then  proved so by the results of these very same method [4]... they  constantly fall into the errors of...confusing what is their  methodological commentary with their theory of explanation  [11]. In other words, not accounting for the method effects will  make it impossible to evaluate whether your conclusions are  accurate or valid, given an inability to distinguish between the  results that represent the psychological phenomena and those that  represent error.   2.2.1 Positivism, Methodology, and Learning  Analytics.  Franks warning about the uncritical acceptance of empirical  data without sophisticated study of assumptions is even more  important in the context of Learning Analytics research. Because  Learning Analytics data is so inexpensively and easily captured,  large collections of data are becoming available for study. Faced  with access to large collections of data and powerful open source  analysis software, researches will be subject to a variety of  temptations to poke about in this data in thoroughly unprincipled  ways. While these fishing expeditions may uncover seemingly  interesting relationships between constructs, without an  interpretive framework grounded in specific theoretical  commitments, the data tail may come to wag the theory dog.   2.3 Positivism and Ontology  Another problem with positivism is how its ontology eschews  meaning. As Stephen Hawking explained, positivism provides no  framework to examine the meaning of phenomena, which is a  necessary consideration when applying research results to the  situations of individuals.   Hawkings belief that prediction is the preponderance of science  is a prime example of the type of philosophy that Philipp Frank  [5] rebuked when saying that science is more than objective  methodology. The philosopher of science noted that, scientific  findings (validated predictions or observations) outstrip the  common sense understanding of them, taking us back to that  condition earlier in history where we could control and predict  without knowing why, what, or how such regularities in events  were really brought about. Man predicted his course of travel  under the stars, controlled the crops through practical know-how,  and cured himself of certain diseases centuries before there was  anything like a scientific account of these beneficial outcomes  [11]. In other words, mathematical models, control, and prediction  are not sufficient to answer questions about why something  happens or what it means. Furthermore, given the irreducibly  interpretive nature of inquiry, not attempting to answer questions  of meaning and purpose may easily lead to the wrong conclusion,  even if one is able to replicate observed behavior.   2.3.1 Positivism, Ontology, and Learning Analytics.  Even if the atheoretical application of Learning Analytics  techniques can uncover stable relationships between an  impoverished lexicon of online behaviors and (equally subjective  measures of) academic performance, we have only arrived back at  that condition earlier in history where we could control and  predict without knowing why. We will never be able to  understand why certain behavioral patterns relate to academic  success or struggle without engaging in explicitly interpretive  workinterpretive work that draws on a wealth of background,  theoretical, and empirical knowledge about the learning process.   Inasmuch as a primary goal of Learning Analytics is to support  and improve learning in human beings, the findings of Learning  Analytics research must be translated in concrete interventions  with human beings before the value of Learning Analytics is  realized. And if the Learning Analytics path leads inexorably to  interventions with human begins, we must consider Learning  Analytics to be an inherently ethical activity. In this context,  asking explicitly interpretive questions of why and how  with regard to the findings of Learning Analytics research gains  importance beyond the requirements of responsible science and  crosses firmly into the realm of ethics, requiring us to proactively  work to protect the interests of the people with whom Learning  Analytics may suggest we intervene by understanding issues of  why and how.   3. LEARNING ANALYTICS,  HERMENEUTICS, HERMENEUTICS, AND   INTERPRETIVE INQUIRY   For a number of reasons outlined above, a positivist view of  Learning Analytics appears to be a combination that leaves out  key components such as tractable, desirable, or ethical. If  positivisms goal of defining universal, perfect models of  phenomena is not achievable because of the subjectivity inherent  in the use and interpretation of inquiry conventions, constructs,  and tools, then what philosophy is reasonable in which to   119    approach science Westerman proposed hermeneutics as an ideal  philosophy of inquiry. It brings meaning and interpretation to the  forefront of its explicit assumptions, Our accounts must always  refer to what people are doing, that is, to meaningful practices,  rather than attempt to fully explain the meaning involved in what  they are doing in some other terms [12].   But how does one arrive at meaningful explanations of  phenomena Westerman identified multiple vehicles for  approaching meaning in scientific inquiry, including metaphors  and reductionism. As he pointed out, however, Note that  these...positions about meaning share something in common.  They represent different ways of maintaining that the nature of  objects is such (whether that nature is characterized by abstract  meanings or the absence of meaning) that a subject reflecting on  those objects from a removed vantage point could arrive at what  Wittgenstein [13] called crystalline understanding, that is, a  complete, determinate account [12]. Approaches to meaning that  assume an objective, removed observer do not fit within the  hermeneutic framework of using Practical activity [as] bedrock  [12]. Hermeneutics differs from crystalline understanding  accounts of meaning by assuming that behavior is concrete and  part of practical, meaning-laden activities. Behavior is concrete  because the people behaving act and live in the world and  within a social context. As we compare Westermans non-  examples of meaning with his proposed interpretation of practical  activity, the case for hermeneutics in Learning Analytics will  begin to emerge.   3.1 Losing Meaning through Metaphor  One way of prescribing meaning to phenomena is by calling out  similarities among new and the known phenomena. Westerman  identified abstractions and metaphors as a path that many  researchers take to achieve this type of meaning, though he felt it  was misleading. According to the traditionrationalism, in  particularmeaning refers to abstract structures that lie behind  the diversity of events. Philosophers proceeding along the lines of  the tradition locate the capacity to appreciate such meanings in  the subjects mind, and psychologists follow suit with ideas about  how there are such abstract structures as scripts or rules inside the  mind [12]. For example, the information processing (IP)  metaphor in cognitive science stands out as a primary instance of  making the metaphor more real than the observed behavior. IP is a  metaphor that cognitive science uses to describe mental processes.  It was derived from computer science [6] and views the world as  information to be inputted into the mind in order to be encoded  for long-term memory [1]. Because computers have inputs,  outputs, processors, and memory, so must humans. Right  Unfortunately for cognitive science, there is no rationale beyond  preference alone to use the IP metaphor in order explain the mind.  Nevertheless, cognitivism has made IP the vernacular for  describing its inquiry, just as the steam engine was the preferred  mind metaphor before the advent of computers (a metaphor which  we enlightened, 21st century scholars now scoff at as  unbelievably puerile).   The problem with metaphors and abstractions is that they all, by  design, illustrate only some properties of the situation or object,  while obfuscating others. Consequently, a metaphor cannot be a  theory of all things. Whichever learning metaphor is employed,  some type of learning will not find place to be adequately  described. In the case of the IP metaphor, it can explain  psychological phenomena as long as they superficially appear  similar to how a computer processes data. As will all abstractions,   IP loses its explanatory power as the psychological phenomena  we attempt to explain diverge from the affordances [7] of the  metaphor. IP reaches its limits in accounting for play, creativity,  exploration, etc.things a computer cannot do. Hence employing  abstractions gives meaning, but the meaning portrayed may not be  sufficiently representative of the phenomena under study.   3.2 Losing Meaning through Reductionism  Another way of assigning meaning to phenomena is by reducing  it to supposed fundamental components, which is to say a  definition of not what something is, but what something is made  of. As previously discussed, linguistic operationalism is a prime  of example reductionism. But behavior can be reduced in other  ways; as Howard Gardner described, It seems to some observers  that an account of the classical psychological phenomenon of  habituation in terms of neurochemical reactions is an important  step on the road to the absorption of cognition by the  neurosciences. Once the basic mechanisms of learning have been  described in this way, no additional level of explanation will be  needed; in a way that would please such behaviorally oriented  philosophers as Richard Rorty, these reductionists believe there is  really nothing more to be said when neurophysiology has had its  say [6]. If the meaning of behavior can be reduced to no meaning  by cutting it down to neurophysiology, the same could be said of  behavior on the electronic networks through which students in  online- and blended-learning environments interact. The  implication would be that behavior is no more than the sum of its  frequency counts, which sheds no light on the meaning of  psychological or learning phenomena.   This by no means is a call to avoid inquiry involving quantitative  analysis. As Westerman explained, The key point here is that  even though mathematics enters into the picture via data analysis,  our examination of phenomena is not mathematical in nature.  Although this statement may seem strange, it is accurate because  the mathematical aspects of the research procedures are embedded  in the larger context of the meaningful, interpretive procedures  [12]. Aspects of behavior may be measured, counted and  analyzed, but the philosophy behind the inquiry determines  whether one interprets an ability as the sum of the observational  data or as a meaning-laden practice in the lives of individuals.   3.3 Finding Meaning through Concreteness in  Learning Analytics  Addressing meaning through interpretive inquiry requires looking  at practical behaviors. Practical points to how the behavior is  concretely embedded in a social practice. As Westerman  eloquently stated, We appreciate the meaning of objects of  inquiry from the inside. Their significance always refers to the  roles they play in the world of practices in which we are already  engaged. This locates meaning in the world (not the dead world of  brute events, but the living world of practices), not in the mind.  As a result, meanings are concrete, not abstractan impossibility  from the point of view of the philosophical tradition, but a central  tenet of a perspective that takes practical activity as the starting  point [12].   But how does viewing the significance of behavior as concrete  change how inquiry is performed Westerman noted that,  Recognizing that behaviors are parts of practices, however, leads  to advocating the use of meaning-laden measurement procedures,  because the significance of behaviors depends on the role they  play in practices [12]. So, interpretive inquiry must include a  way to address the meaning that results from behavior in context.   120    Westerman gave the example of codifying a heated conversation  through either relational codes such as A yells at B or by  decibel-level. He suggested that the objective measure can be  misleading, if, for example, A says something endearing to B  while the two stand on a corner with noisy traffic going by. [12].  Measures recorded and interpreted outside meaning-laden practice  obscure the relationships of interest. The concrete human  experience weaves the interpretation of results into the fabric of  conclusions, not at the loss of objectivity (inquiry is not objective  in the first place), but in the acquisition of meaning, plausibility,  and authenticity.   The question for virtual environments then becomes how to  collect data on and analyze human behavior in context of practice.  One way could be, though there may be others, to create  multilevel, multivariate (or latent), continuous (or count data)  models through structural equation modeling (SEM).  Observational data could be clustered by practices or tasks within  practices (given the appropriate degrees of freedom at each level)  while testing for other organizational fixed and random effects at  and among various structural levels within the data. For example,  such a multilevel SEM model would include level 1 data such as  typing, clicks, time on task, and textual analysis grouped by the  sub-steps of workflow in various tasks such as homework  assignments, document reviews, peer reviews, etc. Clustering  behavior observations within latent and observable tasks and  practices could be seen as forcing abstractions upon the data, but  we feel that to the contrary, nesting observations within practices  has the potential to ground the data concretely (mediated by  interpretation) within the world of human action.   4. CONCLUSION  Learning analytics is an exciting new field of research with the  potential to drastically improve learning. Online learning systems  have become the educational equivalents of physics Large  Hadron Collider, generating massive amounts of quantitative data  that can be subjected to a wide variety of mathematical analyses.  Armed with these huge data sets and powerful computational  tools, there is a temptation for educational researchers to regress  toward positivism in their approaches to inquiry. In this article we  have pointed out problems with the positivist approach in social  science generally and in the context of Learning Analytics  specifically. As Westerman presented the foil, The empiricist  wing of the tradition offers...the idea that the apparent  meaningfulness of events can be reduced to chains of brute  occurrences, behaviors, and sense data. Psychologists turn to this  idea when they argue that they can operationalize constructs and  hypotheses [12]. Nevertheless, meanings are concrete, not  abstract, because they are located in the act of participating in the  practical activity of everyday life. Furthermore, science does not  equal methodology. Methods cannot answer our questions.  Instead, researchers must combine philosophy and science in a  practice of meaning-laden, interpretive inquiry in order to provide  answers to difficult questions, such as how observed patterns in  the population apply to individuals. Consequently, we have  argued that hermeneutics provides a more appropriate  philosophical framework in which to conduct Learning Analytics  research, and have proposed multilevel SEM as one way to  ground data within concrete behaviors. We hope this article will  catalyze a critical discussion in Learning Analytics, enabling  researchers and practitioners to more effectively support learning  in all its forms.   5. ACKNOWLEDGMENTS  Our thanks to Dr. Stephen Yanchar for his thoughtful feedback  and for guiding us to M. A. Westerman and J. F. Rychlak.   6. REFERENCES  [1] Bernstein, D. A. (2006). Memory. Bernsteins Psychology,   7th Edition + Webcard, 6th Edition. Houghton Mifflin  College Div.   [2] Borden, M. (2010, November 1). Measuring Influence One  Click at a Time | Fast Company. Fast Company, (150).  Retrieved from  http://www.fastcompany.com/magazine/150/the-influence- project.html   [3] Bryman, A. (1984). The Debate about Quantitative and  Qualitative Research: A Question of Method or  Epistemology The British Journal of Sociology, 35(1), 75- 92.   [4] Burtt, E. A. (1954). The metaphysical foundations of modern  physical science: the scientific thinking of Copernicus,  Galileo, Newton and their contemporaries. Doubleday.   [5] Frank, P. (1957). Philosophy of science: the link between  science and philosophy. Prentice-Hall.   [6] Gardner, H. E. (1987). The Minds New Science: A History  of the Cognitive Revolution. Basic Books.   [7] Gibson, J. J. (1979). The Ecological Approach to Visual  Perception. Boston: Houghton Mifflin.   [8] Hawking, S. W. (2001). The Universe in a Nutshell (1st ed.).  Bantam.   [9] IAR: Glossary. (n.d.). Retrieved November 6, 2010, from  http://www.utexas.edu/academic/ctl/assessment/iar/glossary. php   [10] Leahey, T. H. (1992). A History of Psychology: Main  Currents in Psychological Thought (3rd ed.). Prentice Hall.   [11] Rychlak, J. F. (1988). The Psychology of Rigorous  Humanism (2nd ed.). New York University Press.   [12] Westerman, M. A. (2006). Quantitative research as an  interpretive enterprise: The mostly unacknowledged role of  interpretation in research efforts and suggestions for  explicitly interpretive quantitative investigations. New Ideas  in Psychology, 24(3), 189-211.  doi:10.1016/j.newideapsych.2006.09.004   [13] Wittgenstein, L. (1968). Philosophical investigations.  Macmillan.   121      "}
